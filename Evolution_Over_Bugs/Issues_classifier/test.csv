Issue Title,Tags
Add dynamic shape tests for important models to guard against regression module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Unexpected poor performance of C++ extension / wish for a fast `operator[]` module: cpp triaged,module: cpp triaged
"Nvidia P100, where to disable upcasting? Plus kernel image missing. module: binaries module: cuda triaged",module: binaries module: cuda triaged
conda package _openmp_mutex  makes pytorch dataloader slower when set num_workers > 0 module: binaries triaged,module: binaries triaged
torch.multinomial - Unexpected (incorrect) results when replacement=True in version 2.1.1+cpu triaged module: random,triaged module: random
torch.einsum may choose a strategy for which there is not enough memory module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
[MPS only] failed assertion `New volume: xxx should match old volume: xxx [reshapeWithCommandBuffer] MPSNDArrayIdentity.' module: crash triaged module: mps,module: crash triaged module: mps
print statements used in torch/utils/cpp_extension.py module: cpp-extensions triaged better-engineering actionable,module: cpp-extensions triaged better-engineering actionable
duplicate code in python_arg_parser.cpp  module: internals triaged,module: internals triaged
`torch.nn.utils.rnn.pack_padded_sequence` heap-buffer-overflow module: nn triaged,module: nn triaged
[CI] Lintrunner takes 60+ min module: ci triaged module: flaky-tests,module: ci triaged module: flaky-tests
single-batch `torch.bmm` is significantly slower with cuBLAS>12.1.0  high priority triage review module: cuda module: third_party topic: performance,high priority triage review module: cuda module: third_party topic: performance
Validations for 2.1.2 release oncall: releng triaged,oncall: releng triaged
Sparse CSC | CSR | Tensor Serialization Load Issue module: sparse module: serialization triaged,module: sparse module: serialization triaged
nn.LSTM tolerates wrong input shape when hidden state isn't provided. module: nn triaged,module: nn triaged
Sparse block tensors (`torch.sparse`) module: sparse feature triaged,module: sparse feature triaged
test_vmapvjpvjp_linalg_tensorsolve_cpu_float32 fails with precision issue triaged module: functorch,triaged module: functorch
`torch.Tensor.index_add` segfault by negative-size-param triaged actionable module: python frontend module: edge cases,triaged actionable module: python frontend module: edge cases
SparseTensor index select uses more CUDA memory than Torch index select module: sparse module: memory usage triaged,module: sparse module: memory usage triaged
[RFC]Intel GPU oneDNN Upstreaming triaged module: intel,triaged module: intel
CUDA graph doesn't update tensor on replay triaged module: cuda graphs,triaged module: cuda graphs
[RFC] Intel GPU Runtime Upstreaming triaged module: intel,triaged module: intel
MacOS tests randomly fail after https://github.com/pytorch/pytorch/commit/165f4f6ccf7522d75df99c30821d583dfc58ad62 high priority triage review module: ci module: macos,high priority triage review module: ci module: macos
Efficient Cholesky and QR updates module: sparse triaged,module: sparse triaged
Addition of Sparse and Dense Tensors Triggers Internal Assertion Failure module: sparse triaged,module: sparse triaged
Unsupported: call_method UserDefinedObjectVariable(_profiler_enabled) __call__ [] {} triaged module: graph breaks,triaged module: graph breaks
Add head_mask for transformers module: nn triaged oncall: transformer/mha,module: nn triaged oncall: transformer/mha
[ONNX] Refactor op level_debug to catch mismatches between ONNX models and ExportedProgram and nn.Module module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
[ONNX] Extend `test_fx_op_conistency.py` to take `ExportedProgram` converter module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
torch.compile x autograd.Function x enum inputs graph breaks triage review triaged oncall: pt2 module: dynamo module: graph breaks,triage review triaged oncall: pt2 module: dynamo module: graph breaks
[AOTAutograd] nit `assert_functional_graph` is misnamed triaged module: aotdispatch,triaged module: aotdispatch
"[aarch64] `nn.Linear(20, 1)` inference fails high priority triage review oncall: jit module: nn module: mkldnn module: regression module: arm",high priority triage review oncall: jit module: nn module: mkldnn module: regression module: arm
aot_module fails to trace modules whose parameters are tied (models with shared params) triaged oncall: pt2 module: pt2-dispatcher,triaged oncall: pt2 module: pt2-dispatcher
torch.cuda.amp.common.amp_definitely_not_available() failed and needs to raise RuntimeError triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
[RFC] Intel GPU Upstreaming  triage review module: intel module: inductor,triage review module: intel module: inductor
AppleSilicon binaries are build without OpenMP support high priority triage review oncall: releng,high priority triage review oncall: releng
`torch.from_numpy` does not support `set_default_device` module: docs triaged actionable module: tensor creation,module: docs triaged actionable module: tensor creation
pytorch cpuinfo submodule to be updated to the latest module: build triaged,module: build triaged
Unexpected Results in PyTorch Tensor Operations with Python Scalars triaged module: numpy,triaged module: numpy
"Upgrading from 1.8.1 to 1.13.0 causes exported ONNX file enlarged greatly, printable graph changed, and crashed when converting to TensorRT module: onnx triaged",module: onnx triaged
'SymFloat' object has no attribute 'is_integer' good first issue triaged oncall: pt2 module: dynamic shapes,good first issue triaged oncall: pt2 module: dynamic shapes
Dynamo guards key error for `guarded_backend_cache.cached_backends` triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
torch.fft.ifft crashes for empty input triaged module: fft,triaged module: fft
__torch_dispatch__ fails on functionalization when lazy device and `out=` arg is used triaged module: __torch_dispatch__ module: lazy module: functionalization,triaged module: __torch_dispatch__ module: lazy module: functionalization
Error when calculating the Jacobian of torch.conj using forward-mode differentiation module: autograd triaged module: complex has workaround needs design,module: autograd triaged module: complex has workaround needs design
[DeviceMesh] back DeviceMesh initialization by custom_pg oncall: distributed triaged,oncall: distributed triaged
RNN argument order module: nn triaged actionable,module: nn triaged actionable
[RFC] macOS x86 builds / test deprecation module: binaries module: ci triaged module: macos,module: binaries module: ci triaged module: macos
Missing packaging dependency in torch 2.1.x oncall: releng triaged module: tensorboard,oncall: releng triaged module: tensorboard
`torch.compiler.disable` causes a guard failure triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
Flawed testing of onesidedness in istft triaged module: fft,triaged module: fft
"In version 2.1, libtorch needs to be woken up every time it is called after the model is initialized, which means that every time the model is called, it is very slow to predict the first picture. needs reproduction module: cpp triaged",needs reproduction module: cpp triaged
inconsistency between nan cast to int32 on CPU and GPU module: cuda triaged module: NaNs and Infs,module: cuda triaged module: NaNs and Infs
inconsistency on torch.clamp module: cuda triaged module: NaNs and Infs,module: cuda triaged module: NaNs and Infs
Internal CI for libTorch module: windows triaged,module: windows triaged
Build LibTorch for Windows ARM64 module: windows triaged,module: windows triaged
Create a reproducible build for LibTorch x64 on VS2022 module: windows triaged,module: windows triaged
[torch.jit.script] Torchscript produces incorrect result when argmax result is used in indexing oncall: jit,oncall: jit
size error when using bits-level ops + broadcasting + view triaged oncall: pt2,triaged oncall: pt2
`PYTORCH_NO_CUDA_MEMORY_CACHING=1` with `torch.multiprocessing` shared tensors seems to perform use-after-free triage review module: multiprocessing module: cuda,triage review module: multiprocessing module: cuda
Tensor copied over to multiple GPUs on its own needs reproduction module: cuda module: memory usage triaged,needs reproduction module: cuda module: memory usage triaged
Bug in element-wise multiplication of `torch.sparse_csr_tensor`s on GPU - 0's in result considered significant - PyTorch 2.1.1 module: sparse triaged module: regression,module: sparse triaged module: regression
"[debugging] Given a flag, `FakeTensor`s should store metadata about their creation stacktrace  triage review triaged enhancement oncall: pt2 module: fakeTensor module: pt2-dispatcher",triage review triaged enhancement oncall: pt2 module: fakeTensor module: pt2-dispatcher
[Inductor] StableDiffusion unet with `cudagraphs` backend raises fake tensor mismatch error triaged module: cuda graphs oncall: pt2 module: inductor,triaged module: cuda graphs oncall: pt2 module: inductor
Update error message on cache size exceeded to mention whether it's from `cache_size_limit` or `accumulated_cache_size_limit` triaged oncall: pt2,triaged oncall: pt2
Change `automatic_dynamic_shapes` to trigger on `cache_size_limit` recompiles but not `accumulated_cache_size_limit` recompiles. triage review triaged oncall: pt2 module: dynamic shapes module: dynamo,triage review triaged oncall: pt2 module: dynamic shapes module: dynamo
"Raise `torch._dynamo.config.accumulated_cache_size_limit` higher, or potentially just remove it altogether. triage review triaged oncall: pt2 module: dynamo",triage review triaged oncall: pt2 module: dynamo
_foreach_supported_types is a list type while it is used for `in` check of _default_to_fused_or_foreach method in optimizer.py module: optimizer triaged,module: optimizer triaged
Parameters between models don't copy in the C++ Pytroch Frontend under windows module: windows module: cpp triaged,module: windows module: cpp triaged
[inductor][cpu]phi_1_5 accuracy failure and AMP single thread performance regression module: performance oncall: pt2 module: cpu inductor,module: performance oncall: pt2 module: cpu inductor
[inductor][cpu]detectron2_fcos_r_50_fpn accuracy and performance failure module: performance oncall: pt2 module: cpu inductor,module: performance oncall: pt2 module: cpu inductor
Installation error: 'CMake Error at third_party/benchmark/CMakeLists.txt:304 (message):   Failed to determine the source files for the regular expression backend' needs reproduction module: build triaged,needs reproduction module: build triaged
Transformer with convolutional position wise feed forward network  module: nn triaged needs research,module: nn triaged needs research
The behavior of Using different device in Autocast context  triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
Torch Cpu Memory Leak with FastApi uvicorn. needs reproduction module: memory usage triaged,needs reproduction module: memory usage triaged
There seems to be redundant calling of findOp in findSchemaOrThrow method of Dispatcher.cpp triaged module: dispatch,triaged module: dispatch
[BE] remove SmallVector optimization in PyInterpreter.cpp when storing custom sizes triaged module: __torch_dispatch__ module: dynamic shapes,triaged module: __torch_dispatch__ module: dynamic shapes
torch.compile <> __torch_dispatch__ support tracker issue module: __torch_dispatch__ oncall: pt2 module: aotdispatch module: pt2-dispatcher,module: __torch_dispatch__ oncall: pt2 module: aotdispatch module: pt2-dispatcher
__torch_dispatch__ + compile: extra guards high priority triaged module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,high priority triaged module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher
refactor TracingContext to take a more limited subset of ViewAndMutationMeta good first issue oncall: pt2 module: aotdispatch module: pt2-dispatcher,good first issue oncall: pt2 module: aotdispatch module: pt2-dispatcher
"[BE] AOTAutograd, refactor `run_functionalized_fw_and_collect_metadata` to not take in `is_training` flag. triaged better-engineering oncall: pt2 module: aotdispatch module: pt2-dispatcher",triaged better-engineering oncall: pt2 module: aotdispatch module: pt2-dispatcher
[BE] better testing for subclasses + compile triaged better-engineering module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,triaged better-engineering module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher
[BUG][pytree] equal `dict`s do not imply equal leaves and equal treespecs triaged module: pytree,triaged module: pytree
Respect user-specified USE_ROCM/USE_CUDA module: build module: rocm triaged enhancement,module: build module: rocm triaged enhancement
TypeError: unhashable type: non-singleton SymInt in AOTAutograd merge_view_inputs triaged oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,triaged oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher
Improve usability of CUDA package by adding description of CUDA module: docs triaged enhancement,module: docs triaged enhancement
Register Meta func for aten::_cslt_sparse_mm module: sparse triaged,module: sparse triaged
Custom Process Group for Each Module in FSDP triaged module: fsdp,triaged module: fsdp
Pytorch DDP across nodes: self._store = TCPStore( # type: ignore[call-arg] RuntimeError: Stop_waiting response is expected oncall: distributed triaged,oncall: distributed triaged
"[WARNING] could not load None, generating random data instead is too spammy triaged module: minifier",triaged module: minifier
The 'out'-parameter in torch.matmul() works for 'cuda' device but not for 'cpu' module: cpu module: error checking triaged module: correctness (silent) module: intel,module: cpu module: error checking triaged module: correctness (silent) module: intel
Incorrect line in description of torch.frombuffer() method module: docs triaged actionable module: python frontend,module: docs triaged actionable module: python frontend
[AOTAutograd] Incorrect CSE aliasing while `requires_grad` meta differs oncall: pt2,oncall: pt2
[AOTAutograd] torch.compile under ambient `no_grad` is broken high priority triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,high priority triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher
`AdaptiveLogSoftmaxWithLoss` division by zero module: cpp module: nn module: error checking triaged actionable,module: cpp module: nn module: error checking triaged actionable
"When cuda graph captures multi-stream function, allocations are not serviced from the correct pool triaged module: cuda graphs",triaged module: cuda graphs
Mutation after `tensor.expand` returns wrong result. high priority triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,high priority triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher
Per-Parameter-Sharding FSDP Tracker triaged module: fsdp,triaged module: fsdp
Compile pytorch for ppc64 redhat8 module: build triaged module: POWER,module: build triaged module: POWER
Minifier doesn't work with dynamic shapes triaged oncall: pt2 module: dynamic shapes module: minifier,triaged oncall: pt2 module: dynamic shapes module: minifier
Recompilation triggered at each step of the loop involving array indexing triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
"Tracing per-param sharding FSDP: User Defined Objects as inputs to HOPs (autograd.Function, specifically)  triaged module: fsdp oncall: pt2 module: higher order operators module: distributed module: compiled autograd module: pt2-dispatcher",triaged module: fsdp oncall: pt2 module: higher order operators module: distributed module: compiled autograd module: pt2-dispatcher
"Tracing per-param sharding FSDP: Tensor keys in dicts over-rely on `specialized_value`, potentially soundness bug triaged module: fsdp oncall: pt2 module: dynamo module: distributed",triaged module: fsdp oncall: pt2 module: dynamo module: distributed
Tracing per-param sharding FSDP: RemovableHandle -> RemovableHandleVariable  module: nn triaged module: fsdp oncall: pt2 module: distributed,module: nn triaged module: fsdp oncall: pt2 module: distributed
"Tracing per-param sharding FSDP: Streams, Stream reconstruction  triaged module: fsdp oncall: pt2 module: dynamo module: distributed",triaged module: fsdp oncall: pt2 module: dynamo module: distributed
Align on the minimum supported Linux version (CentOS 7 is EOL in july 2024) module: cuda oncall: releng triaged,module: cuda oncall: releng triaged
Tracing per-param sharding FSDP oncall: distributed triaged module: fsdp oncall: pt2 module: dynamo module: distributed,oncall: distributed triaged module: fsdp oncall: pt2 module: dynamo module: distributed
torch.linalg.matrix_rank fails on mps triaged module: mps,triaged module: mps
manylinux_2_28 support module: build triaged enhancement,module: build triaged enhancement
inductor with dynamic shapes fails on float tensor creation from a tuple of ints high priority triaged module: correctness (silent) oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,high priority triaged module: correctness (silent) oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher
[Inductor][gpu][miscompile] Outputs of torch.interpolate abnormally change when swapping output sequence module: cuda triaged module: correctness (silent) bug oncall: pt2 module: inductor topic: fuzzer,module: cuda triaged module: correctness (silent) bug oncall: pt2 module: inductor topic: fuzzer
boolean masking creates a graph break due to aten.nonzero triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
torch.nn.functional.max_pool2d outputs inf module: nn module: error checking triaged module: edge cases,module: nn module: error checking triaged module: edge cases
`torch.gradient` heap buffer overflow high priority triaged actionable module: python frontend,high priority triaged actionable module: python frontend
[inductor] huggingface diffusers randn not replaced triaged bug oncall: pt2 module: decompositions module: inductor,triaged bug oncall: pt2 module: decompositions module: inductor
Remove temp variable to improve efficiency in `get_data_ptrs` for `TensorIterator` module: performance triaged module: TensorIterator,module: performance triaged module: TensorIterator
Custom backend not called for compiling backward graph triaged module: backend oncall: pt2,triaged module: backend oncall: pt2
Unsupported operator error: `aten::to_mkldnn` export to ONNX not supported module: onnx triaged module: mkldnn onnx-triaged onnx-needs-info,module: onnx triaged module: mkldnn onnx-triaged onnx-needs-info
funccol collectives rewrite in dynamo does not work w/ kwargs oncall: distributed good first issue triaged oncall: pt2 module: dynamo,oncall: distributed good first issue triaged oncall: pt2 module: dynamo
Unable to build on CUDA 11.8 due to cutlass incompatibility module: build module: cuda triaged,module: build module: cuda triaged
"Indexing into torch.nn.Modulelist with indices >= 2 returns ""torch._dynamo.exc.InternalTorchDynamoError: SymNodeVariable() is not a constant"" triaged oncall: pt2 module: dynamic shapes",triaged oncall: pt2 module: dynamic shapes
Consider adding y/x -> y * 1/x optimization for `_foreach_div_.ScalarList` and other div Scalar overloads module: optimizer triaged actionable module: mta,module: optimizer triaged actionable module: mta
Lintrunner on all files fails locally even though it passes in CI module: lint triaged,module: lint triaged
[Tracker] Inconsistencies between CPU and GPU computation module: numerical-stability module: cuda triaged,module: numerical-stability module: cuda triaged
Mutating graph inputs on torch.export fails triaged export-triage-review oncall: export,triaged export-triage-review oncall: export
test_arange_dynamic doesn't work with capture scalar outputs triaged bug oncall: pt2 module: dynamic shapes module: inductor,triaged bug oncall: pt2 module: dynamic shapes module: inductor
torch.vmap doesn't compose with torch.cond triaged module: vmap oncall: pt2 module: functorch module: higher order operators module: pt2-dispatcher,triaged module: vmap oncall: pt2 module: functorch module: higher order operators module: pt2-dispatcher
Inconsistency of state_dict loading across devices module: nn triaged,module: nn triaged
Cannot fullgraph differentiate through boolean masking triaged module: boolean tensor oncall: pt2 module: dynamic shapes,triaged module: boolean tensor oncall: pt2 module: dynamic shapes
Why there is no nn.LPPool3d? module: nn triaged enhancement actionable,module: nn triaged enhancement actionable
Typo in example of torch.linalg.solve_triangular module: docs triaged module: linear algebra actionable,module: docs triaged module: linear algebra actionable
[JIT] - torch.script - 'Optional[Tensor]' object has no attribute or method 'size' oncall: jit,oncall: jit
Add the possibility to pass a `Generator` to `gumbel_softmax` module: nn triaged enhancement module: random,module: nn triaged enhancement module: random
Unexpected `None` value for stream with dynamo module: cuda triaged oncall: pt2 module: dynamo,module: cuda triaged oncall: pt2 module: dynamo
Found nn.LazyBatchNorm1d(0) has inconsistency bug between GPU and CPU testing module: nn triaged,module: nn triaged
Nested tensors fail on Conv2D triaged module: nestedtensor,triaged module: nestedtensor
inconsistency in torch.Tensor.scatter on GPU and CPU module: numerical-stability triaged,module: numerical-stability triaged
 torch.matrix_exp(x) get inf and nan module: cuda module: cpu triaged,module: cuda module: cpu triaged
Dynamic shapes not properly supported for nested tensor / tensor subclasses triaged module: nestedtensor tensor subclass module: dynamic shapes,triaged module: nestedtensor tensor subclass module: dynamic shapes
Undocumented CUDA graphs requirement that kernels must use stream triaged module: custom-operators module: cuda graphs,triaged module: custom-operators module: cuda graphs
Nan on torch.corrcoef(x.t()) triaged,triaged
Poetry Torch 2.1.1+cu121 problem on Windows module: binaries module: windows triaged,module: binaries module: windows triaged
IndexError: map::at with MPI CUDA collectives module: multiprocessing module: cuda triaged,module: multiprocessing module: cuda triaged
Load model from jit script format. Repeating inference several times can lead to errors. oncall: jit,oncall: jit
[pt2] Make error message clearer for torch.compile when running on windows module: windows triaged oncall: pt2,module: windows triaged oncall: pt2
[v2.1.2] Release Tracker triaged,triaged
iSTFT gives wrong results for some batched input module: cuda triaged module: correctness (silent) module: fft,module: cuda triaged module: correctness (silent) module: fft
AOTAutograd silently drops required runtime assertions triaged module: dynamic shapes,triaged module: dynamic shapes
Unknown CUDA Architecture Name 9.0a in CUDA_SELECT_NVCC_ARCH_FLAGS (compiling from source) module: build module: cuda triaged,module: build module: cuda triaged
torch.jit.trace has incorrect execution for += operation during compilation oncall: jit,oncall: jit
Tensorboard list of tensors as input triaged module: tensorboard,triaged module: tensorboard
"libtorch_cpu.so: Undefined symbol ""__assert_fail"" module: binaries triaged",module: binaries triaged
deep copy 'HigherOrderOperator' bug triaged module: higher order operators,triaged module: higher order operators
How to  re-use torch.compile results in different python processes? high priority triaged oncall: pt2 module: dynamic shapes module: dynamo,high priority triaged oncall: pt2 module: dynamic shapes module: dynamo
Register a torch op for `functorch.experimental.control_flow.map` to lower triaged module: xla oncall: pt2 module: functorch module: pt2-dispatcher,triaged module: xla oncall: pt2 module: functorch module: pt2-dispatcher
Sympy reasoning with true division is broken module: dependency bug triaged,module: dependency bug triaged
Tensor.requires_grad_() does not trace with make_fx triaged oncall: pt2 module: ProxyTensor pre_dispatch tracing module: pt2-dispatcher,triaged oncall: pt2 module: ProxyTensor pre_dispatch tracing module: pt2-dispatcher
Native c10d_functional collectives on inductor + CUDAGraphTrees generate wrong results triaged module: c10d bug oncall: pt2 module: inductor,triaged module: c10d bug oncall: pt2 module: inductor
Fine grained SymNode logging good first issue triaged oncall: pt2 module: dynamic shapes,good first issue triaged oncall: pt2 module: dynamic shapes
torch.fx export graph error  triaged oncall: pt2 oncall: export,triaged oncall: pt2 oncall: export
RuntimeError in use torch 2.1.0 cuda 11.8  module: cuda triaged,module: cuda triaged
torch.einsum is stuck in mp.Process needs reproduction module: multiprocessing triaged,needs reproduction module: multiprocessing triaged
cannot backprop a cnn when intermediate output has size larger than 2**31 module: cuda triaged module: 64-bit actionable module: edge cases,module: cuda triaged module: 64-bit actionable module: edge cases
Update _scaled_mm to support addmm with bias.size == out.size triaged module: float8,triaged module: float8
[ONNX] ONNX export of simple quantized model fails module: onnx low priority triaged,module: onnx low priority triaged
"Error during DDP, torch.compile, and cudagraph_trees oncall: distributed triaged oncall: pt2 module: inductor",oncall: distributed triaged oncall: pt2 module: inductor
InternalTorchDynamoError rewrapping loses exception chaining triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
consistency checks for across minor version builds module: binaries module: build triaged,module: binaries module: build triaged
[FSDP] Raise error when applying FSDP to `nn.ModuleList` or `nn.ModuleDict` triaged module: fsdp,triaged module: fsdp
`torch.compile` fails when applied to tensor views that have been modified by in-place operators triaged oncall: pt2,triaged oncall: pt2
torch.compile CUBLAS_STATUS_EXECUTION_FAILED high priority triaged module: ddp oncall: pt2 module: inductor module: distributed,high priority triaged module: ddp oncall: pt2 module: inductor module: distributed
torch.compile CUDNN_STATUS_MAPPING_ERROR high priority triaged module: ddp oncall: pt2 module: inductor module: distributed,high priority triaged module: ddp oncall: pt2 module: inductor module: distributed
Unsupported builtin operators for torch.export.export high priority triaged export-triaged oncall: export,high priority triaged export-triaged oncall: export
[cusparseLt] CUDA error: internal error when calling `cusparseLtStructuredDescriptorInit` module: cuda triaged,module: cuda triaged
[DTensor][TP] 1-way TP raises index out of range triaged better-engineering module: dtensor,triaged better-engineering module: dtensor
torch.profiler Trace view in Tensorboard is displayed as empty on RoCm version of PyTorch module: rocm low priority triaged module: tensorboard oncall: profiler,module: rocm low priority triaged module: tensorboard oncall: profiler
"Cannot cast tensor to numpy array inside vmap due to ""Access data pointer of tensor that doesn't have storage"" triaged module: numpy module: vmap",triaged module: numpy module: vmap
New feature: Balanced Sampler module: dataloader triaged module: data,module: dataloader triaged module: data
The cuda batched GEMM has a poor performance for bigger batch size with smaller matrix size module: performance module: cuda triaged matrix multiplication,module: performance module: cuda triaged matrix multiplication
Plan for transformer module based ROCm module: rocm triaged ciflow/rocm,module: rocm triaged ciflow/rocm
torch.compile + SAC: mutations in backward are not preserved module: checkpoint triaged oncall: pt2 module: aotdispatch module: distributed module: pt2-dispatcher,module: checkpoint triaged oncall: pt2 module: aotdispatch module: distributed module: pt2-dispatcher
"lax.cond, lax.switch alternatives triaged module: functorch",triaged module: functorch
[ONNX] Refactor xfail API to handle conditional failure scenarios module: onnx triaged,module: onnx triaged
`preserve_rng_state=True` in torch.utils.checkpoint causes error when used with torch.compile + selective checkpointing + CUDA triaged oncall: pt2,triaged oncall: pt2
[cuDNN][cuDNN V8 API] cuDNN Flash-Attention Upstreaming RFC/tracking issue module: cudnn module: cuda oncall: transformer/mha,module: cudnn module: cuda oncall: transformer/mha
`log_softmax` could be `2**124` to `2**1021` times more accurate on small outputs module: nn triaged module: multi-headed-attention,module: nn triaged module: multi-headed-attention
torch.compile doesnt respect use_determistic_algorithms during the backward() triaged module: determinism oncall: pt2 module: aotdispatch module: pt2-dispatcher,triaged module: determinism oncall: pt2 module: aotdispatch module: pt2-dispatcher
C++ backtrace logging for dynamic shapes specialization triaged ezyang's list module: dynamic shapes,triaged ezyang's list module: dynamic shapes
[export] Support capturing of non-tensor inputs triaged export-triaged oncall: export,triaged export-triaged oncall: export
Failure to resume from a normal (non-FSDP) checkpoint due to the optimizer state dict rekey module: optimizer triaged module: fsdp,module: optimizer triaged module: fsdp
FP8 types should not participate in type promotion and should have no math ops defined on them high priority triaged module: float8,high priority triaged module: float8
Error in torch.set_default_tensor_type() documentation online (depreciated) module: docs triaged actionable,module: docs triaged actionable
Distinguish immutable/mutable fake tensor mode triaged module: fakeTensor module: dynamo,triaged module: fakeTensor module: dynamo
interpolate::trilinear returns wrong gradients on CUDA module: autograd module: cuda triaged,module: autograd module: cuda triaged
torch.compile crash on sdxl unet compile with AMD 7900XTX needs reproduction module: rocm triaged oncall: pt2,needs reproduction module: rocm triaged oncall: pt2
App crashes when I attempt to run it with an iOS xcframework that relies on the LibTorch-Lite CocoaPod module: binaries triaged module: ios,module: binaries triaged module: ios
Graph breaks in APEX `FusedRMSNorm` causes bad interaction between NCCL allreduce and cudagraph tree triaged module: ddp module: cuda graphs oncall: pt2 module: dynamo module: distributed,triaged module: ddp module: cuda graphs oncall: pt2 module: dynamo module: distributed
Segmentation fault in RPC worker when DataLoader has num_workers > 0 module: dataloader triaged module: distributed,module: dataloader triaged module: distributed
Implement FlashFFTConv algorithm triaged oncall: transformer/mha module: multi-headed-attention,triaged oncall: transformer/mha module: multi-headed-attention
Fake tensor produces incorrect values w/ is_coalesced and sparse_coo module: sparse triaged module: fakeTensor,module: sparse triaged module: fakeTensor
bsr_dense_mm may produce incorrect results depending on triton kernel num_stages parameter module: sparse triaged module: correctness (silent) bug,module: sparse triaged module: correctness (silent) bug
mps bug: failed assertion `[MPSNDArrayDescriptor sliceDimension:withSubrange:] error: subRange.start (8192) is not less than length of dimension[1] (512)' triaged module: mps,triaged module: mps
"OSS Test Failures surfaced internally, but not detected externally module: ci triaged",module: ci triaged
Creating Gaussian Mixture Models with MultivariateNormal module: distributions module: nn triaged,module: distributions module: nn triaged
Deepcopying an exported model changes numerics for MobileNetV2 triaged export-triage-review oncall: export,triaged export-triage-review oncall: export
t.contiguous() ~10 slower in eager mode compared to torch.compile module: performance module: cuda triaged,module: performance module: cuda triaged
2.1.0 export moduleï¼šWhy can't export() capture reverse graphs? triaged export-triage-review oncall: export,triaged export-triage-review oncall: export
CUDA extension error message doesn't look correct module: cpp-extensions module: cuda triaged,module: cpp-extensions module: cuda triaged
"P0: Improve failure trace back when crashed to identify the cause of a crash and the ranks that the crash, output the real traceback at last. triaged module: c10d module: elastic",triaged module: c10d module: elastic
P0: Logging Granularity checks fixes across torch.distributed + torchelastic launcher triaged module: c10d,triaged module: c10d
"P0: Integrate distributed logger with TORCH_LOGS, make sure upper level library can't override distributed LOG level. triaged module: c10d module: distributed",triaged module: c10d module: distributed
[PTD] Logging Improvements Main Task triaged module: c10d module: distributed,triaged module: c10d module: distributed
Enhanced RNG State Management with Index-Based Control for Graph-Safe Tensor Parallelism module: cuda triaged,module: cuda triaged
torch.compile error triaged oncall: pt2,triaged oncall: pt2
"Make ""torch.load"" multi threaded process module: serialization triaged needs research",module: serialization triaged needs research
[Feature request] `stft` doesn't have `pad_value` argument triaged module: fft,triaged module: fft
_fused_sdp_choice returns wrong backend in pt2 triaged oncall: transformer/mha module: multi-headed-attention,triaged oncall: transformer/mha module: multi-headed-attention
SDPA Tutorial - libcuda.so not found error for torch compile on Google Colab triaged bug oncall: pt2 upstream triton,triaged bug oncall: pt2 upstream triton
torchrun discarding --rdzv-endpoint when it should not triaged module: elastic,triaged module: elastic
[ONNX] STFT ExportProgram error module: onnx triaged,module: onnx triaged
"FSDP.forward() fails ""_is_root should not have been set"" error after saving a distributed checkpoint triaged module: fsdp module: distributed",triaged module: fsdp module: distributed
/nodefaultlib:vcomp doesn't seem to be set when compiling with Intel OpenMP on Windows module: build module: windows triaged,module: build module: windows triaged
Tensors Can't be  Overwritten in Visual Studio Windows module: crash module: windows module: cpp triaged,module: crash module: windows module: cpp triaged
Should `_native_batch_norm_legit_functional` be in native_functions.yaml? module: nn triaged,module: nn triaged
Expose FakeTensor and FakeTensorMode as public APIs triaged module: fakeTensor,triaged module: fakeTensor
Remove approvals when reverting a pr triaged module: devx,triaged module: devx
cuDNN error: CUDNN_STATUS_MAPPING_ERROR on gtx_1080/A10 when conv1d is called module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
ONNX Export - miscompilation for complex-valued operators module: onnx triaged,module: onnx triaged
`CompileProfiler` reports graph breaks while `dynamo.explain` reports no graph breaks triaged bug oncall: pt2 module: dynamo,triaged bug oncall: pt2 module: dynamo
[inductor][cpu] freezing caused a lot of CV model crashed triaged bug oncall: pt2 module: cpu inductor,triaged bug oncall: pt2 module: cpu inductor
code crashes on CI when using keep-going flag on PR triaged module: devx,triaged module: devx
Torch compile with DDP errors on parameterized modules triage review oncall: distributed triaged module: ddp module: nn.utils.parametrize bug oncall: pt2 module: dynamo module: distributed,triage review oncall: distributed triaged module: ddp module: nn.utils.parametrize bug oncall: pt2 module: dynamo module: distributed
Potential Precision Issue in SumKernel CPU Implementation Due to acc_type Selection module: cuda triaged,module: cuda triaged
Early testing stop logic for CUDA error looks wrong for instantiated_test with pytest module: cuda triaged module: testing,module: cuda triaged module: testing
TorchXLA - owner @JackCaoG triaged module: xla,triaged module: xla
[pytree] implement key path API triaged module: pytree,triaged module: pytree
Incorrect stride when permuting shapes where a zero dimension is present. triaged module: edge cases module: empty tensor,triaged module: edge cases module: empty tensor
Gradient of image rotation triaged enhancement module: forward ad,triaged enhancement module: forward ad
Torch2.1 returned  by calling new_group() or _get_default_group() is incorrect oncall: distributed,oncall: distributed
`torch._C._cuda_getDeviceCount` inflates system memory usage module: cuda triaged,module: cuda triaged
[aot_autograd] Handle `requires_grad` mutation in AOTAutograd feature module: autograd triaged module: aotdispatch module: pt2-dispatcher,feature module: autograd triaged module: aotdispatch module: pt2-dispatcher
[dynamo / aot_autograd] AOTAutograd does not guard on input `requires_grad` setting changing for backwards module: autograd triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,module: autograd triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher
Provide a way to AOT torch.compile and serialize a model triage review feature triaged oncall: pt2 module: dynamo module: startup-tracing-compile time module: distributed,triage review feature triaged oncall: pt2 module: dynamo module: startup-tracing-compile time module: distributed
[dynamo] Assigning result of Tensor in-place op destroys mutation tracking triaged bug oncall: pt2 module: dynamo,triaged bug oncall: pt2 module: dynamo
PT2 improperly executes ambient saved_tensors_hooks triage review module: autograd triaged bug oncall: pt2 module: pt2-dispatcher,triage review module: autograd triaged bug oncall: pt2 module: pt2-dispatcher
NCCL error of PyTorch 2.1.0 when using multiple gpus oncall: distributed,oncall: distributed
dataloader use lmdb stuck triaged module: deadlock release notes: dataloader,triaged module: deadlock release notes: dataloader
EPOCH_DEPRECATION_WARNING in ChainedScheduler.step triaged module: LrScheduler topic: deprecation,triaged module: LrScheduler topic: deprecation
`inference_mode` before training results in FSDP AssertionError triaged module: fsdp,triaged module: fsdp
Higher train loss and worse evaluation metrics when using `torch.compile()` high priority triage review needs reproduction triaged oncall: pt2 module: pt2 accuracy,high priority triage review needs reproduction triaged oncall: pt2 module: pt2 accuracy
[Distributed][DTENSOR] Some operators registered under compositeimplicitautograd key can not work if I registered them under privateuse1 oncall: distributed module: dispatch module: dtensor,oncall: distributed module: dispatch module: dtensor
torch._foreach_mul_ segmentation fault module: crash module: error checking triaged module: numpy module: edge cases,module: crash module: error checking triaged module: numpy module: edge cases
different outputs on `torch.asinh` between eager mode and torch.compile triage review triaged bug oncall: pt2,triage review triaged bug oncall: pt2
"Aborted (core dumped) after Run Pytorch2.0.0, which I compiled by myself needs reproduction module: build triaged",needs reproduction module: build triaged
Compilation error on loongarch64 module: build triaged module: third_party,module: build triaged module: third_party
Output mismatch of torch.sum with torch.compile when swapping the input parameters of torch.mul on CPU triaged bug oncall: pt2 module: cpu inductor,triaged bug oncall: pt2 module: cpu inductor
NCCL watchdog thread terminated with exception  oncall: distributed module: nccl,oncall: distributed module: nccl
RuntimeError on `torch.unqiue_consecutive` with torch.compile( fullgraph = true) triaged bug oncall: pt2 module: dynamic shapes,triaged bug oncall: pt2 module: dynamic shapes
FSDP does not move modules without parameters to device triaged module: fsdp,triaged module: fsdp
Add docs for __tensor_flatten__ / __tensor_unflatten__ triage review module: docs triaged module: __torch_dispatch__ tensor subclass oncall: pt2 module: pt2-dispatcher,triage review module: docs triaged module: __torch_dispatch__ tensor subclass oncall: pt2 module: pt2-dispatcher
"Automatically run ""lintrunner -a"" when needed and create review comments with autofixes triaged",triaged
Rework Dynamic Benchmarks To Actually Vary Shapes high priority triage review triaged oncall: pt2 module: dynamic shapes,high priority triage review triaged oncall: pt2 module: dynamic shapes
MPS device: Sample from MultivariateNormal distribution triaged module: mps,triaged module: mps
[CUDA-12.2] cuSPARSE deprecated support for sparse BSR module: sparse module: cuda triaged,module: sparse module: cuda triaged
[PT2] [Hardening] Track recompiles alongside graph breaks in our actual/expected comparison CI runs triage review good first issue triaged oncall: pt2 module: dynamo,triage review good first issue triaged oncall: pt2 module: dynamo
`torch.div` on empty tensors causes segmentation fault high priority module: crash module: cpu triaged module: regression,high priority module: crash module: cpu triaged module: regression
Training a network SUPER slow with Pytorch 2.1 needs reproduction module: performance module: cuda triaged,needs reproduction module: performance module: cuda triaged
make_fx produces incorrect graph when used under FunctionalTensorMode triaged module: fx oncall: fx,triaged module: fx oncall: fx
"Mismatching behaviour of tensor assignment ""a.data = b"" between torch.compile and eager execution triage review triaged oncall: pt2",triage review triaged oncall: pt2
Upsample bilinear 2d decomposition does not match native implementation for uint8 triaged module: decompositions,triaged module: decompositions
Operator with only `Tensor[][]` args unsupported by dispatcher triaged module: dispatch,triaged module: dispatch
`make_fx` failing for `_scaled_dot_product_flash_attention` decomposition triaged module: fx module: decompositions,triaged module: fx module: decompositions
AOTInductor data dependents error when using max().item() triage review triaged oncall: pt2 module: dynamic shapes oncall: export,triage review triaged oncall: pt2 module: dynamic shapes oncall: export
Optim.step() is significantly SLOW on MPS module: performance triaged module: mps,module: performance triaged module: mps
" Error loading ""AppData\\Local\\Temp\\_MEI136882\\torch\\lib\\cufft64_10.dll"" or one of its dependencies. module: windows triaged module: third_party",module: windows triaged module: third_party
Getting an Error when loading a checkpoint :  AttributeError: Can't get attribute 'base_args_dict' on <module '__main__'> needs reproduction module: windows module: serialization triaged module: python frontend,needs reproduction module: windows module: serialization triaged module: python frontend
Can't build PyTorch 2.1 from source by GCC 13.2 on M1 MacOS module: build module: docs triaged module: macos,module: build module: docs triaged module: macos
Export + assume_constant_result does not work for top-level annotated function triage review good first issue triaged oncall: pt2 oncall: export,triage review good first issue triaged oncall: pt2 oncall: export
Support for Bazel workspace function or Bazel module module: build triaged module: bazel,module: build triaged module: bazel
How to handle CVE vulnerabilities in underlying operating system? triaged module: docker security,triaged module: docker security
libtorch exports miniz symbols module: binaries module: cpp triaged,module: binaries module: cpp triaged
Vendored FindCUDAToolkit.cmake deviates from upstream in splayed installation support module: build triaged,module: build triaged
Vmap: There is a performance drop because we have not yet implemented the batching rule for aten::max_pool3d. triaged module: functorch,triaged module: functorch
CSR matrix on MPS module: sparse triaged module: mps,module: sparse triaged module: mps
[dynamo] nit - add `@torch.autocast` function decoration to the testing path good first issue triaged oncall: pt2 module: dynamo,good first issue triaged oncall: pt2 module: dynamo
High dimensional grid sample module: cuda triaged enhancement,module: cuda triaged enhancement
[MPS]Apple MPS produce different Loss value vs. NVIDIA CUDA after couple of digits triaged module: mps,triaged module: mps
Failed to remove output annotation in Quantizer for PT2 QAT quantization oncall: quantization triaged,oncall: quantization triaged
Multi Scale Deformable Attention Support feature module: nn triaged oncall: transformer/mha module: multi-headed-attention,feature module: nn triaged oncall: transformer/mha module: multi-headed-attention
[export] Make serialized pytree type name required  triaged export-triaged oncall: export,triaged export-triaged oncall: export
Multi-Threaded GraphModule / torch.fx inference raises an exception module: multiprocessing triaged module: fx,module: multiprocessing triaged module: fx
Tracing mode for unbacked SymInts using real data triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
torch._dynamo.export raises Unexpected type in sourceless builder <class 'nemo.core.neural_types.elements.VoidType'> for torchaudio model module: onnx triaged module: dynamo,module: onnx triaged module: dynamo
Wrong with code_coverage/readme.md module: docs triaged,module: docs triaged
log_softmax() on CPU and GPU has expected numerical error when used with low-precision bfloat16 module: numerical-stability module: cuda module: cpu triaged,module: numerical-stability module: cuda module: cpu triaged
when huawei NPU triaged module: backend,triaged module: backend
When will Huawei Shengteng atlas be supported triaged module: backend,triaged module: backend
dispatcher cannot determine dispatch key on tuple input triaged module: dispatch,triaged module: dispatch
FSDP requires global device context oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
[ONNX] Assertion in models is not supported by fx exporter  module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
`grad_fn` is not defined in `__torch_dispatch__` arguments when lazy device is used triaged lazy,triaged lazy
Nesting no_grad in autocast causes backwards graph to be (partially) lost outside of no_grad high priority module: autograd triaged actionable module: amp (automated mixed precision),high priority module: autograd triaged actionable module: amp (automated mixed precision)
USE_SYSTEM_ONNX: undefined references module: onnx triaged,module: onnx triaged
Bugs in `torch.ao.quantization.fuse_modules`  cause operator fusion failure (`Conv2d` and `BatchNorm2d`) oncall: quantization triaged,oncall: quantization triaged
[Breaking change 2.1] Passing non-contiguous inputs to SDPA on CUDA device with the mem-efficient attention backend returns garbage high priority triaged module: regression module: correctness (silent) module: multi-headed-attention,high priority triaged module: regression module: correctness (silent) module: multi-headed-attention
"RuntimeError: ""grid_sampler_2d_cuda"" not implemented for 'BFloat16' module: cuda triaged module: bfloat16",module: cuda triaged module: bfloat16
"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch needs reproduction module: cuda module: error checking triaged",needs reproduction module: cuda module: error checking triaged
[inductor][cpu] detectron2 fasterrcnn accuracy failure high priority triage review triaged oncall: pt2 module: cpu inductor,high priority triage review triaged oncall: pt2 module: cpu inductor
"when convert to onnx ,the jit will merge th outputs, it results to we can't distinguish  what the outputs represents module: onnx triaged",module: onnx triaged
[Tracking] Follow ups for itertools infinite iterators good first issue triaged oncall: pt2 module: dynamo,good first issue triaged oncall: pt2 module: dynamo
[Bug Report]FSDP: An error raises when loading FSDP distributed checkpoint with ignoring modules. oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Add a function to torch.nested to create nested tensors from a buffer and sizes module: serialization triaged module: nestedtensor,module: serialization triaged module: nestedtensor
Compilation Failure of torch.special.exp2 in torch.compile Optimized Mode triaged module: cuda graphs oncall: pt2,triaged module: cuda graphs oncall: pt2
Compilation Failure of torch.cumsum in torch.compile Optimized Mode triaged module: cuda graphs oncall: pt2 module: decompositions,triaged module: cuda graphs oncall: pt2 module: decompositions
GRUCell batching rule for vmap triaged module: functorch,triaged module: functorch
torch.export emits node outside of Core ATen IR triaged export-triaged export-triage-review oncall: export,triaged export-triaged export-triage-review oncall: export
[torch.jit.script] Expected a value of type 'Tensor' for argument 'b' but instead found type 'bool'. oncall: jit,oncall: jit
[Tracker] Move nested tensors to beta triaged module: nestedtensor,triaged module: nestedtensor
"KeyError during model export while using ""newer"" data types module: serialization triaged",module: serialization triaged
Possible use-after-free of Tensor in JIT generated code module: crash module: cpp-extensions triaged,module: crash module: cpp-extensions triaged
"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392020201/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch. triaged module: CUDACachingAllocator",triaged module: CUDACachingAllocator
Contradictory Error Message for stride Argument in torch.conv_transpose3d() module: error checking module: convolution triaged,module: error checking module: convolution triaged
Why is flake8 F821 disabled good first issue triaged better-engineering,good first issue triaged better-engineering
"In the func Tensor.to, how can I make privateuse lazy init module: internals triaged",module: internals triaged
RFC: [pytree] node registration namespaces triaged needs design module: pytree,triaged needs design module: pytree
[pytree] Pytree node registration hygeine: deprecate global _register_pytree_node; only allow enabling registered pytree extensions locally triaged module: pytree,triaged module: pytree
Unexpected behaviour with shared modules in multiprocessing on WSL2 module: multiprocessing triaged module: wsl,module: multiprocessing triaged module: wsl
Requesting to add a section to the Installing C++ Distributions of PyTorch documentation for Apple M1/M2 Processors module: binaries module: docs module: cpp triaged module: macos module: arm,module: binaries module: docs module: cpp triaged module: macos module: arm
Library is included twice QNNPACK module: build triaged oncall: mobile actionable,module: build triaged oncall: mobile actionable
IBM AfroHacks at AfroTech Participation triaged,triaged
PyTorch is shipped with different versions on NCCL high priority module: binaries oncall: releng triaged module: nccl,high priority module: binaries oncall: releng triaged module: nccl
Can't access attribute of wrapper tensor subclass under torch.compile triaged tensor subclass oncall: pt2 module: aotdispatch module: pt2-dispatcher,triaged tensor subclass oncall: pt2 module: aotdispatch module: pt2-dispatcher
Optest is unable to xfail on parametrized tests triaged module: opcheck,triaged module: opcheck
"Using the ""mps"" device on x86 Mac with AMD gpu, torch.argmax returns incorrect results. triaged module: correctness (silent) module: mps",triaged module: correctness (silent) module: mps
Validations for 2.1.1 release triaged,triaged
Implement clip grad value on FSDP feature triaged module: fsdp,feature triaged module: fsdp
DDP backpropagated gradients not the same across all gpus when forward inference not using all declared modules oncall: distributed,oncall: distributed
Export List/Tuple type inputs with dynamic size module: onnx triaged,module: onnx triaged
FSDP load sharded state dict + multi-backend init + bf16 + gloo (?) crashes high priority triage review oncall: distributed module: fsdp,high priority triage review oncall: distributed module: fsdp
pyTorch 2.1 3x slower than 2.0 module: performance module: windows module: cuda triaged,module: performance module: windows module: cuda triaged
torch.export fails on a model with optional parameter triaged export-triage-review oncall: export,triaged export-triage-review oncall: export
"Failed to compile: null in call to `__builtin_memmove(__result, __first, sizeof(_Tp) * _Num);` Debian 12, ppc64le, gcc 12.2 module: build triaged",module: build triaged
The error type `UserError` in dynamo is confusing. triaged export-triage-review oncall: export,triaged export-triage-review oncall: export
"AssertionError: graph-captured input #1, of type <class 'torch.Tensor'>, is not among original inputs of typesâ€¦ high priority triaged export-triage-review oncall: export",high priority triaged export-triage-review oncall: export
torch.inference_mode and tensor subclass: RuntimeError: Cannot set version_counter for inference tensor module: autograd triaged actionable tensor subclass,module: autograd triaged actionable tensor subclass
RuntimeError for hessian vector product with jvp triaged module: functorch,triaged module: functorch
Strided tensor in backward cause uninitialized output module: nn triaged,module: nn triaged
[pytree] `pytree.tree_map` does not respect type of `torch.Size` triaged module: pytree bug,triaged module: pytree bug
torch 2.1 FSDP only some layers might not be working with training only a couple of layers triaged module: fsdp,triaged module: fsdp
Inductor with cpu lowering fails to raise exception on invalid getitem triaged oncall: pt2 module: inductor,triaged oncall: pt2 module: inductor
Operators that return dynamic-shape outputs that require_grad choke in AOTAutograd triaged oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,triaged oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher
[opcheck] Faster gradcheck execution triaged module: opcheck,triaged module: opcheck
[opcheck] Way to reduce Hypothesis sampling when running opcheck triaged module: opcheck,triaged module: opcheck
Resize warning in two argument torch.logical_* with broadcasting triaged module: boolean tensor module: safe resize,triaged module: boolean tensor module: safe resize
[opcheck] Cannot share failures_dict between multiple tests with differing sets of tests they run  triaged module: opcheck,triaged module: opcheck
Inconsistent Keyword Arguments behaviors in torch.triangular_solve() triaged topic: deprecation topic: docs module: python frontend,triaged topic: deprecation topic: docs module: python frontend
Migration from c10::variant to std::variant causes undefined symbols when linking against older pytorch module: build triaged,module: build triaged
Cannot build static windows libraries  module: build module: windows triaged module: static linking topic: binaries,module: build module: windows triaged module: static linking topic: binaries
torchscript file can not be loaded if its saved form the export model produced by torch.export.export oncall: quantization low priority triaged export-triage-review oncall: export,oncall: quantization low priority triaged export-triage-review oncall: export
Multiprocess. DataLoader worker  is killed by signal: Segmentation fault. needs reproduction module: crash module: dataloader triaged,needs reproduction module: crash module: dataloader triaged
"OOM when saving model(lora adapter), seems the clause ""FullyShardedDataParallel(model,...)"" will directly cause the OOM. oncall: distributed module: memory usage triaged module: fsdp",oncall: distributed module: memory usage triaged module: fsdp
ImportError: cannot import name 'external_utils' from partially initialized module 'torch._dynamo' needs reproduction module: binaries triaged module: docker topic: binaries,needs reproduction module: binaries triaged module: docker topic: binaries
Custom FFT implementation returns unexpected results when using torch.compile triaged module: complex module: correctness (silent) oncall: pt2,triaged module: complex module: correctness (silent) oncall: pt2
Jit scripting support for `|` and mixing `typing`. oncall: jit,oncall: jit
GroupNorm & InstanceNorm does not handle channels_last correctly module: nn module: cuda triaged module: memory format actionable,module: nn module: cuda triaged module: memory format actionable
__slots__ + inheriting from torch.Tensor triaged tensor subclass module: python frontend,triaged tensor subclass module: python frontend
[aot_inductor]14k models: RuntimeError: t == DeviceType::CUDA INTERNAL triaged oncall: pt2,triaged oncall: pt2
Revisit security implications of #31875 triaged better-engineering topic: security security,triaged better-engineering topic: security security
Precisely monitor the collective communication tasks oncall: distributed triaged enhancement oncall: profiler,oncall: distributed triaged enhancement oncall: profiler
pytorch support for cuda 12.2 ? module: cuda oncall: releng triaged needs design,module: cuda oncall: releng triaged needs design
torch.compile precision bug when the attr object changes high priority triage review triaged oncall: pt2 module: dynamo,high priority triage review triaged oncall: pt2 module: dynamo
Is it a good time to switch to CXX11_ABI? module: binaries triaged,module: binaries triaged
grad is inf/nan when using torch.amp triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
[dynamo] generic `is_` type shortcut is not appropriately guarded triaged bug oncall: pt2 module: dynamo,triaged bug oncall: pt2 module: dynamo
lintrunner job time keeps growing triaged better-engineering module: devx,triaged better-engineering module: devx
Add more flexibility on print / output console module: cpp triaged enhancement,module: cpp triaged enhancement
[export] self.buffer += 1 raises error triaged export-triage-review oncall: export,triaged export-triage-review oncall: export
Dynamo Compile samples should record file/line that raised exception triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
Buffer overflow not prevented on MPS devices module: error checking triaged module: advanced indexing module: mps,module: error checking triaged module: advanced indexing module: mps
"Static Linking C++, Op not available at runtime module: build module: cpp triaged enhancement module: vision has workaround",module: build module: cpp triaged enhancement module: vision has workaround
torch2.1.0 DDP+compile+dynamic_shape cause error triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
Batched matmul gives incorrect result on MPS devices high priority triaged module: correctness (silent) module: mps,high priority triaged module: correctness (silent) module: mps
Missing `ignored_param` when calling wrapper_cls (FSDP) recursively oncall: distributed triaged,oncall: distributed triaged
maximum Python version supported is not indicated module: docs triaged,module: docs triaged
Dynamic shapes doesn't work for torch.diff / resize__symint in some cases triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
Prolonged network hiccup preventing retrieval of workflow job id triaged module: devx,triaged module: devx
Tensor `.cuda()` very slow with specific array sizes  module: performance module: cuda triaged,module: performance module: cuda triaged
build: failure when building pytorch with TBB module: build triaged module: tbb,module: build triaged module: tbb
misusing percision value in test_cuda function in torch/testing/_internal/common_nn.py. triaged module: testing,triaged module: testing
"Higher-order derivatives extremely slow, increasing exponentially module: autograd triaged needs research",module: autograd triaged needs research
Building docs fails triaged oncall: export,triaged oncall: export
[RFC] Add GradScaler on CPU triaged module: half,triaged module: half
[Bug]: some parameters' grad is None when using FSDP with torch2.1.0 oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Bug with as_strided_tensorimpl for MPS devices triaged module: mps,triaged module: mps
Functorch FCD breaks with tensor subclasses triaged module: functorch module: first class dims,triaged module: functorch module: first class dims
MPS Performance regressions on Sonoma 14.0  high priority triaged module: mps,high priority triaged module: mps
Sparse Tensor Sum Still Does Not Work for PyTorch Geometric module: sparse triaged,module: sparse triaged
LBFGS accuracy difference between CPU and GPU needs reproduction module: optimizer triaged,needs reproduction module: optimizer triaged
"When keep_inference_input_mutations=True is set, one dynamic shape test fails triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher",triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher
torch.jit.script persistently changes default from utf-8 to ascii oncall: jit,oncall: jit
multi_head_attention_forward generates different values on MPS compared to CPU triaged module: mps,triaged module: mps
"Rephrase sentence in ""Why and when to use sparsity"" for better understanding. module: docs triaged",module: docs triaged
test_learnable_forward_per_channel fails due to integer overflow oncall: quantization triaged,oncall: quantization triaged
yolov5_train feature module: cuda triaged module: determinism,feature module: cuda triaged module: determinism
torch.autocast() hangs on CPUs module: cpu triaged module: amp (automated mixed precision),module: cpu triaged module: amp (automated mixed precision)
[FX Quant] operator.matmul (@ operator ) is not converted to torch.ops.quantized.matmul oncall: quantization triaged,oncall: quantization triaged
torch.compile of simple loop takes 34 seconds triaged oncall: pt2 module: startup-tracing-compile time module: higher order operators module: pt2-dispatcher,triaged oncall: pt2 module: startup-tracing-compile time module: higher order operators module: pt2-dispatcher
nonnull error needs reproduction module: build triaged,needs reproduction module: build triaged
AOTAutograd generates wrong strides for view+inplace op triaged ezyang's list bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,triaged ezyang's list bug oncall: pt2 module: aotdispatch module: pt2-dispatcher
The results of masked.log_softmax on MPS are inconsistent with those on CPU triaged oncall: transformer/mha module: correctness (silent) module: mps,triaged oncall: transformer/mha module: correctness (silent) module: mps
"Use of -Wl,--as-needed in cmake config files can leak into third-party users' code and modify their own private libraries module: build triaged topic: build",module: build triaged topic: build
Torch Compile Dynamic fails on sample on diffusers VAE triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
[quant][pt2] Default batchnorm aten op has poor numerics during QAT high priority oncall: quantization triaged,high priority oncall: quantization triaged
Tensor.lerp inconsistent when using -Infinity between MPS and CPU triaged module: mps,triaged module: mps
RuntimeError: CUDAPluggableAllocator does not yet support cacheInfo feature triaged module: CUDACachingAllocator,feature triaged module: CUDACachingAllocator
Failed to import transformer. high priority needs reproduction module: binaries module: windows triaged,high priority needs reproduction module: binaries module: windows triaged
Simulating lower memory on GPU does not indicate simulated memory in error message module: cuda module: memory usage triaged module: CUDACachingAllocator,module: cuda module: memory usage triaged module: CUDACachingAllocator
I have a trouble with to_symmetric needs reproduction module: sparse triaged,needs reproduction module: sparse triaged
Couldn't export yolov7 quantized model to onnx module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
CUDA version 12.2 has differential accuracy when executing CPU and GPU module: cuda triaged,module: cuda triaged
Torch 2.1 compile + FSDP (mixed precision) + LlamaForCausalLM: `RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'.` triage review oncall: distributed triaged module: fsdp oncall: pt2 module: distributed,triage review oncall: distributed triaged module: fsdp oncall: pt2 module: distributed
pytorch index_select is too slow module: performance module: cuda triaged,module: performance module: cuda triaged
"RuntimeError: Expected is_sm80 || is_sm90 to be true, but got false. (Using Google Colab) triaged",triaged
`torch.utils.checkpoint` drops custom Tensor attributes module: checkpoint triaged,module: checkpoint triaged
Trace dynamic batch size with make_fx triaged actionable module: fx oncall: pt2 oncall: fx module: functorch module: dynamic shapes module: pt2-dispatcher,triaged actionable module: fx oncall: pt2 oncall: fx module: functorch module: dynamic shapes module: pt2-dispatcher
[inductor][dynamic] fused_attention pattern could not be matched due to sym_size triaged oncall: transformer/mha module: dynamic shapes module: inductor module: multi-headed-attention inductor_pattern_match,triaged oncall: transformer/mha module: dynamic shapes module: inductor module: multi-headed-attention inductor_pattern_match
"torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1331, unhandled cuda error (run with NCCL_DEBUG=INFO for details) oncall: distributed triaged module: wsl",oncall: distributed triaged module: wsl
unique(return_counts=True) fails on MPS for unsorted tensors with 1M+ elements triaged module: mps,triaged module: mps
Result of adding noise is very different in mps vs cuda or cpu triaged module: mps,triaged module: mps
Regression on CUDA 12.1 for vanilla transformer layer needs reproduction module: performance module: cuda triaged,needs reproduction module: performance module: cuda triaged
RuntimeError in run_streaming_llama.py When Using Accelerate with Streaming LLMa Model on A4500 GPU needs reproduction module: error checking triaged,needs reproduction module: error checking triaged
overloads can perhaps be more performant? module: optimizer triaged module: dispatch module: codegen,module: optimizer triaged module: dispatch module: codegen
"No op for aten::where with argument types: Tensor, Tensor, bool. oncall: jit",oncall: jit
Mismatch results of index_add_ between torch.compile Inductor backend and eager mode triaged ezyang's list module: functionalization bug oncall: pt2 module: pt2-dispatcher,triaged ezyang's list module: functionalization bug oncall: pt2 module: pt2-dispatcher
type promotion test for torch.div variants is broken triaged module: testing,triaged module: testing
torch::serialize::OutputArchive::save_to crash if save on C:\\ module: windows module: serialization triaged,module: windows module: serialization triaged
Build failure with Xcode 15 linker module: build triaged module: macos,module: build triaged module: macos
There is a performance drop because we have not yet implemented the batching rule for aten::mkldnn_rnn_layer_backward. triaged module: vmap module: functorch,triaged module: vmap module: functorch
There is a performance drop because we have not yet implemented the batching rule for aten::mkldnn_rnn_layer_backward. triaged module: functorch,triaged module: functorch
AOTAutograd perf: avoid as_strided() calls when we have intermediate bases module: performance triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,module: performance triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher
dump_operator_names.cc uses std::cout but dose not include iostream triaged better-engineering actionable,triaged better-engineering actionable
Export swallows exception good first issue triaged oncall: pt2 export-triaged oncall: export,good first issue triaged oncall: pt2 export-triaged oncall: export
"[JIT] Error when scripting wrapper of `matrix_norm` using `p: Union[str, int]`  oncall: jit",oncall: jit
[PT2.1] SIGSEGV seen with view + sgn operator inside torch.compile triage review triaged ZeroTensor oncall: pt2,triage review triaged ZeroTensor oncall: pt2
Unprompted UserWarning module: optimizer triaged,module: optimizer triaged
ncu python conv2d.py runs indefinitely after activating cudnn.benchmark module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
[Dynamo] Error in speculate_subgraph doesn't report inner user stack trace triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
`pip install deepspeed` fails if number of GPUs greater than a certain small number? needs reproduction module: binaries triaged module: third_party,needs reproduction module: binaries triaged module: third_party
`torch.is_autocast_enabled()` always False on CPU triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
`model.named_buffers()` fails if module not hashable. module: nn triaged,module: nn triaged
kBackendDefaultTimeout is causing a timeout exception when rank 0 process exceeds 30 minutes preparing a dataset. oncall: distributed,oncall: distributed
Enable more flake8-pyi ruff checks good first issue module: lint triaged actionable,good first issue module: lint triaged actionable
RuntimeError: out_ptr == out_accessor[thread_count_nonzero[tid + 1]].data() INTERNAL ASSERT FAILED needs reproduction triaged module: advanced indexing,needs reproduction triaged module: advanced indexing
Issue with torch.distributed.launch oncall: distributed oncall: r2p,oncall: distributed oncall: r2p
"RuntimeError: !needs_dynamic_casting<func_t>::check(iter) INTERNAL ASSERT FAILED at ""../aten/src/ATen/native/cpu/Loops.h"":349, ... please report a bug to PyTorch. module: optimizer triaged",module: optimizer triaged
[Inductor] `ConstantFolder` Utility Breaking in Recent Nightly triaged module: inductor,triaged module: inductor
Depthwise conv3d slower than normal conv3d module: performance module: nn triaged needs research,module: performance module: nn triaged needs research
Gradients (Jacobian) in inference module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
BCEWithLogitsLoss: Check if labels / targets are within zero and one module: nn triaged actionable,module: nn triaged actionable
Broadcasting matmul is much slower than corresponding einsum module: performance module: cpu triaged,module: performance module: cpu triaged
Matmul failure after dtype change on mixed AMD setup module: rocm triaged,module: rocm triaged
"AOTAutograd: set_ under no_grad still triggers ""a view of a leaf Variable that requires grad is being used in an in-place operation"" triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher",triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher
AOTAutograd: set_ on input that ultimately no-ops fails in runtime_wrapper copy_ triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher
[Inductor] [cpu][amp]  Eager model failed to run for some torchbench models triaged module: cpu inductor,triaged module: cpu inductor
Segmentation fault on aarch64 (Rpi4) using Pytorch 2.1.0 & torchaudio  module: binaries triaged,module: binaries triaged
The derivation of swish activation function is wrong. caffe2 triaged,caffe2 triaged
M2 Failing to build example-app in c++ needs reproduction module: binaries module: build module: cpp triaged module: macos,needs reproduction module: binaries module: build module: cpp triaged module: macos
Toggling model.train() causes guard failures every time triage review triaged bug oncall: pt2 module: dynamo,triage review triaged bug oncall: pt2 module: dynamo
Build process failure with torch_shm_manager module: build triaged,module: build triaged
.lldbinit formatters only work when building with clang triaged release notes: devx topic: devs,triaged release notes: devx topic: devs
backward and grad behave inconsistently w.r.t. set_ on leaf variable module: autograd triaged needs research has workaround,module: autograd triaged needs research has workaround
"Repro for non-deterministic ""operation not permitted when stream is capturing"" crash oncall: distributed",oncall: distributed
torch.distributed.pipeline skip module throws assert error that portal.grad is not None oncall: distributed,oncall: distributed
Using ChainedScheduler with ReduceLROnPlateau leads to unexpected keyword argument error module: optimizer triaged needs design module: LrScheduler,module: optimizer triaged needs design module: LrScheduler
Support using SymBool in arithmetics good first issue triaged bug oncall: pt2 module: dynamic shapes,good first issue triaged bug oncall: pt2 module: dynamic shapes
pytorch consuming all cpu cores 100% on ARM module: binaries triaged module: openmp module: arm,module: binaries triaged module: openmp module: arm
Error with monai SwinUNETR and checkpointing needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
More informative variable names in AOTAutograd triaged module: fx module: aotdispatch module: pt2-dispatcher,triaged module: fx module: aotdispatch module: pt2-dispatcher
max_pool3d_with_indices_backward_cuda and avg_pool3d_backward_cuda does not have a deterministic implementation feature module: nn triaged module: determinism,feature module: nn triaged module: determinism
"onnx export jit.script ShapeInferenceError Unexpected axis value: 1. Expected range [-1, 1) module: onnx triaged",module: onnx triaged
[RFC] Scaled Dot Product Attention  API Changes feature module: nn triaged module: python frontend module: multi-headed-attention,feature module: nn triaged module: python frontend module: multi-headed-attention
Backward pass for Nested Tensors using flash attention in sdpa fails triaged module: nestedtensor oncall: transformer/mha module: multi-headed-attention,triaged module: nestedtensor oncall: transformer/mha module: multi-headed-attention
opinfo split is confusing triaged module: testing,triaged module: testing
`pytest test/dynamo -v ` fails locally high priority triaged oncall: pt2,high priority triaged oncall: pt2
[discussion] Have PyTorch functions support python scalars (like NumPy) + introduce convenience constants like `torch.pi` and `torch.e` and maybe analogue of `scipy.constants` namespace triaged module: numpy module: python frontend,triaged module: numpy module: python frontend
Memory efficient attention for tensors where the last dimension is not divisible by 8 triaged oncall: transformer/mha module: multi-headed-attention,triaged oncall: transformer/mha module: multi-headed-attention
torch.compile CPU backend is slower than eager for several transcendental functions triaged module: inductor module: cpu inductor,triaged module: inductor module: cpu inductor
ValueError issued instead of TypeError when tensor is cast to a scalar module: error checking triaged module: numpy,module: error checking triaged module: numpy
AOTAutograd logging: log autograd graphs module: logging triaged module: aotdispatch module: pt2-dispatcher,module: logging triaged module: aotdispatch module: pt2-dispatcher
[torch.compile] Multiple set operations don't work good first issue triaged oncall: pt2 module: dynamo release notes: dynamo,good first issue triaged oncall: pt2 module: dynamo release notes: dynamo
Incorrect docstring / documentation for torch.nn.functional.scaled_dot_product_attention in 2.1 module: docs triaged module: multi-headed-attention,module: docs triaged module: multi-headed-attention
Multiprocessing takes forever after on .get()  with mp.Queue() (Possible Deadlock) needs reproduction module: multiprocessing triaged,needs reproduction module: multiprocessing triaged
libtorch.so: error adding symbols: file in wrong format needs reproduction module: build triaged,needs reproduction module: build triaged
Clean way to distinguish python subclass NT vs. C++ NT triaged module: nestedtensor,triaged module: nestedtensor
On the correctness of torch.signal.windows.cosine triaged module: numpy topic: bc breaking topic: docs,triaged module: numpy topic: bc breaking topic: docs
Torch Nested Issue With Backward Pass In Transpose triaged has workaround module: nestedtensor,triaged has workaround module: nestedtensor
DynamicQuantizedLinear shows incorrect qscheme after applying eager mode dynamic quantization oncall: quantization triaged,oncall: quantization triaged
doc modification of torch.nn.softshrink api module: docs module: nn triaged,module: docs module: nn triaged
[FSDP] [Checkpointing] Loading optimizer state dict with use_orig_params True causes OOM triaged module: fsdp,triaged module: fsdp
Custom tensor attributes not preserved with registered functions triaged module: custom-operators module: library,triaged module: custom-operators module: library
Local build breakage on AWS cluster module: build triaged,module: build triaged
`test_pytorch_onnx_onnxruntime_cuda.py` is not run in CI module: onnx module: cuda module: tests triaged,module: onnx module: cuda module: tests triaged
Using `torch.onnx.export` from file named `onnx.py` results in cryptic error message module: onnx triaged,module: onnx triaged
Torch.onnx.export of module used positional and keyword arguments module: onnx triaged,module: onnx triaged
Pytorch for Python 3.12 not available high priority module: build triaged module: python frontend,high priority module: build triaged module: python frontend
jacrev Issue when Using Cuda triaged module: functorch,triaged module: functorch
Different results for forward pass of two equal tensors through Conv2d triaged module: numerical-reproducibility module: memory format,triaged module: numerical-reproducibility module: memory format
Pytorch LoadNativeLibrary issue oncall: mobile,oncall: mobile
Categorical Simplex constraint throws error for valid values module: distributions triaged,module: distributions triaged
"nn.BatchNorm2d (track_running_stats = True) causes ""modified by an in-place operation"" error when in torch.nn.parallel.DistributedDataParallel oncall: distributed triaged",oncall: distributed triaged
"Dropout signature inconsistent between `torch.dropout`, `torch.nn.Dropout` and `torch.nn.functional.dropout` module: nn triaged module: python frontend",module: nn triaged module: python frontend
Perf-Drop (factor=2) Ubuntu-vs-Windows on same PC (dual-boot) module: windows module: cuda triaged,module: windows module: cuda triaged
"torch.Tensor.__repr__ causes torch.compile to error: ""got an unexpected keyword argument 'tensor_contents'"" triaged oncall: pt2",triaged oncall: pt2
"torch._dynamo.exc.Unsupported: unexpected sourceless type bases: (<class 'torchrec.streamable.Pipelineable'>,) good first issue triaged ezyang's list oncall: pt2 module: dynamo oncall: export",good first issue triaged ezyang's list oncall: pt2 module: dynamo oncall: export
Tests modify global state cause later tests to fail module: ci triaged module: devx,module: ci triaged module: devx
TypeError: Got unsupported ScalarType BFloat16 triaged module: numpy,triaged module: numpy
vmap: Transform single-element tensor to integer triaged module: functorch,triaged module: functorch
Cannot avoid kineto_LIBRARY-NOTFOUND error when using pre-built pytorch module: binaries oncall: profiler,module: binaries oncall: profiler
cuda/tf32 docs are outdated module: docs module: cuda triaged module: tf32,module: docs module: cuda triaged module: tf32
Accessing Particular Nightly Builds Don't Work module: binaries triaged,module: binaries triaged
`torch.func.functional_call` does not work with `__torch_function__ ` Tensor-like objects triaged enhancement module: __torch_function__ module: functorch,triaged enhancement module: __torch_function__ module: functorch
PyTorch with non-shared build (building a single shared lib) is unsupported module: build triaged module: static linking,module: build triaged module: static linking
RuntimeError: Expected packed scalar Tensor to be of dimension 1. Got 0 instead. module: optimizer triaged module: regression actionable,module: optimizer triaged module: regression actionable
cudaMallocAsync cause too much fragmentation. module: cuda module: memory usage triaged module: CUDACachingAllocator,module: cuda module: memory usage triaged module: CUDACachingAllocator
Add _worker_end_fn_t to the DataLoader module: dataloader triaged module: data,module: dataloader triaged module: data
"ValueError: args contained 2 None's after flattening. When exporting a ScriptModule or ScriptFunction, no args may be None because that breaks type propagation. module: onnx triaged module: multi-headed-attention",module: onnx triaged module: multi-headed-attention
Unbacked SymInts get reallocated whenever you repropagate fake tensors triaged oncall: pt2 module: fakeTensor module: dynamic shapes module: pt2-dispatcher,triaged oncall: pt2 module: fakeTensor module: dynamic shapes module: pt2-dispatcher
logging stack_info doesn't do anything module: logging triaged oncall: pt2 module: ProxyTensor module: pt2-dispatcher,module: logging triaged oncall: pt2 module: ProxyTensor module: pt2-dispatcher
Create a new heuristic TD rule for failures coming from base commit of the pull requests triaged module: devx,triaged module: devx
Skip cuda kernel launch with torch.sum when dimension length is 0 module: performance module: cuda triaged,module: performance module: cuda triaged
Dynamo tests in CI seem to not run at times high priority triaged oncall: pt2,high priority triaged oncall: pt2
scatter_add: Mixing 0-dim and 1-dim tensors module: docs triaged module: python frontend module: edge cases,module: docs triaged module: python frontend module: edge cases
ImportError: libc10_cuda.so: cannot open shared object file: No such file or directory module: binaries module: build triaged,module: binaries module: build triaged
[BUG] Elastic cannot kill all subprocesses after sending sigterm. oncall: distributed module: multiprocessing triaged module: elastic,oncall: distributed module: multiprocessing triaged module: elastic
torch.onnx.export causes floating point exception with core dump for empty slice assignment module: onnx triaged,module: onnx triaged
Race condition on shutdown involving PThreadPool and autograd triaged module: multithreading module: sanitizers,triaged module: multithreading module: sanitizers
Dataloader resetting with num_workers=1 and persistent_workers=True module: dataloader triaged,module: dataloader triaged
tan/tanh discrepancies with complex due to jiterator module: cuda triaged module: jiterator,module: cuda triaged module: jiterator
Please offer packages with local version `torch==2.1.0+cpu` for macOS module: build oncall: releng triaged,module: build oncall: releng triaged
RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED module: cuda triaged,module: cuda triaged
Implmenet kthvalue for bfloat16 on CUDA module: cuda triaged module: bfloat16,module: cuda triaged module: bfloat16
Static quantization for Transformer block : AttributeError 'function' object has no attribute 'is_cuda' triaged oncall: transformer/mha,triaged oncall: transformer/mha
torch-<version>.dist-info WHEEL file contains incorrect metadata for M1/M2 macOS platform module: binaries oncall: releng triaged module: m1,module: binaries oncall: releng triaged module: m1
Dtype hard-coded in DataLoader (for python floats). triaged module: data,triaged module: data
WelfordReduction seems to have invalid/dead code when reduction_numel <= 1 triaged module: inductor,triaged module: inductor
Simple script segfaulting when grad is enabled module: autograd triaged needs design module: edge cases,module: autograd triaged needs design module: edge cases
Indexed batch matrix multiplication to support MoEs and FFFs module: sparse triaged,module: sparse triaged
Problems when loading PT files und Linux - Duda which are created under Mac Apple Silicon MPS triaged module: mps,triaged module: mps
pytorch XLA document error module: docs triaged module: xla,module: docs triaged module: xla
Batching for is_in triaged module: functorch,triaged module: functorch
test test_2d_fsdp_integration_fsdp_nested_param_groups failed oncall: distributed triaged,oncall: distributed triaged
Memory access fault with AMD Rocm needs reproduction module: rocm triaged,needs reproduction module: rocm triaged
Import order issue with torch and pybind11 Library Statically Linked to libstdc++ module: abi triaged module: static linking,module: abi triaged module: static linking
basic_gnn_gcn: ERROR:common:TypeError: object of type 'GreaterThan' has no len() triaged ezyang's list oncall: pt2 module: dynamic shapes module: inductor,triaged ezyang's list oncall: pt2 module: dynamic shapes module: inductor
[FSDP ]How to convert sharded_state_dict files into full_state_dict offline without distributed process oncall: distributed triaged module: fsdp module: distributed_checkpoint,oncall: distributed triaged module: fsdp module: distributed_checkpoint
Allow try except check for numpy bfloat16 representation triaged module: numpy,triaged module: numpy
Wrongly returns nan for vectorized complex numbers division on PPC/ZArch triaged module: POWER,triaged module: POWER
[BUG?] Why Allocator use stream to manage Block? module: cuda triaged,module: cuda triaged
Cannot use constrain_as_size from fake tensor implementations: RuntimeError: tried to get Int out of SymInt triaged oncall: pt2 module: dynamic shapes oncall: export,triaged oncall: pt2 module: dynamic shapes oncall: export
Severe performance regression on deterministic algorithm in torch 2.0 module: performance triaged module: cublas module: determinism,module: performance triaged module: cublas module: determinism
torch._export has no logging module: logging triaged oncall: pt2 export-triaged oncall: export,module: logging triaged oncall: pt2 export-triaged oncall: export
Implement Copy-on-write (COW) tensors module: internals triaged module: viewing and reshaping,module: internals triaged module: viewing and reshaping
ValueError: only one element tensors can be converted to Python scalars needs reproduction triaged module: regression,needs reproduction triaged module: regression
Incompatible dimensions error for FusedMatMul module: onnx triaged,module: onnx triaged
Bits types cannot be used under deterministic mode triaged module: determinism,triaged module: determinism
Heap-buffer-overflow during tensor unpickling module: serialization triaged,module: serialization triaged
"`torch.embedding`, `weight[indices]`, `torch.index_select` returns random data with indices on meta device module: bc-breaking triaged module: embedding ezyang's list module: meta tensors topic: bc breaking",module: bc-breaking triaged module: embedding ezyang's list module: meta tensors topic: bc breaking
Wrong vector shift results on PowerPC triaged module: vectorization module: POWER,triaged module: vectorization module: POWER
Slow performance when running torch.jit traced model with Flash Attention using libtorch on Windows module: windows triaged oncall: transformer/mha,module: windows triaged oncall: transformer/mha
fp16 parity issue with traced code on GPU oncall: jit triaged module: half oncall: export,oncall: jit triaged module: half oncall: export
[RFC][TorchElastic] topology info in training apps/ranks oncall: distributed triaged module: elastic,oncall: distributed triaged module: elastic
assert_is_valid_input_type is too weak triaged module: dispatch,triaged module: dispatch
Make torch.cuda.graphs.is_current_stream_capturing() available in TorchScript oncall: jit module: cuda graphs,oncall: jit module: cuda graphs
Make standard container classes satisfy container Protocols. module: nn triaged needs research,module: nn triaged needs research
Extends the functionality of  `nn.BatchNorm1d`. oncall: distributed module: nn triaged needs research,oncall: distributed module: nn triaged needs research
[RFC]: Moving most torch.compile backends out of core triaged topic: bc breaking dependency issue oncall: pt2 module: dynamo,triaged topic: bc breaking dependency issue oncall: pt2 module: dynamo
[FSDP] UnpicklingError when calling save_state_dict in distributed run oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
InstanceNorm does not catch dim mismatch module: nn triaged actionable,module: nn triaged actionable
AsyncCompile loses useful exception backtrace in __get_result good first issue module: logging triaged oncall: pt2 module: inductor,good first issue module: logging triaged oncall: pt2 module: inductor
test_memory_timeline fails on PPC due to extra temopraries triaged module: POWER oncall: profiler,triaged module: POWER oncall: profiler
Max pool with negative integer inputs and channels_last memory layout gives the wrong values module: nn triaged,module: nn triaged
Make Dropout take a dim=... argument module: nn triaged needs research has workaround,module: nn triaged needs research has workaround
ONNX Export error module: onnx triaged,module: onnx triaged
Torch FX SubgraphMatcher Any / Oneof Patterns triaged module: fx module: fx.passes,triaged module: fx module: fx.passes
attn_output_weights sometimes rerurn `None` triaged module: multi-headed-attention,triaged module: multi-headed-attention
_assert_bound_is_rational can fail triaged module: dynamic shapes,triaged module: dynamic shapes
[dynamo] torch._dynamo.exc.Unsupported: comparison SymNodeVariable() <built-in function is_> ListVariable() good first issue triaged oncall: pt2 module: dynamo,good first issue triaged oncall: pt2 module: dynamo
Very big differences in output of `torch.lobpcg` (values and run-time) compared to SciPy on a very ill-conditioned Laplacian matrix triaged module: linear algebra,triaged module: linear algebra
Performance degradation on AMD + A800 when computation is small module: performance module: cuda triaged,module: performance module: cuda triaged
Inconsistent behavior for in-place operations on coalesced sparse tensors module: sparse triaged,module: sparse triaged
[BUG][pytree] treespec serialization for locally defined classes and namedtuple types triaged module: pytree,triaged module: pytree
Training results from using MPS backend are poor compared to CPU and CUDA needs reproduction triaged module: mps,needs reproduction triaged module: mps
Inconsistent Behavior of `ConvTranspose2d` on CPU and CUDA needs reproduction module: nn triaged,needs reproduction module: nn triaged
torch pollutes libgomp symbols when import _C module: binaries triaged module: openmp module: third_party,module: binaries triaged module: openmp module: third_party
Memory usage steadily increasing when using back propagation with sparse CSR parameter matrices on CPU module: sparse module: memory usage triaged,module: sparse module: memory usage triaged
RNN Documentation is Confusing / Wrong module: docs module: nn module: rnn triaged actionable,module: docs module: nn module: rnn triaged actionable
Cannot export a quantized model that permutes a quantized tensor to ONNX module: onnx oncall: quantization low priority triaged,module: onnx oncall: quantization low priority triaged
Interleaved isend and irecv causes hang oncall: distributed triaged,oncall: distributed triaged
[FSDP] Implement additional check for turn on 2D TP + FSDP extension triaged module: fsdp,triaged module: fsdp
Make Fx Generating Incorrect Graph For GPTQ model triaged module: fx module: ProxyTensor,triaged module: fx module: ProxyTensor
FSDP crashes when submodule calls method that isn't `forward()` triaged module: fsdp,triaged module: fsdp
cuda rng state for 2.0.1 cannot be used for 2.1.0 module: cuda triaged module: random,module: cuda triaged module: random
Attribute 'kernel_shape' is expected to have field 'ints' when exporting a module with `List[Tensor]` inputs/outputs module: onnx triaged,module: onnx triaged
Add a unittest for ModuleWrapPolicy callable oncall: distributed good first issue triaged pt_distributed_rampup,oncall: distributed good first issue triaged pt_distributed_rampup
[dynamo] Disable DDPOptimizer or error out if DDPOptimizer + static_graph is detected triaged module: ddp oncall: pt2 module: distributed,triaged module: ddp oncall: pt2 module: distributed
[FSDP] Simplify `_fully_sharded_module_to_handle` triaged module: fsdp,triaged module: fsdp
Pytorch ROCM windows builds module: windows module: rocm triaged,module: windows module: rocm triaged
"F.conv2d(input, weight, bias, self.stride, RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR needs reproduction module: cudnn module: nn module: cuda triaged",needs reproduction module: cudnn module: nn module: cuda triaged
Gradients across different ranks are not synchronized when using DDP oncall: distributed triaged,oncall: distributed triaged
FSDP vs. MiCS oncall: distributed module: docs triaged module: fsdp,oncall: distributed module: docs triaged module: fsdp
SparseSemiStructuredTensors are constructed differently from the original dense ones module: sparse triaged,module: sparse triaged
NAN appears in the backward results of masked.cumprod on macos needs reproduction module: autograd triaged module: NaNs and Infs module: half module: mps,needs reproduction module: autograd triaged module: NaNs and Infs module: half module: mps
FSDP should have tests for partial state_dict and optim state_dict loading triaged module: fsdp module: distributed_checkpoint,triaged module: fsdp module: distributed_checkpoint
"The API ""torch::jit::_load_for_mobile"" is limited to create an object living on the stack. oncall: jit",oncall: jit
Unable to install the latest version of PyTorch using mamba. module: binaries triaged,module: binaries triaged
Cannot construct `torch.sparse_coo_tensor` (but `scipy.sparse.coo_matrix` works fine): `TypeError: only integer tensors of a single element can be converted to an index` module: sparse triaged module: scipy compatibility,module: sparse triaged module: scipy compatibility
FSDP do not support `ignored_parameters` when `auto_wrap_policy` is specified oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Can't initializa NVML triaged,triaged
Parameters of cuda module zero out when used in multiprocessing module: docs module: multiprocessing module: cuda triaged,module: docs module: multiprocessing module: cuda triaged
torch.compile/triton holding GIL during compilation and CompiledKernel call results in deadlocks during distributed training high priority triaged oncall: pt2 module: inductor upstream triton,high priority triaged oncall: pt2 module: inductor upstream triton
torch.argmax fails for device='mps:0' triaged module: mps,triaged module: mps
Allow reductions to write into pinned memory module: cuda module: memory usage triaged needs research module: reductions,module: cuda module: memory usage triaged needs research module: reductions
torch.sparse_coo_tensor argname quirks + [feature request] `.numpy()`/`from_numpy` method for sparse_coo_tensor/sparse_csr_tensor (or maybe name them as `.scipy()`/`.from_scipy()` or at least under some `torch.utils.*` namespace module: sparse triaged module: scipy compatibility,module: sparse triaged module: scipy compatibility
pr build failures in inductor dynamic shape test for operation tests with simple tensors. Side effect of current test framework triaged oncall: pt2 module: dynamic shapes module: cpu inductor,triaged oncall: pt2 module: dynamic shapes module: cpu inductor
Cannot install torchmetrics - ERROR 403 high priority module: binaries triaged,high priority module: binaries triaged
The following will always fail on NixOS module: cudnn triaged topic: build,module: cudnn triaged topic: build
TypeError: mask must have dtype bool triaged module: masked operators,triaged module: masked operators
[FSDP] How can I wrap a model that has both nn.Parameter and nn.Module oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
"Incorrect strides and accuracy when combining `torch.compile` with `op(out=out)` having complex number outputs, `test_ops::test_out` is bugged triaged module: complex bug oncall: pt2",triaged module: complex bug oncall: pt2
PPC64le: GCC 11.2.1 Linker Error in bin/torch_shm_manager triaged topic: build bug,triaged topic: build bug
ZeroTensor (and probably neg/conj) doesn't play well with wrapper tensor subclasses triaged module: __torch_dispatch__ ZeroTensor,triaged module: __torch_dispatch__ ZeroTensor
[feature request] Provide some sparse eigen solver(s) for PyTorch (maybe via `ARPACK` as in scipy) + SPD sparse / laplace linear system solver - maybe NVidia AMGx library? module: sparse triaged module: linear algebra,module: sparse triaged module: linear algebra
About FSDP oncall: distributed triaged,oncall: distributed triaged
Really slow compilation times for torch.compile causing distributed training errors triaged module: ddp oncall: pt2 module: startup-tracing-compile time module: distributed,triaged module: ddp oncall: pt2 module: startup-tracing-compile time module: distributed
Unnecessary cuda synchronizations that we should remove in PyTorch module: performance module: cuda triaged,module: performance module: cuda triaged
torch.compile graph breaks should be independent of DDP buckets triaged module: ddp oncall: pt2 module: distributed,triaged module: ddp oncall: pt2 module: distributed
PPC64le: vsx_helpers.h errors module: build triaged,module: build triaged
Support ONNX export for aten::select_backward and aten::slice_backward module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
[c10d] fix functional collective reduce op naming convention oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
RuntimeError: Unrecognized tensor type ID: ZeroTensor triaged ZeroTensor,triaged ZeroTensor
Inconsistent any( ) between cuda and cpu - Incorrect complex to bool conversion triaged module: complex module: correctness (silent),triaged module: complex module: correctness (silent)
[optimize_ddp] moco - NameError: name 's2' is not defined  oncall: distributed triaged module: ddp oncall: pt2 module: dynamic shapes module: distributed,oncall: distributed triaged module: ddp oncall: pt2 module: dynamic shapes module: distributed
"Inconsistent, platform-dependent torch.ones_like behavior on metatensors triaged module: meta tensors",triaged module: meta tensors
"A100 runners down: apt-get install nvidia-docker2, Could not get lock /var/lib/dpkg/lock-frontend triaged module: infra",triaged module: infra
RuntimeError: DataLoader worker (pid 11011) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit module: dataloader triaged module: data,module: dataloader triaged module: data
Certain torch functions are not handled by torch func wrapper triaged module: nestedtensor,triaged module: nestedtensor
Ubuntu vs Windows: torch.cuda.OutOfMemoryError only happens on Ubuntu module: memory usage triaged,module: memory usage triaged
about nccl not work module: build triaged module: nccl,module: build triaged module: nccl
Tensor Parallel doesn't work with torch.compile oncall: distributed triaged oncall: pt2 module: dtensor module: distributed,oncall: distributed triaged oncall: pt2 module: dtensor module: distributed
Export torchvision detection model retinanet_resnet50_fpn triaged oncall: pt2 module: dynamic shapes oncall: export,triaged oncall: pt2 module: dynamic shapes oncall: export
Dynamo Swallowing Exception In Lambda triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
[C++ Frontend] Simple Changes for Cleaner Options module: cpp triaged,module: cpp triaged
Tracing interpolate with tensor scale_factor is cursed triaged oncall: pt2 module: dynamic shapes oncall: export,triaged oncall: pt2 module: dynamic shapes oncall: export
RuntimeError: tried to get Double out of SymFloat triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
Sourceforge outage causing multiple CI failures module: ci triaged,module: ci triaged
SDPA with nested backend: expose a way to avoid recomputing data layout information triaged oncall: transformer/mha,triaged oncall: transformer/mha
switch more test cases to use MultithreadTestCase good first issue triaged module: dtensor,good first issue triaged module: dtensor
[dtensor] enable tensor metadata check across ranks when run_check=True triaged module: dtensor,triaged module: dtensor
"DDP Elastic ""master_addr"" resolution error in environment variables. oncall: distributed triaged module: elastic",oncall: distributed triaged module: elastic
Adding Maximal Update Parametrization (ÂµP) to torch.nn.init module: nn triaged needs research,module: nn triaged needs research
RuntimeError when calling conv_transpose2d with groups needs reproduction module: nn triaged,needs reproduction module: nn triaged
avg_pool3d_backward fails on meta with grad_input parameter triaged module: meta tensors,triaged module: meta tensors
torch.jit.script produces incorrect gradients oncall: jit,oncall: jit
libtorch: runtime error when iterating batch of dataloader module: cpp triaged,module: cpp triaged
uninformative OOM error module: cuda triaged,module: cuda triaged
torch.topk returned values and indices are reordered if sorted=False triaged module: sorting and selection,triaged module: sorting and selection
torch.compile operation benchmark result is poor module: convolution triaged oncall: pt2 module: inductor module: cpu inductor,module: convolution triaged oncall: pt2 module: inductor module: cpu inductor
autocast not consistent across different GPUs (A100 and RTX A6000) triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
[inductor] Triton matmul templates should use reduced_precision_reduction flags feature good first issue triaged module: half oncall: pt2 module: inductor matrix multiplication,feature good first issue triaged module: half oncall: pt2 module: inductor matrix multiplication
torchrun fails to run on Windows 11 module: windows triaged,module: windows triaged
Call for a deterministic implementation of scatter_add_cuda_kernel triaged module: scatter & gather ops,triaged module: scatter & gather ops
Allow slicing of Nested Tensors along constant dimensions triaged module: nestedtensor,triaged module: nestedtensor
`bytes(...)` support of torch tensor does not match numpy + it would be nice to support tensor.tobytes() as alias feature triaged module: numpy,feature triaged module: numpy
"Breaking incompatibility with Cuda 12.2, pytorch stable, torchvision module: binaries module: crash module: cuda triaged",module: binaries module: crash module: cuda triaged
nn.Transformer has dropout layers that BERT / GPT-2 do not have module: docs triaged oncall: transformer/mha,module: docs triaged oncall: transformer/mha
"resutl of (torch.mm(a,b) does not match result of (a[:part,:], b) module: numerical-stability triaged matrix multiplication",module: numerical-stability triaged matrix multiplication
[inductor] CPU int32 overflow behavior differs between clang and gcc triaged bug oncall: pt2 module: m1 module: inductor module: cpu inductor,triaged bug oncall: pt2 module: m1 module: inductor module: cpu inductor
Pytorch profiler with Tensorboard example not working triaged module: tensorboard,triaged module: tensorboard
torch model to onnx conversion success but failed when inference module: onnx triaged,module: onnx triaged
The CPU version of `torch.cummax` is slow module: performance module: cpu triaged,module: performance module: cpu triaged
backend-friendly distributions module: distributions feature module: cuda triaged,module: distributions feature module: cuda triaged
RWKV + Adam exp_avg_sq will change from positive to negative after loss.backward() needs reproduction module: optimizer triaged,needs reproduction module: optimizer triaged
Suppport Fused AdamW on CPU module: performance feature module: optimizer triaged needs research,module: performance feature module: optimizer triaged needs research
DistributedDataParallel to support __getattr__ oncall: distributed triaged,oncall: distributed triaged
Efficient and robust calculation of diag(sparse @ diag @ sparse) module: sparse feature triaged,module: sparse feature triaged
CNN w variable sized input performance regression 1.10.2 cu113 -> 2.0.1 cu117 module: performance module: nn module: cuda triaged module: regression,module: performance module: nn module: cuda triaged module: regression
_foreach_copy_ with scalar second arg feature module: optimizer triaged actionable module: mta,feature module: optimizer triaged actionable module: mta
Torch compile generates incorrect graph on Llama model high priority triaged module: regression oncall: pt2 module: dynamic shapes,high priority triaged module: regression oncall: pt2 module: dynamic shapes
Wrong result of first run with torch.compile() when backend is using torch.jit.trace() and model has inplace operators  oncall: jit triaged oncall: pt2,oncall: jit triaged oncall: pt2
torch.einsum() computes different results on cpu and cuda on A100 GPU. module: cuda triaged module: linear algebra,module: cuda triaged module: linear algebra
multiple AMD GPUs module: multi-gpu module: rocm triaged,module: multi-gpu module: rocm triaged
Crash on converting circular padding  to onnx module: onnx triaged,module: onnx triaged
Generalize weight prepacking during quantized model deserialization oncall: quantization low priority triaged,oncall: quantization low priority triaged
"Added attention mechanism error,Need to modify torch.use_deterministic_algorithms(True) triaged module: determinism",triaged module: determinism
Transformer performance drop due to slow PyTorch GEMMs module: performance module: cuda triaged,module: performance module: cuda triaged
Pytorch versions without the abi3 flag module: binaries triaged module: python frontend,module: binaries triaged module: python frontend
Unrecognized attribute: axes for operator ReduceMean during onnx model conversion module: onnx triaged,module: onnx triaged
 DistributedSampler class: Change total_size into num_samples oncall: distributed triaged,oncall: distributed triaged
qnnpack quantized model can not be traced oncall: jit oncall: quantization low priority triaged,oncall: jit oncall: quantization low priority triaged
[Compile] Running Llama2 with torch.compile and FSDP results in Type mismatch assert in LlamaRotaryEmbedding  high priority oncall: distributed triaged module: fsdp oncall: pt2 module: distributed,high priority oncall: distributed triaged module: fsdp oncall: pt2 module: distributed
ninja: build stopped: subcommand failed. needs reproduction module: cpp-extensions triaged,needs reproduction module: cpp-extensions triaged
AdaptiveMaxPool documentation is not detailed module: docs module: nn triaged actionable module: pooling,module: docs module: nn triaged actionable module: pooling
[FSDP] incorrect backward prefetch order when using BackwardPrefetch.BACKWARD_POST triaged module: fsdp,triaged module: fsdp
[Performance] Pass in head_size_og to FlashAttentionV2  triaged module: multi-headed-attention,triaged module: multi-headed-attention
Enable FlashAttentionV2 on Windows triaged module: multi-headed-attention,triaged module: multi-headed-attention
FlashAttentionV2 will OOM when building on ci/cd with default settings module: cuda module: ci triaged,module: cuda module: ci triaged
TORCHELASTIC_RESTART_COUNT doesn't seem to be broadcasted to all worker triaged module: elastic oncall: r2p,triaged module: elastic oncall: r2p
`Tensor.uniform_` uses illegal argument name `from` module: distributions triaged module: python frontend,module: distributions triaged module: python frontend
[BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment module: bc-breaking module: nn oncall: transformer/mha topic: bc breaking,module: bc-breaking module: nn oncall: transformer/mha topic: bc breaking
_sampled_addmm_kernel cause 'misaligned address' with new triton pin module: sparse triaged,module: sparse triaged
[Optimizer Perf] Improve speed of _init_group to c++ module: performance module: optimizer triaged needs research,module: performance module: optimizer triaged needs research
Aliased Input/Output Requirement in `aot_export_joint_simple` triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher
DDP training can not accept subnet address in IPV6 oncall: distributed triaged,oncall: distributed triaged
Enhanced Available Backend Discovery and Selection in PyTorch 2 triaged enhancement module: python frontend,triaged enhancement module: python frontend
"Undefined Symobl: pybind11::detail::type_caster<at::Tensor, void>::load(pybind11::handle, bool) needs reproduction triaged module: pybind",needs reproduction triaged module: pybind
[FSDP] Ignored modules on meta device seem to be initialized on CUDA device oncall: distributed triaged actionable module: fsdp,oncall: distributed triaged actionable module: fsdp
Failure in Initiating Pyotch DDP-style code ( Multi-machine multi-card environment) oncall: distributed triaged,oncall: distributed triaged
Installation with rocm5.6 results in error: assert len(weights) == expected_node_count AssertionError needs reproduction module: build module: rocm triaged,needs reproduction module: build module: rocm triaged
`upsample_bilinear2d_backward_out_cuda` is nondeterministic triaged module: determinism,triaged module: determinism
Batching rule for aten::_scaled_dot_product_attention_math not yet implemented triaged actionable module: vmap oncall: transformer/mha module: functorch,triaged actionable module: vmap oncall: transformer/mha module: functorch
"aten.lift throws error in dynamo backends -> RuntimeError: !at::functionalization::impl::isFunctionalTensor(self)  INTERNAL ASSERT FAILED at ""../aten/src/ATen/FunctionalizeFallbackKernel.cpp"":167 triaged module: functionalization",triaged module: functionalization
Torch compile: libcuda.so cannot found triaged dependency issue oncall: pt2,triaged dependency issue oncall: pt2
PyTorch profile issues summary triage review module: regression oncall: profiler,triage review module: regression oncall: profiler
Provide a `reset_parameters()` method for MultiheadAttention to support FSDP meta device initializtion module: nn triaged module: fsdp,module: nn triaged module: fsdp
[FakeTensor] fake tensor mode not working with inference mode on Tensor.item() triaged oncall: pt2 module: fakeTensor module: dynamic shapes module: pt2-dispatcher,triaged oncall: pt2 module: fakeTensor module: dynamic shapes module: pt2-dispatcher
`C10_HOST_DEVICE` for `std::isnan(c10::complex<T>)`? module: cuda triaged,module: cuda triaged
About the multi-node example not working properly oncall: distributed triaged,oncall: distributed triaged
"""file_descriptor"" multiprocessing sharing strategy works incorrectly in dataloading  module: dataloader triaged module: data",module: dataloader triaged module: data
nn.AdaptiveMaxPool2d returns identical results within a batch high priority module: nn module: cuda triaged module: correctness (silent) bug,high priority module: nn module: cuda triaged module: correctness (silent) bug
FSDP custom args per module triaged module: fsdp,triaged module: fsdp
`torch.distributions.Pareto.sample` sometimes gives `inf` module: distributions triaged module: NaNs and Infs,module: distributions triaged module: NaNs and Infs
`add_image_with_boxes` method from `torch.utils.tensorboard.writer.SummaryWriter` is broken triaged module: tensorboard oncall: visualization,triaged module: tensorboard oncall: visualization
[feature request] [discussion] Include basic `ctypes` bindings for `cudart`/`cublasLt`/`cublas`/`nvrtc`/`cudnn` with stock PyTorch feature module: cuda triaged module: cublas,feature module: cuda triaged module: cublas
"Fake Tensor error 'lengths' argument should be a 1D CPU int64 tensor, but got 1D meta Long tensor triage review module: performance oncall: pt2 module: fakeTensor mlperf module: pt2-dispatcher",triage review module: performance oncall: pt2 module: fakeTensor mlperf module: pt2-dispatcher
Add caffe2 ideep/onednn tests to OSS CI oncall: releng module: ci triaged module: mkldnn,oncall: releng module: ci triaged module: mkldnn
libtorch infer error : CUDNN_STATUS_INTERNAL_ERROR oncall: jit,oncall: jit
conv cudnn support integers module: cudnn triaged,module: cudnn triaged
[ONNX] Retire FXSymbolicTracer in FX exporter module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Inconsistencies when handling scalars that are out of the range relative to the input tensor's dtype triaged module: int overflow,triaged module: int overflow
arange.out produces incorrect output when out tensor has dtype long triaged module: python frontend,triaged module: python frontend
where.self_out doesn't fail gracefully when inputs have different dtypes triaged module: type promotion module: advanced indexing module: edge cases,triaged module: type promotion module: advanced indexing module: edge cases
index.Tensor_out & index_put.out errors or segfaults with indices list containing only null tensors  high priority triaged module: advanced indexing,high priority triaged module: advanced indexing
New variables in torch._ops.py pollute the torch.ops namespace triaged module: library,triaged module: library
masked_fill_ outputs incorrect results for 'mps' tensor after transpose triaged module: mps,triaged module: mps
Inconsistencies when casting to integral types triaged module: type promotion module: arm module: int overflow,triaged module: type promotion module: arm module: int overflow
Error in ONNX during Export GLU with Opset 18 module: onnx triaged,module: onnx triaged
[Dynamo] 'NoneType' object is not subscriptable from torchrec (bad error message) triaged oncall: pt2,triaged oncall: pt2
torch.nn.functional.cross_entropy different loss when providing one_hot_target and class weights module: nn module: loss triaged,module: nn module: loss triaged
[Torch.fx] Torch fx failed to trace torch extension library triaged module: fx,triaged module: fx
torch.dot gives wrong result on Macos high priority triaged module: macos module: correctness (silent),high priority triaged module: macos module: correctness (silent)
`RuntimeError: expected scalar type BFloat16 but found Float` with `torch.nn.TransformerEncoder` module: nn triaged oncall: transformer/mha module: amp (automated mixed precision),module: nn triaged oncall: transformer/mha module: amp (automated mixed precision)
A backward bug of dtensor seems to be caused by new_empty_strided high priority oncall: distributed triaged module: dtensor,high priority oncall: distributed triaged module: dtensor
fullgraph=True doesn't actually raise error when you don't manage full graph inside DDP triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
[DDP PT2] TypeError: convert_frame_assert.<locals>._convert_frame_assert() missing 2 required positional arguments: 'hooks' and 'frame_state' triage review module: performance module: ddp oncall: pt2 module: dynamo mlperf module: distributed,triage review module: performance module: ddp oncall: pt2 module: dynamo mlperf module: distributed
torch.fx.Interpreter modules don't get compiled triaged module: dynamo,triaged module: dynamo
ModuleNotFoundError: No module named 'torchgen.code_template' module: build triaged module: android oncall: mobile,module: build triaged module: android oncall: mobile
Previous version not found oncall: releng triaged,oncall: releng triaged
Support AMD Ryzen Unified Memory Architecture (UMA) module: rocm triaged,module: rocm triaged
Select on a coalesced COO tensor returns COO tensor with coalesce flag set to False. module: sparse feature triaged,module: sparse feature triaged
[FakeTensor] `to` doesn't error with `allow_non_fake_inputs=False` triaged module: fakeTensor,triaged module: fakeTensor
[LibTorch/iOS] Building with METAL support script is freezing triaged oncall: mobile module: ios,triaged oncall: mobile module: ios
haloï¼ŒI continue pretrain llama2-13B model ï¼Œbut save state_dict is about 50GB file oncall: distributed triaged,oncall: distributed triaged
caching keys+values in TransformerDecoderLayer for faster inference triaged oncall: transformer/mha,triaged oncall: transformer/mha
RuntimeError: Unsupported value kind: Tensor while torch.jit.script nn.Module oncall: jit,oncall: jit
Integer multiplication overflow when running torch.nn.AdaptiveAvgPool2d triaged module: int overflow,triaged module: int overflow
Integer multiplication overflow when running torch.nn.MaxUnpool3d triaged module: int overflow,triaged module: int overflow
Integer multiplication overflow when running torch.diagflat triaged module: int overflow,triaged module: int overflow
Storage size calculation overflowed when torch.nn.Upsample triaged module: int overflow,triaged module: int overflow
Storage size calculation overflowed when running torch.nn.functional.interpolate triaged module: int overflow,triaged module: int overflow
Integer multiplication overflow when running torch.eye triaged module: int overflow,triaged module: int overflow
Integer calculation overflow when running torch.nn.functional.adaptive_avg_pool2d triaged module: int overflow,triaged module: int overflow
Integer overflow when running torch.nn.functional.upsample_bilinear triaged module: int overflow,triaged module: int overflow
Integer overflow when running torch.nn.functional.upsample triaged module: int overflow,triaged module: int overflow
Integer overflow when running torch.nn.ReplicationPad3d triaged module: int overflow,triaged module: int overflow
Integer overflow when running torch.nn.AdaptiveAvgPool2d triaged module: int overflow,triaged module: int overflow
Integer overflow when running torch.nn.MaxUnpool2d triaged module: int overflow,triaged module: int overflow
Index out of bound when running torch.gather triaged module: advanced indexing,triaged module: advanced indexing
Integer overflow when running torch.nn.functional.max_unpool2d triaged module: int overflow,triaged module: int overflow
[fx] tracing function with in-place mutation results in unexpected behaviour due to local vars becoming persisted in  `GraphModule(nn.Module)` triaged module: fx,triaged module: fx
Appending new logs to existing tbevent files when using tensorboard triaged module: tensorboard,triaged module: tensorboard
NNPACK slow down M1/M2 Mac CPU triaged module: nnpack,triaged module: nnpack
Inconsistent results when running torch.arctanh module: cuda triaged,module: cuda triaged
conv2d wrong results on 3090/3090ti triaged module: half,triaged module: half
[nightly][jit] bad constant exponent (e+38.f) in default_program fused_mul_div_add oncall: jit triaged,oncall: jit triaged
Mismatch in type of error raised when reducing along empty slice between eager and primtorch triaged module: primTorch,triaged module: primTorch
Adding batched CSR tensors with different sparsities produces an invalid tensor module: sparse triaged module: correctness (silent) bug,module: sparse triaged module: correctness (silent) bug
Conversion from COO with two sparse dimensions to CSR with dense_dim specified fails module: sparse feature triaged bug,module: sparse feature triaged bug
Determinism by using datapipes shuffle module: dataloader triaged module: determinism,module: dataloader triaged module: determinism
The generated triton MaxPool2d kernel has poor performance on amd vega20/60 module: rocm triaged module: inductor rocm,module: rocm triaged module: inductor rocm
[FSDP]coding to multi-node save optimizer error triaged module: fsdp,triaged module: fsdp
make backward function explicit in a layer which is a combination of some ops module: nn triaged oncall: profiler,module: nn triaged oncall: profiler
No checks when running torch.nn.functional.ctc_loss with bogus inputs module: loss module: cpu triaged,module: loss module: cpu triaged
"Inconsistent results when running torch.nn.functional.embedding_bag on CPU (1.12.0, 1.13.0) module: cpu triaged module: embedding",module: cpu triaged module: embedding
Abort when running torch.set_num_interop_threads module: crash triaged module: single threaded,module: crash triaged module: single threaded
"max_pool1d, max_pool2d, max_pool3d Integers for cpu and cuda triaged actionable module: pooling",triaged actionable module: pooling
Multiple runners shutdown for an autoupdate while still running jobs triaged ci: sev-mitigated,triaged ci: sev-mitigated
[regression] Not getting `CUDA error: device-side assert triggered` on main for CUDA_KERNEL_ASSERT2 module: cuda triaged module: regression,module: cuda triaged module: regression
[LibTorch/iOS] Unknown custom class type quantized.Conv2dPackedParamsBase. Please ensure it is registered triaged oncall: mobile module: ios,triaged oncall: mobile module: ios
Overly strict type hints for `torch.utils.data.random_split` module: dataloader module: typing triaged,module: dataloader module: typing triaged
caffe does not respect CUDNN_LIB_DIR when building from source (cmake) module: build triaged module: nvfuser,module: build triaged module: nvfuser
Incorrect type hint for `torch.library.Library.define` module: typing triaged module: library,module: typing triaged module: library
sparse_mask method ignores masked-in elements of sparse compressed input tensors module: sparse triaged module: correctness (silent) bug,module: sparse triaged module: correctness (silent) bug
DataParallel scatter method split tensor wrong module: cuda triaged module: data parallel,module: cuda triaged module: data parallel
torch compile error with SyncBatchNorm good first issue triaged module: c10d oncall: pt2 module: dynamo,good first issue triaged module: c10d oncall: pt2 module: dynamo
Regression in text encoding module: performance module: cuda triaged module: multithreading module: regression,module: performance module: cuda triaged module: multithreading module: regression
dist.scatter is incompatible with transpose/permute operation oncall: distributed triaged,oncall: distributed triaged
"using Union[str, Tensor] as an argument to a torch.jit.script function oncall: jit triaged",oncall: jit triaged
NumPy 2.0 Support triaged module: numpy module: python frontend,triaged module: numpy module: python frontend
dist.destroy_process_group did not destroy the process group well oncall: distributed triaged,oncall: distributed triaged
"'MPS' training Issue(s) with NanoGPT: -Inf, NaN's triaged module: mps",triaged module: mps
Sparse compressed tensor values autograd support is not implemented module: sparse module: autograd triaged bug,module: sparse module: autograd triaged bug
CUBLAS_STATUS_NOT_SUPPORTED needs reproduction module: cuda triaged module: cublas,needs reproduction module: cuda triaged module: cublas
model.forward() get error with torch.compile() when using huggingface llama triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
`torch.float8_e4m3fn` does not support `torch.cat` triaged module: float8,triaged module: float8
Conda configuration shouldn't pollute $PATH variable oncall: releng module: ci triaged,oncall: releng module: ci triaged
How to export GNN with dict inputs correctly? module: onnx triaged,module: onnx triaged
"[CPP API] Add Adadelta, Adamax, ASGD, NAdam, RAdam and Rprop module: cpp module: optimizer triaged needs research",module: cpp module: optimizer triaged needs research
Documenting `__getitems__` for slicing support in `torch.utils.data` triaged module: data,triaged module: data
Documenting `IterableDataset`'s needing `StopIteration` for finite data triaged module: data,triaged module: data
The difference between input grad computed by channels last backward and the input grad computed by channels first backward of Hardswish on MPS is too large module: nn triaged module: memory format module: correctness (silent) module: mps,module: nn triaged module: memory format module: correctness (silent) module: mps
The difference between channels last backward and channels first backward of AvgPool2d on CUDA is too large module: nn module: cuda triaged module: memory format,module: nn module: cuda triaged module: memory format
Can't construct a tensor from List[SymFloat] triage review module: performance oncall: pt2 module: dynamic shapes module: dynamo mlperf,triage review module: performance oncall: pt2 module: dynamic shapes module: dynamo mlperf
"sdp_kernel causes dynamo error on torch.compile(model, fullgraph=True) good first issue triaged oncall: pt2 module: dynamo",good first issue triaged oncall: pt2 module: dynamo
Torch randn cannot take symbol shapes as shape argument. triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
jit compilation returns an int rather than a bool when using math.isnan() oncall: jit,oncall: jit
[dynamo] calling __torch_function__ with dynamically created subclass of torch.Tensor fails compilation triaged module: __torch_function__ module: dynamo,triaged module: __torch_function__ module: dynamo
torch.inverse throws error when DP but not in DDP or single GPU needs reproduction triaged module: data parallel module: linear algebra module: edge cases,needs reproduction triaged module: data parallel module: linear algebra module: edge cases
combining `vmap` with NN containing `MaxPool2d' leads to discrepancies in output triaged module: functorch,triaged module: functorch
H100 works differently than rtx4090 on same model needs reproduction triaged module: numerical-reproducibility,needs reproduction triaged module: numerical-reproducibility
from_blob python api triaged enhancement module: python frontend,triaged enhancement module: python frontend
Error when using sparse_coo tensor with optimizer module: sparse triaged,module: sparse triaged
RuntimeError with operations on torch.float8_e5m2 and torch.float_e4m3fn data types triaged module: float8,triaged module: float8
[FSDP] summon_full_params won't change parameters triaged module: fsdp,triaged module: fsdp
[Dynamo] Unable to Trace AdamW Optimizer when there is LR Scheduler high priority module: optimizer triaged oncall: pt2 module: dynamic shapes,high priority module: optimizer triaged oncall: pt2 module: dynamic shapes
Dynamo x FSDP - Issue Tracking Master Task triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
"I want to calculate the matrix multiplication of two Boolean matrices, but torch.mm will report an error. Is there any more efficient alternative? triaged module: boolean tensor",triaged module: boolean tensor
a bug about tensor stride triaged module: viewing and reshaping,triaged module: viewing and reshaping
cov onnx error module: onnx triaged,module: onnx triaged
max_pool3d_with_indices_backward_cuda does not have a deterministic implementation triaged module: determinism module: pooling,triaged module: determinism module: pooling
Apply fusion more aggressively in NAdam and Adagrad compilation good first issue triaged oncall: pt2 module: inductor,good first issue triaged oncall: pt2 module: inductor
Dynamic shapes support for inductor foreach codegen good first issue triaged oncall: pt2 module: dynamic shapes module: inductor,good first issue triaged oncall: pt2 module: dynamic shapes module: inductor
"RuntimeError: 0 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":615, please report a bug to PyTorch. We don't have an op for aten::full but it isn't a special case.  Argument types: int[], bool, NoneType, NoneType, Device, bool,  module: onnx triaged",module: onnx triaged
`ray` multiprocessing interference by torch import module: multiprocessing triaged,module: multiprocessing triaged
Facing error while using onnx from scatterelements module: onnx triaged,module: onnx triaged
RuntimeError: _Map_base::at when exporting squeeze module: onnx module: autograd triaged,module: onnx module: autograd triaged
Found two conflicting CUDA installs module: build triaged,module: build triaged
ONNX Model Producing Different Results Compared to Original PyTorch and JIT Traced Model module: onnx triaged,module: onnx triaged
"`tensor.repeat` quirks: has no `torch.` variant, no `out=` variant, no inplace variant | `torch.tile` also does not have `out=` variant and uses `dims=` instead of `dim=` triaged module: viewing and reshaping module: python frontend",triaged module: viewing and reshaping module: python frontend
Readily available python wheels for windows ARM module: binaries module: windows feature triaged,module: binaries module: windows feature triaged
stride of gradient is not same as the corresponding tensor module: autograd module: optimizer triaged actionable,module: autograd module: optimizer triaged actionable
[Minor Bug] Should consume_prefix_in_state_dict_if_present change ordering of keys? module: docs module: nn triaged actionable,module: docs module: nn triaged actionable
Other overloads of `_foreach_clamp` module: optimizer triaged enhancement actionable module: mta,module: optimizer triaged enhancement actionable module: mta
[autograd.Function] torch.compile w/ once_differentiable leads to opaque graph break high priority triaged oncall: pt2 module: dynamo,high priority triaged oncall: pt2 module: dynamo
Dynamo graph break when using pyton module `heapq` (manipulates with `list`s) low priority triaged module: dynamo,low priority triaged module: dynamo
[Dyanmo] Pre-allocate flag should be a ONNXRuntime inference session level attribute module: onnx triaged,module: onnx triaged
untimeError: The following operation failed in the TorchScript interpreter. Traceback of TorchScript (most recent call last): RuntimeError: nvrtc: error: invalid value for --gpu-architecture (-arch) needs reproduction oncall: jit,needs reproduction oncall: jit
`1/torch.inf` produce inconsistent results module: numerical-stability triaged module: complex module: type promotion module: edge cases,module: numerical-stability triaged module: complex module: type promotion module: edge cases
Use expect tests for error inputs triaged better-engineering module: testing,triaged better-engineering module: testing
Optimizers should use learning rates passed as tensors directly module: optimizer triaged actionable module: dynamic shapes,module: optimizer triaged actionable module: dynamic shapes
"Timer benchmark stores only one time value, and therefore has broken mean/median/etc metrics triaged module: benchmark",triaged module: benchmark
"[ux] Suppot torch.tensor(set([1,2,3])) triaged needs research topic: new features module: python frontend",triaged needs research topic: new features module: python frontend
inf and nan are mapped to quant_min in torch.fake_quantize_per_tensor_affine oncall: quantization triaged,oncall: quantization triaged
[Feature request] Add new API Tensor.device_as  triage review oncall: jit feature module: python frontend,triage review oncall: jit feature module: python frontend
Add AMD image to the .devcontainer spec triaged better-engineering,triaged better-engineering
Provide .devcontainer PyTorch - MPS environment triaged better-engineering,triaged better-engineering
Dev Container Support for PyTorch triaged better-engineering,triaged better-engineering
'CUDA out of memory' when using a GPU services for reinforcement learning in Torch rpc tutorial oncall: distributed module: rpc,oncall: distributed module: rpc
Dataloader extremely slow on in-memory datasets module: dataloader triaged module: data,module: dataloader triaged module: data
C++ API `torch::nn::MultiheadAttention` Crashes by division by zero module: crash module: cpp triaged oncall: transformer/mha,module: crash module: cpp triaged oncall: transformer/mha
torch.jit.script: scripting doesn't work with wraps oncall: jit,oncall: jit
torch.polygamma inconsistent with scipy.special.polygamma for n >= 1 triaged module: numpy module: special,triaged module: numpy module: special
DDP grads not synced when static_graph=True and module output is a dict subclass? oncall: distributed module: pytree,oncall: distributed module: pytree
Inconsistency between CPU and GPU for `Linear()` layer with input size 0 module: nn triaged actionable module: edge cases,module: nn triaged actionable module: edge cases
Will torch.sparse.mm support multiplying two boolean matrices? module: sparse triaged module: boolean tensor,module: sparse triaged module: boolean tensor
Question about garbage collection without GPU sync  triaged module: CUDACachingAllocator,triaged module: CUDACachingAllocator
Using retain_graph in backward() with FSDP oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Confusing error message for DataLoader with num_workers=0 and non-zero timeout module: dataloader triaged,module: dataloader triaged
Refcount problem for torch.distributed.Store objects defined in Python oncall: distributed module: c10d,oncall: distributed module: c10d
"[feature request] [ux] Frontend methods for fused  elementwise affine transform: mul+add+dtype convert + support  `integer_tensor.mul_(float_constant)` and `float_tensor.mul(some_constant, out = integer_tensor)` maybe via new args `rounding_mode=...` and `dtype=...` + maybe support OpenCV-style saturated dtype conversions (e.g. `clamp_` before conversion) triaged module: type promotion module: python frontend",triaged module: type promotion module: python frontend
Meta implementations of FFT operators often have incorrect strides triaged module: fft oncall: pt2,triaged module: fft oncall: pt2
FFT Samples Inputs with More than Three Dimensions module: tests triaged module: fft,module: tests triaged module: fft
Case study of torch.compile / cpp inductor on CPU: min_sum / mul_sum with 1d / matmul-like with static / dynamic shapes triaged module: custom-operators oncall: pt2 module: dynamic shapes module: pt2-dispatcher,triaged module: custom-operators oncall: pt2 module: dynamic shapes module: pt2-dispatcher
More Performant CachingHostAllocator for Pinned Memory Allocation module: cuda triaged,module: cuda triaged
Relu6 not able to process nan values triaged module: NaNs and Infs,triaged module: NaNs and Infs
[discussion] move-semantics for tensors triaged module: python frontend,triaged module: python frontend
Lacking commutativity of `tensor.expand` and `tensor.flatten` triaged module: viewing and reshaping,triaged module: viewing and reshaping
"Boolean valued images loaded from disk, when converted to torch int/float tensor, the True valued pixels gets converted to 255 instead of 1 triaged module: boolean tensor",triaged module: boolean tensor
DTensor Sharding prop cache stats oncall: distributed triaged module: dtensor,oncall: distributed triaged module: dtensor
install cuda version always get cpuonly oncall: releng triaged,oncall: releng triaged
NotImplementedError: Could not run 'aten::multinomial' with arguments from the 'Meta' backend. triaged module: random,triaged module: random
Pytorch: torch.autograd.grad returns NoneType module: autograd triaged,module: autograd triaged
Can't build PyTorch 1.13.1 with Vulkan support triaged module: vulkan ciflow/periodic,triaged module: vulkan ciflow/periodic
Potential Issue with Pandas Dataframe needs reproduction triaged oncall: pt2,needs reproduction triaged oncall: pt2
`softmax` to handle dimensions comprised of `-inf` module: nn triaged,module: nn triaged
"Branch name in double quotes """" module: ci triaged",module: ci triaged
Dataset  with Queue issue module: dataloader triaged module: data,module: dataloader triaged module: data
CUDA device support does not register allocator to c10::GetAllocator(...) module: cpp module: cuda triaged,module: cpp module: cuda triaged
Distributed torch.linalg.eigh (and other functions) on cuda using cuSOLVERMG module: cuda triaged module: linear algebra,module: cuda triaged module: linear algebra
Increasing batch size makes network forward 1000 times slower module: cudnn module: nn module: cuda triaged,module: cudnn module: nn module: cuda triaged
Extreme slowdown of torch.mm for certain sizes and strides with bfloat16 module: cuda triaged module: bfloat16,module: cuda triaged module: bfloat16
nn.CrossEntropyLoss with invalid target generates corrups memory eventualy leading to CUDA error: an illegal memory access module: nn module: loss triaged,module: nn module: loss triaged
AOTAutograd should detect false aliasing. triaged module: viewing and reshaping oncall: pt2 module: aotdispatch module: pt2-dispatcher,triaged module: viewing and reshaping oncall: pt2 module: aotdispatch module: pt2-dispatcher
"vmap, jacrev, jacfwd, hessian, etc., in libTorch module: cpp triaged module: vmap module: functorch",module: cpp triaged module: vmap module: functorch
Check for output_padding <= stride/dilation in ConvTranspose1d module: convolution triaged module: padding,module: convolution triaged module: padding
"[JIT] .item() dict keys cause `RuntimeError: Cannot create dict for key type 'Scalar', only int, float, complex, Tensor, device and string keys are supported` oncall: jit",oncall: jit
inconsistent dtype of scale and zero_point in observers high priority oncall: quantization triaged,high priority oncall: quantization triaged
`torch.nn.utils.clip_grad_norm_()` causes H2D sync with foreach ops. module: nn triaged module: mta,module: nn triaged module: mta
PyTorchMPS not showing up in Instruments for `torch.mps.profiler` triaged module: mps,triaged module: mps
Installing torchvision for CPU leads to unwanted upgrade of torch + pip would not install nightly as considers that release is the latest (?) module: binaries triaged,module: binaries triaged
Command to reproduce error is incorrect good first issue module: tests triaged module: infra module: testing,good first issue module: tests triaged module: infra module: testing
nll_loss reference shouldn't be registered as a decomposition. triaged module: primTorch module: decompositions,triaged module: primTorch module: decompositions
Calling ops.aten.embedding_bag() function got silent crash module: crash module: nn triaged,module: crash module: nn triaged
Improving save_on_cpu's performance by overlapping memory transfers with compute oncall: distributed module: autograd triaged enhancement,oncall: distributed module: autograd triaged enhancement
backwards compatibility about class _LRScheduler triaged module: LrScheduler,triaged module: LrScheduler
Transformer.generate_square_subsequent_mask has nan values on MPS device triaged module: NaNs and Infs module: mps,triaged module: NaNs and Infs module: mps
"ReduceLROnPlateau increases learning rate exponentially, causing training to diverge triaged module: LrScheduler",triaged module: LrScheduler
`torch.nn.modules.MultiheadAttention` yields different graph under pre_dispatch tracing triaged module: __torch_function__ pre_dispatch tracing oncall: export,triaged module: __torch_function__ pre_dispatch tracing oncall: export
Torch.onnx.export a fp16 model but get the output tensor fp32 module: onnx triaged,module: onnx triaged
Can't build with non-static protobuf module: build triaged,module: build triaged
RuntimeError: Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'grad_y' module: autograd triaged actionable module: mps,module: autograd triaged actionable module: mps
Runtime Error: Empty tensor needs reproduction triaged module: mps,needs reproduction triaged module: mps
 Improve Error Message in MultiMarginLoss for Inconsistent Target Size module: nn good first issue module: error checking triaged actionable,module: nn good first issue module: error checking triaged actionable
OneCycleLR's state_dict includes a full reference to the optimizer module: optimizer triaged needs design module: LrScheduler,module: optimizer triaged needs design module: LrScheduler
ghstack + mergebot race condition module: ci triaged,module: ci triaged
RProp improvement tracker module: optimizer triaged actionable,module: optimizer triaged actionable
Build failure due to C++ version mismatch module: build triaged,module: build triaged
Scalar Tensor lowering to Fake Tensor inside Inductor triaged bug module: inductor module: dynamo,triaged bug module: inductor module: dynamo
`torch.ops.aten.split.Tensor._schema` return alias annotations are wrong triaged module: viewing and reshaping,triaged module: viewing and reshaping
distributed.batch_isend_irecv() crash when send/recv refers to itself oncall: distributed module: crash module: c10d,oncall: distributed module: crash module: c10d
Sparse COO indices are torch.Int64 -- is this necessary? module: sparse triaged,module: sparse triaged
"`export(..., pre_dispatch=True)` for model in eval mode still inserts autograd ops triaged pre_dispatch tracing oncall: export",triaged pre_dispatch tracing oncall: export
bc-linter false positive with TypeAliases triaged module: devx,triaged module: devx
Registering function that takes const std::vector<c10::SymInt>& to SymInt[] schema gives confusing error message triaged module: dynamic shapes,triaged module: dynamic shapes
torch._subclasses.fake_tensor.DynamicOutputShapeException: aten.nonzero.default triaged module: fakeTensor,triaged module: fakeTensor
Libtorch report C10 error when compiling on my own project needs reproduction module: binaries module: cpp triaged,needs reproduction module: binaries module: cpp triaged
[feature request] Better argument checks and error messaging for `tensor.repeat` module: error checking triaged actionable release notes: python_frontend,module: error checking triaged actionable release notes: python_frontend
Got error when train models with more than one param_group in torch2.0 module: optimizer triaged has workaround,module: optimizer triaged has workaround
MPS cumprod gradient is broken even when using cpu fallback on macos 13.2.1 triaged module: mps,triaged module: mps
llama model failed for dynamic shape path needs reproduction triaged oncall: pt2 module: dynamic shapes module: dynamo,needs reproduction triaged oncall: pt2 module: dynamic shapes module: dynamo
Potential lack of CI testing on older NVIDIA GPU module: ci triaged,module: ci triaged
"Tensors always get 0/1 specialization guards, even if they're not used triaged oncall: pt2 module: dynamic shapes module: guards",triaged oncall: pt2 module: dynamic shapes module: guards
Avoid incrementing refcount of `grad_fn` in `unpack_list` module: autograd triaged module: mta,module: autograd triaged module: mta
[FSDP] Investigate sharded GPU gradient lifetime when CPU offloading triaged module: fsdp,triaged module: fsdp
Misleading error message in multilabel_margin_loss when passing incompatible tensor dimensions module: nn triaged actionable,module: nn triaged actionable
"vmap and rnn/lstm ""accessing '.data' under vmap transform is not allowed"" triaged module: functorch",triaged module: functorch
"Error in Profiler : RuntimeError: Expected !config.profile_memory to be true, but got false oncall: profiler",oncall: profiler
Ensure PRs are rebased on top of a recent commit (CI check) triaged module: devx,triaged module: devx
Differences in the results of conv2d calculations in PyTorch 1.8 needs reproduction module: cudnn triaged,needs reproduction module: cudnn triaged
Flip default on `add_zero_attn` in `torch.nn.MultiheadAttention` to `True` module: nn triaged oncall: transformer/mha,module: nn triaged oncall: transformer/mha
Torch.jit : RuntimeError:  Unable to extract string literal index for ModuleDict oncall: jit,oncall: jit
Missing coalesced flag from `torch.autograd.Function.backward` module: sparse triaged,module: sparse triaged
Batching rule for aten::bincount. triaged module: functorch,triaged module: functorch
Libtorch linking Error:undefined reference module: build triaged,module: build triaged
FakeTensor detach() gives meta tensor other than FakeTensor under `torch._C._DisableTorchDispatch()` triaged tensor subclass module: fakeTensor,triaged tensor subclass module: fakeTensor
PyTorch 2.0.x `CUDA error: operation not supported` when `Tensor.to` a different device needs reproduction module: cuda triaged module: regression,needs reproduction module: cuda triaged module: regression
Programmation error enabling unlegal memory access on gpu module: error checking triaged module: mps,module: error checking triaged module: mps
Bug when dealing with fallbacks on CPU triaged module: complex module: dynamic shapes module: inductor module: cpu inductor,triaged module: complex module: dynamic shapes module: inductor module: cpu inductor
Strange backward behavior with sparse tensors module: sparse module: autograd triaged,module: sparse module: autograd triaged
[FSDP] FSDP doesn't work (random accuracy performance) when using `param_init_fn` and `sync_module_states=True` feature triaged module: fsdp,feature triaged module: fsdp
"MPS memory issue,  MPS backend out of memory, but works if I empty the MPS cache module: memory usage triaged module: mps",module: memory usage triaged module: mps
Exporting the operator 'aten::grad' to ONNX opset version 18 is not supported. module: onnx module: autograd triaged,module: onnx module: autograd triaged
Conversion Error to ComplexDouble on MPS triaged module: complex module: mps,triaged module: complex module: mps
inconsistent signature for dataloader in docs/source/data.rst module: dataloader triaged,module: dataloader triaged
torch.sparse.mm() with reduce operator for GPU support and COO matrices module: sparse feature triaged module: reductions,module: sparse feature triaged module: reductions
"DDP , error . [c10d] The client socket has timed out after 900s while trying to connect to (XX.XX.XX.XX, 8514). oncall: distributed",oncall: distributed
https://pytorch.org/docs/stable/backends.html does not describe torch.backends.cpu module: docs module: cpu triaged actionable,module: docs module: cpu triaged actionable
torch.compile uses more memory when using less parameters module: memory usage module: convolution triaged oncall: pt2,module: memory usage module: convolution triaged oncall: pt2
Pypi is missing dependencies module: binaries triaged,module: binaries triaged
[FSDP] using CPUOffload cannot make the code runing stop triaged module: fsdp,triaged module: fsdp
Compile error PyTorch 2.0.1 / GCC 13.1.0 module: build triaged,module: build triaged
There is a big precision error between A100 and 3090 when using torch.matmul with fp16 precision module: cuda triaged module: half,module: cuda triaged module: half
Dynamo test pipeline failed on MaxPool2d test when changed to use f-string module: ci module: tests triaged module: dynamo,module: ci module: tests triaged module: dynamo
Unable to build documents module: onnx triaged module: doc infra,module: onnx triaged module: doc infra
Support symmetry in einsum triaged module: linear algebra,triaged module: linear algebra
Error using torch.compile with HF transformers and model `mosaicml/mpt-7b` triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
`torch.autocast(bfloat16)` runs bwd matmuls in fp16 triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
Running Llama 2 on Apple Silicon GPUs - missing MPS types and operators triaged module: mps,triaged module: mps
Pytorch -  cpu only & caffe2 build failing module: build caffe2 triaged,module: build caffe2 triaged
Tensor subclass is not preserved during backward with gradient checkpointing module: checkpoint triaged module: __torch_function__ tensor subclass,module: checkpoint triaged module: __torch_function__ tensor subclass
Turn indexing with a scalar tensor into an copy into a view and avoid a D2H synchronization. module: bc-breaking triaged module: numpy module: advanced indexing topic: bc breaking,module: bc-breaking triaged module: numpy module: advanced indexing topic: bc breaking
Syntax error when compileing Megatron-LM models. triaged ezyang's list oncall: pt2,triaged ezyang's list oncall: pt2
FSDP with gradient checkpointing lead to redundant allgathers during backward triaged module: fsdp,triaged module: fsdp
[inductor] unexpected dynamic shape error encountered in TritonTemplate triaged ezyang's list oncall: pt2 module: dynamic shapes,triaged ezyang's list oncall: pt2 module: dynamic shapes
F.pad will accept 0 and negative values as parameter module: nn module: error checking triaged module: padding,module: nn module: error checking triaged module: padding
[ONNX] fix `test_fx_op_consistency.py` test failure when running on torch built with cuda module: onnx triaged,module: onnx triaged
Out of bounds error with `nn.MultiMarginLoss` low priority triaged hackathon oncall: pt2,low priority triaged hackathon oncall: pt2
torch.sparse.sampled_addmm doesn't compute gradients for 3D tensors module: sparse triaged,module: sparse triaged
Efficient BMM for sparse-dense tensors module: sparse triaged topic: new features,module: sparse triaged topic: new features
torch.onnx.export error module: onnx triaged,module: onnx triaged
[FSDP] Revisit mixed-precision casting logic triaged module: fsdp,triaged module: fsdp
torch.save throws an error when the path uses mixed separators on Windows module: windows triaged,module: windows triaged
[OpInfo] index.Tensor triaged module: testing,triaged module: testing
"[proposal] Bit ops: e.g. setbit/getbit/togglebit/byteswap + introduce well-standardized unsigned dtypes (uint16, uint32, uint64) feature triaged needs research module: python frontend",feature triaged needs research module: python frontend
Specify version module: docs triaged,module: docs triaged
Top level Glossary for users (not contributers) module: docs triaged,module: docs triaged
Will nn.unfold support non-4D-tensor input in future version? module: nn triaged enhancement actionable,module: nn triaged enhancement actionable
Silent Error of torch.fx.symbolic_trace when forward hooks are registered triaged module: fx,triaged module: fx
`vmap` causes unpredictable behavior when combined with `autocast` triaged module: vmap module: amp (automated mixed precision) module: functorch,triaged module: vmap module: amp (automated mixed precision) module: functorch
Need support and testing for Adam optimizer for MPS high priority module: optimizer triaged enhancement module: mps,high priority module: optimizer triaged enhancement module: mps
FSDP loading with a partial state triggers KeyError triaged module: fsdp,triaged module: fsdp
Quadric Layer feature module: nn triaged needs research,feature module: nn triaged needs research
FSDP Full Shard compatibility with BF16 AMP oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
[ONNX] Refactor `test_fx_op_consistency.py` module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Enable SLEEF on ARM module: build triaged module: sleef module: arm topic: improvements,module: build triaged module: sleef module: arm topic: improvements
TorchInductor Hack-a-Day on July 19th triaged tracker,triaged tracker
Can't vmap over torch.tensor constructor triaged module: functorch,triaged module: functorch
Padded tensor subclass feature triaged module: nestedtensor tensor subclass,feature triaged module: nestedtensor tensor subclass
DeadKernel when training GNN for Cora on MPS triaged module: mps,triaged module: mps
Failed to convert model that has LeakyReLU to ONNX module: onnx triaged,module: onnx triaged
Batching rule not implemented for aten::unsafe_chunk triaged actionable module: vmap module: functorch,triaged actionable module: vmap module: functorch
"Backward pass with sparse parameters results in error ""Sparse division requires a scalar or zero-dim dense tensor divisor"" module: sparse module: loss module: optimizer triaged",module: sparse module: loss module: optimizer triaged
Torch.compile Error: RuntimeError: aten::_conj() Expected a value of type 'Tensor' for argument 'self' but instead found type 'complex'. triaged module: complex module: functionalization oncall: pt2 module: pt2-dispatcher,triaged module: complex module: functionalization oncall: pt2 module: pt2-dispatcher
Inductor generates incorrect CPU code for `uint8` operations triaged oncall: pt2 module: cpu inductor,triaged oncall: pt2 module: cpu inductor
[discussion] Integrate widely used utilities from fvcore into the core repo oncall: distributed feature module: nn triaged needs research module: LrScheduler,oncall: distributed feature module: nn triaged needs research module: LrScheduler
Multiple linux jobs are failing with version `GLIBCXX_3.4.30' not found  module: ci triaged,module: ci triaged
Significant time difference of calculating Jacobian matrix using jacrev and oracle functions module: autograd triaged module: functorch,module: autograd triaged module: functorch
Export+AOTInductor issue tracker triaged tracker,triaged tracker
[DTensor] Dtensor API should report the correct device when GPU is used triaged module: dtensor,triaged module: dtensor
[DTensor] Module parallelized with ColwiseParallel should return a sharded tensor triaged module: dtensor,triaged module: dtensor
autocast + torch.no_grad inference cause backward graph nodes to be lost module: autograd triaged,module: autograd triaged
Pytorch dataloader not loading first-available data with multiple workers module: dataloader triaged,module: dataloader triaged
Error loading TorchScript model with torchvision::nms operation in libtorch oncall: jit,oncall: jit
Repro str could be displayed with slightly wrong env vars module: docs module: cuda triaged actionable,module: docs module: cuda triaged actionable
TypeError: 'NoneType' object is not subscriptable (Occurred when translating col2im). Can't translate torch.nn.functional.fold in opset_version 18. module: onnx triaged,module: onnx triaged
MacOS arm64 runners are not available in CI module: ci triaged,module: ci triaged
Remaining functions without meta registrations triaged module: meta tensors,triaged module: meta tensors
workaround for using vmap when .item() is being used internally triaged module: vmap module: functorch,triaged module: vmap module: functorch
[RFC] Proposal to upgrade LLVM version triaged NNC module: cpu inductor,triaged NNC module: cpu inductor
[linalg] test_ops.py::test_python_ref_meta__refs_linalg_svd_cpu_complex failing triaged module: linear algebra module: meta tensors,triaged module: linear algebra module: meta tensors
test_view_dynamic_zero_dim no longer testing zero input module: onnx triaged,module: onnx triaged
[feature request] make the input k in rot90 a list of int to rotate tensors individually in a batch feature triaged module: python frontend,feature triaged module: python frontend
extra information messages for mac in setup.py would help.  module: build module: docs triaged,module: build module: docs triaged
Support Delay Loading of c10.dll in when using libtorch as a thirdparty library. module: windows module: abi triaged,module: windows module: abi triaged
Multiple dimensions support for `torch.max` feature triaged module: numpy needs design module: python frontend,feature triaged module: numpy needs design module: python frontend
"`assert has_same_metadata(inpt_new, inpt_old)` fails when capturing forwards + backwards in train_step with resnet18 triaged oncall: pt2 module: functorch module: pt2-dispatcher",triaged oncall: pt2 module: functorch module: pt2-dispatcher
NotImplementedError: Could not run 'aten::_spdiags' with arguments from the 'CUDA' backend. module: sparse module: cuda triaged,module: sparse module: cuda triaged
Saving a LightningModule torch.jit.ScriptModule is incompatible with torch.amp.autocast oncall: jit has workaround module: amp (automated mixed precision),oncall: jit has workaround module: amp (automated mixed precision)
[torch.compile] RuntimeError during Gradient Computation in torch.compile() triaged has workaround bug oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,triaged has workaround bug oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher
torch version compare triaged module: python frontend,triaged module: python frontend
Unnecessary record_stream call for backend:cudaMallocAsync module: cuda module: logging triaged,module: cuda module: logging triaged
Refactor Adam and AdamW by abstracting out common code module: optimizer triaged better-engineering actionable,module: optimizer triaged better-engineering actionable
[dynamo][higher_order_op] assert in check_kwargs leads to error instead of graph-break good first issue triaged oncall: pt2 module: dynamo,good first issue triaged oncall: pt2 module: dynamo
torch/testing/_comparison.py: If you are a user and see this message during normal operation please file an issue triaged module: testing,triaged module: testing
errors in CONTRIBUTING.md module: docs triaged,module: docs triaged
Conversion of a CSR tensor with batches to COO tensor fails module: sparse triaged,module: sparse triaged
rfftn and irfftn operations in pt2 return different results compared to v1.12.1 module: cuda triaged module: third_party module: fft,module: cuda triaged module: third_party module: fft
torch.nn.Conv2d's padding mode circular cannot accept 3-dim input module: nn triaged actionable,module: nn triaged actionable
Torch's `LayerNorm` and Adam optimizer vs those in tensorflow needs reproduction module: numerical-stability module: nn module: optimizer triaged,needs reproduction module: numerical-stability module: nn module: optimizer triaged
torch.norm inconsistency? module: numerical-stability triaged module: norms and normalization,module: numerical-stability triaged module: norms and normalization
Implement `diag` method for sparse COO tensors module: sparse triaged,module: sparse triaged
"MPS matmul with sliced (strided) out argument produces wrong output, may corrupt memory triaged module: mps",triaged module: mps
Unrelated error messages with torch.nn.AdaptiveAvgPool3d module: nn triaged module: pooling,module: nn triaged module: pooling
torch.func.jvp fails with BERT training module: autograd triaged actionable module: forward ad module: functorch,module: autograd triaged actionable module: forward ad module: functorch
[RFC] Let in-place foreach functions return a list of Tensors triaged module: mta,triaged module: mta
[compile][dynamic] dsplit is seeing a list of mixed ints and symints triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
PyTorch built with CuDNN-8.8.1 crashes if CuDNN-8.9.2 is installed on the system module: cudnn module: ci triaged,module: cudnn module: ci triaged
"inductor/triton fails on `view(..., dtype=torch.int16)` triaged bug oncall: pt2 module: inductor",triaged bug oncall: pt2 module: inductor
[BE] Evaluate and improve eager for-loop optimizer memory perf module: optimizer triaged better-engineering actionable,module: optimizer triaged better-engineering actionable
Use `isinstance` instead of `type` when checking for `torch.nn.Parameter` module: nn triaged,module: nn triaged
torch.nn.CrossEntropyLoss: class weighting changes label_smoothing module: nn module: loss triaged,module: nn module: loss triaged
Support for `eval` in functional_call module: nn triaged module: functional UX,module: nn triaged module: functional UX
"Torch Filename Storage hangs on ""file_system"" sharing strategy after in-place fill module: multiprocessing triaged module: mps",module: multiprocessing triaged module: mps
fsdp load model causing insufficient CPU memory triaged module: fsdp,triaged module: fsdp
Error reporting uses formal parameter names of downstream C++ function triaged module: multi-headed-attention,triaged module: multi-headed-attention
"torch.jit.trace says ""Arguments for call are invalid"" on torch.ops.aten.sub(3, x, alpha=3) oncall: jit",oncall: jit
torch.jit slicing error (styleganv2) oncall: jit,oncall: jit
generate_vmap_rule=True sometimes gives batched grad_output triaged module: functorch,triaged module: functorch
System memory leak when using different input size of torch.nn.Conv3d module: cudnn module: nn module: cuda module: memory usage triaged,module: cudnn module: nn module: cuda module: memory usage triaged
Incorrect Error Message Ordering for nn.AdaptiveAvgPool2d with Incorrect output_size  module: nn triaged,module: nn triaged
LSTM built-in dropout not reproducible on GPU module: cudnn module: nn triaged module: random,module: cudnn module: nn triaged module: random
[compile] DDPOptimizer + activation checkpointing not supported module: checkpoint triaged module: ddp oncall: pt2 module: distributed,module: checkpoint triaged module: ddp oncall: pt2 module: distributed
`torch.distributed.rpc.backend_registry.register_backend` fails to update `BackendType` enum oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
"Failure in optimize_for_mobile when using conv1d(..., padding='same') triaged oncall: mobile",triaged oncall: mobile
"F.adaptive_avg_pool3d(input, 1) returns infinity in half precision module: numerical-stability module: nn module: cpu triaged",module: numerical-stability module: nn module: cpu triaged
ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. needs reproduction module: binaries triaged,needs reproduction module: binaries triaged
Bug in Conv/BN fuser with torch.fx triaged module: fx oncall: fx,triaged module: fx oncall: fx
version libcudnn_ops_infer.so.8 not defined in file libcudnn_ops_infer.so.8 with link time reference module: cuda triaged,module: cuda triaged
Issue with FSDP does not reduce memory footprint  when scaling up GPUs oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Conv1d step-by-step numerical error  needs reproduction triaged module: macos module: determinism module: arm,needs reproduction triaged module: macos module: determinism module: arm
Init_rpc() errors when running the test code in the TorchPRC document on two different machines  triaged module: rpc,triaged module: rpc
vec_test_all_types_xxx with dtype c10::complex<float> and c10::complex<double> has failures on division module: cpu triaged module: complex module: vectorization,module: cpu triaged module: complex module: vectorization
"Using the latest version of Torch, when the code executes tcpstore, there is no response oncall: distributed triaged",oncall: distributed triaged
TImeout in NCCL doesn't work oncall: distributed triaged,oncall: distributed triaged
Wrong functionalization of as_strided leads to wrong results high priority module: docs triaged oncall: pt2,high priority module: docs triaged oncall: pt2
Get errors after compiling and running PyTorch MINIMAL EXAMPLE for c++ Mac M1 with make module: binaries module: cpp triaged module: macos module: arm,module: binaries module: cpp triaged module: macos module: arm
FSDP Optimizer Overlap - follow ups oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Investigate numerical stability of forward-mode AD of some foreach functions triaged module: forward ad,triaged module: forward ad
`torch.view_as_real(tensor)` should return `nn.identity(tensor)` if its not complex instead of raising an error triaged module: complex,triaged module: complex
"Numpy/scipy module works fine with Torch modules, but not TorchScript. How to torchscript a numpy/scipy module? oncall: jit",oncall: jit
LibTorch 2.0.1 scripting in Debug mode on Windows module: windows module: cpp triaged,module: windows module: cpp triaged
Support CUDA 12.2  module: cuda triaged,module: cuda triaged
"RuntimeError: t == DeviceType::CUDA INTERNAL ASSERT FAILED at HIPGuardImplMasqueradingAsCUDA.h:60, please report a bug to PyTorch module: rocm triaged",module: rocm triaged
Detailed error: Tensor-likes are not close! When use torch.jit.trace oncall: jit,oncall: jit
Inconsistencies in ONNX exporting of operation `torch.full()` module: onnx low priority triaged OSS contribution wanted,module: onnx low priority triaged OSS contribution wanted
FakeTensor can't handle meta impls that perform device conversion triaged module: fakeTensor,triaged module: fakeTensor
ReduceLROnPlateau will throw IndexError: list index out of range with modified optimizer's param_groups. module: optimizer triaged actionable module: LrScheduler,module: optimizer triaged actionable module: LrScheduler
"Segmentation error while using F.cross_entropy with mps(for code that works fine with device= ""cpu"") triaged module: mps",triaged module: mps
Illegal Memory Access on H100 `TestSparseCompressedTritonKernelsCUDA.test_triton_sampled_addmm_block_size_16_cuda_bfloat16` module: sparse module: cuda triaged,module: sparse module: cuda triaged
Torch randperm with device mps does not sample exactly uniformly from all possible permutations triaged module: random module: mps,triaged module: random module: mps
Attempt to use minifier on sam model fails triaged oncall: pt2 module: dynamic shapes module: minifier,triaged oncall: pt2 module: dynamic shapes module: minifier
affine_grid and grid_sample operators merge/accelleration module: performance feature triaged,module: performance feature triaged
`F.conv1d` and `F.conv2d` propagate `nan`'s incorrectly when minibatch > 15 needs reproduction module: nn triaged module: macos module: NaNs and Infs module: arm,needs reproduction module: nn triaged module: macos module: NaNs and Infs module: arm
ImportError: libcudnn.so.8: cannot open shared object file: No such file or directory needs reproduction module: binaries module: cuda triaged,needs reproduction module: binaries module: cuda triaged
[FSDP] `ignored_states` is broken with auto wrap triaged module: fsdp,triaged module: fsdp
[RFC] Make `_HYBRID_SHARD_ZERO2` public as `HYBRID_SHARD_GRAD_OP` triaged module: fsdp,triaged module: fsdp
"[proposal] ""Name"" string attribute for modules, parameters, buffers, tensors for more pleasant debugging (especially for graph printouts / export / studying compiled generated code) feature triaged needs design",feature triaged needs design
Torchscript with dynamic quantization produces inconsistent model outputs oncall: jit oncall: quantization low priority triaged,oncall: jit oncall: quantization low priority triaged
View ops on fake tensors can dispatch `detach`es to backend kernels triaged module: fakeTensor,triaged module: fakeTensor
Conversion from strided to batched sparse compressed tensor with a non-constant number of zeros in batches fails module: sparse triaged,module: sparse triaged
torch.embedding: Trying to convert BFloat16 to the MPS backend but it does not have support for that dtype. triaged enhancement module: bfloat16 module: mps,triaged enhancement module: bfloat16 module: mps
Add memory managemenet information for Apple silicon mps backend triaged enhancement module: mps,triaged enhancement module: mps
No document for parameter `load_debug_files` in `torch::jit::load` in C++ API module: docs triaged actionable,module: docs triaged actionable
Incorrect Reduce collective result with `_coalescing_manager`  oncall: distributed,oncall: distributed
Nested Tensor with PyG dataset custom class triaged module: nestedtensor actionable,triaged module: nestedtensor actionable
"Network does not return any thing, not even None and breaks loops needs reproduction module: windows triaged module: third_party",needs reproduction module: windows triaged module: third_party
Numbers bigger than the range should be inf while the implementation just keeps the original. module: docs triaged module: NaNs and Infs,module: docs triaged module: NaNs and Infs
Error 101: invalid device ordinal (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.) module: cuda triaged module: third_party,module: cuda triaged module: third_party
PyTorch2.0 ROCM LayerNorm HIP error: invalid configuration module: rocm triaged,module: rocm triaged
Python Crashes When Importing Torch With C API needs reproduction module: windows triaged module: third_party,needs reproduction module: windows triaged module: third_party
Re-enable `test_typing` module: typing module: ci triaged,module: typing module: ci triaged
Documentation building fails due to torchgen module: build module: docs triaged actionable,module: build module: docs triaged actionable
Tensor to_sparse fails on large matrices module: sparse module: cuda triaged,module: sparse module: cuda triaged
batch size unexpectedly affects model inference on Mac M1 triaged module: mps,triaged module: mps
Inductor does not check input SymInt invariant on GraphModules passed in good first issue triaged oncall: pt2 module: dynamic shapes module: inductor,good first issue triaged oncall: pt2 module: dynamic shapes module: inductor
(Possible) Memory leak on deleting a compiled model high priority triaged has workaround module: cuda graphs oncall: pt2,high priority triaged has workaround module: cuda graphs oncall: pt2
RuntimeError: _ivalue_ INTERNAL ASSERT FAILED oncall: jit,oncall: jit
Regressions with torch.compile + amp + ddp with recent nightly builds needs reproduction oncall: distributed module: cuda triaged module: ddp oncall: pt2 module: distributed,needs reproduction oncall: distributed module: cuda triaged module: ddp oncall: pt2 module: distributed
Tracking issue for optimizer graph not being an inference graph triaged tracker oncall: pt2 module: aotdispatch module: pt2-dispatcher,triaged tracker oncall: pt2 module: aotdispatch module: pt2-dispatcher
[torch.compile] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method normal of type object at ***: got an unexpected keyword argument 'device' triaged module: random oncall: pt2 module: decompositions,triaged module: random oncall: pt2 module: decompositions
Torch.compile tutorial shows incorrect triton kernel triaged topic: docs,triaged topic: docs
Scripted model is loaded on GPU but the inference seems to utilize the CPU with zero GPU utilization oncall: jit,oncall: jit
Improved error checking for custom Function when saving intermediates module: double backwards module: autograd triaged actionable,module: double backwards module: autograd triaged actionable
Reproducibility documentation should be updated module: docs triaged module: numerical-reproducibility,module: docs triaged module: numerical-reproducibility
SummaryWriter.add_embedding not working for RGBA images triaged enhancement module: tensorboard,triaged enhancement module: tensorboard
[torch.compile] `permute_linear_fusion` ignores the inplace operation for the tensor high priority triaged module: correctness (silent) oncall: pt2 module: aotdispatch module: inductor module: pt2-dispatcher,high priority triaged module: correctness (silent) oncall: pt2 module: aotdispatch module: inductor module: pt2-dispatcher
Several Torchbench models don't run with float16 or bfloat16 in the inference eager mode triaged module: bfloat16 module: half module: benchmark oncall: pt2,triaged module: bfloat16 module: half module: benchmark oncall: pt2
'MPS' Issue Running HuggingFace Transformer Pix2Struct Model triaged module: mps,triaged module: mps
How to unwrap after auto_wrap in FSDP? oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
CODEOWNERS file has errors due to non existent people being referred to module: docs triaged,module: docs triaged
"Need the full ""Release Compatibility Matrix"" of torch module: binaries oncall: releng triaged",module: binaries oncall: releng triaged
torch.save() fails if path contains multibyte characters module: serialization triaged,module: serialization triaged
"[torch.fx] Deserialization Error - TypeError: ones() received an invalid combination of arguments - got (tuple, device=Attribute)  high priority triage review module: fx oncall: fx",high priority triage review module: fx oncall: fx
Issue with loading similar checkpoints in a distributed fashion  module: serialization triaged,module: serialization triaged
Runtime Error outerNode->outputs().size() == node->inputs().size() INTERNAL ASSERT FAILED when exporting custom operator module: onnx triaged,module: onnx triaged
Can ``torch.vmap`` add ``grad_fn``= SelectBackward when maping over some dimension of the inputs? triaged actionable module: functorch,triaged actionable module: functorch
"row.device().is_cpu() INTERNAL ASSERT FAILED at ""csrc/cpu/diag_cpu.cpp"":7 triaged module: macos",triaged module: macos
"Long PR description leads to ""Argument list too long"" error from docker module: ci triaged module: devx",module: ci triaged module: devx
Segmentation fault when tensorrt is imported before torch needs reproduction module: build triaged has workaround,needs reproduction module: build triaged has workaround
torch compile aten::floor_divide error triaged oncall: pt2,triaged oncall: pt2
Some parameters are missing type descriptions module: docs triaged actionable,module: docs triaged actionable
"The document style is inconsistent with other documents, and the parameter type is not clearly highlight module: docs triaged actionable",module: docs triaged actionable
[question] [docs] Short/mid/long-term status of TorchScript / JIT / torch.jit.trace / FX / symbolic tracing and its replacement by Dynamo oncall: jit module: docs triaged,oncall: jit module: docs triaged
type conflict module: cpp triaged,module: cpp triaged
Torch 1.13 for GPU breaks if libcublas is already present. triaged module: docker,triaged module: docker
RPC Framework support custom backend oncall: distributed triaged,oncall: distributed triaged
Upgrading SpGEMM algorithm to resolve Cusparse SpGEMM insufficient resources problem module: sparse module: cuda triaged,module: sparse module: cuda triaged
"abnormal behavior in function ""scatter"" triaged module: python frontend",triaged module: python frontend
Error when building with USE_TENSORRT=1 module: onnx module: build triaged,module: onnx module: build triaged
Support `Sequence` type in JIT oncall: jit module: typing triaged enhancement,oncall: jit module: typing triaged enhancement
Eager PTDQ Performs Worse Than Non-Quantized Linear Layer on CPU(in Terms of Speed) triaged,triaged
Mis-annotated return for `F._no_grad_embedding_renorm_` (also JIT related) oncall: jit module: typing triaged bug,oncall: jit module: typing triaged bug
Type misalignments in `nn.functional` (also JIT related) oncall: jit module: typing triaged,oncall: jit module: typing triaged
"[Torch Mlir] avg_pool1d function padding init value should be (0,) triage review oncall: jit",triage review oncall: jit
Generate complete annotations for `torch._C._nn` module: typing triaged enhancement release notes: devx,module: typing triaged enhancement release notes: devx
Merge type stubs for `torch.nn.functional` module: typing triaged,module: typing triaged
dlrm and hf_T5_generate fails aot_eager with bfloat16+dynamic_shapes triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
libtorch > 1.9.1 produces segfault on Qt5 gui application exit high priority module: crash module: cpp triaged,high priority module: crash module: cpp triaged
Pytorch not calling to C code from a docker container needs reproduction triaged module: docker module: __torch_function__,needs reproduction triaged module: docker module: __torch_function__
SDPA produces NaN with padding mask triage review triaged oncall: transformer/mha,triage review triaged oncall: transformer/mha
[FSDP] train throughput become slow down when loaded shard optimizer dict triaged module: fsdp,triaged module: fsdp
[FSDP] save model checkpoint with StateDictType.LOCAL_STATE_DICT and LocalStateDictConfig(offload_to_cpu=True) fail triaged module: fsdp,triaged module: fsdp
torch.compile() bug in AOTAutograd or Dynamo triaged oncall: pt2,triaged oncall: pt2
DataParallel interfering with TorchDispatchMode high priority oncall: distributed triaged module: data parallel module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,high priority oncall: distributed triaged module: data parallel module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher
fairseq distributed training dumps core with flash attention triaged oncall: transformer/mha,triaged oncall: transformer/mha
(fsdp) Support for accessing unsharded parameters for methods other than `forward()` triaged module: fsdp,triaged module: fsdp
Exported model with dropout incorrectly applies dropout during eval module: nn triaged module: correctness (silent) oncall: pt2 oncall: export,module: nn triaged module: correctness (silent) oncall: pt2 oncall: export
detectron2_fcos_r_50_fpn and other models have enough graph breaks that we end up with multiple cache entries on module blocks triaged module: dynamic shapes,triaged module: dynamic shapes
"""Y.getIntrusivePtr()->set_storage(X.getIntrusivePtr()->storage()); "" in C++ is not supported module: cpp triaged",module: cpp triaged
MultiheadAttention should split embed_dim into four parameters triaged oncall: transformer/mha,triaged oncall: transformer/mha
"""addmm_out_sparse_csr_impl_mkl"" not implemented for 'Byte' module: sparse triaged",module: sparse triaged
Disclose C++ ATen ops type promotion rules under OpOverload in Python module: docs triaged,module: docs triaged
ARM based GPU support for Distributed Data Parallelism Module oncall: distributed,oncall: distributed
Installing Torch on AMD Platform Leads to Huge Docker Image module: rocm triaged,module: rocm triaged
test_fstrings2 fails with dynamic good first issue triaged module: dynamic shapes,good first issue triaged module: dynamic shapes
`interpolate` with `antialias=True` on CUDA doesn't work if the difference of spatial size is large module: cuda triaged module: interpolation,module: cuda triaged module: interpolation
LSTM/RNN operation agnostic module: nn triaged needs research,module: nn triaged needs research
torch.cuda.mem_get_info to return 0 if CUDA context isn't initialized module: cuda triaged actionable,module: cuda triaged actionable
Passing dict in datapipe/dataset will have memory leak problem triaged module: data,triaged module: data
"ImportError: undefined symbol: cublasSetWorkspace_v2, version libcublas.so.11 module: binaries module: cuda triaged",module: binaries module: cuda triaged
[ONNX] Support aten::mT module: onnx low priority triaged OSS contribution wanted,module: onnx low priority triaged OSS contribution wanted
File Missing When i build with C++ module: cpp triaged,module: cpp triaged
torch.fx.passes.split_module.split_module doesn't support dynamic shapes good first issue triaged module: dynamic shapes,good first issue triaged module: dynamic shapes
`gradcheck` produces false positives with sparse inputs when `masked=False`. module: sparse module: autograd triaged,module: sparse module: autograd triaged
"[functorch] [FakeTensorMode, meta tensor] + aot_autograd Bug. triaged oncall: pt2 module: fakeTensor module: aotdispatch module: pt2-dispatcher",triaged oncall: pt2 module: fakeTensor module: aotdispatch module: pt2-dispatcher
CUBLAS_WORKSPACE_CONFIG can not be parsed triaged module: cublas,triaged module: cublas
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6047) of binary: /home/win10-ubuntu/anaconda3/envs/vicuna-7b/bin/python oncall: distributed triaged,oncall: distributed triaged
No backward implementation for `torch._native_multi_head_attention` triaged module: multi-headed-attention,triaged module: multi-headed-attention
Document CI retry rules triaged module: devx,triaged module: devx
Error encountered when tracing model with Dynamo/Functorch for export with trilinear interpolation triaged oncall: pt2 module: dynamic shapes oncall: export,triaged oncall: pt2 module: dynamic shapes oncall: export
Memory efficient SDP yields wrong gradients triaged module: multi-headed-attention,triaged module: multi-headed-attention
Asynchronous CUDA AveragedModel module: optimizer triaged needs research,module: optimizer triaged needs research
Deprecation warning on lr_scheduler.step(num_steps) module: optimizer triaged actionable,module: optimizer triaged actionable
test_generate_tensor_from_list_of_numpy_primitive_type fails if run under pytest triaged module: dynamic shapes,triaged module: dynamic shapes
The document does not emphasize Illegal value in nn.Bilinear module: nn triaged actionable module: edge cases,module: nn triaged actionable module: edge cases
Possible memory leak when using Torch and Torchvision in conjunction with XGBoost  module: memory usage triaged module: vision,module: memory usage triaged module: vision
"Torch  model compile error  ""/usr/bin/ld: cannot find -lcuda""  though cuda is installed via run file triaged oncall: pt2 upstream triton",triaged oncall: pt2 upstream triton
LayerNorm freeze processes using torch multiprocessing module: multiprocessing triaged,module: multiprocessing triaged
Typing missing on arithmetic ops on `Tensor` module: typing triaged,module: typing triaged
NotImplementedError Could not run 'c10d::alltoall_' with arguments from the 'Meta' backend.  triaged,triaged
Inplace binary ops on tensor subclasses can cause mypy error module: typing triaged,module: typing triaged
ImportError: cannot import name 'Store' from 'torch.distributed' oncall: distributed triaged,oncall: distributed triaged
torchgen/gen_backend_stubs.py compatibility with DispatchStubs triaged module: dispatch module: codegen module: structured kernels,triaged module: dispatch module: codegen module: structured kernels
test_workspace_allocation_error fails on my local devgpu triaged module: cuda graphs,triaged module: cuda graphs
RuntimeError: CUDA error: unknown error oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Libtorch compile error when defining D_GLIBCXX_DEBUG module: build module: abi triaged,module: build module: abi triaged
Add a requirements.txt for windows pip packages triaged module: devx,triaged module: devx
"[feature request] Native method for iterating Python items of tensors: `iteritems()` and a new `tensor.item(i, j, k, ...)` method feature triaged module: python frontend",feature triaged module: python frontend
mps and cpu give far different results when training a transformer. triaged module: mps,triaged module: mps
Improve `_group_tensors_by_device_and_dtype` module: optimizer triaged better-engineering actionable module: mta,module: optimizer triaged better-engineering actionable module: mta
"RuntimeError: torch.vmap a function that includes in-place arithmetic operations on a zero-initialized tensor, an error ""vmap: inplace arithmetic(self, *extra_args) is not possible"" is raised. triaged module: functorch",triaged module: functorch
binary_cross_entropy (loss) seems to be giving incorrect values for very negative logits module: nn triaged,module: nn triaged
Fast kernels for low rank matrix multiplication triaged module: linear algebra,triaged module: linear algebra
setup.py fails to pass USE_ROCM to CAFFE2 build  module: rocm triaged,module: rocm triaged
DTensor uneven sharding corner cases. oncall: distributed triaged,oncall: distributed triaged
Dynamo trouble shooting dead link good first issue triaged topic: docs oncall: pt2 module: dynamo,good first issue triaged topic: docs oncall: pt2 module: dynamo
oneDNN kernel fails to compile triaged module: mkldnn,triaged module: mkldnn
"Warn / deprecate / remove ProcessGroupNCCL._group_start(), _group_end() APIs oncall: distributed triaged",oncall: distributed triaged
Unexpected High PCIe traffic in Distributed Training since PT 2 oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
torch.jit.script mean(keepdim=True) segfaults on GPU oncall: jit,oncall: jit
torch.cuda.memory_reserved always returns 0 bytes module: cuda triaged,module: cuda triaged
compilation fails `error: invalid argument '-std=c++17' not allowed with 'C'` module: build triaged,module: build triaged
PyTorch can not be compiled with MKLDNN if system compiler is clang module: build triaged module: mkldnn,module: build triaged module: mkldnn
[inductor] test_fft_real_inputs fails with dynamic shapes good first issue triaged oncall: pt2 module: dynamic shapes module: inductor,good first issue triaged oncall: pt2 module: dynamic shapes module: inductor
(fsdp - maybe a bug) SHARDED_STATE_DICT returns tensor with no data triaged module: fsdp,triaged module: fsdp
Calling jacrev with LSTM and functional_call  gives error triaged module: functorch,triaged module: functorch
Allow overriding __repr__ to call dataclass_repr (infinite recursion right now) triaged better-engineering module: codegen,triaged better-engineering module: codegen
Build fails at linking torch_shm_manager on aarch64 module: build triaged,module: build triaged
"error: â€˜aligned_allocâ€™ was not declared in this scope        static_cast<char*>(aligned_alloc(FLATBUFFERS_MAX_ALIGNMENT, size)), free); module: build triaged",module: build triaged
Observed regress in DataLoader spawn from PyTorch1.13 to PyTorch2.0 high priority module: performance triaged module: regression module: data,high priority module: performance triaged module: regression module: data
Turn on Inductor Max Pool2d Backward Lowering For Channels Last feature good first issue triaged oncall: pt2 module: inductor,feature good first issue triaged oncall: pt2 module: inductor
Increased / more verbose type aliases for improved readability of user defined content module: typing triaged enhancement needs research,module: typing triaged enhancement needs research
PyTorch should not use `windows.8xlarge.nvidia.gpu` to test binary builds module: ci triaged,module: ci triaged
Refactor mm_plus_mm to check conditions upfront feature good first issue triaged oncall: pt2 module: inductor,feature good first issue triaged oncall: pt2 module: inductor
Inconsistent memory allocation using FSDP between PT 2.0 and Nightlies high priority triage review oncall: distributed triaged module: fsdp,high priority triage review oncall: distributed triaged module: fsdp
torch.compile crash for tensor computing when tensor size is bigger  needs reproduction triaged oncall: pt2,needs reproduction triaged oncall: pt2
Unexpected failure in LLVM JIT when running TorchScript model in C++ oncall: jit,oncall: jit
Symbolic trace error about torch.nn.functional.pad triaged module: fx,triaged module: fx
[Pytorch 2.0] torch::nn::Dropout output is incorrect on Windows   module: binaries module: windows module: cpp triaged module: regression,module: binaries module: windows module: cpp triaged module: regression
"lit-llama lora fine tuning NetworkXUnbounded: Infinite capacity path, flow unbounded above triaged oncall: pt2 module: dynamic shapes",triaged oncall: pt2 module: dynamic shapes
MPS bug: padding_idx in nn.Embedding does not prevent gradient accumulation  triaged module: mps,triaged module: mps
Preserve weight_g/weight_v accessors on new weight_norm module: nn triaged module: nn.utils.parametrize,module: nn triaged module: nn.utils.parametrize
raise `RuntimeError` faster when loading an object with a torch CUDA tensor on a CPU-only machine module: nn module: serialization triaged actionable,module: nn module: serialization triaged actionable
Discussion and Design  for Masked Loss Functions which can be used with PackedSequence training (but not exclusively) module: loss triaged module: masked operators,module: loss triaged module: masked operators
"how to workaround the error ""don't have an op for vulkan_prepack::create_linear_context"" ? module: build triaged module: vulkan ciflow/periodic",module: build triaged module: vulkan ciflow/periodic
torch.svd fails on large matrices module: cuda triaged module: cublas module: linear algebra,module: cuda triaged module: cublas module: linear algebra
TypeError: (): incompatible function arguments oncall: distributed triaged,oncall: distributed triaged
"torch.onnx.export error ------RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select) module: onnx triaged",module: onnx triaged
Support for efficiently processing categorical distributions with varying dimensions module: distributions feature triaged needs research,module: distributions feature triaged needs research
torch.cuda.is_available() returns False on GTX 1650 with cuda 11.7 and torch==2.0.0+cpu module: build triaged,module: build triaged
Unbox expectedml triaged,triaged
PackedSequences on MPS accelerator yields `grad_y` missing or crashes the kernel. triaged module: mps,triaged module: mps
nn.ChannelShuffle1d module: nn triaged needs research,module: nn triaged needs research
Unable to checkpoint model and optimizer state when using Hybrid Sharding Strategy high priority oncall: distributed triaged module: fsdp module: distributed_checkpoint,high priority oncall: distributed triaged module: fsdp module: distributed_checkpoint
"BCELoss and BCEWithLogitsLoss differ when one of the input logits is float(""inf"") module: nn triaged module: edge cases",module: nn triaged module: edge cases
Inductor: delete code that extracts out sizevars by inspecting tensor inputs to find a size that handled it triaged better-engineering module: inductor,triaged better-engineering module: inductor
LibTorch-Lite 1.13.0.1 Crash on iOS 12 on app startup triaged module: ios,triaged module: ios
Unknow error when using `make_graphed_callables` triaged module: cuda graphs,triaged module: cuda graphs
"Unable to resume job using FSDP with 64 nodes, errors appeared during loading sharded optimizer state dict  triaged module: fsdp",triaged module: fsdp
mark_dynamic may error too aggressively triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
[DTensor] Error in distribute_module with module._apply oncall: distributed triaged,oncall: distributed triaged
`torch.poisson(torch.tensor([torch.inf))` returns 0 module: distributions module: error checking triaged module: edge cases,module: distributions module: error checking triaged module: edge cases
Support In-place Triangular Matrix Multiplication triaged enhancement module: linear algebra,triaged enhancement module: linear algebra
Followup on the extra graph breaks for yolov3 model caused by layout optimization triaged module: inductor,triaged module: inductor
Pytorch Build images for RISCV64 Devices in the nightly builds module: build feature triaged,module: build feature triaged
Error: no matching constructor for initialization of 'at::OptionalIntArrayRef' module: build triaged module: macos,module: build triaged module: macos
parameterizations.orthogonal does not work as intended with nn.GRU or nn.LSTM module: rnn triaged module: nn.utils.parametrize,module: rnn triaged module: nn.utils.parametrize
Building NCCL with `make -l $MAX_JOBS` slows down builds module: build triaged,module: build triaged
"[FSDP]  When amp is enabled, there is a noticeable difference during training between `FSDP `and `DDP` triaged module: fsdp",triaged module: fsdp
Best practices clarification for initialization strategies module: nn triaged module: initialization needs research module: python frontend,module: nn triaged module: initialization needs research module: python frontend
Fix dynamo-related debug Python 3.11 failures triaged bug release notes: dynamo,triaged bug release notes: dynamo
Duplicate parameters (_flat_params and original params) in the state_dict when using `use_orig_params=True` and `StateDictType.LOCAL_STATE_DICT` triaged module: fsdp,triaged module: fsdp
Test test_vjp_nn_functional_scaled_dot_product_attention_cuda_float32 fails with `query: last dimension must be contiguous` on H100 triaged oncall: transformer/mha module: functorch,triaged oncall: transformer/mha module: functorch
"Error when exporting to onnx for albert-base-v2, issue with attention_mask module: onnx triaged",module: onnx triaged
Cannot invoke prims.sum with output_dtype triaged module: primTorch,triaged module: primTorch
[prims] torch.ops.aten.le decomposition confuses scalars and tensors triaged module: primTorch,triaged module: primTorch
Support for activation checkpoint on demand in custom function module: checkpoint triaged,module: checkpoint triaged
Mergebot should merge non-stacked PR triaged enhancement module: devx,triaged enhancement module: devx
test_functional_autograd_benchmark.py::TestFunctionalAutogradBenchmark::test_fast_tasks passes with all NaNs module: autograd triaged module: functorch,module: autograd triaged module: functorch
[RFC] Add third-party malloc library to improve pytorch memory performance on Windows module: performance module: windows module: cpu triaged intel,module: performance module: windows module: cpu triaged intel
Segfault when running vulkan program linked against libtorch triaged module: vulkan ciflow/periodic,triaged module: vulkan ciflow/periodic
Faster BatchSampler with big batch size module: dataloader triaged,module: dataloader triaged
[Utils][tensorboard]Enhancement: Include 'max_outputs' parameter in torch.utils.tensorboard.summary's 'image' method triaged module: tensorboard,triaged module: tensorboard
Matrix multiplication performance regression in case of an additional dimension of size 1 module: dependency bug triaged module: cublas,module: dependency bug triaged module: cublas
Batching rule for `aten::_scaled_dot_product_efficient_attention` triaged module: vmap,triaged module: vmap
RuntimeError using torch.nn.functional.pad when using MPS triaged module: mps,triaged module: mps
"Add additional ""sigmoid"" approximation to GeLu activation? module: nn triaged enhancement needs research",module: nn triaged enhancement needs research
discuss.pytorch.org signup issue module: docs triaged,module: docs triaged
multiple mps for base X86 Mac with multiples gpus triaged module: mps,triaged module: mps
torch.distributed.all_reduce() has inconsistent behavior oncall: distributed triaged,oncall: distributed triaged
_view_func but without keeping original view tensor alive module: autograd triaged needs design module: viewing and reshaping,module: autograd triaged needs design module: viewing and reshaping
Unexpected Behavior when using torch.isclose() triaged module: NaNs and Infs module: testing,triaged module: NaNs and Infs module: testing
Hooks not working in version 2.0.1+cu118 module: nn triaged,module: nn triaged
"[cuda] Switching CI to CUDA 12.1 timing out linux-bionic-cuda12.1-py3.10-gcc7 / test (distributed, 3, 3, linux.8xlarge.nvidia.gpu) module: cuda module: ci triaged",module: cuda module: ci triaged
Issue with ShufflerIterDataPipe in torch 1.13.1 module: dataloader triaged module: data,module: dataloader triaged module: data
[Dynamo] Better graph-break message for unsupported ctx managers good first issue triaged oncall: pt2 module: dynamo,good first issue triaged oncall: pt2 module: dynamo
Tensors that share same underlying storage to also share gradient storage module: autograd triaged needs research module: python frontend,module: autograd triaged needs research module: python frontend
There is a memory leak in torch.load module: memory usage module: serialization triaged,module: memory usage module: serialization triaged
"transformer encoder-layer, the sample-Independent attn_mask(dim=3) has different behaviors when training and validating triaged oncall: transformer/mha",triaged oncall: transformer/mha
"after add /path_to_libtorch/libtorch/lib to LD_LIBRARY_PATH, I can't import torch_scatter. module: build module: cpp module: abi triaged",module: build module: cpp module: abi triaged
Import of torch breaks standard multiprocessing module: dependency bug module: multiprocessing triaged module: openmp,module: dependency bug module: multiprocessing triaged module: openmp
ExponentialLR unexpectedly calls `step()` when init argument `last_epoch` is larger than -1 module: optimizer triaged actionable module: LrScheduler,module: optimizer triaged actionable module: LrScheduler
skipIfTorchInductor Tracking Issue  good first issue triaged oncall: pt2 module: inductor,good first issue triaged oncall: pt2 module: inductor
aot_export_joint_simple on plain callable (not graph module) doesn't attach stack traces triaged better-engineering oncall: pt2 module: aotdispatch module: pt2-dispatcher,triaged better-engineering oncall: pt2 module: aotdispatch module: pt2-dispatcher
scipy.ndimage.find_objects feature module: nn triaged needs research,feature module: nn triaged needs research
torch.func.jvp fails when acting on a DistributedDataParallel model oncall: distributed module: forward ad module: functorch,oncall: distributed module: forward ad module: functorch
Extend fake fast path to more situations triaged module: fakeTensor,triaged module: fakeTensor
Calling pin_memory() fails for nested tensor  triaged module: nestedtensor topic: new features,triaged module: nestedtensor topic: new features
NotImplementedError in backprop on on dense-sparse matrices module: sparse triaged,module: sparse triaged
Pytorch CXX11 ABI version module: build module: abi triaged,module: build module: abi triaged
Enable DEBUG asserts for C++ builds triaged module: devx,triaged module: devx
BatchNorm can't be symbolically traced with torch.fx as a standalone module  triaged module: functorch,triaged module: functorch
Documentation Error of torch.onnx module: onnx module: docs triaged,module: onnx module: docs triaged
CPU Fallback does not convert Tensor?[] module: internals triaged,module: internals triaged
AddressSanitizer: heap-buffer-overflow in test_comprehensive_nn_functional_embedding_bag_cpu_bfloat16  needs reproduction module: cpu triaged module: embedding module: decompositions,needs reproduction module: cpu triaged module: embedding module: decompositions
Can't vmap over a slice expression triaged module: functorch,triaged module: functorch
torch.compile FakeTensor tracing fails with foreach ops with multiple devices module: optimizer triaged module: custom-operators release notes: foreach_frontend oncall: pt2 module: fakeTensor module: pt2-dispatcher,module: optimizer triaged module: custom-operators release notes: foreach_frontend oncall: pt2 module: fakeTensor module: pt2-dispatcher
[FSDP] Ensure full precision checkpoints oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
"Request for adding support for `torch.rand_like`, `torch.randn_like`, `torch.randint_like` with `torch.Generator` triaged module: random",triaged module: random
No pytorch_android 2.0.x builds triaged module: android oncall: mobile,triaged module: android oncall: mobile
CrossEntropyLoss output difference on Windows vs. Linux module: windows triaged,module: windows triaged
scaled_dot_product_attention produces NaN when input has NaN in masked-out positions triaged module: NaNs and Infs module: edge cases module: multi-headed-attention,triaged module: NaNs and Infs module: edge cases module: multi-headed-attention
torch.flip is inplaced too aggressively in torch inductor triaged bug oncall: pt2 module: inductor,triaged bug oncall: pt2 module: inductor
mps device bug - a weird inconsistency on tensor indexing operations triaged module: mps,triaged module: mps
Parameter gradient is not moved parameter is moved across devices module: docs module: nn triaged actionable module: correctness (silent),module: docs module: nn triaged actionable module: correctness (silent)
[feature request] [minor] Inplace torch.flip_ triaged enhancement module: python frontend,triaged enhancement module: python frontend
[compile] Tracker for `torchrec_dlrm` issues triaged module: custom-operators module: functionalization oncall: pt2 module: pt2-dispatcher,triaged module: custom-operators module: functionalization oncall: pt2 module: pt2-dispatcher
Can't reproduce/non-deterministic results with CUDA module: cuda triaged module: determinism,module: cuda triaged module: determinism
Crash on Python //  PyArrow //  needs reproduction triaged,needs reproduction triaged
torch.quantile on MPS doesn't sort values when dim is not None high priority triaged module: correctness (silent) module: mps,high priority triaged module: correctness (silent) module: mps
Can group convolution support other grouping methods? module: convolution triaged,module: convolution triaged
"Error, attribute exists on the Python module, but we failed to convert Python type: 'list' to a TorchScript type  oncall: jit module: onnx",oncall: jit module: onnx
Observing negative number in PyTorch profiling triaged oncall: profiler,triaged oncall: profiler
torch.jit.trace() Floating point exception oncall: jit,oncall: jit
Unexpected modification to CPU affinity of Dataloader workers module: multiprocessing triaged module: openmp,module: multiprocessing triaged module: openmp
pytorch-nightly not have torch/version.py.tpl:cuda specified module: binaries module: windows triaged,module: binaries module: windows triaged
Implement `to_numpy` method to speed up matplotlib with PyTorch arrays triaged enhancement module: numpy needs research,triaged enhancement module: numpy needs research
Add support for bfloat16 in torch.from_numpy() triaged module: numpy module: bfloat16,triaged module: numpy module: bfloat16
"[TorchScript] aten::__and__ with argument types: Tensor, bool not supported oncall: jit triaged",oncall: jit triaged
[refs] inplace references resize the input to match the broadcasted input shape triaged module: primTorch,triaged module: primTorch
Unexpected behavior of fmod op in some float32 input module: numerical-stability triaged,module: numerical-stability triaged
Unexpected behavior comparing uint8 tensor to value greater than 255 triaged module: type promotion module: edge cases,triaged module: type promotion module: edge cases
torch.profiler.profile has an empty python replay stack under certain circumstances triaged oncall: profiler,triaged oncall: profiler
multiple values for argument `softmax_scale` triaged module: fsdp oncall: pt2 module: distributed,triaged module: fsdp oncall: pt2 module: distributed
Unable to do tensor comparison on Metal Performance Shaders (MPS) triaged module: mps,triaged module: mps
"torch.cuda.set_device cannot use to set cpu device, but give an ambiguity hint triaged",triaged
torch.nn.functional.scaled_dot_product_attention() : support both attn_mask and is_causal triaged module: multi-headed-attention,triaged module: multi-headed-attention
Inconsistent performance degradation of 3x3 convolution (torch 2.0.1+cu118) module: performance module: cudnn module: cuda triaged module: regression,module: performance module: cudnn module: cuda triaged module: regression
Investigate random sequence number broadcast initially incorrect oncall: distributed triaged,oncall: distributed triaged
onnx runtime error module: onnx triaged,module: onnx triaged
[bazel] add inductor to bazel build module: build triaged module: bazel,module: build triaged module: bazel
Regression in NCCL error handling oncall: distributed,oncall: distributed
RuntimeError: Triton Error [CUDA]: device-side assert triggered when trying torch.compile max-autotune on nanoGPT triaged oncall: pt2,triaged oncall: pt2
Enhance FSDP debugability oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
not yet implemented the batching rule for torchaudio::_lfilter triaged module: batching module: functorch,triaged module: batching module: functorch
2D inputs to linear layers run up to 25% slower than 4D ones on some Nvidia GPUs triaged module: cublas,triaged module: cublas
import functorch.dim monkeypatches torch triaged module: functorch,triaged module: functorch
Delete old vmap prototype triaged better-engineering module: functorch,triaged better-engineering module: functorch
problem of compilation for torch2.0 needs reproduction module: build triaged module: third_party,needs reproduction module: build triaged module: third_party
torch.Tensor.is_sparse returns false for non-COO sparse tensors module: sparse triaged,module: sparse triaged
SparseAdam: working with dense parameters but sparse gradients - usecase  module: optimizer triaged actionable,module: optimizer triaged actionable
Theme update module: docs triaged module: doc infra,module: docs triaged module: doc infra
RuntimeError in Scaled Dot Product Attention Tutorial Code module: cpu triaged,module: cpu triaged
fsdp training with the seq2seqTranier module gets stuck during evaluation. triaged module: fsdp,triaged module: fsdp
Functions for Calculating Skewness and Kurtosis  feature triaged module: reductions,feature triaged module: reductions
Pytorch 2.1.0.dev20230512 cuda not available needs reproduction module: binaries module: cuda triaged,needs reproduction module: binaries module: cuda triaged
Speed when installing from source is very low with CUDA 11 needs reproduction module: performance module: build module: cuda triaged,needs reproduction module: performance module: build module: cuda triaged
Deprecated File bug low priority triaged topic: build,low priority triaged topic: build
Shared library loading logic breaks when CUDA packages are installed in a non-standard location triaged module: bazel topic: build bug,triaged module: bazel topic: build bug
Docs suggestion `FullyShardedDataParallel.summon_full_params` must be called on all ranks/processes triaged module: fsdp,triaged module: fsdp
Operations to shared tensors in the forked process could lead to silent crash module: multiprocessing triaged,module: multiprocessing triaged
Should be ok to call _dynamo.export and torch.compile under FakeTensorMode triaged module: dynamo,triaged module: dynamo
"Noisy warning - torch.fx.experimental.symbolic_shapes: [WARNING] Ignored guard (...), this could result in accuracy problems low priority triaged oncall: pt2 module: dynamic shapes",low priority triaged oncall: pt2 module: dynamic shapes
Mac m2 MPSNDArray.mm:78: failed assertion `[MPSNDArrayDescriptor sliceDimension:withSubrange:] error: dimension index (2) not within number of dimensions (2) 	Dimension indices are 0-based' triaged module: mps,triaged module: mps
`einsum` is about 40x slower on CUDA than manually multiplying and summing module: performance triaged module: linear algebra,module: performance triaged module: linear algebra
Tool for identifying where in eager model an operation is nondeterministic triaged module: determinism,triaged module: determinism
Different results with vmap when using torch.jit.script oncall: jit triaged module: functorch,oncall: jit triaged module: functorch
GLCM implementation in pytorch C++ api and cuda module: cpp triaged,module: cpp triaged
Migrate windows runners to non-ephemeral instances module: ci triaged,module: ci triaged
custom_op API follow-ups triaged,triaged
`dense -> sparse compressed` to work with empty batches. module: sparse triaged,module: sparse triaged
Weird dataloader performance degradation caused by torch and numpy import order triaged module: openmp,triaged module: openmp
Pure virtual function call exception on Python interpreter exit when using debug wheel triaged module: python frontend,triaged module: python frontend
"Fork run CI from upstream remote (more than 10,000 emails)  module: ci triaged",module: ci triaged
version 4.26.1 to 4.29.0 has two bugs oncall: distributed triaged,oncall: distributed triaged
[torch.compile] torch._dynamo.exc.Unsupported: setattr(UserDefinedObjectVariable) for yolov7 triaged bug oncall: pt2 module: dynamo,triaged bug oncall: pt2 module: dynamo
round float16 calculation error in mps backend triaged module: mps,triaged module: mps
Fine-tuning HuggingFace wav2vec 2.0 with `torch.compile` oncall: distributed module: crash triaged module: nccl ezyang's list bug oncall: pt2 module: dynamic shapes,oncall: distributed module: crash triaged module: nccl ezyang's list bug oncall: pt2 module: dynamic shapes
Inconsistency between GPU memory usage in torch.cuda.memory_summary and nvidia-smi module: cuda triaged,module: cuda triaged
Importing torch after TensorFlow results in std::runtime_error triaged module: tensorflow,triaged module: tensorflow
[ONNX] OnnxFunction of aten_index_put_bool operation isn't consistent to aten::index_put inx FX exporter module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Fault and vauge error when invoking nvcc: The system cannot find the file specified module: windows module: cpp-extensions triaged,module: windows module: cpp-extensions triaged
Pytorch compile failure on Windows with CUDA 12.1 because of lacking NVTX component module: build module: windows triaged,module: build module: windows triaged
Tensorboard graph tracing with torch fx API triaged module: tensorboard module: fx,triaged module: tensorboard module: fx
"addmv doesn't do type promotion correctly, triaged module: type promotion module: linear algebra",triaged module: type promotion module: linear algebra
[BE] Refactor logic for MultiTensorApply triaged better-engineering actionable module: mta,triaged better-engineering actionable module: mta
Cannot export quantized model to onnx: cannot call qscheme on UnknownQuantizer module: onnx oncall: quantization low priority triaged,module: onnx oncall: quantization low priority triaged
Multiple Learning Rate Scheduler for Specific Parameters Groups module: optimizer triaged needs research module: LrScheduler,module: optimizer triaged needs research module: LrScheduler
Sequence annotation in type hints is wrong module: typing triaged,module: typing triaged
torch.lobpcg producing different largest eigenvalue than scipy and np.linalg.eig triaged module: numpy module: linear algebra,triaged module: numpy module: linear algebra
Lazily format C++ stack trace if it is not used module: performance triaged,module: performance triaged
"torch.autograd.detect_anomaly should report the original forward trace as part of the error, rather than as out of band warning module: autograd triaged actionable",module: autograd triaged actionable
"Tensor __getitem__ not documented, sparse grad? module: sparse triaged module: advanced indexing",module: sparse triaged module: advanced indexing
DISABLE libtorch-2.0.0+cu117 destructor exception oncall: jit,oncall: jit
[torch.compile] the sum of `softmax` isn't `1` on cuda module: numerical-stability triaged oncall: pt2 module: inductor,module: numerical-stability triaged oncall: pt2 module: inductor
Unsupported: ONNX export of operator interpolate (with scales) error module: onnx triaged,module: onnx triaged
Extending compatibility of LibTorch module: cpp feature triaged needs design,module: cpp feature triaged needs design
"RuntimeError: nonzero is not supported for tensors with more than INT_MAX  elements, file a support request oncall: quantization module: cuda triaged module: 64-bit module: sorting and selection",oncall: quantization module: cuda triaged module: 64-bit module: sorting and selection
"native_batch_norm has different size results on ""CPU"" vs ""META"" device triaged module: meta tensors module: fakeTensor",triaged module: meta tensors module: fakeTensor
Issues building with caffe2 enabled caffe2 triaged,caffe2 triaged
PyTorch installs the file mkldnn.cmake that looks for the package MKLDNN that doesn't exist module: build triaged module: mkldnn,module: build triaged module: mkldnn
torch.concat fails with float16 input in autocast(device_type=cpu) context module: cpu triaged module: bfloat16 module: amp (automated mixed precision),module: cpu triaged module: bfloat16 module: amp (automated mixed precision)
[MPS] Track failures of test_module.py for MPS backend triaged module: backend module: mps,triaged module: backend module: mps
Revise glossary module: docs triaged,module: docs triaged
`torch.distributions.categorical.Categorical` samples indices with zero probability module: distributions triaged,module: distributions triaged
[BUG] Poor torch.bmm performance on H100 module: performance module: cuda triaged module: cublas matrix multiplication,module: performance module: cuda triaged module: cublas matrix multiplication
"Accuracy issues with Jitterated complex kernels for acos, acosh, asin, asinh, tan and tanh module: cuda good first issue triaged module: jiterator",module: cuda good first issue triaged module: jiterator
Dynamo infers different return type vs. eager for `torch.ops.aten` good first issue triaged oncall: pt2 module: decompositions,good first issue triaged oncall: pt2 module: decompositions
[torch.compile] returns output with WRONG SHAPE after `cat_slice_cat` triaged inductor_pattern_match,triaged inductor_pattern_match
Wrong type for `get_lr` inside lr_scheduler.pyi module: optimizer module: typing triaged actionable module: LrScheduler,module: optimizer module: typing triaged actionable module: LrScheduler
There is a performance drop because we have not yet implemented the batching rule for aten::native_dropout_backward triaged actionable module: functorch,triaged actionable module: functorch
Quickstart notebook fails to train properly with ROCm module: rocm triaged,module: rocm triaged
inductor cpp wrapper: crash when disable lowmem_dropout triaged oncall: pt2 module: cpu inductor,triaged oncall: pt2 module: cpu inductor
compile torch2.0 in debug mode triaged topic: build,triaged topic: build
[CUDA RPC] Incorrect results of GPU Tensor transferring using RPC when parallelized with other GPU programs oncall: distributed module: cuda,oncall: distributed module: cuda
[CUDA RPC] Incorrect messages in CUDA Support RPC when parallelized with other GPU programs oncall: distributed,oncall: distributed
torch.cuda.amp.GradScaler initialization  triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
[Discussion] Investigate possibilities for Windows Arm64 BLAS and LAPACK module: windows triaged module: linear algebra,module: windows triaged module: linear algebra
"On UMA systems, pytorch fails to reserve memory exceeding the initial memory size module: rocm module: memory usage triaged",module: rocm module: memory usage triaged
GPU VRAM usage significantly higher for Lenet5 models when compared to other frameworks triaged better-on-discuss-forum,triaged better-on-discuss-forum
Add support for aten::tril_indices for MPS backend  feature triaged module: linear algebra module: mps,feature triaged module: linear algebra module: mps
undocumented error on torch.autograd.Function.jvp for non-Tensor forward returns module: docs module: autograd triaged actionable,module: docs module: autograd triaged actionable
Use a label instead of body text for merge blocking CI SEVs module: ci triaged,module: ci triaged
Backward hook execution order changes when input.requires_grad is False module: docs module: autograd module: nn triaged actionable,module: docs module: autograd module: nn triaged actionable
Arithmetic of single-element Tensors with different dtypes on 'cpu' and 'mps' results in obscure/unhelpful `TypeError` triaged module: mps,triaged module: mps
Higher GPU consumption for Lenet-5 and LSTM models when compared to other frameworks module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
Can't export onnx model from a torch script model oncall: jit module: onnx triaged,oncall: jit module: onnx triaged
Sparse Matrix nnz Overflow when casting from COO to CSR module: sparse triaged,module: sparse triaged
Import setuptools.command.build_ext from torch.utils.cpp_extension somehow indirectly imports Cython when it is installed triaged oncall: pt2 module: startup-tracing-compile time,triaged oncall: pt2 module: startup-tracing-compile time
[BUG] add 1 to different tensor but get same value triaged module: correctness (silent) module: mps,triaged module: correctness (silent) module: mps
some of the enteries in the previous version of pytorch section are invalid  module: binaries module: docs triaged,module: binaries module: docs triaged
Tensor on shared memory is set to 0 when using concurrent.futures and CUDA oncall: distributed triaged,oncall: distributed triaged
[torch.compile] `sum` out-of-bound read triaged module: cpu inductor,triaged module: cpu inductor
[torch.compile] raises RuntimeError in `sdfp_pattern_1` that `Expected size for first two dimensions of batch2 tensor` triaged bug oncall: pt2 module: inductor inductor_pattern_match,triaged bug oncall: pt2 module: inductor inductor_pattern_match
graph._export_onnx() incorrect data types in the binary string representation module: onnx triaged,module: onnx triaged
profiler.export_stacks doesn't return stack trace unless experimental_config is provided high priority module: regression oncall: profiler,high priority module: regression oncall: profiler
"[discussion] ""TensorList"" as first-class abstraction (including python frontend) and as key for dispatch for merging `torch._foreach_*` into regular `torch.*` functions feature triaged module: nestedtensor",feature triaged module: nestedtensor
torch.utils._content_store will deduplicate storage with identical contents; may be problematic for mutation module: serialization triaged,module: serialization triaged
torch.compile is not compatible with DPP with torch.nn.SyncBatchNorm.convert_sync_batchnorm() triaged module: ddp module: norms and normalization oncall: pt2 module: inductor module: distributed,triaged module: ddp module: norms and normalization oncall: pt2 module: inductor module: distributed
Add missing `OpInfo`s for prims ops module: tests triaged module: primTorch,module: tests triaged module: primTorch
`torch.sparse_csc_tensor` matrix multiplication produces MKL error SPARSE_STATUS_ALLOC_FAILED when density is too high module: sparse triaged,module: sparse triaged
Illegal instruction in ARM64 (ver 2.0.0) module: binaries triaged module: arm,module: binaries triaged module: arm
This flag not work : torch.backends.cudnn.allow_tf32 = False   module: cuda triaged,module: cuda triaged
Error saving MONAI pytorch model to ONNX module: onnx triaged,module: onnx triaged
Error building Pytorch from source module: build module: rocm triaged,module: build module: rocm triaged
[regression] torch.norm with out dtype bfloat16 cause runtime error triaged module: regression module: norms and normalization,triaged module: regression module: norms and normalization
[Indexing] Incoherent Tensor indexing for nested lists triaged module: numpy module: advanced indexing module: edge cases,triaged module: numpy module: advanced indexing module: edge cases
[compile] output does not match eager mode high priority triaged oncall: pt2 module: functorch module: pt2 accuracy module: pt2-dispatcher,high priority triaged oncall: pt2 module: functorch module: pt2 accuracy module: pt2-dispatcher
Issue with FSDP + HuggingFace generate triaged module: fsdp,triaged module: fsdp
add github check that diffs generated code triaged module: infra module: codegen,triaged module: infra module: codegen
pre_autograd `make_fx` broken with simple F.linear with symbolic shape triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
Add compile option -Werror=return-type compile error module: build triaged actionable,module: build triaged actionable
Issue of HistogramObserver to handle abnormal value oncall: quantization triaged,oncall: quantization triaged
Dataloader multiprocess loading with num_worker > 0 calls __main__ file to run module: dataloader triaged,module: dataloader triaged
Revive multigpu testing module: ci triaged,module: ci triaged
torch.triu() may returns wrong values using MPS high priority triaged module: NaNs and Infs module: correctness (silent) module: mps,high priority triaged module: NaNs and Infs module: correctness (silent) module: mps
Runtime Error triaged bug,triaged bug
Copying an MPS tensor to a CPU tensor using a for loop fails triaged module: mps,triaged module: mps
torch.cuda.is_available() crashes python in systems with disabled gpu module: crash module: cuda triaged module: edge cases,module: crash module: cuda triaged module: edge cases
Group Norm crashes on Apple M1/MPS devices for versions 2.0+ needs reproduction triaged module: regression module: mps,needs reproduction triaged module: regression module: mps
I encountered an error while trying to save the stylegan2 network as torch. onnx. export module: onnx triaged,module: onnx triaged
torch.jit.trace can not trace buffer by Module.register_buffer() when use DDP Module. oncall: distributed,oncall: distributed
FSDP + gradient clipping raises an odd warning with the simplest model on torch 2.0 oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
ciflow/inductor should run both inference and training even if inference fails module: ci triaged module: inductor module: devx,module: ci triaged module: inductor module: devx
[RFC] DebugMode triaged,triaged
Can the CUDA device LUID be exposed as part of _CudaDeviceProperties? module: cuda triaged enhancement actionable needs design,module: cuda triaged enhancement actionable needs design
Many models are failing on periodic dynamic shape benchmark tests dynamic_aot_eager triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
Logs output_code and inductor do not interact as expected module: logging triaged,module: logging triaged
NTK notebook calculates wrong object - wrong output dimensions triaged module: functorch,triaged module: functorch
cuda.is_available() error module: cuda triaged,module: cuda triaged
`cat` gradgrad tests failing module: autograd triaged module: edge cases,module: autograd triaged module: edge cases
torch.multinomial() always returns [0] using MPS triaged module: mps,triaged module: mps
Windows fatal exception: stack overflow while using pytorch for computing triaged module: functorch,triaged module: functorch
Automatic broadcasting for sparse csr tensors module: sparse triaged,module: sparse triaged
Apple metal (MPS)  device returning incorrect keypoints for YOLOv8 pose estimation model  high priority module: binaries oncall: releng triaged module: correctness (silent) module: mps,high priority module: binaries oncall: releng triaged module: correctness (silent) module: mps
Cannot compile torch 1.10 in CentOS 7.3 triaged,triaged
2.0.0+cu118 package missing proper libnvrtc-builtins.so.11.8 module: binaries module: cpp triaged,module: binaries module: cpp triaged
cuda 12.0 support request for building pytorch from source code module: build module: cuda triaged enhancement,module: build module: cuda triaged enhancement
no-duplicate-decl-specifier as a invalid compile flag for CXX in GCC module: build module: rocm triaged,module: build module: rocm triaged
pca_lowrank and svd_lowrank broken under automatic mixed precision. module: cuda triaged module: half module: linear algebra module: amp (automated mixed precision),module: cuda triaged module: half module: linear algebra module: amp (automated mixed precision)
"WARNING: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. oncall: jit onnx-triaged",oncall: jit onnx-triaged
"gpu training  work well, but cpu training not work module: cpu triaged module: fft",module: cpu triaged module: fft
In torchelastic support running worker rank 0 on agent rank 0 consistently oncall: distributed triaged module: elastic,oncall: distributed triaged module: elastic
`torch.ops.aten.empty` is not discoverable from `dir(torch.ops.aten)` until explicitly calling getattr triaged module: library,triaged module: library
DistributedDataParallel doesn't work with complex buffers oncall: distributed module: complex,oncall: distributed module: complex
[torch.compile] raises an error that expanded size doesn't match when enabling `shape_padding` triaged bug oncall: pt2 module: inductor,triaged bug oncall: pt2 module: inductor
Ban GradScaler scale from being less than 1 triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
Torch hangs at import if tensorflow is imported first module: binaries triaged,module: binaries triaged
Parameterisation of MultivariateNormal distribution using Cholesky decomposition of precision matrix module: distributions feature triaged,module: distributions feature triaged
Conda Pytorch set processor affinity to the first physical core after fork high priority module: dependency bug module: binaries triaged module: mkl module: third_party module: intel,high priority module: dependency bug module: binaries triaged module: mkl module: third_party module: intel
CUPTI Initialization error  module: cuda triaged oncall: profiler,module: cuda triaged oncall: profiler
Training Faster R-CNN model with COCO dataset has been consistently unsuccessful. module: dataloader triaged,module: dataloader triaged
lintrunner mypy raises error in numpy module: lint triaged,module: lint triaged
torch.func.jacrev fails if model contains full_backward_hook module: autograd triaged module: functorch,module: autograd triaged module: functorch
Batching rule not implemented for aten::narrow.Tensor triaged module: functorch,triaged module: functorch
Cross compile Pytorch for ARM in Bazel module: build triaged enhancement module: arm,module: build triaged enhancement module: arm
"Jacfwd become slower after update pytorch (""Weâ€™ve integrated functorch into PyTorch---Documentation"") high priority needs reproduction triaged module: functorch",high priority needs reproduction triaged module: functorch
Support polyphase channelizer feature triaged module: fft,feature triaged module: fft
'Illegal instruction (core dumped)' for gpt-j bf16 generation task using greedy search  module: crash module: cpu triaged module: intel,module: crash module: cpu triaged module: intel
Not Preserving Grad For Tensor Created Inside torch.compile triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher
vision_maskrcnn failing on periodic dynamic_aot_eager_torchbench triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
[DTensor] parallelize_module failed with nn.Transformer and the PairwiseParallel plan oncall: distributed triaged,oncall: distributed triaged
Question about GRU(RNN/LSTM) outputs shape module: docs module: nn module: rnn triaged actionable,module: docs module: nn module: rnn triaged actionable
The meta implementation of `index_put` does not do any check triaged module: meta tensors release notes: Meta API,triaged module: meta tensors release notes: Meta API
"torch.nn.functional.multilabel_margin_loss cuda lacks checking of ""out of bound"" module: nn module: cuda module: error checking triaged",module: nn module: cuda module: error checking triaged
Torch.fx.symbolic_trace removes some of the keys from module state_dict triaged module: fx,triaged module: fx
FakeTensor lacks support for sparse compressed tensors module: sparse triaged module: fakeTensor,module: sparse triaged module: fakeTensor
Internal errors with cuda graph (CUBLAS_STATUS_NOT_INITIALIZED and jit failure) triaged module: cuda graphs,triaged module: cuda graphs
torch.compile error triaged oncall: pt2 module: cpu inductor,triaged oncall: pt2 module: cpu inductor
PyTorch 2.0.0 encountered CUDA error: an illegal memory access was encountered module: cuda triaged module: multithreading,module: cuda triaged module: multithreading
[c++17] Replace lock_guard with scoped_lock  module: internals triaged,module: internals triaged
add `-std=c++20` build-only CI job module: build module: ci triaged module: devx,module: build module: ci triaged module: devx
we should make semantically meaningless positional arguments positional only in our operator API feature triaged actionable module: python array api module: python frontend,feature triaged actionable module: python array api module: python frontend
torch.linalg.lstsq doc arguments error module: docs triaged module: linear algebra,module: docs triaged module: linear algebra
Functorch pytrees with custom iterables triaged enhancement module: pytree module: functorch,triaged enhancement module: pytree module: functorch
Torch func Documentation for trees module: docs triaged module: functorch,module: docs triaged module: functorch
CI for s390x module: ci triaged enhancement,module: ci triaged enhancement
There has implmenet bug in LTC IrBuilder's MakeSizeMul method. triaged lazy module: lazy,triaged lazy module: lazy
Slicing and indexing support negative steps feature triaged module: advanced indexing,feature triaged module: advanced indexing
Add `TORCH_ASSERT_ONLY_METHOD_OPERATORS` to functorch codebase triaged better-engineering module: functorch,triaged better-engineering module: functorch
Build error on libstc++ header stl_alogbase.h on riscv module: build good first issue triaged,module: build good first issue triaged
Remove lr_scheduler.print_lr module: optimizer triaged module: LrScheduler,module: optimizer triaged module: LrScheduler
Embedding layer tensor shape needs reproduction triaged module: embedding,needs reproduction triaged module: embedding
the error message of torch.addcmul is wrong module: error checking triaged,module: error checking triaged
tools PYTHONPATH trick in run_test.py does not work reliably triaged module: testing,triaged module: testing
vision_maskrcnn failing in periodic/trunk triaged bug oncall: pt2,triaged bug oncall: pt2
Libtorch consumes too much memory as 16225 module: cpp module: memory usage triaged,module: cpp module: memory usage triaged
Sporadic CUDA error in `test_nccl_warn_not_in_group_debug_detail` oncall: distributed,oncall: distributed
opacus_cifar10 fails in dynamo due to hooks  triaged bug oncall: pt2 module: dynamo,triaged bug oncall: pt2 module: dynamo
Unused `import torch` followed by `cuml.NearestNeighbors` leads to nondeterministic segfault (during Python process exit?) needs reproduction module: crash triaged,needs reproduction module: crash triaged
Add Debug builds for python with pydebug triaged module: devx,triaged module: devx
"Run ChatRWKV on MBP(intel CPU)+eGPU[rx6800 16G], returna a very big number -9223372036854775808, looks like overflow triaged module: mps",triaged module: mps
"Spectral Normalization can not be applied to Conv{1,2,3}d module: nn triaged needs research",module: nn triaged needs research
`torch.sparse.sum` backward fails when reducing over dense dimensions. module: sparse module: autograd triaged,module: sparse module: autograd triaged
No documentation to show how to implement aten::view for custom backend module: cpp-extensions module: docs triaged,module: cpp-extensions module: docs triaged
"Why nn.Upsample/F.interpolate followed by nn.InstanceNorm2d will report error ""Unsupported: ONNX export of instance_norm for unknown channel size."" module: onnx triaged module: norms and normalization",module: onnx triaged module: norms and normalization
torch.cuda.is_available() return False module: docs module: cuda triaged,module: docs module: cuda triaged
Invalid Reference to Class oncall: quantization low priority triaged topic: docs,oncall: quantization low priority triaged topic: docs
Look into test coverage for `UntypedStorage` module: typing module: tests triaged,module: typing module: tests triaged
Memory allocation issues in distributions.multivariate_normal.MultivariateNormal module: distributions triaged,module: distributions triaged
AttributeError: type object 'torch._C._profiler.ProfilerActivity' has no attribute 'MPS' triaged oncall: profiler module: mps,triaged oncall: profiler module: mps
Issue on building from source: Remove -mfpu=neon option on MacOS with Apple silicon module: build triaged,module: build triaged
Is there a way to get the full call stack of pytorch from python to C/C++? triaged,triaged
Dtype changes while going from FX graph -> Torchscript triaged FX-TorchScript Compatibility module: fx,triaged FX-TorchScript Compatibility module: fx
"[BUG]Float32 attention mask not working with torch.autocast(""cpu"") triaged oncall: transformer/mha",triaged oncall: transformer/mha
[torch.compile] makes `linear(permute(input))` succeed for integer input in `torch.no_grad` context triaged bug module: inductor,triaged bug module: inductor
[BE] Dedup the functorch skipOps mechanism and the common_method_invocations one triaged module: testing,triaged module: testing
Sparse Tensor: in-place operation on detached tensors no longer raised error module: sparse triaged,module: sparse triaged
[torch.compile] `replace_fx`  triaged bug module: inductor,triaged bug module: inductor
behaviour of `torch.tensor()` changes after editing `Tensor.__getitem__` triaged module: python frontend,triaged module: python frontend
[torch.fx] Upgrade on node info triaged module: fx,triaged module: fx
"torch.dist with minus norm returns tensor(0.), while with -inf can return result module: distributions triaged",module: distributions triaged
TracingContext.get().frame_summary_stack doesn't produce full stack trace triaged module: dynamo,triaged module: dynamo
torch.sparse_csr_tensor() stops gradients module: sparse triaged,module: sparse triaged
Request for deterministic support for reflection_pad2d_backward_cuda module: cuda triaged enhancement module: padding,module: cuda triaged enhancement module: padding
Integrate open device privateuse1 customized method registration triaged module: backend,triaged module: backend
Unable to load MultiStepLR with torch.load(weights_only=True)  module: serialization triaged,module: serialization triaged
Change module to module_ in torch/csrc/api/include/torch/python.h module: build triaged,module: build triaged
Move template code to header module: cpp triaged,module: cpp triaged
Test failure: TestCommonCPU.test_python_ref__refs_abs_cpu_complex32 module: tests triaged,module: tests triaged
Changes to TorchScript autodiff changing default behavior are no longer accepted triage review oncall: jit,triage review oncall: jit
Expand component configurable logging system to C++ module: logging triaged,module: logging triaged
Document the user-facing API for the component-level logging system module: docs triaged,module: docs triaged
Problem with instalation torch2 on a100+cu12.1 module: binaries module: cuda triaged,module: binaries module: cuda triaged
Sparse Tensor not working for `torch.cat` module: sparse triaged,module: sparse triaged
Sharded Grad Scaler Issue Tracker oncall: distributed triaged module: amp (automated mixed precision) module: fsdp,oncall: distributed triaged module: amp (automated mixed precision) module: fsdp
PyTorch's packaged libgomp causes significant performance penalties on CPU when used together with other Python packages module: build triaged module: multithreading,module: build triaged module: multithreading
[functorch] vmap_hessian_fc - fails under torch.compile triaged oncall: pt2 module: functorch module: pt2-dispatcher,triaged oncall: pt2 module: functorch module: pt2-dispatcher
[functorch] functorch_maml_omniglot - fails under torch.compile triaged oncall: pt2 module: functorch module: pt2-dispatcher,triaged oncall: pt2 module: functorch module: pt2-dispatcher
[FSDP] summon_full_params with_grad=True CPU offload can crash oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
File-level retry enhancements triaged module: devx,triaged module: devx
autocast does not work properly on embedding module triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
[FSDP] move up the first all gather oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Discrepancy of supported Python versions between Get Started page and index of pre-built binaries for PIP installation module: docs triaged,module: docs triaged
DataLoader doesn't accept non-cpu device for loading.  module: dataloader triaged,module: dataloader triaged
Conflict between ``torch.func`` transformations and ``torch.jit.trace`` triaged module: functorch,triaged module: functorch
Ubuntu 22.04 LTS issue <built-in function load_binary> returned NULL without setting an exception module: rocm triaged oncall: pt2,module: rocm triaged oncall: pt2
torch.matmul with batched CSR matrix module: sparse triaged,module: sparse triaged
[ux] Non-blocking tensor constructors triaged enhancement has workaround module: tensor creation,triaged enhancement has workaround module: tensor creation
Wrong illustration in README.md module: docs triaged,module: docs triaged
Cannot use AT_CUDA_DRIVER_CHECK from user code module: build module: cpp-extensions module: internals module: cuda triaged,module: build module: cpp-extensions module: internals module: cuda triaged
`F.interpolate` and `F.grid_sample` - documentation error and bug module: docs triaged,module: docs triaged
torch.jit.script codegen warning with cuda and vmap oncall: jit triaged module: functorch,oncall: jit triaged module: functorch
Memory corruption using torch.ops.* to access re-registered operator module: internals triaged,module: internals triaged
Segfault when using torch.ops.* to access de-registered op module: crash triaged module: dispatch module: library,module: crash triaged module: dispatch module: library
torch.cond should work with expressions involving SymInt triaged module: functorch,triaged module: functorch
Power VSX vectorization support disabled module: build triaged module: regression module: POWER,module: build triaged module: regression module: POWER
`torch.nn.utils.rnn.unpad_sequence` modifies arguments in-place module: docs module: rnn triaged,module: docs module: rnn triaged
Strided to batch BSR/BSC conversion fails when the number of zeros per block varies while the number of blocks per patch is constant module: sparse triaged,module: sparse triaged
Improvements to FSDP debugability oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Bring CudaPluggableAllocator to feature parity with the Native Allocator module: internals module: cuda triaged module: CUDACachingAllocator,module: internals module: cuda triaged module: CUDACachingAllocator
tacotron2 times out triaged bug oncall: pt2 module: inductor,triaged bug oncall: pt2 module: inductor
Need better error message when a merge cancelled because of timeout module: ci triaged,module: ci triaged
Fail to pass test HAVE_XXX_REGEX while  building pytorch  module: build triaged,module: build triaged
README could use link to governance high priority module: docs triaged,high priority module: docs triaged
assert callable(unaltered_fn) high priority triaged oncall: pt2,high priority triaged oncall: pt2
Backwards graph is labeled incorrectly when dynamic=True triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
"PyTorch 1.12, high failure rate for test_optim/test_nadam module: optimizer triaged",module: optimizer triaged
`torch.Tensor.layout` is not documented module: docs triaged module: python frontend,module: docs triaged module: python frontend
Contribute to the privateuse1 backend. module: internals triaged module: backend,module: internals triaged module: backend
[PTD][Checkpoint] Enable single_file_per_rank for fsspec storage read/write oncall: distributed triaged,oncall: distributed triaged
pip doesn't install the right version of pytorch when torchtext is involved module: binaries triaged,module: binaries triaged
Intermittent failure of mobilenet_v3_large triaged module: flaky-tests oncall: pt2,triaged module: flaky-tests oncall: pt2
[functorch] [vmap] tests fail when `_set_vmap_fallback_enabled(False)`. triaged module: functorch,triaged module: functorch
"When I use the DDP model, I use a custom loss function, when the batch size changes during training, the process will be stuck. oncall: distributed triaged",oncall: distributed triaged
"Inconsistent nn.KLDivLoss behavior: 0s in target OK on cpu, but gives nan on mps triaged module: mps",triaged module: mps
Broken mypy check in test_type_hints.py::TestTypeHints::test_doc_examples module: typing triaged,module: typing triaged
Wrong results for GELU forward pass (CPU vs MPS) while  inferencing a GLPN model from huggingface high priority triaged module: correctness (silent) module: mps,high priority triaged module: correctness (silent) module: mps
Add a deterministic version of reflection_pad2d_backward_cuda module: nn triaged enhancement module: determinism actionable,module: nn triaged enhancement module: determinism actionable
NaN appears when initializing tensor needs reproduction triaged module: NaNs and Infs,needs reproduction triaged module: NaNs and Infs
torch.nn.init functions with `generator` argument module: nn triaged module: random actionable,module: nn triaged module: random actionable
"RuntimeError: CUDA error: an illegal memory access was encountered, torch/cuda/streams.py"", line 94, in synchronize module: cuda triaged",module: cuda triaged
how can i load seperate pytorch_model.bin? module: serialization triaged,module: serialization triaged
forward AD implimentation : _scaled_dot_product_efficient_attention  triaged module: forward ad,triaged module: forward ad
A Segment Fault can be triggered in torch._grid_sampler_2d_cpu_fallback module: crash module: nn triaged,module: crash module: nn triaged
[interoperability] zero-size cuda arrays do not look supported module: cuda triaged,module: cuda triaged
Request to cherrypick a fix into v1.13.1 (v1.8 has a CVE) module: binaries triaged,module: binaries triaged
GroupNorm cpu/gpu parity tests fail with pretty large differences module: nn triaged needs research,module: nn triaged needs research
Is there a recommended implementation of yuv2RGB for the current torch? module: onnx module: nn triaged module: python frontend,module: onnx module: nn triaged module: python frontend
Unexpected results with torch.nn.functional.layer_norm module: numerical-stability triaged module: norms and normalization,module: numerical-stability triaged module: norms and normalization
 Add PrivateUse1 folder in aten/src/ATen triaged module: backend,triaged module: backend
Request custom backend device memory Allocator. module: memory usage triaged module: backend module: CUDACachingAllocator,module: memory usage triaged module: backend module: CUDACachingAllocator
Automate aarch64 builds oncall: releng triaged,oncall: releng triaged
Write Binary Builds oncall runbook oncall: releng triaged,oncall: releng triaged
Create release checklist template for the Launch Date oncall: releng triaged,oncall: releng triaged
Create a plan on removing conda dependency from CI/CD oncall: releng triaged,oncall: releng triaged
matmul with CSR matrix in inference mode throws an exception module: sparse module: autograd triaged,module: sparse module: autograd triaged
DataLoader with collate_fn that returns tensors in GPU memory raises warnings when deleted module: dataloader triaged,module: dataloader triaged
torch.compile not compatible with multiprocessing pool triaged bug oncall: pt2 module: inductor,triaged bug oncall: pt2 module: inductor
Relax version dependencies on CUDA pip wheels? module: binaries module: cuda triaged,module: binaries module: cuda triaged
torch.randn signature is missing generator module: docs triaged enhancement module: random module: python frontend,module: docs triaged enhancement module: random module: python frontend
[CI/Infra] Record keeping: runner shutdown spike triaged,triaged
Investigate Lazy{*}Norm{*}d modules no batch dim support module: nn triaged,module: nn triaged
BUG torch.jit.annotate on List + torch.stack give wrong DTYPE oncall: jit,oncall: jit
`torch.func.functional_call` doesn't work with compiled models high priority triaged module: correctness (silent) oncall: pt2 module: dynamo,high priority triaged module: correctness (silent) oncall: pt2 module: dynamo
Multiple model init using OpenMP in c++ does not speed up module: multiprocessing triaged,module: multiprocessing triaged
A parameterized fill value for triu and tril functions triaged enhancement module: viewing and reshaping,triaged enhancement module: viewing and reshaping
Type conversion between float/complex triaged module: complex enhancement,triaged module: complex enhancement
Unable to install torch on python 3.8.16 needs reproduction module: binaries triaged,needs reproduction module: binaries triaged
make tensor data const correct module: internals triaged,module: internals triaged
Functionalize crashes on train_step GraphModule triaged module: functionalization,triaged module: functionalization
TORCH_LIBRARIES variable leads to undefined reference function error in compiling while using libtorch in c++ module: cpp triaged,module: cpp triaged
Document _wrap_fx_args_as_onnxscript_args module: onnx triaged,module: onnx triaged
CUDA 10.2 cudnn 8.2.4 run Conv2d error needs reproduction module: cudnn module: cuda triaged,needs reproduction module: cudnn module: cuda triaged
Memory leak when saving an input tensor returned as-is if mark_dirty and running with dual tensors module: autograd triaged module: edge cases module: forward ad,module: autograd triaged module: edge cases module: forward ad
Using `param in param_list` can trigger `non-singleton dimension` error? triaged module: python frontend,triaged module: python frontend
Some c++ library docstrings incorrectly linked/repeated module: docs module: cpp triaged module: mps,module: docs module: cpp triaged module: mps
consider bumping `DEFAULT_PROTOCOL` module: serialization triaged,module: serialization triaged
torch.testing.assert_close: allow check to fail on part on the input triaged module: testing,triaged module: testing
Test Failure: TestUnaryUfuncsCPU.test_reference_numerics_normal_cos_cpu_float32 on s390x triaged module: numpy,triaged module: numpy
oneDNN 3.0+ support triaged module: intel,triaged module: intel
irrelevant error output for Minified repro triaged oncall: pt2 module: minifier,triaged oncall: pt2 module: minifier
Bug on Minified repro example  triaged oncall: pt2 module: minifier,triaged oncall: pt2 module: minifier
TypeError: 'torch._C._TensorMeta' object is not iterable module: windows module: rocm triaged,module: windows module: rocm triaged
Dynamo generates invalid frame when graph-breaking due to opacus_cifar10 hooks triaged bug module: dynamo release notes: dynamo,triaged bug module: dynamo release notes: dynamo
 Compile targts cuda:0 rather than the device the model is on triaged bug oncall: pt2 module: inductor,triaged bug oncall: pt2 module: inductor
[FSDP] Consolidate test_fsdp_state_dict.py oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Pytorch 2 compile + fsdp + transformers crash triaged module: xla,triaged module: xla
[FSDP] test model.eval() + keep_low_precision_grads oncall: distributed triaged,oncall: distributed triaged
sparse_csr_tensor matmul wrong output in bfloat16 module: sparse triaged,module: sparse triaged
[bug] Internal assert failed when using pyro module: distributions triaged module: macos module: linear algebra module: python frontend,module: distributions triaged module: macos module: linear algebra module: python frontend
Regression in jit for f-strings with new lines oncall: jit triaged,oncall: jit triaged
JAX + PyTorch produces `OMP: Error #13: Assertion failure at kmp_affinity.cpp(532)` needs reproduction triaged,needs reproduction triaged
torch.zeros_like on a zero-sized BSR/BSC tensor results invalid tensor module: sparse triaged module: correctness (silent),module: sparse triaged module: correctness (silent)
Compile dynamic does not support GroupNorm in module triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
"MPS: grid_sampler_2d falls back to CPU, even though warning says it is natively supported on macOS >=13.1 triaged module: mps",triaged module: mps
Insufficient MPS Documentation module: docs triaged module: mps,module: docs triaged module: mps
can get_submodule be called within a ScriptFunction ? oncall: jit triaged,oncall: jit triaged
Torch 2.0 import hangs forever module: build module: cuda triaged,module: build module: cuda triaged
Multi-output derivative formulas can save unnecessary tensors module: autograd triaged module: nestedtensor actionable,module: autograd triaged module: nestedtensor actionable
PackedSequence failure with MPS triaged module: mps,triaged module: mps
InfoNCE loss for contrastive learning module: loss triaged enhancement,module: loss triaged enhancement
"Burn benchmark suites into CI docker image. Not only this saves test time, but also it will get rid of occasional model installation failures. (@weiwangmeta ) triaged",triaged
torch.cppExtension won't work with wsl2 triaged module: wsl,triaged module: wsl
.set_ operation on a view (detach()) of the view tensor changes grad_fn of the original view tensor from ViewBackward0 to AsStridedBackward0 module: autograd triaged has workaround,module: autograd triaged has workaround
Function Registry for extending collate_fn module: dataloader triaged enhancement,module: dataloader triaged enhancement
Improve collectives fingerprinting good first issue triaged module: c10d,good first issue triaged module: c10d
Change progressbar for hub triaged module: hub,triaged module: hub
torch.compile not working with gradient checkpointing module: checkpoint triaged oncall: pt2 module: distributed,module: checkpoint triaged oncall: pt2 module: distributed
suspicious memory leak when increase DataLoader's prefetch_factor and enable pin_memory module: dataloader module: memory usage triaged,module: dataloader module: memory usage triaged
"After the release of pytorch 2.0.0, the compilation of ACLs is problematic. module: build triaged",module: build triaged
Import fails when both `USE_TENSORPIPE=OFF` and `USE_DISTRIBUTED=ON`. module: build triaged module: tensorpipe,module: build triaged module: tensorpipe
Expanded weights tests broken triaged module: functorch,triaged module: functorch
Sparse is not available on Windows module: sparse module: build module: windows triaged,module: sparse module: build module: windows triaged
Traced module shows non-deterministic behaviour on CUDA oncall: jit,oncall: jit
`torch.fmod` produces inconsistent results in eager and compile mode module: cpu triaged module: half oncall: pt2 module: inductor module: cpu inductor,module: cpu triaged module: half oncall: pt2 module: inductor module: cpu inductor
"torch.ops.aten.pow(2.0, 3) return unexpected value with complex type module: internals triaged module: custom-operators",module: internals triaged module: custom-operators
MPS: `unique` and `unique_consecutive` extremely slow when `return_counts=True` module: performance triaged module: mps,module: performance triaged module: mps
`jacrev` and `jacfwd` raise an error that `Sparse CSR tensors do not have strides` module: sparse triaged module: functorch,module: sparse triaged module: functorch
"test_sparse_addmm fails on linux-bionic-py3.11-clang9 / test (crossref, 1, 2, linux.2xlarge) module: sparse module: autograd triaged module: decompositions",module: sparse module: autograd triaged module: decompositions
`jacfwd` fails when computing the gradient for `channels_last` tensor triaged module: memory format module: functorch,triaged module: memory format module: functorch
[composable FSDP] clip_grad_norm oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Desync debugger encounters traceMap error oncall: distributed triaged module: c10d bug,oncall: distributed triaged module: c10d bug
functorch roll-up issue for 2.1 triaged module: functorch,triaged module: functorch
Non-deterministic results when training a model on GPU with MPS backend triaged module: determinism module: mps,triaged module: determinism module: mps
Incompatibility with complex tensors needs reproduction triaged module: complex oncall: pt2,needs reproduction triaged module: complex oncall: pt2
"INTERNAL ASSERT FAILED at ""../c10/cuda/CUDAGraphsC10Utils.h"":73, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus32729 triaged module: cuda graphs",triaged module: cuda graphs
aten::sym_size is not using torch._ops.OpOverload in FX graph triaged module: dynamic shapes,triaged module: dynamic shapes
Sequential/Partial unpickling and loading of models module: pickle triaged,module: pickle triaged
torch.randint range for torch.int64 dtype seems wrong triaged topic: docs module: python frontend,triaged topic: docs module: python frontend
Building LibTorch on Ubuntu with Mac M1 module: build triaged module: torchbind module: m1,module: build triaged module: torchbind module: m1
Nightly conda binaries failed to pass tests since 2023-03-17  module: binaries oncall: releng module: ci triaged,module: binaries oncall: releng module: ci triaged
[FSDP][optim_state_dict] Need more comprehensive tests for optim_state_dict interface oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Implement `torch.distributions.Poisson.cdf()` module: distributions feature triaged,module: distributions feature triaged
Custom recurrent network takes very long to compile for long sequences triaged ezyang's list oncall: pt2 module: python dispatcher,triaged ezyang's list oncall: pt2 module: python dispatcher
RPC Tutorial can not profile the  rpc operations communication between workers oncall: distributed triaged module: rpc oncall: profiler,oncall: distributed triaged module: rpc oncall: profiler
Problem with Hugging Face model that is not in training loop triaged oncall: pt2 module: cpu inductor,triaged oncall: pt2 module: cpu inductor
Incorrect gradient calculation for upsample nearest on CUDA needs reproduction module: autograd module: nn triaged module: interpolation,needs reproduction module: autograd module: nn triaged module: interpolation
"MultiHeadAttention, fast path broken with `bias=False` or uneven number of heads triaged oncall: transformer/mha",triaged oncall: transformer/mha
nn.Conv function to compute conv formula module: docs module: nn triaged actionable,module: docs module: nn triaged actionable
[Feature Proposal: New Distributed Training Algorithms] LSGD and EASGD oncall: distributed triaged,oncall: distributed triaged
TransformerEncoder truncates output when some token positions are masked by `src_key_padding_mask` across batch triaged module: nestedtensor oncall: transformer/mha,triaged module: nestedtensor oncall: transformer/mha
"""Adaptive pool MPS: input sizes must be divisible by output sizes"", I keep getting this error even when I try to adjust for size triaged module: mps",triaged module: mps
slow torch import on macos  module: performance triaged module: macos,module: performance triaged module: macos
torch.cuda.FloatTensor().normal_() generate (partially) different sample on different gpu machines module: docs triaged,module: docs triaged
A Segment Fault can be triggered in torch.embedding triaged module: edge cases,triaged module: edge cases
A Segment Fault can be triggered in torch.adjoint triaged module: edge cases,triaged module: edge cases
A crash due to Floating Point Exception can be triggered in torch.index_select module: crash module: error checking triaged module: edge cases,module: crash module: error checking triaged module: edge cases
Inconsitent results before/after compilation for squeeze + tensor mutation + if statement triaged bug oncall: pt2 module: aotdispatch module: inductor module: pt2-dispatcher,triaged bug oncall: pt2 module: aotdispatch module: inductor module: pt2-dispatcher
[RFC] CPU float16 performance optimization on eager mode. feature module: cpu triaged module: half,feature module: cpu triaged module: half
Optimize for mobile produces incorrect result with INSERT_FOLD_PREPACK_OPS optimization oncall: jit oncall: mobile actionable,oncall: jit oncall: mobile actionable
DDP static graph fails for static model oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
deprecate integral and boolean dtype support torch.logit and torch.special.logit triaged module: deprecation module: special,triaged module: deprecation module: special
Small learning rate with `capturable=True` causes Adam optimizer to blow up model parameters. module: optimizer triaged,module: optimizer triaged
[mps] conv1d outputs zeros triaged module: mps,triaged module: mps
Adding sparse `addmv` and `triangular_solve` support on CPU - Mac OS - Apple Silicon M2 module: sparse triaged module: macos module: arm,module: sparse triaged module: macos module: arm
GPUï¼š7900xtx Pytorch2.0.0  rocBLAS error: module: binaries module: rocm triaged,module: binaries module: rocm triaged
torch.onnx.export failed for models with Bernoulli operator module: onnx triaged,module: onnx triaged
Doing inplace on a inplace view of tensor that retains_grad triggers internal assert module: autograd triaged has workaround,module: autograd triaged has workaround
Performance Drop for linalg_ldl_factor and ldl_solve triaged enhancement actionable module: vmap module: functorch,triaged enhancement actionable module: vmap module: functorch
`cumprod` triggers INTERNAL ASSERT FAILED when `out` is a tensor on cuda but input is on cpu module: error checking triaged module: assert failure,module: error checking triaged module: assert failure
 Segmentation fault (core dumped) during Torch finetuning (at random step) needs reproduction module: crash triaged,needs reproduction module: crash triaged
[MPS] pinverse dtype error triaged module: linear algebra module: mps,triaged module: linear algebra module: mps
`sparse.mm` triggers INTERNAL ASSERT FAILED when backwarding module: sparse module: autograd triaged,module: sparse module: autograd triaged
Follow-ups to do after adding nested checkpoint module: autograd triaged actionable,module: autograd triaged actionable
Improve checkpoint thread-safety module: autograd triaged module: multithreading needs design,module: autograd triaged module: multithreading needs design
Pruning under channels_last format triaged module: memory format module: pruning,triaged module: memory format module: pruning
Pytorch2.0 compile error needs reproduction triaged oncall: pt2 module: distributed,needs reproduction triaged oncall: pt2 module: distributed
Many padding Module fail memory_format tests module: nn triaged module: memory format actionable module: intel,module: nn triaged module: memory format actionable module: intel
when run python run_test.py -i test_ops_jit error like this. ValueError: option names {'--junit-xml-reruns'} already added oncall: jit,oncall: jit
[MPS] `.to('mps')` zeroes out elements in tensors taking up >=2^32 bytes triaged module: mps,triaged module: mps
`logical_xx` operations trigger INTERNAL ASSERT FAIL when `input` is complex tensor on cuda and `other` is on cpu triaged module: complex,triaged module: complex
[H100] `test_ops.py::TestFakeTensorCUDA.test_fake_crossref_backward_amp_nn_functional_scaled_dot_product_attention_cuda_float32` failed module: cuda triaged module: fakeTensor,module: cuda triaged module: fakeTensor
"No GPU found, using CPU during preprocessing Error processing dataset with NsfHifiGAN  needs reproduction module: windows module: cuda triaged",needs reproduction module: windows module: cuda triaged
Harden composable fully_shard: Checklist oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
PyTorch SGEMV is using 1 single core on AMD CPUs (very slow) module: rocm triaged,module: rocm triaged
Not allow force merge when lint fails and not because of broken trunk module: ci triaged,module: ci triaged
"torch.cuda.graph ""Invalid capture"" with torch.linalg.solve triaged module: cuda graphs",triaged module: cuda graphs
Dataloader should kill & restart workers when timeout is hit module: dataloader triaged,module: dataloader triaged
Tensor Permutation Along Given Axis feature triaged module: python frontend,feature triaged module: python frontend
[MPS] Incorrect results for cumsum with bool tensors triaged module: mps,triaged module: mps
The output of torch.histc is incorrect on both CPU and CUDA triaged module: edge cases,triaged module: edge cases
 No matching distribution found for torch==1.13.1+cu117 module: binaries triaged,module: binaries triaged
"[MPS] softmax returns NaN attention probabilities for large tensors, in float16 and float32. triaged module: mps",triaged module: mps
Why doesn't PyTorch install the REAL nvidia cuDNN pip package? module: binaries module: cuda triaged,module: binaries module: cuda triaged
Proposal: Disable GC in test suite; GC after every test case triaged module: flaky-tests module: infra module: testing module: devx,triaged module: flaky-tests module: infra module: testing module: devx
Wrong return type from operation on custom tensor inside registered hook  module: autograd triaged needs research module: __torch_function__ tensor subclass,module: autograd triaged needs research module: __torch_function__ tensor subclass
Enable functorch testing for rocm module: rocm triaged module: functorch,module: rocm triaged module: functorch
fft should ignore dims with shape 1 triaged module: fft,triaged module: fft
The sign of torch.distributions.transforms.PowerTransform seems to be incorrect module: distributions triaged,module: distributions triaged
"[MINIFIER] Running code snippet with  TORCHDYNAMO_REPRO_AFTER=""dynamo"" leads to error needs reproduction triaged oncall: pt2 module: dynamic shapes module: minifier",needs reproduction triaged oncall: pt2 module: dynamic shapes module: minifier
Shape Error when training HF deberta-base with Inductor good first issue triaged module: meta tensors oncall: pt2 module: pt2-dispatcher,good first issue triaged module: meta tensors oncall: pt2 module: pt2-dispatcher
Information about CPU in `collect_env` is too verbose module: collect_env.py triaged,module: collect_env.py triaged
Compressed sparse constructor allows mixed `int32/int64` indices which leads to dtype promotion/demotion in conversions. module: sparse triaged,module: sparse triaged
Add location information when exception are thrown in `torch.jit.annotations.try_ann_to_type` oncall: jit,oncall: jit
Proxy Options for Pytorch Hub triaged module: hub,triaged module: hub
"Initialization on `meta` device failing for models containing `nn.utils.weight_norm`, with `NotImplementedError: Could not run 'aten::_weight_norm_interface' with arguments from the 'Meta' backend.` triaged actionable module: meta tensors",triaged actionable module: meta tensors
"[export] ""strict subset of traced input/output"" error when huggingface `ModelOutput` is returned triaged module: dynamo oncall: export",triaged module: dynamo oncall: export
"`dynamo.export` ""input not consistent with traced input"" error when input default value type is `torch.Tensor`. triaged onnx-needs-info module: dynamo oncall: export",triaged onnx-needs-info module: dynamo oncall: export
views created in __torch_dispatch__ share storage but not version_counter module: autograd module: molly-guard triaged needs design module: __torch_dispatch__,module: autograd module: molly-guard triaged needs design module: __torch_dispatch__
`FractionalMaxPool3d` INTERNAL ASSERT FAILED when computing `jacrev` module: nn triaged actionable module: edge cases,module: nn triaged actionable module: edge cases
"Reuse autograd.grad graph for rapid, repeated gradient calculation feature module: autograd triaged module: cuda graphs",feature module: autograd triaged module: cuda graphs
Better error message when trying to run fp16 weights on CPU good first issue module: error checking triaged,good first issue module: error checking triaged
A Segment Fault can be triggered in torch.adaptive_max_pool1d with an edge case module: crash triaged module: edge cases,module: crash triaged module: edge cases
A Segment Fault can be triggered in torch.geqrf with an edge case module: crash triaged module: edge cases,module: crash triaged module: edge cases
A Segment Fault can be triggered in torch.pinverse module: crash triaged module: edge cases,module: crash triaged module: edge cases
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation triaged,triaged
nn.interpolate scale_factor floors output size with floating  module: nn triaged module: edge cases,module: nn triaged module: edge cases
[MPS] F.conv1d and F.conv2d produce incorrect gradients when minibatch >= 2^16 triaged module: mps,triaged module: mps
`gradgradcheck` does not work with sparse inputs. module: sparse module: autograd triaged,module: sparse module: autograd triaged
[torchdistx] Future of the large model initialization module: nn triaged ezyang's list module: meta tensors module: fsdp,module: nn triaged ezyang's list module: meta tensors module: fsdp
mps bug: failed assertion `[MPSNDArrayDescriptor sliceDimension:withSubrange:] error: subRange.start (6) is not less than length of dimension[0] (6)' high priority triaged module: regression module: viewing and reshaping module: mps,high priority triaged module: regression module: viewing and reshaping module: mps
mkldnn matmul kernel may be slower than openblas kernel for very small tensor shapes module: performance triaged module: mkldnn,module: performance triaged module: mkldnn
`torch.utils.checkpoint` should avoid updating BatchNorm statistics twice module: checkpoint triaged,module: checkpoint triaged
Static size boolean masking triaged module: advanced indexing,triaged module: advanced indexing
torch.where behaves differently from in place replacement needs reproduction module: autograd triaged,needs reproduction module: autograd triaged
Error during inference on iOS: INTERNAL ASSERT FAILED at it_type_base.h:535 oncall: jit,oncall: jit
Add support for `__collate__` attrib on dataset elements in `default_collate` module: dataloader triaged,module: dataloader triaged
Pytorch 2.0 installation tutorial does not work under Macbook module: docs triaged module: macos,module: docs triaged module: macos
Linking libtorch with QT5 OpenGL application using llvmpipe mesa opengl crashes module: binaries triaged,module: binaries triaged
MPS device throws error for `F.adaptive_avg_pool2d` triaged module: mps,triaged module: mps
No speedup and a null pointer exception needs reproduction triaged oncall: pt2,needs reproduction triaged oncall: pt2
Add arm64 builds for libtorch on MacOS with mps support triaged module: macos module: infra module: arm,triaged module: macos module: infra module: arm
Cannot access data pointer of Tensor that doesn't have storage when using `torch.func.jvp` with `torch.compile` triaged actionable oncall: pt2 module: functorch module: pt2-dispatcher,triaged actionable oncall: pt2 module: functorch module: pt2-dispatcher
Questions and Possible Features: Pytorch RPC 'future.wait()' will not release GIL which will block other thread's execution when using multithreading. triaged module: multithreading,triaged module: multithreading
FSDP fails to load state dict under inference_mode triaged enhancement inference mode module: fsdp,triaged enhancement inference mode module: fsdp
[vulkan] missing aten::reflection_pad1d.out operator triaged module: vulkan,triaged module: vulkan
The torch.sparse document's typo error module: sparse module: docs triaged,module: sparse module: docs triaged
"Build from source,  Undefined symbol: c10::detail::maybe_wrap_dim_slow(long long, long long, bool) triaged oncall: mobile module: ios",triaged oncall: mobile module: ios
CPU time performance is unstable module: performance module: cpu triaged,module: performance module: cpu triaged
training hangs at line torch.cuda.synchronize() module: cuda triaged module: deadlock,module: cuda triaged module: deadlock
arange bug module: numerical-stability triaged,module: numerical-stability triaged
ROCm distributed flaky on test_distributed_spawn module: rocm triaged,module: rocm triaged
torchvision Caltech101 collate_fn error triaged,triaged
corrupted size vs prev size error triaged module: ddp,triaged module: ddp
Let Nested Tensor Metadata be cached on GPU triaged module: nestedtensor,triaged module: nestedtensor
drastic speed regression of torch.jit.load starting with the 20230301 nightly oncall: jit,oncall: jit
Static asserts on accessor templates module: cpp-extensions triaged,module: cpp-extensions triaged
Fully quantized model (`torch.quantization.convert`) produces incorrect output compared to analytical solution oncall: quantization triaged,oncall: quantization triaged
SymInt'ify _gather_sparse_backward triaged,triaged
`torch.Tensor.is_set_to` raises `NotImplementedError` when inputs contain sparse tensor  module: sparse triaged,module: sparse triaged
Implementing the batching rule for aten::bucketize.Tensor. triaged actionable module: vmap module: functorch,triaged actionable module: vmap module: functorch
Inconsistent behaviour of torch.all() module: cuda triaged,module: cuda triaged
`torch.nanmedian` return a negative value when input is empty  module: error checking triaged,module: error checking triaged
functorch.compile.memory_efficient_fusion errors with: RuntimeError: forward() Expected a value of type 'Tensor (inferred)' for argument 'primals_356' but instead found type 'int'.  triaged module: functorch,triaged module: functorch
Multiheadattention module doesn't implement the function about kdim and vdim triaged module: multi-headed-attention,triaged module: multi-headed-attention
`copy.deepcopy` does not copy gradients of nn.Parameter module: bc-breaking module: autograd module: nn triaged topic: bc breaking,module: bc-breaking module: autograd module: nn triaged topic: bc breaking
Can only import torch after Tensorflow accessed its gpu device module: cuda triaged,module: cuda triaged
Unable to import ``torch.linalg`` needs reproduction triaged,needs reproduction triaged
torch needs to SHOW that it support sm_89 even if functionally the same as sm_86 module: cuda triaged,module: cuda triaged
Create a new Docker image with all inductor benchmarks and pre-trained models downloaded module: ci triaged module: devx,module: ci triaged module: devx
"Performance bugs exists in multiple convolution operations(e.g., `Convtranspose2d`) when useing the `groups` argument module: cudnn module: docs module: convolution triaged",module: cudnn module: docs module: convolution triaged
TorchInductor fails with memoy violations in `test_comprehensive_grid_sampler_2d_cuda_float16` and `test_reflection_pad2d_dynamic_shapes_cuda` high priority triaged oncall: pt2 module: inductor,high priority triaged oncall: pt2 module: inductor
Confusing error messages from `torch.nn.LazyLinear` in different versions. module: error checking triaged module: lazy,module: error checking triaged module: lazy
Support datatype argument for torch.distributed.all_gather() (And the whole distributed module) feature triaged module: nccl module: c10d,feature triaged module: nccl module: c10d
test_layer_norm_backward and test_layer_norm_backward_5d run OOM in slow gradcheck triaged module: nestedtensor,triaged module: nestedtensor
torch.jit.load documentation doesn't specify if it is safe to load untrusted models or not oncall: jit module: docs security,oncall: jit module: docs security
" torch.distributions.kumaraswamy.Kumaraswamy generates samples outside its support (0,1) module: distributions triaged module: NaNs and Infs",module: distributions triaged module: NaNs and Infs
Tensor.all() fails on MPS for tensors with more than 4 dimensions triaged module: reductions module: mps,triaged module: reductions module: mps
dynamo+aot improperly handles dupe args via *args triaged bug oncall: pt2 module: dynamo,triaged bug oncall: pt2 module: dynamo
Import parameters from jit oncall: jit,oncall: jit
Torch RPC on multiple nodes with GPU returns a EOF error oncall: distributed triaged module: rpc module: tensorpipe,oncall: distributed triaged module: rpc module: tensorpipe
Enrich shape operations with nested tensors triaged module: nestedtensor,triaged module: nestedtensor
`add/add_` for CSC: errors when trying to access non-existent `crow_indices`. module: sparse triaged,module: sparse triaged
torch.profiler.tensorboard_trace_handler Generates an incorrect JSON file triaged module: tensorboard oncall: visualization,triaged module: tensorboard oncall: visualization
It seems that `torch.Tensor.addmv` and `torch.Tensor.addr` will check some inputs' dtype if and only if in `backward()` module: autograd triaged module: complex module: type promotion module: linear algebra actionable complex_autograd,module: autograd triaged module: complex module: type promotion module: linear algebra actionable complex_autograd
Regression bug in `torch.nn.ReLU6` and `torch.nn.Hardtanh` that `inplace=True` doesn't work in PyTorch 1.10.0~1.13.1 high priority module: nn module: memory usage triaged actionable,high priority module: nn module: memory usage triaged actionable
test_ddp_apply_optim_in_backward in distributed_test.py fails for gloo backend oncall: distributed triaged,oncall: distributed triaged
Add log highlights to Dr. CI's failed jobs triaged,triaged
Investigate/add Windows Arm64 support for cpuinfo module: windows triaged module: arm,module: windows triaged module: arm
build failed when strictly following the guidelines module: build triaged,module: build triaged
Changing behavior of module.to() to better support mixed real- and complex-valued parameters module: nn triaged module: complex needs design,module: nn triaged module: complex needs design
Circular padding error for 3D arrays triaged module: padding,triaged module: padding
`torch.distributed.Store` triggers INTERNAL ASSER FAILED when seting oncall: distributed triaged,oncall: distributed triaged
`torch.cartesian_prod` returns inconsistent dimensions with only one input triaged module: linear algebra,triaged module: linear algebra
Continuous dropout layer module: nn triaged enhancement,module: nn triaged enhancement
tabulate is used by `torch.fx.graph_module.GraphModule.print_tabular` but is not installed when installing pytorch triaged module: fx,triaged module: fx
Make this ridiculously long error message more user friendly triaged module: infra,triaged module: infra
test_foreach failing cuda memory leak check module: cuda triaged module: mta,module: cuda triaged module: mta
Remove conda virtualenv from the docker image module: binaries triaged module: docker,module: binaries triaged module: docker
"new backend privateuseone with ""to"" op triaged module: backend",triaged module: backend
High Cuda Memory Consumption for Simple ResNet50 Inference oncall: jit,oncall: jit
COO @ COO tries to allocate way too much memory on CUDA module: sparse module: cuda triaged matrix multiplication,module: sparse module: cuda triaged matrix multiplication
"Reversing along a dimension, similarly to numpy feature triaged module: numpy module: advanced indexing",feature triaged module: numpy module: advanced indexing
Whether to consider native support for intel gpuï¼Ÿ triaged module: intel,triaged module: intel
Add local version identifier to wheel file names module: build triaged,module: build triaged
Differentiate with regard a subset of the input feature module: autograd triaged,feature module: autograd triaged
Default value of `validate_args` is set to `True` when passed as `None` in `Multinomial` module: distributions triaged,module: distributions triaged
"`INTERNAL ASSERT FAILED` -When using the PyTorch docker environment released by pytorch, a Vulcan support issue occurs module: build triaged module: docker",module: build triaged module: docker
CosineAnnealingWarmRestarts but restarts are becoming more frequent triaged module: LrScheduler,triaged module: LrScheduler
cuda 12 support request. module: cuda triaged,module: cuda triaged
"When using `ceil_mode=True`, `torch.nn.AvgPool1d` could get negative shape. module: bc-breaking triaged module: shape checking topic: bc breaking",module: bc-breaking triaged module: shape checking topic: bc breaking
`torch.nn.LazyLinear` crash when using torch.bfloat16 dtype in pytorch 1.12.0 and 1.13.0 module: nn triaged intel,module: nn triaged intel
"RuntimeError: view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: BFloat16 triaged module: complex module: bfloat16",triaged module: complex module: bfloat16
Implement a `torch.cuda.visible_device_indexes` function.  module: cuda triaged,module: cuda triaged
Make artifacts easier to discover on HUD module: ci triaged,module: ci triaged
A100 Perf Job artifact zipfiles unzip to generic folder that loses job information module: ci triaged,module: ci triaged
`torch.cuda.device_count` cached return value does not reflect environment changes. module: cuda triaged,module: cuda triaged
Upsampling ResBlock GPU memory spike module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
[Inductor] [CPU] Huggingface model AllenaiLongformerBase performance regression > 10% on ww07.4 triaged module: inductor module: cpu inductor,triaged module: inductor module: cpu inductor
[Inductor] [CPU] Huggingface model MT5ForConditionalGeneration &T5ForConditionalGeneration & T5Small performance regression > 10% on ww07.4 triaged module: inductor module: cpu inductor,triaged module: inductor module: cpu inductor
[Inductor] [CPU] Torchbench model hf_Longformer performance regression > 10% on ww07.4 triaged module: inductor module: cpu inductor,triaged module: inductor module: cpu inductor
[Inductor] [CPU] Torchbench model hf_T5 & hf_T5_large & hf_T5_base performance regression > 10% on ww07.4 triaged module: inductor,triaged module: inductor
cuDNN doesn't support convolutions with more than `INT_MAX` elements and native kernel uses too much memory module: cudnn module: convolution triaged module: CUDACachingAllocator,module: cudnn module: convolution triaged module: CUDACachingAllocator
interactions between views + autograd.Function + AOTAutograd causes memory leak module: autograd triaged actionable,module: autograd triaged actionable
[libtorh]Consistency problem of gpu computing module: cuda triaged module: determinism,module: cuda triaged module: determinism
CSR matrix add_ error with RuntimeError: CUDA error: kernel launch failure when calling cusparseXcsrgeam2Nnz module: sparse module: cuda triaged,module: sparse module: cuda triaged
PR #88607 breaks build for POWER9 CPU module: build triaged module: POWER actionable,module: build triaged module: POWER actionable
[numpy] mean & nanmean should support int dtypes triaged module: numpy module: reductions,triaged module: numpy module: reductions
ASSERT(initialized()) Debug Error after JIT fusion on Windows oncall: jit module: nvfuser,oncall: jit module: nvfuser
Memory leak in torch.fft.rfft module: cuda module: memory usage triaged module: fft,module: cuda module: memory usage triaged module: fft
aten::cudnn_convolution chooses different conv implementation given the same inputs.  module: cudnn module: cuda module: convolution triaged,module: cudnn module: cuda module: convolution triaged
[FSDP] Gradients not propagating for mixed precision case oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Linking error with Libtorch module: cpp triaged module: mkl,module: cpp triaged module: mkl
Make `torch.onnx.utils._optimize_graph` use several CPU cores module: performance module: onnx triaged,module: performance module: onnx triaged
grid_sample with relative grid feature module: nn triaged actionable module: interpolation,feature module: nn triaged actionable module: interpolation
Memory Corruption in torch.lstm caused by edge cases module: crash module: rnn triaged module: edge cases,module: crash module: rnn triaged module: edge cases
ImportError: cannot import name 'Backend' from 'torch._C._distributed_c10d' (unknown location) oncall: distributed triaged,oncall: distributed triaged
Build Error: no matching function for call to â€˜dnnl::graph::stream::stream(<brace-enclosed initializer list>)â€™ module: build triaged module: mkldnn,module: build triaged module: mkldnn
Compiling PyTorch from Source on Xavier module: build triaged module: vectorization,module: build triaged module: vectorization
Compiling libtorch from Source on Mac Beyond v1.11.0 module: build module: cpp triaged module: macos,module: build module: cpp triaged module: macos
pytorch log level API and env var high priority feature module: logging triaged,high priority feature module: logging triaged
Better Numpy API (interoperability between ML frameworks) triaged module: numpy,triaged module: numpy
MPS internal error in `torch.gather` when last dimension is a singleton dimension module: crash triaged module: mps,module: crash triaged module: mps
Update PyTorch's default C standard to C17 from C11 module: build triaged,module: build triaged
Option to bypass NOLA check in torch.istft triaged module: fft,triaged module: fft
Investigate queue disparity between `windows.4xlarge` and `linux.4xlarge` high priority module: ci triaged module: regression,high priority module: ci triaged module: regression
Split getitem OpInfo into dynamic and non-dynamic inputs module: tests triaged,module: tests triaged
`where` triggers INTERNAL ASSERT FAILED when `out` is a long tensor due to mixed types module: error checking triaged module: type promotion,module: error checking triaged module: type promotion
A segment fault can be triggered in torch.avg_pool1d module: crash triaged module: edge cases,module: crash triaged module: edge cases
A segment fault can be triggered in torch.max_pool1d_with_indices module: crash triaged module: edge cases,module: crash triaged module: edge cases
inductor `compile_fx_inner` output is incorrect on graph with trailing copy_() triaged bug module: inductor,triaged bug module: inductor
Nan is output by GRU on mps triaged module: mps,triaged module: mps
"`UnsupportedOperatorError`, `OnnxExporterError` and `SymbolicValueError` related to MultiheadAttention export to onnx with torch.jit.script module: onnx triaged",module: onnx triaged
A segment fault can be triggered in torch.svd module: crash triaged module: edge cases,module: crash triaged module: edge cases
A segment fault can be triggered in torch.lstm with edge cases module: crash triaged module: edge cases,module: crash triaged module: edge cases
Missing FX documents for some modules module: docs triaged,module: docs triaged
[RFC] Add a static_graph mode for FSDP oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Jetson CI needs Updates triaged module: jetson,triaged module: jetson
Saving a `torch.nn.HuberLoss` using `torch.jit.script().save()` doesn't seem to implicitly convert from `int` type to `float` type. oncall: jit,oncall: jit
A segment fault can be triggered in torch.histogramdd module: crash triaged module: edge cases,module: crash triaged module: edge cases
Memory corruptions can be triggered in torch._remove_batch_dim triaged module: vmap,triaged module: vmap
Issue with `upsample_nearest2d` decomposition triaged module: decompositions,triaged module: decompositions
A Segment Fault can be triggered in torch.affine_grid_generator triaged module: edge cases,triaged module: edge cases
`permute` for named tensors triaged module: named tensor,triaged module: named tensor
Abort Caused by Virtual Function module: build triaged module: regression,module: build triaged module: regression
torch.lgamma CUDA driver error needs reproduction triaged module: special,needs reproduction triaged module: special
pybind11 SymNode binding is a footgun py::cast triaged module: pybind,triaged module: pybind
[Functionalization] `index_reduce_` op tests with functionalization enabled triaged module: meta tensors module: functionalization,triaged module: meta tensors module: functionalization
LSTM on CPU is significantly slower on PyTorch compared to other frameworks module: performance module: cpu triaged,module: performance module: cpu triaged
"`jacrev` raise ""Cannot access storage of TensorWrapper"" error when computing the grad of `storage` module: autograd triaged actionable module: functorch",module: autograd triaged actionable module: functorch
Pickling OneCycleLR.state_dict() with an unpickleable optimizer will result in an error. module: optimizer module: pickle triaged needs research,module: optimizer module: pickle triaged needs research
A better error msg for `cuda.jiterator` when input is on `cpu` triaged module: jiterator,triaged module: jiterator
`get_debug_state` a script function causes INTERNAL ASSERT FAILED oncall: jit triaged,oncall: jit triaged
[RFC]FSDP API should make limit_all_gathers and forward_prefetch both default to be True triaged module: fsdp,triaged module: fsdp
[fake_tensor] torch._subclasses.fake_tensor.DynamicOutputShapeException when calling torch.nonzero using aot_function triaged oncall: pt2 module: dynamic shapes module: graph breaks,triaged oncall: pt2 module: dynamic shapes module: graph breaks
jacfwd and jacrev are fundamentally broken for complex inputs module: autograd triaged module: complex complex_autograd module: functorch,module: autograd triaged module: complex complex_autograd module: functorch
`func.jacrev()` should be implemented as `func.jacfwd().mT.contiguous()` triaged module: complex module: functorch,triaged module: complex module: functorch
Inconsistent results when using torch.Tensor.bernoulli with float instead of Tensor probabilities module: distributions triaged module: random module: determinism,module: distributions triaged module: random module: determinism
[fx] const_fold.split_const_subgraphs leads to UserWarning triaged module: fx,triaged module: fx
"QAT + torch.autocast does not work with default settings, missing fused fake_quant support for half oncall: quantization low priority triaged",oncall: quantization low priority triaged
`scatter` fails the gradient computation in reverse mode for `src` when `index` is empty module: autograd triaged actionable module: scatter & gather ops,module: autograd triaged actionable module: scatter & gather ops
cpu log1p for bfloat16 gives wrong result. module: cpu triaged module: bfloat16,module: cpu triaged module: bfloat16
RFC: Enabling AVX512 dispatch for compute-intensive ATen ops module: performance module: cpu triaged module: intel,module: performance module: cpu triaged module: intel
"RuntimeError: p.block != nullptr && p.block->ptr != nullptr INTERNAL ASSERT FAILED at ""../c10/cuda/CUDACachingAllocator.cpp"":1275, please report a bug to PyTorch. triaged module: assert failure module: CUDACachingAllocator",triaged module: assert failure module: CUDACachingAllocator
CUBLAS_STATUS_NOT_SUPPORTED when calling cublasDgemv module: cuda triaged module: cublas,module: cuda triaged module: cublas
What type of attributes does symbolic function support? triaged,triaged
`PyTorchFileWriter` should drop the GIL while writing files module: serialization triaged,module: serialization triaged
unsqueeze a single dimension multiple times feature triaged module: viewing and reshaping,feature triaged module: viewing and reshaping
`zeros_like` + `fill_` makes the gradient computation in forward mode fail triaged module: forward ad,triaged module: forward ad
Addition of hybrid CSR tensors produces incorrect and invalid CSR tensor module: sparse triaged module: correctness (silent) bug,module: sparse triaged module: correctness (silent) bug
Addition of CSC/BSR/BSC tensors raises RuntimeError exceptions module: sparse triaged,module: sparse triaged
Addition of batch CSR tensors produces incorrect and invalid CSR tensor module: sparse triaged module: correctness (silent) bug,module: sparse triaged module: correctness (silent) bug
"Faster `pad_sequence` and `tensor_split` function with CUDA kernel, are they possible? module: rnn triaged module: dynamic shapes",module: rnn triaged module: dynamic shapes
JIT: Dropout fails codegen on the third forward passes triaged module: nvfuser,triaged module: nvfuser
Subclassed Tensors Decrease Training GPU Throughput by ~40%  module: performance triaged module: __torch_function__ tensor subclass oncall: pt2 module: pt2-dispatcher,module: performance triaged module: __torch_function__ tensor subclass oncall: pt2 module: pt2-dispatcher
Asking for a LAZYMODULEMIXIN warning module: nn module: molly-guard triaged module: lazy,module: nn module: molly-guard triaged module: lazy
faster WeightedRandomSampler implementation based on alias method module: dataloader triaged,module: dataloader triaged
A Floating Point Exception can be trigerred in torch._C._nn.slow_conv3d module: crash triaged module: edge cases,module: crash triaged module: edge cases
`cat` fails the gradient computation in forward mode with empty tensors when used with legacy vmap triaged module: edge cases module: forward ad,triaged module: edge cases module: forward ad
`svd` triggers INTERNAL ASSERT FAILED when computing jacobian in forward mode module: autograd triaged module: complex has workaround module: linear algebra module: forward ad,module: autograd triaged module: complex has workaround module: linear algebra module: forward ad
`MSELoss` fails to compute the gradients when inputs have different dtype module: autograd module: nn triaged actionable,module: autograd module: nn triaged actionable
`unfold` fails in forward mode when unfolding a scalar tensor triaged module: forward ad,triaged module: forward ad
Tracker for `scatter_reduce` additional reduction options requests triaged module: scatter & gather ops,triaged module: scatter & gather ops
Set AVX2 is minimum supported instruction set for Linux X86 triaged enhancement module: intel,triaged enhancement module: intel
Type promotion for accumulate operation differs between eager and CPP dynamo  module: cpu triaged bug oncall: pt2,module: cpu triaged bug oncall: pt2
test_nccl_warn_not_in_group_debug_detail is flaky oncall: distributed triaged module: flaky-tests,oncall: distributed triaged module: flaky-tests
`linalg.lstsq` fails the gradient computation in forward mode triaged module: forward ad,triaged module: forward ad
"Enable Link Time Optimization in PyTorch 2.0 Release Binaries - Smaller, Faster, Better Binaries module: binaries module: performance module: build oncall: releng triaged topic: performance",module: binaries module: performance module: build oncall: releng triaged topic: performance
[RFC] Support Huge Model Init Without mallocs for Compile/Distributed Use Cases oncall: distributed triaged,oncall: distributed triaged
error: no member named 'residual_with_sum_zero_point' in 'ideep::attr_t module: build triaged module: macos,module: build triaged module: macos
"`torch.jit.trace` memory usage increase although forward is constant, and gets much slower than forward with model depth increase oncall: jit",oncall: jit
"[FSDP] `summon_full_params(writeback=True, rank0_only=True)` oncall: distributed triaged module: fsdp",oncall: distributed triaged module: fsdp
onnx_torch.ModelProto exceeded maximum protobuf size of 2GB module: onnx triaged,module: onnx triaged
[pt20][aot_eager] Exceed Python recursion limit with huge model or frequent recompilation triaged ezyang's list oncall: pt2 module: dynamic shapes module: dynamo,triaged ezyang's list oncall: pt2 module: dynamic shapes module: dynamo
[BUG] jit.trace not working for torchvision ViT models oncall: jit,oncall: jit
"Why does the torch model have no memory leaks under gpu, but there is a memory leak under cpu, torch version 1.10.1 needs reproduction triaged",needs reproduction triaged
Importing tensorflow (2.12) before torch (2.0) hangs at import torch module: binaries triaged,module: binaries triaged
"`PYTORCH_DEBUG_MODE`, better invalid index embedding lookup error message on cuda high priority triaged needs design module: python frontend",high priority triaged needs design module: python frontend
Bitwise-perfect method for (de)serializing tensors in base64 feature module: serialization triaged,feature module: serialization triaged
Enable CUPTI module: windows triaged,module: windows triaged
torchdim can not be compiled for Python-3.11 on Windows module: windows triaged module: functorch,module: windows triaged module: functorch
large number of temporary files generated when using dataloader with num_workers>0 high priority module: dataloader triaged module: openmp,high priority module: dataloader triaged module: openmp
EmbeddingBag to support mini-batches with offsets triaged enhancement module: nestedtensor,triaged enhancement module: nestedtensor
"ONNX Export Fails: Model input type is Dict[str, Tensor]  module: onnx triaged",module: onnx triaged
[pt2] MMDet meets Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode error with aot_eager backend needs reproduction triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher,needs reproduction triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher
torch.jit.script does not work with DataParallel oncall: jit,oncall: jit
Make torch.testing functions overrideable with torch_function? triaged module: __torch_function__ module: testing,triaged module: __torch_function__ module: testing
"minifier_launcher.py silently swallows ""ran into runtime exception which is likely an unrelated an issue"" warnings triaged oncall: pt2",triaged oncall: pt2
"when I want to use a new backend, how to deal with the op with 'device' argument?  triaged module: backend",triaged module: backend
Quantized Transformer ONNX Export Fails module: onnx oncall: quantization low priority triaged,module: onnx oncall: quantization low priority triaged
Minifier should not use pickle to save state into minifier launcher triaged module: fx module: functorch,triaged module: fx module: functorch
Minifier doesn't save/load functorch config triaged module: fx module: functorch,triaged module: fx module: functorch
[CI]  PyTorch Windows Test AMIs contains CUDA-11.3 installation module: windows module: ci triaged,module: windows module: ci triaged
`torch.compile()` failed on Huggingface Flan-T5 `torch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(forward) [] OrderedDict()` triage review triaged oncall: pt2,triage review triaged oncall: pt2
Errors when running the fsdp benchmarks for hf_Bert and hf_T5 triaged module: fsdp bug oncall: pt2 module: dynamo module: distributed,triaged module: fsdp bug oncall: pt2 module: dynamo module: distributed
Estimate effort needed to bring PyTorch to Windows Arm64 module: windows triaged,module: windows triaged
Bug in torch.linalg.svd  triaged module: mkl,triaged module: mkl
MaskRCNN model loaded fail with torch::jit::load(model_path) (C++ API) oncall: jit,oncall: jit
USE_CUDNN=1 doesn't force cmake to fail if cudnn is not found module: build module: cuda triaged enhancement module: build warnings,module: build module: cuda triaged enhancement module: build warnings
"Minifier produces minifier script that doesn't fail accuracy on Background_Matting (dynamic shapes, inductor, inference) triaged module: fx module: functorch",triaged module: fx module: functorch
squeezenet1_1 fails accuracy with AMP (but not on CI and dashboard); minifier does not work (when not using cuDNN?) triaged shadow review module: amp (automated mixed precision),triaged shadow review module: amp (automated mixed precision)
Build from Source Issues on MacOS Ventura 13.2 module: build triaged module: macos,module: build triaged module: macos
Add Support for RockChip NPUs (RKNN(2))  triaged module: arm,triaged module: arm
Why is AvgPool2D taking longer than Conv2D for the same input? module: performance module: cpu triaged,module: performance module: cpu triaged
Segmentation fault between Numpy and Pytorch using torch.bmm module: binaries module: crash triaged,module: binaries module: crash triaged
Failed to Open libnvrtc-builtins.so.11.7 oncall: jit module: cuda,oncall: jit module: cuda
[RFC] Flop counters in PyTorch feature triaged module: python frontend,feature triaged module: python frontend
numpy v1.24 does not work with `writer.add_histogram` triaged module: tensorboard,triaged module: tensorboard
[BE] Improve FSDP <> AC Unit Tests oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Test Failure: TestUpgraders.test_aten_div_scalar_at_3 on a big-endian machine (issue in torch.jit.load()) oncall: jit,oncall: jit
Tracking issue for segfaults and floating point exceptions on 1.12.0 triaged,triaged
Add Stride Argument For Constructors triaged enhancement module: tensor creation,triaged enhancement module: tensor creation
[Functionalization] Some ops need additional meta tensor support after functionalization triaged module: xla module: meta tensors module: functionalization,triaged module: xla module: meta tensors module: functionalization
functorch.functionalize doesn't error out with logcumsumexp.out triaged module: xla module: meta tensors module: functionalization,triaged module: xla module: meta tensors module: functionalization
torch.jit.save() generates different contents in a file among different endian machines oncall: jit module: POWER,oncall: jit module: POWER
[RFC] XLA Lazy Backend Support In DistributedTensor API triaged module: xla,triaged module: xla
Unable to find an engine to execute when using pip to install but not with conda module: binaries module: cudnn module: convolution triaged,module: binaries module: cudnn module: convolution triaged
"[RFC] Make more operations inplace (GELU, BatchNorm, LayerNorm) module: nn triaged enhancement actionable",module: nn triaged enhancement actionable
JIT Function Fails when run a second time oncall: jit,oncall: jit
Double free when running torch.linalg.ldl_solve triaged module: edge cases,triaged module: edge cases
segfault when running torch.igamma triaged,triaged
Ability to manually set the gradient in FSDP while inside `summon_full_params` and make it persistent needs reproduction oncall: distributed triaged module: fsdp,needs reproduction oncall: distributed triaged module: fsdp
Segfault when running torch.atan2 triaged module: complex,triaged module: complex
[complex] Jacobian of a non-holomorphic complex valued function triaged complex_autograd,triaged complex_autograd
"[BE] move _apply_to_tensors from FSDP to torch.distributed.utils, use in _recursive_to oncall: distributed triaged better-engineering module: ddp",oncall: distributed triaged better-engineering module: ddp
Segmentation fault when running torch.ge needs reproduction triaged,needs reproduction triaged
Process get killed when running torch.combinations needs reproduction module: performance triaged module: edge cases,needs reproduction module: performance triaged module: edge cases
Floating point exception when running torch.nn.AdaptiveMaxPool3d triaged,triaged
Process get killed when running torch.normal triaged module: numpy,triaged module: numpy
segfault when running torch.lu_unpack module: crash triaged module: linear algebra module: edge cases,module: crash triaged module: linear algebra module: edge cases
no attribute torch._dynamo unless you explicitly import torch._dynamo triaged module: dynamo,triaged module: dynamo
'MPS' issue: torch.multinomial() returning [-9223372036854775808] triaged module: mps,triaged module: mps
[JIT] Consecutive use of `addmm` Leads to Exception oncall: jit,oncall: jit
[JIT] Applying `conv2d` over Constants Leads to Exception oncall: jit,oncall: jit
Dynamo can not trace 'int(a_scalar_tensor.item())' triaged bug oncall: pt2,triaged bug oncall: pt2
iter(TensorVariable) fail triaged bug oncall: pt2,triaged bug oncall: pt2
set_default_device/torch.device has performance impact for non-factory functions module: performance triaged module: __torch_function__,module: performance triaged module: __torch_function__
DDP+inductor+profiler crashes on  toy model module: crash triaged oncall: profiler bug oncall: pt2 module: inductor,module: crash triaged oncall: profiler bug oncall: pt2 module: inductor
Torchscript troubles with complex values. RuntimeError: isInt() INTERNAL ASSERT FAILED oncall: jit triaged module: complex,oncall: jit triaged module: complex
[JIT] `Linear` + `BatchNorm2d` Trigger Inconsistency between Eager Mode and JIT oncall: jit,oncall: jit
Update quantization to make source files complient with /Zc:lambda module: windows oncall: quantization low priority triaged,module: windows oncall: quantization low priority triaged
INTERNAL ASSERT FAILED when mixed dtypes for `addcmul_` triaged module: assert failure module: type promotion,triaged module: assert failure module: type promotion
Improve Fake Tensor Error When Data Ptr is Accessed triaged module: fakeTensor,triaged module: fakeTensor
[JIT] INTERNAL ASSERT FAILED when `Conv2d` and `clamp` used together oncall: jit,oncall: jit
"[JIT][TracingCheckError] inplace ops incompatible with `contiguous(.., channels_last)` oncall: jit",oncall: jit
Major bug in Transformers' masks high priority triage review oncall: transformer/mha module: correctness (silent),high priority triage review oncall: transformer/mha module: correctness (silent)
[JIT] Inconsistency  in tensor shape between eager mode and JIT oncall: jit,oncall: jit
Pytorch AMP performance issue. triaged module: memory format module: amp (automated mixed precision),triaged module: memory format module: amp (automated mixed precision)
multiprocessing not work on WSL2 module: multiprocessing triaged module: wsl,module: multiprocessing triaged module: wsl
[Inductor] support complex dtypes triaged module: complex module: random module: inductor module: cpu inductor,triaged module: complex module: random module: inductor module: cpu inductor
TypeError: no implementation found for 'torch._ops.aten.max.default' on types that implement __torch_dispatch__: [<class 'torch.masked.maskedtensor.core.MaskedTensor'>] triaged module: masked operators,triaged module: masked operators
support setattr of arbitrary user provided types in tracing triaged bug oncall: pt2,triaged bug oncall: pt2
"fft.fftshift, fft.ifftshift, roll not implemented triaged module: fft module: mps",triaged module: fft module: mps
backward(inputs= does not need to execute grad_fn of the inputs module: bc-breaking module: autograd triaged actionable topic: bc breaking,module: bc-breaking module: autograd triaged actionable topic: bc breaking
Simplify module backward hooks to use multi-grad hooks instead module: bc-breaking module: autograd module: nn triaged needs research topic: bc breaking,module: bc-breaking module: autograd module: nn triaged needs research topic: bc breaking
[Releng] Windows AMI needs to be pinned for release high priority oncall: releng triaged,high priority oncall: releng triaged
Cost & performance estimation for Windows Arm64 compilation module: windows triaged,module: windows triaged
jit.fork stalls multiprocessing dataloader oncall: jit module: dataloader module: data,oncall: jit module: dataloader module: data
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation triaged bug oncall: pt2,triaged bug oncall: pt2
"""Get Started"" tells us to use the anaconda installer for PyTorch 3.x - but this should be python 3.x module: binaries triaged",module: binaries triaged
InstanceNorm operator support for Vulkan devices triaged module: vulkan,triaged module: vulkan
Always install cpu version automatically module: binaries triaged,module: binaries triaged
distributions.Beta returning incorrect results at 0 and 1 module: distributions triaged,module: distributions triaged
"`model.to(""cuda:0"")` does not release all CPU memory module: memory usage triaged",module: memory usage triaged
"`torch.load(..., map_location=""cuda:0"")` allocates memory on both CPU and GPU module: serialization triaged module: python frontend",module: serialization triaged module: python frontend
torch.cuda.is_available() returns True even if the CUDA hardware can't run pytorch module: cuda triaged,module: cuda triaged
Hijacked package names from nightly repository module: binaries triaged security,module: binaries triaged security
Improve make_fx tracing speed module: performance triaged module: ProxyTensor,module: performance triaged module: ProxyTensor
"false INTERNAL ASSERT FAILED at ""../c10/cuda/CUDAGraphsC10Utils.h"":73, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus32680 triaged module: cuda graphs",triaged module: cuda graphs
Triton Autotuning Cache-Clearing Adds 256MB Memory Overhead triaged bug oncall: pt2,triaged bug oncall: pt2
Exception in distributed context doesn't propagate to child processes launched with multiprocessing oncall: distributed,oncall: distributed
Occassional OverflowError with mps running yolov7 triaged module: mps,triaged module: mps
[Bug][Dataloader] unable to mmap 2048 bytes from file <filename not specified>: Cannot allocate memory (12) module: multiprocessing module: dataloader triaged,module: multiprocessing module: dataloader triaged
Torchrun seems to have problem with virtual environment oncall: distributed triaged oncall: r2p,oncall: distributed triaged oncall: r2p
Unable to export timm models with torch._dynamo triaged module: dynamo,triaged module: dynamo
 Forward arguments are not updated in DDP oncall: distributed triaged,oncall: distributed triaged
Error while building pytorch mobile binaries from source triaged oncall: mobile,triaged oncall: mobile
Implement forward AD with grid_sampler_2d  triaged module: forward ad,triaged module: forward ad
jit testing fails on 3.11 debug build oncall: jit,oncall: jit
[Releng] Add repo dispatch via webhook to trigger domain builds after the core oncall: releng triaged,oncall: releng triaged
autograd.functional.jacobian : Imaginary part is lost for functions with real input and complex output. module: autograd triaged has workaround,module: autograd triaged has workaround
export does not support boolean tensor indexing triaged module: dynamic shapes module: dynamo,triaged module: dynamic shapes module: dynamo
Torch's affinity setting lead to openvino using only one core. triaged module: intel intel priority,triaged module: intel intel priority
An error happend when I convert pytorch model to onnx module: onnx triaged,module: onnx triaged
sympy failure on model when dynamic_shapes=True triaged module: dynamic shapes module: dynamo,triaged module: dynamic shapes module: dynamo
Unknown CUDA graph CaptureStatus21852 needs reproduction triaged,needs reproduction triaged
Torchdynamo with onnxrt backend generating fake tensor errors triaged bug oncall: pt2,triaged bug oncall: pt2
Pytorch is using system-installed mkl-dnn. module: build triaged,module: build triaged
"Hessian produces wrong results, but works if I add a perturbation triaged",triaged
Proxy/cache server option/hooks for downloading model checkpoints and dataset archive files in cloud environment triaged module: hub,triaged module: hub
CUDA error `CUBLAS_STATUS_NOT_INITIALIZED`  module: cuda triaged module: cublas,module: cuda triaged module: cublas
RuntimeError: philox_cuda_state for an unexpected CUDA generator used during capture triaged module: random module: cuda graphs,triaged module: random module: cuda graphs
_pack_padded_sequence fails in dynamo due to requiring a non-fake 2nd argument triage review module: performance bug oncall: pt2 mlperf,triage review module: performance bug oncall: pt2 mlperf
elastic job failed when scale down oncall: distributed triaged oncall: r2p,oncall: distributed triaged oncall: r2p
torchrun elastic always â€œaddress already in useâ€ error oncall: distributed triaged oncall: r2p,oncall: distributed triaged oncall: r2p
Fails to build on ppc64le with clang module: cpu triaged module: vectorization,module: cpu triaged module: vectorization
from torch import * does not import dtypes triaged actionable module: python frontend,triaged actionable module: python frontend
Profiling with stack enabled results in error when Python's cProfile is also running oncall: profiler,oncall: profiler
ONNXRuntime outputs numerically incorrect results for mixed precision models. module: onnx triaged,module: onnx triaged
Lazily start worker threads in the autograd engine module: autograd triaged better-engineering actionable,module: autograd triaged better-engineering actionable
ToTensor deadlock in subprocess module: multiprocessing triaged,module: multiprocessing triaged
Only the first logged trace in a given log dir is visible in tensorboard. triaged module: tensorboard,triaged module: tensorboard
torch.Categorical samples indexes with 0 probability when given logits as argument module: distributions triaged,module: distributions triaged
torchrun default value of command line options oncall: distributed triaged oncall: r2p,oncall: distributed triaged oncall: r2p
jacrev over huber function needs reproduction triaged module: functorch,needs reproduction triaged module: functorch
Adding a page for subfolder/subfile overview/descriptions in the developer wiki module: docs triaged,module: docs triaged
[Bug/functorch] Cannot use `tensor.detach().numpy()` for `GradTrackingTensor`: Cannot access data pointer of Tensor that doesn't have storage triaged module: functorch,triaged module: functorch
Better API for `torch.cov` (and `Tensor.cov`) feature triaged module: python frontend,feature triaged module: python frontend
Inconsistent rank among torch.distributed primitives oncall: distributed triaged,oncall: distributed triaged
"Error while building pytorch from source on windows - Ninja Build Stopped, Subcommand Failed triaged",triaged
CUDA error: initialization error module: cuda triaged module: regression module: nvfuser,module: cuda triaged module: regression module: nvfuser
SymIntType gets translated to int when going through pybind triaged oncall: pt2 module: dynamic shapes,triaged oncall: pt2 module: dynamic shapes
[bazel] replace //c10:headers dependency by //c10 dependency triaged module: bazel,triaged module: bazel
tracing torchvision detection model results in an error oncall: jit module: vision,oncall: jit module: vision
[MPS] Improve the performance of torch.linear() triaged enhancement module: backend module: mps,triaged enhancement module: backend module: mps
linspace (and arange) behaves differently on GPU and CPU triaged,triaged
Add vmap support for torch.linalg.vander good first issue triaged module: functorch,good first issue triaged module: functorch
Segmentation fault after trying to create a tensor with float values needs reproduction module: rocm triaged,needs reproduction module: rocm triaged
Build from source fails: undefined reference to caffe2::DeviceQuery module: build triaged,module: build triaged
[discussion] Analyzing a list of tensors stored as intermediate values / saved_for_backward in autograd graph module: autograd good first issue triaged actionable,module: autograd good first issue triaged actionable
Wrong in building torch from source triaged,triaged
AssertionError: tensor's device must be `meta` when trying to export a fake-initialized module triaged module: fakeTensor,triaged module: fakeTensor
FakeTensors not moving between device properly on Module.cuda() triaged module: fakeTensor,triaged module: fakeTensor
Stochastic Illegal Memory Access error mid-epoch on AWS p4d instances module: cuda triaged module: cublas matrix multiplication,module: cuda triaged module: cublas matrix multiplication
Segmentation fault when running torch.nn.functional.fractional_max_pool3d on torch 1.13.1 needs reproduction module: crash triaged,needs reproduction module: crash triaged
Periodic ROCM distribtued jobs are broken module: rocm triaged,module: rocm triaged
trainer triaged,triaged
Investigate CUDA enabled build-time difference between MSVC and GCC+WSL module: build module: windows triaged,module: build module: windows triaged
Cross-compiled libtorch Windows Arm64 binaries module: windows feature module: ci triaged module: arm,module: windows feature module: ci triaged module: arm
There is no developer documentation about getting started with MPS native debugging module: docs triaged module: mps,module: docs triaged module: mps
MPS: `torch.sub` erroneously returns 0 on outputs of `chunk` via `layer_norm` triaged module: mps,triaged module: mps
"sparse.mm(coo, dense) produces wrong results on T4/V100 GPUs module: sparse triaged",module: sparse triaged
SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model within a company that transforms SSL certificates for security purposes triaged module: hub,triaged module: hub
wrong assert message oncall: quantization triaged,oncall: quantization triaged
vmap + nn.SyncBatchNorm.convert_sync_batchnorm oncall: distributed module: data parallel module: ddp module: functorch,oncall: distributed module: data parallel module: ddp module: functorch
"`mul(CSC, CSC)` fails with layout mismatch between the inputs and the output. module: sparse triaged",module: sparse triaged
Division by zero error when running torch.nn.functional.lp_pool1d triaged module: edge cases,triaged module: edge cases
Crashes of linalg.ldl_solve on different edge cases not coming from linalg.ldl_factor module: crash triaged module: linear algebra module: edge cases,module: crash triaged module: linear algebra module: edge cases
Softmax function slows down for data with large range module: performance module: cuda triaged,module: performance module: cuda triaged
LBFGS wolfe exceeds the maximum allowed iterations module: optimizer triaged actionable,module: optimizer triaged actionable
Add BlockWise Distribution Support to the torch.distributions Package module: distributions triaged,module: distributions triaged
Security policy impractical / lacks contact information? high priority module: docs triaged security,high priority module: docs triaged security
torch.compiled mish function is x5 slower than eager (CPU) triaged bug oncall: pt2 module: cpu inductor,triaged bug oncall: pt2 module: cpu inductor
Build Error: OpenMP library could not be found.  Proceeding might lead to highly sub-optimal performance. module: build triaged module: mkldnn module: third_party,module: build triaged module: mkldnn module: third_party
min/max not supported for Long dtype on MPS triaged module: mps,triaged module: mps
`torch::jit::optimize_for_inference` doesn't preserve exported methods when calling `freeze` oncall: jit,oncall: jit
Segmentation fault when running torch.nn.AdaptiveMaxPool3d triaged module: edge cases,triaged module: edge cases
Overflow when running torch.nn.AdaptiveMaxPool3d on torch 1.12.0 and 1.13.1 triaged module: edge cases,triaged module: edge cases
Segmentation fault when running torch.nn.AdaptiveMaxPool2d triaged module: edge cases,triaged module: edge cases
Overflow when running torch.nn.AdaptiveMaxPool2d triaged module: pooling module: edge cases,triaged module: pooling module: edge cases
Adding label smoothing option to `nn.BCELoss`  and `nn.BCEWithLogitsLoss`? module: nn module: loss triaged actionable,module: nn module: loss triaged actionable
"Python 3.11.1 , even with nightly version of PyTorch: ERROR: No matching distribution found for torch module: binaries triaged",module: binaries triaged
`torch.compile` frees computation graph in a GAN training setup and tries to call `backward` a second time module: autograd triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,module: autograd triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher
The speed of matrix inversion is relatively slow for many small matrices module: performance module: cuda triaged module: linear algebra,module: performance module: cuda triaged module: linear algebra
Cannot cast float64 to float32 needs reproduction module: crash triaged module: macos module: numpy,needs reproduction module: crash triaged module: macos module: numpy
functorch.so is installed back into the source directory module: build triaged module: functorch,module: build triaged module: functorch
[functorch] make batch norm docs point to UX limitations triaged module: functorch,triaged module: functorch
Update map_nt to take into account size and strides triaged module: nestedtensor bug,triaged module: nestedtensor bug
torch.jit.script ERR: RuntimeError: Can't redefine method: forward on class: __torch__.SoSadModule oncall: jit,oncall: jit
Clean up nt impl duplicates where one can triaged better-engineering module: nestedtensor release notes: nested tensor,triaged better-engineering module: nestedtensor release notes: nested tensor
torch.compile loud error on functorch transforms triaged oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher,triaged oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher
torch.compile incorrect when imperative autograd APIs are used high priority module: autograd triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,high priority module: autograd triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher
"Degenerate ranges are allowed in NumPy, but not in PyTorch. triaged module: numpy",triaged module: numpy
using Tensor subclass between vmap layers triaged module: __torch_dispatch__ tensor subclass module: functorch,triaged module: __torch_dispatch__ tensor subclass module: functorch
[bazel] error: use of undeclared identifier 'cudaGraphDebugDotPrint' module: build module: cuda triaged,module: build module: cuda triaged
Pytorch clang-tidy header-filter is still broken module: build triaged,module: build triaged
PyObject preservation and resurrection for `StorageImpl` module: internals triaged enhancement better-engineering,module: internals triaged enhancement better-engineering
getting issue 'typeindex' file not found in Littorch-Lite/install/include/ATen/core/custom_class.h oncall: mobile,oncall: mobile
Internal Assert failed module: cuda triaged module: cuda graphs,module: cuda triaged module: cuda graphs
[RFC] `quantile` should work for `float16`/`half` on the GPU module: cuda triaged enhancement module: half,module: cuda triaged enhancement module: half
"PyTorch memory leak reference cycle in for loop, Mac M1  triaged module: arm module: mps",triaged module: arm module: mps
MPS backend does not accept int64 model weights or input data triaged module: mps,triaged module: mps
[JIT] .backward() not supported by JIT trace oncall: jit,oncall: jit
Perf reduction due to munmap with dataloader pinning thread ? module: dataloader triaged,module: dataloader triaged
"Internal error during ONNX export, diagnostic unusable  module: onnx triaged",module: onnx triaged
Pytorch 1.13 conda package with cuda requires too many unneccessary packages module: binaries module: cuda triaged,module: binaries module: cuda triaged
Support for saving multiple storages/tensors that view same data as different dtypes feature module: serialization triaged,feature module: serialization triaged
Expand torch.utils._pytree.tree_map feature triaged actionable module: pytree module: functorch,feature triaged actionable module: pytree module: functorch
iSTFT produces RuntimeError with center=False and Blackman/Bartlett/Hann windows triaged module: fft,triaged module: fft
[ðŸš€ Feature Request] pdf and sampling from Alpha-stable distribution  module: distributions feature triaged,module: distributions feature triaged
torch.fx tracer emits type error when tracing module that directly contains and uses the torch.cat() function oncall: fx,oncall: fx
custom Function that supports functorch jvp doesn't work with in-place triaged module: forward ad module: functorch,triaged module: forward ad module: functorch
Keep getting ChildFailedError Error oncall: distributed,oncall: distributed
tensor.to_sparse() handling indices incorrectly under dynamo/fake tensor triaged bug oncall: pt2 module: dynamic shapes,triaged bug oncall: pt2 module: dynamic shapes
Make quant_min/quant_max required for observer/fake_quant oncall: quantization triaged,oncall: quantization triaged
Open file leak when dataloader is using persistent_workers and pin_memory AND you create multiple dataloaders.   module: dataloader triaged module: data,module: dataloader triaged module: data
Potential bug found with pybind11 dec_ref while gil released triaged module: pybind shadow review bug,triaged module: pybind shadow review bug
Use dynamo to detect incorrect op schemas automatically triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
Segmentation faults in DataLoader (in latest torch version). needs reproduction module: dataloader triaged,needs reproduction module: dataloader triaged
first class dims leak memeory triaged module: functorch,triaged module: functorch
ONNX export question (using torchdynamo) triaged bug oncall: pt2,triaged bug oncall: pt2
Odd/hand-wavy mathematical notation for Conv2D module: docs triaged,module: docs triaged
dcp resharding does not work for optimizer state_dict triaged module: distributed_checkpoint,triaged module: distributed_checkpoint
functorch.functionalize doesn't work with torch.autograd.grad triaged module: functorch,triaged module: functorch
onednn(mkldnn) backend support for quantized operators oncall: quantization triaged module: arm,oncall: quantization triaged module: arm
[FSDP] FSDP with CPU offload consumes `1.65X` more GPU memory when training models with most of the params frozen high priority triage review oncall: distributed triaged module: fsdp,high priority triage review oncall: distributed triaged module: fsdp
`quantile` fails for `float16`/`half` inputs module: cpu triaged enhancement module: half,module: cpu triaged enhancement module: half
[Composable] Enable summon_full_params for fully_shard oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
[BE] Investigate FSDP test _zero_model  oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Remove redundant memory copy for HF multi-attention submodule for cpu path using MKL prepack triaged bug oncall: pt2 module: cpu inductor,triaged bug oncall: pt2 module: cpu inductor
Implement L1 and L2 gradient as hooks with the option of changing the weight decay value. module: nn module: optimizer triaged needs design,module: nn module: optimizer triaged needs design
Unexpected behavior when running torch.max in cuda needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
"If minifier test fails, stderr/stdout of subprocess calls is not printed triaged bug",triaged bug
Simple deleting from the sys cache fails on reimport triaged enhancement module: python frontend,triaged enhancement module: python frontend
A Simple Function Causing Graph Break triaged bug,triaged bug
overflow (?) on cuda tensor after matrix multiplication module: numerical-stability module: cuda triaged,module: numerical-stability module: cuda triaged
"Crash in `index_select` with singleton `self`, non-singleton `index` triaged",triaged
as_strided_scatter : INTERNAL_ASSERT_FAILED for requires_grad=True and non-config input triaged module: functionalization,triaged module: functionalization
TorchDynamo doesn't inline modified nn.Modules forward - Fails with Huggingface Accelerate high priority triaged bug oncall: pt2,high priority triaged bug oncall: pt2
[Composable] Enable setting state_dict_type high priority triage review oncall: distributed triaged module: fsdp,high priority triage review oncall: distributed triaged module: fsdp
Support for Transformer Models on Android with Vulkan Backend triaged module: vulkan,triaged module: vulkan
Functorch does not work with CrossEntropyLoss and label=-100 triaged actionable module: functorch,triaged actionable module: functorch
Torch SummaryWriter import fails with torch 2.0 with an error on numpy.object module: tensorboard oncall: visualization,module: tensorboard oncall: visualization
Error in guard code crashes process NULL ERROR: /Users/ezyang/Dev/pytorch-metal/torch/csrc/dynamo/eval_frame.c:251 triaged bug oncall: pt2,triaged bug oncall: pt2
Retrieve Tensor from Tensor.data_ptr() triaged core issue,triaged core issue
Check that SymPy semantics match Python semantics triaged,triaged
ModuleNotFoundError: No module named 'torch._C._distributed_c10d'; 'torch._C' is not a package module: build triaged,module: build triaged
Umbrella issue for weakref related Dynamo PyTorch test suite failures triaged bug oncall: pt2,triaged bug oncall: pt2
Umbrella issue for only populate real_value_cache in export test suite fails triaged bug oncall: pt2,triaged bug oncall: pt2
Support arbitrary masks for _nested_tensor_from_mask in nn.TransformerEncoder triaged module: nestedtensor oncall: transformer/mha,triaged module: nestedtensor oncall: transformer/mha
Umbrella issue for PyTorch test suite failures from torch.* returned non-Tensor output unimplemented triaged bug oncall: pt2,triaged bug oncall: pt2
[FSDP] Prepare to deprecate `FullyShardedDataParallel.<attrs>` oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
[FSDP] `fully_shard` Follow-Ups & Known Issues oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Error when using torch.compile with Pytorch2.0 needs reproduction triaged oncall: pt2,needs reproduction triaged oncall: pt2
Compare oneDNN and OpenBLAS backend of PyTorch on arm64 architecture module: cpu triaged module: bfloat16 module: arm,module: cpu triaged module: bfloat16 module: arm
Support for Pylint module: lint triaged,module: lint triaged
Support `divmod` for tensors triaged module: numpy,triaged module: numpy
nn.CrossEntropyLoss error out when the sample size is large module: nn module: cuda module: memory usage triaged module: edge cases,module: nn module: cuda module: memory usage triaged module: edge cases
"[Composable API] Add `fully_shard` state dict unit test after manual ""wrapping"" is supported oncall: distributed triaged module: fsdp",oncall: distributed triaged module: fsdp
[FSDP] Investigate `test_fsdp_pure_fp16.py` inaccuracy oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
"Extend ""torch.utils.cpp_extension.load"" for both lib64 and **lib** module: cpp-extensions triaged",module: cpp-extensions triaged
Large slow down by not calling `torch.set_num_threads` module: performance module: cpu triaged module: linear algebra,module: performance module: cpu triaged module: linear algebra
Adam (fused=True) issues module: performance module: optimizer module: cuda triaged,module: performance module: optimizer module: cuda triaged
pytorch prune in libtorch module: cpp triaged,module: cpp triaged
Adopt full_backward_pre_hook in DDP oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
test_copy_broadcast triaged,triaged
PixelShuffle/Unshuffle Channels Last Support on GPU triaged module: memory format,triaged module: memory format
RuntimeError: kind_.is_prim() INTERNAL ASSERT FAILED. Only prim ops are allowed to not have a registered operator but aten::mul doesn't have one either. We don't know if this op has side effects. module: onnx triaged module: primTorch,module: onnx triaged module: primTorch
`torch.empty` produces incorrect tensors with `layout=sparse_csr|sparse_csc` on the CPU module: sparse module: docs module: cpu triaged,module: sparse module: docs module: cpu triaged
In distributed get SIGTERM and run crash oncall: distributed,oncall: distributed
RuntimeError: Error in dlopen: libnvJitLink.so.12: cannot open shared object file: No such file or directory module: crash module: distributions triaged,module: crash module: distributions triaged
[Feature Request] An alternative sampling routine for Dirichlet to fix Dirichlet and Beta sampling bugs module: distributions feature triaged,module: distributions feature triaged
[FSDP][BE] `test_fsdp_comm_hooks.py` cleanup oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
torch.min document not up to date module: docs triaged,module: docs triaged
`torch.inverse` multi-threading RuntimeError: lazy wrapper should be called at most once triaged module: multithreading module: linear algebra,triaged module: multithreading module: linear algebra
Operator overload priority should not rely on static initialization order oncall: jit,oncall: jit
[threaded pg] All threads share one Random Number Generator triaged module: random module: dtensor,triaged module: random module: dtensor
AttributeError: 'tuple' object has no attribute 'grad' triaged,triaged
Bfloat16 tensor .numpy() support  triaged module: numpy module: bfloat16,triaged module: numpy module: bfloat16
"[discussion, idea] Batched, vectorized base64 decoding / encoding + maybe RLE decoding / encoding feature triaged module: vision module: nestedtensor",feature triaged module: vision module: nestedtensor
[RFC] Add torch.backends.tbb.is_available() triaged enhancement module: tbb,triaged enhancement module: tbb
Embedding dynamic quantization is not documented and hard to use oncall: quantization triaged,oncall: quantization triaged
Could not run 'aten::as_strided' with arguments from the 'Metal' backend. triaged module: ios,triaged module: ios
Unable to link LibTorch against CUDA and CUDNN statically triaged module: static linking,triaged module: static linking
[Dispatchable Collectives] Follow up tasks oncall: distributed triaged,oncall: distributed triaged
torch.compile() BackendCompilerFailed: _compile_fn raised RuntimeError needs reproduction triaged bug oncall: pt2 module: dynamo,needs reproduction triaged bug oncall: pt2 module: dynamo
Illegal hardware instruction following Real Time Inference on Raspberry Pi 4 tutorial oncall: quantization low priority triaged module: arm,oncall: quantization low priority triaged module: arm
Illegal hardware instruction using torch.nn.Conv2d on aarch64 (Raspberry Pi 4) module: crash triaged,module: crash triaged
valgrind failure `Conditional jump or move depends on uninitialised value(s)` triaged module: sanitizers,triaged module: sanitizers
[A ERROR in Docker] RuntimeError: CUDA error: no kernel image is available for execution on the device needs reproduction module: cuda triaged module: docker,needs reproduction module: cuda triaged module: docker
Tensor indexing and slicing documentation should explicitly state that indexing follows numpy semantics and link to the numpy indexing documentation. module: docs triaged module: numpy,module: docs triaged module: numpy
Internal assert when ctx.saved_tensors fails when saving results of an intermediate view tensor with torch.utils.checkpoint and use_reentrant=False module: checkpoint module: autograd triaged has workaround actionable,module: checkpoint module: autograd triaged has workaround actionable
Saving a scripted module to a buffer does not work. oncall: jit,oncall: jit
[FSDP] Revisit meta device initialization oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
PR #89436 looks like it causes or enables a memory leak module: memory usage triaged module: mps,module: memory usage triaged module: mps
Strange issue with tensor asyncio and RPC oncall: distributed,oncall: distributed
Different behavior for complex numbers operations with numpy triaged module: complex module: numpy module: NaNs and Infs,triaged module: complex module: numpy module: NaNs and Infs
A few functions in fbgemm_utils.cpp are defined in global namespace module: cpp oncall: quantization triaged,module: cpp oncall: quantization triaged
Importing numpy makes Tensor min max crash triaged module: numpy,triaged module: numpy
IValue(c10::List<IValue>) constructor is confusing and undocumented module: internals triaged,module: internals triaged
"Cannot add target-level dependencies to non-existent target ""gloo_cuda"". module: build triaged",module: build triaged
torch.utils.tensorboard import fails if a new protobuf > 3.20 is installed (bug in tensorboard/tensorflow but better guard against it) oncall: visualization,oncall: visualization
"""Reached a code path in Module.get_extra_state() that should never be called."" needs reproduction oncall: jit triaged",needs reproduction oncall: jit triaged
[JIT] Wrong type inference leads to misleading error message oncall: jit,oncall: jit
Get the error: AttributeError: Can't pickle local object 'convert_frame.<locals>._convert_frame' high priority triage review triaged bug oncall: pt2 module: dynamo,high priority triage review triaged bug oncall: pt2 module: dynamo
[JIT] INTERNAL ASSERT FAILED `torch.add` with boolean primitive constant oncall: jit,oncall: jit
[JIT] INTERNAL ASSERT FAILED `torch.mul` with boolean primitive constant oncall: jit,oncall: jit
[JIT] INTERNAL ASSERT FAILED when dispatching for `torch.Tensor.view` oncall: jit,oncall: jit
[Tracking Issue] Mixed precision does not work with ignored modules  oncall: distributed module: fsdp,oncall: distributed module: fsdp
Inconsistent Hash of IValue between aten/src/ATen/core/ivalue.cpp and aten/src/ATen/core/Dict_inl.h oncall: jit,oncall: jit
torch._dynamo.exc.Unsupported: dynamic shapes: arange triaged bug oncall: pt2,triaged bug oncall: pt2
quantization qconfig: can we set per-channel quant as default for qnnpack? oncall: quantization triaged,oncall: quantization triaged
quantization observers: can we relax the default epsilon value? oncall: quantization low priority triaged,oncall: quantization low priority triaged
Public API definition is not compatible with `torch.testing` triaged module: testing module: python frontend,triaged module: testing module: python frontend
cannot backward() needs reproduction triaged module: mps,needs reproduction triaged module: mps
Why torch.mode return different value between CPU and GPU module: cuda triaged,module: cuda triaged
LibTorch static build from source missing libshm.so module: build triaged,module: build triaged
[Distributed] `Invalid scalar type` when `dist.scatter()` boolean tensor oncall: distributed,oncall: distributed
Strategy for optimizing away transient dynamic shapes / device syncs triaged bug oncall: pt2,triaged bug oncall: pt2
Graph breaks with HuggingFace Stable Diffusion triaged bug,triaged bug
Unexpected behaviour of 1.13.0 triaged module: advanced indexing,triaged module: advanced indexing
wav2vec2 model: error trying to do inference triaged bug,triaged bug
p.block != nullptr && p.block->ptr != nullptr INTERNAL ASSERT FAILED needs reproduction triaged,needs reproduction triaged
torch._dynamo.exc.BackendCompilerFailed: compile_fx raised TypeError: tqdm.__init__() got an unexpected keyword argument 'desc' triaged module: hub,triaged module: hub
Couldn't install pytorch 2.0 needs reproduction module: binaries triaged,needs reproduction module: binaries triaged
documentation need to be as per python version module: docs triaged,module: docs triaged
Tensor.uniform_ fails to compile when using torch._dynamo triaged module: initialization module: dynamo,triaged module: initialization module: dynamo
[GradScaler] Inconsistent scale values across different GPUs caused by uneven inputs for AMP DDP training triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
forward-mode AD formula for torch.add (and possibly others) accidentally upcasts float32 to float64 module: performance module: autograd triaged actionable ZeroTensor module: forward ad module: functorch,module: performance module: autograd triaged actionable ZeroTensor module: forward ad module: functorch
DDP overlapped optimizer: set grads to None enhancements oncall: distributed module: data parallel module: ddp,oncall: distributed module: data parallel module: ddp
Traceable tensor subclasses cannot actually be used with AOTAutograd triaged bug,triaged bug
TensorWithTFOverrideVariable unwraps too early triaged bug,triaged bug
"Can not use x=torch.tensor(b), to create a Tensor out of a List[List[Tensor]] (A List of Lists of Tensors) oncall: jit triaged",oncall: jit triaged
Support getattr/setattr user properties on Tensor triaged bug,triaged bug
"Error in Adam.step(): If capturable=True, params and state_steps must be CUDA tensors. module: optimizer triaged actionable",module: optimizer triaged actionable
minified code can not produce fp64_ref result triaged,triaged
Calling item() on symbolic shape fake tensor should give more clear error message triaged module: fakeTensor,triaged module: fakeTensor
"Random sampling from a tensor constructed on MPS device, results in elements returning as torch.zeros(tensor[i].shape) triaged module: mps",triaged module: mps
Random K compression hook in PyTorch DDP feature triaged module: ddp,feature triaged module: ddp
nn.CrossEntropy/nn.NLLLoss : Request for option to specify invalid ignore_index for perf. optimization module: loss triaged,module: loss triaged
Way to run accuracy minifier on only one particular subgraph triaged bug,triaged bug
[RFC] PyTorch Tensor Parallel(TP) User API for Distributed Training triaged module: dtensor,triaged module: dtensor
Performance regression on interpolation in Kornia triaged bug,triaged bug
torch1.13  quantized model export onnx error module: onnx triaged,module: onnx triaged
Wrong output type hint for `F.one_hot` module: typing triaged,module: typing triaged
"Basic math operations produce a ""floating point exception"" needs reproduction module: crash module: cpu triaged",needs reproduction module: crash module: cpu triaged
InvokeAI using MPS is broken by torch nightlies since torch-1.14.0.dev20221104 inclusive  triaged module: correctness (silent) module: mps,triaged module: correctness (silent) module: mps
addcmul on CUDA does not have the correct FMA behavior module: numerical-stability module: cuda triaged,module: numerical-stability module: cuda triaged
MMDet 3.x cannot run successfully in inductor mode triaged bug,triaged bug
third-order gradient of torch.pow with tensor args and certain input returns NaN module: autograd triaged module: NaNs and Infs,module: autograd triaged module: NaNs and Infs
torch.addbmm throws different exception differences on CPU and GPU. module: cuda module: error checking triaged,module: cuda module: error checking triaged
Sample Weighted BatchNorm1d feature module: nn triaged needs research module: norms and normalization,feature module: nn triaged needs research module: norms and normalization
`torch.Tensor.flatten` Trigger Segmentation Fault when trying to provide and output named dim module: crash triaged module: named tensor,module: crash triaged module: named tensor
Segfault on torch.nn.functional.one_hot with large tensor on Python 3.9 needs reproduction module: crash triaged,needs reproduction module: crash triaged
M1 mps issue triaged module: mps,triaged module: mps
Enable NCCL for PyTorch on Windows oncall: distributed module: windows triaged,oncall: distributed module: windows triaged
Dynamo is over-guarding on Tensor locals triaged bug,triaged bug
MultiProcess tests fail when run on nodes with 1 GPU oncall: distributed triaged,oncall: distributed triaged
PTX codegen race? triaged bug oncall: pt2,triaged bug oncall: pt2
`positive_semidefinite` constraint fails on CUDA 11.7 module: distributions triaged module: linear algebra,module: distributions triaged module: linear algebra
MPS bug on `torch.transpose` and `torch.log` triaged module: mps,triaged module: mps
MPS device ComplexFloat triaged module: fft module: mps,triaged module: fft module: mps
torchinductor tests attempt to access internet triaged bug,triaged bug
[dynamo] RuntimeError: Failed running call_function aten.nll_loss_backward(*(FakeTensor(FakeTensor(... triaged module: dynamo,triaged module: dynamo
[dynamo] RuntimeError: Failed running call_function aten.convolution_backward(*(FakeTensor(FakeTensor(.. triaged module: dynamo,triaged module: dynamo
[dynamo] RuntimeError: Failed running call_function aten.lift_fresh_copy(*(FakeTensor(FakeTensor(... triaged module: dynamo,triaged module: dynamo
"Can not access to ""sbgemm"" routine with user-defined OpenBLAS module: build triaged module: bfloat16 module: openblas",module: build triaged module: bfloat16 module: openblas
NVFuser failing masked.{amax|amin|sum} extremal and correctness tests triaged module: nvfuser,triaged module: nvfuser
Building PyTorch with Vulkan backend fails (1.13 and master) triaged module: vulkan,triaged module: vulkan
Caching a model's weights and state_dict to disk to save RAM module: nn triaged,module: nn triaged
Finish deprecation of autograd decorator over class objects module: autograd triaged,module: autograd triaged
[accuracy] [aot_eager] mobilenet_v2_quantized_qat fails accuracy  triaged bug oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,triaged bug oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher
Partitioner generates useless constant SymInt edges between forward-backwards triaged bug oncall: pt2,triaged bug oncall: pt2
AOTAutograd generates useless tangent inputs for SymInt outputs triaged bug oncall: pt2,triaged bug oncall: pt2
Feature Request: deterministic CUDA cumsum feature module: cuda triaged module: determinism,feature module: cuda triaged module: determinism
build: cmake: functorch.so not installed at expected location module: build triaged,module: build triaged
build: cmake: need to uniformize installation of libraries in CMAKE_INSTALL_LIBDIR (not lib) module: build module: cpp triaged needs research,module: build module: cpp triaged needs research
"kind_.is_prim() INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/ir.cpp"":1098 oncall: jit module: cpp",oncall: jit module: cpp
Unexpected behavior from torchscript (mixing trace with script) oncall: jit,oncall: jit
"torch.split: argument 'split_sizes' (position 1) must be tuple of ints, not list triaged module: sorting and selection",triaged module: sorting and selection
Higher order derivatives of sinc explode triaged module: derivatives actionable,triaged module: derivatives actionable
Partitioner that doesn't require functionalized graph triaged bug oncall: pt2,triaged bug oncall: pt2
Accuracy minifier can find spurious accuracy failures involving uninitialized memory triaged bug oncall: pt2,triaged bug oncall: pt2
Accuracy minifier should also work even if an exception is raised triaged bug oncall: pt2,triaged bug oncall: pt2
Allow `low` and `high` to be tensors in `torch.randint` feature triaged module: random,feature triaged module: random
The problem caused by the parameter dim of torch.norm module: error checking triaged module: edge cases,module: error checking triaged module: edge cases
fx.wrap is ignored with make_fx proxy tensor tracer triaged module: fx,triaged module: fx
Edge case: torch.baddbmm supports double x int8 x int8 inputs on CPU but not CUDA triaged module: type promotion module: linear algebra module: edge cases,triaged module: type promotion module: linear algebra module: edge cases
torch.equal can still run successfully when the parameter types are different. triaged module: numpy module: type promotion,triaged module: numpy module: type promotion
"torch.floor_divide: The dividend of torch.floor_divide is set to 0, but it can still run on the GPU. module: error checking triaged",module: error checking triaged
OSError: libcublas.so.11: cannot open shared object file: No such file or directory module: binaries triaged,module: binaries triaged
"When the torch.masked_select operator passes in the same parameters, it behaves differently on CPU and GPU. triaged module: masked operators",triaged module: masked operators
torch.nn.MultiLabelMarginLoss has different performance on CPU and GPU. module: loss triaged,module: loss triaged
[MPS] Using unsqueeze in inference mode returns anomalous result triaged inference mode module: mps,triaged inference mode module: mps
torch.nn.TransformerEncoderLayer missing exception description information. module: nn module: error checking triaged oncall: transformer/mha,module: nn module: error checking triaged oncall: transformer/mha
Edge case: CPU bool abs is not supported triaged module: numpy module: edge cases,triaged module: numpy module: edge cases
How can i patch the torch.jit in the second solution? Could not figure out entrypoint ? oncall: jit,oncall: jit
torch.nn.ReplicationPad1d:The description of the exception information thrown is not accurate module: error checking triaged module: padding,module: error checking triaged module: padding
prod_cpu not implemented for 'BFloat16' module: cpu triaged module: bfloat16,module: cpu triaged module: bfloat16
torch.nn.functional.normalize: whether true is equal to 1 triaged module: norms and normalization,triaged module: norms and normalization
RuntimeError: CUDA error: device-side assert triggered module: loss triaged,module: loss triaged
"torch.nn.functional.embedding_bag throws an exception when it runs on a CPU, but it runs successfully on a GPU. module: nn module: cuda module: error checking triaged actionable module: embedding",module: nn module: cuda module: error checking triaged actionable module: embedding
Documentation: torch.nn.functional.embedding docs could more clearly state the requirement that weight be a 2D tensor module: docs module: nn triaged module: embedding topic: docs,module: docs module: nn triaged module: embedding topic: docs
Quantizable LSTM has different behavior than LSTM in bidirectional setting oncall: quantization module: rnn triaged,oncall: quantization module: rnn triaged
Per-sample input xfail / test generation triaged module: testing,triaged module: testing
AdaptiveAvgPool1d failed in the lower version module: nn triaged module: pooling,module: nn triaged module: pooling
AdaptiveAvgPool1d throws different exceptions when using the gpu module: nn module: cuda module: error checking triaged actionable module: pooling,module: nn module: cuda module: error checking triaged actionable module: pooling
torch.mm: Exceptions thrown on the CPU and GPU are inconsistent module: cuda module: error checking triaged,module: cuda module: error checking triaged
"Conv2d error on M1 mac, RuntimeError: NNPACK SpatialConvolution_updateOutput failed module: convolution triaged module: macos module: arm",module: convolution triaged module: macos module: arm
`masked_fill` with `FloatTensor` mask will never mask but fails silently. triaged module: correctness (silent) module: masked operators,triaged module: correctness (silent) module: masked operators
code sharing for fundamental ops in quantization oncall: quantization triaged,oncall: quantization triaged
Meta implementation for copy_ is wrong triaged module: meta tensors,triaged module: meta tensors
fbgemm_avx512 build failure module: build triaged,module: build triaged
[Inductor] [CPU] Crash failure in torchbench model mobilenet_v2_quantized_qat & resnet50_quantized_qat triaged bug oncall: pt2,triaged bug oncall: pt2
torch.randn and torch.normal sometimes produce NaN on mps device triaged module: random module: mps,triaged module: random module: mps
"torch.addcdiv: input, tensor1, and tensor2 parameters should be of the same type module: docs triaged module: assert failure",module: docs triaged module: assert failure
torch.lobpcg should support black-box linear operators like SciPy feature triaged module: linear algebra,feature triaged module: linear algebra
"`torch.nn.ReplicationPad2D` Report ""invalid configuration argument"" Error under Compute Sanitizer module: nn module: error checking triaged module: padding",module: nn module: error checking triaged module: padding
sm_80 support module: cuda triaged,module: cuda triaged
"Can't use JIT modules traced with AMP autocast, with Triton Server (or any C++ environment) - freeze() issue ? oncall: jit",oncall: jit
Dynamo + NNC: incorrect results with in-place ops on inputs triaged NNC module: dynamo,triaged NNC module: dynamo
"`torch.nn.LayerNorm` Abort with ""invalid device ordinal"" Error module: nn module: error checking triaged module: norms and normalization",module: nn module: error checking triaged module: norms and normalization
`torch.nn.CTCLoss` Trigger out-of-bound Read under Compute Sanitizer module: nn module: loss module: cuda triaged module: sanitizers,module: nn module: loss module: cuda triaged module: sanitizers
Libtorch's CPU inference is much slower on Windows than on Linux module: performance module: windows triaged,module: performance module: windows triaged
[aot-autograd] [hf_BigBird] Output 0 of CompiledFunctionBackward is a view and is being modified inplace triaged ezyang's list bug module: aotdispatch module: pt2-dispatcher,triaged ezyang's list bug module: aotdispatch module: pt2-dispatcher
Got many TestDTensorOpsCUDA.test_dtensor_op_db_X test failures module: cuda triaged,module: cuda triaged
[FSDP] Investigate Unit Testing when Gradient Computation Differs on CPU/GPU oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
torch.normal(...) on MPS sometimes produces NaN's triaged module: mps,triaged module: mps
binary_cross_entropy/bce_with_logits (+ other loss functions) for nested_tensor module: loss triaged module: nestedtensor,module: loss triaged module: nestedtensor
Zero-copy way to make flat tensor into a nested_tensor given a shape triaged module: nestedtensor,triaged module: nestedtensor
jit.script() fails to resolve/cast Optional[Tensor] fields of sub-modules or base classes of the object being scripted oncall: jit,oncall: jit
Bad string in GLSL shader triaged module: vulkan topic: build,triaged module: vulkan topic: build
pytreeify decorators feature triaged module: pytree module: functorch,feature triaged module: pytree module: functorch
Unable to backprop through dense weighted sum of sparse_coo_tensors module: sparse triaged,module: sparse triaged
view_copy out= does not reshape zero element tensors triaged module: viewing and reshaping module: functionalization,triaged module: viewing and reshaping module: functionalization
"A more systematic API for resolving the ""vmap-incompatible in-place operation"" error triaged actionable needs design module: functorch",triaged actionable needs design module: functorch
Extend test_proxy_tensor tests to support ops test non floating point types module: tests triaged module: ProxyTensor,module: tests triaged module: ProxyTensor
Add a `device` keyword argument to `torch.manual_seed` triaged module: random,triaged module: random
caffe2_interface_library CMake macro prevents linking to LibTorch as a transitive dependency module: cpp triaged,module: cpp triaged
"cross compile pytoch using cmake , get an error : protobuf::protoc: command not found needs reproduction module: build triaged",needs reproduction module: build triaged
[PT][1.13] torch .numpy() fn broke for some scenario module: cpu triaged module: numpy module: regression actionable ZeroTensor,module: cpu triaged module: numpy module: regression actionable ZeroTensor
Add smoke-tests for CPP extensions compilations high priority module: cpp-extensions triaged module: regression,high priority module: cpp-extensions triaged module: regression
Incorrect version in the instructions on official website module: docs triaged,module: docs triaged
nvprims.native_batch_norm doesn't support fake tensor inputs triaged module: nvfuser module: primTorch,triaged module: nvfuser module: primTorch
Glog macro redefinition problem when including headers from both libtorch and glog module: build triaged,module: build triaged
M1 runner i-090e1df32b6f48a20 run out of disk space module: ci triaged module: m1,module: ci triaged module: m1
`torch.nn.functional.embedding_bag` Trigger RuntimeError under UndefinedBehaviorSanitizer triaged module: sanitizers,triaged module: sanitizers
`torch.set_rng_state` Trigger RuntimeError under UndefinedBehaviorSanitizer triaged module: sanitizers,triaged module: sanitizers
torch.linalg.matrix_rank memory leak needs reproduction module: memory usage triaged module: linear algebra,needs reproduction module: memory usage triaged module: linear algebra
`torch.Tensor.msort` Trigger RuntimeError under UndefinedBehaviorSanitizer triaged module: sanitizers,triaged module: sanitizers
`torch.linalg.eigvals` Trigger RuntimeError under UndefinedBehaviorSanitizer triaged module: sanitizers,triaged module: sanitizers
`torch.topk` Trigger RuntimError under UndefinedBehaviorSanitizer triaged module: sanitizers,triaged module: sanitizers
`torch.vander` Trigger RuntimeError with UndefinedBehaviorSanitizer triaged module: linear algebra module: sanitizers module: edge cases,triaged module: linear algebra module: sanitizers module: edge cases
`torch.svd_lowrank` Trigger RuntimeError under UndefinedBehaviorSanitizer module: sparse triaged module: linear algebra actionable module: sanitizers,module: sparse triaged module: linear algebra actionable module: sanitizers
`torch.linalg.lstsq` Trigger RuntimeError under UndefinedBehaviorSanitizer triaged module: linear algebra actionable module: sanitizers module: edge cases,triaged module: linear algebra actionable module: sanitizers module: edge cases
INTERNAL ASSERT FAILED. Missing scalar type infromation. triaged module: nvfuser,triaged module: nvfuser
"torchdynamo is not properly setting up input tracking (e.g., for symbolic shape guards) for view bases triaged bug oncall: pt2",triaged bug oncall: pt2
MPS test_numpy_ref_mps_nn_functional_group_norm_mps_float32 is flaky? triaged module: flaky-tests module: mps,triaged module: flaky-tests module: mps
[dynamo+ddp+symbolic-shapes] Issue Tracker triaged oncall: pt2,triaged oncall: pt2
RuntimeError: derivative for aten::mps_max_pool2d_backward is not implemented triaged module: mps,triaged module: mps
Investigate why `test_aot_autograd_symbolic_exhaustive_masked_median_cpu_float32` is flaky triaged module: flaky-tests,triaged module: flaky-tests
Can't import torch --> OSError related to libcublasLt.so.11 module: binaries module: cuda triaged module: regression,module: binaries module: cuda triaged module: regression
test/test_ops.py is segfaulting on master build with DEBUG assets high priority module: ci triaged module: testing,high priority module: ci triaged module: testing
Inductor may merge two output tensors into one triaged bug module: inductor internal ramp-up task,triaged bug module: inductor internal ramp-up task
Return the attention weights using the Transformer Encoder class.  triaged oncall: transformer/mha,triaged oncall: transformer/mha
[feature request] Get/set fastmath CPU bit (and some other FPU flags?) module: numerical-stability module: cpu triaged,module: numerical-stability module: cpu triaged
ImportError: libcupti.so.11.2: cannot open shared object file: No such file or directory module: build module: cuda triaged,module: build module: cuda triaged
Quantization error between fake-quantized model and quantized model using the new observer oncall: quantization low priority triaged,oncall: quantization low priority triaged
Potential bug in torch.optim.lr_scheduler.CosineAnnealingWarmRestarts triaged actionable module: LrScheduler,triaged actionable module: LrScheduler
Batched Random Number Generators triaged module: random,triaged module: random
torch.jit.trace() - AttributeError: 'NoneType' object has no attribute '__module__ oncall: jit,oncall: jit
scatter_ op convert onnx exception module: onnx triaged onnx-needs-info,module: onnx triaged onnx-needs-info
forward AD for _euclidean_dist module: autograd triaged actionable module: forward ad module: functorch,module: autograd triaged actionable module: forward ad module: functorch
Consolidate binary build matrix for core and validation workflows module: ci triaged,module: ci triaged
"nn.Linear allocate too many space which lead to CPUAllocator ""allocate memory failure"" if it's BF16. good for FP32. module: cpu triaged intel",module: cpu triaged intel
Minifier crash triaged bug oncall: pt2,triaged bug oncall: pt2
`MultiMarginLoss` doesn't check the value of `target` on CUDA module: cuda module: error checking triaged,module: cuda module: error checking triaged
`ConvTranspose` fails on CPU but returns an empty tensor on CUDA module: cuda module: error checking triaged,module: cuda module: error checking triaged
pack_sequence() always fail after set_default_tensor_type to CUDA module: rnn triaged,module: rnn triaged
CUDA unknown error after suspend during debugging module: cuda triaged,module: cuda triaged
GitHub first-time contributors box pops up unexpectedly module: ci triaged,module: ci triaged
"The libtorch tests Simplify.{SimplifySymbolicMinMax,SimplifyNestedMax,SimplifyNestedMin} fail on Apple Silicon oncall: jit module: m1",oncall: jit module: m1
The libtorch test SequentialTest.ModuleForwardMethodOptionalArg fails on Apple Silicon module: build triaged module: m1,module: build triaged module: m1
The libtorch test TestScalarTensor.TestScalarTensorMPS fails on Apple Silicon triaged actionable module: mps module: m1,triaged actionable module: mps module: m1
The libtorch test ConstantPropagation.CustomClassesCanBePropagated fails on Apple Silicon oncall: quantization low priority triaged module: m1,oncall: quantization low priority triaged module: m1
Bernoulli uses legacy contiguous memory format triaged module: memory format,triaged module: memory format
quantization convert should warn the user if calibration has not happened oncall: quantization triaged,oncall: quantization triaged
"Despite having aten::diag_embed.out, torch.diag_embed doesn't support out= argument triaged module: codegen",triaged module: codegen
`pack_padded_sequence` not compatible with deterministic mode it calls `torch.scatter` module: rnn triaged module: determinism,module: rnn triaged module: determinism
"cpp_extension CUDA library path hard-coded as ""lib64"" but may be ""lib"" module: cpp-extensions triaged actionable",module: cpp-extensions triaged actionable
[Quant] Validate FixedQParams observers in eager mode oncall: quantization low priority triaged,oncall: quantization low priority triaged
What causes CPU to degrade when I load the weight with torch.hub.load() module: cpu triaged module: hub intel,module: cpu triaged module: hub intel
`nn.functional.embedding_bag` Trigger out-of-bound Read under Compute Sanitizer module: cuda triaged,module: cuda triaged
We probably are allowing mutations to happen on fake tensor in VariableTracker triaged bug module: dynamo,triaged bug module: dynamo
quantization: error message when using `convert_fx` on a model on cuda should be better oncall: quantization triaged,oncall: quantization triaged
Don't store example_value on FX node meta triaged bug,triaged bug
torch.set_grad_enabled results in RuntimeError with torch.jit.script oncall: jit,oncall: jit
Mixed precision training fails due to NaN in batch norm running_mean triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
MaxPool1D output shapes can be negative when ceil_mode=True module: nn triaged module: pooling,module: nn triaged module: pooling
`unique` will reverse the input when `sort=False` on cpu (not sorting) module: cpu triaged intel,module: cpu triaged intel
Hang: sampling VonMises distribution gets stuck in rejection sampling for small kappa high priority module: distributions module: cpu triaged module: numpy module: deadlock,high priority module: distributions module: cpu triaged module: numpy module: deadlock
Enable AMP for MPS devices feature triaged module: amp (automated mixed precision) module: mps,feature triaged module: amp (automated mixed precision) module: mps
benchmark cache persist module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
Unit test with `--subprocess` command doesn't respect the `-k` filter flag and runs all available sub tests module: ci module: tests triaged module: testing,module: ci module: tests triaged module: testing
Whether to support libtorch source code compilation of C++11 ï¼Ÿ module: build triaged,module: build triaged
cuDNN error (CUDNN_STATUS_NOT_SUPPORTED) for torch.nn.functional.grid_sample() module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
[PrimTorch] Functionalization pass removes Instance Norm / Batch Norm running stats transformations high priority triaged module: primTorch,high priority triaged module: primTorch
TorchBench - moco - RuntimeError: Tensors must be CUDA and dense triaged bug,triaged bug
MSE documentation is weak module: docs triaged,module: docs triaged
Group losses in a common namespace module: nn module: loss triaged needs research,module: nn module: loss triaged needs research
`torch.load()` cannot load data saved at non-zero position in a file (`failed finding central directory`) module: serialization triaged,module: serialization triaged
ASAN shard 4 started to OOM after unrelated commit module: ci triaged,module: ci triaged
AttributeError: module 'tensorboard.compat.tensorflow_stub.io.gfile' has no attribute 'MakeDirs' triaged module: tensorboard,triaged module: tensorboard
Diffuser pipeline device attribute broken when using optimized model triaged bug,triaged bug
build: failure when upgrade oneTBB to 2021.7.0 module: build triaged module: tbb,module: build triaged module: tbb
"Hessian is (incorrectly) zero when using MPS on M1 Mac, but not on cpu  triaged module: mps module: functorch",triaged module: mps module: functorch
Enable `torch.topk` to support `stable` flag  triaged module: sorting and selection,triaged module: sorting and selection
Add a config option to raise errors instead of warnings in nvFuser integration triaged module: nvfuser,triaged module: nvfuser
`torch.nn.RReLU` not reporting `lower > upper` on CUDA module: cuda triaged,module: cuda triaged
Moving tensor to GPU by .cuda() gets stucked when AMD Secure Encripted Virtualization (SEV) is activated module: cuda module: rocm triaged,module: cuda module: rocm triaged
`torch.mm` Trigger RuntimeError with UndefinedBehaviorSanitizer triaged module: sanitizers,triaged module: sanitizers
â˜‚ï¸  Issues that trigger crashes due to corner-case API usages triaged module: edge cases,triaged module: edge cases
Conv2d is not deterministic when input tensor has different strides module: convolution triaged,module: convolution triaged
AvgPool2D output shapes are inconsistent when ceil_mode=True triaged module: correctness (silent) module: pooling,triaged module: correctness (silent) module: pooling
Refactor `torch.return_types.topk` to behave like a `namedtuple` or a `dict` triaged enhancement module: sorting and selection,triaged enhancement module: sorting and selection
"Add eq, to, masked_select, index_select, narrow to nested tensors triaged module: nestedtensor",triaged module: nestedtensor
Placing LSTM model on bfloat16 on GPU causes error module: rnn module: cuda triaged module: bfloat16,module: rnn module: cuda triaged module: bfloat16
Python Dispatcher registrations beyond BackendSelect do nothing triaged module: dispatch module: python dispatcher,triaged module: dispatch module: python dispatcher
ProcessGroupNCCL watchdog can't catch NCCL comm initialization issues oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Add nondeterministic alert to `torch.Tensor.scatter()` triaged module: determinism,triaged module: determinism
out of memory with pytorch version after 1.8.1 module: cuda module: memory usage triaged module: CUDACachingAllocator,module: cuda module: memory usage triaged module: CUDACachingAllocator
Cannot import `traverse_dps` from torch.data.utils.graph triaged module: data,triaged module: data
Different behaviour in sparse matmul module: sparse triaged,module: sparse triaged
`torch.nn.CTCLoss` Trigger heap-buffer-overflow under AddressSanitizer module: nn module: loss triaged actionable module: sanitizers module: edge cases,module: nn module: loss triaged actionable module: sanitizers module: edge cases
Minifier doesn't work on DebertaForQuestionAnswering triaged bug,triaged bug
Inductor gives obscure error when FX graph to be compiled returns tuple triaged bug,triaged bug
Turning on minifier causes bug to go away (on DebertaForMaskedLM) triaged bug,triaged bug
A segment fault can be triggered in fbgemm_pack_gemm_matrix_fp16 module: crash triaged module: third_party module: half,module: crash triaged module: third_party module: half
"getting error error: namespace ""cub"" has no member ""Debug"" when try to build v1.8.2 with CUDA 11.6 module: cuda triaged",module: cuda triaged
[WIP] Composable FSDP Follow-Ups oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Einsum Optimization Tracker module: performance triaged module: linear algebra,module: performance triaged module: linear algebra
multi-node distributed training rank0 hang at dataloader after a few epochs oncall: distributed module: dataloader,oncall: distributed module: dataloader
torch.rand(...) is not consistent for large shape dimensions across GPUs (with the same random seed) module: cuda triaged module: random,module: cuda triaged module: random
amp with `bf16`: backward happens in `f16` when using `@torch.cuda.amp.custom_bwd` triaged module: bfloat16 module: half module: amp (automated mixed precision),triaged module: bfloat16 module: half module: amp (automated mixed precision)
`torch.distributed` crash with abort only inside if oncall: distributed module: crash,oncall: distributed module: crash
crash in `torch.package.PackageExporter` triaged module: edge cases,triaged module: edge cases
crash when call `torch.set_num_interop_threads` twice module: crash triaged,module: crash triaged
VS2022Preview ParallelCommon.cpp.obj : fatal error LNK1161: invalid export specification module: build triaged module: static linking,module: build triaged module: static linking
Autograd doesn't stop executing backward graph early enough in situations involving set_ module: autograd triaged has workaround actionable,module: autograd triaged has workaround actionable
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next' module: dataloader triaged,module: dataloader triaged
link error happen when intergrate libtorch to other tool module: build module: cpp triaged,module: build module: cpp triaged
test_conv_large_cuda: RuntimeError: CUDA error: an illegal memory access was encountered triaged,triaged
test_batchnorm_eval_cuda_float32: AttributeError: 'NoneType' object has no attribute 'clone' triaged,triaged
"test_LSTM_grad_and_gradgrad_cuda_float64: ValueError: gradcheck expects at least one input tensor to require gradient, but none of the them have requires_grad=True. triaged",triaged
"test_memory_format_ao_nn_quantized_MaxPool2d_cuda_float32: assert not memory_format, ""TODO"" triaged",triaged
Permute module: docs triaged,module: docs triaged
"""No CUDA GPUs are available"" coming from GHA g5 runners oncall: releng module: ci triaged",oncall: releng module: ci triaged
Add aten::empty.memory_format for SparseMPS module: sparse triaged enhancement module: mps,module: sparse triaged enhancement module: mps
Failure to export scripted models to ONNX when input is a list of tensor needs reproduction module: onnx triaged onnx-needs-info,needs reproduction module: onnx triaged onnx-needs-info
RuntimeError: unable to mmap 29764 bytes from file </torch_10182_3020184674_63991>: Cannot allocate memory (12) triaged module: tensor creation,triaged module: tensor creation
"M1 Mac, MPS: Buffer is not large enough triaged module: mps",triaged module: mps
`max_unpool3d` will trigger an assertion fail under compute sanitizer module: cuda triaged module: sanitizers module: pooling,module: cuda triaged module: sanitizers module: pooling
CUDA error: operation not permitted when stream is capturing (2 GPUs) module: multi-gpu triaged,module: multi-gpu triaged
`AvgPool` and `MaxPool` will crash in JIT w/o profiling executor oncall: jit,oncall: jit
`BatchNorm` a 0-shape tensor will crash in JIT trace w/o profiling executor on cuda oncall: jit,oncall: jit
AssertionError: Unknown expression s2 triaged bug,triaged bug
Libtorch windows binaries publishing module: ci triaged topic: binaries,module: ci triaged topic: binaries
Improvements to fuse optimization oncall: quantization low priority triaged module: fx,oncall: quantization low priority triaged module: fx
Autograd precision for CONV + BN  between pytorch version 1.11.0 and 1.12.1 module: numerical-stability module: autograd triaged module: numerical-reproducibility,module: numerical-stability module: autograd triaged module: numerical-reproducibility
`torch.min`/`torch.max` returns bogus values for default int tensors on MPS triaged module: correctness (silent) module: mps,triaged module: correctness (silent) module: mps
TorchDynamo: there has a accuracy issue for conv+unary(binary) post ops for gpu path triaged module: dynamo,triaged module: dynamo
"FakeTensorMode doesn't support two Scalar inputs, if we use prims' impl as the meta function  triaged module: primTorch module: fakeTensor",triaged module: primTorch module: fakeTensor
C++ Adagrad optimizer doesn't initialize parameter state module: cpp module: optimizer triaged actionable,module: cpp module: optimizer triaged actionable
Get https://github.com/pytorch/benchmark working module: windows triaged,module: windows triaged
Enable PostLocalSGDOptimizer on CUDA tensors oncall: distributed module: cuda triaged,oncall: distributed module: cuda triaged
Investigate possibilities of automation for build pipeline module: windows triaged,module: windows triaged
"Performance issue on Windows with a ""benchmark"" comparing to Linux and WLS module: windows triaged",module: windows triaged
INTERNAL ASSERT FAILED !(has_different_input_dtypes && !config.promote_inputs_to_common_dtype_ && (has_undefined_outputs || config.enforce_safe_casting_to_output_ || config.cast_common_dtype_to_outputs_)) needs reproduction triaged module: TensorIterator,needs reproduction triaged module: TensorIterator
`libtorch_cpu.so` is exposing some LLVM symbols module: build module: cpp-extensions triaged,module: build module: cpp-extensions triaged
torchdynamo.export doesn't work with data-dependent control flow triaged enhancement,triaged enhancement
ninja: build stopped: subcommand failed needs reproduction module: build triaged,needs reproduction module: build triaged
"#error ""Expected GLOO_USE_CUDA to be defined"" module: build triaged module: third_party",module: build triaged module: third_party
Crash on backwards step when using `batch_first=True` for LSTMs on MPS (1.14 nightly build) module: rnn triaged module: mps,module: rnn triaged module: mps
Dynamic shapes exhaustive tests should fail (not xfail) if data mismatch triaged module: dynamic shapes,triaged module: dynamic shapes
Functionalization does something wrong with pad backward when it uses as_strided triaged module: functionalization,triaged module: functionalization
Testing insufficient to catch incorrect dispatch key for bernoulli.p re functionalization triaged module: functionalization,triaged module: functionalization
diagonal of Jacobian matrix module: autograd triaged enhancement,module: autograd triaged enhancement
The behavior of cast `NaN` is different on cpu and cuda triaged module: NaNs and Infs module: edge cases,triaged module: NaNs and Infs module: edge cases
Improve `c10d::ReduceOp` & `torch.distributed.distributed_c10d.ReduceOp` oncall: distributed triaged,oncall: distributed triaged
`bmm` will return wrong result on cpu with in-place triaged module: edge cases,triaged module: edge cases
RAM leak when copying tensor from cpu to cuda module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
invalid_arguments.cpp is busted module: bootcamp triaged actionable module: python frontend,module: bootcamp triaged actionable module: python frontend
Loading model trained on MPS cannot be opened on non MPS system triaged module: mps,triaged module: mps
Synchronize domain builds to be executed after core build have completed module: ci triaged,module: ci triaged
"built from source windows static library with multiple ""unresolved external symbol"" module: build module: windows triaged",module: build module: windows triaged
[JIT] Inconsistent handling of tracing dict output leads to assertion  oncall: jit,oncall: jit
Categorical fails simplex validation after its own normalisation on CUDA module: distributions triaged,module: distributions triaged
Placeholder tensor is empty triaged module: mps,triaged module: mps
Some operations do not keep `channels_last` memory format which yields accuracy drop triaged module: memory format,triaged module: memory format
pytorch could not build from source with cudnn 8.0.5 module: build module: cudnn module: cuda triaged,module: build module: cudnn module: cuda triaged
Semantics of sparse operations clarification - Sparsity of the gradient with respect to a sparse tensor input module: sparse module: docs module: autograd triaged enhancement module: masked operators,module: sparse module: docs module: autograd triaged enhancement module: masked operators
ipykernel crash importing torch after scipy in .ipynb file needs reproduction module: crash triaged,needs reproduction module: crash triaged
index_select() applied in sparse tensor can't backprop module: sparse triaged,module: sparse triaged
`lower_cholesky` constraint incorrectly fails on MPS triaged module: mps,triaged module: mps
`chunk` a 0-dim tensor will crash in JIT script w/o profiling executor oncall: jit,oncall: jit
Installing PyTorch with BUILD_SPLIT_CUDA=ON and CUDNN fails on linker error module: build module: cudnn module: cuda triaged actionable,module: build module: cudnn module: cuda triaged actionable
Document dist.new_subgroups oncall: distributed module: docs triaged,oncall: distributed module: docs triaged
Better type annotations for `torch.Tensor` subclasses module: typing triaged actionable tensor subclass,module: typing triaged actionable tensor subclass
"Implementation of CG, and BICGSTAB methods module: optimizer triaged needs research",module: optimizer triaged needs research
test_ao_sparsity fails when build without FBGEMM triaged,triaged
Triangular solver for sparse matrices module: sparse triaged module: linear algebra,module: sparse triaged module: linear algebra
Speed of torch.istft module: performance triaged module: fft,module: performance triaged module: fft
RuntimeError: Tensors of type TensorImpl do not have numel module: build module: cuda triaged module: docker,module: build module: cuda triaged module: docker
buffer is not large enough when running pytorch on M1 mps triaged module: mps,triaged module: mps
Better error message when attempting to `torch.save` an optimized model good first issue triaged oncall: pt2 module: dynamo,good first issue triaged oncall: pt2 module: dynamo
'str' object has no attribute '__module__' in jit is_final oncall: jit,oncall: jit
Missing string parsing for some parameter types in python arg parsing logic high priority triaged module: pybind,high priority triaged module: pybind
torch.save throws ValueError: ctypes objects containing pointers cannot be pickled module: serialization triaged,module: serialization triaged
The installation commands given on the pytorch website will not install properly module: binaries triaged,module: binaries triaged
nvprims.div doesn't work with FakeTensor cpu scalars triaged module: nvfuser module: fakeTensor,triaged module: nvfuser module: fakeTensor
`custom_jvp` and `custom_vjp` triaged module: functorch,triaged module: functorch
"Reproducible ""CUDA error: an illegal memory access was encountered"" needs reproduction module: crash module: cuda triaged",needs reproduction module: crash module: cuda triaged
Missing `docker` directory in `tools/`  module: docs triaged module: docker,module: docs triaged module: docker
The autogenerated out variants via `autogen:` do not check that the dtype of the `out` kwarg via `canCast`. triaged module: codegen,triaged module: codegen
Unstable results in sin/arcsin/arccos calls module: numerical-stability triaged module: edge cases,module: numerical-stability triaged module: edge cases
torch.linalg.cond gives inconsistent results on CPU/CUDA triaged module: NaNs and Infs module: linear algebra,triaged module: NaNs and Infs module: linear algebra
New APIs for cuda graph inspection and manipulation triaged module: cuda graphs,triaged module: cuda graphs
"torch/csrc/utils/python_arg_parser.h:424:94: error: format â€˜%ldâ€™ expects argument of type â€˜long intâ€™, but argument 7 has type â€˜intâ€™ module: build triaged module: arm",module: build triaged module: arm
torch.clamp does not clamp out of -0 from 0 when ran on the CPU triaged module: numpy module: edge cases,triaged module: numpy module: edge cases
[MPS] sum on a size=1 dim is ~5x slower than squeeze triaged module: mps,triaged module: mps
Bug in Histogram Observer Implementation oncall: quantization triaged,oncall: quantization triaged
MPS memory usage significantly higher than on CPU module: memory usage triaged module: mps,module: memory usage triaged module: mps
gradcheck failure with sparse matrix multiplication module: sparse module: autograd triaged,module: sparse module: autograd triaged
cppextension host compiler check ignores executable symbolic link in CUDA bin directory module: build module: cpp-extensions triaged,module: build module: cpp-extensions triaged
Nandense layer for missing values module: sparse module: nn triaged module: masked operators,module: sparse module: nn triaged module: masked operators
Pipe conveys inconsistent value in GPU env module: multiprocessing triaged,module: multiprocessing triaged
"Segmentation fault: 11 when running ""import torch"" on Mac OS X triaged module: macos",triaged module: macos
Saving and loading from physical storage module: memory usage module: serialization triaged,module: memory usage module: serialization triaged
Improve Readability of error(s) when provided unexpected keyword arguments. module: error checking triaged actionable module: python frontend,module: error checking triaged actionable module: python frontend
Rewrite `narrow_copy_dense_cpu_out` using `copy_` and `narrow` triaged module: functionalization,triaged module: functionalization
Multiprocessing DataLoader pickles multiprocessing.Queues incorrectly module: dataloader triaged,module: dataloader triaged
Error: unknown architecture `armv7-a;' and Error: selected processor does not support `command' in ARM mode oncall: quantization low priority triaged module: arm topic: build,oncall: quantization low priority triaged module: arm topic: build
Drop deprecated behavior from NumPy-style `T` triaged module: numpy module: deprecation,triaged module: numpy module: deprecation
Upgrade to a newer llvm-openmp version to avoid `/dev/shm` pollution module: binaries triaged module: openmp,module: binaries triaged module: openmp
Importing torch 1.12.0 breaks subprocess module needs reproduction module: binaries triaged module: macos,needs reproduction module: binaries triaged module: macos
torch.cat on empty tensor is bogus triaged module: edge cases,triaged module: edge cases
[FSDP] Investigate `torch.cuda.current_stream()` usage in post-backward oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
View-based advanced indexing (Integer array/LongTensor indexing) of nested_tensor feature triaged module: advanced indexing,feature triaged module: advanced indexing
Broadcasting add for nested_tensor triaged module: nestedtensor,triaged module: nestedtensor
compile torch from source needs reproduction module: build triaged,needs reproduction module: build triaged
.view(dtype) on a quantized tensor throws SegmentationFault oncall: quantization triaged,oncall: quantization triaged
Distributed collective ops fail in `inference_mode` for CPU-only oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Could not run select_backward [vmap] [dlrm] [functorch] triaged module: functorch,triaged module: functorch
JIT model returns different value on cpu with uniform-initialized input oncall: jit,oncall: jit
Expanding the parameters of `torch.svd_lowrank` triaged module: linear algebra,triaged module: linear algebra
TF32 conv_transpose2d with groups has bad precision compared to fp32 module: numerical-stability module: cuda triaged module: tf32,module: numerical-stability module: cuda triaged module: tf32
Poisson sampling on GPU fails for high rates module: cuda triaged module: random,module: cuda triaged module: random
NVFuser `FusionRootMappingMultipleBroadcast_CUDA` raises exception on sm_80+  module: cuda module: ci triaged module: nvfuser,module: cuda module: ci triaged module: nvfuser
NVFuser `FusionComputeAtMultiBCast_CUDA` and `FusionDetectSelfMappedDomains_CUDA` does not raise exception on sm_80+ module: cuda module: ci triaged module: nvfuser,module: cuda module: ci triaged module: nvfuser
Performance tests mnist_hogwild-cpu_memory CPU memory increase by 30% triaged module: regression,triaged module: regression
Documentation and typing hints for RProp module: docs module: optimizer triaged actionable,module: docs module: optimizer triaged actionable
Pytorch built for Jetson errors if CUDA is not found module: build module: cuda triaged module: arm,module: build module: cuda triaged module: arm
Adding a linear layer leads to failure of `optimize_for_mobile` triage review oncall: jit module: mkldnn,triage review oncall: jit module: mkldnn
libtorch make failed  module: build triaged,module: build triaged
"[NvFuser] INTERNAL ASSERT FAIL ""ScalarType should be static for Tensors in fusion for amp optimization"" triaged module: assert failure module: nvfuser",triaged module: assert failure module: nvfuser
RFC(from users): nn.Module behavior with in-place changes triaged needs design module: functorch,triaged needs design module: functorch
[ONNX] CSE pass in export pollutes Scope information module: onnx triaged module: regression bug,module: onnx triaged module: regression bug
Move functorch tests from functorch/test/* to test/*; delete functorch CI configs module: ci triaged module: functorch,module: ci triaged module: functorch
JIT returns different values for a model on cuda and returns a strange error message on cpu oncall: jit module: correctness (silent),oncall: jit module: correctness (silent)
Decomposition table is ignored with use_functionalize=True in AOT Autograd triaged oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,triaged oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher
Nonoptimal trace of silu_backward with AOT Autograd triaged oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,triaged oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher
NVFuser batch norm with prims: internal assert failure from test suite module: tests triaged module: nvfuser module: primTorch,module: tests triaged module: nvfuser module: primTorch
`squeeze_` fails with JIT but succeeds without it oncall: jit,oncall: jit
JIT returns different values for `cos + frac` on cpu oncall: jit module: correctness (silent),oncall: jit module: correctness (silent)
`CTCLoss` returns a different value with JIT on cuda oncall: jit module: cuda module: correctness (silent),oncall: jit module: cuda module: correctness (silent)
JIT model with `relu+div+sgn` will crash when computing the gradient oncall: jit module: crash module: functorch,oncall: jit module: crash module: functorch
JIT model with mean will crash when computing the gradients on cuda triage review oncall: jit module: crash,triage review oncall: jit module: crash
"Easy way to ""freeze"" BatchNorm running_mean/running_var module: nn triaged needs research",module: nn triaged needs research
[functorch] colab links on functorch 0.2.0 website should be linked to a permalinked version of the colabs module: docs triaged module: functorch,module: docs triaged module: functorch
Data conversion ops ignore `memory_format=torch.contiguous_format`  triaged module: primTorch,triaged module: primTorch
[NvFuser] would change the output for some inaccurate dtype triaged module: nvfuser,triaged module: nvfuser
`topk` will return the wrong value and could read out-of-bound value after jit oncall: jit,oncall: jit
`max_unpool` and `max_pool` will trigger INTERNAL ASSERT FAIL in JIT oncall: jit,oncall: jit
`MultiLabelMarginLoss` will return incorrect values in JIT after the first run on cuda oncall: jit module: cuda triaged module: nvfuser,oncall: jit module: cuda triaged module: nvfuser
About autocast triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
Segmentation fault (core dumped) in RTX3090 needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
Compile failed at allreduce without gloo module: build triaged,module: build triaged
cuda.list_gpu_processes() uses the 'wrong' device order (PCI_BUS_ID) module: cuda triaged,module: cuda triaged
[functorch] [vmap] [SymInt][fake tensor] triaged module: functorch,triaged module: functorch
Running JIT trace for many times leads to OOM oncall: jit module: memory usage triaged,oncall: jit module: memory usage triaged
Conv2d will crash by using `jit.trace` oncall: jit module: crash,oncall: jit module: crash
[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient triaged module: nvfuser,triaged module: nvfuser
Tooling Issue Tracking triaged oncall: pt2,triaged oncall: pt2
Automatic broadcasting for batch addition for sparse tensors module: sparse triaged enhancement,module: sparse triaged enhancement
`mem_get_info` reserves memory and can not be destroyed / deallocated.  module: cuda triaged,module: cuda triaged
onnx.export make size operations return Tensor instead of int module: onnx triaged,module: onnx triaged
FSDP support to load DDP optim checkpoints oncall: distributed triaged better-engineering module: fsdp,oncall: distributed triaged better-engineering module: fsdp
torch.tensor obj automatically moved to shared memory upon Process launch module: multiprocessing triaged,module: multiprocessing triaged
Wrong results with torch.linalg.inv on batched matrices when using cuda module: cuda triaged module: linear algebra module: correctness (silent) module: magma,module: cuda triaged module: linear algebra module: correctness (silent) module: magma
(JIT) x:Optional[T] cannot not expect content type after `if x is None or x.shape[0]==1` oncall: jit,oncall: jit
torch.cuda.empty_cache() is not working module: cuda triaged,module: cuda triaged
Dedicated function for shallow_copy_and_detach module: autograd triaged tensor subclass,module: autograd triaged tensor subclass
[functorch] [aot_autograd]  triaged module: vmap module: functionalization oncall: pt2 module: aotdispatch module: pt2-dispatcher,triaged module: vmap module: functionalization oncall: pt2 module: aotdispatch module: pt2-dispatcher
OpInfo Tests To Validate that All Operators Are Being Tested With Strided Tensors module: tests triaged,module: tests triaged
`conv_transpose` is not similar to `nn.grad.conv_input` when `output_padding` is passed with non-default values. module: nn triaged actionable,module: nn triaged actionable
Add complex support for SparseAdam and LBFGS optimizers module: optimizer triaged module: complex actionable,module: optimizer triaged module: complex actionable
`torch.special.round` doesn't support the same dtypes as `torch.round` triaged module: special,triaged module: special
Feature request: Tests for `int` should be tests for `numbers.Integral` feature triaged module: numpy module: python frontend,feature triaged module: numpy module: python frontend
AOT Autograd Device Partitioning triaged module: functorch,triaged module: functorch
JIT `lgamma` will return `inf` only with dual input in forward mode oncall: jit,oncall: jit
`torch.multinomial` on MPS crashes with `Error: total bytes of NDArray > 2**32'` triaged module: regression module: mps,triaged module: regression module: mps
JIT miss the argument `as_tuple` for API `nonzero` oncall: jit,oncall: jit
TransformerEncoder/TransformerDecoder has same initial parameters for all layers triaged oncall: transformer/mha,triaged oncall: transformer/mha
AUTOGRAD is not working on IOS module: autograd triaged oncall: mobile module: ios,module: autograd triaged oncall: mobile module: ios
Autocast with BF16 on CPU slows down model more than 2X module: performance triaged module: bfloat16 module: amp (automated mixed precision),module: performance triaged module: bfloat16 module: amp (automated mixed precision)
TORCH_WARN is executed just once per set of parameters module: logging triaged,module: logging triaged
How to perform unstructured interpolation  triaged module: interpolation,triaged module: interpolation
path in WORKSPACE triaged module: bazel,triaged module: bazel
fmt/src/os.cc: error: unknown type name 'error_code'; did you mean 'std::error_code'? module: build triaged,module: build triaged
torch.nn.functional.one_hot only works for int64 module: nn triaged,module: nn triaged
MPSNDArray.mm:782: failed assertion; bufer is not large enough Mac M1 MPS module: memory usage triaged module: regression module: mps,module: memory usage triaged module: regression module: mps
Debuggability++: Share instructions for building exotic CI configurations module: ci triaged,module: ci triaged
[TorchDispatch] Scalar Only Inputs Gets Matched To Tensor Schema triaged module: __torch_dispatch__,triaged module: __torch_dispatch__
TransformerEncoder src_key_padding_mask does not work in eval() high priority triaged oncall: transformer/mha,high priority triaged oncall: transformer/mha
JIT fails to trace binary cross entropy with a strange error msg oncall: jit,oncall: jit
When will the torch.sparse module be usable? module: sparse triaged,module: sparse triaged
`F.affine_grid` crashes on MPS triaged module: mps,triaged module: mps
[Activation Checkpointing] Investigate pin_memory for CPU offload oncall: distributed module: checkpoint triaged,oncall: distributed module: checkpoint triaged
Figure out the future of Metal backend given the existence of MPS module: build triaged module: arm module: mps,module: build triaged module: arm module: mps
torch.remainder and torch.fmod produce wrong results module: numerical-stability triaged module: correctness (silent),module: numerical-stability triaged module: correctness (silent)
partial view/reshaping triaged enhancement module: python frontend,triaged enhancement module: python frontend
Significantly worse MPS performance between torch 1.13.0.dev20220922 and torch 1.13.0.dev20220930 module: performance triaged module: mps,module: performance triaged module: mps
Functorch memory_efficient_fusion gives wrong output batch size module: docs triaged module: functorch,module: docs triaged module: functorch
Discrepancy in output shape for batch_norm inference mode between CUDA and CPU module: nn module: cuda triaged actionable,module: nn module: cuda triaged actionable
CUDA OOM issue when running tests in CI high priority module: ci triaged,high priority module: ci triaged
Setup ssh sometimes fail module: ci triaged,module: ci triaged
Steam Deck Core Dump high priority module: rocm triaged,high priority module: rocm triaged
High occupation on GPU 0 when converting Tensor to multi GPU module: performance module: cuda triaged,module: performance module: cuda triaged
JIT model could return 'NaN' gradient after the first execution oncall: jit,oncall: jit
`torch.mm` produces wrong result on cpu when using in-place computation module: docs triaged,module: docs triaged
Print a warning when user specifies a qconfig for some node and the qconfig is not supported by BackendConfig oncall: quantization low priority triaged,oncall: quantization low priority triaged
Setting the cuda device when using start_processes in Jupyter on Ampere leads to CUDA reinitialization error module: multiprocessing module: cuda triaged,module: multiprocessing module: cuda triaged
[primTorch] Need to update data-dependent check policy triaged module: primTorch,triaged module: primTorch
[FSDP] `use_orig_params=True` Follow-Ups & Known Issues oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
string interning for dispatcher operator names triaged module: dispatch,triaged module: dispatch
TorchScript error for `Enum` inside a module oncall: jit,oncall: jit
"`vector_norm` will trigger ""Tracing failed sanity checks"" for JIT when ord is boolean tensor oncall: jit",oncall: jit
JIT fails to trace `sparse.mm` with a strange error oncall: jit,oncall: jit
TorchScript causes range_check error after a few iterations of forward-backward passes oncall: jit,oncall: jit
nn.CrossEntropyLoss overflow with FP16 and minibatch module: nn module: loss triaged module: edge cases,module: nn module: loss triaged module: edge cases
Timed out receiving the shared seed from the distribtued store on Rank 2 oncall: distributed module: dataloader triaged,oncall: distributed module: dataloader triaged
Conda Pytorch (Pytorch channel) in WSL2 Ubuntu can't find libcudnn shared objects module: build module: cuda triaged,module: build module: cuda triaged
Replace same with TestCase assertEqual good first issue triaged oncall: pt2 module: dynamo,good first issue triaged oncall: pt2 module: dynamo
[ONNX] Conversion failed when using dict as input to a scripted module module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Minifier should not produce repro with backward call if it is not necessary to trigger error triaged oncall: pt2,triaged oncall: pt2
Minifier dumps checkpoints which don't actually reproduce the error triaged bug oncall: pt2 module: minifier,triaged bug oncall: pt2 module: minifier
[Quant] Remove or clarify the meaning of Nones in QConfig/BackendConfig oncall: quantization low priority triaged,oncall: quantization low priority triaged
RuntimeError: [enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 4860032 bytes. Error code 12 (Cannot allocate memory) oncall: jit,oncall: jit
AMP consumes 30x gpu memory with bmm triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
[functorch] CUDA Graph failure with AOTAutograd triaged module: cuda graphs module: functorch,triaged module: cuda graphs module: functorch
"[functorch] conv.{1, 2, 3}d should raise errors triaged module: functorch",triaged module: functorch
CUDA allocator feature requests module: cuda triaged module: CUDACachingAllocator,module: cuda triaged module: CUDACachingAllocator
Could not run 'aten::native_batch_norm' with arguments from the 'SparseCUDA' backend.  using batch_norm module: sparse triaged,module: sparse triaged
How to install pytorch with cuda 11.7 in anaconda envirment? triaged,triaged
Gloo DDP SocketTimeout error on Windows oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Build from source failed with error of different gpu architecture (compiler shows sm_30-related error but I use sm_86 GPU) module: build module: cuda triaged,module: build module: cuda triaged
[MPS?] .to(memory_format=contiguous_format) behaves incorrectly; differently to .contiguous() triaged module: mps,triaged module: mps
[Distributed: RPC] Failed to initialize RPC with >18 workers oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
Creating NumPy array with `dtype=object` of PyTorch tensors fails feature triaged module: numpy,feature triaged module: numpy
"Multiple GPUs get  ""errno: 98 - Address already in use"" oncall: distributed triaged module: c10d",oncall: distributed triaged module: c10d
Solve default argument induced include cycles by not using defaults / moving the defaults to inl module: build module: internals triaged,module: build module: internals triaged
`jit` could make some undifferentiable APIs differentiable oncall: jit,oncall: jit
`mvlgamma_` will fail when compiling with trace `jit` oncall: jit,oncall: jit
torch.Tensor.transpose().contiguous() on dimension of size 1 gives  wrong stride  module: docs triaged,module: docs triaged
NvFuser single mode changes the output triaged module: nvfuser,triaged module: nvfuser
Iterative Global Pruning Cause GPU Memory Leak high priority module: nn module: memory usage triaged,high priority module: nn module: memory usage triaged
"[functorch] transforms like jacrev, jacfwd, grad, etc don't work with BatchNorm triaged module: functorch",triaged module: functorch
Implement `rand_like` ref and implement nvfuser_impl for `uniform` prim triaged module: nvfuser,triaged module: nvfuser
Execute smoke test for Better Transformer feature  module: ci triaged,module: ci triaged
"`max_pool2d_with_indices(self, ...)` shouldn't need to save `self` for backward module: autograd triaged",module: autograd triaged
Issue with converting Comet model to ONNX. Split-node error. module: onnx triaged,module: onnx triaged
Can we rewrite numpy operators to pytorch operators? good first issue triaged module: numpy,good first issue triaged module: numpy
Cannot index into a tensor using indices from another device - regression from 1.12 triaged module: regression module: advanced indexing,triaged module: regression module: advanced indexing
`aminmax` will trigger INTERNAL ASSERT if input is empty on cuda good first issue triaged actionable module: edge cases,good first issue triaged actionable module: edge cases
Prim Output Spec is Not Always Consistent With Eager triaged module: primTorch,triaged module: primTorch
Feature Request: Deterministic Algorithm for MaxPool3d feature module: nn good first issue triaged module: determinism module: pooling,feature module: nn good first issue triaged module: determinism module: pooling
torch.nn.utils.prune.remove reorders the parameters of a module unexpectedly triaged module: pruning,triaged module: pruning
please report a bug to PyTorch. Expected Object but got PyObject triaged module: torchbind,triaged module: torchbind
Please put back missing rocm builds of Torch Vision. module: binaries module: rocm triaged,module: binaries module: rocm triaged
very strange speed of torch.bmm with specific tensor shape module: performance module: cuda triaged module: linear algebra,module: performance module: cuda triaged module: linear algebra
CI fails for test_compare_cpu_nn_functional_embedding_cuda_float32 which is not reproducible locally module: cuda module: tests triaged module: embedding,module: cuda module: tests triaged module: embedding
Inconsistency between geometric distributions module: distributions module: molly-guard triaged,module: distributions module: molly-guard triaged
More windows for filtering and spectral analysis triaged module: scipy compatibility,triaged module: scipy compatibility
functorch aten::scatter_add_  not implemented triaged module: functorch,triaged module: functorch
Crash in `torch.package.PackageExporter` module: crash triaged oncall: package/deploy imported module: edge cases,module: crash triaged oncall: package/deploy imported module: edge cases
Remove `TypedStorage` and use only `UntypedStorage` triaged module: python frontend,triaged module: python frontend
torchrun substitutes host names for IP addresses oncall: distributed triaged module: elastic,oncall: distributed triaged module: elastic
Have NVIDIA driver and other related dependencies as part of the Linux AMI module: ci triaged,module: ci triaged
"nvFuser support for {ceil,floor,round,trunc}(int) triaged module: nvfuser",triaged module: nvfuser
Add `persistent` option to `nn.Module.buffers`. module: nn triaged actionable,module: nn triaged actionable
[MPS] load checkpoints gives zero weights when map_location is mps triaged has workaround module: correctness (silent) module: mps,triaged has workaround module: correctness (silent) module: mps
TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function.  oncall: jit triaged,oncall: jit triaged
topk returns different results with the same input in cuda and cpu module: cuda triaged module: sorting and selection,module: cuda triaged module: sorting and selection
Segmentation fault in native_batch_norm module: crash module: error checking triaged module: norms and normalization,module: crash module: error checking triaged module: norms and normalization
Floating point exception in gather gradient computation. module: crash module: error checking triaged module: edge cases,module: crash module: error checking triaged module: edge cases
Segmentation fault in mkldnn_reorder_conv2d_weight and mkldnn_reorder_conv3d_weight module: crash triaged module: mkldnn module: edge cases,module: crash triaged module: mkldnn module: edge cases
OSError libstdc++.so.6 at import needs reproduction module: binaries triaged,needs reproduction module: binaries triaged
performance between manually created graph and CUDAGraph.replay triaged module: cuda graphs,triaged module: cuda graphs
NestedTensor 2.0 issue tracking triaged module: nestedtensor,triaged module: nestedtensor
torch::quantile performance? module: performance triaged,module: performance triaged
[ONNX] Using values from a different tensor to index a tensor returns a tensor with incorrect shape in exported ONNX model oncall: jit module: onnx triaged bug,oncall: jit module: onnx triaged bug
"PT Dispatcher confusing error message ""There were no tensor arguments to this function"" triaged module: dispatch",triaged module: dispatch
make_traced() doesn't respect setting the seed module: tests triaged module: primTorch,module: tests triaged module: primTorch
[ONNX][bug] `nn.Transformer` contains unsupported tensor scalar type module: onnx triaged onnx-triaged bug,module: onnx triaged onnx-triaged bug
[ONNX] Produce error message for incorrect number of dummy inputs instead of Internal assert failure needs reproduction module: onnx triaged onnx-triaged,needs reproduction module: onnx triaged onnx-triaged
Python dispatch for PyOps needs to respect tensor subclasses triaged module: dispatch module: __torch_dispatch__,triaged module: dispatch module: __torch_dispatch__
install libtorch cxx11 ABI as default in PyTorch pip installation module: binaries triaged,module: binaries triaged
[ddp] must set `static_graph=False` when running with dynamo triaged module: ddp oncall: pt2 module: dynamo module: distributed,triaged module: ddp oncall: pt2 module: dynamo module: distributed
Update `use_deterministic_algorithms` documentation and tests to include `nn.functional` counterparts for all `nn` modules module: docs module: tests triaged module: determinism,module: docs module: tests triaged module: determinism
Memoizing AOT Autograd Input Conversion Breaks Training with Tied Parameters triaged module: functorch,triaged module: functorch
reentrant torch.utils.checkpoint does not work with NamedTuple outputs high priority triage review oncall: distributed module: checkpoint,high priority triage review oncall: distributed module: checkpoint
"[NNC] loop vectorization fails, `Ramp` and `Broadcast` undefined triaged NNC",triaged NNC
primTorch/nvfuser: have a way to check that refs are added to __all__ feature triaged module: nvfuser module: primTorch,feature triaged module: nvfuser module: primTorch
"libtorch create a tensor is very slow, who can tell me why module: performance module: cpp triaged",module: performance module: cpp triaged
Segmentation fault in `torch.jit.wait` oncall: jit,oncall: jit
Selectively sync internal Meta discussions / posts to dev-discuss.pytorch.org triaged,triaged
Add an opaque epilogue in AOTAutograd for aliasing/mutations triaged module: functionalization module: functorch,triaged module: functionalization module: functorch
Cuda tensor is zero when passed through multiprocessing queue module: multiprocessing triaged,module: multiprocessing triaged
Segmentation fault in `torch.futures.collect_all` module: crash module: error checking triaged,module: crash module: error checking triaged
test_warp_softmax_64bit_indexing_cuda_float16 takes ~147GB of CPU memory and is very slow module: memory usage triaged module: testing,module: memory usage triaged module: testing
CPU and MPS floating point math is different (in a significant way) module: numerical-stability triaged module: mps,module: numerical-stability triaged module: mps
`is_pinned()` support in PrimTorch and FakeTensor. triaged module: primTorch module: fakeTensor,triaged module: primTorch module: fakeTensor
Functorch functionalization causes increased memory usage triaged module: functionalization module: functorch,triaged module: functionalization module: functorch
Re-Running PR Sanity Check after Adding `skip-pr-sanity-checks` Label Still Fails module: ci triaged,module: ci triaged
torch.utils.checkpoint (with use_reentrant=False) doesn't work with all PyTorch features that set TLS module: checkpoint triaged module: __torch_dispatch__ tensor subclass,module: checkpoint triaged module: __torch_dispatch__ tensor subclass
View consistency for PrimTorch+nvFuser tests triaged module: nvfuser module: primTorch,triaged module: nvfuser module: primTorch
Feature Request: deterministic adaptive_avg_pool2d_backward_cuda feature module: cuda triaged module: determinism,feature module: cuda triaged module: determinism
ONNX exporter error needs reproduction module: onnx triaged onnx-needs-info,needs reproduction module: onnx triaged onnx-needs-info
"TypeError: finfo(): argument 'type' (position 1) must be torch.dtype, not HFProxy triaged module: fx",triaged module: fx
Batch multiplication for torch.sparse matrix multiplication module: sparse triaged module: linear algebra,module: sparse triaged module: linear algebra
INTERNAL ASSERT FAILED for _jit_pass_vulkan_optimize_for_mobile (Google Colab) oncall: jit oncall: mobile module: vulkan,oncall: jit oncall: mobile module: vulkan
MPS: allow selecting specific MTLDevice by registryID via environment variable triaged enhancement module: mps,triaged enhancement module: mps
compiling failed from source module: sparse module: build module: cuda triaged,module: sparse module: build module: cuda triaged
macOS Pyinstaller: libc++abi: terminating with uncaught exception of type c10::Error: Type c10::intrusive_ptr<ConvPackedParamsBase<2>> could not be converted to any of the known types module: cpp-extensions triaged module: third_party needs research module: m1,module: cpp-extensions triaged module: third_party needs research module: m1
Test aten decompositions match their alias information triaged better-engineering module: primTorch,triaged better-engineering module: primTorch
About the different ways to print models module: printing triaged,module: printing triaged
Set dtype if tensor converted to numpy needs reproduction triaged module: numpy,needs reproduction triaged module: numpy
NotImplementedError: The operator aten::native_group_norm_backward triaged module: mps,triaged module: mps
Error when trying to export MONAI model to ONNX module: onnx triaged onnx-needs-info,module: onnx triaged onnx-needs-info
test_public_bindings is not robust to various build options triaged topic: build,triaged topic: build
Autograd will take `init` module API into account when using `jit` oncall: jit,oncall: jit
[ONNX] Track non-exportable pattern as diagnostics. module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
JIT script calculation/dtype inconsistent depending on operator expression oncall: jit,oncall: jit
"torch.nn.functional.interpolate fails on some degenerate shapes, but passes on others triaged module: interpolation",triaged module: interpolation
INTERNAL ASSERT when the type of argument is not considered in JIT oncall: jit,oncall: jit
Beta distribution behaves incorrectly for small parameters module: distributions triaged module: edge cases,module: distributions triaged module: edge cases
torch.hub.load local model triaged module: hub,triaged module: hub
Autogenerated out functions are missing at::cpu:: and co bindings triaged module: codegen topic: build,triaged module: codegen topic: build
Accept SymInts and SymFloats For Scalar Inputs triaged,triaged
Uneven and/or Dynamically sized collectives good first issue triaged module: c10d,good first issue triaged module: c10d
torch.jit.script IndentationError: unexpected indent oncall: jit,oncall: jit
module: multiprocessing SimpleQueue put cannot bigger 716 in windows.And it is not has any info.The program is blocked and does not move. module: multiprocessing triaged,module: multiprocessing triaged
Tensor slice copy across multiple devices fails silently triaged module: advanced indexing,triaged module: advanced indexing
Tensor Subclass that doesn't require grad may wrap a Tensor subclass that requires grad triaged tensor subclass,triaged tensor subclass
[optim] asgd : handling of complex params as real params (NaN vs inf) module: optimizer triaged module: edge cases,module: optimizer triaged module: edge cases
Pytorch does not recognize GPU in WSL2 triaged module: wsl,triaged module: wsl
list of tensors can't be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor triaged module: numpy,triaged module: numpy
OpInfo tests should compare gpu to cpu implementations module: tests triaged topic: not user facing,module: tests triaged topic: not user facing
`tensordot` not working for dtype int32 and lower when there is only 1 element in the given axis triaged module: linear algebra actionable bug,triaged module: linear algebra actionable bug
"test_prims.py:test_nvfuser_no_args_cuda, memory leak triaged module: primTorch",triaged module: primTorch
nn.Softmax should not allow default/implicit/unset dim constructor argument module: nn triaged needs research module: deprecation,module: nn triaged needs research module: deprecation
"Issue with MPS ops lead to make_grid broken with mps device Tensors, whole grid is the 'first' image triaged module: mps",triaged module: mps
MPS backend appears to be limited to 32 bits triaged module: mps,triaged module: mps
Torch.FX work with autograd.Function triaged module: fx fx,triaged module: fx fx
[NVFuser] RuntimeError: ref_id_it != replayed_concrete_ids_.vector().end() INTERNAL ASSERT FAILED triaged module: assert failure module: nvfuser,triaged module: assert failure module: nvfuser
functionalize: Does not compose cleanly with torch.jit.script/torch.jit.trace oncall: jit module: functionalization,oncall: jit module: functionalization
"For PyTorch Nightly, failure when changing MPS device to CPU after PYTORCH_ENABLE_MPS_FALLBACK occurs. triaged module: mps",triaged module: mps
Install LibTorch by Conan or other C++ package manager module: cpp triaged topic: binaries,module: cpp triaged topic: binaries
Strange cuda illegal memory allocation error module: cuda triaged,module: cuda triaged
Set up tests to run periodically and surface them on HUD module: ci triaged,module: ci triaged
Deepcopy of FX graph fails with nested make_fx and constant tensors triaged module: fx fx,triaged module: fx fx
several questions about pytorch DDP triaged module: nccl module: ddp,triaged module: nccl module: ddp
Odd type-casting behaviour in prims.div triaged module: type promotion module: primTorch,triaged module: type promotion module: primTorch
Installation prefix is not passed to CMake appropriately needs reproduction module: build triaged,needs reproduction module: build triaged
torch.Size should convert all elements to ints triaged module: python frontend,triaged module: python frontend
"RuntimeError: ""slow_conv2d_cpu"" not implemented for 'Half' triaged module: half",triaged module: half
Lack of newly raised optimizers high priority feature module: optimizer triaged needs research,high priority feature module: optimizer triaged needs research
fix ATen tests that do not compile triaged,triaged
torch 1.12.1 cuda 10.2 runs slower than torch 1.8.2 cuda 10.2 module: performance module: cudnn module: cuda triaged,module: performance module: cudnn module: cuda triaged
Should enable skipped tests for `to` OpInfo  triaged module: primTorch,triaged module: primTorch
Torch.fx tracing bug with dictionary.update calls on input triaged module: fx fx,triaged module: fx fx
DecompositionInterpreter creates invalid graph triaged module: nvfuser,triaged module: nvfuser
Unable to run a single convolutional layer in different CUDA-contexts module: cuda triaged,module: cuda triaged
Fix convert path for fixed qparam ops (sigmoid and softmax) oncall: quantization low priority triaged,oncall: quantization low priority triaged
torch.Tensor.to.dtype_layout overload is not available in Python triaged module: codegen module: python frontend,triaged module: codegen module: python frontend
relu-gru mse is 0.022 much greater than 0.003 with half dtype. needs reproduction triaged module: half,needs reproduction triaged module: half
would you like  upload to the cpp libtorch to  vcpkg  package repo? module: cpp module: ci triaged enhancement topic: binaries,module: cpp module: ci triaged enhancement topic: binaries
Ensure ops account for offsets and strides triaged module: nestedtensor release notes: nested tensor,triaged module: nestedtensor release notes: nested tensor
Randomness should be consistent across devices with use_deterministic_algorithms triaged module: random module: determinism,triaged module: random module: determinism
Gradient value calculation error in MultiLabelMarginLoss module: loss module: cuda triaged module: correctness (silent),module: loss module: cuda triaged module: correctness (silent)
Pytorch gets small bias on the result of different types of divisors while doing floating point division. module: numerical-stability module: cuda triaged,module: numerical-stability module: cuda triaged
CUDA 11.6 linux-bionic-cuda11.6-py3-gcc7-slow-gradcheck failure module: ci triaged module: linear algebra,module: ci triaged module: linear algebra
"RuntimeError: outputs_[i]->uses().empty() INTERNAL ASSERT FAILED at /pytorch/torch/csrc/jit/ir.cpp:1027, please report a bug to PyTorch.  (eraseOutput at /pytorch/torch/csrc/jit/ir.cpp:1027) triage review oncall: jit",triage review oncall: jit
Session of Google Colab crashes when `torch.utils::SummaryWriter` is called after importing `torchaudio` high priority module: crash triaged module: tensorboard,high priority module: crash triaged module: tensorboard
Support setting strides on quantized weights of Embedding oncall: quantization low priority triaged,oncall: quantization low priority triaged
Please include virtual/physical batch sizes in the tutorials module: docs triaged,module: docs triaged
MPS convolution is sometimes returning NaNs for valid inputs. triaged module: mps,triaged module: mps
[jit] ignored method calling static method results in an error oncall: jit,oncall: jit
Move self.subtest calls in FSDP test suite to run_subtests utility oncall: distributed triaged better-engineering module: fsdp,oncall: distributed triaged better-engineering module: fsdp
Better error message for qlinear_prepack oncall: quantization triaged,oncall: quantization triaged
scripted fasterRCNN model cannot be loaded with libtorch c++ API oncall: jit module: cpp,oncall: jit module: cpp
 Index out of bounds Error with PerChannel Quantization  oncall: quantization triaged,oncall: quantization triaged
model.load_state_dict won't work in Child process if a sufficiently large tensor was padded in the Parent (even if empty padded) high priority module: multiprocessing module: memory usage triaged,high priority module: multiprocessing module: memory usage triaged
I cannot install pytorch by Bad CRC-32 for file 'torch/lib/libtorch_cpu.so' module: build triaged,module: build triaged
COREMLTOOLs/NNPACK Python Issue triaged module: nnpack,triaged module: nnpack
"hipErrorNoBinaryForGpu, but reversed module: rocm triaged",module: rocm triaged
[MPS] MPSNDArray error: product of dimension sizes > 2**31 triaged module: mps,triaged module: mps
"fill_ OpInfo code not used, also, doesn't test the case where the second argument is a Tensor module: tests triaged",module: tests triaged
[Nested Tensor] Enable Nestedtensor to work with OpInfos triaged module: nestedtensor,triaged module: nestedtensor
Linux cuda-11.x binary build  jobs intermittently take more than 4 hours high priority oncall: releng module: ci triaged,high priority oncall: releng module: ci triaged
General NestedTensor op coverage tracking issue feature triaged module: nestedtensor,feature triaged module: nestedtensor
PyTorch EC2 runners can not be used with standard actions module: ci triaged,module: ci triaged
Scatter min/max reduce operation that returns the corresponding indices triaged enhancement module: scatter & gather ops,triaged enhancement module: scatter & gather ops
Undefined reference in libtorch_cpu.so `...std::__cxx11::basic_string...` module: build triaged,module: build triaged
pytorch 1.12.1 doesn't build with ffmpeg 5.0 module: build triaged,module: build triaged
"Python3 Depletes 2021 M1 Mac Memory Running Training Ops For Model's M, L and X triaged module: macos",triaged module: macos
[FSDP] Make sharded / unsharded check more robust oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Are PyTorch Android nightly builds getting automatically published module: ci triaged module: android,module: ci triaged module: android
empty_quantized should probably be new_empty_quantized oncall: quantization triaged,oncall: quantization triaged
Add torch nightly builds pipeline for aarch64 linux module: ci triaged enhancement module: arm,module: ci triaged enhancement module: arm
Hitting rate limits for pytorchbot token  triaged module: infra,triaged module: infra
primTorch: support refs and decompositions when ATen and Python disagree triaged module: primTorch,triaged module: primTorch
 ModuleNotFoundError: No module named 'torch.ao.quantization.experimental' oncall: quantization low priority triaged,oncall: quantization low priority triaged
Support primtorch view ops in functionalization triaged module: viewing and reshaping module: functionalization module: primTorch,triaged module: viewing and reshaping module: functionalization module: primTorch
"RAM not free when deleting a model in CPU? worse after inference, is there some cache hidden? module: memory usage triaged",module: memory usage triaged
Tracking nested tensor functions with backward kernels registered in derivatives.yaml triaged module: nestedtensor,triaged module: nestedtensor
Grad strides do not match bucket view strides oncall: distributed triaged module: memory format module: ddp,oncall: distributed triaged module: memory format module: ddp
"Bug in batch names with matmul (result tensor has names=('i', 'i', 'k')). triaged module: named tensor",triaged module: named tensor
Improve FSDP error msg on wrong attr access oncall: distributed module: bootcamp triaged pt_distributed_rampup module: fsdp,oncall: distributed module: bootcamp triaged pt_distributed_rampup module: fsdp
bfloat16 matmul gives incorrect result on CPU (without mkldnn) module: cpu triaged module: bfloat16 module: linear algebra,module: cpu triaged module: bfloat16 module: linear algebra
Pytorch/Nova CI should monitor service outages for major dependencies module: ci triaged needs design,module: ci triaged needs design
torch fx cannot trace assert for some cases triaged fx,triaged fx
test_lazy spuriously fails if LAPACK is not installed module: tests triaged module: lazy,module: tests triaged module: lazy
torch.linalg.eigh crashe for matrices of size 2895Ã—2895 or larger on eigen and M1 module: crash triaged module: linear algebra module: m1,module: crash triaged module: linear algebra module: m1
torch.var_mean is slower than layer norm module: performance module: nn triaged needs research,module: performance module: nn triaged needs research
Error on installation module: rocm triaged,module: rocm triaged
[Nested Tensor] Move nested tensor specific ops to nested namespace triaged module: nestedtensor,triaged module: nestedtensor
[Nested Tensor] view + inplace for autograd.  module: autograd triaged module: nestedtensor,module: autograd triaged module: nestedtensor
Missing header file triaged,triaged
[Nested Tensor] Update TestCase.AssertEqual triaged module: nestedtensor module: testing,triaged module: nestedtensor module: testing
BCELoss results in autocast CUDA warning triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
nvfuser + prim stack generated illegal PTX code on hardware with sm <= 70 triaged module: nvfuser module: primTorch,triaged module: nvfuser module: primTorch
Build from source failed on MacOS 10.6 with CUDA 10.1  module: build triaged module: macos,module: build triaged module: macos
[Bug] Circular Import  caffe2 triaged,caffe2 triaged
Inconsistency between index_select and __get_item__ triaged module: advanced indexing,triaged module: advanced indexing
quantization: unexpected casting of tensor min and max to int in histogram observer oncall: quantization triaged,oncall: quantization triaged
[Discussion] Add custom device triaged module: dispatch module: backend,triaged module: dispatch module: backend
Unhelpful error message from torch.linalg.ldl_factor triaged module: linear algebra module: edge cases,triaged module: linear algebra module: edge cases
"test_profiler_experimental_tree_cuda_detailed is too unstable, and as its CUDA only difficult to regen high priority triage review module: tests triaged oncall: profiler",high priority triage review module: tests triaged oncall: profiler
Segfault when profiling with_stack=True on model with jit.optimize_for_inference oncall: jit,oncall: jit
Silent promotion of bool to int in the dispatcher triaged module: type promotion module: pybind module: library,triaged module: type promotion module: pybind module: library
Conv1d: NNPACK SpatialConvolution_updateOutput failed when batchsize or padding is too large module: nn module: convolution triaged module: nnpack,module: nn module: convolution triaged module: nnpack
libtorch malloc cause coredump   module: crash module: cpp triaged module: static linking,module: crash module: cpp triaged module: static linking
KL-divergence of two Generalized Dirichlet distributions module: distributions feature triaged,module: distributions feature triaged
I have the same issue as @samgelman on my MacOS. triaged module: macos module: openmp module: third_party,triaged module: macos module: openmp module: third_party
Add a new argument `check_inf=True` (by default) or check_pos_inf / check_neg_inf to anomaly mode module: autograd triaged enhancement,module: autograd triaged enhancement
quantize_per_tensor/quantize_per_channel operators should honor the quant_min/quant_max from observer oncall: quantization triaged,oncall: quantization triaged
Cdist backward dependent on compute_mode module: autograd triaged actionable,module: autograd triaged actionable
 Build and Run QNNPACK on X86 module: build triaged,module: build triaged
"[Installation] conda installation hangs on ""Solving environment"" module: binaries triaged",module: binaries triaged
`torch.pinverse` produces wrong output! module: docs triaged module: linear algebra,module: docs triaged module: linear algebra
Calling torch.linalg.cholesky on a CPU tensor requires compiling PyTorch with LAPACK. triaged module: linear algebra,triaged module: linear algebra
`Frozen` module for transfer learning. module: nn triaged needs design,module: nn triaged needs design
Find way to add comments to merge_rules json module: ci triaged,module: ci triaged
Test public bindings in CI gives weird output on error high priority module: ci module: tests triaged module: python frontend,high priority module: ci module: tests triaged module: python frontend
"How to turn off determinism just for specific operations, e.g. upsampling through bilinear interpolation? module: cuda triaged module: determinism",module: cuda triaged module: determinism
"zero-numel tensor has ""RuntimeError: strides[cur - 1] == sizes[cur] * strides[cur] INTERNAL ASSERT FAILED"" in multi-thread. oncall: jit triaged module: nvfuser",oncall: jit triaged module: nvfuser
Complex-Valued Gaussian distributions module: distributions triaged module: complex module: random,module: distributions triaged module: complex module: random
DDP + FSDP: Investigate behavior for nn.Module APIs high priority triage review oncall: distributed triaged better-engineering module: ddp module: fsdp,high priority triage review oncall: distributed triaged better-engineering module: ddp module: fsdp
 Torch1.10.2 is slower than torch1.9.1 oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
dataparallel function doesn't work triaged module: data parallel,triaged module: data parallel
torch.Tag doesn't have accurate mypy info module: typing triaged,module: typing triaged
TorchVision testing in CI + test_fx triaged module: fx fx,triaged module: fx fx
Improvements to ProcessGroupGloo monitored_barrier high priority triage review oncall: distributed triaged module: c10d,high priority triage review oncall: distributed triaged module: c10d
addcdiv_ (in and out of place) not implemented for torch.float16 and cpu low priority triaged enhancement module: half,low priority triaged enhancement module: half
bmm operator in bfloat16 has low TFLOPS for some tensor shapes with CUDA 11.6 triaged module: cublas,triaged module: cublas
cannot import name 'ProcessGroup' from 'torch.distributed'  oncall: distributed triaged module: macos,oncall: distributed triaged module: macos
Emulating FP64 and increased precisions on Apple silicon feature triaged needs research module: mps,feature triaged needs research module: mps
PyYAML not listed as a dependency oncall: releng triaged,oncall: releng triaged
"During DDP training timm densenet121, mobilenetv2(v3) models do not save state_dict correctly. oncall: distributed triaged module: ddp",oncall: distributed triaged module: ddp
torch.nn.Upsample's error message is inconsistent with the documentation module: docs module: nn triaged actionable,module: docs module: nn triaged actionable
RPC: wait method of Future object return 0 sometimes in rpc framework high priority oncall: distributed triaged module: rpc,high priority oncall: distributed triaged module: rpc
torch.nn.TripletMarginLoss margin can be less than 0 module: nn triaged,module: nn triaged
The type of parameter 'p' in torch.nn.TripletMarginLoss wrong module: nn triaged,module: nn triaged
torch.nn.ReplicationPad{1|2}d supports more input dimension than are written on documentation module: docs module: nn triaged,module: docs module: nn triaged
torch.nn.PixelShuffle error message wrong module: nn triaged,module: nn triaged
torch.nn.MaxUnpool2d get negative size tensor module: nn triaged,module: nn triaged
torch.nn.InstanceNorm{1|2|3}d doesn't verify the value type of parameter num_features module: nn triaged,module: nn triaged
torchgen.model.FunctionSchema.parse fails with following ops' schema  triaged module: codegen,triaged module: codegen
Enable freezing parts of the model in Fully Sharded Data Parallel oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Check support of FSDP + set_materialize_grads(False) oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
"module 'torch.distributed' has no attribute 'pipeline' - macOS, PyTorch 1.12.1 oncall: distributed triaged pipeline parallelism release notes: distributed (pipeline)",oncall: distributed triaged pipeline parallelism release notes: distributed (pipeline)
"torch.nn.GRU runs long time, when num_layers is large module: nn triaged module: edge cases",module: nn triaged module: edge cases
torch.nn.functional.softplus / torch.nn.Softplus parameter beta can be set to zero module: nn triaged,module: nn triaged
deepcopy of LazyLinear fails module: nn triaged actionable,module: nn triaged actionable
torch.nn.functional.log_softmax  parameter '_stacklevel' undocumented module: nn triaged actionable,module: nn triaged actionable
Fail to install torch for source module: build triaged,module: build triaged
torch.nn.Hardtanh allows min_val > max_val module: nn triaged,module: nn triaged
"When padding is big int, torch.nn.functional.fold runs too long and can't return result module: nn triaged",module: nn triaged
Make FSDP easier to debug when erroring in backward pass high priority triage review oncall: distributed triaged module: fsdp,high priority triage review oncall: distributed triaged module: fsdp
bf16 strided tensor wrong calculation high priority triaged module: bfloat16 module: correctness (silent) module: reductions module: intel,high priority triaged module: bfloat16 module: correctness (silent) module: reductions module: intel
Cannot call CUDAGeneratorImpl::current_seed during CUDA graph capture module: cuda triaged,module: cuda triaged
[MPS] Bug on training CNN+LSTM triaged module: mps,triaged module: mps
Bug in building pytorch deploy from source in macos USE_DEPLOY=1  oncall: package/deploy imported,oncall: package/deploy imported
torch.nn.functional.avg_pool{1|2|3}d error message does not match what is described in the documentation module: docs module: nn triaged,module: docs module: nn triaged
One dlpack to rule them all triaged better-engineering module: dlpack,triaged better-engineering module: dlpack
[FSDP] `test_summon_single_param()` is misleading triaged module: fsdp,triaged module: fsdp
Redirect the old metrics.pytorch.org url to the new page module: ci triaged,module: ci triaged
[CI] Create periodic fuzzy testing for PyTorch build flags module: ci triaged,module: ci triaged
[CI] Split up periodic.yml into forward-fixable.yml and periodic.yml module: ci triaged,module: ci triaged
DPP training incompatibility with checkpoint and detach oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
make_fx + aot_autograd segfaults module: crash triaged module: fx fx module: functorch,module: crash triaged module: fx fx module: functorch
Updating the LTS version of the torch (1.8.2 -> 1.10.2\\1.11.2?) module: binaries triaged,module: binaries triaged
torch.empty_strided argument 'size'and 'stride' documentation wrong module: docs triaged,module: docs triaged
FSDP init can crash with shared parameters high priority triage review oncall: distributed triaged module: fsdp,high priority triage review oncall: distributed triaged module: fsdp
[JIT] Scripting modules fails for modules that contain nested NamedTuples oncall: jit,oncall: jit
Support for CSR Tensor with NN layers module: sparse module: nn triaged,module: sparse module: nn triaged
TestCommon.test_dtypes error message is confusing triaged module: testing,triaged module: testing
Incorrect tensor conversion to m1 MPS. triaged module: mps,triaged module: mps
torch.bitwise_xor argument 'other' documentation wrong module: docs triaged,module: docs triaged
"Slice operation on ""ragged"" dimension in NestedTensor triaged enhancement module: nestedtensor",triaged enhancement module: nestedtensor
Adding a warning of non-compatibility with forward hooks for the fast path of TransformerEncoderLayer triaged oncall: transformer/mha,triaged oncall: transformer/mha
functorch slow tests not being run in slow CI module: ci module: tests triaged module: functorch,module: ci module: tests triaged module: functorch
linalg and lu tests fail when run in parallel on linux cuda high priority module: cuda module: ci triaged module: linear algebra,high priority module: cuda module: ci triaged module: linear algebra
CUDA graph capturing fails for nn.Embedding and large batch sizes module: cuda triaged module: embedding module: cuda graphs,module: cuda triaged module: embedding module: cuda graphs
`torch.tensor` and `torch.as_tensor` keyword argument `device` documentation wrong module: docs triaged module: tensor creation,module: docs triaged module: tensor creation
GPU arch 8.6 is not covered by the `TORCH_CUDA_ARCH_LIST = All` option  module: build module: cuda triaged,module: build module: cuda triaged
Tensor operation hangs when used with multiprocessing module: multiprocessing triaged module: determinism shadow review,module: multiprocessing triaged module: determinism shadow review
Error building Pytorch 13.1 from Source on OS X 12.5 module: build module: protobuf triaged,module: build module: protobuf triaged
getDLContext in DLConvertor.h cannot be found triaged module: dlpack,triaged module: dlpack
functionalize and make_fx are not composable resulting in segfault and cuda error module: crash triaged module: fx fx module: functorch,module: crash triaged module: fx fx module: functorch
"[ROCm] build instruction is haphazard missing information unclear, build does not work module: docs module: rocm triaged",module: docs module: rocm triaged
Profiling results on CPU is not reliable module: performance triaged,module: performance triaged
[LibTorch] the C++ api needs detailed error reports like pytorch  module: logging triaged enhancement,module: logging triaged enhancement
UnaryUfuncInfo Sample Generation Ignores sample_kwarg function high priority triaged module: correctness (silent) module: testing,high priority triaged module: correctness (silent) module: testing
Subclass of Tensor doesn't support __format__ triaged tensor subclass,triaged tensor subclass
torch.Tensor.bag() should automatically implement bagging triaged enhancement,triaged enhancement
Refactor how errors decide whether to append C++ stacktrace triaged better-engineering,triaged better-engineering
DecompositionInterpreter creates invalid graphs for FX graph modules created with torch.fx.symbolic_trace triaged module: fx fx,triaged module: fx fx
torchdynamo backend failure suppression is insufficient when backend fails at runtime triaged oncall: pt2 module: dynamo,triaged oncall: pt2 module: dynamo
"Automating release process - Binary validation, Automatically generating get started page module: ci triaged",module: ci triaged
cur_dim == dimINTERNAL ASSERT FAILED at module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
"tensor.unfold don't check the parameter size value, that maybe less than 0.  module: error checking triaged module: edge cases",module: error checking triaged module: edge cases
build fail when using lto with gcc module: build triaged,module: build triaged
Move nested-tensor tutorial from prototype triaged module: nestedtensor,triaged module: nestedtensor
SequentialLR does not work correctly with multiple ConstantLR triaged module: LrScheduler,triaged module: LrScheduler
unittest.subTest and way to selectively mark subTests as expected failures triaged better-engineering module: testing,triaged better-engineering module: testing
Schema information for torch.* operations triaged module: __torch_function__ module: testing,triaged module: __torch_function__ module: testing
in-place variants should get their own OpInfos triaged better-engineering module: testing,triaged better-engineering module: testing
PyTorch crashes when running with OpenACC module: crash triaged module: openmp module: third_party,module: crash triaged module: openmp module: third_party
FakeTensor Support For Pickling triaged module: fakeTensor,triaged module: fakeTensor
contiguous() not work for rank 1 length 1 tensor. triaged module: dlpack,triaged module: dlpack
Deep copy models with `create_feature_extractor` produces different parameters triage review triaged module: vision oncall: fx,triage review triaged module: vision oncall: fx
DataLoader parameter pin_memory_device should accept torch.device type module: dataloader triaged,module: dataloader triaged
RFC: Add flag for RNN decomposition to all RNN modules feature module: rnn triaged,feature module: rnn triaged
make_fx is broken for all tracing modes high priority module: crash triaged module: fx fx,high priority module: crash triaged module: fx fx
Libtorch C++ torch::stack error needs reproduction module: cpp triaged,needs reproduction module: cpp triaged
Incorrect CPU implementation of CTCLoss backward step module: autograd module: loss triaged,module: autograd module: loss triaged
Use NestedTensor in RNN models triaged enhancement module: nestedtensor,triaged enhancement module: nestedtensor
Split up `common_methods_invocations.py`? triaged needs research better-engineering module: testing,triaged needs research better-engineering module: testing
Symbolic tensors are not printable module: printing triaged module: dynamic shapes,module: printing triaged module: dynamic shapes
Complex addition result in NaN when it shouldn't triaged module: complex module: NaNs and Infs,triaged module: complex module: NaNs and Infs
Implement torch.clamp() on sparse tensors with SparseCPU backend module: sparse triaged,module: sparse triaged
Cloning conjugate tensor in torch_dispatch context produces non equality. triaged module: complex module: __torch_dispatch__,triaged module: complex module: __torch_dispatch__
Does torch.utils.checkpoint compatible with torch.cuda.make_graphed_callables? module: checkpoint triaged module: cuda graphs,module: checkpoint triaged module: cuda graphs
Quantization issue in transformers oncall: quantization low priority triaged,oncall: quantization low priority triaged
Minor inconsistency in description of `attn_output_weights` in MultiheadAttention docs module: docs module: nn triaged actionable,module: docs module: nn triaged actionable
The torch::deploy document is not updated triaged module: deploy,triaged module: deploy
[JIT] _unsafe_view returns alias when size(input) = size argument oncall: jit,oncall: jit
Bilinear interpolation with antialiasing is slow in performance module: performance triaged,module: performance triaged
Problems in built-from-source pytorch with USE_DEPLOY=1 in Ubuntu triaged module: deploy,triaged module: deploy
masked_scatter_ is very lacking module: docs triaged module: scatter & gather ops,module: docs triaged module: scatter & gather ops
ufmt and flake8 lints race triaged,triaged
Offer a way to really force merges via pytorchbot module: ci triaged,module: ci triaged
[JIT] SchemaInfo warning appears out in the wild oncall: jit,oncall: jit
test_make_fx_symbolic_exhaustive should pass dynamic ints for shape arguments triaged fx module: dynamic shapes,triaged fx module: dynamic shapes
Add more Vulkan operations triaged module: vulkan ciflow/periodic,triaged module: vulkan ciflow/periodic
when distribute training  load pretrain model error oncall: distributed module: serialization,oncall: distributed module: serialization
Race condition between torch.tensor's view and /= (/= returns incorrect result) triaged module: partial aliasing,triaged module: partial aliasing
pytorch's checkpoint_wrapper does not save memory while fairscale's checkpoint_wrapper saves huge memory high priority oncall: distributed module: checkpoint,high priority oncall: distributed module: checkpoint
`torch.matrix_exp` doesn't handle NaN properly module: cuda triaged module: NaNs and Infs module: linear algebra,module: cuda triaged module: NaNs and Infs module: linear algebra
DEBUG=1 env var doesn't actually set DEBUG preprocessor macro module: build triaged enhancement,module: build triaged enhancement
[Reproducibility] Make tests say when unusual environment variables are set that change behavior of the test module: ci triaged,module: ci triaged
logspace inconsistently casts inputs to int before performing computation triaged module: tensor creation,triaged module: tensor creation
primtorch refs should be composite compliant triaged module: __torch_dispatch__ tensor subclass module: primTorch,triaged module: __torch_dispatch__ tensor subclass module: primTorch
logspace and linspace off by one on cuda for integer dtypes for some inputs triaged module: tensor creation,triaged module: tensor creation
Reordering test in PyTorch test suite induces dynamo failure triaged bug oncall: pt2 module: dynamo,triaged bug oncall: pt2 module: dynamo
[feature request] DataLoader to accept num_threads argument to auto-set number of threads for OpenMP / intra-op parallelism module: dataloader triaged module: openmp,module: dataloader triaged module: openmp
OOM during backward() leads to memory leaks needs reproduction module: autograd module: memory usage triaged,needs reproduction module: autograd module: memory usage triaged
backward not available for index and mask  needs reproduction module: autograd triaged module: sorting and selection,needs reproduction module: autograd triaged module: sorting and selection
"RuntimeError: ""reflection_pad2d"" not implemented for 'Half' in autocast enabled region triaged module: amp (automated mixed precision)",triaged module: amp (automated mixed precision)
"model.to(device) takes time forever on A40-8Q, NVIDIA. cuda11.1, torch1.9.1. module: cuda triaged",module: cuda triaged
Provide error handling for ops that don't yet support Dynamic Shape triaged lazy,triaged lazy
DataLoader: `pin_memory` should respect object attributes before object collection type module: dataloader triaged,module: dataloader triaged
`torch.sum` promotes integral tensors to `int64`. module: docs triaged module: type promotion actionable module: reductions,module: docs triaged module: type promotion actionable module: reductions
[Checkpoint] Support multiple unpack in saved tensor hooks module: checkpoint triaged,module: checkpoint triaged
DistributedDataParallel hangs when not using GPU 0 oncall: distributed module: ddp,oncall: distributed module: ddp
set_grad_enabled not respected when running on a web server module: dependency bug module: autograd triaged actionable,module: dependency bug module: autograd triaged actionable
Stop manually binding sparse factory functions module: sparse triaged,module: sparse triaged
Re-enable DynamicQuantModule in iOS simulator tests module: ci triaged module: ios,module: ci triaged module: ios
External libraries cannot have a requirements.txt that needs to install a cpp_extension module: cpp-extensions triaged,module: cpp-extensions triaged
Move functorch tests to under test/ module: tests triaged,module: tests triaged
[feature request] Discover actually loaded shared libraries at runtime module: build triaged enhancement,module: build triaged enhancement
torch.concat type hints fail for keyword argument module: typing triaged,module: typing triaged
"When using libtorch v1.10.2, calling at::slow_conv_dilated3d directly returns wrong results on cpu backend module: cpp module: convolution triaged",module: cpp module: convolution triaged
"RuntimeError: [1] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Timeout waiting for key: default_pg/0/0 after 1800000 ms  oncall: distributed triaged module: nccl module: c10d",oncall: distributed triaged module: nccl module: c10d
linear.matrix_power is not composite compliant triaged module: linear algebra,triaged module: linear algebra
Could be clearer that Cross Entropy takes logits as input module: docs module: nn module: loss triaged actionable,module: docs module: nn module: loss triaged actionable
Using DDP with num_workers > 0 hangs before entering the first training epoch loop oncall: distributed module: dataloader,oncall: distributed module: dataloader
Autocast documentation examples would break module: docs triaged module: amp (automated mixed precision),module: docs triaged module: amp (automated mixed precision)
CUDACachingAllocator should be cuda memory merge/compact friendly module: cuda triaged module: CUDACachingAllocator,module: cuda triaged module: CUDACachingAllocator
cant build with USE_VULKAN=1 high priority module: build triaged oncall: mobile module: vulkan,high priority module: build triaged oncall: mobile module: vulkan
[FSDP] deepcopy FSDP model for EMA results in error oncall: distributed module: fsdp,oncall: distributed module: fsdp
upsample_bilinear2d() received an invalid combination of arguments module: onnx module: nn triaged module: regression,module: onnx module: nn triaged module: regression
Documentation for torch.cuda.Event(blocking=True) is wrong module: docs module: cuda triaged,module: docs module: cuda triaged
Inconsistent implementation of quant_utils:: ChooseQuantizationParams compared with fbgemm:: ChooseQuantizationParams oncall: quantization triaged,oncall: quantization triaged
[PyTorch/XLA] Improve the XLA PR landing process module: ci triaged module: xla,module: ci triaged module: xla
linspace cpu and sometimes cuda is wrong on integral types triaged module: correctness (silent) module: python frontend module: edge cases,triaged module: correctness (silent) module: python frontend module: edge cases
Unify c10::Event and at::cuda::CUDAEvent module: cuda triaged better-engineering,module: cuda triaged better-engineering
"nn.InstanceNorm and nn.GroupNorm are affected by padding, so they need to masking triaged module: nestedtensor module: norms and normalization module: padding module: masked operators oncall: pt2",triaged module: nestedtensor module: norms and normalization module: padding module: masked operators oncall: pt2
backwards compatibility ALLOWLIST is misused module: ci triaged,module: ci triaged
test_sparse_matmul_cpu_complex128 fails on my local copy module: sparse triaged module: complex,module: sparse triaged module: complex
test_sparse_spdiags_cpu_bool fails on my local working copy module: sparse triaged,module: sparse triaged
Tensor.backward type hints clarification module: docs module: autograd module: typing triaged actionable,module: docs module: autograd module: typing triaged actionable
Overloading multiple signatures for a single ref triaged module: primTorch,triaged module: primTorch
Investigate adding shell linter/checker to CI module: ci triaged,module: ci triaged
Investigate adding Dockerfile linter hadolint to CI module: ci triaged,module: ci triaged
"Devirtualize sym_sizes, virtualize sym_sizes_custom triaged",triaged
implement sym_numel triaged,triaged
Make sure we always redispatch through a dispatcher for all SymInt ops triaged,triaged
Dependency header directory is not properly expanded in the utils.cpp_extention in ninja mode module: cpp-extensions triaged,module: cpp-extensions triaged
RuntimeError: CUDA error: no kernel image is available for execution on the device module: cuda triaged,module: cuda triaged
dtype mismatch when after using auto mixed precision triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
grid_sample and mode='bilinear' induces errors at discrete pixel locations module: nn triaged,module: nn triaged
Compatibility with newest MKL module: build triaged module: mkl,module: build triaged module: mkl
Enable jit error when using FSDP oncall: jit,oncall: jit
Workflows fail silently when the workflow file is invalid module: ci triaged,module: ci triaged
"Rename DispatchKey Dense/Sparse/etc to DenseFunctionality/SparseFunctionality, use original name for alias module: internals triaged",module: internals triaged
Modernize logging tensor in torch.testing._internal module: internals module: logging triaged,module: internals module: logging triaged
BatchNorm for complex tensor triaged module: complex module: primTorch,triaged module: complex module: primTorch
Inconsistent naming convention for end of enum in DispatchKey triaged module: dispatch,triaged module: dispatch
PyTorch Embedding Op with max_norm is not working as expected module: cuda triaged module: norms and normalization module: embedding bug,module: cuda triaged module: norms and normalization module: embedding bug
Dispatcher debug/logging mode triaged module: dispatch,triaged module: dispatch
Failed to static link latest cuDNN while compiling module: build module: cudnn triaged,module: build module: cudnn triaged
Message exchange failure when perform alltoallv (cpus)  high priority triage review oncall: distributed module: c10d,high priority triage review oncall: distributed module: c10d
Python operator registration API for subclasses feature triaged module: dispatch module: __torch_dispatch__,feature triaged module: dispatch module: __torch_dispatch__
FakeTensor consolidated strategy for in_kernel_invocation and dispatch keys triaged module: fakeTensor,triaged module: fakeTensor
Provide an option to disable CUDA_GCC_VERSIONS module: build triaged,module: build triaged
Export quantized shufflenet_v2_x0_5 to ONNX module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Register refs for CompositeImplicitAutograd ops as decompositions triaged module: primTorch,triaged module: primTorch
[Tracker] AO migration of quantization from `torch.nn` to `torch.ao.nn` oncall: quantization low priority triaged,oncall: quantization low priority triaged
[packaging] Conda install missing python local version label (+cu123 or +cpu) oncall: releng triaged,oncall: releng triaged
optimize_for_mobile has an issue with constant operations at the end of a loop oncall: mobile,oncall: mobile
RFC: auto-generated plain Tensor argument only sparse primitives module: sparse triaged,module: sparse triaged
Idiom for PrimTorch refs for Tensor methods triaged module: primTorch,triaged module: primTorch
`sparse_coo.to_dense()` produces different results between CPU and CUDA backends for boolean non-coalesced inputs. module: sparse triaged,module: sparse triaged
Windows Debug binaries crash on forward: assert fail on IListRefIterator destructor oncall: jit,oncall: jit
[bug] the output shape from torch::mean and torch::var is different  in libtorch module: cpp triaged,module: cpp triaged
PyTorch 1.12 cu113 Illegal Memory Access or Internal Error instead of Out of Memory cases module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
"FakeTensorMode cannot handle non-fake tensor, but non-fake tensors can arise from non-interposable Tensor construction calls needs reproduction triaged oncall: pt2",needs reproduction triaged oncall: pt2
Improve interaction of PyTorch downstream libraries and torchdeploy triaged module: deploy,triaged module: deploy
__getitem__ is returned as an OverloadPacket instead of an OpOverload in __torch_dispatch__ triaged module: __torch_dispatch__ bug,triaged module: __torch_dispatch__ bug
[Profiler] Defer thread assignment for python startup events. triaged oncall: profiler,triaged oncall: profiler
float' object is not callable when using scheduler.step() with MultiplicativeLR module: optimizer triaged actionable,module: optimizer triaged actionable
Precision error from torch.distributed.send() to recv() oncall: distributed,oncall: distributed
Torch does not build with Lazy TS disabled module: build triaged,module: build triaged
Linking pytorch libraries causes sstream behavior to be overridden globally module: build triaged,module: build triaged
[vulkan]compiling VulkanOpContext.cpp with some errors module: build triaged module: vulkan,module: build triaged module: vulkan
CapabilityBasedPartitioner treats non-compute ops inconsistently triaged module: fx module: CapabilityBasedPartitioner module: fx.passes,triaged module: fx module: CapabilityBasedPartitioner module: fx.passes
forward program terminated from __cxa_pure_virtual needs reproduction module: crash module: cpp module: autograd triaged shadow review,needs reproduction module: crash module: cpp module: autograd triaged shadow review
CapabilityBasedPartitioner doesn't support horizontal (vertical?) fusion triaged module: nvfuser,triaged module: nvfuser
jit gives surprising results with lists of objects oncall: jit,oncall: jit
Missing corner case handling in ATen ctc_loss implementation module: loss module: error checking triaged,module: loss module: error checking triaged
torch.utils.checkpoint optimization opportunity module: autograd triaged enhancement has workaround,module: autograd triaged enhancement has workaround
torch.randint should accept high=2**63 triaged module: random module: edge cases,triaged module: random module: edge cases
torch.stft does not normalize non-rectangular windows correctly triaged module: complex module: fft,triaged module: complex module: fft
[FSDP] `test_mp_embedding_reduce()` fails with `transformer_auto_wrap_policy()` triaged module: fsdp,triaged module: fsdp
Add a check to detect mutation of the inputs during backward module: autograd module: molly-guard triaged actionable,module: autograd module: molly-guard triaged actionable
torch.searchsorted error message and documentation is unclear module: docs triaged module: sorting and selection,module: docs triaged module: sorting and selection
num_worker and prefetch_factor in DataLoader do not scale module: multiprocessing module: dataloader triaged,module: multiprocessing module: dataloader triaged
"""Attempted to resize a view tensor to a larger size. This is not allowed in the functionalization pass"" reported on non view tensor triaged module: functionalization",triaged module: functionalization
quantization: QConfigMapping should be easy to print oncall: quantization triaged,oncall: quantization triaged
Segfault with fake tensor triaged module: fakeTensor,triaged module: fakeTensor
[Prims+NvFuser] Issue with aten.where.ScalarSelf triaged module: nvfuser,triaged module: nvfuser
JIT trace takes forever on a simple method oncall: jit,oncall: jit
Reductions on tensors larger than GPU memory feature module: cuda triaged needs research,feature module: cuda triaged needs research
`torch.overrides.get_testing_overrides` does not function as intended for native tensor methods/operations triaged module: __torch_function__,triaged module: __torch_function__
Incorrect results for mean or sum kernels on aarch64 when building with gcc-7 triaged module: arm,triaged module: arm
[Prims+NvFuser] Non-fusible ops Tracker triaged module: nvfuser,triaged module: nvfuser
Files downloaded with torch.hub should respect umask triaged module: hub,triaged module: hub
Runtime error in Libtorch cpp project (Didn't find engine for operation quantized::conv2d_prepack NoQEngine) oncall: quantization triaged,oncall: quantization triaged
Refactor linter adapters to avoid code duplication module: lint triaged enhancement,module: lint triaged enhancement
High GPU context memory on Torch 1.11.0 but none on Torch 1.10.0 needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
"Libtorch cannot load TrochScript Module correctly, when a network contains conv2d(inchannels=64, outchannels=128, kernelsize=1) . oncall: jit",oncall: jit
CapabilityBasedPartitioner does not work correctly with mutating operations triaged module: fx module: CapabilityBasedPartitioner,triaged module: fx module: CapabilityBasedPartitioner
Functionalization and fake tensors failure in torture test triaged module: __torch_dispatch__ module: functionalization,triaged module: __torch_dispatch__ module: functionalization
torch.fx.node.map_aggregate and torch.utils._pytree.tree_map do the same thing triaged module: fx module: pytree,triaged module: fx module: pytree
torch._weight_norm with specified dim returns wrong output module: nn module: error checking triaged module: regression module: norms and normalization,module: nn module: error checking triaged module: regression module: norms and normalization
grad not preserved during copying or pickling triaged module: python frontend,triaged module: python frontend
[Mac M1] `torch.mm` sometimes produces incorrect results high priority module: cpu triaged module: correctness (silent) module: arm module: m1,high priority module: cpu triaged module: correctness (silent) module: arm module: m1
build libtorch with the same mkl as Matlab module: binaries triaged module: mkl,module: binaries triaged module: mkl
move bazel files out of pytorch repo root triaged module: bazel,triaged module: bazel
SparseAdam performance issue during optimizer step module: performance module: sparse module: optimizer triaged,module: performance module: sparse module: optimizer triaged
libprotobuf version compatibility  triaged module: build warnings,triaged module: build warnings
Docker updates cause subsequent builds to fail high priority module: ci triaged,high priority module: ci triaged
torch.package can not be used to serialize `resnet18` from TorchVision-0.12 high priority module: vision module: regression oncall: package/deploy imported,high priority module: vision module: regression oncall: package/deploy imported
CI: Run cpu tests in parallel processes? module: ci triaged,module: ci triaged
Resize/reshape of sparse compressed tensors - design module: sparse triaged,module: sparse triaged
[discussion] Consolidation of audio-visual I/O in a new package module: build triaged module: vision module: third_party,module: build triaged module: vision module: third_party
[jit] Failed to load a saved scripted function oncall: jit,oncall: jit
RuntimeError: required keyword attribute 'value' is undefined high priority triage review oncall: jit,high priority triage review oncall: jit
[Releng] Improve the tutorials release process module: build module: ci triaged,module: build module: ci triaged
three typing inconsistencies on Tensor methods module: typing triaged,module: typing triaged
"[Prims+NVFuser] nvFuser running into ""Tensors of type SparseTensorImpl do not have strides"" triaged module: nvfuser",triaged module: nvfuser
[Prims+NVFuser] Aten2Aten decomp hurting performance triaged module: nvfuser module: primTorch,triaged module: nvfuser module: primTorch
ExpandedWeights sometimes fail silently and doesn't compute .grad_sample attribute module: nn triaged,module: nn triaged
ExpandedWeights can't handle modules with tied weights module: nn triaged,module: nn triaged
torch.nn.functional.linear fails for multi-dimensional bias from torch 1.12 module: nn triaged module: regression,module: nn triaged module: regression
[LTC] OOM on mnist example triaged module: lazy,triaged module: lazy
[jit] script backward wrong gradient oncall: jit,oncall: jit
Position embedding aware global circular convolution feature module: nn triaged,feature module: nn triaged
"Interpolation artifacts when using nn.interpolate, trilinear mode for 3D label images module: nn triaged module: interpolation",module: nn triaged module: interpolation
[primTorch] `|` operator does not work with FakeTensor in _refs feature triaged module: meta tensors module: primTorch,feature triaged module: meta tensors module: primTorch
make_fx doesn't work with truly dynamic argument functions (e.g. fx.Interpreter) triaged module: fx,triaged module: fx
slow test infra cannot handle nested suites module: ci triaged,module: ci triaged
C++ extensions inject a bunch of compilation flags module: binaries module: cpp-extensions triaged better-engineering,module: binaries module: cpp-extensions triaged better-engineering
[BE] Refactor FSDP Unit Tests triaged better-engineering module: fsdp,triaged better-engineering module: fsdp
SummaryWriter add_embedding issue with label_img oncall: visualization,oncall: visualization
"jit.freeze throws RuntimeError: stack_out && stack_out->size() == 1 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/passes/frozen_conv_folding.cpp"":281 oncall: jit",oncall: jit
Compatibility List module: binaries module: docs triaged,module: binaries module: docs triaged
[bug][nvfuser] Applying nvfuser to the model leads to runtime error triaged module: nvfuser,triaged module: nvfuser
Inconsistent computation of gradient in MaxUnPooling module: autograd triaged module: determinism actionable module: correctness (silent),module: autograd triaged module: determinism actionable module: correctness (silent)
Ne op does not behaves as expected with nan high priority needs reproduction triaged,high priority needs reproduction triaged
Add typing support to ModuleList and ModuleDict module: typing triaged,module: typing triaged
"The result of doing a dot product between two vectors, using einsum, depends on another unrelated vector triaged module: numerical-reproducibility",triaged module: numerical-reproducibility
`torch.renorm` gives wrong gradient for 0-valued input when `p` is even and `maxnorm=0`. module: autograd triaged module: edge cases,module: autograd triaged module: edge cases
`hardshrink` gives wrong gradient for 0 input when `lambd` is 0. module: autograd triaged module: edge cases,module: autograd triaged module: edge cases
`torch.inverse()` crash in cuda triaged module: linear algebra module: correctness (silent),triaged module: linear algebra module: correctness (silent)
Anaconda is not a package manager module: docs triaged,module: docs triaged
Let torch.utils.tensorboard support multiprocessing module: multiprocessing triaged module: tensorboard,module: multiprocessing triaged module: tensorboard
`atan2` will gradcheck fail when `other` is a tensor with `int8` dtype module: autograd triaged module: edge cases,module: autograd triaged module: edge cases
`det` will return wrong gradient for `1x1` matrix with 0 value. module: autograd triaged module: edge cases,module: autograd triaged module: edge cases
Unable to use vmap atop torch.distribution functionality high priority triaged module: functorch,high priority triaged module: functorch
Add TorchDynamo as a submodule to Pytorch? module: build triaged,module: build triaged
[jit.script] jit.script give uncertain results using torch.half oncall: jit module: nvfuser,oncall: jit module: nvfuser
pad_sequence and pack_sequence should support length zero tensors module: rnn triaged enhancement,module: rnn triaged enhancement
RuntimeError: DataLoader worker (pid 22822) is killed by signal: Aborted.  module: dataloader triaged,module: dataloader triaged
Semi-reproducible random torch.baddbmm NaNs needs reproduction triaged module: NaNs and Infs,needs reproduction triaged module: NaNs and Infs
`torch.ops.aten.find` inconsistent with `str.find` module: cpp triaged module: sorting and selection,module: cpp triaged module: sorting and selection
2-dimensional arange triaged enhancement module: nestedtensor module: tensor creation,triaged enhancement module: nestedtensor module: tensor creation
`bmm_sparse_cuda` kernel for `bfloat16` module: sparse module: cuda triaged module: bfloat16,module: sparse module: cuda triaged module: bfloat16
NVFuser should extend caching to remove necessity for PrimTorch's executor to Provide Tensor Contiguity Info triaged module: nvfuser module: primTorch,triaged module: nvfuser module: primTorch
Allow parameterization of Layouts module: sparse feature triaged module: python frontend,module: sparse feature triaged module: python frontend
[Prims+NVFuser] Prims with missing NVFuser ops triaged module: nvfuser module: primTorch,triaged module: nvfuser module: primTorch
[bug] libtorch bug in nn::MultiheadAttention and nn::Transformer module: cpp module: nn triaged oncall: transformer/mha module: correctness (silent),module: cpp module: nn triaged oncall: transformer/mha module: correctness (silent)
Negative values still produced by torch.nn.functional.kl_div high priority module: nn triaged,high priority module: nn triaged
Revisit OpInfo samples for nn.functional.max_poolNd module: nn triaged actionable module: pooling module: testing,module: nn triaged actionable module: pooling module: testing
scatter_reduce choosed indices triaged enhancement module: scatter & gather ops,triaged enhancement module: scatter & gather ops
CMake Error: File /opt/pytorch/build_variables.bzl does not exist. triaged module: regression module: docker,triaged module: regression module: docker
Torch fx print line number of each node triaged module: fx,triaged module: fx
Guard Failures in T5 Model triaged bug oncall: pt2 module: dynamo,triaged bug oncall: pt2 module: dynamo
[DDP] output_device argument appears completely unused oncall: distributed triaged better-engineering module: ddp,oncall: distributed triaged better-engineering module: ddp
[c10d] Async object-based collectives oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Tracker: Slow gradcheck failures possibly indicating incorrect gradients module: autograd triaged actionable,module: autograd triaged actionable
Support for learnable p Values in LPPOOL like Pool module: nn triaged needs research,module: nn triaged needs research
Modify _add_docstr to also set the correct module for the APIs triaged better-engineering actionable module: python frontend,triaged better-engineering actionable module: python frontend
[BE] Update ProcessGroupWrapper tests to test other collective message oncall: distributed triaged better-engineering module: c10d,oncall: distributed triaged better-engineering module: c10d
Distributed Store `get` doesn't work well with `add` high priority triage review oncall: distributed module: docs triaged module: c10d,high priority triage review oncall: distributed module: docs triaged module: c10d
Illegal Memory Access from nonzero method when Tensor is Too Large module: dependency bug module: crash triaged module: edge cases,module: dependency bug module: crash triaged module: edge cases
CosineAnnealingWarmRestarts with initial warm up and weight decay applied on consecutive cycles without warm up feature module: optimizer triaged needs research,feature module: optimizer triaged needs research
AttributeError: 'LinearPackedParams' object has no attribute '_modules' needs reproduction oncall: quantization module: nn triaged,needs reproduction oncall: quantization module: nn triaged
"Need ""valid"" and ""same"" padding mode for convTranspose2d feature module: nn module: convolution triaged module: padding",feature module: nn module: convolution triaged module: padding
Sort tensors inplace feature triaged module: sorting and selection,feature triaged module: sorting and selection
Cudnn batch norm kernel (batchnorm_bwtr_nhwc_semiPersist) gets blocked by overlapping NCCL all_reduce calls module: dependency bug module: cudnn triaged module: nccl module: memory format,module: dependency bug module: cudnn triaged module: nccl module: memory format
[complex] dropout and it's variants should support complex tensors feature module: nn triaged module: complex,feature module: nn triaged module: complex
Write some torch.distributed.nn.* tests for the new dispatcher passable ops oncall: distributed triaged,oncall: distributed triaged
Change c10d APIs in ProcessGroup to accept const std::vector<at::Tensor>& oncall: distributed triaged,oncall: distributed triaged
test_conv_backend tests OOMing in 10.2 slow_gradcheck CI module: nn module: ci module: convolution triaged,module: nn module: ci module: convolution triaged
[Prims+NVFuser] Supports 0-sized inputs triaged module: nvfuser module: primTorch,triaged module: nvfuser module: primTorch
[Prims+NVFuser] Aten2Prim refs tracking items triaged module: nvfuser module: primTorch,triaged module: nvfuser module: primTorch
Support tensor subclasses as `UninitializedParameter`s module: nn triaged enhancement module: lazy tensor subclass,module: nn triaged enhancement module: lazy tensor subclass
OpInfos for torch.ops.aten operations feature module: tests triaged,feature module: tests triaged
F.binary_cross_entropy_with_logits unexpected behaviour module: nn module: loss triaged,module: nn module: loss triaged
`soft_margin_loss` gives wrong gradient when `target` with dtype uint8 module: autograd module: nn module: loss triaged,module: autograd module: nn module: loss triaged
`max_unpool` gives wrong gradient when `indices` has duplicate module: autograd module: nn triaged module: pooling module: edge cases,module: autograd module: nn triaged module: pooling module: edge cases
[NVFuser] Investigate models without any fusion groups found  triaged module: nvfuser,triaged module: nvfuser
[NVFuser] Investigate modules with bad performance relative to eager triaged module: nvfuser module: primTorch,triaged module: nvfuser module: primTorch
Torch.fx: add reporting of the name of a module not found during tracing triaged module: fx,triaged module: fx
Catch value errors if cell in match_nested_cell is empty needs reproduction triaged bug oncall: pt2 module: dynamo,needs reproduction triaged bug oncall: pt2 module: dynamo
GEGLU activation module: nn triaged enhancement needs research,module: nn triaged enhancement needs research
AMP step() enforce synchronization triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
[RFC] Module specific workflows module: rocm triaged,module: rocm triaged
Elliptic Functions and Integrals feature triaged module: special,feature triaged module: special
[primTorch] No _refs support for torch.Tensor.requires_grad.__get__ triaged module: primTorch,triaged module: primTorch
Orthogonal Polynomials feature triaged module: special,feature triaged module: special
activation checkpointing with non_reentrant implementation memory leaks high priority triage review oncall: distributed triaged,high priority triage review oncall: distributed triaged
[feature request] Add support for a custom DatasetFetcher in DataLoader  module: dataloader triaged enhancement module: data,module: dataloader triaged enhancement module: data
Expose more MAGMA backends for solve_triangular triaged module: linear algebra module: magma,triaged module: linear algebra module: magma
"Allow a user provided ""test name - test time"" mapping file work with pytorch's test sharding mechanism module: ci triaged",module: ci triaged
Provide error message when thread pool is exhausted in RPC high priority oncall: distributed triaged module: rpc,high priority oncall: distributed triaged module: rpc
Complex support in DDP oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
"FakeTensor: Support torch.tensor([FakeTensor, 0]) triaged module: meta tensors",triaged module: meta tensors
pow CUDA tensor raised to CPU scalar tensor result can't backward properly module: autograd triaged actionable,module: autograd triaged actionable
`torch.special.gammainc` backward pass with respect to the first argument module: distributions module: autograd triaged,module: distributions module: autograd triaged
memory leaking when doing all_to_all_single communication  oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
RPC init fails and crashes when world_size is greater than 18 oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
static builds are broken by MKL_DNN module: build triaged module: regression module: third_party,module: build triaged module: regression module: third_party
Comprehensive documentation for Tensor indexing? module: docs triaged module: advanced indexing,module: docs triaged module: advanced indexing
Deterministic `index_put` on CUDA fails when broadcasting is required triaged module: advanced indexing,triaged module: advanced indexing
[CI] Do we run all cpp tests on CI? module: ci triaged,module: ci triaged
Modify update-viable-strict GHA to use internal version of checkout module: ci triaged,module: ci triaged
Write lint for isGreen module: ci triaged,module: ci triaged
`CosineAnnealingWarmRestarts` does not update parameters added with `add_param_group` triaged module: LrScheduler,triaged module: LrScheduler
All {view}_scatter variants should support all (or most) dtypes triaged module: functionalization,triaged module: functionalization
[bazel] [ci] `//:lazy_tests` Could not run 'aten::mul.Tensor' with arguments from the 'Lazy' backend module: cuda triaged lazy module: lazy module: bazel,module: cuda triaged lazy module: lazy module: bazel
[bazel] [ci] `//:module_test` CUDA error: CUDA driver version is insufficient for CUDA runtime version module: cuda triaged module: bazel,module: cuda triaged module: bazel
Automatically calculate output_shape of sequential model (or any other fCNN) triaged module: meta tensors,triaged module: meta tensors
Multi-node training meets unknown error oncall: distributed triaged,oncall: distributed triaged
Automatically use CUDA triaged needs research module: python frontend,triaged needs research module: python frontend
Parameter.__deepcopy__ doesn't preserve view relationships module: nn triaged module: correctness (silent),module: nn triaged module: correctness (silent)
Improve clarity by making sharding a static nightly update module: ci triaged,module: ci triaged
android-tests is often flaky module: ci triaged,module: ci triaged
[FSDP] Test that module using mixed precision can be loaded into non-mp module triaged better-engineering module: fsdp,triaged better-engineering module: fsdp
[JIT] failures with nested with blocks + loop continuation oncall: jit,oncall: jit
quantization: misleading backend config for linear_dynamic_fp16 oncall: quantization triaged,oncall: quantization triaged
[FX] TypeError when tracing cat taking split's output as input triaged module: fx,triaged module: fx
ONEDNN testing is not done properly in quantization codebase oncall: quantization triaged,oncall: quantization triaged
gradgradcheck fails for torch.native_layer_norm module: double backwards module: autograd triaged,module: double backwards module: autograd triaged
Float and double tensors randomly initialized with the same seed get different values for size >= 16 triaged module: random,triaged module: random
Does Torch JIT Support Trace High-level Custom Op? oncall: jit,oncall: jit
tensorboard SummaryWriter.add_graph fails when model uses empty tuples triaged module: tensorboard,triaged module: tensorboard
[FSDP] Progress of ParamExecOrderWrapPolicy in progress triaged module: fsdp,in progress triaged module: fsdp
Missing the time unit in duration time of DDP logging triaged module: ddp,triaged module: ddp
[FSDP] Verify that FSDP-managed parameters are the same across ranks triaged better-engineering module: fsdp,triaged better-engineering module: fsdp
PyTorch Preview (Nightly) version number does not comply with Conda conventions module: binaries triaged,module: binaries triaged
Some unit tests are failing oncall: distributed triaged module: docker,oncall: distributed triaged module: docker
__torch__dispatch does not return new output in inplace function triaged module: __torch_dispatch__,triaged module: __torch_dispatch__
Unable to use a parameter with torch.sparse_coo layout with DDP oncall: distributed module: sparse triaged module: ddp,oncall: distributed module: sparse triaged module: ddp
test_ops.py extremely slow on cuda11.3 triaged,triaged
"Display a ""reference"" link for ops that points to primTorch implementations module: docs triaged better-engineering module: primTorch",module: docs triaged better-engineering module: primTorch
[META] Sign up to discuss significantly modifying CI module: ci triaged,module: ci triaged
Add a new _broadcast_coalesced op for DDP triaged,triaged
Iteration # 1-offset in DDP logging oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
[BE][ZeRO] Enable multigpu unit tests oncall: distributed triaged better-engineering,oncall: distributed triaged better-engineering
[LTC] Make `torch::lazy::BackendImplInterface::ExecuteComputation` takes `ComputationPtr` instead of `Computation` triaged lazy,triaged lazy
Use c10d broadcast_object in Zero oncall: distributed module: bootcamp good first issue triaged better-engineering pt_distributed_rampup,oncall: distributed module: bootcamp good first issue triaged better-engineering pt_distributed_rampup
API for accessing SymIntNode mandates refcount bump even when it is unnecessary triaged,triaged
Add doc formatting check to lintrunner module: lint triaged better-engineering,module: lint triaged better-engineering
Conda enviroment triaged,triaged
SymInt equality tests are unsound triaged,triaged
caffe2_nvrtc is produced even when it won't be used module: build triaged module: selective build,module: build triaged module: selective build
Incorrect image upscaling on MPS backend triaged module: mps,triaged module: mps
torch failure to open libcuda.so.1 on macOS triaged module: macos,triaged module: macos
[build] No documented way to install C++ binaries for pure-python development of pytorch module: binaries module: build triaged,module: binaries module: build triaged
[bazel] build spams warnings triaged module: bazel,triaged module: bazel
Adam not optimally implemented: unnecessary torch.div module: performance module: optimizer triaged actionable,module: performance module: optimizer triaged actionable
[bazel] ability to run gpu tests on gpu machines in RBE triaged module: bazel,triaged module: bazel
Conda install from pytorch-nightly channel does not install the expected version on macOS module: binaries triaged module: macos,module: binaries triaged module: macos
Batches are being duplicated from go http call triaged,triaged
[ONNX] Internal assert error during export oncall: jit module: onnx onnx-triaged,oncall: jit module: onnx onnx-triaged
[NVFuser] hitting fallbacks on demucs (from torchbench + lazy tensor) triaged module: nvfuser,triaged module: nvfuser
`prepare_qat_fx` docstring doesn't run oncall: quantization triaged module: fx,oncall: quantization triaged module: fx
PyTorch gets stuck when using an NVLink/A6000 and more than two GPUs oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
"PyTorch leaks a macro definition called ""CHECK"" in the C++ version module: cpp triaged",module: cpp triaged
[NVFuser] bad performance on pyhpc_isoneutral_mixing triaged module: nvfuser,triaged module: nvfuser
[NVFuser] bad performance on mobilenet_v2 and mobilenet_v3_large triaged module: nvfuser,triaged module: nvfuser
[NVFuser] bad performance on pyhpc_equation_of_state triaged module: nvfuser,triaged module: nvfuser
scripted fft Convolutions are faster than nn.Conv1d with large kernels module: performance module: cuda module: convolution triaged,module: performance module: cuda module: convolution triaged
[ONNX] Enable more operators to support data propagation module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
[bug] Device dispatcher can choose CPU path for CUDA tensors. module: build triaged module: dispatch module: codegen,module: build triaged module: dispatch module: codegen
[feature request] Support dataclass derivations of nn.Module module: nn triaged,module: nn triaged
"[bug] fill_, masked_fill_ : fill ops allow lossy downcasting of fill value module: bc-breaking triaged topic: bc breaking module: primTorch",module: bc-breaking triaged topic: bc breaking module: primTorch
Mismatch in clang toolchain lead to binary incompatibilities on M1 between torch and torchvision module: binaries module: ci triaged module: macos module: arm,module: binaries module: ci triaged module: macos module: arm
"Triangular solve fails on batches of matrices of size > (*, 524280) module: cuda triaged module: linear algebra",module: cuda triaged module: linear algebra
_make_elementwise_unary_reference and other function factories in torch._refs don't set __name__ correctly triaged module: primTorch,triaged module: primTorch
DistributedDataParallel `static_graph=True` fails to handle unused parameters oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
PyTorch/XLA's DDP XLABackend is broken by upstream change oncall: distributed triaged module: xla module: ddp,oncall: distributed triaged module: xla module: ddp
Redundant info are saved when using torch.save to save part of torch.tensor module: serialization triaged,module: serialization triaged
torchvision.models.mobilenetv3 can't save pre-trained model to custom dir? triaged module: vision,triaged module: vision
Hide or fuse TupleConstruct / TupleUnpack from tensorboard graph triaged module: tensorboard,triaged module: tensorboard
[ONNX] `.squeeze(1)` on the B X T (not B X 1 X T) tensor causes export error in masking module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Error with Named Tensors and multiple threads triaged module: named tensor,triaged module: named tensor
Improve PrimTorch testing for view consistency. module: tests triaged module: viewing and reshaping module: primTorch,module: tests triaged module: viewing and reshaping module: primTorch
CI workflow creates too many tags in RSS feed module: ci triaged,module: ci triaged
"Multi-node, Multi-GPU set up tutorial for Slurm cluster oncall: distributed triaged",oncall: distributed triaged
[MetaIssue] Propagating SymInts through Autograd triaged lazy,triaged lazy
Mirror and implement `SymbolicIntNode` API for `SymInt` so we can trace in C++ triaged lazy,triaged lazy
[MetaIssue] Investigate if we should be reusing primtorch formulas for `is_dynamic` triaged lazy,triaged lazy
DDP Freezes w/ No Output for PyTorch Geometric GNN Multi-GPU Node Classification oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
Add a test that shows that lazy_ir reuse breaks SizeNodes triaged lazy,triaged lazy
Implement SymbolicIntNode interface for lazy (i.e. lazy::SymbolicIntNode) triaged lazy,triaged lazy
Devirtualize `sym_sizes`. It still has to work for python tensor subclasses and LTC/Xla triaged lazy,triaged lazy
Building PyTorch from Source with BUILD_LAZY_TS_BACKEND_ON module: build triaged,module: build triaged
"When setting sizes and strides on a tensor subclass in `THPVariable_make_wrapper_subclass`, also make offset symbolic triaged lazy module: lazy",triaged lazy module: lazy
Corner cases of ShardedTensor checkpoint when using TorchRec oncall: distributed module: checkpoint triaged sharded_tensor release notes: distributed (sharded),oncall: distributed module: checkpoint triaged sharded_tensor release notes: distributed (sharded)
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR module: crash module: cudnn module: cuda triaged module: ddp,module: crash module: cudnn module: cuda triaged module: ddp
Multi30k can't be downloaded the destination domain can't be reached triaged,triaged
Libtorch C++ mobile build linking error module: build oncall: mobile,module: build oncall: mobile
DataLoader leaking resources? module: dataloader triaged,module: dataloader triaged
[forwardAD] torch.no_grad has no effect under forward_ad triaged module: forward ad,triaged module: forward ad
Can we have Additive Attention? triaged oncall: transformer/mha,triaged oncall: transformer/mha
Add type() support for mps backend triaged actionable module: mps,triaged actionable module: mps
"If large enough tensor is being cloned, parallel dataloading hangs on M1 Mac high priority module: dataloader triaged module: macos module: deadlock module: arm",high priority module: dataloader triaged module: macos module: deadlock module: arm
Do we really need sampler for IterableDataset? module: performance module: dataloader triaged,module: performance module: dataloader triaged
LambdaLR changes the learning rate in an undesired way triaged module: LrScheduler,triaged module: LrScheduler
torch.fx deepcopy does not copy attributes added to GraphModule or Nodes triaged module: fx,triaged module: fx
Abnormal GPU memory usage when using CUDA tensors with multiprocessing module: multiprocessing module: cuda module: memory usage triaged module: jiterator,module: multiprocessing module: cuda module: memory usage triaged module: jiterator
Cannot build master on AWS cluster: error: â€˜__fatDeviceTextâ€™ was not declared in this scope module: build module: cuda triaged,module: build module: cuda triaged
fatal_signal_asan_no_sig_test in current master hang. module: cpp triaged module: deadlock module: testing,module: cpp triaged module: deadlock module: testing
Improving clarity in the docs of different losses module: docs module: nn triaged module: python frontend,module: docs module: nn triaged module: python frontend
Remove const from function return type if returning const value module: cpp triaged,module: cpp triaged
[Profiler] Capture more information about inputs triaged oncall: profiler,triaged oncall: profiler
[RecordFunction] Hold a durable schema reference triaged oncall: profiler,triaged oncall: profiler
MPS: Adding int64 tensor does not work on AMD GPU triaged module: correctness (silent) module: mps,triaged module: correctness (silent) module: mps
"[Modes] no_dispatch is not the same as DisableTorchFunction, causing differences in modes triaged module: __torch_function__ module: __torch_dispatch__",triaged module: __torch_function__ module: __torch_dispatch__
Mac M1 Build Failure on DEBUG=1 needs reproduction module: build triaged module: macos module: arm,needs reproduction module: build triaged module: macos module: arm
Certain import order triggers segmentation fault module: crash triaged has workaround,module: crash triaged has workaround
Feature Request: Hessenberg and Schur decompositions feature triaged module: linear algebra,feature triaged module: linear algebra
Feature request: Integer system decompositions feature triaged module: linear algebra,feature triaged module: linear algebra
"torch.jit.script segmentation fault (pytorch debayer module) 1.10, 1.11 and nightly oncall: jit",oncall: jit
Efficiency of unary operations on CPU for large tensors module: performance triaged,module: performance triaged
Deprecate hardtanh type promotion behavior. module: nn triaged module: primTorch,module: nn triaged module: primTorch
[FSDP] Customizable gradient pre-divide for mixed precision training oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Extend tag testing for aliases triaged module: testing,triaged module: testing
Add `inplace_view` tag for `resize_` triaged module: viewing and reshaping,triaged module: viewing and reshaping
Unable to programmatically update models using references from model.named_modules()...requires additional parsing module: nn triaged enhancement,module: nn triaged enhancement
Expose docs from the yaml for each torch.Tag in Python  module: docs triaged module: dispatch module: library,module: docs triaged module: dispatch module: library
Test approximation and numerical stability of numerical operators  module: numerical-stability triaged module: testing,module: numerical-stability triaged module: testing
[primTorch] Sensible Error Messages triaged module: primTorch,triaged module: primTorch
New c10 constants module: internals triaged,module: internals triaged
[Better Engineering] Make OpInfo-based test failures easy to reproduce module: tests triaged better-engineering,module: tests triaged better-engineering
AlBert quantization oncall: quantization triaged,oncall: quantization triaged
[ONNX] Scripted `reshape` incorrect if shape is dynamically calculated module: onnx triaged onnx-triaged bug,module: onnx triaged onnx-triaged bug
ValueError during `yaml.dump(dtype)` module: serialization triaged,module: serialization triaged
BuildExtension does not choose correct CUDA installation module: cpp-extensions module: cuda triaged,module: cpp-extensions module: cuda triaged
"Unable to install Preview (Nightly) on M1 macOS: ""Symbol not found"" module: binaries triaged",module: binaries triaged
Allow batch_norm_backward_elemt and batch_norm_gather_stats_with_counts handle 0 counts triaged enhancement module: norms and normalization,triaged enhancement module: norms and normalization
"torch.distributed.init_process_group(backend=""nccl"") NCCL version error oncall: distributed",oncall: distributed
Debug job does not build in debug mode module: ci triaged,module: ci triaged
linalg.pinv_singular tests are slow module: autograd module: tests triaged module: linear algebra,module: autograd module: tests triaged module: linear algebra
Module parameters/submodules can be shadowed by class attributes silently module: nn triaged actionable,module: nn triaged actionable
torch.svd_lowrank fails for complex matrices triaged module: complex module: linear algebra,triaged module: complex module: linear algebra
RFC: Improve the performance and usability of linear algebra on CUDA devices module: cuda triaged module: linear algebra module: magma,module: cuda triaged module: linear algebra module: magma
LibTorch cannot be used without nvcc module: build triaged,module: build triaged
test_python_dispatch fails on DEBUG=1 triaged module: dispatch,triaged module: dispatch
Exponentiating floating number with cuda tensor is slow module: cuda triaged topic: performance,module: cuda triaged topic: performance
clear input shape declaration  on pytorch model inputs and outputs triaged module: shape checking module: python frontend,triaged module: shape checking module: python frontend
Parallel execution of multiple unrelated statements written sequentially triaged enhancement,triaged enhancement
[1.9.1] [collect_env] collect_env does not collect actual runtime-loaded cudnn version module: cudnn module: collect_env.py triaged enhancement,module: cudnn module: collect_env.py triaged enhancement
New feature requested: vmap for torch.histc high priority triaged module: functorch,high priority triaged module: functorch
torch.fx: symbolic_trace: ones() received an invalid combination of arguments triaged module: fx,triaged module: fx
Exception in torch.jit.script doesn't indicate where in the code the problem lies. oncall: jit,oncall: jit
torch.lerp: discrepancy between CUDA and CPU (with extremal inputs) triaged module: NaNs and Infs,triaged module: NaNs and Infs
is the issue resolved? windows not pytorch_jni in path oncall: java,oncall: java
RuntimeError: Event device type CUDA does not match blocking streamâ€™s device type CPU  module: autograd module: cuda module: tests triaged,module: autograd module: cuda module: tests triaged
`with torch.backends.cudnn.flags(deterministic=True)` doesn't give an exception for ctc_loss backward on CUDA module: cudnn triaged module: determinism,module: cudnn triaged module: determinism
"Softmax, LogSoftmax are over parameterized module: nn triaged",module: nn triaged
`layer_norm` triggers INTERNAL ASSERT with input requiring grad + zero-size int tensor module: autograd triaged actionable,module: autograd triaged actionable
`index_fill` will trigger INTERNAL ASSERT when float tensor requiring grad + int tensor module: autograd triaged actionable,module: autograd triaged actionable
fx.Tracer with param_shapes_constant=True not working for RobertaForMaskedLM triaged module: fx,triaged module: fx
Permutation of Sparse Tensor module: sparse triaged,module: sparse triaged
.lldbinit for lldb debuger feature triaged module: macos actionable,feature triaged module: macos actionable
torch.angle differs from np.angle for -0. triaged module: numpy module: primTorch,triaged module: numpy module: primTorch
Split up and reorganize RPC tests oncall: distributed triaged better-engineering module: rpc,oncall: distributed triaged better-engineering module: rpc
`gradcheck` fails for `torch.distribution.transform` APIs in forward mode module: distributions module: autograd triaged module: forward ad,module: distributions module: autograd triaged module: forward ad
TRACK: integral + floating inputs to an op with floating requiring grad result in INTERNAL_ASSERT module: autograd triaged actionable,module: autograd triaged actionable
 Memory allocation errors when attempting to initialize a large number of small feed-forward networks in RAM with shared memory despite having enough memory  module: memory usage triaged,module: memory usage triaged
Request for adding the possibility for training on sparse tensors module: sparse triaged,module: sparse triaged
"pytorch-android-lite use its own libfbjni.so, which is not compatible with any other version at all.. module: binaries triaged module: android",module: binaries triaged module: android
[CI] Detect when tests are no longer running from CI module: ci triaged,module: ci triaged
Floating point exception in _conv_depthwise2d triaged module: edge cases,triaged module: edge cases
Any plan to add Noam scheduling? triaged module: LrScheduler,triaged module: LrScheduler
`max_unpool2d` is not deterministic module: numerical-stability module: nn triaged module: determinism module: pooling,module: numerical-stability module: nn triaged module: determinism module: pooling
" USE_NATIVE_ARCH flag causes nvcc build failure due to ""'arch=native': expected a number"" module: build triaged",module: build triaged
Performance with MPS on AMD GPUs are worse than CPU module: performance triaged module: mps,module: performance triaged module: mps
nn.Sequential causes fx.replace_pattern to not find any match.  triaged module: fx,triaged module: fx
test_to (__main__.TestTorch) fails with multiple gpus module: multi-gpu triaged actionable,module: multi-gpu triaged actionable
[chalf] reference_testing: low quality test for fast growing ops triaged module: complex module: half,triaged module: complex module: half
[Optimizer Overlap] Parameter group support oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
[Optimizer Overlap] Proper checkpointing support oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
[Optimizer Overlap] Custom optimizer registration oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
`pack_sequence` crash triaged module: edge cases,triaged module: edge cases
`ctc_loss` will backward crash module: autograd triaged module: edge cases,module: autograd triaged module: edge cases
`baddmm` triggers INTERNAL ASSERT FAILED when input requires grad module: autograd triaged actionable,module: autograd triaged actionable
"`matmul, mm` triggers INTERNAL ASSERT FAILED when input requires grad module: autograd triaged actionable",module: autograd triaged actionable
Segfault in _pad_packed_sequence triaged module: edge cases,triaged module: edge cases
Segfault in _grid_sampler_2d_cpu_fallback triaged module: edge cases,triaged module: edge cases
Segfault in _embedding_bag_forward_only triaged module: edge cases,triaged module: edge cases
Segfault in torch._C._nn.thnn_conv2d triaged module: edge cases,triaged module: edge cases
Segfault in torch._C._nn.reflection_pad2d triaged module: edge cases,triaged module: edge cases
Segfault in max_unpool3d triaged module: edge cases,triaged module: edge cases
Segfault in grid_sampler_3d triaged module: edge cases,triaged module: edge cases
Segfault in bincount triaged module: edge cases,triaged module: edge cases
[ONNX] Support tensors as scale and zero_point arguments module: onnx triaged OSS contribution wanted onnx-triaged,module: onnx triaged OSS contribution wanted onnx-triaged
torch.multiprocessing.spawn raise PicklingError inside a decorator module: multiprocessing module: serialization triaged,module: multiprocessing module: serialization triaged
[primTorch] item prim can't return a bool properly triaged module: primTorch,triaged module: primTorch
[primTorch] Meta function for item creates a dummy value triaged module: primTorch,triaged module: primTorch
Installation on Jetson target board triaged module: jetson,triaged module: jetson
Gamma and Related Functions feature triaged module: special,feature triaged module: special
nn.CosineSimilarity returns value larger than 1 module: nn triaged module: correctness (silent),module: nn triaged module: correctness (silent)
Adam is 30% slower than SGD on Apple Metal. module: performance module: optimizer triaged module: mps,module: performance module: optimizer triaged module: mps
Python memory allocator called without holding the GIL when running torchrun under Python debug version triaged oncall: r2p,triaged oncall: r2p
toleranceOverride should override atol and rtol even when explicitly specified in a test module: tests triaged module: primTorch,module: tests triaged module: primTorch
RFC: [primTorch] Stride-agnostic Operator Semantics triaged module: python frontend module: primTorch,triaged module: python frontend module: primTorch
DDP multi host with single GPU each.  oncall: distributed triaged,oncall: distributed triaged
FFT operators are not supported on MPS device high priority triaged module: complex module: fft topic: new features module: mps,high priority triaged module: complex module: fft topic: new features module: mps
"Error occurred , when compile source code setting  BUILD_CAFFE2=ON module: build caffe2 triaged",module: build caffe2 triaged
Three memory copies of every dataloader cpu tensor module: multiprocessing module: dataloader module: cuda triaged enhancement,module: multiprocessing module: dataloader module: cuda triaged enhancement
Override sym_sizes to create LTC IR for SymIntNode triaged lazy,triaged lazy
"forward-mode support for ""logically composite"" operators triaged module: derivatives module: forward ad",triaged module: derivatives module: forward ad
Inference Tensors should not be allowed to hold `grad_fn` triaged inference mode,triaged inference mode
`logaddexp2` fails to backward module: autograd triaged module: edge cases,module: autograd triaged module: edge cases
Operating on boolean torch tensor and numpy array casts to `unit8` triaged module: numpy module: type promotion module: boolean tensor,triaged module: numpy module: type promotion module: boolean tensor
NaN tensor values problem for GTX16xx users  (no problem on other devices) module: cudnn triaged,module: cudnn triaged
`topk` returns different results with the same input twice in cuda module: cuda triaged module: python frontend,module: cuda triaged module: python frontend
[failing test] test_foreach::test_binary_op_scalarlist_fastpath triaged,triaged
Fails to compile with GCC 12.1.0 module: build triaged,module: build triaged
Heap corruption in slow_conv_transpose3d module: convolution triaged module: edge cases,module: convolution triaged module: edge cases
Floating point exception in slow_conv3d module: convolution triaged module: edge cases,module: convolution triaged module: edge cases
Floating point exception in native_channel_shuffle triaged release notes: python_frontend,triaged release notes: python_frontend
Floating point exception in channel_shuffle triaged release notes: python_frontend,triaged release notes: python_frontend
Segmentation fault in _remove_batch_dim needs reproduction triaged module: vmap,needs reproduction triaged module: vmap
Make the appropriate backend `DimensionNode` visible to LTC core triaged lazy,triaged lazy
Throw warning if python optimise flags are enabled triaged module: python frontend,triaged module: python frontend
Conv2D with large different number of input and output channels gives a CUDNN_STATUS_INTERNAL_ERROR module: cudnn triaged,module: cudnn triaged
ONNX export of CumSum produces different data type module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
BUG: reference count leak when using `THPLayout_New` and `THPMemoryFormat_New` (static analyzer reports) module: memory usage triaged module: python frontend,module: memory usage triaged module: python frontend
Sporadic convolution error with dilation=0 module: convolution triaged,module: convolution triaged
cannot convert to channels last format for conv2d conv3d hybrid model module: convolution triaged,module: convolution triaged
"`addmv, mv` will trigger INTERNAL ASSERT FAILED when input requiring grad module: autograd triaged actionable",module: autograd triaged actionable
`Could not start gRPC server` flakiness in XLA tests triaged module: xla,triaged module: xla
`torch.utils.benchmark.examples.blas_compare` can not be parsed by Python-3.7 runtime triaged module: benchmark,triaged module: benchmark
General MPS op coverage tracking issue feature triaged module: mps,feature triaged module: mps
strange behaviour in torch.div high priority triaged,high priority triaged
net_observer_reporter_print.h missing module: build triaged,module: build triaged
"torchrun leads to `ModuleNotFoundError: No module named 'tensorboard'`, but python -m torch.distributed.launch is ok triaged oncall: r2p",triaged oncall: r2p
Adding Vulkan Support  triaged module: vulkan,triaged module: vulkan
complex abs strides are wrong on empty tensors and tensors with 1 dimension triaged module: complex module: primTorch,triaged module: complex module: primTorch
FSDP: enhanced shared parameter support oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
PrimTorch refs do not match argument naming with their PyTorch counterparts triaged module: primTorch,triaged module: primTorch
Extend BC test to test for __torch_function__ overridability triaged module: __torch_function__,triaged module: __torch_function__
Werror=nonnull in dataloader.cpp (part of tests) module: dataloader triaged,module: dataloader triaged
PyTorch fails to build on gcc 12 due to gloo module: build triaged module: third_party,module: build triaged module: third_party
How to handle __module__  attribute for Public API bindings module: tests triaged,module: tests triaged
FSDP: test mixed precision with checkpoint oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
`stateless.functional_call` doesn't work with `nn.DataParallel` module: nn triaged module: data parallel actionable,module: nn triaged module: data parallel actionable
Functional Jacobian does not work with Torchdiffeq module: autograd triaged,module: autograd triaged
lintrunner doesn't give good error message suggesting lintrunner init module: lint triaged,module: lint triaged
Build check for AVX512 fails with AMD CPU and march=native module: build triaged module: vectorization,module: build triaged module: vectorization
Modernize LoggingTensorMode triaged module: __torch_dispatch__,triaged module: __torch_dispatch__
Failed to run on iOS - Couldn't find an operator for `aten::conv1d` triaged module: ios,triaged module: ios
batch Kronecker product  triaged enhancement,triaged enhancement
softmarginloss should use `log1p` and has an incorrect out= behaviour. module: loss triaged,module: loss triaged
CUDA: Illegal memory access in `torch.linalg.solve()` high priority module: cuda triaged module: linear algebra,high priority module: cuda triaged module: linear algebra
DDP window TCP bug [socket.cpp:558] [c10d] The client socket has failed to  oncall: distributed,oncall: distributed
Inplace Bool API + `sum` will trigger INTERNAL ASSERT FAILED module: autograd triaged module: edge cases,module: autograd triaged module: edge cases
`max_pool1d` can succeed when padding is negative for tensor requiring grad module: nn triaged actionable module: pooling module: edge cases,module: nn triaged actionable module: pooling module: edge cases
Standalone unittests for checkpoint_wrapper oncall: distributed module: bootcamp triaged better-engineering module: fsdp,oncall: distributed module: bootcamp triaged better-engineering module: fsdp
conda CPU installation for LTS fails with UnsatisfiableError triaged module: lts,triaged module: lts
FSDP: Mixed precision should not cast ignored buffers oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Suboptimal error message - nn.Linear with double argument module: error checking triaged,module: error checking triaged
Process hangs after calling conv2d() in pytorch 1.11.0 with CUDA 11.3 module: cudnn module: convolution triaged module: deadlock,module: cudnn module: convolution triaged module: deadlock
Allow force building with/without AVX module: build triaged,module: build triaged
Large numerical inconsistency for `torch.einsum` on RTX30 series GPU. module: cuda triaged module: tf32,module: cuda triaged module: tf32
microbenchmark-style tests module: tests triaged,module: tests triaged
"outputs_[i]->uses().empty()INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1646755853042/work/torch/csrc/jit/ir/ir.cpp"":1314, please report a bug to PyTorch.  oncall: jit",oncall: jit
Disable issue doesn't disable multiple dtypes correctly module: ci triaged,module: ci triaged
"wrong overload resolved for `torch.mul(x, 4)` in `__torch_dispatch__` triaged module: __torch_dispatch__",triaged module: __torch_dispatch__
non-rentrant checkpointing uses same memory as non-checkpointed code high priority oncall: distributed module: checkpoint triaged,high priority oncall: distributed module: checkpoint triaged
Subclasses with unwrapping `__torch_dispatch__` impls as parameters module: nn triaged module: __torch_dispatch__ tensor subclass,module: nn triaged module: __torch_dispatch__ tensor subclass
CrossEntropyLoss computes SoftMax always across the second dimension module: nn module: loss triaged actionable,module: nn module: loss triaged actionable
lintrunner not working module: lint triaged,module: lint triaged
The codegen unconditionaly generate code even when it is not going to be used triaged module: codegen,triaged module: codegen
libtorch1.8 torch::sigmoid is wrong module: cpp triaged module: jetson,module: cpp triaged module: jetson
`tensordot` does check the dtype of empty tensor triaged module: linear algebra,triaged module: linear algebra
Write decomposition conditionals in a way that leads to simpler shape expressions triaged module: primTorch,triaged module: primTorch
`torch.scatter_add` will succeed when the `index` is a complex tensor triaged module: scatter & gather ops,triaged module: scatter & gather ops
fast `gradcheck` fails when outputs that do not require grad precede outputs that do module: autograd triaged module: linear algebra actionable,module: autograd triaged module: linear algebra actionable
torch.ops.aten.ceil(1.5) returns Tensor rather than scalar triaged module: type promotion,triaged module: type promotion
[primTorch] Reduction references don't return views consistent with their original operators triaged module: primTorch,triaged module: primTorch
[RFC] Allow device override during Tensor unpickling without torch.load module: pickle module: serialization triaged enhancement,module: pickle module: serialization triaged enhancement
Undefined symbol error when compiling and loading C++ extension module: cpp-extensions module: cpp triaged,module: cpp-extensions module: cpp triaged
Improve the overall design of MPSGraphCache triaged enhancement module: backend module: mps,triaged enhancement module: backend module: mps
Allow users to express fused matmul/bias/relu triaged enhancement needs research module: python frontend,triaged enhancement needs research module: python frontend
Move the MPSGuardImpl to inherit from NoOpDeviceGuardImpl triaged module: backend module: mps,triaged module: backend module: mps
nn.functional.pad accepts bool values but raises internal assert when converted to JIT oncall: jit,oncall: jit
"torch.cholesky has been deprecated in favour of torch.linalg.cholesky. However, torch.cholesky_inverse remains as is. It should also be moved to torch.linalg triaged module: linear algebra",triaged module: linear algebra
Automate cleanup of header includes module: internals triaged,module: internals triaged
SymInt shouldn't be in dynamic_type.h module: cpp triaged,module: cpp triaged
"A somewhat cryptic error message (for newcomers) - ""Cannot re-initialize CUDA in forked subprocess"" - report and suggestion for a possible solution module: dataloader triaged",module: dataloader triaged
FSDP: ability to ignore parameters high priority triage review oncall: distributed module: fsdp,high priority triage review oncall: distributed module: fsdp
Add Tensor compare support for MPS backend triaged module: backend module: testing module: mps,triaged module: backend module: testing module: mps
"torch.randperm uses too much cpu, but not efficient. module: performance triaged module: random module: multithreading",module: performance triaged module: random module: multithreading
__name__ on OpOverload should not contain period triaged module: __torch_dispatch__,triaged module: __torch_dispatch__
broadcast_object_list with GPU tensors can lead to deadlock on PyTorch CI machines high priority triage review oncall: distributed module: fsdp,high priority triage review oncall: distributed module: fsdp
Unable to continue adding modules to `nn.Sequential` after using `del` method module: nn triaged enhancement,module: nn triaged enhancement
Incorrect documentation in ``gumble_softmax`` function. module: docs module: nn triaged,module: docs module: nn triaged
Building from source results in broken __version__ module: binaries module: build triaged,module: binaries module: build triaged
ENORMOUS OVERHEAD from mp.get_context('spawn') module: performance module: multiprocessing triaged module: POWER,module: performance module: multiprocessing triaged module: POWER
Peak GPU-memory usage extremely huge when sorting with torch.sort module: cuda module: memory usage triaged module: sorting and selection,module: cuda module: memory usage triaged module: sorting and selection
Stop calling sizes/numel/dim/is_contiguous on undefined tensors module: internals triaged,module: internals triaged
torch.stack test_conj_view and test_neg_view are failing after 77043 triaged module: complex module: viewing and reshaping module: primTorch,triaged module: complex module: viewing and reshaping module: primTorch
Where is fx2trt fx to tensorrt tool? triaged module: fx,triaged module: fx
Display EC2 information module: ci triaged,module: ci triaged
[bug] `NATIVE` and `OMP` `parallel_for` implementations are inconsistent. triaged module: openmp module: multithreading,triaged module: openmp module: multithreading
"When using Rsqrt, the output of the 1/x process is very likely to have nan/inf needs reproduction triaged",needs reproduction triaged
"When using Lambda, the output of the 1/x process is very likely to have nan/inf needs reproduction triaged",needs reproduction triaged
GlobalAvgPool2d causes the inconsistency of output between frameworks needs reproduction triaged,needs reproduction triaged
GaussianNoise causes the inconsistency of output between frameworks needs reproduction triaged,needs reproduction triaged
ReduceSum causes the inconsistency of output between frameworks needs reproduction triaged,needs reproduction triaged
primTorch references don't handle scalar x scalar inputs correctly triaged module: type promotion module: primTorch,triaged module: type promotion module: primTorch
"Private API for accessing all ""internal"" attributes on Tensors module: internals triaged",module: internals triaged
NVFuser opinfos - check for CudaFusionGroup in the graph triaged module: nvfuser,triaged module: nvfuser
Can't pickle model torch._C._distributed_c10d.ProcessGroupNCCL' object  oncall: distributed module: c10d module: ddp,oncall: distributed module: c10d module: ddp
Avoid Self-loops on Module Creation module: nn triaged,module: nn triaged
torch.nn.functional.linear sometimes incorrectly accepts arguments of the different type module: nn triaged module: type promotion,module: nn triaged module: type promotion
Unwanted behavior with some in-place operations on CPU triaged module: viewing and reshaping,triaged module: viewing and reshaping
Multiprocessing DataLoader hangs on exception inside iterator when using a simple queue and a producer thread module: multiprocessing module: dataloader triaged,module: multiprocessing module: dataloader triaged
EmbeddingBag: Does CUDA calculate error in EmbeddingBag forward when include_last_offset=True ? module: nn triaged module: embedding,module: nn triaged module: embedding
RecursionError when running torch.jit.script inside JitTestCase oncall: jit,oncall: jit
PrimTorch binary refs do not handle CUDA + CPU scalar tensors correctly triaged module: primTorch,triaged module: primTorch
Object-base collectives create tensors at unexpected devices high priority oncall: distributed better-engineering module: c10d,high priority oncall: distributed better-engineering module: c10d
Feature requests for optimizer overlapping high priority triage review oncall: distributed module: ddp module: fsdp,high priority triage review oncall: distributed module: ddp module: fsdp
Inconsistent results between Pow and Float Pow with their numpy references for complex types triaged module: complex module: NaNs and Infs,triaged module: complex module: NaNs and Infs
Unify torch.ops argument parsing code with PythonArgParser triaged module: __torch_dispatch__ module: python frontend,triaged module: __torch_dispatch__ module: python frontend
Windows CUDA TTS tracking task module: ci triaged,module: ci triaged
TYPEIGNORE lint run locally disagrees with CI module: lint triaged module: macos,module: lint triaged module: macos
There is a bug with latest stable torch version and the following Nightly versions related to `optimize_for_mobile` oncall: mobile,oncall: mobile
torch.Tensor.__rdiv__ long x scalar float type promotion is incorrect triaged module: type promotion module: primTorch,triaged module: type promotion module: primTorch
"torch.add bool x bool allows integer alpha, inconsistent with other dtype type checking triaged module: primTorch",triaged module: primTorch
`gradcheck` for `torch.solve` may trigger INTERNAL ASSERT FAILED needs reproduction module: autograd triaged,needs reproduction module: autograd triaged
"`cumprod, prod` will backward fail if `dtype` argument is different than the dtype of input tensor module: autograd triaged module: complex complex_autograd",module: autograd triaged module: complex complex_autograd
"`addr, baddmm, dist, l1_loss` will backward fail when input tensors have different dtypes module: autograd triaged module: complex actionable complex_autograd",module: autograd triaged module: complex actionable complex_autograd
`gradcheck` fails for `torch.trace` module: autograd triaged actionable,module: autograd triaged actionable
`gradcheck` should support the comparison of NaN module: autograd triaged enhancement actionable,module: autograd triaged enhancement actionable
"Strange warning from `matmul(..., out=...)` triaged module: linear algebra",triaged module: linear algebra
`torch.addmv` backward fails module: autograd triaged module: complex actionable,module: autograd triaged module: complex actionable
[JIT] Infinite RecursionError with self-referential models (also affects `__repr__`)!! oncall: jit,oncall: jit
[typing] distribution.lazy_property is not typed module: distributions triaged,module: distributions triaged
Quantization in Libtorch oncall: quantization low priority triaged,oncall: quantization low priority triaged
Default repr of __get__ methods in __torch_function__ is bad triaged module: __torch_function__,triaged module: __torch_function__
Bug in dataloader iterator found by mypy module: dataloader triaged,module: dataloader triaged
[checkpoint] Stable file format for checkpoints oncall: distributed triaged sharded_tensor,oncall: distributed triaged sharded_tensor
[checkpoint] Handle overlapping storage during save and load oncall: distributed triaged sharded_tensor,oncall: distributed triaged sharded_tensor
Supporting torch.tensor.apply_ over GPU module: cuda triaged,module: cuda triaged
__torch_function__ callers should always pass kwargs triaged module: __torch_function__,triaged module: __torch_function__
test_python_reference_meta_functions takes too long to run module: tests triaged,module: tests triaged
Adding Polyloss to `torch` module: nn module: loss triaged,module: nn module: loss triaged
[JIT] magic methods do not work after reloading model oncall: jit,oncall: jit
Torchscript model Runtime Error after quantization oncall: jit oncall: quantization low priority triaged,oncall: jit oncall: quantization low priority triaged
`reshape` for distributions. module: distributions feature triaged,module: distributions feature triaged
"Torch `x += y.bmm(z)` is faster than `x.baddbmm_(y, z)` module: performance triaged module: linear algebra",module: performance triaged module: linear algebra
Performance bad on ARM AArch64 for PyTorch C++ module: build triaged module: arm,module: build triaged module: arm
Remove all docstrings when python is running in optimization mode module: performance module: docs triaged better-engineering module: python frontend,module: performance module: docs triaged better-engineering module: python frontend
Clarify dependency on NumPy (related to maskedtensor?) triaged,triaged
[feature request] no-param sort to exploit parallelization module: performance module: cpu triaged module: multithreading module: sorting and selection,module: performance module: cpu triaged module: multithreading module: sorting and selection
`torch.sort` does not exploit parallelization when invoked without the `dim` parameter. module: performance feature module: cpu triaged module: sorting and selection,module: performance feature module: cpu triaged module: sorting and selection
Add `ldl_unpack` functionality feature triaged module: linear algebra,feature triaged module: linear algebra
`torch.nn.HuberLoss` backwards unexpectedly fail module: nn module: loss triaged module: type promotion,module: nn module: loss triaged module: type promotion
`torch.smm` backward fail with strange error message module: sparse triaged,module: sparse triaged
Pytorch can't process special unicode needs reproduction module: serialization triaged,needs reproduction module: serialization triaged
Design API for accessing sparse tensor indices module: sparse triaged,module: sparse triaged
WeightNorm: Add reset_parameters Linear override feature module: nn triaged module: norms and normalization,feature module: nn triaged module: norms and normalization
RestrictPtrTraits in CUDA potentially has no effect. module: cuda triaged better-engineering,module: cuda triaged better-engineering
`Tensor.logit`'s signature in doc misses `eps` argument module: docs triaged module: python frontend,module: docs triaged module: python frontend
pylint segfault module: lint triaged,module: lint triaged
Improve error message for `unfold` when generating tensor with negative dimension triaged actionable module: shape checking module: viewing and reshaping,triaged actionable module: shape checking module: viewing and reshaping
MeanVarianceNormalization feature module: nn triaged module: norms and normalization,feature module: nn triaged module: norms and normalization
OpInfo incorrectly advertises lu_solve support on CUDA even when compiled without magma module: tests triaged better-engineering module: linear algebra,module: tests triaged better-engineering module: linear algebra
OpInfo CUDA bfloat16 support detection is buggy module: tests triaged better-engineering,module: tests triaged better-engineering
AttributeError: '_thread._local' object has no attribute 'rel_tol' (cannot use TestCase.assertEqual from other threads) module: tests triaged better-engineering,module: tests triaged better-engineering
`torch.linalg.cond` has different results for tensor requiring autograd module: autograd triaged module: linear algebra,module: autograd triaged module: linear algebra
Random Generator for Dropout feature triaged module: random,feature triaged module: random
[JIT][Autocast] Batchnorm folding pass during freezing doesn't preserve types oncall: jit,oncall: jit
torch.unique() nondeterministic behavior on nan inputs (on GPU) high priority module: cuda triaged module: NaNs and Infs module: determinism module: sorting and selection,high priority module: cuda triaged module: NaNs and Infs module: determinism module: sorting and selection
[jiterator] perf regression when jiterating few ops for complex dtype module: performance triaged module: complex module: jiterator,module: performance triaged module: complex module: jiterator
Pip packaging and publishing improvements in pytorch wheels for better integration with poetry module: binaries triaged,module: binaries triaged
[numpy] Missing Tensor-Scalar support for multiple binary ops  triaged module: numpy module: python frontend,triaged module: numpy module: python frontend
Segfault in ~PyFunctionPreHook needs reproduction module: crash module: autograd triaged,needs reproduction module: crash module: autograd triaged
Add ability to add custom suffixes to tensor repr module: printing triaged enhancement needs design tensor subclass module: python frontend,module: printing triaged enhancement needs design tensor subclass module: python frontend
Discrepancy in einsum when done in batch vs non-batch triaged module: linear algebra module: tf32,triaged module: linear algebra module: tf32
`torch.clamp` does not distribute gradients as element-wise`min/max` do module: autograd triaged actionable,module: autograd triaged actionable
Fix layout of masked output when all sparse dimensions are reduced module: sparse triaged,module: sparse triaged
Clean up PyTorch's private operators module: internals triaged,module: internals triaged
`Tensor.register_hook()` Source Link Broken module: docs triaged,module: docs triaged
[JIT] make IRAttributeError extend jit::Error oncall: jit,oncall: jit
[NVFuser] Automated generation of microbenchmarks triaged module: nvfuser,triaged module: nvfuser
`F.interpolate` uses incorrect size when `align_corners=True` triaged module: interpolation,triaged module: interpolation
Expand pow and float_pow sampling function for more coverage module: tests triaged,module: tests triaged
Pyre type checking fails  module: typing triaged,module: typing triaged
Enhance _verify_param_shape_across_processes high priority triage review oncall: distributed better-engineering pt_distributed_rampup,high priority triage review oncall: distributed better-engineering pt_distributed_rampup
[checkpoint] Switch away from pickle-base serialization triaged sharded_tensor,triaged sharded_tensor
Remove _log_softmax/_softmax in favor of log_softmax and softmax respectively. module: internals triaged,module: internals triaged
Error in DistributedDataParallel with 'CPU' device oncall: distributed,oncall: distributed
Suggestion to throw a UserWarning when a user forgot .eval() mode during inference module: nn module: molly-guard triaged,module: nn module: molly-guard triaged
[ONNX] Intermediate values are encoded when exporting operators with custom namespace module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
onnx export fails when using torchvision.transforms.CenterCrop module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Whether 'targetSize' in inferExpandGeometryImpl needs to be checked when it is less than 0 module: error checking triaged,module: error checking triaged
NVFuser failing extremal opinfos triaged module: nvfuser,triaged module: nvfuser
`index_select` allows negative `index` for sparse but not for strided `self` module: sparse triaged module: advanced indexing,module: sparse triaged module: advanced indexing
"[ONNX] Use topk to export max(dim,keepdim) to onnx module: onnx triaged onnx-triaged bug",module: onnx triaged onnx-triaged bug
torch.bucketize doc typo on the left boundary when 'right=True' module: docs triaged module: sorting and selection,module: docs triaged module: sorting and selection
[checkpoint] Avoid loading whole tensor when resharding  triaged sharded_tensor,triaged sharded_tensor
Bessel and Related Functions feature triaged module: special,feature triaged module: special
[Checkpoint] Add module documentation triaged sharded_tensor,triaged sharded_tensor
Quantization fails when padding parameter given as string oncall: quantization low priority triaged,oncall: quantization low priority triaged
Unexpected _LinAlgError appeared only on my device triaged module: linear algebra,triaged module: linear algebra
TorchScript: jit.script fails when using 'tolist' with int32 tensors. oncall: jit,oncall: jit
torch._remove_batch_dim is interceptable by __torch_function__ / batch tensors don't print correctly triaged module: vmap module: __torch_function__,triaged module: vmap module: __torch_function__
Multi-GPU distributed training reports errors oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Initial integration of ZenDNN as backend into PyTorch feature module: convolution triaged,feature module: convolution triaged
Observing a strange behavior - Row parallelism module: numerical-stability module: nn triaged module: correctness (silent),module: numerical-stability module: nn triaged module: correctness (silent)
"RuntimeError: bucket_count == per_bucket_sizes.size()INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1646755853042/work/torch/csrc/distributed/c10d/reducer.cpp"":980, please report a bug to PyTorch.  oncall: distributed triaged",oncall: distributed triaged
Update NCCL to 2.12 oncall: distributed module: build triaged,oncall: distributed module: build triaged
"[Feature request] Exclusive prefix sum, `torch.cumsum(input, dim=0, exclusive=True)` high priority module: cuda triaged enhancement module: reductions",high priority module: cuda triaged enhancement module: reductions
fx: cannot find module <built-in method matmul> when using apex.amp triaged module: fx,triaged module: fx
Ensure custom Function are correct in double backward setting module: autograd triaged enhancement actionable,module: autograd triaged enhancement actionable
Indexing assignment can have no effect on CUDA with deterministic algorithms high priority triaged module: advanced indexing,high priority triaged module: advanced indexing
Many dispatch keys do not print to string correctly triaged module: dispatch,triaged module: dispatch
at::real and at::imag as methods triaged module: complex,triaged module: complex
test_wishart_log_prob fails locally for me module: distributions module: tests triaged,module: distributions module: tests triaged
Replace `RuntimeError` by custom exception for unsupported ONNX operators during export module: onnx triaged onnx-triaged topic: improvements,module: onnx triaged onnx-triaged topic: improvements
Coverage test is only checking packages and not all submodules module: docs triaged module: python frontend,module: docs triaged module: python frontend
test_license_for_wheel always fails on my local dev copy module: tests triaged,module: tests triaged
run_test.py option to write out failed tests module: tests triaged better-engineering,module: tests triaged better-engineering
Deprecation warning from SequentialLR high priority triaged module: deprecation module: LrScheduler,high priority triaged module: deprecation module: LrScheduler
RuntimeError: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\caffe2\\serialize\\inline_container.cc:300] module: serialization triaged,module: serialization triaged
Allow any operation that takes a Storage to also take a contiguous Tensor instead feature triaged module: python frontend,feature triaged module: python frontend
Failed to build on Ubuntu 18.04 due to bad MPI linker flags module: build triaged,module: build triaged
Eliminate uses of deprecated `FindCUDA.cmake` module: build module: cuda triaged better-engineering,module: build module: cuda triaged better-engineering
HIPFFT_EXEC_FAILED when using AMD GPU run FFT module: cuda triaged module: fft,module: cuda triaged module: fft
NVFuser microbenchmark classifier - hash on memory formats triaged module: nvfuser,triaged module: nvfuser
`init_process_group` hanging on HPC multi-node system w GPU  oncall: distributed triaged,oncall: distributed triaged
RuntimeError: bucket_count == per_bucket_sizes.size() INTERNAL ASSERT FAILED module: cuda triaged module: ddp,module: cuda triaged module: ddp
SSL certificate error: urlopen error [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1091)> triaged,triaged
__torch_function__ and generator input hazard triaged module: __torch_function__,triaged module: __torch_function__
Computer using CPU instead of GPU nvidia with CUDA module: cuda triaged,module: cuda triaged
Dirichlet with small concentration module: distributions triaged,module: distributions triaged
A bug in instructions for building PyTorch with ASAN module: docs module: ci triaged topic: bug fixes,module: docs module: ci triaged topic: bug fixes
Numerical instability: matrix multiplication got different results on cpu and gpu  triaged module: tf32,triaged module: tf32
The prediction results of different equipment are inconsistent needs reproduction module: nn triaged,needs reproduction module: nn triaged
torch.nn.LayerNorm is very slow on GPU (much slower than a custom LayerNorm version in the ConvNext model) module: nn module: cuda triaged,module: nn module: cuda triaged
backcompat tests in test_nn.py are slow module: nn module: tests triaged,module: nn module: tests triaged
"Build a default NVFuser comparison callback, e.g. for use with torchbench triaged module: nvfuser",triaged module: nvfuser
gql_mocks.json has really long lines triaged better-engineering,triaged better-engineering
Add build support for GCC 11.2 needs reproduction module: build triaged,needs reproduction module: build triaged
[JIT] [Autocast] JIT Autocast Pass operations' list should be extendable and consistent with imperative path oncall: jit,oncall: jit
Potential memory leak in Adam optimizer in AMD chips (CPU) needs reproduction module: optimizer module: rocm module: memory usage triaged,needs reproduction module: optimizer module: rocm module: memory usage triaged
FSDP remove the requirement of all trainable parameters   high priority oncall: distributed triaged module: fsdp,high priority oncall: distributed triaged module: fsdp
Add nesting of nested Tensor triaged module: nestedtensor,triaged module: nestedtensor
torch.jit.trace error when custom autograd function used in the model oncall: jit,oncall: jit
Disable TracerWarnings on NVFuser opinfo tests oncall: jit module: bootcamp triaged,oncall: jit module: bootcamp triaged
aten::_softmax.out doesn't work with non-contiguous Tensors  module: nn triaged,module: nn triaged
"interaction with psychopy during imports, script exits with: free(): invalid pointer. Aborted (core dumped) triaged",triaged
'python setup.py build' failed but succeed using  'pip install -v .' which calls 'python setup.py build'. module: build triaged,module: build triaged
[FSDP] Verify buffer checkpointing high priority triage review oncall: distributed module: fsdp,high priority triage review oncall: distributed module: fsdp
Add batching rules for `{view}_copy` operators triaged module: batching,triaged module: batching
Move _SKIP_PYTHON_BINDINGS to native_functions.yaml triaged module: codegen,triaged module: codegen
torch.jit.script'd function very slow on first invocation on latest nightly oncall: jit NNC module: nvfuser,oncall: jit NNC module: nvfuser
add -D_GLIBCXX_ASSERTIONS in debug mode module: build triaged,module: build triaged
LayerNorm and GroupNorm with num_groups=1 not equivalent module: nn triaged module: norms and normalization,module: nn triaged module: norms and normalization
Fix workaround `__module__` used to appease public binding checks triaged,triaged
"[ONNX] Expected quantizer->qscheme() == kPerTensorAffine to be true, but got false. oncall: quantization low priority triaged",oncall: quantization low priority triaged
`torch.matmul` produces wrong results on A4000 for matrices (n*m) with large m and small n  high priority triaged module: tf32,high priority triaged module: tf32
Handle noncontiguous inputs in distributed backend layer oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Depthwise Conv1d performance (a naive CUDA kernel is 10x faster) module: cuda triaged,module: cuda triaged
Large numerical error when applying nn.Linear in RTX A6000 with cuda>=11.1 high priority module: cuda triaged module: tf32,high priority module: cuda triaged module: tf32
"`torch.sum, prod, cumsum, cumprod, sparse.sum` INTERNAL ASSERT FAIL module: error checking triaged module: reductions",module: error checking triaged module: reductions
Warning originating in C10 backend does not get translated to Python warning if run from subprocess high priority triage review oncall: distributed,high priority triage review oncall: distributed
Support batch indexing with sparse tensors with torch.sparse module: sparse triaged,module: sparse triaged
Let's host NVIDIA dependencies in our own S3 module: ci triaged,module: ci triaged
Einsum should have an `out=` parameter triaged module: linear algebra,triaged module: linear algebra
Addressing skips in OpInfo nn.functional.binary_cross_entropy_with_logits module: nn module: tests triaged,module: nn module: tests triaged
Tensorboard Issue with visualizing the connections of encoder-decoder network oncall: visualization,oncall: visualization
Implement histc for bfloat16 on CPU triaged module: bfloat16 module: sorting and selection,triaged module: bfloat16 module: sorting and selection
Off main thread symbolic evaluation module: internals triaged enhancement oncall: pt2 module: dynamo module: startup-tracing-compile time,module: internals triaged enhancement oncall: pt2 module: dynamo module: startup-tracing-compile time
"multiprocessing and torch.tensor, Cannot allocate memory error module: multiprocessing triaged",module: multiprocessing triaged
Misleading documentation for cholesky_inverse module: docs triaged module: linear algebra,module: docs triaged module: linear algebra
jit fails when trying to assign values to model via hook oncall: jit,oncall: jit
Op segfaults with ForwardAD and Subclassed Tensor as Tangent triaged module: forward ad,triaged module: forward ad
[ONNX] Enable stacktrace print for TORCH_INTERNAL_ASSERT errors in export. module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
kthvalue 20x slower than sort  module: performance triaged module: sorting and selection,module: performance triaged module: sorting and selection
Add ZeroTensor support for `mm` module: performance module: autograd triaged actionable ZeroTensor,module: performance module: autograd triaged actionable ZeroTensor
Add `balance` flag to `random_split` triaged enhancement module: data,triaged enhancement module: data
Cannot use socks5h proxy because of urllib: `urlopen error Remote end closed connection without response` triaged module: vision module: hub,triaged module: vision module: hub
mypy typing strategy for Tensor-likes (`__torch_function__`) module: typing triaged module: __torch_function__,module: typing triaged module: __torch_function__
make lint should advertise make setup_lint module: build module: lint triaged,module: build module: lint triaged
"clang-tidy ""error: do not use const_cast"" cppcoreguidelines-pro-type-const-cast is counterproductive module: cpp module: lint triaged",module: cpp module: lint triaged
[JIT] `torch.jit.ignore` is not working on hooks oncall: jit,oncall: jit
torch.overrides testing is not catching people adding new kwargs and not passing on to handle_torch_function module: tests triaged module: __torch_function__,module: tests triaged module: __torch_function__
`torch.linalg.lstsq` raises `CUBLAS_STATUS_EXECUTION_FAILED` for large `B` in CUDA tensors triaged module: cublas,triaged module: cublas
Connection closed by peer when using dist.isend in gloo backend oncall: distributed triaged,oncall: distributed triaged
"[doc] view appears to mean different things, `view/reshape` vs `transpose/permute`. module: docs triaged module: numpy module: partial aliasing module: viewing and reshaping topic: docs",module: docs triaged module: numpy module: partial aliasing module: viewing and reshaping topic: docs
Profiling graphed callables or cuda graphs raises a RuntimeError high priority triage review oncall: profiler module: cuda graphs,high priority triage review oncall: profiler module: cuda graphs
make_dual errors out when primal is a Tensor and tangent is a subclass Tensor triaged module: forward ad,triaged module: forward ad
TensorBoard frontend fails to display embeddings when `add_embedding()` writes large `label_img` module: tensorboard oncall: visualization,module: tensorboard oncall: visualization
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED module: dependency bug module: cudnn module: cuda module: convolution triaged,module: dependency bug module: cudnn module: cuda module: convolution triaged
Forward AD convolution fails for the empty backend  module: convolution triaged module: forward ad,module: convolution triaged module: forward ad
`torch.pinverse` on CUDA tensors produces non-optimal output for certain type of (invertible) matrix on torch > 1.7.1 but not on torch <= 1.7.1 module: numerical-stability module: cuda triaged module: linear algebra,module: numerical-stability module: cuda triaged module: linear algebra
[NVFuser] call kernels with informative names (e.g. pow_mul_add) triaged module: nvfuser,triaged module: nvfuser
NotImplemented confusion between __torch_function__ and __rpow__ (and other dunder magic methods) triaged module: __torch_function__,triaged module: __torch_function__
Placing model on bfloat16 on CPU make it freeze/hang needs reproduction triaged module: deadlock module: bfloat16 module: intel,needs reproduction triaged module: deadlock module: bfloat16 module: intel
[quant] PackedLinearWeight::apply_dynamic_impl does not handle ReLUFused template argument oncall: quantization low priority triaged,oncall: quantization low priority triaged
Rollup: Top forward-over-reverse formulas high priority module: autograd triaged actionable module: forward ad,high priority module: autograd triaged actionable module: forward ad
`torch.cuda.is_bf16_supported()` seem to not work properly module: cuda triaged module: bfloat16,module: cuda triaged module: bfloat16
Require PyTorch test suite to be warnings clean module: tests triaged better-engineering,module: tests triaged better-engineering
Feature request: fast way to approximate the diagonal of the hessian module: autograd triaged enhancement needs design module: functorch,module: autograd triaged enhancement needs design module: functorch
Pytorch linalg test failure with cuda 11.6 module: cuda triaged module: linear algebra,module: cuda triaged module: linear algebra
[ONNX] Error when exporting adaptive_max_pool2d to ONNX module: crash module: onnx triaged onnx-triaged bug,module: crash module: onnx triaged onnx-triaged bug
Pytorch test failure with CUDA 11.6 module: cuda triaged,module: cuda triaged
"NVFuser bad ""reshape"" performance triaged module: nvfuser",triaged module: nvfuser
conv3d has numerical issue where same input produces output that are not bit-wise identical module: convolution triaged module: determinism,module: convolution triaged module: determinism
`torch.fx.operator_schemas.normalize_function` is too permissive triaged module: fx,triaged module: fx
Libtorch Crash after Attempting to Evaluate Model after Opening an OpenGL Context oncall: jit NNC,oncall: jit NNC
[jit] failing OpInfo JIT tests for conv1d and complex input oncall: jit,oncall: jit
[RFC] Consolidated and unified state_dict and load_state_dict hooks module: bootcamp feature module: nn good first issue triaged better-engineering pt_distributed_rampup,module: bootcamp feature module: nn good first issue triaged better-engineering pt_distributed_rampup
Potential runtime optimization of Mish activation feature module: nn triaged module: python frontend,feature module: nn triaged module: python frontend
"large model, low memory: need `torch.load` that loads one submodule at a time module: serialization triaged",module: serialization triaged
Large cumulative sums appear to be nondeterministic.  high priority module: cuda triaged module: determinism,high priority module: cuda triaged module: determinism
One of the backends for `lu_factor` fails on windows. triaged module: linear algebra,triaged module: linear algebra
Storing LTC tensor shape information in jit::Value oncall: jit,oncall: jit
Improve wording in _store_based_barrier logging and identify ranks that have not joined oncall: distributed triaged better-engineering,oncall: distributed triaged better-engineering
`torch.cuda.get_device_name` fails to identify RTX 3090 Ti  module: cuda triaged,module: cuda triaged
Failed to build `convert_and_benchmark.cc` due to missing `net_observer_reporter_print.h`. caffe2 triaged,caffe2 triaged
"PyTorch source code compile fail after ""Built target fbgemm_avx2"" module: build triaged",module: build triaged
Add BUILD_LAZY_CUDA_SPARSE option module: sparse module: build module: cuda triaged,module: sparse module: build module: cuda triaged
LowRankMultivariateNormal doesn't work with 0 diagonal module: distributions triaged,module: distributions triaged
Runtime configuration to disable TORCH_WARN temporally? module: internals triaged enhancement,module: internals triaged enhancement
torch.jit.load fails when path contains non-ascii characters oncall: jit,oncall: jit
Offical libtorch (pytorch c++ frontend) docker image module: binaries feature triaged,module: binaries feature triaged
Dataloader hangs. Potential deadlock with `set_num_threads` in worker processes? module: dataloader triaged module: deadlock,module: dataloader triaged module: deadlock
[BE] add documentation for adjust learning rate when going to distributed training oncall: distributed triaged better-engineering,oncall: distributed triaged better-engineering
torch.fx deepcopy bug triaged module: fx,triaged module: fx
comm hook error in BWD pass oncall: distributed module: ddp,oncall: distributed module: ddp
torch.cuda.init() unstacks existing CUDA contexts module: cuda triaged,module: cuda triaged
JIT autocasting fails on Optional[Tensor] oncall: jit,oncall: jit
Tensor Subclass not preserved for Tensor subclasses created via inheritance of a TensorImpl subclass triaged better-engineering module: __torch_dispatch__ tensor subclass,triaged better-engineering module: __torch_dispatch__ tensor subclass
LSTM quantization fails if proj_size > 0 oncall: quantization low priority triaged,oncall: quantization low priority triaged
[feature request] Make `index_select` parallel. module: performance feature triaged module: advanced indexing,module: performance feature triaged module: advanced indexing
[feature request] `coalesce` to support `dim` argument. module: performance module: sparse feature triaged,module: performance module: sparse feature triaged
Calling `torch.ops.aten.add_` is ludicrously slow oncall: jit triaged,oncall: jit triaged
Store SourceDataset in MapDataset using pointer module: cpp module: dataloader triaged,module: cpp module: dataloader triaged
[RFC]A suggestion of channels last memory format implementation for 3D tensor feature triaged module: memory format,feature triaged module: memory format
"Include finfo(dtype).[min, max, eps, tiny] in the extremal test case module: tests triaged",module: tests triaged
[Proposal] Use batched oprations to accelerate PowerSGD module: performance oncall: distributed module: optimizer triaged module: ddp module: mta,module: performance oncall: distributed module: optimizer triaged module: ddp module: mta
"torch.fx.wrap will not work, when encapsulate the code triaged module: fx",triaged module: fx
Install PyTorch from source on power machine module: build triaged module: POWER,module: build triaged module: POWER
Problematic ASGD Optimizer module: optimizer triaged enhancement module: LrScheduler,module: optimizer triaged enhancement module: LrScheduler
Failure to set number of threads on AWS Lambda module: cpu triaged module: multithreading,module: cpu triaged module: multithreading
Enable dtype keyword argument for to_dense method module: sparse triaged enhancement,module: sparse triaged enhancement
error in quantization by quantize_fx.prepare_fx triaged module: fx,triaged module: fx
C10d Elastic Training master_addr ERROR oncall: distributed oncall: r2p,oncall: distributed oncall: r2p
Incorrect results for `torch.distributed.gather` for tensor created from permuted NumPy array oncall: distributed module: c10d,oncall: distributed module: c10d
Irrelevant warning during ONNX export of torch.jit.ScriptFunction: Model has no forward function oncall: jit module: onnx onnx-triaged,oncall: jit module: onnx onnx-triaged
[Structured] Make it possible to override create_out implementation on a per dispatch key basis (structured sparse) triaged enhancement module: structured kernels,triaged enhancement module: structured kernels
[bug] `torch.multinomial` should throw error as documented triaged topic: docs,triaged topic: docs
[jiterator] Jiterate Complex Ops triaged module: complex module: jiterator,triaged module: complex module: jiterator
Change the type hint for nn.Module.__call__ to be friendly to overrides. module: nn module: typing triaged enhancement,module: nn module: typing triaged enhancement
GradScaler support FP16 gradients? triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
RuntimeError: Unconvertible NCCL type Short when sending torch.cuda.ShortTensor. oncall: distributed module: c10d,oncall: distributed module: c10d
Embedding isn't determinstic on linux-xenial-cuda11.3-py3.7-gcc7 module: cuda triaged module: embedding,module: cuda triaged module: embedding
`check_batched_forward_grad` fails for `torch.norm` and related ops module: autograd triaged module: batching module: norms and normalization,module: autograd triaged module: batching module: norms and normalization
Can't install Pytorch needs reproduction module: build triaged,needs reproduction module: build triaged
String dtypes for torch Tensors feature triaged,feature triaged
Some easy way to add xfails to OpInfos feature triaged module: testing,feature triaged module: testing
Sampling Issue With Distributions triaged module: half,triaged module: half
Improve sharding algorithm for ASAN (any maybe other jobs as well) module: ci triaged enhancement,module: ci triaged enhancement
"__rpow__(self, other) OpInfo should not test the case where `other` is a Tensor module: tests triaged",module: tests triaged
compilation error with PyTorch v1.11 for CPU on ppc64le triaged module: POWER,triaged module: POWER
OpInfo request for `nn.functional` and `unbind` high priority module: tests triaged,high priority module: tests triaged
"Training, Forward / backward pass with _different_ batch-size, no speedup observed when backward pass has smaller batch-size feature module: autograd triaged",feature module: autograd triaged
torch.nn.ConvTranspose2d's example in docstring is invalid module: docs module: nn triaged actionable,module: docs module: nn triaged actionable
[CTA] Let's Stamp Out Flaky Tests! oncall: distributed oncall: jit module: sparse module: onnx oncall: quantization module: multiprocessing module: autograd module: nn module: optimizer module: dataloader module: cuda module: rocm module: tests module: hub module: data parallel module: linear algebra module: rpc oncall: profiler module: mta onnx-triaged oncall: r2p,oncall: distributed oncall: jit module: sparse module: onnx oncall: quantization module: multiprocessing module: autograd module: nn module: optimizer module: dataloader module: cuda module: rocm module: tests module: hub module: data parallel module: linear algebra module: rpc oncall: profiler module: mta onnx-triaged oncall: r2p
[FSDP]  using CPUOffload creates 3-10x slowdown due to slow cpu optimizer step/update oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Specifying left-right-padding as tuple for asymmetric padding feature module: nn triaged module: padding,feature module: nn triaged module: padding
importing open3d before pytorch causes matmul to produce a segfault module: crash module: cuda triaged module: cublas module: regression module: third_party,module: crash module: cuda triaged module: cublas module: regression module: third_party
Many inplace operators are not being tested for variant consistency (test_variant_consistency_eager) module: tests triaged better-engineering,module: tests triaged better-engineering
No factory functions for strided quantized tensors oncall: quantization feature low priority triaged,oncall: quantization feature low priority triaged
ComplexHalf Coverage Tracker feature triaged module: complex module: half,feature triaged module: complex module: half
Lazy TS Test Failures triaged lazy,triaged lazy
Figure out a lint for directly returning a nullptr instead of raising python_error triaged better-engineering release notes: python_frontend,triaged better-engineering release notes: python_frontend
Compilation error on M1 Mac module: build triaged module: macos module: arm,module: build triaged module: macos module: arm
__torch_function__ is hitting private functions in some cases triaged enhancement module: __torch_function__,triaged enhancement module: __torch_function__
dir(torch._VF) doesn't work triaged better-engineering release notes: python_frontend,triaged better-engineering release notes: python_frontend
torch has no attribute sparse_csr_tensor module: sparse triaged,module: sparse triaged
kron has unnecessary and undocumented dependence on memory layout triaged module: memory format,triaged module: memory format
Embedding Pytorch in C++ using pybind fails on interpreter shutdown module: crash triaged module: pybind module: third_party,module: crash triaged module: pybind module: third_party
"can not build pytorch, failing due to missing _ctypes module module: build triaged",module: build triaged
`torch.histogram` has wrong output dtype and doesn't support integer inputs feature triaged module: numpy module: sorting and selection,feature triaged module: numpy module: sorting and selection
"Warning: ""Specified kernel cache directory could not be created"" triaged module: jiterator",triaged module: jiterator
"UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead. module: nn triaged",module: nn triaged
_run_ninja_build failing with clang module: build triaged,module: build triaged
Quantized cannot inference with cuda oncall: quantization module: cuda triaged,oncall: quantization module: cuda triaged
Can this cudaDeviceSynchronize call be removed? module: cuda triaged,module: cuda triaged
torch.package unpickling transforms: ModuleNotFoundError: No module named 'torch._C._linalg'; 'torch._C' is not a package module: pickle module: serialization triaged module: linear algebra,module: pickle module: serialization triaged module: linear algebra
Package and Deploy: Generate External Python Registration files at runtime [WIP] triaged module: deploy,triaged module: deploy
"Efficient very large (e.g., 31x31) depth-wise convolution module: convolution triaged needs research",module: convolution triaged needs research
None returned from data loader causes debugging difficulty from collate function module: dataloader triaged,module: dataloader triaged
CompositeImplicitAutograd addr implementation does not have correct behavior for bool (meta gives wrong dtype) triaged module: meta tensors,triaged module: meta tensors
instantiate_device_type_tests is misnamed triaged module: testing,triaged module: testing
torch.combinations requires unnecessary CUDA sync module: cuda triaged topic: performance,module: cuda triaged topic: performance
[c10d] `gather`/`scatter` inconsistency behavior on non-dst/src rank oncall: distributed better-engineering module: c10d,oncall: distributed better-engineering module: c10d
Make OpInfo repr more useful triaged better-engineering module: testing,triaged better-engineering module: testing
Error in lobpcg when using largest=False module: sparse triaged module: masked operators,module: sparse triaged module: masked operators
repeat_interleave is not meta friendly; not transform friendly either triaged tensor subclass,triaged tensor subclass
Gradients tests are very time consuming module: autograd module: ci triaged,module: autograd module: ci triaged
Time Gated Lstm feature module: nn module: rnn triaged,feature module: nn module: rnn triaged
"TypeError: new_empty(): argument 'size' (position 1) must be tuple of ints, not list module: error checking triaged",module: error checking triaged
RuntimeError: cuDNN error: CUDNN_STATUS_VERSION_MISMATCH for torchvision models module: binaries module: cudnn triaged module: vision,module: binaries module: cudnn triaged module: vision
Move torchbench workflow to `workflow_dispatch`? module: ci triaged,module: ci triaged
"Can't download torch vision models for older versions via torch.hub.load: ""multiple choices"" triaged module: hub module: models",triaged module: hub module: models
Create secure credential storage for metrics credentials and associated documentation on how to regenerate them if needed module: ci triaged,module: ci triaged
dir() on torch.ops.aten doesn't work triaged enhancement tensor subclass,triaged enhancement tensor subclass
"""histogram_cpu"" not implemented for 'Int' triaged module: sorting and selection",triaged module: sorting and selection
RuntimeError: CUDA error: unspecified launch failure module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
Autograd API to get saved for backwards tensors for an autograd graph module: double backwards module: autograd triaged,module: double backwards module: autograd triaged
Cannot get op through FX when using nn.Sequential triaged module: fx,triaged module: fx
[docker] test_corrcoef_cpu_complex64 fails on CPU build module: tests triaged,module: tests triaged
[DDP] Parallelize initialization collectives oncall: distributed triaged better-engineering pt_distributed_rampup module: ddp,oncall: distributed triaged better-engineering pt_distributed_rampup module: ddp
Slower performance of `torch.mm` method with sparse CSR tensor module: sparse triaged module: mkl,module: sparse triaged module: mkl
[Android Studio] DefaultCPUAllocator: not enough memory: you tried to allocate 280166432 bytes module: memory usage triaged module: android,module: memory usage triaged module: android
powersgd can not get linear growth due to  extra 2 times allreduce  oncall: distributed triaged module: ddp topic: performance,oncall: distributed triaged module: ddp topic: performance
RuntimeError: Connection reset by peer when backened by NCCL module: cuda triaged module: nccl,module: cuda triaged module: nccl
Tracing model parameter shapes without instantiating the model parameters feature module: nn triaged,feature module: nn triaged
`torch.set_printoptions` overwrites settings of its own previous calls module: printing triaged module: ux,module: printing triaged module: ux
No module named 'pygame': soft_actor_critic triaged module: lazy,triaged module: lazy
Support capturing code in exception handlers triaged enhancement oncall: pt2 module: dynamo,triaged enhancement oncall: pt2 module: dynamo
DistributedDataParallel high peak memory usage with find_unused_parameters=True high priority triage review oncall: distributed triaged module: ddp,high priority triage review oncall: distributed triaged module: ddp
torch.jit.script does not custom state_dicts oncall: jit oncall: quantization low priority triaged,oncall: jit oncall: quantization low priority triaged
C++ Context::setDeterministicAlgorithms default 2nd arg not defined in header module: cpp triaged module: determinism,module: cpp triaged module: determinism
Two consecutive nn.LayerNorm are used in transformer model when norm_first is False module: nn triaged oncall: transformer/mha,module: nn triaged oncall: transformer/mha
torch.fx.symbolic_trace is non-deterministic triaged module: fx,triaged module: fx
assertEqual gives confusing error when comparing tuple with Tensor with Tensor module: tests triaged,module: tests triaged
FSDP does not work on GLOO backend oncall: distributed triaged better-engineering module: fsdp,oncall: distributed triaged better-engineering module: fsdp
torch.nn.Module.__init__ does not call super().__init__ module: nn triaged,module: nn triaged
ONNX export of torch.histc module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Ambiguous docstring on `register_module_forward_hook` module: docs module: nn triaged actionable,module: docs module: nn triaged actionable
Inconsistent implementation on SWA module: docs module: optimizer triaged actionable,module: docs module: optimizer triaged actionable
torch.utils.data.Dataset combined with pycuda issue triaged module: data,triaged module: data
Lazy: add support for index ops triaged module: lazy,triaged module: lazy
[FX] Tensor constants are not lifted to attributes in direct Graph construction triaged module: fx,triaged module: fx
Invalid example tensor: detectron2_maskrcnn triaged module: lazy,triaged module: lazy
Forward_AD and Torchscript Functions results in Nones or wrong values. oncall: jit module: autograd module: forward ad,oncall: jit module: autograd module: forward ad
processes hang when executing cross-machine asynchronous P2P communication on NCCL backend oncall: distributed module: nccl module: c10d,oncall: distributed module: nccl module: c10d
"[Quant] Framework observes weight in convert, changing numerics oncall: quantization low priority triaged",oncall: quantization low priority triaged
"User raised TypeError from __torch_dispatch__ gets turned into generic ""unsupported operand type"" message module: bootcamp triaged module: __torch_dispatch__",module: bootcamp triaged module: __torch_dispatch__
torch.cuda.amp: Remove SPMD DDP doc portion oncall: distributed module: docs module: cuda triaged module: amp (automated mixed precision),oncall: distributed module: docs module: cuda triaged module: amp (automated mixed precision)
"pytorch/pytorch:1.8.1-cuda10.2-cudnn7-devel docker container contains cudnn 8.2, not 7.x as the name implies triaged module: docker",triaged module: docker
Invalid code inrandom_ kernel module: distributions triaged,module: distributions triaged
Adam optimizer doesn't work with CyclicLR scheduler but works with OneCycleLR. module: optimizer triaged module: LrScheduler,module: optimizer triaged module: LrScheduler
SummaryWriter reports encoding error triaged module: tensorboard,triaged module: tensorboard
operation not supported crash when initializing RPC tensorpipe high priority triage review oncall: distributed module: crash triaged,high priority triage review oncall: distributed module: crash triaged
Possible issue with memory allocation. module: memory usage triaged module: numpy,module: memory usage triaged module: numpy
ONNX: Document public API for shape inference for custom symbolics module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
max_pool1d() returns when given invalid large `kernel_size` inputs module: nn triaged module: correctness (silent) module: edge cases,module: nn triaged module: correctness (silent) module: edge cases
[FSDP][BE] TestAutoWrap should inherit from MultiProcessTestCase oncall: distributed triaged better-engineering module: fsdp,oncall: distributed triaged better-engineering module: fsdp
tensor subclass and `__torch_function__` performance issues triaged module: __torch_function__,triaged module: __torch_function__
torch.distributions.multinomial.Multinomial (an example mistake of docs)? module: distributions module: docs triaged,module: distributions module: docs triaged
max_pool1d(): `RuntimeError: [enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory` triaged module: pooling,triaged module: pooling
"max_pool1d(): argument 'dilation' must be tuple of ints, but found element of type int at pos 1 triaged module: pooling module: edge cases",triaged module: pooling module: edge cases
"Tensor.new_tensor now preserves input tensor device, making it inconsistent with documentation triaged module: tensor creation",triaged module: tensor creation
Support unscaling grad on CPU feature module: cpu triaged module: amp (automated mixed precision),feature module: cpu triaged module: amp (automated mixed precision)
Legacy sparse tensor constructor (e.g. torch.cuda.sparse.FloatTensor) silently ignores device kwarg triaged module: tensor creation,triaged module: tensor creation
"Run libtorch examples, export error ""undefined reference to xxx"" module: binaries module: build module: abi triaged",module: binaries module: build module: abi triaged
Better Error report in torch.distribution.*.sample (when passing a non-iterable) module: distributions module: error checking triaged,module: distributions module: error checking triaged
TypeError: cannot pickle 'torch._C._distributed_c10d._ProcessGroupGloo' object oncall: distributed triaged module: c10d module: ddp,oncall: distributed triaged module: c10d module: ddp
Bug: torch.distributions.mixture_same_distribution._pad_mixture_dimension high priority module: distributions triaged module: correctness (silent),high priority module: distributions triaged module: correctness (silent)
[Discussion][FSDP] Enhancements to auto_wrap_policy oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Group convolution slower than manually running separate convolutions in CUDA streams module: performance module: cudnn module: cuda module: convolution triaged,module: performance module: cudnn module: cuda module: convolution triaged
CI: Bake as many dependencies as we can in the AMI (windows) module: ci triaged,module: ci triaged
torch.ao.quantization.ReuseInputObserver should also reuse the dtype of the input oncall: quantization low priority triaged,oncall: quantization low priority triaged
Why is distributed RPC using the default pickler? oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
Audit exception rewrapping to ensure stack traces are preserved oncall: distributed triaged better-engineering module: rpc,oncall: distributed triaged better-engineering module: rpc
Release pytorch docker images with newer python versions oncall: releng triaged module: docker,oncall: releng triaged module: docker
"Refactor/Cleanup LazyTensor, LTCTensorImpl, Data triaged module: lazy",triaged module: lazy
Refactor/Cleanup Lazy Tensor Core triaged module: lazy,triaged module: lazy
DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training feature module: memory usage triaged,feature module: memory usage triaged
How to get tolerance override in OpInfo-based test? module: docs triaged module: testing,module: docs triaged module: testing
Improving error message RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior. module: autograd triaged better-engineering actionable,module: autograd triaged better-engineering actionable
test_broadcast_coalesced_nccl fails on A100 GPUs oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Nice way to override string representation of tensor subclasses triaged enhancement tensor subclass,triaged enhancement tensor subclass
`torch.fx.ProxyableClassMeta` does not work if Proxy objects are not included in constructor arguments triaged module: fx,triaged module: fx
torchrun: Hostname/endpoint mismatch not handled triaged module: c10d,triaged module: c10d
addcdiv is failing the ASAN test for zero divisors module: error checking module: tests triaged,module: error checking module: tests triaged
Report DDP efficiency metric to users and guide them for setting up DDP correctly high priority triage review oncall: distributed triaged,high priority triage review oncall: distributed triaged
Add a section in DDP tutorial to explain why DDP sometimes is slower than local training and how to improve it oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
Add optional log_scale argument for torch.distributions.Normal module: distributions triaged,module: distributions triaged
Bug when using `nn.Linear` needs reproduction module: cuda triaged module: linear algebra,needs reproduction module: cuda triaged module: linear algebra
Pytorch Installation from source fails module: build triaged,module: build triaged
Build failure using GCC 11.2.0 module: build triaged module: mkldnn module: third_party,module: build triaged module: mkldnn module: third_party
Investigate why tensor shapes are not populated when printing WorkNCCL for broadcast high priority triage review oncall: distributed better-engineering module: c10d module: ddp,high priority triage review oncall: distributed better-engineering module: c10d module: ddp
Functionalization doesn't work when applied twice with wrapper tensor triaged tensor subclass module: functionalization,triaged tensor subclass module: functionalization
Mention docker build process in RELEASE.md and automate building those for release module: ci triaged enhancement module: docker,module: ci triaged enhancement module: docker
Teach tools.codegen.api.translate about IValues module: bootcamp triaged better-engineering module: codegen,module: bootcamp triaged better-engineering module: codegen
`torch.distributed.nn.functional.all_gather`: Tensors must be contiguous oncall: distributed module: bootcamp triaged better-engineering pt_distributed_rampup module: c10d,oncall: distributed module: bootcamp triaged better-engineering pt_distributed_rampup module: c10d
Wrapper tensor level confusion triaged needs research module: __torch_dispatch__ tensor subclass,triaged needs research module: __torch_dispatch__ tensor subclass
Implement SiLU method for the QuantizedCPU backend oncall: quantization low priority triaged,oncall: quantization low priority triaged
Idiom for extensible string printing for TensorImpl subclasses module: internals module: printing triaged,module: internals module: printing triaged
Figure out what to do with functions that take both Tensor and TensorOptions triaged module: codegen,triaged module: codegen
`ge` and `div` behaves differently when converting an overflow number triaged module: type promotion module: edge cases,triaged module: type promotion module: edge cases
`storage` does support `complex32` tensor triaged module: complex module: half,triaged module: complex module: half
`index_copy` has different index behavior with `index_fill` triaged module: advanced indexing,triaged module: advanced indexing
After XNNPack update `TestXNNPACKSerDes.test_linear` started to fail high priority module: rocm triaged module: regression module: xnnpack,high priority module: rocm triaged module: regression module: xnnpack
PyTorch not recognizing GPU on WSL - installed cudnn and cuda module: cuda triaged module: wsl,module: cuda triaged module: wsl
Rename `keep_vars` in `nn.Module.state_dict` module: nn triaged,module: nn triaged
Strange case of empty non-coalesced sparse tensor module: sparse triaged,module: sparse triaged
different results on each batch_size in torch==1.10.2+cu113 on RTX 3080 triaged module: numerical-reproducibility,triaged module: numerical-reproducibility
torch.fx failed when tracing functions from other Libs. triaged module: fx,triaged module: fx
`TestCommonCUDA.test_noncontiguous_samples_pca_lowrank_cuda_float32` fails on A100 due to TF32 operation in `svd_lowrank` module: cuda module: tests triaged module: linear algebra module: tf32,module: cuda module: tests triaged module: linear algebra module: tf32
ufunc codegen support for dtypes that are supported on CUDA but not CPU triaged module: codegen,triaged module: codegen
"""munmap_chunk(): invalid pointer"" interaction error with pytorch (< 1.10), pybind, and cv_bridge triaged module: lts",triaged module: lts
MaybeEncodingError: Error sending result module: multiprocessing module: dataloader triaged,module: multiprocessing module: dataloader triaged
Conv3D consumes lots of memory on Mac with Apple Silicon module: convolution triaged module: macos,module: convolution triaged module: macos
Build from source failed module: build triaged,module: build triaged
Make it easier to figure out if packages need to be interned/mocked/externed module: build triaged,module: build triaged
elastic/rendezvous: _matches_machine_hostname doesn't resolve hostnames fully triaged module: elastic oncall: r2p topic: bug fixes,triaged module: elastic oncall: r2p topic: bug fixes
"Pytorch Typing, for Tensor type annotations module: typing triaged",module: typing triaged
Review and refactor  the way libcublas static libraries are linked module: build oncall: releng triaged module: cublas,module: build oncall: releng triaged module: cublas
Segfault on unloading a model oncall: jit module: crash module: cpp,oncall: jit module: crash module: cpp
Large performance difference of loss.backward() between torch-1.9.0 and torch-1.8.0 module: autograd triaged module: regression,module: autograd triaged module: regression
Internal assert failed at rref_context.cpp oncall: distributed module: crash,oncall: distributed module: crash
Add detection of interned submodule of externed module in PackageExporter triaged module: deploy,triaged module: deploy
Inconsistent numpy indexing triaged module: numpy module: advanced indexing,triaged module: numpy module: advanced indexing
Having rrelu functional + module take a generator object to match native functions entry module: nn triaged module: random,module: nn triaged module: random
"This is not completely a bug, but something that will be amazing if can be taken care of regarding nn.DataParallel. triaged module: data parallel",triaged module: data parallel
Bug in label smoothing with ignored samples module: loss triaged,module: loss triaged
`torch.pow` errors out on specific input module: cuda triaged,module: cuda triaged
Segmentation fault in max_pool1d module: error checking triaged module: pooling module: edge cases,module: error checking triaged module: pooling module: edge cases
Segmentation fault in fractional_max_pool3d high priority module: cpu module: error checking triaged module: pooling,high priority module: cpu module: error checking triaged module: pooling
Segmentation fault in fractional_max_pool2d high priority module: cpu module: error checking triaged module: pooling,high priority module: cpu module: error checking triaged module: pooling
Segmentation fault in _sobol_engine_scramble_ module: error checking triaged module: random module: edge cases,module: error checking triaged module: random module: edge cases
Segmentation fault in _sobol_engine_initialize_state_ module: error checking triaged module: random module: edge cases,module: error checking triaged module: random module: edge cases
Segmentation fault in _sobol_engine_ff_ module: error checking triaged module: random module: edge cases,module: error checking triaged module: random module: edge cases
Floating point exception in _nnpack_spatial_convolution high priority module: cpu module: error checking module: convolution triaged module: nnpack,high priority module: cpu module: error checking module: convolution triaged module: nnpack
"Cuda lacks checking of ""out of bound"" module: nn module: loss module: cuda module: error checking triaged",module: nn module: loss module: cuda module: error checking triaged
`LayerNorm` triggers INTERNAL ASSERT high priority module: cpu module: error checking triaged module: norms and normalization,high priority module: cpu module: error checking triaged module: norms and normalization
LazyLinear with equal in_features and out_features module: nn triaged needs research,module: nn triaged needs research
libtorch need operator= in torch::Device module: cpp triaged enhancement,module: cpp triaged enhancement
native_batch_norm and native_layer_norm have strange epsilon behaviors triaged module: norms and normalization,triaged module: norms and normalization
`max_unpool2d` returns a tensor with negative dimension high priority module: cpu triaged module: pooling,high priority module: cpu triaged module: pooling
How do we handle metadata-modifying in-place operators (like `squeeze_`) with `__torch_dispatch__`? triaged module: __torch_dispatch__,triaged module: __torch_dispatch__
Add nan-safe einsum and bilinear triaged enhancement module: linear algebra needs design,triaged enhancement module: linear algebra needs design
libtorch: `DistributedRandomSampler` uses the same random order in every epoch triaged module: data,triaged module: data
`create_graph=True` results in grad_fn error for differentiable functions module: autograd triaged,module: autograd triaged
Test functionalization doesn't run module: internals triaged,module: internals triaged
make c++ logger preamble meaningful triaged module: c10d,triaged module: c10d
PyTorch fails to compile on gcc 11.2 due to breakpad module: build triaged,module: build triaged
Improve test_test_history.py module: ci triaged,module: ci triaged
[FSDP checkpoint] Test replace_by_prefix util oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Multiple new caffe2-related build failures. module: build caffe2 triaged,module: build caffe2 triaged
`enumerate_support` for continuous distributions module: distributions triaged,module: distributions triaged
`index_fill_` accepts wrong dtype for meta tensors module: tests triaged module: meta tensors,module: tests triaged module: meta tensors
"Clang Compilation Error: more than one constructor applies to convert from ""ptrdiff_t"" to ""c10::Scalar"" module: build module: cuda triaged module: macos",module: build module: cuda triaged module: macos
Cannot install latest pytorch into Docker on Apple M1 module: binaries triaged module: macos module: arm,module: binaries triaged module: macos module: arm
aten::batch_norm schema does not mention mutation of running_mean/running_var triaged,triaged
[Main Issue] FSDP Model Checkpoint oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Is it intentional that PyTorch linux binaries aren't manylinux1 compliant? module: binaries triaged,module: binaries triaged
Unable to build and use libtorch function via pybind11: undefined symbol error upon import module: cpp triaged module: mkldnn,module: cpp triaged module: mkldnn
Improve loading for nn.modules.lazy.LazyModuleMixin module: nn triaged actionable,module: nn triaged actionable
Followup requires for MKL link issue / cannot find -lmkl_core module: binaries module: cpp triaged,module: binaries module: cpp triaged
pin_memory hangs instead of throwing module: dataloader module: cuda triaged,module: dataloader module: cuda triaged
Refactor/Cleanup lazy IR codegen triaged module: lazy,triaged module: lazy
Feature: a consistent Python and C++ logging facility that handles different classes of warnings module: logging triaged,module: logging triaged
Why there are 8 flavors of iOS build jobs for every commit module: ci triaged module: ios,module: ci triaged module: ios
Is the current behavior with addcmul and integer dtypes intended? module: tests triaged,module: tests triaged
Vectorized Jacobian and Hessian errors with ffts module: autograd triaged module: vmap,module: autograd triaged module: vmap
"`torch.svd_lowrank` should set the default value of `q` as `min(6, m, n)` triaged module: linear algebra",triaged module: linear algebra
Hanging Validation oncall: distributed module: cuda triaged module: deadlock module: amp (automated mixed precision) module: ddp,oncall: distributed module: cuda triaged module: deadlock module: amp (automated mixed precision) module: ddp
Bazel fails in an obscure way if submodules are not initialized low priority triaged module: third_party module: bazel,low priority triaged module: third_party module: bazel
SequentialLR scheduler incorrect initialization module: optimizer triaged module: LrScheduler,module: optimizer triaged module: LrScheduler
Profiler crashes with ProfilerActivity.CUDA on AWS p4d.24xlarge with A100-SXM4-40GB high priority triage review module: cuda oncall: profiler,high priority triage review module: cuda oncall: profiler
Profiler crashes in export_chrome_trace with seg fault if any of record_shapes=True or with_flops=True oncall: profiler,oncall: profiler
N-dimensional Convolutions module: convolution triaged enhancement,module: convolution triaged enhancement
Toggling deterministic mode for individual autograd backward functions module: autograd triaged needs research module: determinism,module: autograd triaged needs research module: determinism
Some loss functions support `dtype` broadcast but some do not module: nn module: loss triaged module: type promotion,module: nn module: loss triaged module: type promotion
"`{Batch,Instance}Norm{1,2,3}d` works when `num_features != C`! module: nn triaged",module: nn triaged
Standardize Naming for Workflows/Jobs module: ci triaged,module: ci triaged
"why ram memory surges while loading model, with change in torch load device from CPU to GPU module: cuda module: memory usage triaged",module: cuda module: memory usage triaged
Docs bug: type annotations for linspace (and logspace) start and end arguments is wrong module: docs triaged module: tensor creation,module: docs triaged module: tensor creation
Torch version in docker container does not match tag triaged module: docker,triaged module: docker
Error during training: falseINTERNAL ASSERT FAILED high priority needs reproduction module: multiprocessing triaged module: assert failure,high priority needs reproduction module: multiprocessing triaged module: assert failure
[vulkan] Vulkan backend fails creating tensor on x86_64 Linux triaged module: vulkan,triaged module: vulkan
Add NCCL and MPI version printing to torch.utils.collect_env module: collect_env.py triaged module: mpi enhancement module: nccl,module: collect_env.py triaged module: mpi enhancement module: nccl
Feature Request: Deterministic MaxPool3d and AvgPool3d feature module: nn triaged module: determinism module: pooling,feature module: nn triaged module: determinism module: pooling
Add softplus inverse module: numerical-stability triaged function request,module: numerical-stability triaged function request
"pytorchmergebot doesn't react to comments left from ""files"" tab module: ci triaged",module: ci triaged
CPU execution/dispatch time dominates and slows down small TorchScript GPU models  oncall: jit,oncall: jit
Add `pct_end` parameter to `OneCycleLR` module: optimizer triaged enhancement module: LrScheduler,module: optimizer triaged enhancement module: LrScheduler
Better support for pypip packages implementing torch cuda extentions module: binaries feature triaged needs design release notes: releng,module: binaries feature triaged needs design release notes: releng
Feature: Use iterative refinement algorithm from cuSOLVER for the least-squares solver on CUDA feature module: cuda triaged module: linear algebra,feature module: cuda triaged module: linear algebra
nn.functional No-batch-dim support should have OpInfo examples module: nn module: tests triaged,module: nn module: tests triaged
Some inplace ops don't raise on incompatible shapes and meta device module: tests triaged module: meta tensors,module: tests triaged module: meta tensors
upstream `apex.normalization.FusedRMSNorm` feature module: nn triaged module: norms and normalization,feature module: nn triaged module: norms and normalization
TorchScript assertion failure for a `List[...]` inside a `NamedTuple` oncall: jit,oncall: jit
"Can't forward pass conv2d with kernel_size=1, and padding=1 needs reproduction module: build module: nn module: convolution triaged module: macos module: nnpack module: arm",needs reproduction module: build module: nn module: convolution triaged module: macos module: nnpack module: arm
"Avoid using thrust:: directly, use THRUST_NS_QUALIFIER:: instead module: build module: cuda triaged",module: build module: cuda triaged
"Setting a list of Modules as an attribute does not work like setting a Module as an attribute, and throws no warning module: nn triaged",module: nn triaged
Pin dependencies + expand the current linter  module: bootcamp module: ci triaged,module: bootcamp module: ci triaged
"Clarify test dependencies (e.g., into a test-requirements.txt file) module: bootcamp module: ci triaged",module: bootcamp module: ci triaged
Enforce quotas on CI users module: ci triaged enhancement,module: ci triaged enhancement
Split up torch.distributions docs into multiple pages module: distributions module: docs triaged,module: distributions module: docs triaged
Feature: Add tril_embed and triu_embed feature triaged module: linear algebra module: tensor creation,feature triaged module: linear algebra module: tensor creation
Remove Caffe2 triage review caffe2 better-engineering,triage review caffe2 better-engineering
KL divergence between two Continuous Bernoulli is negative module: distributions triaged,module: distributions triaged
test_del (jit.test_builtins.TestBuiltins) fails due to highlight assertions oncall: jit,oncall: jit
Re-raise the exception when the `forward` of a parametrization raises module: nn triaged module: nn.utils.parametrize,module: nn triaged module: nn.utils.parametrize
Use Module `__getattr__` in `torch.ops` and friends triaged better-engineering,triaged better-engineering
ProcessGroupWrapper: Additional Improvements oncall: distributed better-engineering module: c10d module: ddp,oncall: distributed better-engineering module: c10d module: ddp
input.dim() == 4 INTERNAL ASSERT FAILED mkldnn/Pooling.cpp:201 high priority needs reproduction module: nn triaged module: mkldnn module: pooling,high priority needs reproduction module: nn triaged module: mkldnn module: pooling
Matrix multiplication is 30 times slower for integers than floats on CPU module: performance module: cpu triaged module: linear algebra,module: performance module: cpu triaged module: linear algebra
Some system-installed headers are mistakenly used. module: build triaged,module: build triaged
ArgMax for Multi Dimensional Tensor feature triaged module: ux module: reductions,feature triaged module: ux module: reductions
Custom ProcessGroup Destructor Not Correctly Called in PT 1.10 oncall: distributed module: c10d module: ddp,oncall: distributed module: c10d module: ddp
InstanceNorm doesn't preserve memory format triaged module: memory format module: norms and normalization,triaged module: memory format module: norms and normalization
Ability to assign to `tensor.require_grad` might lead to bugs module: error checking triaged module: ux,module: error checking triaged module: ux
Feature request: a mode to disallow calling prototype or beta features module: bc-breaking triaged topic: bc breaking module: python frontend,module: bc-breaking triaged topic: bc breaking module: python frontend
DISABLED test_memory_profiler (__main__.TestProfiler) high priority triage review triaged module: flaky-tests skipped oncall: profiler,high priority triage review triaged module: flaky-tests skipped oncall: profiler
Removing deprecated `cpp_custom_type_hack` oncall: jit oncall: quantization triaged better-engineering oncall: profiler,oncall: jit oncall: quantization triaged better-engineering oncall: profiler
NotImplementedError in torch.distributions module: distributions triaged,module: distributions triaged
`torch.hasnan` module: bootcamp triaged enhancement module: NaNs and Infs,module: bootcamp triaged enhancement module: NaNs and Infs
Caffe2 uses FFMPEG functions that are deprecated in FFMPEG 4.0 and gone in 5.0 module: dependency bug caffe2 triaged,module: dependency bug caffe2 triaged
Transformer Initialization module: docs module: nn triaged actionable oncall: transformer/mha,module: docs module: nn triaged actionable oncall: transformer/mha
recurrent neural network module module: docs module: nn module: rnn triaged,module: docs module: nn module: rnn triaged
[JIT] Assert that the autodiff implementation of backward() returns the correct number of values oncall: jit,oncall: jit
`TestCase.assertEqual` has `equal_nan` default to `True` module: tests triaged module: testing,module: tests triaged module: testing
test_fn_fwgrad_bwgrad_[trapezoid|trapz]_cuda_complex128 causes CUDA memory exception module: crash module: cuda module: ci triaged ZeroTensor,module: crash module: cuda module: ci triaged ZeroTensor
 test_fn_fwgrad_bwgrad_special_ndtr_cuda_float64 fails module: crash module: cuda triaged ZeroTensor,module: crash module: cuda triaged ZeroTensor
[RFC] PyTorch Sharder for distributed training oncall: distributed triaged sharded_tensor,oncall: distributed triaged sharded_tensor
Add support for `complex` `mean` to the `normal` operator module: distributions feature triaged module: complex,module: distributions feature triaged module: complex
Force PyTorch to clear CUDA cache module: cuda triaged,module: cuda triaged
Conversion Error in pytorch mobile with metal module: memory format oncall: mobile,module: memory format oncall: mobile
DistributedDataParallel creates too many threads oncall: distributed module: dataloader better-engineering module: ddp,oncall: distributed module: dataloader better-engineering module: ddp
Update MKL version used with MAGMA module: ci triaged module: magma,module: ci triaged module: magma
ImportError: cannot import name '_VF' from partially initialized module 'torch' needs reproduction low priority triaged,needs reproduction low priority triaged
Batched sparse-sparse matrix multiplication/ sparse torch.einsum module: sparse feature triaged,module: sparse feature triaged
"`svd_backward`: does not handle inputs of rank `r < min(m, n)`. module: autograd triaged module: linear algebra",module: autograd triaged module: linear algebra
index_add : Inconsistent between CPU and CUDA module: autograd module: cuda triaged,module: autograd module: cuda triaged
index_put : INTERNAL ASSERT FAILED high priority module: cuda triaged,high priority module: cuda triaged
Building PyTorch with Vulkan backend don't work triaged module: vulkan,triaged module: vulkan
`pip==22.0` breaks installation process high priority module: binaries oncall: releng triaged,high priority module: binaries oncall: releng triaged
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward module: autograd triaged module: nn.utils.parametrize,module: autograd triaged module: nn.utils.parametrize
Bug about distributed launch oncall: distributed,oncall: distributed
JIT function crashing or failing (depending on profiling) oncall: jit NNC,oncall: jit NNC
[PT-D] To make the ShardedTensor Reshard more generic oncall: distributed triaged sharded_tensor,oncall: distributed triaged sharded_tensor
multiple PRs on pytorch are closed by push to unrelated branches such as pytorch-canary triaged module: third_party,triaged module: third_party
from_blob / make_tensor support MemoryFormat triaged module: memory format,triaged module: memory format
Multivariate normal defined by eigendecomposition  module: distributions triaged,module: distributions triaged
Graph Mode Quantization does not keep NamedTuple Information oncall: quantization low priority triaged module: fx,oncall: quantization low priority triaged module: fx
Support mixed python scalar/tensor types for torch.clamp's min/max args feature triaged module: type promotion,feature triaged module: type promotion
Functions depending on SVD are broken for inputs with non-finite values with MKL 2022+ and OpenBLAS 0.3.15+ triaged module: mkl module: linear algebra module: openblas,triaged module: mkl module: linear algebra module: openblas
CUDA Graph API Improvement triaged module: cuda graphs,triaged module: cuda graphs
Make scale/zero_point accessible from TorchScript traced module for QFlaotFunctional modules oncall: quantization low priority triaged,oncall: quantization low priority triaged
Add `output_size` argument to `Upsample` forward method (just like for `ConvTranspose` Modules) module: nn triaged enhancement,module: nn triaged enhancement
Clarify the behavior of DataLoader sampler and batch_sampler parameters module: docs module: dataloader triaged,module: docs module: dataloader triaged
Regression in multi-node training speed with Transformers + PyTorch high priority triage review oncall: distributed triaged module: regression oncall: transformer/mha module: ddp,high priority triage review oncall: distributed triaged module: regression oncall: transformer/mha module: ddp
Migrate master to main: https://github.com/pytorch/tutorials  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/ignite  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/ELF  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/captum  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/glow  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/serve  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/xla  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/QNNPACK  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/tnt  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/hub  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/extension-cpp  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/android-demo-app  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/translate  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/elastic  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/tvm  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/ios-demo-app  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/accimage  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/extension-ffi  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/nestedtensor  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/cppdocs  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/workshops  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/extension-script  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/java-demo  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/csprng  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/pytorch_sphinx_theme  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/rfcs  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/add-annotations-github-action  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/ossci-job-dsl  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/pytorch-integration-testing  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/labeler-github-action  triaged module: infra,triaged module: infra
Migrate master to main: https://github.com/pytorch/pytorch triaged module: infra,triaged module: infra
matmul returns uninitialized memory for int64 tensors with inner dimension of zero high priority triaged module: linear algebra module: correctness (silent),high priority triaged module: linear algebra module: correctness (silent)
[ONNX] Support aten::bilinear module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Adapting the citation style according to GitHub's CFF triaged module: doc infra,triaged module: doc infra
"When someone calls detach() on a __torch_dispatch__ object, detach() gets called twice triaged module: __torch_dispatch__",triaged module: __torch_dispatch__
Improve performance of index for quantized ops oncall: quantization low priority triaged,oncall: quantization low priority triaged
[FX] Support call_method in NormalizeArgs triaged module: fx,triaged module: fx
M1 Pro Apple Silicon chip support. needs reproduction triaged module: macos module: nnpack module: arm,needs reproduction triaged module: macos module: nnpack module: arm
[libtorch]can not save a  vector<int> to AutogradContex->saved_data. module: cpp module: autograd triaged,module: cpp module: autograd triaged
Libtorch dlls delayed loading module: cuda triaged module: static linking,module: cuda triaged module: static linking
torch.distributions.categorical.Categorical does not work with 0 batch size module: distributions triaged,module: distributions triaged
_GLIBCXX_USE_CXX11_ABI=0 does not work when building from source code module: build triaged,module: build triaged
torch.bmm backward with sparse input module: sparse triaged,module: sparse triaged
Fancy indexing bug when combining masks with indexes triaged module: numpy module: advanced indexing,triaged module: numpy module: advanced indexing
Many APIs `INTERNAL ASSERT FAILED` when promoting `complex32` dtype triaged module: complex,triaged module: complex
Build release binaries with USE_GLOG=ON by default module: build triaged enhancement module: infra,module: build triaged enhancement module: infra
`torch.median` will return -2147483648 when input is an empty tensor module: error checking triaged,module: error checking triaged
"`torch.nn.functional.{instance, batch}_norm` trigger INTERNAL ASSERT FAILED when input is empty tensor with `complex32` triaged module: complex module: half",triaged module: complex module: half
Negative Exponents of Int tensors result in output of zero module: docs triaged,module: docs triaged
Wrapping make_graphed_callables with autocast issue module: cuda triaged module: amp (automated mixed precision) module: cuda graphs,module: cuda triaged module: amp (automated mixed precision) module: cuda graphs
More than 4 Dimensions for Channel Last Memory Format triaged enhancement module: memory format,triaged enhancement module: memory format
"`torch.{max,min}` have strange error message when `input.numel()==0` triaged module: reductions",triaged module: reductions
NCCL Backend does not support ComplexFloat data type high priority triage review oncall: distributed module: complex module: nccl has workaround,high priority triage review oncall: distributed module: complex module: nccl has workaround
Optimizer Overlap: Follow up features oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
Better Engineering: test_..._mem_overlap in test_torch.py should be ported to ErrorInputs module: tests triaged better-engineering,module: tests triaged better-engineering
Better Engineering: Create test_dlpack module: tests triaged better-engineering,module: tests triaged better-engineering
pip installation SSLError [SSL: CERTIFICATE_VERIFY_FAILED] needs reproduction module: binaries triaged,needs reproduction module: binaries triaged
Initialize DataLoader workers in parallel triaged module: data,triaged module: data
"`torch.sub` behaves differently with `add`, `mul`, `div` module: error checking triaged",module: error checking triaged
Compilation instructions are not exhaustive: <<parameter packs not expanded with â€˜...â€™>> on Fedora 35/CUDA 11.6 module: build triaged,module: build triaged
Memory Leak in PyTorch 1.10.1 module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
[numpy compat] torch.stack and torch.tensor doesn't support nested list+tensors (NumPy does support) - at least document the difference in the error message triaged module: numpy needs design module: viewing and reshaping,triaged module: numpy needs design module: viewing and reshaping
"`torch.cum{min,max}, torch.sort, argsort` do not check the `dim` when the input is 0-d tensor module: error checking triaged module: sorting and selection",module: error checking triaged module: sorting and selection
The signature of `torch.nanmedian` in the doc is wrong triaged module: NaNs and Infs,triaged module: NaNs and Infs
pytorch installation error with cuda!! module: binaries triaged,module: binaries triaged
Feature request: Add complex support to `torch.nanmean` feature triaged module: complex module: NaNs and Infs module: reductions,feature triaged module: complex module: NaNs and Infs module: reductions
"[pytorch1.5.0] subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '6']' returned non-zero exit status 2. module: build triaged",module: build triaged
torch.jit.script failed to compile nn.MultiheadAttention when specifying the kdim and vdim parameters. oncall: jit,oncall: jit
torch.nn.LayerNorm support for arbitrary axis in order to allow NCHW application high priority module: nn triaged,high priority module: nn triaged
Discussion of TorchQuantum and QuantumNAS triaged module: complex complex_autograd,triaged module: complex complex_autograd
Gamma distribution returns some wrong extreme values  module: numerical-stability module: distributions module: cuda triaged,module: numerical-stability module: distributions module: cuda triaged
Random Shuffle along Axis feature triaged module: random,feature triaged module: random
"Push to fork failed with cryptic ""refusing to allow a Personal Access Token to create or update workflow `.github/workflows/run_torchbench.yml` without `workflow` scope"" module: ci triaged",module: ci triaged
[feature request] [discussion] Generalize / recommend behavior of reset_parameters (and potentially rename) module: nn triaged module: fx,module: nn triaged module: fx
[discussion] torch.flatten to allow unsqueeze of inexisting dimension triaged module: batching,triaged module: batching
cannot pickle 'torch._C.Generator' object for torch.Generator triaged module: random,triaged module: random
a lot nightly builds are canceled due to VM errors since Jan14 module: build module: ci triaged,module: build module: ci triaged
Error in `torch.trapz` documentation module: docs triaged,module: docs triaged
Calling .backward() inside of an LBFGS closure function throws an exception in Libtorch v1.6.0+ module: cpp triaged module: regression,module: cpp triaged module: regression
Create a nested directory while saving objects using `torch.save` module: serialization triaged enhancement,module: serialization triaged enhancement
Allow to shutdown persistent workers triaged module: multithreading,triaged module: multithreading
Leaky cmake cuda compile options module: build module: cpp-extensions module: cpp triaged,module: build module: cpp-extensions module: cpp triaged
is_alias_of support for storageless tensors triaged module: partial aliasing module: ddp module: lazy,triaged module: partial aliasing module: ddp module: lazy
"F.cross_entropy do not have a deterministic implementation,  adding deterministic support for this operation. module: loss triaged",module: loss triaged
nn.Batchnorm1d input shape notation inconsistency module: docs triaged module: norms and normalization,module: docs triaged module: norms and normalization
make pytorch support different hardware acceleratioin configuration module: binaries module: cuda low priority triaged,module: binaries module: cuda low priority triaged
"Torchvision Installation Logic Python vs C++ ""Mismatch"" module: build triaged",module: build triaged
locally installed PyTorch upgraded or superseded on windows/macos conda tests triaged module: lts,triaged module: lts
Change the order of checks for tensor indexing errors triaged module: advanced indexing,triaged module: advanced indexing
[RFC] Gossip SGD (as a DDP Communication Hook) oncall: distributed feature module: optimizer triaged module: ddp,oncall: distributed feature module: optimizer triaged module: ddp
"[RFC] Implement array methods (extend, insert, pop, etc) for container classes feature module: nn triaged",feature module: nn triaged
Allow users to pass use_reentrant=False to checkpoint_sequential module: checkpoint triaged,module: checkpoint triaged
[torch.deploy] Using zipped torch modules in torch.deploy interpreter  triaged days oncall: package/deploy imported,triaged days oncall: package/deploy imported
"path\\tp\\torch\\torch.h(14,1): fatal error C1001: Internal compiler error. module: dependency bug module: windows module: cpp triaged",module: dependency bug module: windows module: cpp triaged
Convert a tensor with type caffe2::TypeMeta to std::vector triaged module: meta tensors,triaged module: meta tensors
[RFC] Cross-Process Performance Analysis: Straggler Detection high priority triage review oncall: distributed feature module: c10d module: ddp,high priority triage review oncall: distributed feature module: c10d module: ddp
Why AMP make backward speed more slow? module: cuda triaged,module: cuda triaged
"Kernel fusion for Gather, Apply, Scatter (GAS) model module: sparse triaged module: scatter & gather ops",module: sparse triaged module: scatter & gather ops
"[JIT][tensorexpr] cat, batch_norm opinfo tests failing oncall: jit NNC",oncall: jit NNC
JIT support for `torch.__version__` & str comparison operations oncall: jit module: bootcamp,oncall: jit module: bootcamp
"UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate   warnings.warn(""Seems like `optimizer.step()` has been overridden after learning rate scheduler needs reproduction module: optimizer triaged module: LrScheduler",needs reproduction module: optimizer triaged module: LrScheduler
"""Memory Leak"" when creating an iterator from a tensor module: memory usage triaged",module: memory usage triaged
AdaptiveAvgPool2d Failed to jit script oncall: jit,oncall: jit
The jit model will fail when calling the torch.autograd.functional.jacobian with multiple inputs and setting the vectorize to true. oncall: jit module: bootcamp NNC,oncall: jit module: bootcamp NNC
1.10.11 fails to compile libtorch_cpu with -fopenmp module: build triaged module: undefined reference,module: build triaged module: undefined reference
Move torch::deploy tests to their own workflow job high priority triaged module: deploy,high priority triaged module: deploy
torch.linalg.lstsq is nondeterministic needs reproduction triaged module: determinism module: linear algebra,needs reproduction triaged module: determinism module: linear algebra
Lack of type check in `nn.functional` APIs module: nn triaged enhancement,module: nn triaged enhancement
axis to dim remapping is not working for flip and roll triaged module: python array api,triaged module: python array api
support setting `keepdim` without setting `dim` triaged module: python array api,triaged module: python array api
Possible security issue of `torch.hub.load` triaged module: hub,triaged module: hub
`torch.diag` unexpectedly fails triaged module: numpy module: viewing and reshaping,triaged module: numpy module: viewing and reshaping
Keys of a `ModuleDict` cannot have the same name as existing `ModuleDict` class attributes. module: nn triaged,module: nn triaged
DataLoader tests are quite flaky high priority module: dataloader triaged,high priority module: dataloader triaged
traced module fails on second execution oncall: jit,oncall: jit
Slow backward for matrix multiplication of two sparse COO tensors on CPU module: sparse triaged,module: sparse triaged
PyTorch bug: Cannot pass gradient through index_add high priority module: autograd triaged module: advanced indexing module: correctness (silent),high priority module: autograd triaged module: advanced indexing module: correctness (silent)
clang format hash mismatched for linux64 module: ci module: lint triaged,module: ci module: lint triaged
Memory leak in distributions.multivariate_normal.MultivariateNormal high priority needs reproduction module: distributions module: memory usage triaged,high priority needs reproduction module: distributions module: memory usage triaged
Slowdown in torch.distributed.new_group when scaling to large clusters.  oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Rollup: forward-mode AD operator coverage module: autograd triaged actionable module: forward ad,module: autograd triaged actionable module: forward ad
`TestOperators.test_c2_op` : different raw data generated for the test in my local and CI environment module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
"`torch.{inverse,cholesky}` have wrong shape check of square matrices module: error checking triaged module: linear algebra",module: error checking triaged module: linear algebra
`torch.combinations` will allocate large memory when `r` is greater than the length of input feature module: memory usage triaged module: sorting and selection,feature module: memory usage triaged module: sorting and selection
"`torch.nn.{Constant,Zero}Pad` unexpectedly fail module: error checking triaged module: padding",module: error checking triaged module: padding
Error in `torch.Tensor.logit` documentation module: docs triaged,module: docs triaged
C++ torch::nn::Sequential clone() method overwrites child module names module: cpp module: nn triaged actionable,module: cpp module: nn triaged actionable
`torch.scatter` will return random value when `input` is empty tensor module: error checking triaged module: scatter & gather ops,module: error checking triaged module: scatter & gather ops
`torch.Tensor.where` cannot work when `y` is float triaged module: type promotion,triaged module: type promotion
Distributed broadcast fails with simple GPU tensor on Windows + GLOO oncall: distributed module: c10d,oncall: distributed module: c10d
"remote failure INTERNAL ASSERT FAILED at ""../torch/csrc/distributed/rpc/rref_context.cpp"":389 oncall: distributed triaged module: rpc",oncall: distributed triaged module: rpc
Skip LSTM quantization by default in get_default_qconfig_dict and get_default_qat_qconfig_dict oncall: quantization low priority triaged,oncall: quantization low priority triaged
installation of pytorch `cpuonly` from conda with `nomkl` installs `mkl` module: binaries triaged,module: binaries triaged
Rewrite tests in test_nn to not depend on LAPACK module: nn module: tests triaged actionable,module: nn module: tests triaged actionable
argmin/argmax incorrect doc for the first form module: docs triaged module: reductions,module: docs triaged module: reductions
Training grouped Conv2D is slow module: performance module: cudnn module: convolution triaged,module: performance module: cudnn module: convolution triaged
"ThreadLocalState::setThreadLocalState is not setting the ""enabled"" flag of SavedTensorDefaultHooks triaged module: multithreading",triaged module: multithreading
Redefinition of `cub` namespace misses Debug module: build module: cuda triaged,module: build module: cuda triaged
allow `dim=None` in `concat` triaged module: viewing and reshaping module: python array api,triaged module: viewing and reshaping module: python array api
change supported arguments for parameter `dim` in `squeeze` module: bc-breaking triaged module: python array api topic: bc breaking,module: bc-breaking triaged module: python array api topic: bc breaking
`sort` should only return the sorted input module: bc-breaking triaged module: sorting and selection module: python array api topic: bc breaking,module: bc-breaking triaged module: sorting and selection module: python array api topic: bc breaking
`unique` should be split into four partial functions triaged module: python array api,triaged module: python array api
`linspace` should support an `endpoint` parameter triaged module: tensor creation module: python array api,triaged module: tensor creation module: python array api
uint8 scalar tensors cannot be used for integer indexing triaged module: python array api,triaged module: python array api
`arange` should return empty array if bounds are inconsistent with step sign triaged module: tensor creation module: python array api,triaged module: tensor creation module: python array api
support setting `step` in `arange` without setting `end` triaged module: tensor creation module: python array api,triaged module: tensor creation module: python array api
`eye` should support other diagonals than the main one triaged module: tensor creation module: python array api,triaged module: tensor creation module: python array api
`full` should take an integer size triaged module: tensor creation module: python array api,triaged module: tensor creation module: python array api
Accuracy problem of `torch.batch_norm_gather_stats_with_counts` when `running_mean` is half tensor module: numerical-stability module: cuda triaged module: half,module: numerical-stability module: cuda triaged module: half
Memory leak while training model generated by torch.fx.symbolic_trace() in data parallel mode triaged module: fx,triaged module: fx
Add flag for functional.Jacobian to return output as well module: autograd triaged enhancement,module: autograd triaged enhancement
Support hooks-based checkpointing API with DDP high priority triage review oncall: distributed module: ddp,high priority triage review oncall: distributed module: ddp
named tensor doesn't work with deepcopy triaged module: named tensor,triaged module: named tensor
conv3d padding=`same` gradgradcheck fails on CUDA needs reproduction module: autograd module: convolution triaged module: determinism,needs reproduction module: autograd module: convolution triaged module: determinism
AdaptiveAvgPool1d - RuntimeError: shmem_size <= sharedMemPerBlockINTERNAL ASSERT FAILED module: cuda triaged,module: cuda triaged
Decouple `TensorIteratorBase` output from structured kernel outputs. triaged module: structured kernels,triaged module: structured kernels
CUSOLVER_STATUS_EXECUTION_FAILED when using the torch.logdet() module: cuda triaged module: linear algebra,module: cuda triaged module: linear algebra
[FSDP] Run parity tests for activation checkpoint and offload oncall: distributed triaged better-engineering module: fsdp,oncall: distributed triaged better-engineering module: fsdp
gloo_test test_close_connection not working as intended due to unwanted comma caffe2 triaged,caffe2 triaged
Cannot compile C++ documentation: Sphynx assertion module: docs triaged,module: docs triaged
Add nondeterministic alert to `torch.scatter_` module: cuda triaged module: determinism,module: cuda triaged module: determinism
JIT magic method which returns class instance fails. oncall: jit,oncall: jit
Behavior of torch.nn.functional.interpolate with unchanged output size and recompute_scale_factor=False module: nn triaged module: interpolation,module: nn triaged module: interpolation
[feature request] Exponential moving average (EMA) of a tensor across a dimension feature triaged module: numpy,feature triaged module: numpy
cumcount / cumulative count triaged enhancement,triaged enhancement
tmpxft_00008487_00000000-6_THCStorage.compute_86.cudafe1.cpp:(.text+0x60b): additional relocation overflows omitted from the output when build PyTorch 1.8.2 from source module: build module: cuda triaged,module: build module: cuda triaged
RuntimeError: cublas runtime error needs reproduction triaged module: cublas,needs reproduction triaged module: cublas
Conda Repodata.json file not found in Pytorch channel module: binaries triaged,module: binaries triaged
`UninitializedParameter.to(device='meta')` creates a zero-sized meta tensor instead of remaining uninitialized triaged module: meta tensors module: lazy,triaged module: meta tensors module: lazy
backward checks len of inputs before it's converted to a tuple module: autograd triaged actionable,module: autograd triaged actionable
torch.get_autocast_cpu_dtype() returns a new dtype (still?) high priority oncall: releng triaged,high priority oncall: releng triaged
Multiple invalid summaries in torch.nn documentation page module: docs module: nn triaged,module: docs module: nn triaged
Cuda sync mode input checking is wrong for non-string/int inputs. module: cuda triaged Stale,module: cuda triaged Stale
Fusion of Convolution and BatchNorm module: cudnn feature triaged,module: cudnn feature triaged
Cannot run FX tracer on a vision transformer model triaged Stale module: fx,triaged Stale module: fx
Dynamic quantified Conv2d have accuracy issue oncall: quantization low priority triaged,oncall: quantization low priority triaged
Make it possible to remove all hooks on a specified module without needing the hook handles module: nn triaged enhancement Stale,module: nn triaged enhancement Stale
Building docs locally fails module: docs triaged,module: docs triaged
[FX] Detect attribute mutation during tracing triaged module: fx,triaged module: fx
The same code can not be reproduced on multiple GPUs with dataparallel triaged module: data parallel Stale,triaged module: data parallel Stale
Exporting the operator prim_DictConstruct to ONNX opset version 13 is not supported module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
LazyModules `cls_to_become` field exposes implementation detail module: docs module: nn triaged Stale,module: docs module: nn triaged Stale
libtorch cuda use too much system memory module: cpp triaged Stale,module: cpp triaged Stale
"PyTorch crashes without an error message, when running this code snippet with torch.tensor subclassing & forward hooks (Not sure what the exact cause is, but the code snippet reliably causes it) triaged Stale tensor subclass",triaged Stale tensor subclass
`torch.broadcast_to` can create tensor with negative dimension. triaged module: correctness (silent) module: shape checking,triaged module: correctness (silent) module: shape checking
`torch.empty_strided` works when the stride is negative! triaged module: correctness (silent) module: tensor creation,triaged module: correctness (silent) module: tensor creation
Intel MKL FATAL ERROR: This system does not meet the minimum requirements for use of the Intel(R) Math Kernel Library. module: dependency bug module: binaries triaged module: macos module: mkl module: intel,module: dependency bug module: binaries triaged module: macos module: mkl module: intel
third_party/breakpad/ compilation failure triaged module: third_party,triaged module: third_party
linalg.lstsq INTERNAL ASSERT FAILED triaged module: mkl module: linear algebra Stale,triaged module: mkl module: linear algebra Stale
RuntimeError: tensor has too many (>25) dims when permuting tensor with GPU backend triaged module: assert failure Stale,triaged module: assert failure Stale
AT_ASSERT fail with DataLoaderOptions().drop_last() module: dataloader triaged Stale,module: dataloader triaged Stale
Core dumped with large matmul on aarch64 module: crash triaged module: arm Stale,module: crash triaged module: arm Stale
nan return by nn.CrossEntropyLoss when all the labels are ignore_index in torch 1.11 module: nn triaged Stale,module: nn triaged Stale
boolean mask + ellipsis lead to incorrect indexing triaged module: numpy module: advanced indexing Stale,triaged module: numpy module: advanced indexing Stale
Add hints for gradient long time overflow when using torch.cuda.amp module: cuda triaged module: amp (automated mixed precision) Stale,module: cuda triaged module: amp (automated mixed precision) Stale
Empty or NaN data pollute gradient even if they are not involved during backward module: autograd triaged module: NaNs and Infs,module: autograd triaged module: NaNs and Infs
Inconsistent multi-node latency with NCCL and OpenMPI oncall: distributed module: nccl module: openmp,oncall: distributed module: nccl module: openmp
"Shape parameter inconsistency in torch.Tensor.view, torch.reshape, torch.Tensor.reshape module: docs triaged module: viewing and reshaping",module: docs triaged module: viewing and reshaping
"SEGFAULT on ""import torch"" needs reproduction module: crash triaged",needs reproduction module: crash triaged
Unable to compile PyTorch when libcudart_static.so is not available  module: build module: cuda triaged,module: build module: cuda triaged
`TensorIterator`: provide a two-argument version of `set_output` triaged module: TensorIterator module: structured kernels,triaged module: TensorIterator module: structured kernels
CPU parallelization across batch has random faulty behavior on backward triaged module: multithreading,triaged module: multithreading
`pytorch` hangs during interaction with `ray` package module: cpu triaged module: deadlock,module: cpu triaged module: deadlock
`TensorIterator`: refactor `build_ternary_op` to match binary versions triaged module: TensorIterator,triaged module: TensorIterator
Grad strides do not match bucket view strides triaged module: ddp,triaged module: ddp
Missing Docker image for 1.10.1 module: binaries triaged module: docker Stale,module: binaries triaged module: docker Stale
[libtorch] Loading in Java two differente libtorch_cpu.so from different versions fails module: binaries feature triaged oncall: java,module: binaries feature triaged oncall: java
Optimization: convolution_backward doesn't always need to call .contiguous on certain inputs module: performance module: convolution triaged,module: performance module: convolution triaged
Can you provide the torch.trt module to directly convert the pytorch weights to tensorrt? triaged enhancement module: unknown,triaged enhancement module: unknown
Dynamic Tensor Rematerialization (DTR) feature module: memory usage triaged,feature module: memory usage triaged
Channels last performance problem module: performance triaged module: memory format Stale,module: performance triaged module: memory format Stale
EmbeddingBag allows out of index ranges.! module: dependency bug triaged,module: dependency bug triaged
[feature request] Autocast module and function wrappers feature triaged module: amp (automated mixed precision),feature triaged module: amp (automated mixed precision)
Dot product return completely incorrect result when using pip but not when using conda high priority module: cuda triaged module: correctness (silent) module: lts,high priority module: cuda triaged module: correctness (silent) module: lts
NewOperatorRegistrationTest.testImplNoDefGetsCaught failed. module: cpp module: tests triaged module: dispatch,module: cpp module: tests triaged module: dispatch
pin_memory *still* destroys custom containers  module: dataloader triaged Stale,module: dataloader triaged Stale
RuntimeError: Tensor must be CUDA and dense when calling all_gather_object even though there is no tensor in the object. oncall: distributed module: c10d,oncall: distributed module: c10d
build_ios.sh prevents iOS.cmake from configuring ios deployment target correctly module: build triaged module: ios Stale,module: build triaged module: ios Stale
API to support combined activation offloading or checkpointing oncall: distributed module: autograd triaged module: ddp module: fsdp,oncall: distributed module: autograd triaged module: ddp module: fsdp
channels_last/channels_last_3d memory format not supported for some modules on ROCm that should be supported on CUDA module: rocm triaged Stale,module: rocm triaged Stale
Derivative for _ctc_loss_backward high priority module: double backwards module: autograd triaged actionable,high priority module: double backwards module: autograd triaged actionable
torch jit script segm fault oncall: jit Stale,oncall: jit Stale
Feature request: [STFT] Add warning message if signal length is not a multiple of hop_length in torch.stft feature triaged module: complex module: fft,feature triaged module: complex module: fft
Tool for detecting inefficent striding for nn.Conv2d module: cudnn triaged enhancement oncall: profiler Stale,module: cudnn triaged enhancement oncall: profiler Stale
[RFC] Activation Checkpoint API improvements oncall: distributed module: checkpoint module: ddp Stale module: fsdp,oncall: distributed module: checkpoint module: ddp Stale module: fsdp
Tensor transfer between gpus doesnt work needs reproduction module: cuda triaged Stale,needs reproduction module: cuda triaged Stale
[JIT] Cannot `jit.export` a `@staticmethod` oncall: jit Stale,oncall: jit Stale
Could we leave the _two_ most recent nightlies in the conda channel? module: binaries feature module: ci triaged,module: binaries feature module: ci triaged
Multiprocessing - shared memory module: multiprocessing triaged,module: multiprocessing triaged
Error in `torch.cdist` documentation module: docs triaged module: distance functions,module: docs triaged module: distance functions
comile error needs reproduction module: build triaged module: mkldnn module: third_party,needs reproduction module: build triaged module: mkldnn module: third_party
Multigpu test configs intermittently timeout high priority triage review module: cuda module: ci module: flaky-tests,high priority triage review module: cuda module: ci module: flaky-tests
Torch function runtime seemingly dependent on scipy call needs reproduction module: performance triaged module: linear algebra,needs reproduction module: performance triaged module: linear algebra
Composite Compliance Problems Tracker module: internals triaged module: linear algebra module: __torch_dispatch__,module: internals triaged module: linear algebra module: __torch_dispatch__
Docs for torch.nn.MSELoss are confusing module: docs module: nn module: loss triaged Stale,module: docs module: nn module: loss triaged Stale
cpu - gpu calculation results differs by far with torch.nn.functional.linear module: numerical-stability module: cuda triaged,module: numerical-stability module: cuda triaged
torch.nn.DataParallel caused inference failure with cpu set as device on NV machine needs reproduction triaged module: data parallel Stale,needs reproduction triaged module: data parallel Stale
"FeatureAlphaDropout doesn't drop channels for (C, D, H, W) module: nn triaged module: batching Stale",module: nn triaged module: batching Stale
"JIT is overriding variables of type List[int] to type Tuple[int, int] for an unknown reason during function calls oncall: jit",oncall: jit
"Would pytorch like some free multi-factor authentication (MFA) tokens from Google & GitHub, via the OpenSSF? triaged",triaged
Context manager to enable/disabled TensorFloat32 on demand triaged module: tf32,triaged module: tf32
"torch.nn.functional.ctc_loss with invalid input produce NaN or infinity gradient, while the batch entries are fine module: nn module: loss module: error checking triaged Stale",module: nn module: loss module: error checking triaged Stale
Port MarginRankingLoss to TensorIterator module: performance module: nn module: loss triaged enhancement,module: performance module: nn module: loss triaged enhancement
subclassing torch.Tensor triaged enhancement tensor subclass,triaged enhancement tensor subclass
[feature request] Support `like=` argument in tensor factory methods feature triaged module: numpy module: tensor creation,feature triaged module: numpy module: tensor creation
Exporting the operator unfold to ONNX is not supported. module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
`CosineAnnealingWarmRestarts` should use integer epoch module: optimizer triaged module: LrScheduler,module: optimizer triaged module: LrScheduler
Incorrect error for integer `out=` dtypes when a float is expected. triaged module: type promotion module: structured kernels,triaged module: type promotion module: structured kernels
Conversion error from pytorch model to libtorch model oncall: jit Stale,oncall: jit Stale
Implement torch.*_like tensor creation functions on sparse inputs module: sparse feature triaged module: tensor creation,module: sparse feature triaged module: tensor creation
typing error in the signatures of `torch.unbind` when getting them using `torch.fx`. triaged module: fx,triaged module: fx
"Dropout2d doesn't drop channels for (C, H, W) high priority module: nn triaged module: correctness (silent)",high priority module: nn triaged module: correctness (silent)
Incomplete error message at tensor indexing (when indexing with set) module: error checking triaged Stale,module: error checking triaged Stale
torch.optim.lr_scheduler.SequentialLR.get_last_lr() does not work triaged module: LrScheduler,triaged module: LrScheduler
Implement aten::equal for sparse tensors module: sparse triaged Stale,module: sparse triaged Stale
RuntimeError: t == DeviceType::CUDAINTERNAL ASSERT FAILED when trying to calculate gradients module: cuda triaged,module: cuda triaged
[Feature request] nn.Model API: Standard model interface module: nn triaged needs research,module: nn triaged needs research
Introduction of nn.LazyRNNs module: nn module: rnn triaged enhancement Stale,module: nn module: rnn triaged enhancement Stale
last_epoch parameter of CyclicLR and OneCycleLR is not the number of epochs module: optimizer triaged Stale module: LrScheduler,module: optimizer triaged Stale module: LrScheduler
Add ability to ignore arguments/outputs in `torch.autograd.functional.jacobian` module: autograd triaged enhancement Stale,module: autograd triaged enhancement Stale
Possibly out of date error in autograd codegen module: autograd triaged Stale,module: autograd triaged Stale
[bug] the LTS torch==1.8.2 pip package is incomplete module: cuda triaged Stale module: lts,module: cuda triaged Stale module: lts
`torch.cuda.set_per_process_memory_fraction()` does not perform VRAM isolation module: docs module: cuda module: memory usage triaged,module: docs module: cuda module: memory usage triaged
Performance improvement in Autograd Forward AD using ZeroTensors module: performance module: autograd triaged actionable module: forward ad,module: performance module: autograd triaged actionable module: forward ad
Wrong PyTorch version in Docker image triaged module: docker Stale,triaged module: docker Stale
"[LazyTensor] model after to(device), traced IR changed and lose type info triaged lazy Stale",triaged lazy Stale
No dtype check for zero sparse tensor! module: sparse triaged Stale,module: sparse triaged Stale
"After updated to pytorch1.10.0 cuda11.1, NCCL is not available oncall: distributed triaged Stale",oncall: distributed triaged Stale
Fused AlphaFold 2 modules feature triaged,feature triaged
c2r fft input generation module: tests triaged module: fft,module: tests triaged module: fft
Multi-GPU training stuck when using grad scaler oncall: distributed module: cuda module: amp (automated mixed precision) Stale,oncall: distributed module: cuda module: amp (automated mixed precision) Stale
_pickle.UnpicklingError: pickle data was truncated - Windows multiprocessing during training module: windows module: multiprocessing module: dataloader module: serialization triaged,module: windows module: multiprocessing module: dataloader module: serialization triaged
[Question] How to extract/expose the complete PyTorch computation graph (forward and backward)? module: autograd triaged oncall: visualization,module: autograd triaged oncall: visualization
Make DDP + Zero have the same communication volume as regular DDP oncall: distributed module: ddp Stale,oncall: distributed module: ddp Stale
[subgraph_rewriter] Add match_filter for the matches triaged Stale module: fx,triaged Stale module: fx
[Distributed Tests] Print stacktraces of all processes when one fails oncall: distributed better-engineering Stale,oncall: distributed better-engineering Stale
cuSOLVER backend for Sparse CSR direct linear solvers module: sparse module: cuda triaged module: linear algebra,module: sparse module: cuda triaged module: linear algebra
[feature request] A rank-revealing SVD for better stability in backward. module: numerical-stability feature module: autograd triaged module: linear algebra,module: numerical-stability feature module: autograd triaged module: linear algebra
"`(svd|pca)_lowrank`: backward is unstable when for a matrix `A`, the parameter `q` is set to a value `q > rank(A)`. module: numerical-stability module: autograd triaged module: linear algebra Stale",module: numerical-stability module: autograd triaged module: linear algebra Stale
Extend composite compliant testing to backward formulas triaged Stale module: __torch_dispatch__,triaged Stale module: __torch_dispatch__
[feature request] Accepting python scalar inputs for torch.minimum/torch.maximum and friends triaged module: distance functions,triaged module: distance functions
PyTorch 1.9.1 incorrect result for all-reduce oncall: distributed,oncall: distributed
race condition of agent's TCP store on exit_barrier() triaged Stale oncall: r2p,triaged Stale oncall: r2p
Feature Request: CUDA torch.histogram (and histogramdd) module: bootcamp feature module: cuda triaged actionable module: sorting and selection,module: bootcamp feature module: cuda triaged actionable module: sorting and selection
`torch.sparse.softmax` and `torch.sparse.log_softmax` do not support negative dim. module: sparse triaged Stale,module: sparse triaged Stale
Inconsistent behavior of cosine_similarity between fp16 and fp32 inputs high priority module: numerical-stability triaged module: half module: distance functions,high priority module: numerical-stability triaged module: half module: distance functions
"with the same environment ,pytorch 1.8 worked but 1.10 can't work module: binaries module: cuda triaged Stale module: wsl",module: binaries module: cuda triaged Stale module: wsl
[RFC] Support MemoryView for Tensors module: internals feature module: cuda triaged module: numpy module: viewing and reshaping,module: internals feature module: cuda triaged module: numpy module: viewing and reshaping
Include Declarations.yaml in Libtorch distributions module: cpp triaged module: codegen Stale,module: cpp triaged module: codegen Stale
Distribution `covariance` property module: distributions triaged,module: distributions triaged
Distribution `cross_entropy` method module: distributions triaged,module: distributions triaged
Jacobian elliptic functions feature triaged module: special,feature triaged module: special
Complete elliptic integral of the first kind feature triaged module: special,feature triaged module: special
Exponentially scalable modified Bessel function of the second kind feature triaged module: special,feature triaged module: special
The formula for KL-divloss is wrong in the document module: docs module: nn triaged Stale,module: docs module: nn triaged Stale
"torch.is_tensor(obj) doesn't work with JIT, despite the fact that isinstance(obj, Tensor) already works with JIT oncall: jit Stale",oncall: jit Stale
"JIT RuntimeError:  'Union[Tensor, List[float], List[int]]' object is not subscriptable oncall: jit",oncall: jit
`torch.transpose` should raise an error when indexing 0 for 0 dimensional tensor. triaged module: viewing and reshaping,triaged module: viewing and reshaping
Integrate Libtorch into Unreal Engine 4: _ivalue_INTERNAL ASSERT FAILED oncall: jit Stale,oncall: jit Stale
`torch.hstack` should raise an error when tensor is 0 dimensional module: docs triaged Stale,module: docs triaged Stale
Port `normal` to structured kernel triaged module: structured kernels,triaged module: structured kernels
"[feature request] quantized and low-level int8 operators (matmul, gemm etc) on CUDA + integrate LLM.int8 + integrate ZeroQuant? oncall: quantization module: cuda triaged",oncall: quantization module: cuda triaged
1.10.0 failed to build due to missing TensorBody.h module: build triaged Stale,module: build triaged Stale
"[feature request] More masked reductions: amin/amax, argmin/argmax, quantile, mean/var/std/std_mean/var_mean module: sparse triaged module: masked operators",module: sparse triaged module: masked operators
Pruning `torch.nn.MultiheadAttention` causes RuntimeError triaged oncall: transformer/mha module: pruning Stale,triaged oncall: transformer/mha module: pruning Stale
I want to know how to read the LMDB file once when using DDP oncall: distributed module: dataloader,oncall: distributed module: dataloader
`torch.sspaddmm` should broadcast the input tensor module: sparse triaged Stale,module: sparse triaged Stale
`nn.functional.fractional_max_pool2d` and `nn.functional.fractional_max_pool3d` produce incorrect output on non-contiguous inputs module: nn triaged Stale,module: nn triaged Stale
[JIT] Break up JIT Opinfo Tests  oncall: jit better-engineering,oncall: jit better-engineering
`torch.tensor` relies on implicit conversion being deprecated in Python 3.10 high priority triaged module: numpy module: tensor creation,high priority triaged module: numpy module: tensor creation
Enable DDP checkpointing tests for Gloo backend oncall: distributed better-engineering module: ddp Stale,oncall: distributed better-engineering module: ddp Stale
Deprecation warnings generated when including header files in C++17 code triaged module: build warnings Stale,triaged module: build warnings Stale
Inplace and `out` variants for `positive` operator in PyTorch triaged module: ux Stale,triaged module: ux Stale
`out=` variant for `conj` unary operator triaged module: complex Stale,triaged module: complex Stale
"subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '32']' returned non-zero exit status 1. module: build triaged Stale",module: build triaged Stale
Implement reparameterized sampling for LKJCholesky distribution w.r.t. concentration parameter module: distributions triaged Stale,module: distributions triaged Stale
[ROCM] elementwise kernel launch module: rocm triaged Stale,module: rocm triaged Stale
torch.stack should have a pin_memory parameter triaged module: memory format,triaged module: memory format
Function(s) expecting a tuple argument don't accept generators module: nn triaged Stale,module: nn triaged Stale
Error in SVD cusolver on Linux needs reproduction triaged module: linear algebra,needs reproduction triaged module: linear algebra
CPU only mode with cudatoolkit 11.3 module: binaries triaged,module: binaries triaged
Protocol buffers library mismatch. module: protobuf triaged module: tensorboard,module: protobuf triaged module: tensorboard
RendezvousConnectionError when use C10d on multi nodes triaged oncall: r2p,triaged oncall: r2p
DISABLED test_remote_timeout_to_here_in_jit (__main__.FaultyJitFaultyAgentRpcTest) high priority triage review oncall: distributed oncall: jit module: flaky-tests module: rpc skipped,high priority triage review oncall: distributed oncall: jit module: flaky-tests module: rpc skipped
Fusing module with multiple hooks causes `RuntimeError: OrderedDict mutated during iteration` in fuse_known_modules oncall: quantization low priority triaged Stale,oncall: quantization low priority triaged Stale
[FX] `concrete_args` with unpacking breaks `fx.Interpreter` triaged Stale module: fx,triaged Stale module: fx
Confusing value of `TORCH_CUDA_ARCH_LIST` module: build triaged Stale,module: build triaged Stale
Generalized matmul feature triaged module: linear algebra,feature triaged module: linear algebra
Libtorch dll versioning module: binaries triaged enhancement Stale,module: binaries triaged enhancement Stale
make: Makefile: No such file or directory module: build triaged Stale,module: build triaged Stale
Lazy Tensor Core Documentation Out-of-Date module: docs triaged module: xla module: lazy,module: docs triaged module: xla module: lazy
DDP only syncs parameters used in most recent pass when `find_unused_parameters` is True. oncall: distributed better-engineering pt_distributed_rampup module: ddp Stale,oncall: distributed better-engineering pt_distributed_rampup module: ddp Stale
`TestProfilerCUDA. test_mem_leak` failing for CUDA 11.5 on Linux triage review module: performance module: cuda module: ci module: memory usage module: infra oncall: profiler,triage review module: performance module: cuda module: ci module: memory usage module: infra oncall: profiler
libtorch_cuda links against wrong libnccl.so module: build module: abi triaged module: third_party Stale,module: build module: abi triaged module: third_party Stale
Mamba does not respect `cpuonly` when creating a conda environment from YAML file oncall: releng triaged Stale,oncall: releng triaged Stale
PyTorch installation doesn't work with Python 3.10.0  oncall: releng triaged,oncall: releng triaged
Support for arbitrary schedulers in SequentialLR feature module: optimizer triaged module: LrScheduler,feature module: optimizer triaged module: LrScheduler
SequentialLR cannot be used with ReduceLROnPlateau due to .step() not allowing for optional arguments module: optimizer triaged module: LrScheduler,module: optimizer triaged module: LrScheduler
`torch.ldexp` generated tests fail on call to `torch.mul` module: autograd triaged,module: autograd triaged
Deterministic implementation for upsample_bilinear2d_backward_cuda feature module: cuda triaged module: determinism,feature module: cuda triaged module: determinism
Deterministic implementation for grid_sampler_2d_backward_cuda feature module: cuda triaged module: determinism,feature module: cuda triaged module: determinism
Strange ouput of torch.histc high priority triaged module: correctness (silent) module: sorting and selection,high priority triaged module: correctness (silent) module: sorting and selection
repeat_interleave hangs in a forked subprocess module: multiprocessing triaged,module: multiprocessing triaged
Dead lock happened ran pytorch 1.9.0 cuda11.2 on xeon gold 6326 cpu oncall: distributed triaged module: data parallel,oncall: distributed triaged module: data parallel
How to implement `bucket_by_sequence_length` with IterableDataset and DataLoader module: dataloader triaged module: data,module: dataloader triaged module: data
[cpp extension] making it possible to parallelize the building process module: build module: cpp-extensions module: ci triaged,module: build module: cpp-extensions module: ci triaged
[Bug] forward function error occurs when the scriptmodule (made in Python) including the hook function is loaded from C++(Libtorch1.8.2) oncall: jit Stale,oncall: jit Stale
Breakes with -OO flag in script needs reproduction triaged module: regression,needs reproduction triaged module: regression
"Bugs related to NCCL on RTX 6000, code freezes with no output when using DistributedDataParallel  oncall: distributed module: ddp",oncall: distributed module: ddp
Crash loading fairseq based model triaged module: hub,triaged module: hub
Bug about Dropout CUDA Kernel module: nn module: cuda triaged,module: nn module: cuda triaged
Python version is 3.7.11 in latest pytorch docker image oncall: releng triaged module: docker,oncall: releng triaged module: docker
fusion in fx graph mode did not take care of direct attribute access oncall: quantization low priority triaged,oncall: quantization low priority triaged
Setting 'padding' to a string will fail exporting Conv2d operator to ONNX. module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Stop gradient option for padding module: autograd module: nn triaged enhancement needs research Stale,module: autograd module: nn triaged enhancement needs research Stale
Incorrect accuracy  pytorch(1.0.1) and libtorch   because of nn.LSTM needs reproduction triaged,needs reproduction triaged
[In Gunicorn & multiprocessing environment] Cannot re-initialize CUDA in forked subprocess module: multiprocessing triaged Stale,module: multiprocessing triaged Stale
[DDP] Verify ignored parameter names in debug mode during init oncall: distributed triaged better-engineering module: c10d Stale,oncall: distributed triaged better-engineering module: c10d Stale
[1.10 regression][jit][cuda fuse] cat codegen wrong var name oncall: jit Stale,oncall: jit Stale
Clarify variables of BatchNorm*d functions module: docs module: nn triaged needs research module: norms and normalization Stale,module: docs module: nn triaged needs research module: norms and normalization Stale
[FSDP] Wrap API improvements triaged module: fsdp,triaged module: fsdp
[docs] Clarify DDP activation checkpointing support oncall: distributed module: docs triaged better-engineering pt_distributed_rampup module: ddp Stale,oncall: distributed module: docs triaged better-engineering pt_distributed_rampup module: ddp Stale
Incorrect documentation for `BCEWithLogitsLoss` `weight`? module: docs module: nn triaged Stale,module: docs module: nn triaged Stale
[feature request] making pytorch less noisy  high priority module: cpp-extensions feature triaged needs design module: ux,high priority module: cpp-extensions feature triaged needs design module: ux
`softmin` and `softmax` operators support different input dtypes based on whether the `dtype` kwarg is passed module: nn triaged module: ux,module: nn triaged module: ux
Add kl_divergence between Normal and Laplace distribution. module: distributions triaged enhancement Stale,module: distributions triaged enhancement Stale
RFC: Deprecate Bottleneck high priority triage review module: performance module: cpu module: deprecation oncall: profiler Stale,high priority triage review module: performance module: cpu module: deprecation oncall: profiler Stale
SequentialLR object has no attribute '_last_lr' module: optimizer triaged module: LrScheduler,module: optimizer triaged module: LrScheduler
torch.utils.data.Sampler is not recognized as a collections.abc.Sized module: dataloader module: typing triaged Stale,module: dataloader module: typing triaged Stale
Feature request: Extend `torch.eye` for creating a batch of eye matrices  feature triaged module: tensor creation,feature triaged module: tensor creation
Tensor precision manifests differently between CPU and GPU. module: numerical-stability module: cuda triaged module: arm Stale module: jetson,module: numerical-stability module: cuda triaged module: arm Stale module: jetson
Transpose of a sparse tensor is not a view operation module: sparse module: autograd triaged needs design module: viewing and reshaping Stale,module: sparse module: autograd triaged needs design module: viewing and reshaping Stale
Is it possible to use torch.linalg.cholesky for cholesky decomposition of a huge matrix that doesn't fit in memory? triaged module: linear algebra Stale,triaged module: linear algebra Stale
[ux] F.binary_cross_entropy (and maybe other losses) to auto-cast bool category mask to float module: nn triaged enhancement module: ux Stale,module: nn triaged enhancement module: ux Stale
[feature request] Sticky/force regime of train/eval modes module: nn triaged enhancement needs research module: ux,module: nn triaged enhancement needs research module: ux
DISABLED test_ind_worker_queue (__main__.TestIndividualWorkerQueue) high priority module: dataloader triaged module: macos skipped,high priority module: dataloader triaged module: macos skipped
"torch.{tensor, Tensor, LongTensor, ...} isn't captured under `enable_python_mode()` triaged module: __torch_dispatch__",triaged module: __torch_dispatch__
"`torch.flip`, `torch.roll`, `torch.tile` has arguments `dims=` for both dim/multidim while other functions have argument `dim=` for the same usage module: internals triaged module: python frontend",module: internals triaged module: python frontend
`torch.unique_consecutive`: passing positional optional arguments results in empty tensors triaged module: sorting and selection,triaged module: sorting and selection
Inconsistent list indexing behavior triaged module: advanced indexing,triaged module: advanced indexing
[ux] Print tensor's dtype by default triaged enhancement module: ux,triaged enhancement module: ux
"About ModuleDict indexing when save model based on ""torch.jit.script()"" method oncall: jit",oncall: jit
[distributed] elastic/agent/server/api.py could react faster on exit / Ctrl+D / Ctrl+C triaged oncall: r2p,triaged oncall: r2p
`vmap` performance warnings from `jacobian` module: performance triaged module: vmap,module: performance triaged module: vmap
Exporting a model with `_save_for_lite_interpreter()` fails with Segmentation fault oncall: mobile,oncall: mobile
dataloader will miss batch data when num worker>0 module: dataloader triaged,module: dataloader triaged
"[ux] [feature request] Arguments to modules(), named_modules(), children(), named_children() to filter modules of specific types module: nn triaged enhancement needs research",module: nn triaged enhancement needs research
torch.fx cannot trace torch.Size() properly triaged module: fx,triaged module: fx
'replicate' padding in convolution is 77 times slower on cpu than 'zeros' module: performance module: nn triaged module: padding,module: performance module: nn triaged module: padding
Multiprocessing hang and queue short circuiting module: multiprocessing triaged module: deadlock,module: multiprocessing triaged module: deadlock
cudnn_convolution_relu is 17x slower for 7x7 convolutions and channels last module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
[LTC][BE] Make the compile flags more strict triaged lazy,triaged lazy
[LTC][BE] Fix all compile warnings triaged lazy,triaged lazy
"Changing the type of dtype argument in softmax, log_softmax, and softmin is possibly BC-breaking. module: bc-breaking triaged module: norms and normalization topic: bc breaking",module: bc-breaking triaged module: norms and normalization topic: bc breaking
Strides issue on unsqueezed channels last tensor triaged module: memory format,triaged module: memory format
istft gradcheck fails on ROCm module: autograd module: rocm triaged module: fft,module: autograd module: rocm triaged module: fft
`torch.utils.checkpoint.checkpoint_sequential` is not optimal module: checkpoint triaged,module: checkpoint triaged
There is a problem with GPU memory reclamation module: memory usage triaged,module: memory usage triaged
`nn.functional.max_unpool(2|3)d`: failing shape check for correct inputs (with `dilation > 1`) with specified `output_size` module: nn triaged module: pooling,module: nn triaged module: pooling
[Runtime Error in ddp_pipeline.py] RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation. high priority triage review oncall: distributed module: docs triaged module: ddp,high priority triage review oncall: distributed module: docs triaged module: ddp
"Windows multi machine multi card training, card initialization in GLOO communication. oncall: distributed module: windows triaged",oncall: distributed module: windows triaged
"INTERNAL ASSERT FAILED at ""../aten/src/ATen/MapAllocator.cpp"":323, please report a bug to PyTorch. unable to mmap 68 bytes from file </torch_530808_7992>: Cannot allocate memory (12) --------------------------------------------------------------------------- module: multiprocessing module: cpu triaged",module: multiprocessing module: cpu triaged
[subgraph_rewriter] Support for non-Tensor replacement triaged module: fx,triaged module: fx
ImportError: cannot import name 'ProcessGroup' from 'torch.distributed' module: binaries oncall: distributed module: macos,module: binaries oncall: distributed module: macos
Misleading Error Message from nn.RNN when Passing Incorrect Data Type module: rnn triaged,module: rnn triaged
`Delta` not defined in the docs of `HingeEmbeddingLoss` module: docs module: nn module: loss triaged actionable,module: docs module: nn module: loss triaged actionable
"SIGILL, Illegal Instruction from libtorch_cpu.so when callling backward() function in a VM. module: dependency bug module: crash module: cpu triaged module: mkldnn",module: dependency bug module: crash module: cpu triaged module: mkldnn
"`nn.functional.max_unpool{n}d`: shape checks fail with `output_size=(C, ...)`. module: nn triaged module: pooling",module: nn triaged module: pooling
Casting real parameter to complex during forward produces warning on backward module: autograd triaged module: complex complex_autograd,module: autograd triaged module: complex complex_autograd
"isObject()INTERNAL ASSERT FAILED at ""../aten/src/ATen/core/ivalue_inl.h"":115, please report a bug to PyTorch. Expected Object but got Tensor oncall: jit module: android oncall: mobile",oncall: jit module: android oncall: mobile
sparse.mm: CUDA error: internal error when calling `cusparseSpGEMM_workEstimation [...]` module: sparse module: cuda triaged,module: sparse module: cuda triaged
Potential race conditions between multiple workers trying to download and cache the same file in torch.hub.load_state_dict_from_url and torch.hub.download_url_to_file <- duplicate dataset/model downloads across DDP workers high priority triaged module: hub actionable,high priority triaged module: hub actionable
[docs] Strange signatures for torch.autocast module: docs triaged module: amp (automated mixed precision),module: docs triaged module: amp (automated mixed precision)
[FX] [BUG] Tensor.{inplace_method}_(.) is eliminated as dead code triaged module: fx,triaged module: fx
[DDP] Debug mode should ensure reduction is finished in backward pass oncall: distributed better-engineering pt_distributed_rampup module: ddp,oncall: distributed better-engineering pt_distributed_rampup module: ddp
"INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h"":244 triaged module: linear algebra",triaged module: linear algebra
List of issues to unblock dynamic shapes in Lazy Tensor triaged lazy,triaged lazy
Query CUDA version LibTorch has been compiled with module: cuda triaged function request,module: cuda triaged function request
Option to configure TorchElastic multiprocessing/api.py PContext.close() timeout triaged enhancement,triaged enhancement
RuntimeError: CUDA error: initialization error when calling torch.distributed.init_process_group using torch multiprocessing oncall: distributed,oncall: distributed
Does Pytorch1.10 enable cuda graph default? triaged module: cuda graphs,triaged module: cuda graphs
"bug when loss backward, related to AdaptiveAvgPool1d module: cuda triaged module: pooling",module: cuda triaged module: pooling
Make `torch.Tensor.view` support autograd for appropriate cases module: autograd triaged enhancement module: viewing and reshaping,module: autograd triaged enhancement module: viewing and reshaping
clang-tidy shows false positive failure on PRs that rename files module: ci module: lint triaged,module: ci module: lint triaged
torch.nn.Linear wrong type annotation for bias module: nn module: typing triaged,module: nn module: typing triaged
RFC: â€œWhatâ€™s in a (NumPY) name?â€ â€” Changing PyTorch Operator Names high priority triaged module: numpy,high priority triaged module: numpy
torch.histogram: Sum of PDFs is num of bins instead of 1 module: docs triaged module: sorting and selection,module: docs triaged module: sorting and selection
Upgrade NCCL2.11.4 in the new PyTorch module: cuda triaged,module: cuda triaged
wsl2 install failed from source code module: build triaged module: mkl,module: build triaged module: mkl
CPU Memory Deallocation  module: memory usage triaged,module: memory usage triaged
zeta(1/2) returns nan triaged module: special,triaged module: special
Some type combinations of cublas gemm are not supported when they should module: cuda triaged module: type promotion module: linear algebra,module: cuda triaged module: type promotion module: linear algebra
Can not guarantee reproducible for `nn.ReflectionPad2d` triaged module: numerical-reproducibility,triaged module: numerical-reproducibility
[rfc] Target Determinator module: ci triaged,module: ci triaged
C++ at::Tensor's pinned_memory status is not printing out correctly. high priority module: cpp triaged module: correctness (silent),high priority module: cpp triaged module: correctness (silent)
Incorrect return type annotation for _get_mobile_model_contained_types oncall: jit,oncall: jit
pytorch/ExampleRepo module: ci triaged,module: ci triaged
functorch transforms are silently incorrect with autograd.Function high priority module: autograd triaged module: vmap module: correctness (silent),high priority module: autograd triaged module: vmap module: correctness (silent)
Some lr-schedulers docs seems to have typos/missing information module: docs module: optimizer triaged module: LrScheduler,module: docs module: optimizer triaged module: LrScheduler
Release official Python 3.9 images module: binaries triaged,module: binaries triaged
"register_ Hook causes the CUDA out of memory, and remove() is useless module: autograd triaged",module: autograd triaged
[RFC] APEX style fused optimizers in PyTorch module: performance module: optimizer triaged module: mta,module: performance module: optimizer triaged module: mta
MacOS CI result is not in /README.md module: docs triaged module: macos actionable,module: docs triaged module: macos actionable
Add TORCHELASTIC_RESTART_COUNT env variable to DDP logging oncall: distributed module: bootcamp better-engineering pt_distributed_rampup module: ddp,oncall: distributed module: bootcamp better-engineering pt_distributed_rampup module: ddp
Properly document the `to.dtype_layout` overload of `Tensor.to` module: docs triaged actionable,module: docs triaged actionable
the abnormal display of bottleneck of resnet in tensorboard triaged module: tensorboard,triaged module: tensorboard
torch.jit.export does not working oncall: jit,oncall: jit
Feature Request: Support prelu_cuda for BFloat16 module: bootcamp module: nn module: cuda triaged enhancement module: bfloat16,module: bootcamp module: nn module: cuda triaged enhancement module: bfloat16
c10::CUDAError oncall: distributed module: cuda triaged module: nccl,oncall: distributed module: cuda triaged module: nccl
Error when using torch.fx on bert triaged module: fx,triaged module: fx
"unresolved external symbol ""__declspec(dllimport) struct _object * __cdecl THPVariable_Wrap module: build triaged",module: build triaged
The `SequentialLR` scheduler uses a deprecated pattern  module: optimizer module: tests triaged module: LrScheduler,module: optimizer module: tests triaged module: LrScheduler
Torch 1.9.1.post3 - Error in magma_getdevice_arch: MAGMA not initialized (call magma_init() first) or bad device triaged,triaged
Tracker: ModuleInfo-based testing module: nn module: tests triaged tracker,module: nn module: tests triaged tracker
torch::jit::pickle_load is unable to read some files saved by torch.save oncall: jit,oncall: jit
AdaptiveMaxPool2d bug oncall: jit,oncall: jit
[libtorch][torch.jit.script][input dynamic shape]Error in loading â€œ. Ptâ€ file on C + +: index out of range oncall: jit,oncall: jit
Add more explanation on multithreaded graph building of Autograd module: docs module: multiprocessing module: autograd triaged module: multithreading actionable,module: docs module: multiprocessing module: autograd triaged module: multithreading actionable
AMP does not improve performance triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
It seems to be a bugï¼ï¼ï¼ needs reproduction triaged,needs reproduction triaged
Conv2d kernel performance regression on CPU since PyTorch 1.9 module: performance module: convolution triaged module: mkldnn module: regression,module: performance module: convolution triaged module: mkldnn module: regression
Instability in `test_input_weight_equalization_activation_values` test for random test values. oncall: quantization low priority module: tests triaged,oncall: quantization low priority module: tests triaged
Functions are rendered incorrectly module: docs module: autograd triaged,module: docs module: autograd triaged
Allow LRScheduler to take in param_groups directly without an optimizer module: optimizer triaged enhancement module: LrScheduler,module: optimizer triaged enhancement module: LrScheduler
Add an LRScheduler interface for torch schedulers. module: optimizer triaged enhancement module: LrScheduler,module: optimizer triaged enhancement module: LrScheduler
Building libtorch from source requires too much RAM module: build module: cuda triaged,module: build module: cuda triaged
Remove logic for constructing symbolic shapes in Profiler triaged better-engineering jit-backlog,triaged better-engineering jit-backlog
A `sieve` operation for separating values of a tensor into two tensors based on a condition. triaged enhancement module: ux module: viewing and reshaping,triaged enhancement module: ux module: viewing and reshaping
`torch.utils.data.random_split` example broken in 1.10 needs reproduction triaged module: data,needs reproduction triaged module: data
"Decorate tests with a ""deadline"" module: ci triaged better-engineering",module: ci triaged better-engineering
"Trying to quantize and save a pre-trained deberta model, where i get a runtime error  oncall: jit triaged",oncall: jit triaged
[feature request] torch.clamp on BoolTensors triaged enhancement module: numpy module: boolean tensor,triaged enhancement module: numpy module: boolean tensor
Feature request: bfloat16 support for CUDA matmuls module: cuda triaged enhancement module: bfloat16 matrix multiplication,module: cuda triaged enhancement module: bfloat16 matrix multiplication
OOM with a lot of GPU memory left module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
torch.jit.script fails to find attribute '_modules' of nn.Module oncall: jit,oncall: jit
"""self.graph.owning_module not set for purity check"" error when trying to remove a node from torch.fx.graph triaged module: fx",triaged module: fx
"TSAN issue in autograd ""set_next_edges"" module: autograd triaged actionable module: sanitizers",module: autograd triaged actionable module: sanitizers
MultiHeadAttention in quantizable seems incorrect with batch_first=True oncall: quantization low priority triaged oncall: transformer/mha,oncall: quantization low priority triaged oncall: transformer/mha
test_jit_fuser_legacy and test_jit_fuser_te are failing with SIGIOT high priority triage review oncall: jit module: tests NNC,high priority triage review oncall: jit module: tests NNC
Is there a pre-built pytorch package for CUDA 10.1? module: build triaged,module: build triaged
Feature request: Grid sample on complex tensors with float grid feature module: nn triaged module: complex needs research module: interpolation,feature module: nn triaged module: complex needs research module: interpolation
unnecessary warning with autocast  triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
torch.kthvalue (and maybe torch.quantile) should support argument descending/largest (=True) like topk/sort feature triaged module: sorting and selection,feature triaged module: sorting and selection
Feature request: set_grad_enabled(*mods_or_params) as a safe context manager feature module: autograd module: nn low priority triaged,feature module: autograd module: nn low priority triaged
Add `OptState.UPDATED` to `torch.cuda.amp.GradScaler` feature triaged module: amp (automated mixed precision),feature triaged module: amp (automated mixed precision)
Expose the `OptState` in `torch.cuda.amp.GradScaler` feature triaged module: amp (automated mixed precision),feature triaged module: amp (automated mixed precision)
Re-implement `Optimizer.__repr__` module: optimizer triaged needs design module: python frontend,module: optimizer triaged needs design module: python frontend
SequentialLR have a question and why it use `step(epoch)` module: optimizer triaged module: LrScheduler,module: optimizer triaged module: LrScheduler
[discussion] Have default size/shape equal to a scalar triaged module: random,triaged module: random
Adding a new kwarg to a torch.nn.functional function breaks FC for JIT  oncall: jit module: nn triaged has workaround better-engineering,oncall: jit module: nn triaged has workaround better-engineering
[FX] Support `int` and similar variants in FX triaged module: fx,triaged module: fx
TestBenchNetwork::test_forward[resnet50_jit-profiling-te] is failing intermittently high priority triage review oncall: jit NNC,high priority triage review oncall: jit NNC
LTC asynchronous executor is not synced with python code and ignores context manager. triaged LazyTensor_nvfuser_integration module: lazy,triaged LazyTensor_nvfuser_integration module: lazy
[LTC] `UncachedCompile` numbers increases for several iterations until fixed  triaged lazy module: lazy,triaged lazy module: lazy
XLA not being tested in TestAutogradDeviceType module: autograd module: tests triaged module: xla,module: autograd module: tests triaged module: xla
Batchnorm2D does behaves differently with different shapes when batch_size=1 module: nn triaged module: norms and normalization,module: nn triaged module: norms and normalization
"integer (and possibly float16) matmul fails test_noncontiguous_samples on CPU (and all backward dtypes, too?) high priority triaged module: linear algebra module: correctness (silent)",high priority triaged module: linear algebra module: correctness (silent)
`test_forward_mode_AD` hangs for `nn.functional.cosine_embedding_loss` module: autograd module: nn triaged module: deadlock module: forward ad,module: autograd module: nn triaged module: deadlock module: forward ad
Jacobian mismatch for `nn.functional.ctc_loss` high priority module: autograd module: nn triaged actionable module: correctness (silent),high priority module: autograd module: nn triaged actionable module: correctness (silent)
OpInfo JIT tests fail for torch.nonzero oncall: jit,oncall: jit
"Reductions on zero-size dims: 1) by accepting a custom default value, 2) if tensor has another non-reduced zero-size dim triaged enhancement module: reductions",triaged enhancement module: reductions
CUDA gradcheck tests can occasionally leak memory in HUD CI high priority module: cuda module: memory usage module: tests triaged,high priority module: cuda module: memory usage module: tests triaged
Deadlock in test_multiprocessing_spawn.py high priority module: multiprocessing triaged module: deadlock,high priority module: multiprocessing triaged module: deadlock
`/var/lib/jenkins/workspace/xla/test/test_mp_rendezvous.py` potentially flaky high priority triage review triaged module: flaky-tests module: xla,high priority triage review triaged module: flaky-tests module: xla
Exception in thread when using dataloader module: dataloader triaged module: multithreading,module: dataloader triaged module: multithreading
Avoid warnings when jitting pytorch modules oncall: jit,oncall: jit
[Documentation] Incomplete FX module triaged module: fx,triaged module: fx
Missing doc for torch.distributions functions module: distributions module: docs triaged,module: distributions module: docs triaged
Missing doc for torch.distributed functions high priority triage review oncall: distributed module: docs better-engineering,high priority triage review oncall: distributed module: docs better-engineering
Missing doc for torch.cuda functions module: docs module: cuda triaged,module: docs module: cuda triaged
[LTC] Replace mv with addmv in ts_native_functions.yaml triaged,triaged
[FX] FX torchvision tests fail with torchvision 0.10 triaged module: fx,triaged module: fx
JIT fuser throws compilation error (1.10 regression) oncall: jit module: regression NNC,oncall: jit module: regression NNC
[FX] autowrap_functions doesn't work for module-scoped functions triaged module: fx,triaged module: fx
Support prim_layout operator in onnx module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Feature request: Complex Number Support for Special Functions feature triaged module: complex module: special,feature triaged module: complex module: special
torch.stft - fill_cuda not implemented for ComplexHalf module: cuda triaged module: complex module: half,module: cuda triaged module: complex module: half
alias to generate tensor with random uniform distribution. feature good first issue triaged module: random,feature good first issue triaged module: random
ProcessGroupNCCLTest.testSequenceNumInit is failing on 4xlarge oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Failed to build with master. module: build triaged module: mkl,module: build triaged module: mkl
register_dispatch_key should provide a way to tell if a native function has a no-op meta kernel triaged module: dispatch module: lazy,triaged module: dispatch module: lazy
pytorch_linux_xenial_py3_6_gcc5_4_test may timeout during test_multiprocessing_spawn high priority module: multiprocessing module: tests triaged,high priority module: multiprocessing module: tests triaged
"init_process_group hangs for multi-node, Pytorch > v1.3.1 and file init_method oncall: distributed module: nccl module: deadlock",oncall: distributed module: nccl module: deadlock
[docs] Google finds docs pages that give 404 high priority module: docs triaged,high priority module: docs triaged
TestForeachCUDA.test_binary_op_tensorlists_fastpath__foreach_add_cuda_bool and TestForeachCUDA.test_pointwise_op_fastpath__foreach_addcmul_cuda_uint8 fail intermittently module: cuda module: tests triaged module: mta,module: cuda module: tests triaged module: mta
Expose `isDifferentiableType` to python module: autograd triaged better-engineering actionable,module: autograd triaged better-engineering actionable
Multiply two named tensor causes RuntimeError triaged module: named tensor,triaged module: named tensor
"Include function name in ""This Python function is annotated to be ignored and cannot be run"" error message oncall: jit",oncall: jit
test_local_optimizer_parity (__main__.TestZeroRedundancyOptimizerDistributed) is flaky on rocm high priority triage review module: rocm module: flaky-tests,high priority triage review module: rocm module: flaky-tests
Make streams used for NCCL operations configurable oncall: distributed triaged module: nccl,oncall: distributed triaged module: nccl
Build not working with cuda 11.5 module: cuda triaged,module: cuda triaged
`torch.jit.is_scripting()` not set when scripting a Module oncall: jit,oncall: jit
addition of loss function RMSE in the torch.nn  feature module: nn module: loss triaged actionable,feature module: nn module: loss triaged actionable
JIT: inconsistency in LSTM.forward oncall: jit,oncall: jit
Sparse matrix multiplication (torch.sparse.mm) NotImplementedError module: sparse feature triaged,module: sparse feature triaged
Failing test_neg_view_nn_functional_embedding_cuda_float64 high priority triage review module: nn module: cuda module: flaky-tests module: complex actionable,high priority triage review module: nn module: cuda module: flaky-tests module: complex actionable
Forward results vary depending on batch size on A100 machine module: cuda module: convolution triaged module: numerical-reproducibility,module: cuda module: convolution triaged module: numerical-reproducibility
 fx graph mode quantizing error   triaged module: fx,triaged module: fx
Importing numpy interacts with `tensor.sum` perf module: performance triaged module: numpy,module: performance triaged module: numpy
kwonly arguments without defaults don't work with test_overrides.py module: tests triaged better-engineering,module: tests triaged better-engineering
Bug? :Run torch.unique twice get different running time?  module: performance triaged module: sorting and selection module: benchmark,module: performance triaged module: sorting and selection module: benchmark
Significantly difference in execution time when convolution is run as nn.Conv2d and as nn.Sequential module: performance module: cuda module: convolution triaged,module: performance module: cuda module: convolution triaged
Lifetime issues when recording external CUDA streams with the caching allocator module: cuda triaged,module: cuda triaged
JIT vs. eager mismatches for jit.traced `int8` to `int32` casting oncall: jit NNC,oncall: jit NNC
[BUG] Inconsistent initialization on different machines (tensor.uniform_()) triaged module: random,triaged module: random
readme not update module: docs triaged,module: docs triaged
torch.chunk return type may not be documented correctly module: docs triaged,module: docs triaged
Modify Dr. CI so it could detect runner disconnection failures module: ci triaged,module: ci triaged
test_nccl_barrier_timeout_new_group_non_member fails intermittently high priority triage review oncall: distributed module: tests,high priority triage review oncall: distributed module: tests
"Make the evaluated value of function f(x) accessible from `torch.autograd.functional.jacobian(f,x)` module: autograd triaged",module: autograd triaged
Error using _stateless version of Module module: nn triaged enhancement actionable,module: nn triaged enhancement actionable
torch.triu behaves differently when diagonal out of range module: error checking triaged,module: error checking triaged
CUDAgraph error while capturing  triaged module: cuda graphs,triaged module: cuda graphs
"Libtorch C++ model forward  crashed on windows10, CUDA 11.2, Qt ,RTX 3070, but libtorch C++ works with cpu successfully module: windows triaged",module: windows triaged
Migrate C++ tests to Python runner module: tests triaged,module: tests triaged
Use `__slots__` for the `nn.Module` class module: performance module: nn good first issue triaged better-engineering actionable,module: performance module: nn good first issue triaged better-engineering actionable
Missing doc for `torch.segment_reduce` module: docs triaged module: numpy,module: docs triaged module: numpy
jit tracerï¼šLost imported pre-model parameters oncall: jit,oncall: jit
loading large model not finished after 16 hours module: performance module: serialization triaged,module: performance module: serialization triaged
"Segmentation Fault when importing Torch, 2021 version module: binaries module: crash triaged",module: binaries module: crash triaged
lr_scheduler.py  /  list index out of range needs reproduction module: optimizer triaged module: LrScheduler,needs reproduction module: optimizer triaged module: LrScheduler
Feature request: FFT operations on Metal feature module: ci triaged module: macos oncall: mobile module: fft,feature module: ci triaged module: macos oncall: mobile module: fft
Domain Transformation APIs for LibTorch and LibTorch-Lite module: cpp triaged module: vision module: data,module: cpp triaged module: vision module: data
torch.nn.functional.embedding behave differently in two cases of cpu and cuda module: error checking triaged module: embedding,module: error checking triaged module: embedding
torch.nn.EmbeddingBag behave differently in two cases of cpu and cuda module: error checking triaged module: embedding,module: error checking triaged module: embedding
`layer_norm` needs to be done in fp32 for fp16 inputs module: numerical-stability triaged module: half actionable module: norms and normalization,module: numerical-stability triaged module: half actionable module: norms and normalization
RFC: Create unified CI experience for pytorch and domain libraries module: binaries module: ci triaged better-engineering,module: binaries module: ci triaged better-engineering
[JIT] Profile optional tensor oncall: jit,oncall: jit
Static Build of Libtorch not linking correctly module: build triaged,module: build triaged
Overhaul error handling in `TCPStore` oncall: distributed better-engineering,oncall: distributed better-engineering
Separate libtorch and non-libtorch specific files under `torch/csrc/distributed` triage review oncall: distributed better-engineering module: rpc,triage review oncall: distributed better-engineering module: rpc
tools/amd_build/build_amd.py should fail if any file fails to write module: build module: rocm triaged,module: build module: rocm triaged
Bazel target all_tests improperly reports failures on CPU-only (non-CUDA) build module: bootcamp module: tests triaged module: bazel,module: bootcamp module: tests triaged module: bazel
Give a better error message when REGISTER_DISPATCH is used in improper context module: internals triaged module: dispatch,module: internals triaged module: dispatch
Is there a bug in transposed convolutionï¼Ÿ module: nn module: convolution triaged,module: nn module: convolution triaged
BatchNorm runtimeError: one of the variables needed for gradient computation has been modified by an inplace operation high priority triage review oncall: distributed better-engineering module: ddp,high priority triage review oncall: distributed better-engineering module: ddp
Normalize handling of scalar arguments feature module: tests triaged better-engineering,feature module: tests triaged better-engineering
The uniform operator param names in the C++ impl use python keywords triaged module: numpy,triaged module: numpy
BAR1 memory of GPU is not released when main process is killed. module: dataloader module: cuda module: memory usage triaged,module: dataloader module: cuda module: memory usage triaged
"0INTERNAL ASSERT FAILED, We don't have an op for aten::eq but it isn't a special case oncall: jit triaged",oncall: jit triaged
nccl comm init needs a global barrier. module: cuda triaged module: nccl,module: cuda triaged module: nccl
Inconsistencies in LSTM outputs when processing sequence stepwise on CPU module: numerical-stability module: rnn triaged,module: numerical-stability module: rnn triaged
CIFAR10 doesn't work on M1 MacBook  triaged module: arm,triaged module: arm
[opinfo] Confusing interface for `ops` decorator  module: tests triaged,module: tests triaged
Feature Request: Add constant padding_mode for grid_sample feature module: nn triaged module: padding module: interpolation,feature module: nn triaged module: padding module: interpolation
Hope to obtain authorization for Chinese translation of official tutorials. module: docs triaged,module: docs triaged
question on installing GPU-enabled Pytorch module: binaries module: cuda triaged module: lts,module: binaries module: cuda triaged module: lts
OpInfos disabled for batched forward grad computation module: autograd module: tests triaged module: vmap module: forward ad,module: autograd module: tests triaged module: vmap module: forward ad
Invalid handling of out args with type Tensor[] in native_functions.yaml module: internals triaged module: codegen,module: internals triaged module: codegen
`torch.fx.symbolic_trace()` loses module class information triaged module: fx,triaged module: fx
Modify convolution kernels for ops triaged module: fft,triaged module: fft
Memory leak issue with pytorch_java_only 1.9.0 and libtorch 1.9.0+cpu module: memory usage triaged module: android oncall: mobile oncall: java mobile_perf,module: memory usage triaged module: android oncall: mobile oncall: java mobile_perf
1.10.0-rc1 fails to compile with gcc 6.4.0 module: build triaged,module: build triaged
Should the drop_last parameter of Dataloader be mutually exclusive with the batch samplers? module: dataloader triaged,module: dataloader triaged
Latency issue with torch.sin module: performance triaged,module: performance triaged
Floating point exception in mkl_vml_serv_GetMinN () on a specific computer high priority needs reproduction module: binaries module: crash triaged module: mkl,high priority needs reproduction module: binaries module: crash triaged module: mkl
[feature request][quant] Support FakeQuant qconfigs in `test_module_init` module: tests triaged better-engineering,module: tests triaged better-engineering
Data Loader tests hang when run in ASAN test job module: dataloader module: tests triaged,module: dataloader module: tests triaged
Move the `queue_callback()` API out of `Variable._execution_engine` and into a public API module: autograd triaged better-engineering actionable,module: autograd triaged better-engineering actionable
`torch.fx.replace_pattern` doesn't work with untraceable wrapped functions triaged module: fx,triaged module: fx
Specifying USE_VULKAN=0 in launching build_android.sh does not disable Vulkan module: build triaged module: vulkan,module: build triaged module: vulkan
Cuda Out of Memory - MMDMatic module: memory usage triaged,module: memory usage triaged
Removal of BufferedShuffleDataset module: docs triaged module: data,module: docs triaged module: data
[JIT][Sym Shape Analysis] Extend analysis to extract output shape logic oncall: jit module: bootcamp,oncall: jit module: bootcamp
[feature request] Allow broadcasting for F.huber_loss and potentially other losses feature module: loss triaged module: shape checking,feature module: loss triaged module: shape checking
Bad Input to torch.nn.NLLLoss causes CUDA Error module: loss module: cuda triaged,module: loss module: cuda triaged
[feature request] BatchNorm frozen mode in core module: nn triaged enhancement module: norms and normalization,module: nn triaged enhancement module: norms and normalization
Finishing OpInfos: test_autograd.py module: autograd module: tests triaged better-engineering hackathon,module: autograd module: tests triaged better-engineering hackathon
torch.jit.script fails to cast explicit Optional parameter to bool oncall: jit,oncall: jit
[LTC] Investigate why max_pool2d_with_indices needs explicit eager fallback triaged lazy module: lazy,triaged lazy module: lazy
Crashed with `terminating with uncaught exception of type std::__1::system_error: condition_variable wait failed: Invalid argument` needs reproduction module: binaries module: crash triaged module: macos module: multithreading,needs reproduction module: binaries module: crash triaged module: macos module: multithreading
`__torch_dispatch__` can result in returning `None` for an op that should return Tensors. triaged module: pybind module: __torch_dispatch__,triaged module: pybind module: __torch_dispatch__
Torchscript `jit.script` breaks with OSError if dataclass used to pass results during model execution oncall: jit,oncall: jit
torch.cuda.power_usage (ESG ðŸŒ±) module: cuda oncall: profiler,module: cuda oncall: profiler
libtorch: collate_fn equivalent module: cpp module: dataloader triaged enhancement,module: cpp module: dataloader triaged enhancement
torch.unique signature is not descriptive module: docs triaged,module: docs triaged
[POLL][RFC] DataParallel Deprecation triaged module: deprecation,triaged module: deprecation
Support differentiability through clone and update feature module: autograd module: nn triaged needs design,feature module: autograd module: nn triaged needs design
Deprecation: Remove nn.functional.sigmoid  module: nn triaged module: deprecation,module: nn triaged module: deprecation
Inplace error of BatchNorm layer in DistributedDataParallel module oncall: distributed,oncall: distributed
`pad_packed_sequence` is not exactly the inverse of `pack_padded_sequence` module: rnn triaged,module: rnn triaged
torch.hub.load can confuse external python package with local python package.  triaged module: hub,triaged module: hub
torch.mul is not consistent with torch.multiply module: numerical-stability triaged module: numpy,module: numerical-stability triaged module: numpy
NVFuser Examples to Help with Integration with Lazy Tensor Core triaged LazyTensor_nvfuser_integration module: lazy,triaged LazyTensor_nvfuser_integration module: lazy
Incorrect reference to previous kernel in the warning displayed when overriding a previously registered kernel for the same operator and the same dispatch triaged module: dispatch,triaged module: dispatch
Failing to compile /torch/csrc/jit/frontend/ir_emitter.cpp with pointer is null warning oncall: jit,oncall: jit
Overflow error in torch.linalg uncaught high priority module: numerical-stability triaged module: multithreading module: linear algebra module: correctness (silent) module: reductions,high priority module: numerical-stability triaged module: multithreading module: linear algebra module: correctness (silent) module: reductions
INTERNAL ASSERT FAILED when reusing JIT compiled module oncall: jit,oncall: jit
"Beta distribution yields wrong results for a,b >1 and x=0 or x=1 module: distributions triaged",module: distributions triaged
Autocast sometimes discards changes to mutable arguments triaged module: amp (automated mixed precision) module: intel,triaged module: amp (automated mixed precision) module: intel
`torch.jit.save` fails on Windows oncall: jit,oncall: jit
Non-persistent modules module: nn low priority module: serialization triaged enhancement,module: nn low priority module: serialization triaged enhancement
Temporarily disabling grad doesn't work if AMP is enabled triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
torch.utils._pytree -> stable high priority feature triaged better-engineering needs design,high priority feature triaged better-engineering needs design
Codegen issues with Tensor(a!)? arguments in native_functions.yaml schemas triaged module: codegen,triaged module: codegen
[POC] Convolution-relu fusion via torch function and one-step lazy evaluation triaged module: __torch_function__ module: lazy,triaged module: __torch_function__ module: lazy
Deprecate torch.cross with optional dim triaged module: deprecation,triaged module: deprecation
Sobol state API module: docs triaged module: random,module: docs triaged module: random
Migrate current Windows CI scripts off of batch module: windows module: ci triaged better-engineering,module: windows module: ci triaged better-engineering
Indicate support for more general use of `pos_weight` in binary cross entropy module: docs module: nn triaged actionable,module: docs module: nn triaged actionable
torch.onnx.export error  module: onnx triaged onnx-needs-info,module: onnx triaged onnx-needs-info
Plain Tensor serialization does not save the content of `__dict__`. module: serialization triaged,module: serialization triaged
Add at least one config running GPU testing on sm_80+ cards module: ci triaged enhancement,module: ci triaged enhancement
Parametrization silently disables RNN weight updates  module: nn module: rnn triaged,module: nn module: rnn triaged
"repeat_interleave changes memory format from ""channels last"" to ""contiguous"" triaged module: memory format",triaged module: memory format
`torch.nn.functional.l1_loss` fails gradgradcheck for complex inputs module: autograd module: nn triaged module: complex,module: autograd module: nn triaged module: complex
[libtorch]incorrect sigmoid result on arm chip(rk3326) triaged module: arm,triaged module: arm
Memory Leak in Distributed RPC nightly oncall: distributed triaged,oncall: distributed triaged
Pipe method for torch tensors module: dataloader triaged,module: dataloader triaged
C10dProcessGroupSerialization.test_process_group_as_module_member is flaky oncall: distributed triaged,oncall: distributed triaged
Model Evaluation Returns Nan Values Sometimes for the Same Input triaged module: NaNs and Infs,triaged module: NaNs and Infs
mobile deployment: optimized_for_mobile---wrap_cpp_module error oncall: mobile mobile_perf,oncall: mobile mobile_perf
[BUG] ConvTranspose with out_channels=1  module: convolution triaged,module: convolution triaged
[JIT] torch.load(model) fails for Unicode Variable Names. oncall: jit,oncall: jit
Does ZeroRedundancyOptimizer support re-split partition on load_state_dict? oncall: distributed triaged,oncall: distributed triaged
[LTC] Building unit tests on Debian results in double defined _GLIBCXX_USE_CXX11_ABI flag triaged module: lazy,triaged module: lazy
[LTC] Memory growing up during training until OOM triaged lazy module: lazy,triaged lazy module: lazy
Test ZeRO on gloo backend for GPU oncall: distributed triaged pt_distributed_rampup module: ddp,oncall: distributed triaged pt_distributed_rampup module: ddp
Transformer modules should not hard-code the activation function module: nn triaged,module: nn triaged
Integration of __torch_dispatch__ with masked tensor semantics triaged module: __torch_dispatch__,triaged module: __torch_dispatch__
Derivative not implemented for narrow_copy module: autograd triaged actionable,module: autograd triaged actionable
"`torch.jit.trace` fails claiming ""forward method already defined"" oncall: jit",oncall: jit
pytorch serializes entire tensor when you try to pickle a slice module: serialization triaged,module: serialization triaged
Support cat() for meta tensors triaged enhancement module: meta tensors,triaged enhancement module: meta tensors
[LTC] Code-gen IRs and TorchScript lowerings needed by running full TorchBench oncall: jit triaged module: lazy,oncall: jit triaged module: lazy
PyTorch setup.py should confirm with user if the user changes build flags module: build triaged,module: build triaged
TestMultiprocessing.test_fs_sharing is flaky high priority triage review oncall: distributed module: tests triaged module: flaky-tests,high priority triage review oncall: distributed module: tests triaged module: flaky-tests
Pass backward flags such as retain_graph to context of custom torch.autograd.Function module: autograd triaged,module: autograd triaged
I cannot use x.to(GPU) or x.cuda(GPU) module: cuda triaged module: cuda graphs,module: cuda triaged module: cuda graphs
"pytorch framework tests using make_tensor hangs with pytest's boxed exec option ""--forked"" module: tests triaged",module: tests triaged
Improve input checking for running_var of nn.functional.batch_norm on CPU/GPU module: nn triaged,module: nn triaged
 Performance problems of eigh operator on CPU module: performance triaged,module: performance triaged
nn.conv1d padding='same' module: docs module: convolution triaged,module: docs module: convolution triaged
building pytorch from source without conda module: build triaged,module: build triaged
API torch.ops.image.read_file reports RuntimeError - No such operator image::read_file high priority module: internals triaged module: vision module: custom-operators,high priority module: internals triaged module: vision module: custom-operators
[LTC] Fail to run testcase of latest lazy_tensor_core branch triaged lazy module: lazy,triaged lazy module: lazy
linking error when building on Linux module: build module: cuda triaged,module: build module: cuda triaged
"Why here save `store`, rather than `prefix_store`? oncall: distributed triaged",oncall: distributed triaged
Magma : Intel MKL Errors triaged module: mkldnn module: intel module: magma,triaged module: mkldnn module: intel module: magma
No support for torch.trace on CPU for float16 tensors triaged enhancement module: half,triaged enhancement module: half
libtorch compile problem. How to get the correct protobuf version? what PROTOBUF_VERSION <3011000 and 3011004 <PROTOBUF_MIN_PROTOC_VERSION? module: build module: protobuf triaged,module: build module: protobuf triaged
Make DDP uneven inputs work with custom buffer reduction oncall: distributed triaged pt_distributed_rampup module: ddp,oncall: distributed triaged pt_distributed_rampup module: ddp
module 'torch.distributed' has no attribute 'optim' oncall: distributed triaged,oncall: distributed triaged
Add a `__torch_pre_dispatch__` that will live at the top of the dispatcher feature triaged module: dispatch,feature triaged module: dispatch
Enable is_contiguous slowpath for torch_dispatch triaged module: __torch_dispatch__,triaged module: __torch_dispatch__
Sending tensors from short-lived multiprocessing process fails on Linux module: multiprocessing triaged,module: multiprocessing triaged
torch.sparse.sum on scalar sparse tensor fails when dim is specified module: sparse triaged,module: sparse triaged
[JIT] JIT does not support non-persistent buffers oncall: jit,oncall: jit
torch.sparse.sum result has wrong dtype when reducing over all dimensions high priority module: sparse triaged needs design module: correctness (silent),high priority module: sparse triaged needs design module: correctness (silent)
[LTC] Setup a subset of TorchBench for the current implementation triaged module: lazy,triaged module: lazy
Make python_error use c10/util/Exception.h C++ backtrace machinery module: error checking triaged better-engineering,module: error checking triaged better-engineering
Exception thrown in final autograd callback (queue_callback) not caught if not on CPU thread: terminate called after throwing an instance of 'python_error' module: crash module: bootcamp module: autograd triaged,module: crash module: bootcamp module: autograd triaged
Improve DDP tutorial high priority triage review oncall: distributed module: docs triaged better-engineering module: ddp,high priority triage review oncall: distributed module: docs triaged better-engineering module: ddp
Unexpected behaviour when resuming from checkpoint using CosineAnnealingLR module: docs module: optimizer triaged module: LrScheduler,module: docs module: optimizer triaged module: LrScheduler
Make it easier to accurately reflect storage/view relationships in meta/wrapper tensors triaged module: meta tensors module: __torch_dispatch__,triaged module: meta tensors module: __torch_dispatch__
How can I re-weight a sample based on both class weights and instance weights? module: loss triaged enhancement,module: loss triaged enhancement
ChannelShuffle: Missing CUDA implementation module: cuda triaged enhancement,module: cuda triaged enhancement
openSUSE Build: CMake Generate step failed.  Build files cannot be regenerated correctly. module: build triaged,module: build triaged
[numpy] Add torch.newdim/torch.newaxis feature triaged module: numpy,feature triaged module: numpy
[numpy] Add `iscomplexobj` and `isrealobj` feature triaged module: complex module: numpy,feature triaged module: complex module: numpy
Anomaly detection: Error detected in CudnnRnnBackward0 module: cudnn module: rnn module: cuda triaged,module: cudnn module: rnn module: cuda triaged
[JIT] TorchScript not works as expected when ModuleDict has modules with different signatures oncall: jit,oncall: jit
Add an `all_paths` method to PackageExporter triaged oncall: package/deploy imported,triaged oncall: package/deploy imported
TestCppExtensionJit's test_crash_handler fails for force_on_cpu job on GHA oncall: jit triaged,oncall: jit triaged
RuntimeError: Trying to create tensor with negative dimension -1741885395: [-1741885395] module: sparse triaged,module: sparse triaged
at::parallel_for created max_threads for inputs larger than GRAIN_SIZE module: performance triaged module: openmp module: multithreading,module: performance triaged module: openmp module: multithreading
Indexing a tensor with a NumPy array sometimes works and sometimes doesn't triaged module: numpy module: advanced indexing,triaged module: numpy module: advanced indexing
Add function that retrieves a batch from a DataLoader module: dataloader triaged enhancement,module: dataloader triaged enhancement
_disabled_torch_function_impl should not be necessary for __torch_dispatch__ triaged module: __torch_function__,triaged module: __torch_function__
Multi-processing leaking file descriptors module: multiprocessing module: dataloader triaged,module: multiprocessing module: dataloader triaged
[Better Engineering] Generalize DDPSink and consolidate it with FSDP's pre-backward hooks triaged better-engineering module: fsdp,triaged better-engineering module: fsdp
Review the Tensor and Operator Basics Onboarding page module: docs triaged,module: docs triaged
Parametrization cannot be parametrized module: nn triaged,module: nn triaged
error: â€˜_mm512_set_epi8â€™ was not declared in this scope when build from source on AMD 3600 CPU module: build triaged module: vectorization,module: build triaged module: vectorization
[feature request] Jagged / padding version of torch.stack / torch.cat + some general nested tensor discussion module: sparse feature triaged module: nestedtensor module: padding,module: sparse feature triaged module: nestedtensor module: padding
Unification of model initialization methods / naming across domain libraries + support of skip_init triaged enhancement module: initialization,triaged enhancement module: initialization
Registering a global fallback for all operators that defaults us to assuming that autograd is not implemented module: bc-breaking feature module: autograd module: molly-guard triaged better-engineering topic: bc breaking,module: bc-breaking feature module: autograd module: molly-guard triaged better-engineering topic: bc breaking
profiler_legacy `_parse_legacy_records` deduplication logic has false-positive. triaged oncall: profiler,triaged oncall: profiler
How to reference a tensor variable from a superclass of `torch.Tensor`? triaged module: xla,triaged module: xla
Adding a device variable to a `torch.Tensor` subclass fails triaged module: xla,triaged module: xla
[Comment] remove torch.cuda.synchronize() in state_dict() triaged better-engineering module: fsdp,triaged better-engineering module: fsdp
[Bug] [CUDA IPC] CUDA IPC memory cost module: performance module: multiprocessing module: cuda triaged,module: performance module: multiprocessing module: cuda triaged
Bug: torch.jit.load cannot load from IOBuffer oncall: jit,oncall: jit
[FX] `passes.split_module` doesn't have documentation triaged module: fx,triaged module: fx
THPVariable_subclass_dealloc (i.e. tp_dealloc for torch.Tensor) should handle Python error state gracefully high priority module: internals triaged module: correctness (silent),high priority module: internals triaged module: correctness (silent)
torch.linalg.cholesky does not raise a RuntimeError when the matrix is not symmetric module: docs triaged module: linear algebra,module: docs triaged module: linear algebra
[DataPipe] Add Test Cases to Ensure Correct Behaviors When IterDataPipes Reset triaged module: data,triaged module: data
Gloo and TensorPipe depend on different version of libuv oncall: distributed module: build triaged module: third_party module: tensorpipe,oncall: distributed module: build triaged module: third_party module: tensorpipe
deepcopy of Lazy modules returns an exception module: nn triaged actionable module: __torch_function__ module: lazy,module: nn triaged actionable module: __torch_function__ module: lazy
torch.cross precision problem module: numerical-stability triaged,module: numerical-stability triaged
Setting a random seed or passing a generator to Modules for deterministic sampling. module: nn triaged enhancement module: random module: determinism,module: nn triaged enhancement module: random module: determinism
"[pytorch v1.9.0] for torch sync.pipeline in a single GPU, passing tensor layer by layer is better or using the skip-connection API is better? oncall: distributed triaged pipeline parallelism",oncall: distributed triaged pipeline parallelism
CUDA out of memory needs reproduction module: cuda module: memory usage triaged,needs reproduction module: cuda module: memory usage triaged
"[AWS EC2 P3DN, EFA is enabled] Torch RPC tensorpipe/common/ibv.h:172 """": Operation not supported oncall: distributed triaged module: rpc module: tensorpipe pipeline parallelism",oncall: distributed triaged module: rpc module: tensorpipe pipeline parallelism
Traceback tensor mode feature triaged better-engineering,feature triaged better-engineering
FX failed to normalize op `block_diag` and `broadcast_tensors` triaged module: fx,triaged module: fx
`INTERNAL ASSERT FAILED` after `torch.jit.freeze` oncall: jit,oncall: jit
Quantile is limited to 16 million elements and have poor performance. module: performance triaged module: numpy module: sorting and selection,module: performance triaged module: numpy module: sorting and selection
BC CI error message should link to some information about how to squash the warning module: ci triaged better-engineering,module: ci triaged better-engineering
[proposal] Let's unify the various gather/scatter ops. triaged module: advanced indexing better-engineering module: scatter & gather ops,triaged module: advanced indexing better-engineering module: scatter & gather ops
native functions should not be allowed to take in a `grad` argument module: autograd triaged actionable module: codegen,module: autograd triaged actionable module: codegen
TorchScript error loading module with float literal default argument oncall: jit,oncall: jit
Either support torch.mean on BoolTensors or fix the error message module: error checking triaged module: boolean tensor module: reductions,module: error checking triaged module: boolean tensor module: reductions
pytorch1.9.0 can not use torch.jit.load() to load model which save in pytorch1.3 oncall: jit,oncall: jit
"""RuntimeError: CUDA error: out of memory"" for no reason  needs reproduction module: cuda module: memory usage triaged",needs reproduction module: cuda module: memory usage triaged
Add support for GPU based data loading and transformation module: dataloader triaged,module: dataloader triaged
I am unable to detect torch.Tensor._version change during __torch_dispatch__ call triaged module: __torch_dispatch__,triaged module: __torch_dispatch__
CMake outputs lots of warnings in CI module: ci triaged,module: ci triaged
Base parallelism on CPU count available to process rather than system total triaged module: multithreading,triaged module: multithreading
Track Reliability of Test Reporting and Uploading triaged,triaged
Inconsistent NaN handling by cholesky between CPU and CUDA triaged module: NaNs and Infs module: linear algebra,triaged module: NaNs and Infs module: linear algebra
PyTorch inference on tensors of a particular size cause Illegal Instruction (core dumped) on Jetson Nano triaged module: arm module: jetson,triaged module: arm module: jetson
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation needs reproduction module: nn triaged,needs reproduction module: nn triaged
linalg.lstsq out variant fails internal assert because it uses non-inplace view op for some inputs high priority module: ci triaged module: linear algebra,high priority module: ci triaged module: linear algebra
[JIT] Canonicalize aten::rsub  oncall: jit,oncall: jit
[Efficiency] No shard + CPU offload in FSDP  oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Ability to explicitly close/dispose a DataLoader feature module: dataloader module: memory usage triaged module: data,feature module: dataloader module: memory usage triaged module: data
Consolidate ProcessGroup allgather_coalesced and allgather oncall: distributed module: bootcamp triaged better-engineering module: c10d,oncall: distributed module: bootcamp triaged better-engineering module: c10d
Consolidate ProcessGroup allreduce_coalesced and allreduce oncall: distributed module: bootcamp triaged better-engineering module: c10d,oncall: distributed module: bootcamp triaged better-engineering module: c10d
Named Tensors to support custom unify functions triaged enhancement module: named tensor,triaged enhancement module: named tensor
Enable BatchNorm to use running mean/variance during train feature module: nn triaged needs research module: norms and normalization,feature module: nn triaged needs research module: norms and normalization
Add out= resize warning to cat and maybe_native_stack triaged better-engineering module: viewing and reshaping,triaged better-engineering module: viewing and reshaping
Remove the dependency with CUDA_LIBRARIES in TorchConfig.cmake module: build triaged,module: build triaged
_disabled_torch_function_impl is unimplementable (must be special cased) triaged module: __torch_function__,triaged module: __torch_function__
[Beta] Have a generic `no_sync` for all synchronized training features  oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
[Tools] Extend generic join to support FSDP oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
[Tools] Improve FSDP debuggability oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
[Tools] Add FSDP logging data oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
[Beta] FSDP initialization redesign oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
Compilation from source fails module: build module: protobuf module: abi triaged,module: build module: protobuf module: abi triaged
Improve meta tensor testing module: tests triaged module: structured kernels,module: tests triaged module: structured kernels
Evaluating on single GPU (DDP) triaged module: ddp,triaged module: ddp
'LSTM' object has no attribute '_flat_weights_names' module: rnn triaged,module: rnn triaged
Private bytes consumption exception needs reproduction module: cuda module: memory usage triaged,needs reproduction module: cuda module: memory usage triaged
Interpreter initialized state changed by torch dispatch triaged module: deploy module: __torch_dispatch__,triaged module: deploy module: __torch_dispatch__
[Feature Request] Any plan to add 'Sparse Convolution' as default nn module? module: sparse feature module: convolution triaged,module: sparse feature module: convolution triaged
[LTC][Documentation] Python examples in API_GUIDE.md don't work module: docs triaged lazy module: lazy,module: docs triaged lazy module: lazy
Memory Leak in MKL OpenMP on AVX2 machine module: cpu module: memory usage triaged module: mkldnn module: mkl module: multithreading module: intel,module: cpu module: memory usage triaged module: mkldnn module: mkl module: multithreading module: intel
Replace `clone.detach` with `detach.clone` module: optimizer triaged enhancement actionable,module: optimizer triaged enhancement actionable
"Asking for ""git submodule update"" when building a specific version module: build triaged",module: build triaged
TensorPipeDistAutogradTest is frequently failing high priority triage review oncall: distributed module: tests triaged better-engineering,high priority triage review oncall: distributed module: tests triaged better-engineering
[numpy] torch.nonzero is similar to np.argwhere not np.nonzero triaged module: numpy,triaged module: numpy
FX tracing on Fairseq-based roberta fails. triaged module: fx,triaged module: fx
TypeError when using torch.cuda.list_gpu_processes() on Windows with the WDDM driver module: cuda triaged,module: cuda triaged
Error in torchScript based on Pytorch tutorial oncall: jit,oncall: jit
torch.onnx.export crash while export quantized model module: onnx triaged,module: onnx triaged
Rename C++ Serialization APIs module: serialization triaged,module: serialization triaged
Memory leak in multi-thread inference high priority module: performance module: memory usage triaged module: multithreading,high priority module: performance module: memory usage triaged module: multithreading
Create a boxed/templated ADInplaceOrView kernel module: autograd triaged,module: autograd triaged
[Prototype][RFC] PyTorch FullyShardedDataParallel(FSDP) API Proposal oncall: distributed triaged module: fsdp,oncall: distributed triaged module: fsdp
[feature request] `numpy.append` / `numpy.insrt` / `numpy.delete` equivalents and implement dynamic arrays (reallocate storage with a surplus) feature triaged module: numpy module: viewing and reshaping,feature triaged module: numpy module: viewing and reshaping
Profiler UTF-8 decode issue high priority triage review oncall: profiler,high priority triage review oncall: profiler
When compiling TAG 1.9.0 or master for androidï¼ŒAn error occurred and the compilation failedï¼ABIS_LIST=arm64-v8a module: android oncall: mobile,module: android oncall: mobile
RFC: multi-part `torch.load`/`torch.save` to support huge models and/or low CPU memory module: nn module: serialization triaged,module: nn module: serialization triaged
issues flutter in android studio triaged module: android oncall: mobile,triaged module: android oncall: mobile
Building from source results in no member named 'cerr' in namespace 'std' module: build triaged,module: build triaged
GradGrad of max_pool2d fails with empty batch dimension module: autograd module: nn triaged,module: autograd module: nn triaged
torch.empty_like not taking None as layout? module: docs triaged module: tensor creation,module: docs triaged module: tensor creation
FR: Record results of OpInfo reference tests and detect when numerics of an operator change feature module: tests triaged,feature module: tests triaged
[feature request] New copyseq_ method  feature triaged module: tensor creation,feature triaged module: tensor creation
"[clang-tidy] Errors aren't reported, runner fails if no ranges are found module: ci triaged",module: ci triaged
"[feature request] ""Batched"" index_select (i.e. simplified torch.gather with not specifying full index) high priority triaged enhancement module: scatter & gather ops",high priority triaged enhancement module: scatter & gather ops
Document how to generate Pybind bindings for C++ Autograd module: cpp triaged,module: cpp triaged
Uniformly test that defaulted Tensor arguments appropriately handle __torch_function__ module: tests triaged module: __torch_function__,module: tests triaged module: __torch_function__
 Complex recurrent layers produce NaN as grad  module: nn triaged module: complex module: NaNs and Infs,module: nn triaged module: complex module: NaNs and Infs
PyTorch public API cleanup high priority triaged better-engineering,high priority triaged better-engineering
[feature request] torch.Generator constructor to accept seed directly triaged module: random,triaged module: random
Get zero when using torch.matmul and torch.dot with 1-D tensor in torch-1.9.0-cp39-none-macosx_11_0_arm64.whl needs reproduction triaged module: correctness (silent) module: arm,needs reproduction triaged module: correctness (silent) module: arm
F.conv2d: confusing error message when using uint8 input instead of float32 module: error checking module: convolution triaged enhancement,module: error checking module: convolution triaged enhancement
AttributeError: module 'torchvision.models' has no attribute 'load_model' needs reproduction module: serialization triaged,needs reproduction module: serialization triaged
Longer int and normal Floats conversion to FloatTensor is strange module: docs triaged,module: docs triaged
nn.MultiheadAttention does not belong to activation function module: docs module: nn triaged,module: docs module: nn triaged
ciflow tracking meta issues triaged,triaged
as_tensor and negative strided np arrays triaged module: numpy module: tensor creation,triaged module: numpy module: tensor creation
modulefinder_determinator is incompatible with imports into the test/ directory module: tests triaged,module: tests triaged
CUDA error: CUBLAS_STATUS_INVALID_VALUE needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
torch.equal does not support sparse tensors module: sparse triaged,module: sparse triaged
Execute C++ based GTest unit tests from a python wrapper good first issue module: ci triaged enhancement,good first issue module: ci triaged enhancement
Some linalg operations are not taking advantage from batched computation module: performance triaged module: linear algebra,module: performance triaged module: linear algebra
"Be able to use pytest to run a ""core"" set of tests low priority module: tests triaged enhancement",low priority module: tests triaged enhancement
Uncleared memory use after torch.load() module: memory usage module: serialization triaged,module: memory usage module: serialization triaged
torch.unique acting up for a binary tensor triaged module: numpy module: boolean tensor module: correctness (silent) module: sorting and selection,triaged module: numpy module: boolean tensor module: correctness (silent) module: sorting and selection
torch.nn.functional.softmax _stacklevel undocumented module: docs module: nn triaged,module: docs module: nn triaged
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR module: cudnn triaged,module: cudnn triaged
A modest proposal: delete arithmetic overloads from c10::Half triaged module: half,triaged module: half
[LTC] Find a way to convert at::Generator into torch::jit::NamedValue oncall: jit module: lazy,oncall: jit module: lazy
cat out= variant memory_format inconsistent for cpu and cuda module: cuda triaged module: memory format,module: cuda triaged module: memory format
Direct inversion and linear systems solutions for small matrices module: performance module: cuda triaged module: linear algebra,module: performance module: cuda triaged module: linear algebra
bazel build warning: Artifact 'torch/csrc/api/include/torch/version.h' is duplicated module: build triaged module: build warnings module: bazel,module: build triaged module: build warnings module: bazel
benchmark.Compare raises: TypeError: object of type 'NoneType' has no len() module: error checking triaged module: benchmark,module: error checking triaged module: benchmark
[LTC] Investigate a testing utility to make sure correct lowering has happened triaged module: lazy,triaged module: lazy
torch.gather with sparse_grad=True does not work with SGD optimizer with momentum; gives bad error message module: sparse module: optimizer module: error checking triaged actionable,module: sparse module: optimizer module: error checking triaged actionable
Inconsistent behaviour of index put when assigning from other type triaged module: type promotion module: advanced indexing,triaged module: type promotion module: advanced indexing
More efficient colon backwards in advanced indexing module: performance triaged module: advanced indexing,module: performance triaged module: advanced indexing
quicklint clang-tidy step fails with many unrelated errors module: lint triaged,module: lint triaged
quicklint mypy step fails with assertion module: typing module: lint triaged,module: typing module: lint triaged
Add tests for DDP broadcast_buffers=True high priority triage review oncall: distributed module: bootcamp pt_distributed_rampup module: ddp,high priority triage review oncall: distributed module: bootcamp pt_distributed_rampup module: ddp
Add efficient symmetric tensor representations triaged module: linear algebra module: __torch_function__,triaged module: linear algebra module: __torch_function__
Scalar construction in C++: PY`torch.tensor(7)` !~= C++`at::tensor(7)` module: cpp triaged module: tensor creation,module: cpp triaged module: tensor creation
torch median / nanmedian w/ nans speed module: performance triaged module: NaNs and Infs module: sorting and selection module: reductions,module: performance triaged module: NaNs and Infs module: sorting and selection module: reductions
Include tensor shape in its default print formatting module: printing triaged enhancement,module: printing triaged enhancement
"Installed successfully the torch-1.9 and torch_xla-1.9. RuntimeError requested XLA_GPU:0ï¼Œbut available devices are [CPU:0, XLA_CPU:0 ] module: binaries triaged module: xla",module: binaries triaged module: xla
RuntimError:CUDA error: an illegal memory access was encountered when copying data to CUDA needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
[torch.distributed] Make dynamic_rendezvous log handler configurable feature triaged module: elastic oncall: r2p,feature triaged module: elastic oncall: r2p
`torch.scatter` doesn't fail correctly on CUDA (memory overlap) module: cuda module: tests triaged module: scatter & gather ops module: structured kernels,module: cuda module: tests triaged module: scatter & gather ops module: structured kernels
[FX] Documentation for Graph.flatten_inps and Graph.unflatten_outs triaged module: fx,triaged module: fx
Calling backward with create_graph on the output of a DistributedDataParallel throws error oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
[torch.distributed.launch|run] Hangs on SIGINT when using a TCPStore backed rdzv_backend  oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
[rfc] Hardcoded Target Determination feature module: tests triaged,feature module: tests triaged
`__torch_function__` docs should use @classmethod module: docs triaged module: __torch_function__,module: docs triaged module: __torch_function__
Segmentation fault when using C++/pybind11 module without also importing torch module: cpp-extensions triaged module: pybind module: regression,module: cpp-extensions triaged module: pybind module: regression
a problem happened in torch.randperm needs reproduction triaged module: random,needs reproduction triaged module: random
Torch.save with zip serialization causes memory bloat module: serialization triaged,module: serialization triaged
[JIT] Typing on math.ceil is inaccurate for Scalar input oncall: jit,oncall: jit
Create a new OptionalArrayRef template class to replace Optional<ArrayRef> module: internals triaged,module: internals triaged
LeakyReLU and Elu use more VRAM than needed module: autograd module: nn triaged,module: autograd module: nn triaged
Raw saved tensors can survive the deletion of the underlying SavedVariable object module: autograd triaged,module: autograd triaged
Expose a SavedTensorsHooks nn.Module for users to register saved tensors hooks feature module: autograd module: nn triaged,feature module: autograd module: nn triaged
C++ torch::cuda::synchronize speeds up training module: cuda triaged,module: cuda triaged
Java NVIDIA GPU Inference Support triaged oncall: mobile,triaged oncall: mobile
compile_commands.json doesn't contain nvcc invocations module: build triaged,module: build triaged
"Functional grid_sample: example of padding_mode=""reflection"" description error module: docs module: nn triaged module: interpolation",module: docs module: nn triaged module: interpolation
universal binaries for libtorch on mac (x86_64+arm) module: binaries feature triaged module: macos module: arm,module: binaries feature triaged module: macos module: arm
pytorch 1.8.1+cu111 used much more CPU RAM than pytorch 1.8.1 after run `import torch` module: binaries module: cuda module: cpu module: memory usage triaged,module: binaries module: cuda module: cpu module: memory usage triaged
[FX] Data type of target should be validated on Node construction and lint triaged module: fx,triaged module: fx
MultiStepLR with different gammas for each parameter group module: optimizer triaged module: LrScheduler,module: optimizer triaged module: LrScheduler
Distributed / MPI / CUDA: Incorrect messages received from isend/irecv in PyTorch 1.9 high priority oncall: distributed triaged module: correctness (silent),high priority oncall: distributed triaged module: correctness (silent)
Preserve tensor subclasses when unpacking a SavedTensor module: autograd triaged,module: autograd triaged
Cannot include extension.h under Windows - linker error  THPVariable_Wrap module: cpp-extensions module: cpp triaged,module: cpp-extensions module: cpp triaged
Supporting Pytorch for Custom Compiler Backend triaged module: dispatch module: backend,triaged module: dispatch module: backend
Error loading Pytorch lite model in Android oncall: mobile,oncall: mobile
C++ Inference error causes Identity Module oncall: jit triaged,oncall: jit triaged
Add a GitHub actions workflow for Macos module: ci triaged module: infra,module: ci triaged module: infra
"getNcclVersion() in NCCLUtils.cpp handles ""2.10.3"" incorrectly oncall: distributed module: nccl module: ddp",oncall: distributed module: nccl module: ddp
OpenCV causes backpropagation to get stuck triaged module: deadlock module: multithreading,triaged module: deadlock module: multithreading
torch.arange has issue tracing with bool input oncall: jit,oncall: jit
Consolidate distributed benchmark folders oncall: distributed module: bootcamp pt_distributed_rampup module: ddp,oncall: distributed module: bootcamp pt_distributed_rampup module: ddp
[python codegen] correctly plumb TensorOptions defaults through the python binding layer. triaged module: codegen,triaged module: codegen
Property based testing like AFL module: tests triaged,module: tests triaged
lstm's input h0 and c0 bug needs reproduction module: rnn triaged module: numpy module: NaNs and Infs,needs reproduction module: rnn triaged module: numpy module: NaNs and Infs
Automatic mixed precision works worse for 3D neural networks. module: performance triaged module: amp (automated mixed precision),module: performance triaged module: amp (automated mixed precision)
Alternate Model Loading - Android triaged enhancement module: android oncall: mobile,triaged enhancement module: android oncall: mobile
Channels last mode doesn't work with inputs with 0 batch dimension module: nn triaged module: memory format,module: nn triaged module: memory format
Named tensor in tracer triaged module: named tensor,triaged module: named tensor
pytorch 1.8.2  cuda test errors module: build module: autograd module: cuda triaged module: linear algebra,module: build module: autograd module: cuda triaged module: linear algebra
Support `torch.linalg.outer` feature triaged module: linear algebra module: python array api,feature triaged module: linear algebra module: python array api
[typing] new_ones has wrong signature module: bootcamp module: typing triaged,module: bootcamp module: typing triaged
DistributedDataParallel documentation oncall: distributed triaged better-engineering module: ddp,oncall: distributed triaged better-engineering module: ddp
Serialising `torch.bool` generates a warning about `np.bool` being deprecated.  module: error checking module: serialization triaged module: numpy,module: error checking module: serialization triaged module: numpy
torch.scatter_: support index.size(d) > src.size(d) good first issue triaged enhancement module: scatter & gather ops,good first issue triaged enhancement module: scatter & gather ops
`help(torch.finfo)` doesn't give useful information even though class has documentation module: docs triaged,module: docs triaged
"cuda.is_available returns True in pycharms python console, False in code module: cuda triaged",module: cuda triaged
[bug] torch.distributed.elastic logging: Failed to print the first info statement triaged module: elastic oncall: r2p,triaged module: elastic oncall: r2p
Torch.nn.functional.interpolate() function memory release problem module: memory usage triaged module: interpolation,module: memory usage triaged module: interpolation
Memory leak in `ddp_zero_hook.py` hooks module: memory usage triaged module: ddp,module: memory usage triaged module: ddp
"document our compatibility with upstream packages (e.g. languages, compilers, tools) module: docs triaged",module: docs triaged
split up test/test_autograd.py module: autograd triaged better-engineering,module: autograd triaged better-engineering
ROCm build documentation lacks needed dependencies module: build module: docs module: rocm triaged actionable,module: build module: docs module: rocm triaged actionable
[documentation] torch.distributed.elastic: illustrate how to write load_checkpoint and save_checkpoint in Train Script  module: docs triaged module: elastic oncall: r2p,module: docs triaged module: elastic oncall: r2p
Sign in slogdet is set to requires_grad = False even when using complex numbers.  triaged module: complex module: linear algebra complex_autograd,triaged module: complex module: linear algebra complex_autograd
torch.jit.trace quantized bigbird leads to 0INTERNAL ASSERT FAILED runtime error oncall: jit,oncall: jit
Implement half_to_float flag for both _log_softmax and _softmax operators triaged module: lazy,triaged module: lazy
Split up test_nn.py module: nn triaged better-engineering,module: nn triaged better-engineering
script/build_android nn::Module support triaged module: android oncall: mobile,triaged module: android oncall: mobile
"PyTorch 1.9.0, test_optim fails on Nvidia A10. module: optimizer module: cuda module: tests triaged",module: optimizer module: cuda module: tests triaged
torch.cuda.amp fails with torch.sparse.softmax module: sparse triaged module: amp (automated mixed precision),module: sparse triaged module: amp (automated mixed precision)
Tracing models all the way down to basic functions feature triaged module: fx,feature triaged module: fx
CUDA error: invalid configuration argument for torch.sparse tensor backward module: sparse module: cuda triaged,module: sparse module: cuda triaged
Find out why TestSparseCPU.test_softmax_cpu_float64 grad and input dtypes are different triaged module: structured kernels,triaged module: structured kernels
[RFC] Should DDP support custom reduction logic for registered module buffers? oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
[bug] nn.functional.pad (circular) ubsan failure module: nn triaged module: padding,module: nn triaged module: padding
[feature request] Type promotions for Boolean tensors with sub operation + Numpy compatability triaged module: numpy module: type promotion module: boolean tensor,triaged module: numpy module: type promotion module: boolean tensor
grid_sample should not require grid to be the same dimension as input. module: nn triaged enhancement module: interpolation,module: nn triaged enhancement module: interpolation
ignore_index does not work as described with nn.CrossEntropyLoss module: nn triaged,module: nn triaged
Add nn.Module full pre-backward hooks feature module: nn triaged,feature module: nn triaged
Implement `set_flush_denormal ` for ARM triaged enhancement module: arm,triaged enhancement module: arm
Don't use try-catch to handle overload resolution handling in torch.ops module: bootcamp triaged module: __torch_dispatch__,module: bootcamp triaged module: __torch_dispatch__
Audit use of `C10_UNUSED` triaged better-engineering,triaged better-engineering
[JIT] NVRTC unknown error  oncall: jit NNC,oncall: jit NNC
Make axes selection keyword arguments in torch.diagonal and torch.transpose consistent triaged module: numpy module: linear algebra module: python array api,triaged module: numpy module: linear algebra module: python array api
`tools/autograd/derivatives.yaml` doesn't support methods on optional tensor module: autograd triaged actionable module: codegen,module: autograd triaged actionable module: codegen
Compilation error with gcc11 and libstdc++11 (thread_id != operator removed) triaged module: mkldnn,triaged module: mkldnn
GPU compatibility check without initializing CUDA module: cuda triaged,module: cuda triaged
cudnn_convolution_add_relu fails under basic conditions module: dependency bug module: cudnn triaged,module: dependency bug module: cudnn triaged
[Testing] memory_format decorator module: tests triaged module: memory format,module: tests triaged module: memory format
Use `linecache.lazycache` in torch.fx module: bootcamp triaged module: fx,module: bootcamp triaged module: fx
Add a mechanism to not universally dispatch to python for all for all operations when using __torch_dispatch__  triaged module: __torch_dispatch__,triaged module: __torch_dispatch__
Type of first constructor parameter of ElasticDistributedSampler and DistributedSampler oncall: distributed module: dataloader,oncall: distributed module: dataloader
Conjugate fallback dispatch key should be per-backend triaged module: complex module: dispatch,triaged module: complex module: dispatch
Key already registered with the same priority: uv module: internals triaged module: tensorpipe,module: internals triaged module: tensorpipe
Multiplication of non-int torch.tensor with string gives unexpected TypeError module: error checking triaged,module: error checking triaged
Convert unset variables in `Reducer::Timer` to use c10::optional triaged better-engineering pt_distributed_rampup module: ddp,triaged better-engineering pt_distributed_rampup module: ddp
"aten::pixel_shuffle(Tensor self, int upscale_factor) -> (Tensor): Argument self not provided. oncall: jit",oncall: jit
"slice_embed, select_embed like diag_embed module: autograd triaged module: viewing and reshaping",module: autograd triaged module: viewing and reshaping
allow specification of variadic arguments in native_functions.yaml module: internals triaged,module: internals triaged
Enable fft  support for mobile builds enhancement module: android oncall: mobile module: arm module: fft,enhancement module: android oncall: mobile module: arm module: fft
Support `torch.linalg.einsum` feature triaged module: linear algebra,feature triaged module: linear algebra
Remove c10d::kDefaultFirstBucketBytes oncall: distributed module: bootcamp triaged pt_distributed_rampup module: ddp,oncall: distributed module: bootcamp triaged pt_distributed_rampup module: ddp
Runtime error when passing dim as None in torch.squeeze (and Tensor.squeeze) triaged module: numpy,triaged module: numpy
Seeing unit test failures for ProcessGroupShareTensorTest high priority triage review oncall: distributed module: nccl module: c10d,high priority triage review oncall: distributed module: nccl module: c10d
Add a way to edit CI env variables from PRs low priority triaged better-engineering,low priority triaged better-engineering
Can not parse the caffe2 pretrain model. google.protobuf.message.DecodeError: Error parsing message caffe2 triaged,caffe2 triaged
Use fast gpuAtomicAdd for FP16 data type in CUDA kernels module: cuda triaged module: half,module: cuda triaged module: half
[FX] Support pytree `concrete_args` for *args triaged module: fx,triaged module: fx
dynamic shapes triaged module: lazy,triaged module: lazy
Make printing to stdout / stderr in tests in CI an error triaged better-engineering,triaged better-engineering
Add Convolution support for lazy tensor module: convolution triaged module: lazy,module: convolution triaged module: lazy
Rewrite PyTorch CUDA backend in Triton module: cuda triaged,module: cuda triaged
"Building pytorch from source results in ""an object with that name is already defined"" error on import module: build triaged module: pybind",module: build triaged module: pybind
Make KernelFunction::makeFromUnboxedFunctor infer KernelFunctor from input argument module: bootcamp triaged module: dispatch,module: bootcamp triaged module: dispatch
Checkpointing without re-entrant autograd module: autograd triaged,module: autograd triaged
Error while importing torch: libtorch_python.so: undefined symbol: PyThread_tss_alloc module: abi triaged,module: abi triaged
Feature request: Generate Sobol points directly on the GPU module: distributions feature module: cuda triaged module: tensor creation,module: distributions feature module: cuda triaged module: tensor creation
add gamma to CosineAnnealingWarmRestarts so max lr can decrease in cycles module: optimizer triaged module: LrScheduler,module: optimizer triaged module: LrScheduler
Instructions for CMAKE_PREFIX_PATH seem to be broken module: build module: docs triaged,module: build module: docs triaged
LazyTensor support for fuser integration investigation triaged LazyTensor_nvfuser_integration module: lazy,triaged LazyTensor_nvfuser_integration module: lazy
LazyTensor cuda fallback triaged LazyTensor_nvfuser_integration module: lazy,triaged LazyTensor_nvfuser_integration module: lazy
LazyTensor recomputes intermediate tensor in fragmented computation triaged LazyTensor_nvfuser_integration module: lazy,triaged LazyTensor_nvfuser_integration module: lazy
Unexpected `is_training` after using `train` in a JIT Module oncall: jit,oncall: jit
"""test_events_wait"" is flaky on ROCm module: rocm triaged module: flaky-tests",module: rocm triaged module: flaky-tests
Unnecessary python bindings and documentation for internal functions/ops triaged module: codegen,triaged module: codegen
static library kineto_LIBRARY-NOTFOUND not found. triaged module: build warnings,triaged module: build warnings
`c10::optional<T>` operators should delegate to corresponding operators on `T` module: cpp triaged,module: cpp triaged
[TorchScript] Inconsistency between Python and PyTorch when raising 0 to a negative power oncall: jit,oncall: jit
`torch.fx.node.normalized_arguments` does not pass `normalize_to_only_use_kwargs` module: bootcamp triaged module: fx,module: bootcamp triaged module: fx
[FX] to_folder breaks when a directly-traced `nn.Sequential` is dumped module: bootcamp triaged module: fx,module: bootcamp triaged module: fx
Incosistency with args for `nn.functional.max_poolNd` vs `nn.MaxPoolNd` functions module: nn triaged module: pooling,module: nn triaged module: pooling
[Meta] Change default branch name to `main` for repos in `pytorch` project module: bootcamp triaged module: infra,module: bootcamp triaged module: infra
torch.prod internal asserts when passed a tensor that requires_grad (and a dtype) module: autograd triaged module: assert failure,module: autograd triaged module: assert failure
meta tensor `set_` seems fishy module: internals triaged module: meta tensors,module: internals triaged module: meta tensors
Learning rate scheduler list index out of range module: optimizer triaged module: LrScheduler,module: optimizer triaged module: LrScheduler
Sparse updates to logits in distributions.Categorical module: sparse module: distributions module: autograd module: memory usage triaged,module: sparse module: distributions module: autograd module: memory usage triaged
torch.Generator ignores the device argument? triaged module: random,triaged module: random
Review replacing test/test_namedtuple_return_api.py with an OpInfo-based test feature module: tests triaged,feature module: tests triaged
Dynamic tensor rematerialization module: internals module: checkpoint feature triaged,module: internals module: checkpoint feature triaged
Understatement about how running_var is computed in BatchNorm module: docs triaged module: norms and normalization,module: docs triaged module: norms and normalization
"Make interpolation output size compatible with opencv, scikit-image and scipy for floating scale factor triaged enhancement module: interpolation",triaged enhancement module: interpolation
crash in libtorch jit oncall: jit,oncall: jit
Bad performance of stock model on Windows compared to Linux module: performance module: windows module: cpu triaged,module: performance module: windows module: cpu triaged
libtorch on Apple m1 module: cpp module: ci triaged module: macos enhancement module: arm,module: cpp module: ci triaged module: macos enhancement module: arm
torch.cholesky_inverse supports tensors with more than 2 dimensions module: docs triaged module: linear algebra,module: docs triaged module: linear algebra
Some way to specify expected failures for OpInfo-based tests module: tests triaged enhancement,module: tests triaged enhancement
[feature request] Expand a given dim triaged enhancement module: viewing and reshaping,triaged enhancement module: viewing and reshaping
NotImplementedError: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. module: cuda triaged module: vision,module: cuda triaged module: vision
"How to Fix â€œAssertionError: CUDA unavailable, invalid device 0 requestedâ€ module: binaries triaged",module: binaries triaged
broken pipe error  needs reproduction module: crash module: cuda triaged,needs reproduction module: crash module: cuda triaged
Bus error (core dumped) when import torch needs reproduction module: crash module: build triaged,needs reproduction module: crash module: build triaged
[jit]][script] handling of 'void' returns oncall: jit,oncall: jit
[JIT] Support JAX-style statically shaped nonzero to avoid host-device synchronization high priority module: cuda triaged module: advanced indexing needs design,high priority module: cuda triaged module: advanced indexing needs design
Error while running FINETUNING TORCHVISION MODELS needs reproduction module: crash triaged module: vision,needs reproduction module: crash triaged module: vision
Refactor serialization tests to use device parametrization module: tests module: serialization triaged,module: tests module: serialization triaged
`periodic-pytorch-linux-xenial-cuda11.3-cudnn8-py3.6-gcc7-build` started to fail  module: cuda triaged,module: cuda triaged
Adding a new operator shouldn't need to trigger a rebuild of CUDA kernels module: build module: cuda triaged,module: build module: cuda triaged
Support `torch.linalg.trace` triaged module: python array api,triaged module: python array api
Dispatch to Python with wrappers triggers Autograd view asserts triaged module: __torch_dispatch__,triaged module: __torch_dispatch__
Wrong implementation of method log_prob in torch.distributions.negative_binomial module: distributions module: docs triaged module: numpy,module: distributions module: docs triaged module: numpy
[feature request] torch.nn.Conv3d on tensors having more than 5 dimensions feature module: nn module: convolution triaged,feature module: nn module: convolution triaged
Error installing from source module: build triaged,module: build triaged
"RuntimeError: xnn_status_success == run_status INTERNAL ASSERT FAILED at ""/pytorch/aten/src/ATen/native/xnnpack/Linear.cpp"":158 module: android oncall: mobile module: ios",module: android oncall: mobile module: ios
Clarify sparse COO tensor coalesce behavior wrt overflow + how to binarize a sparse tensor module: sparse module: docs triaged,module: sparse module: docs triaged
AttributeError: module 'torch.jit' has no attribute '_script_if_tracing' oncall: jit,oncall: jit
cuSPARSELt Integration module: sparse module: cuda triaged enhancement,module: sparse module: cuda triaged enhancement
Tensor subclassing does not support bool input triaged module: boolean tensor module: tensor creation,triaged module: boolean tensor module: tensor creation
Dedupe code in functional optim classes module: bootcamp triaged better-engineering pt_distributed_rampup module: ddp,module: bootcamp triaged better-engineering pt_distributed_rampup module: ddp
"nn.DataParallel not working on A100 with nvidia-driver 470.42.01, Cuda 11.1, Debian 10 module: multi-gpu triaged module: data parallel",module: multi-gpu triaged module: data parallel
"[FR] Support for non-Parameter, non-Buffer objects in state_dict module: nn triaged enhancement",module: nn triaged enhancement
Memory Leak Found in Persistent DataLoader module: dataloader module: memory usage triaged,module: dataloader module: memory usage triaged
cudnn_batch_norm_backward is extremely imprecise for some input shapes module: cudnn triaged,module: cudnn triaged
"Create a boxed fallback / template recipe for Autograd that forwards, but errors on backwards module: internals module: autograd triaged module: dispatch",module: internals module: autograd triaged module: dispatch
Caffe2 tensor to ATen conversion doesn't initialize PyTorch CUDA state caffe2 module: internals module: cuda triaged,caffe2 module: internals module: cuda triaged
`x.to(memory_format=torch.contiguous_format)` does not always return a contiguous tensor module: docs triaged module: memory format needs design,module: docs triaged module: memory format needs design
Should we be upcasting integral types to int64 in torch.sum and torch.prod? triaged module: numpy module: reductions module: python array api,triaged module: numpy module: reductions module: python array api
[FX] Ability to provide autowrap function to Tracer init triaged module: fx,triaged module: fx
ppc64le build fail: invalid conversion from Bfloat16 in functional_base.h:180 triaged module: POWER module: bfloat16,triaged module: POWER module: bfloat16
Enable clang-tidy on all of master triaged,triaged
multi-input and multi-output support for nn.Module module: nn triaged,module: nn triaged
FX incorrectly provides `int` instead of `float` for `value` kwarg of `nn.functional.pad` triaged module: fx,triaged module: fx
Bootcamp Task: Add `prim::to_mkldnn` to convert from aten's nchw to mkldnn's nChw8c directly oncall: jit module: bootcamp jit-backlog,oncall: jit module: bootcamp jit-backlog
Vulkan: error -5 when trying to use it with Nvidia card module: android oncall: mobile module: vulkan,module: android oncall: mobile module: vulkan
NNAPI delegate library cannot be built on MacOS module: build triaged module: macos,module: build triaged module: macos
Calling `torch.save()` twice writes a corrupted file module: serialization triaged,module: serialization triaged
RPC memory leak for CPU high priority triage review oncall: distributed,high priority triage review oncall: distributed
RuntimeError: !ref.requires_grad()INTERNAL ASSERT FAILED oncall: jit triaged,oncall: jit triaged
Support matrix operations between complex and real tensors triaged module: complex module: type promotion module: linear algebra,triaged module: complex module: type promotion module: linear algebra
Reducing over empty dimensions for reductions without identity triaged module: numpy needs design module: reductions,triaged module: numpy needs design module: reductions
The matrix multiplication operator can't get correct results on 3090 !! module: cuda triaged module: tf32,module: cuda triaged module: tf32
Investigate fairscale's use of storage APIs and find alternatives triaged ezyang's list,triaged ezyang's list
died with <Signals.SIGSEGV: 11> oncall: distributed triaged,oncall: distributed triaged
Broadcasting documentation diverges from implementation module: docs module: bootcamp triaged,module: docs module: bootcamp triaged
 Bail out if `-std=c++` setting is detected during build module: build triaged,module: build triaged
Improved OpInfo dtype testing module: tests triaged,module: tests triaged
"[discussion] Support other index dtypes for scatter, scatter_reduce and other indexing functions in addition to int64: uint8, int16, int32 (without nya casting reallocations) triaged enhancement module: type promotion module: advanced indexing actionable module: scatter & gather ops",triaged enhancement module: type promotion module: advanced indexing actionable module: scatter & gather ops
Better argument names for torch.atan2 and other math functions triaged module: numpy,triaged module: numpy
Letting `_allgather_base` to support multiple tensors as inputs and outputs oncall: distributed triaged,oncall: distributed triaged
Can I set the algorithm of torch.nn.Conv to CUDNN_CONVOLUTION FWD_ALGO_GEMM by myself? module: cudnn module: cuda module: convolution triaged,module: cudnn module: cuda module: convolution triaged
`make_tensor` tracking issue triaged module: testing tracker,triaged module: testing tracker
"Can't specify compiler, leading to SUPPORT_GLIBCXX_USE_C99 failed module: build triaged",module: build triaged
```torch.distributions.Categorical``` unintended  ```log_prob``` gradient w.r.t ```probs``` module: distributions triaged,module: distributions triaged
ATen codegen is always re-running for Makefile generator module: build triaged,module: build triaged
[pytorch android] use Vulkan backend crash module: android oncall: mobile module: vulkan,module: android oncall: mobile module: vulkan
miss header file while using python setup.py install to install pytorch with anaconda env module: build triaged,module: build triaged
[discussion] Remove the need of mandatory super() nn.Module's call module: nn triaged enhancement,module: nn triaged enhancement
TCPStoreTest.testWatchKeyCallback Timed Out oncall: distributed triaged module: flaky-tests module: c10d,oncall: distributed triaged module: flaky-tests module: c10d
Default Collate doesn't work for subtypes of ndarray module: dataloader triaged,module: dataloader triaged
Support adding new keyword-only parameters without breaking FC module: internals triaged module: codegen needs design,module: internals triaged module: codegen needs design
`super().__init__()` usage for `_Joinable` oncall: distributed triaged,oncall: distributed triaged
Return `Future` instead of `Work` in `notify_join_context()` oncall: distributed triaged,oncall: distributed triaged
Pytorch model load failure in Gunicorn with Gevent workers needs reproduction module: nn module: dataloader triaged module: numpy,needs reproduction module: nn module: dataloader triaged module: numpy
Python program locks up when using some innocuous code inside mp.spawn() oncall: distributed triaged,oncall: distributed triaged
Move `torch.cholesky_solve` into `torch.linalg`. triaged enhancement module: linear algebra,triaged enhancement module: linear algebra
Allow creation of pseudo devices for testing purposes module: internals triaged,module: internals triaged
Move `lobpcg` into `torch.linalg` triaged enhancement module: linear algebra,triaged enhancement module: linear algebra
"Move `{svd,pca}_lowrank` into `torch.linalg` triaged enhancement module: linear algebra",triaged enhancement module: linear algebra
Move `tensordot` into `torch.linalg` triaged enhancement module: linear algebra module: python array api,triaged enhancement module: linear algebra module: python array api
`channel_shuffle` output is sometimes aliased with its input module: autograd module: nn triaged,module: autograd module: nn triaged
Enhance Distributed `get_future()` docs oncall: distributed triaged better-engineering pt_distributed_rampup,oncall: distributed triaged better-engineering pt_distributed_rampup
"When 'trapezoid' is called with an empty tensor input, it does not produce an output with requires_grad module: autograd triaged",module: autograd triaged
Possibly flaky test: TestMultiprocessing.test_fd_sharing triaged module: flaky-tests,triaged module: flaky-tests
"Proposal: allow using axis, axes, dim and dims interchangeably  triaged needs design module: ux module: python array api",triaged needs design module: ux module: python array api
Many reduction operators do not support reducing over multiple dimensions triaged module: numpy module: reductions module: python array api,triaged module: numpy module: reductions module: python array api
[nn] Passing dtype to `_stacklevel` argument in `log_softmax` silently works module: bc-breaking module: nn triaged module: deprecation module: functional UX topic: bc breaking,module: bc-breaking module: nn triaged module: deprecation module: functional UX topic: bc breaking
`__torch_function__` always records default kwargs as kwargs triaged module: __torch_function__,triaged module: __torch_function__
The use of epsilon in torch.nn.functional.pairwise_distance leads to incorrect minima of the distances module: nn triaged module: correctness (silent) module: distance functions,module: nn triaged module: correctness (silent) module: distance functions
Function Request: scipy.ndimage.map_coordinates  triaged module: numpy function request module: interpolation,triaged module: numpy function request module: interpolation
"I have graphic card of P4, but torch.cuda.is_available() always shows false module: cuda triaged",module: cuda triaged
F.nll_loss with 16-bit CUDA tensors and reduction=mean produces NaNs module: numerical-stability module: loss triaged module: NaNs and Infs module: reductions,module: numerical-stability module: loss triaged module: NaNs and Infs module: reductions
MemoryError with pip Install even with --no-cache-dir option module: binaries triaged,module: binaries triaged
Buggy page with overlapped tabs module: docs triaged,module: docs triaged
What is find_package(Torch REQUIRED) doing that a manual include/glob doesnt? module: build triaged,module: build triaged
UCI Data Sets feature module: dataloader triaged,feature module: dataloader triaged
Replace unbiased parameter in torch.(std|var|std_mean|var_mean) with correction=0 triaged module: deprecation module: reductions module: python array api,triaged module: deprecation module: reductions module: python array api
Deprecate torch.(min|max|median|mode) to only return values and not indices triaged module: numpy module: deprecation needs design module: reductions module: python array api,triaged module: numpy module: deprecation needs design module: reductions module: python array api
Some reduction operators have double signatures triaged module: deprecation module: reductions module: python array api,triaged module: deprecation module: reductions module: python array api
CompositeImplicitAutograd ops should not call out= variants of operators module: autograd triaged,module: autograd triaged
Implement missing torch.nan* operators triaged module: numpy module: NaNs and Infs module: reductions tracker,triaged module: numpy module: NaNs and Infs module: reductions tracker
torch.nn.utils.weight_norm fails with DDP high priority oncall: distributed triaged module: ddp,high priority oncall: distributed triaged module: ddp
Version is set to 1.8.0 in the torch tag v1.8.1 module: build oncall: releng triaged,module: build oncall: releng triaged
Allow `ScriptModule`s to be symbolically traced triaged FX-TorchScript Compatibility module: fx,triaged FX-TorchScript Compatibility module: fx
using DistributedSampler   occur RuntimeError: Expected a 'cuda:0' generator device but found 'cpu' oncall: distributed module: dataloader triaged,oncall: distributed module: dataloader triaged
"Reduce with any(), all(), median() over multiple dimensions triaged enhancement module: numpy module: reductions",triaged enhancement module: numpy module: reductions
New module: Split log softmax with loss feature module: nn triaged,feature module: nn triaged
Reductions tracking issue triaged module: numpy module: reductions tracker,triaged module: numpy module: reductions tracker
"I do have the GPU, but the print(torch.cuda.is_available()) always shows False module: cuda triaged",module: cuda triaged
requires grad get lost during transform. module: autograd triaged module: deprecation module: tensor creation,module: autograd triaged module: deprecation module: tensor creation
Merge fork of upload-artifact-s3 to a single commit triaged better-engineering,triaged better-engineering
inerror: reference to â€˜DeviceTypeâ€™ is ambiguous needs reproduction module: build module: cpp triaged,needs reproduction module: build module: cpp triaged
The class weights implementation is incorrect module: nn module: loss triaged,module: nn module: loss triaged
torch.linspace tensor support triaged enhancement module: numpy,triaged enhancement module: numpy
Zero seed in DistributedSampler oncall: distributed triaged,oncall: distributed triaged
DataLoader with IterativeDataset throws an error when providing a BatchSampler module: dataloader triaged,module: dataloader triaged
Unknown type name '__torch__.torch.classes.metal.Conv2dOpContext' module: convolution module: macos oncall: mobile module: ios,module: convolution module: macos oncall: mobile module: ios
Incorrect trace with MKLDNN (adding scalar) oncall: jit module: mkldnn,oncall: jit module: mkldnn
libtorch_cpu.so is getting too large. oncall: releng triaged,oncall: releng triaged
libtorch_cpu.so: undefined symbol: _ZN3c1010ThreadPool3runESt8functionIFvvEE module: build module: abi triaged,module: build module: abi triaged
"torch.quantization.fx.fuser forget to pass ""additional_fuser_method_mapping"" to fuse method oncall: quantization low priority triaged module: fx",oncall: quantization low priority triaged module: fx
error: invalid conversion in vec256_bfloat16.h module: build triaged,module: build triaged
[typing] ModuleDict ctor has wrong signature module: typing triaged,module: typing triaged
Document torch.Size and its methods (like torch.Size.numel) module: docs triaged,module: docs triaged
optimize_for_inference does not handle MKLDNN for BatchNorm1d high priority oncall: jit triaged,high priority oncall: jit triaged
The syncbatchnorm can not  be employed in the MODEL that the gradients including in the loss function oncall: distributed module: autograd triaged module: derivatives module: norms and normalization,oncall: distributed module: autograd triaged module: derivatives module: norms and normalization
Unable to add a scalar to an MKLDNN tensor oncall: jit module: mkldnn module: intel,oncall: jit module: mkldnn module: intel
[feature request] Low pass filtering of FFT results triaged enhancement module: fft,triaged enhancement module: fft
[feature request] Do zero-padding in high-frequency modes in `ifft` triaged enhancement module: fft,triaged enhancement module: fft
Jitted function cannot be pickled oncall: jit good first issue OSS contribution wanted,oncall: jit good first issue OSS contribution wanted
possible SummaryWriter memory leak triaged module: tensorboard,triaged module: tensorboard
"Feature Request: torch.functional.interpolate to quietly ignore ""align_corners"" when mode is set to ""nearest"" module: nn triaged enhancement module: interpolation",module: nn triaged enhancement module: interpolation
 Increased memory usage with AMP  module: cuda triaged module: amp (automated mixed precision),module: cuda triaged module: amp (automated mixed precision)
`pad(mode='reflect')` beyond `input.size` module: nn triaged enhancement module: padding,module: nn triaged enhancement module: padding
rename(**{}) should throw no error triaged module: named tensor,triaged module: named tensor
Make copy_ use dispatcher module: bootcamp triaged,module: bootcamp triaged
flatten renames in place triaged module: named tensor,triaged module: named tensor
[custom op]RuntimeError: Error compiling objects for extension module: build module: cpp-extensions triaged,module: build module: cpp-extensions triaged
Incorrect exponential calculation on Jetson devices with float32 dtype module: cuda triaged module: arm module: jetson,module: cuda triaged module: arm module: jetson
Audit use of __ARM_NEON__ define oncall: quantization low priority triaged enhancement,oncall: quantization low priority triaged enhancement
[Docker] Incompatible torchvision for 1.9.0-cuda10.2-cudnn7-runtime tag. triaged module: vision module: docker,triaged module: vision module: docker
Merge seemethere/upload-artifact-s3 and driazati/upload-artifact-s3 triaged better-engineering,triaged better-engineering
Dispatcher doesn't handle ops with an empty list of tensors (e.g. `torch.cat()`) triaged module: dispatch,triaged module: dispatch
Automate bumping of `clang-tidy` docker image tag on CI module: ci triaged module: infra,module: ci triaged module: infra
[package] pattern usage can potentially create broken packages triaged oncall: package/deploy imported,triaged oncall: package/deploy imported
jit scripting for the parametrizations oncall: jit module: nn.utils.parametrize,oncall: jit module: nn.utils.parametrize
Naming convention for variants of operations that never short circuit feature triaged module: ux,feature triaged module: ux
Singular build setting CLANG_CXX_LANGUAGE_STANDARD has different values module: build triaged module: macos,module: build triaged module: macos
Incorrect dtype cast for binary ops w/ mixed ComplexFloat + Double operands triaged module: complex module: type promotion,triaged module: complex module: type promotion
[jit] About traced script module using in c++ oncall: jit,oncall: jit
jit.load error when changed Folder Name module: serialization triaged,module: serialization triaged
Is Python version of Docker image on DockerHub downgraded? triaged module: docker,triaged module: docker
error for using faster rcnn on c++ & GPU needs reproduction oncall: jit,needs reproduction oncall: jit
[rfc][local lint] Create lint runner as a setup.py target triaged better-engineering,triaged better-engineering
RuntimeError: derivative for im2col_backward is not implemented needs reproduction module: autograd module: nn triaged,needs reproduction module: autograd module: nn triaged
Scipy 1.7.0 may cause some test failures module: distributions module: tests triaged,module: distributions module: tests triaged
[elastic launcher] redirects/tee support for global rank triaged module: elastic,triaged module: elastic
F.max_pool1d docs are really bad module: docs module: nn triaged module: pooling,module: docs module: nn triaged module: pooling
Sparse CSR layout CPU backend tracking issue module: sparse triaged module: mkl tracker,module: sparse triaged module: mkl tracker
Sparse CSR layout GPU backend tracking issue module: sparse module: cuda triaged tracker,module: sparse module: cuda triaged tracker
Fail to build with gcc11 module: build triaged module: xnnpack,module: build triaged module: xnnpack
DDP fails if you have multiple forward passes and a single backwards pass with `find_unused_parameters=True` oncall: distributed module: ddp,oncall: distributed module: ddp
"Pytorch V1.1 runs OK, but Pytorch v1.5 to 1.9 run wrong needs reproduction module: autograd triaged module: regression",needs reproduction module: autograd triaged module: regression
complie error when i use python setup.py build on arm computer module: build triaged module: arm,module: build triaged module: arm
State of Torch Named Tensors triaged module: named tensor,triaged module: named tensor
Where OpInfo doesn't handle cases where one of the inputs is a scalar module: tests triaged module: sorting and selection,module: tests triaged module: sorting and selection
Bug in `pytorch/benchmarks/tensorexpr/pt_engine.py` triaged op-bench,triaged op-bench
AccumulateGrad short circuits before calling variable hooks when the incoming grad is None module: autograd triaged,module: autograd triaged
"_foreach_maximum returns ""incompatible type"" mypy error unexpectedly module: typing triaged",module: typing triaged
"Record file/line number when creating test data, and then report it in backtraces associated with this data feature module: tests triaged",feature module: tests triaged
bilinear interpolate is very slow under mixed precision training mode. module: performance module: nn module: cuda triaged module: amp (automated mixed precision) module: interpolation,module: performance module: nn module: cuda triaged module: amp (automated mixed precision) module: interpolation
Turn on -Winfinite-recursion in OSS CI module: build triaged better-engineering,module: build triaged better-engineering
pip install nightly torch torchaudio and torchvision together will install 0.3.0 version of torchvsion on Ubuntu high priority module: binaries triaged,high priority module: binaries triaged
Impossible to raise the limit on number of shared memory tensors high priority module: multiprocessing triaged,high priority module: multiprocessing triaged
Install PyTorch on Windows with Conda and other Torch packages with cpuonly fails module: binaries triaged,module: binaries triaged
Ensure warnings relate to user code with stacklevel triaged better-engineering module: deprecation,triaged better-engineering module: deprecation
torch.cuda.Event(blocking=True) doesn't work module: cuda triaged,module: cuda triaged
Enhanced torch.chunk and torch.split  good first issue triaged enhancement,good first issue triaged enhancement
ROCm miopenStatusInternalError /MIOpen/src/sqlite_db.cpp:109: open memvfs: unable to open database file needs reproduction module: rocm triaged,needs reproduction module: rocm triaged
PyTorch unfold could be faster module: performance triaged,module: performance triaged
[Mkldnn] has_bf16 check only works on Linux for tests module: tests triaged module: mkldnn,module: tests triaged module: mkldnn
Enable large file support module: build feature triaged,module: build feature triaged
Implement simple view methods for TensorAccessor/PackedTensorAccessor module: bootcamp triaged,module: bootcamp triaged
"Output of non-inplace, non-CompositeImplicitAutograd op has TensorImpl > 1 or StorageImpl use_count != 1 module: autograd triaged",module: autograd triaged
Need workaround to support multiprocess CUDA tensor sharing on Jetson Platforms module: multiprocessing feature triaged,module: multiprocessing feature triaged
test_old_models_bc is flaky high priority triage review oncall: jit module: flaky-tests,high priority triage review oncall: jit module: flaky-tests
What changes we need to make in metrics calculation and visualization part when we use Distributed Data Parallel for distributed training oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
C++ version 1.9.0 libtorch dynamic load fails -- GCC only needs reproduction module: binaries module: cpp module: abi triaged,needs reproduction module: binaries module: cpp module: abi triaged
USE_SYSTEM_SLEEF: undefined reference to symbol 'Sleef_expd4_u10' module: build triaged module: sleef,module: build triaged module: sleef
Two-element ModuleList results in error in inference_mode when jit'ed oncall: jit,oncall: jit
USE_SYSTEM_CPUINFO: internal symbol `clog_vlog_fatal' isn't defined module: build triaged,module: build triaged
Divergent code is needed to record usage streams on different TensorImpl types module: sparse module: cuda triaged module: vmap,module: sparse module: cuda triaged module: vmap
Optimize torch.einsum module: performance triaged module: linear algebra,module: performance triaged module: linear algebra
Add a NumPy-like `pad` function triaged module: numpy function request module: padding,triaged module: numpy function request module: padding
provide hashsum for downloads module: binaries triaged,module: binaries triaged
Sparse CSR tensor should not accept equal column indices in the same row module: sparse module: error checking triaged,module: sparse module: error checking triaged
Subtensor operations like `tril/triu` to have an option to respect the strides of the input triaged module: memory format module: linear algebra,triaged module: memory format module: linear algebra
Mixed logical indexing / numerical indexing fails. triaged module: advanced indexing,triaged module: advanced indexing
Numerical-reproducibility issue in torch.matmul module: cuda triaged module: numerical-reproducibility,module: cuda triaged module: numerical-reproducibility
cannot build with tensorrt module: build caffe2 module: cuda triaged,module: build caffe2 module: cuda triaged
[testing] SkipInfo should error if `cls_name` and `test_name` combination is not valid module: tests triaged needs design,module: tests triaged needs design
Quantized model using `boolean_dispatch` not picklable oncall: quantization low priority triaged,oncall: quantization low priority triaged
Improvement to CUDA mem leak check module: ci module: tests triaged,module: ci module: tests triaged
[Android] Upgrading to 1.9.0 causes NNAPI model loading to fail module: android oncall: mobile,module: android oncall: mobile
torch.load non backwards compatible on Transformer between 1.8.1 and 1.9.0 module: serialization triaged,module: serialization triaged
Training goes wrong with amp and no_grad since pytorch 1.8 module: cuda triaged module: amp (automated mixed precision),module: cuda triaged module: amp (automated mixed precision)
Some ops throw runtime errors when called with complex tensors that require_grad but work when requires_grad=False module: autograd triaged module: complex complex_autograd,module: autograd triaged module: complex complex_autograd
PEP-585 type annotations not supported for class-level annotations oncall: jit triaged,oncall: jit triaged
backward compatibility - need a way to find out when a certain API was added/modified/etc. module: docs module: internals triaged module: doc infra better-engineering module: deprecation module: codegen,module: docs module: internals triaged module: doc infra better-engineering module: deprecation module: codegen
[torch.profiler] enhancements + corrections triaged enhancement oncall: profiler,triaged enhancement oncall: profiler
"torch.jit.save gives error - RuntimeError: Could not export Python function call 'NMSop'. Remove calls to Python functions before export. Did you forget to add @script or @script_method annotation? If this is a nn.ModuleList, add it to __constants__ oncall: jit",oncall: jit
Allow packaging of classes defined in a notebook triaged,triaged
Tutorials that require `pip install` not loading correctly in Colab module: docs triaged,module: docs triaged
bundled libiomp5 causing segfaults in other libraries that use libomp needs reproduction module: binaries triaged module: macos module: openmp,needs reproduction module: binaries triaged module: macos module: openmp
`memory_format` flag for `tensor.new_empty()`.  triaged enhancement module: memory format,triaged enhancement module: memory format
Profiler does not contain NCCL kernel high priority triaged module: regression oncall: profiler,high priority triaged module: regression oncall: profiler
[Bug] numpy is no longer a required dependency high priority triaged module: numpy,high priority triaged module: numpy
Symbolic trace with *args triaged module: fx,triaged module: fx
Replace native NHWC BN kernel triaged module: memory format,triaged module: memory format
RReLU and PReLU do not propagate input strides to outputs module: nn triaged module: memory format,module: nn triaged module: memory format
Dynamic quantized LSTM does not support output projection now. oncall: quantization low priority triaged,oncall: quantization low priority triaged
Can't perform any operation on Vulkan device - macOS M1 triaged module: macos module: arm module: vulkan,triaged module: macos module: arm module: vulkan
Error /usr/local/lib/libopenblas.so: error adding symbols: File in wrong format while building pytorch for ppc64le module: build triaged module: POWER,module: build triaged module: POWER
Implicit module registration permits silent programming errors in torch.nn.Module module: nn triaged,module: nn triaged
Lazy modules should accept keyword arguments to forward method module: nn triaged actionable,module: nn triaged actionable
[JIT] batch operators in training/inference  oncall: jit,oncall: jit
Autograd engine current graph_task should be in the TLSState module: autograd triaged,module: autograd triaged
"matmul causing cuDNN error: CUDNN_STATUS_INTERNAL_ERROR at future, unrelated Conv2d needs reproduction module: cudnn module: cuda triaged",needs reproduction module: cudnn module: cuda triaged
[ROCm] test_gather_stress_cuda is flaky module: rocm triaged module: flaky-tests,module: rocm triaged module: flaky-tests
Python scalars should be promoted to the same `dtype` as the respective tensor triaged module: type promotion needs design module: python array api,triaged module: type promotion needs design module: python array api
Sum issues with FP32 module: numerical-stability module: cpu triaged module: multithreading module: reductions,module: numerical-stability module: cpu triaged module: multithreading module: reductions
pytorch test failed module: autograd module: cuda module: tests triaged,module: autograd module: cuda module: tests triaged
Misreported linting messages module: lint triaged,module: lint triaged
TransformedDistribution's log_prob gradient is inconsistent w.r.t. cache_size module: distributions triaged,module: distributions triaged
Remove support for multiple ellipses in slicing triaged module: advanced indexing module: deprecation needs design module: python array api,triaged module: advanced indexing module: deprecation needs design module: python array api
Support negative step sizes for slicing triaged module: python array api,triaged module: python array api
[Feature Request] Deterministic implementation for AdaptiveMaxpool1d. module: nn triaged enhancement module: determinism module: pooling,module: nn triaged enhancement module: determinism module: pooling
CUDA Memory Error: PyTorch doen't allocate memory even though it's available module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
[c10d] Work objects should have a general operator<<  high priority oncall: distributed triaged better-engineering pt_distributed_rampup module: c10d,high priority oncall: distributed triaged better-engineering pt_distributed_rampup module: c10d
m.fallback(torch::CppFunction::makeFromBoxedFunction<&my_fallback>) gives bad error message module: internals module: cpp triaged module: assert failure,module: internals module: cpp triaged module: assert failure
Quantized conv2d with dilation and groups much slower than float32 module: performance oncall: quantization low priority module: convolution triaged,module: performance oncall: quantization low priority module: convolution triaged
Segmentation fault when using CUDA with RNN high priority needs reproduction module: crash module: rnn module: cuda triaged,high priority needs reproduction module: crash module: rnn module: cuda triaged
"cuda streams run sequentially, expected to run parallel module: performance module: cuda triaged",module: performance module: cuda triaged
Add a lint rule to recommend TORCH_CHECK(false) over throw std::runtime_error triaged better-engineering,triaged better-engineering
Tensor.type() does not work with meta tensors module: crash triaged module: meta tensors,module: crash triaged module: meta tensors
Potentially misleading note in documentation for PackedSequence module: docs module: nn module: rnn triaged module: nestedtensor,module: docs module: nn module: rnn triaged module: nestedtensor
what():  result type Float can't be cast to the desired output type Long needs reproduction module: cpp triaged,needs reproduction module: cpp triaged
multi scripted model on the same gpu on multi gpu machine needs reproduction module: multi-gpu triaged,needs reproduction module: multi-gpu triaged
torch.split() infer -1 entry from the other split sizes triaged enhancement module: viewing and reshaping,triaged enhancement module: viewing and reshaping
Build failed with TBB module: build triaged module: tbb,module: build triaged module: tbb
[clang-tidy] check for iteration over unordered data structures triaged better-engineering,triaged better-engineering
[discussion] Make docs consistent for torch.nn Modules module: docs module: nn triaged,module: docs module: nn triaged
"error when trying to call ""torch::jit::load""method, use Metal backend in PyTorch Mobile oncall: mobile module: ios",oncall: mobile module: ios
Multiprocessing model evaluation gets stuck module: multiprocessing triaged,module: multiprocessing triaged
max-sum operation triaged module: linear algebra module: reductions,triaged module: linear algebra module: reductions
Build Pytorch with Aten? module: build triaged,module: build triaged
Failed to enable libuv module: build triaged,module: build triaged
AttributeError: module 'torch.jit' has no attribute '_script_if_tracing' oncall: jit,oncall: jit
Documentation for torch.finfo doesn't match implementation module: docs triaged module: numpy,module: docs triaged module: numpy
Conv1d with large batch size and half precision in cuda returns incorrect result module: cuda triaged module: correctness (silent),module: cuda triaged module: correctness (silent)
ctc loss bug module: autograd module: loss triaged,module: autograd module: loss triaged
Error compiling PyTorch: `/usr/bin/ld: cannot find -lmagma /usr/bin/ld: cannot find -lnvToolsExt` module: build module: cuda triaged,module: build module: cuda triaged
NaN values on torch.nn.functional.conv2d (aarch64) module: nn module: convolution triaged module: arm module: jetson,module: nn module: convolution triaged module: arm module: jetson
" DataLoader worker (pid(s) 18056, 20540, 4512) exited unexpectedly module: dataloader triaged",module: dataloader triaged
pytorch/manylinux-cuda102 support for aarch64 module: build triaged module: docker module: arm,module: build triaged module: docker module: arm
Test timeout and assertion failure in distributed/rpc/test_tensorpipe_agent oncall: distributed module: flaky-tests module: rpc,oncall: distributed module: flaky-tests module: rpc
Libtorch segfault in packed GRU evaluation with cuda batch_sizes module: cpp module: nn module: rnn triaged,module: cpp module: nn module: rnn triaged
pytorch build failure on arm64 module: build module: cuda triaged module: arm module: jetson,module: build module: cuda triaged module: arm module: jetson
CUDA memory error when using torch.device('cpu')  module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
Ubuntu 20.04 GeForce RTX 3070  / NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 /  GPU Visibility Issue module: binaries module: cuda triaged,module: binaries module: cuda triaged
"Possibly alignment issues on NEON vectorized ops, Jetson platforms triaged module: arm module: jetson",triaged module: arm module: jetson
Optimizer step stuck during distributed training oncall: distributed module: data parallel module: amp (automated mixed precision),oncall: distributed module: data parallel module: amp (automated mixed precision)
ProcessGroupGloo creation crashes when world_size > 150 oncall: distributed triaged module: data parallel module: c10d,oncall: distributed triaged module: data parallel module: c10d
Ahead of time C++ Extensions should have an option to build with verbose=False module: cpp-extensions triaged,module: cpp-extensions triaged
Support complex numbers in `at::nan_to_num`. triaged enhancement module: numpy,triaged enhancement module: numpy
Can't install pytorch =1.7 with python 3.8.5 on Raspberry Pi  triaged module: arm,triaged module: arm
Multiplication of `torch.tensor` with `np.array` does the operation with numpy. high priority triaged module: numpy,high priority triaged module: numpy
Clarify BLAS configuration option module: build module: docs triaged module: linear algebra,module: build module: docs triaged module: linear algebra
torch.zeros memory leak : cpu high priority module: memory usage triaged,high priority module: memory usage triaged
Add numerically-stable function for angle between vectors module: numerical-stability triaged enhancement module: distance functions,module: numerical-stability triaged enhancement module: distance functions
Add supports_nnc metadata to OpInfos module: tests triaged NNC,module: tests triaged NNC
[jit] Error: Results of original model and exported/imported version of model differed  oncall: jit,oncall: jit
[CUDA] Add channels_last_3d support for commonly used modules module: cuda triaged module: memory format,module: cuda triaged module: memory format
JIT compiling a ParameterList raises an unnecessary warning oncall: jit,oncall: jit
No index type check of torch.tensor in array manner module: error checking triaged module: advanced indexing,module: error checking triaged module: advanced indexing
[autograd] `fill_` gradgradcheck raises RuntimeError module: autograd triaged,module: autograd triaged
[JIT] OpInfo / Autodiff tests dont seem to actually invoke symbolic backwards  oncall: jit,oncall: jit
Data access pattern in the loop in add_out_dense_sparse_csr_cuda could be pretty bad module: sparse triaged,module: sparse triaged
Use of storage_offset is not needed  in add_out_dense_sparse_csr_cuda module: sparse module: cuda triaged,module: sparse module: cuda triaged
CSR: Relaxing constraints to s_addmm_out_sparse_dense_cuda_worker module: sparse triaged open source,module: sparse triaged open source
`test_lstm` in `quantization.bc.test_backward_compatibility.TestSerialization` fails on Intel Cascade Lake machines oncall: quantization low priority triaged,oncall: quantization low priority triaged
c++ use pybind11 to import torch  free(): invalid pointer module: cpp triaged module: pybind,module: cpp triaged module: pybind
test_bottleneck_cuda fails without error message module: cuda module: tests triaged,module: cuda module: tests triaged
Issue: support auto generation of device check for sparse tensors module: sparse triaged module: codegen,module: sparse triaged module: codegen
[discussion] Should Optimizers be also Modules? module: nn module: optimizer triaged,module: nn module: optimizer triaged
libtorch conflict with cxxopts and cause memery leak module: memory usage triaged,module: memory usage triaged
gradient layout contract triaged module: memory format,triaged module: memory format
Improper use of quantization API for MHA should fail fast oncall: quantization triaged,oncall: quantization triaged
Operating on a shared memory tensor with multiple threads hangs oncall: distributed module: multiprocessing triaged,oncall: distributed module: multiprocessing triaged
Use PYTHONHASHSEED during pytorch build to avoid nondeterminism module: build triaged enhancement module: determinism module: codegen,module: build triaged enhancement module: determinism module: codegen
[local lint] Shellcheck quicklint file filter is incorrect triaged better-engineering,triaged better-engineering
"JIT fails when calling an exported method inside another one, if kwargs with defaults are not passed oncall: jit",oncall: jit
[deploy] Enable `torch.distributed.rpc` Python bindings in deploy oncall: distributed triaged module: rpc module: deploy,oncall: distributed triaged module: rpc module: deploy
[cuDNN v8] Improve cuDNN convolution v8 API error reporting module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
[cuDNN v8] Extend current cuDNN convolution v8 API binding to support conv-bias-activation fusion module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
[cuDNN v8] Extend current cuDNN convolution v8 API binding to support cuDNN benchmark module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
[cuDNN v8] Extend current cuDNN v8 API binding to support convolution backward and transposed convolution forward module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
Call non class methods from `torch::jit::Module` in C++ oncall: jit,oncall: jit
NCCL multi-gpu test intermittently failing after NCCL version upgrade high priority triage review oncall: distributed triaged module: flaky-tests module: nccl,high priority triage review oncall: distributed triaged module: flaky-tests module: nccl
Dataloader Rerunning with num_workers=0 may give better error trace oncall: distributed module: dataloader triaged module: ddp,oncall: distributed module: dataloader triaged module: ddp
Simplification of pruned models feature module: nn triaged,feature module: nn triaged
[bug] torch.topk sometimes supports `float16` and sometimes doesn't triaged enhancement module: half module: sorting and selection,triaged enhancement module: half module: sorting and selection
Foreach Functions Tracking Issue module: performance triaged tracker module: mta,module: performance triaged tracker module: mta
[Feature Pitch] Fast extremal eigensolvers module: sparse feature triaged module: linear algebra,module: sparse feature triaged module: linear algebra
[docs] Tensor.bernoulli_ formatting is hard to read and UX inconsistent with the function variant triaged module: ux,triaged module: ux
TensorIterator for sparse layouts module: sparse feature triaged,module: sparse feature triaged
Single-Process Multi-GPU is not the recommended mode for DDP oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
"Transferring tensor to the gpu and converting dtype in a single call to .to() is slower than first transferring, then converting. module: performance module: cuda triaged",module: performance module: cuda triaged
A Null pointer dereference bug oncall: jit,oncall: jit
Type annotations for torch.jit.* decorators oncall: jit,oncall: jit
C++ Extensions that use ninja should enable colors by default module: cpp-extensions triaged enhancement,module: cpp-extensions triaged enhancement
MKL csr matmul issue module: sparse triaged module: mkl,module: sparse triaged module: mkl
[codegen] generated inplace/out= wrappers don't have input checks module: error checking triaged module: codegen,module: error checking triaged module: codegen
precision/consistency issue in `linspace` triaged module: numpy module: numerical-reproducibility module: tensor creation,triaged module: numpy module: numerical-reproducibility module: tensor creation
"`torch.(min|max)(..., dim=...)` diverges from array API specification triaged module: deprecation module: reductions module: python array api",triaged module: deprecation module: reductions module: python array api
Python Array API Compatibility Tracker high priority triaged module: python array api tracker,high priority triaged module: python array api tracker
Python Array API New Operators Tracker triaged module: python array api tracker,triaged module: python array api tracker
`torch.size()` diverges from array API specification triaged module: deprecation module: python array api,triaged module: deprecation module: python array api
type promotion with 0d-tensors diverges from array API specification triaged module: type promotion module: python array api,triaged module: type promotion module: python array api
CSR construction: safe_get_attr_string suppresses real errors module: sparse triaged open source,module: sparse triaged open source
`pickle_save`/`pickle_load` are undocumented oncall: jit triaged,oncall: jit triaged
Incorrect channel in installation command for stable (1.8.1)+windows+conda+python+cuda 11.1 oncall: releng triaged has workaround,oncall: releng triaged has workaround
Werror should include -Wmissing-prototypes module: build triaged module: build warnings,module: build triaged module: build warnings
torch.load with dill is unable to unserialize from buffer module: serialization triaged,module: serialization triaged
Ability to enabling/disabling cuDNN and cuBLAS API logging in PyTorch API directly module: cudnn feature module: cuda triaged module: cublas,module: cudnn feature module: cuda triaged module: cublas
"Improvement suggestions for the error message ""Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false"" module: error checking triaged module: linear algebra",module: error checking triaged module: linear algebra
[JIT] Concat QKV Linear Layers On Frozen Models oncall: jit,oncall: jit
GPU 0 context created on GPU 1 worker when using pin_memory=True module: dataloader module: cuda triaged,module: dataloader module: cuda triaged
[rfc] Bug developers about release notes for new APIs triaged better-engineering,triaged better-engineering
Some cuda objects take very long time to build module: build module: cuda triaged enhancement,module: build module: cuda triaged enhancement
"link error on BatchNormImplBase<D, Derived>::pretty_print  when using icpc  triaged module: undefined reference",triaged module: undefined reference
Calling benchmark.Timer with default `num_threads=1` disables parallelism permanently module: docs triaged module: macos module: multithreading,module: docs triaged module: macos module: multithreading
ONNX exported EmbeddingBag fails for not strictly increasing offset. module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
fx: unable to symbolically trace model with torch.full(Proxy) triaged module: fx,triaged module: fx
Pixel shuffle support for PyTorch Mobile NN-API triaged enhancement module: android oncall: mobile,triaged enhancement module: android oncall: mobile
Migrate wheels to manylinux2010 oncall: releng triaged,oncall: releng triaged
DDP grads dont have parity with local training when grads are undefined oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
Unhelpful error message when type annotation is violated. oncall: jit,oncall: jit
OpInfo JIT tests do not work with Tensor kwarg arguments oncall: jit module: tests better-engineering,oncall: jit module: tests better-engineering
Inter-category type promotion has undocumented behavior module: docs triaged module: type promotion,module: docs triaged module: type promotion
Make SavedVariables aware of pyobjects module: autograd triaged,module: autograd triaged
nn.Upsample result mismatch in 1.1.0a0+828a6a3 and 1.9.0 module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
Why the same training code running on different GPUs (V100 vs P100) brings a significiant difference in outputs and thus train loss / model accuracy? triaged module: determinism,triaged module: determinism
`set_per_process_memory_fraction()` does not ensure max used GPU memory below fraction module: cuda triaged,module: cuda triaged
Data race in RecordFunction::callbackShouldRun triaged oncall: profiler,triaged oncall: profiler
TSAN issues in at::RecordFunction triaged oncall: profiler,triaged oncall: profiler
TSAN issues in autograd module: autograd triaged,module: autograd triaged
TSAN issues in libkineto triaged oncall: profiler,triaged oncall: profiler
cuDNN v8 API tracking issue high priority module: cudnn triaged tracker,high priority module: cudnn triaged tracker
fx OpInfo tests don't work with nn.functional test names triaged module: fx,triaged module: fx
"In Android, the memory used by the tensor or model cannot be recycled. Is there any way to solve it? triaged module: android oncall: mobile",triaged module: android oncall: mobile
torch.cuda.amp.autocast nan values when model.eval() needs reproduction triaged module: amp (automated mixed precision),needs reproduction triaged module: amp (automated mixed precision)
Better shared memory allocation under Docker module: memory usage triaged enhancement module: docker,module: memory usage triaged enhancement module: docker
"A ""sanitizer"" mode for the CUDA caching allocator? feature module: cuda triaged oncall: profiler",feature module: cuda triaged oncall: profiler
Clarity of error message in einsum regressed in performance improvements module: error checking triaged module: linear algebra,module: error checking triaged module: linear algebra
Tests do not pass even though build successful oncall: distributed triaged,oncall: distributed triaged
Add better python operators annotations for IDE type checking high priority feature module: typing triaged,high priority feature module: typing triaged
Broadcasting multiple tensors to all procs in distributed training oncall: distributed triaged,oncall: distributed triaged
Pytorch build issues on PowerPC module: build triaged module: POWER oncall: profiler,module: build triaged module: POWER oncall: profiler
torchdeploy doesn't prevent Obj from being used on wrong interpreter triaged module: deploy,triaged module: deploy
"""_amp_foreach_non_finite_check_and_unscale_cuda"" not implemented for 'ComplexFloat' module: cuda triaged module: complex needs design module: amp (automated mixed precision)",module: cuda triaged module: complex needs design module: amp (automated mixed precision)
"After the Adam optimizer used weight_decay, the model became extremely slow when tested on the CPU.(Time from 7 seconds to 46 seconds) module: performance module: optimizer triaged",module: performance module: optimizer triaged
"Torch.nn.DataParallel training model, the output of the model becomes list type, the number of lists is batch size triaged module: data parallel",triaged module: data parallel
`torch.autograd.Function` subclasses *sometimes* throw away custom subclasses module: autograd triaged actionable,module: autograd triaged actionable
[FX] record_function tracing in FX triaged module: fx,triaged module: fx
torch.split_with_sizes is not documented module: docs triaged,module: docs triaged
Mobile Android: Could not run 'aten::quantize_per_tensor' with arguments from the 'Vulkan' backend triaged module: android oncall: mobile,triaged module: android oncall: mobile
"Error while converting model for NNAPI ""Unsupported node kind ('quantized::batch_norm2d')"" triaged module: android oncall: mobile",triaged module: android oncall: mobile
"""uncorrectable NVLink error"" making the tests fail oncall: distributed module: cuda triaged module: nccl",oncall: distributed module: cuda triaged module: nccl
scatter_add_ 6000-times slower with int64 compared to int32 module: performance module: docs module: cuda triaged module: scatter & gather ops,module: performance module: docs module: cuda triaged module: scatter & gather ops
Segmentation fault need help needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
the same input with different batchsize got different precision output module: nn triaged module: numerical-reproducibility module: batching,module: nn triaged module: numerical-reproducibility module: batching
Save checkpoint error module: optimizer module: serialization triaged,module: optimizer module: serialization triaged
Python API binding code generation does not need to pack TensorOptions for `xxx_like` generators triaged module: codegen,triaged module: codegen
Generate special clear error messages for known common misuses in TorchScript module: bootcamp good first issue triaged OSS contribution wanted,module: bootcamp good first issue triaged OSS contribution wanted
Static graph training fails if forward is called multiple times before backward high priority triage review oncall: distributed triaged module: ddp,high priority triage review oncall: distributed triaged module: ddp
"TrilinearBackward takes 98.4% of total computational time, is this to be expected? module: performance module: nn triaged",module: performance module: nn triaged
TensorList upfront checks parameter types in argument parsing and throws an error unexpectedly triaged module: codegen,triaged module: codegen
PyTorch master failed to build with glog 0.5.0. module: build triaged,module: build triaged
quantize_per_tensor not symbolically traceable with FX if scale+zp are proxied triaged module: fx,triaged module: fx
linalg.eigh and linalg.cholesky UPLO flag breaks in backward module: autograd triaged module: linear algebra,module: autograd triaged module: linear algebra
[feature request] torch.as_tensor to support any object that NumPy's asarray or array can consume (consume __array_interface__) triaged module: numpy,triaged module: numpy
Add a `descending` flag to `linalg.eigh` and `linalg.svd`  module: docs feature good first issue triaged module: linear algebra,module: docs feature good first issue triaged module: linear algebra
Eliminate potential double device check triaged better-engineering,triaged better-engineering
Support a dist.group_like API oncall: distributed module: bootcamp triaged pt_distributed_rampup module: c10d,oncall: distributed module: bootcamp triaged pt_distributed_rampup module: c10d
torch.distributed.nn.all_reduce incorrectly scales the gradient high priority oncall: distributed triaged,high priority oncall: distributed triaged
`type: ignore` everything triaged better-engineering,triaged better-engineering
Use the same CUDA stream for all RPCs within the same dist autograd context oncall: distributed feature triaged module: rpc,oncall: distributed feature triaged module: rpc
Add a few tests to make sure new dispatch keys for backends are added properly. triaged module: dispatch,triaged module: dispatch
[structured] Clarify if NoNamesGuard is actually needed in impls or not triaged module: structured kernels,triaged module: structured kernels
torch.lerp to support argument type promotion / broadcasting similar to torch.where triaged module: type promotion module: interpolation,triaged module: type promotion module: interpolation
Backward pass for a nn.Conv2d with half-precision on Quadro RTX 8000 leads to CUDNN_STATUS_INTERNAL_ERROR module: cudnn triaged,module: cudnn triaged
Circular padding in Convolution layers should not only be wrap for once. module: nn module: convolution triaged enhancement module: padding,module: nn module: convolution triaged enhancement module: padding
[Mobile] Valid scale ratio with invalid error oncall: mobile,oncall: mobile
Torch.save doesn't make new file module: serialization triaged,module: serialization triaged
copy.deepcopy fails on spectral-norm layers after the first forward module: nn triaged,module: nn triaged
BatchNorm grad calculation is imprecise needs reproduction module: numerical-stability module: autograd triaged,needs reproduction module: numerical-stability module: autograd triaged
CUDA error: an illegal memory access was encountered needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
Add CI automation tests for Infiniband for DDP oncall: distributed triaged,oncall: distributed triaged
[rfc] Build a debug tool to help users find which parameters are unused during DDP training oncall: distributed triaged,oncall: distributed triaged
BFloat16 CUDA GEMM ops unsupported on Nvidia P100 (SM_60) on CUDA 11.3 module: cuda triaged module: cublas module: bfloat16,module: cuda triaged module: cublas module: bfloat16
[structured] Meta-impl split implies redundant dtype tests triaged better-engineering,triaged better-engineering
Unknown builtin op: torchvision::nms in LibTorch oncall: jit module: serialization module: vision,oncall: jit module: serialization module: vision
Unify `std::getenv` usages triaged better-engineering,triaged better-engineering
torch.cdist returns high diagonal values with CUDA module: numerical-stability triaged module: tf32,module: numerical-stability triaged module: tf32
"get_future() should be documented once it is enabled for gloo, mpi backends oncall: distributed triaged",oncall: distributed triaged
Pruners' compute_mask returns tensors with dtypes that are not consistent with each other module: nn triaged module: pruning,module: nn triaged module: pruning
complex128 autograd failures on PPC triaged module: complex module: POWER complex_autograd,triaged module: complex module: POWER complex_autograd
[perf] 10x improvement on element-wise operations with manual broadcast module: performance triaged module: TensorIterator,module: performance triaged module: TensorIterator
[perf] 10x improvement when doing `x.sum(-1)` manually module: performance triaged module: reductions,module: performance triaged module: reductions
[RFC] Provide an API for Structural Performance Tips in DDP oncall: distributed feature triaged module: ddp,oncall: distributed feature triaged module: ddp
`jit.trace` fails to capture single-element tensor passed into op requiring scalar-type argument oncall: jit,oncall: jit
Delete max_pool2d_with_indices_backward.grad_input? module: autograd triaged module: deprecation,module: autograd triaged module: deprecation
Add distributed testing for CUDA aware MPI oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
`torch.view_as_complex()` does not work when `storage_offset` is odd triaged module: complex,triaged module: complex
test_grid_sample (from TestNN) fails on POWER module: tests triaged module: POWER,module: tests triaged module: POWER
Cleanup usage of IS_PYTHON_3_9_PLUS in autograd engine module: autograd triaged module: pybind,module: autograd triaged module: pybind
[new feature] Adaptive gradient clipping module: autograd module: nn triaged,module: autograd module: nn triaged
[structured] Fail if CompositeExplicitAutograd is in dispatch table of structured_delegate function module: autograd triaged,module: autograd triaged
Please remove this assertion - it triggers on valid use cases triaged,triaged
setting  all seed still get different result.  needs reproduction triaged module: random module: determinism,needs reproduction triaged module: random module: determinism
Profiler creates sequence numbers that are off by 1 between the forward and backwards pass triaged oncall: profiler,triaged oncall: profiler
CUDA RPC stream synchronization does not work with @rpc.functions.async_execution oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
`torch._dirichlet_grad` returns `nan` value on cuda device module: autograd module: cuda triaged module: NaNs and Infs,module: autograd module: cuda triaged module: NaNs and Infs
"I have just wasted 3 solid days trying to build PyTorch, without success module: build triaged",module: build triaged
internal compiler error compiling PyTorch master on Fedora 33 module: build triaged shadow review,module: build triaged shadow review
Poor torch.cat performance in the quantized Unet oncall: quantization low priority triaged,oncall: quantization low priority triaged
torch 1.8.1+cuda crashes when setting breakpoints. triaged,triaged
nn.Module custom setattr leads very confusing behaviors module: nn triaged enhancement,module: nn triaged enhancement
[lint] Make sure that all CMake options have a corresponding output in the summary triaged better-engineering,triaged better-engineering
Simplified API for custom inplace & view kernel  triaged enhancement module: custom-operators module: dispatch,triaged enhancement module: custom-operators module: dispatch
Asserts all tensor are defined in dispatch wrapper triaged enhancement,triaged enhancement
gen_proto_typestubs_helper.py is slow module: build triaged,module: build triaged
Delay errors of inference tensor to backward pass? triaged enhancement module: dispatch inference mode,triaged enhancement module: dispatch inference mode
torch.return_types does not exist triaged enhancement,triaged enhancement
Missing complex autograd support for some operators triaged module: complex complex_autograd,triaged module: complex complex_autograd
Renaming Autograd dispatch keys to ADCreateGraph triaged enhancement module: dispatch,triaged enhancement module: dispatch
[jit] pickling custom classes may lead to invalid pickle files oncall: jit,oncall: jit
Avoid code repeat in create sample inputs for sort/msort module: tests triaged module: sorting and selection,module: tests triaged module: sorting and selection
Attributes consistency in Sampler and DistributedSampler oncall: distributed triaged,oncall: distributed triaged
Consistent treatment of non-differentiability in linear algebra operations module: docs triaged module: linear algebra,module: docs triaged module: linear algebra
Segmentation fault with ITIMER_REAL triaged module: macos,triaged module: macos
MaxPool2D Returns Wrong Shape With Ceil_Mode high priority module: docs triaged module: pooling,high priority module: docs triaged module: pooling
Segmentation fault when using add_graph from tensorboard in combination with try except in forward pass  oncall: visualization,oncall: visualization
Increasing RTT when using rpc_async in a PS architecture setup oncall: distributed triaged,oncall: distributed triaged
Using register_full_backward_hook with target module for intermediate activation module: docs module: autograd module: nn triaged,module: docs module: autograd module: nn triaged
Intermittent segmentation fault when using rpc_async/rpc_sync with CUDA tensors oncall: distributed triaged,oncall: distributed triaged
test_eig_with_eigvec_cuda_float64 is flaky on ROCm module: rocm triaged module: flaky-tests,module: rocm triaged module: flaky-tests
Allow `ScriptFunction`s to be symbolically traced triaged FX-TorchScript Compatibility module: fx,triaged FX-TorchScript Compatibility module: fx
LibTorch ships with two identical library files: libdnnl and libmlkdnn module: build triaged,module: build triaged
"einsum ""jk,ijkl->il"" is ~16x slower than numpy module: performance triaged module: numpy module: linear algebra",module: performance triaged module: numpy module: linear algebra
How to view VLOG information module: logging triaged,module: logging triaged
:attr: does not behave as expected. module: docs triaged module: linear algebra,module: docs triaged module: linear algebra
Compilation failure in caffe2/core/plan_executor_test.cc with GCC 5.4 module: build caffe2 triaged,module: build caffe2 triaged
[FR] unflatten support empty shape if unflattened dim has size 1 triaged module: viewing and reshaping,triaged module: viewing and reshaping
Static Linking Pytorch triaged module: static linking,triaged module: static linking
Get rid of weak_intrusive_ptr module: internals triaged,module: internals triaged
Attempting to concatenate scalar tensors throws a runtime error triaged module: numpy,triaged module: numpy
about torch install triaged,triaged
"apex-0.1-py3.6-linux-x86_64.egg/apex/amp/wrap.py"", line 28, in wrapper     return orig_fn(*new_args, **kwargs) RuntimeError: CUDA error: an illegal memory access was encountered module: cuda triaged module: amp (automated mixed precision)",module: cuda triaged module: amp (automated mixed precision)
Do padding of weight and activation tensors to match optimized backend implementation oncall: quantization low priority triaged,oncall: quantization low priority triaged
scatter does not accept scalar src= module: docs triaged module: numpy module: scatter & gather ops,module: docs triaged module: numpy module: scatter & gather ops
remove the dependence on node names in fx graph mode quant  triaged module: fx,triaged module: fx
torch-1.8.1 wheel verification fails with distlib needs reproduction module: binaries triaged,needs reproduction module: binaries triaged
CUDA 11 tracking issue module: binaries module: cuda triaged tracker,module: binaries module: cuda triaged tracker
torch.multiprocessing implement SyncManager module: multiprocessing triaged,module: multiprocessing triaged
Static prebuilt libraries contain shared libraries module: binaries triaged module: static linking,module: binaries triaged module: static linking
masked_select is x3 slower than reshaping and index_select module: performance triaged module: advanced indexing module: viewing and reshaping,module: performance triaged module: advanced indexing module: viewing and reshaping
Batched SVD_LOWRANK being much slower than loop implementation (both CPU and GPU)  module: performance triaged module: linear algebra,module: performance triaged module: linear algebra
[FR] integral discrete distributions should support dtype=int64 module: distributions triaged enhancement,module: distributions triaged enhancement
Non-symbolic FX tracer triaged module: fx,triaged module: fx
Comprehensively test NCCL's `get_future()` API oncall: distributed triaged better-engineering pt_distributed_rampup,oncall: distributed triaged better-engineering pt_distributed_rampup
Remove single device constraint from ProcessGroupNCCL profiling  triaged module: nccl better-engineering pt_distributed_rampup module: c10d,triaged module: nccl better-engineering pt_distributed_rampup module: c10d
Autograd engine worker thread initialization fails with python 3.9 debug build triaged,triaged
Implement random SeedSequence triaged module: random,triaged module: random
No tensor operations allowed inside at::parallel_for high priority triaged module: multithreading,high priority triaged module: multithreading
Named Tensors in C++ Is Undocumented module: cpp triaged module: named tensor,module: cpp triaged module: named tensor
can't find user's fuser method in quantization.fx.fusion_patterns.py when run fx graph qat. triaged module: fx,triaged module: fx
Error when building PyTorch from source in CentOS Linux 7 module: build triaged module: tensorpipe,module: build triaged module: tensorpipe
Support `expand_dims` triaged module: numpy function request module: python array api,triaged module: numpy function request module: python array api
The multi-fc losses calculating in DistributedDataParallel. oncall: distributed triaged,oncall: distributed triaged
Problems with initial communication between GPUs needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
Change make_reduction to reflect input resizing. triaged better-engineering module: reductions,triaged better-engineering module: reductions
Matrix multiplication broken on PyTorch 1.8.1 with CUDA 11.1 and Nvidia GTX 1080 Ti high priority triaged module: cublas,high priority triaged module: cublas
Hard fail build when there is no CUDA but `USE_CUDA=1` module: build triaged,module: build triaged
[TorchScript JIT] `dtype` propagation pass (meta-issue) triaged,triaged
[Pytorch Mobile] Error running build_pytorch_android.sh module: android oncall: mobile,module: android oncall: mobile
Convolution2D may have a numerical error. module: numerical-stability triaged,module: numerical-stability triaged
Avoid no-op suggest_memory_format call in SparseCsrTensorImpl::resize_as_sparse_csr_tensor_ module: sparse triaged open source better-engineering,module: sparse triaged open source better-engineering
PyTorch 1.8.0 / 1.8.1 CUDA memory corruption on oddly sized tesnor sections of strided tensor module: cuda triaged,module: cuda triaged
[structured] at::cpu_unchecked functions triaged module: structured kernels,triaged module: structured kernels
Torch FX does not work with torchvision model triaged module: fx,triaged module: fx
Segmentation fault on Pytorch 1.8.1 + cuda 11.1 on  GTX 1050 Ti  high priority needs reproduction module: crash module: cuda triaged,high priority needs reproduction module: crash module: cuda triaged
{h / v / d}split methods are missing triaged module: numpy,triaged module: numpy
consistency jit tests gives error for gradient operator oncall: jit,oncall: jit
[package] Module name reported in error message does not always match what is needed to extern/mock it triaged,triaged
new torch.profiler slow for large number of events? triaged oncall: profiler,triaged oncall: profiler
"Support multi-dim reductions for torch.prod, torch.all, torch.any triaged enhancement module: reductions",triaged enhancement module: reductions
"All thread stack trace dumps interleave output in distributed tests, giving garbled output oncall: distributed triaged",oncall: distributed triaged
CrossEntropyLoss target shape expectation is inconsistent with BCEWithLogits triaged module: shape checking,triaged module: shape checking
Can't deepcopy memory format objects triaged module: memory format,triaged module: memory format
[cpp op] TORCH_LIBRARY schema doesn't respect keyword only module: cpp-extensions module: cpp triaged,module: cpp-extensions module: cpp triaged
Marking variable not required in backward calculation if it is not needed feature module: autograd triaged actionable,feature module: autograd triaged actionable
[torchdeploy] ar: _bz2module.o: No such file or directory  triaged module: deploy,triaged module: deploy
 Sparse tensor CSR layout for CUDA module: sparse module: cuda triaged,module: sparse module: cuda triaged
Add compute_residuals flag for torch.linalg.lstsq triaged enhancement module: numpy module: linear algebra,triaged enhancement module: numpy module: linear algebra
unsafe_reclaim_from_nonowning is not that unsafe module: internals module: bootcamp triaged,module: internals module: bootcamp triaged
shared torch.tensor with multiprocesses using python Queue cause coredump  module: multiprocessing triaged shadow review,module: multiprocessing triaged shadow review
Provide a pkg-config file module: build feature triaged,module: build feature triaged
Scatter tests missing when passing a reduction operation. triaged module: scatter & gather ops,triaged module: scatter & gather ops
"I meet an error assert key in deserialized_objects when I torch.load(pthname),and pth file is trained on multi gpu oncall: distributed module: serialization triaged module: ddp",oncall: distributed module: serialization triaged module: ddp
Raise exception when torch.clamp min value underflows the input tensor's dtype module: error checking triaged,module: error checking triaged
Figure out what mojo FB common/process/StackTrace.h has that we don't module: internals triaged better-engineering,module: internals triaged better-engineering
Symbolicate crashes from releases triaged,triaged
Tests in CI are run from the test/ directory module: docs module: tests triaged,module: docs module: tests triaged
Distributed tests don't always check the exit code of worker processes oncall: distributed module: flaky-tests module: ddp,oncall: distributed module: flaky-tests module: ddp
"Acquiring ""is_grad_enabled()"" inside an autograd function  feature module: autograd triaged module: custom-operators actionable",feature module: autograd triaged module: custom-operators actionable
Support for stereo audio data in from torch.utils.tensorboard.SummaryWriter feature triaged module: tensorboard module: data,feature triaged module: tensorboard module: data
"Resolved: Only add type promotion support to unary pwise, binary pwise, and reduction operations triaged module: type promotion module: ux",triaged module: type promotion module: ux
logcumsumexp (and maybe other cum* ops) has divergent CUDA and CPU out= behavior module: cuda triaged module: safe resize,module: cuda triaged module: safe resize
log_softmax(x) != x - logsumexp(x) module: numerical-stability triaged,module: numerical-stability triaged
Unable to pip install PyTorch on M1 Mac - Errors module: build triaged module: macos module: arm,module: build triaged module: macos module: arm
Type checking doesn't match actual type in error message module: error checking triaged,module: error checking triaged
Change `other` to `src` in torch.Tensor.scatter_add_ module: docs triaged,module: docs triaged
Why does output_padding have constraints in ConvTranspose1d? module: docs triaged,module: docs triaged
test_variant_consistency_jit fails for torch.tensordot with dtype float32 with error INTERNAL ASSERT FAILED oncall: jit module: tests,oncall: jit module: tests
torch.where with input Tensor and other Scalar raises type mismatch error triaged module: type promotion,triaged module: type promotion
Audit destructors of classes bound using pybind11::class_ to see if they can block; such cases can deadlock triaged module: pybind,triaged module: pybind
qnnpack uses deprecated pthreadpool APIs triaged,triaged
cumprod gradgradcheck fails in fast_mode=True module: autograd module: tests triaged,module: autograd module: tests triaged
Reflect padding_mode should be supported for Conv3d module: docs module: convolution triaged,module: docs module: convolution triaged
torch.nn.quantized.functional.conv_transpose1d/2d/3d support module: docs oncall: quantization feature low priority triaged,module: docs oncall: quantization feature low priority triaged
Silent incorrect running with zero padding for Conv1d module: error checking module: convolution triaged module: padding,module: error checking module: convolution triaged module: padding
Cuda RPC error when using then() oncall: distributed module: tensorpipe,oncall: distributed module: tensorpipe
`test_cholesky_solve` gradgradcheck fails sometimes module: autograd module: tests triaged module: linear algebra module: magma,module: autograd module: tests triaged module: linear algebra module: magma
Half precision support for torch.sparse.mm feature triaged module: half,feature triaged module: half
Error message regarding Padding of Conv2d needs improving module: convolution triaged module: padding,module: convolution triaged module: padding
Tolerance for non-determinism operators in gradcheck module: autograd module: tests triaged module: determinism,module: autograd module: tests triaged module: determinism
[FX] Issues with names of functions in user packages module: bootcamp triaged module: fx,module: bootcamp triaged module: fx
AVX512 and Vec512 feature triaged module: vectorization,feature triaged module: vectorization
"scripting LSTM module generates a script that has no .code, no .graph, and cannot be executed.  oncall: jit triaged",oncall: jit triaged
torch.gradient  not throwing error when spacing to be equal to 0 module: autograd low priority triaged,module: autograd low priority triaged
"[rfc] Add a ""core only"" build flag module: build triaged enhancement better-engineering",module: build triaged enhancement better-engineering
libtorch static linking results in undefined references to onnx_torch and caffe2::EmbeddingLookup triaged module: undefined reference,triaged module: undefined reference
torch.profiler does not work out of the box on nightly triaged oncall: profiler,triaged oncall: profiler
AttributeError: module 'torch._C' has no attribute 'ComplexDoubleStorageBase' module: build triaged,module: build triaged
caffe2/utils/signal_handler.cc is failing to symbolize libtorch_cpu.so module: build triaged,module: build triaged
[package] implicit externing can miss cases where the stdlib depends on further stdlib modules triaged,triaged
Converting float16->bool causes an internal compiler error with llvm NNC,NNC
A more flexible torch.hub search strategy feature triaged module: hub,feature triaged module: hub
Optionally include padding_idx items in the EmbeddingBag reduction feature module: nn triaged module: embedding,feature module: nn triaged module: embedding
[Meta] PyTorch features build/test matrix module: build module: ci triaged,module: build module: ci triaged
Add Dirichlet Multinomial to PyTorch Distributions module: distributions triaged function request,module: distributions triaged function request
[Opinfo] Better ErrorMsg for test_out with wrong shape module: tests triaged,module: tests triaged
when building apk with libtorch needs reproduction module: build triaged,needs reproduction module: build triaged
Failure in complex CUDA numerics tests for sigmoid triaged module: complex,triaged module: complex
Segmentation fault when using torch.profiler high priority needs reproduction module: crash triaged oncall: profiler,high priority needs reproduction module: crash triaged oncall: profiler
[JIT][ProfilingExecutor] A reliable and consistent API/protocol to query & propagate profiling data oncall: jit,oncall: jit
"LazyEmbedding, an embedding layer with a dynamically sized vocabulary feature module: nn triaged module: embedding",feature module: nn triaged module: embedding
"Cannot init, destroy, and then re-init process groups oncall: distributed triaged",oncall: distributed triaged
Finish deprecating torch.range triaged module: deprecation,triaged module: deprecation
Deprecations tracking issue high priority triaged module: deprecation tracker,high priority triaged module: deprecation tracker
Compiler warnings tracking triaged module: build warnings,triaged module: build warnings
Update linspace and logspace to throw an error when steps is not provided triaged module: deprecation module: tensor creation,triaged module: deprecation module: tensor creation
Deprecate torch.stft returning real-valued tensors and torch.istft accepting real-valued inputs triaged module: deprecation module: fft,triaged module: deprecation module: fft
Quantizable LSTMCell does not work correctly. oncall: quantization low priority triaged,oncall: quantization low priority triaged
Sparse-sparse matrix multiplication only works with torch.sparse.mm() module: sparse triaged module: ux,module: sparse triaged module: ux
[Tracking] Remove unneeded BC duplicates from native_functions.yaml triaged better-engineering,triaged better-engineering
test_stream_event_nogil: Is the test making a wrong assumption? in progress module: rocm module: tests triaged actionable,in progress module: rocm module: tests triaged actionable
Support float_qparams in quantized_clone oncall: quantization low priority triaged,oncall: quantization low priority triaged
Allow variable intermediate hidden dimensions for stacked RNN/LSTM layers module: nn module: rnn triaged enhancement,module: nn module: rnn triaged enhancement
test_variant_consistency_eager_addbmm fails on both cpu and cuda module: tests triaged module: linear algebra,module: tests triaged module: linear algebra
Attempt to use jited `torch.isnan` hit internal assert oncall: jit module: crash module: ci,oncall: jit module: crash module: ci
Warn users when cross entropy is called after softmax module: nn module: loss triaged,module: nn module: loss triaged
[Bug] RuntimeError: could not create a primitive on Xeon triaged module: backend module: intel,triaged module: backend module: intel
Vulkan backend on desktop platforms triaged module: backend module: vulkan,triaged module: backend module: vulkan
[rfc] Trigger callback when backwards begins for DDP with custom autograd function oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
We should perhaps prevent linking against LibTorch DLLs with the wrong configuration (debug vs release) module: windows triaged,module: windows triaged
Don't define __assert_fail on systems using `musl-dev` headers module: build triaged,module: build triaged
Add torch.nn.Conv2D correctness test module: nn module: tests triaged enhancement,module: nn module: tests triaged enhancement
pybind11 Tensor type caster forces reference count bump triaged module: pybind,triaged module: pybind
torch.load is very slow with gzip.open module: serialization triaged has workaround,module: serialization triaged has workaround
clang format on OS X ssl verification failure module: lint triaged,module: lint triaged
Synchronize RRef.to_here() CUDA Streams properly when the profiler is enabled triaged module: rpc,triaged module: rpc
Cuda OOM after several steps for DataParallel model module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
"Some float16 inputs to CPU matmul are supported, others aren't triaged module: half module: linear algebra",triaged module: half module: linear algebra
"[question] Influence of divisibility of B, C, T by 16 on Conv1d (and Conv2d perf) with CuDNN, including presence of padding module: performance module: docs triaged",module: performance module: docs triaged
Fail to introduce torch::jit::Module as a parameter of a cusomized operator. oncall: jit,oncall: jit
[JIT] TorchScript to represent typed tensor annotations in IR graph oncall: jit,oncall: jit
RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)` module: cuda triaged,module: cuda triaged
symbolic tracing does not support patterns with multiple arguments? triaged module: fx,triaged module: fx
Segmentation fault when loss.backward() module: crash module: cuda triaged,module: crash module: cuda triaged
GraphModule.to_folder generates code with syntax error (imports in the middle of a class) [torch.fx] triaged module: fx,triaged module: fx
torch.nn.functional.log_softmax on Arm CPU gives partly NaN results needs reproduction triaged module: arm,needs reproduction triaged module: arm
"RuntimeError: ""max_cuda"" not implemented for 'ComplexFloat' triaged module: complex enhancement",triaged module: complex enhancement
Performance debugging / warning mode module: performance triaged enhancement,module: performance triaged enhancement
at::globalContext().hasCUDA()'s forked tongue tells lies module: multiprocessing module: cuda triaged,module: multiprocessing module: cuda triaged
torch.arctanh not implemented for torch.float16 triaged module: half,triaged module: half
ATen/native/sparse headers not in LibTorch or pypi package module: binaries triaged,module: binaries triaged
requires_grad does not get propagated properly when using the JIT compiler module: autograd triaged,module: autograd triaged
pytorch inference lead to memory leak in cpu module: memory usage triaged,module: memory usage triaged
[ONNX] Incorrect handling of tuple multiplication with zero module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
"[amp scaler] unable to prevent ""scheduler before optimizer step"" warning module: optimizer triaged module: amp (automated mixed precision)",module: optimizer triaged module: amp (automated mixed precision)
Improve test runtime of distributed tests. oncall: distributed module: tests triaged better-engineering,oncall: distributed module: tests triaged better-engineering
Source location range issue for dictionary  oncall: jit module: bootcamp good first issue OSS contribution wanted,oncall: jit module: bootcamp good first issue OSS contribution wanted
Update TCPStore.wait() error message to be more friendly oncall: distributed triaged,oncall: distributed triaged
Gradcheck failure for `torchaudio.functional.phase_vocoder` module: autograd triaged module: correctness (silent),module: autograd triaged module: correctness (silent)
`embedding_bag` has unexpected behavior when given `offsets` that are not monotonically increasing module: nn triaged module: embedding,module: nn triaged module: embedding
[FX] See if we can type-annotate attribute accesses on proxy triaged module: fx,triaged module: fx
Support quantized functional linear/conv operators with 32b output oncall: quantization feature low priority triaged,oncall: quantization feature low priority triaged
Cross-compiling for cortexa9 processor module: build triaged,module: build triaged
Include the cuda headers in the libtorch pre-built packages module: binaries module: build module: cuda triaged,module: binaries module: build module: cuda triaged
C++ sparse_coo_tensor ignores TensorOptions argument module: sparse triaged module: tensor creation,module: sparse triaged module: tensor creation
[DOC] Incorrect image for the confusion matrix module: docs module: rnn triaged,module: docs module: rnn triaged
"Errors occur when operating broadcast with error log: Got completion with error 12, opcode 1, len 32547, vendor err 129 oncall: distributed triaged",oncall: distributed triaged
More AVX2 vectorization support for half (float16) triaged module: vectorization module: half,triaged module: vectorization module: half
Have the ability to disable `__torch_function__` dispatch for torch.nn.functional functions module: nn triaged module: __torch_function__,module: nn triaged module: __torch_function__
Error building docker image: No module named 'typing_extensions' module: build triaged module: docker,module: build triaged module: docker
[FX][testing] Add symbolic tracing tests for torchaudio triaged module: fx,triaged module: fx
[FX][testing] Add symbolic tracing tests for torchtext triaged module: fx,triaged module: fx
[FX][testing] Test symbolic tracing of detectron2 triaged module: fx,triaged module: fx
[FX][testing] Run symbolic tracing tests over torch benchmark in CI triaged module: fx,triaged module: fx
[FX][testing] Test tracing into all the standard `torch.nn.functional` instances triaged module: fx,triaged module: fx
[FX][testing] Test tracing into all the standard `nn.Module` instances triaged module: fx,triaged module: fx
`DataParallel` (`broadcast_coalesced`) with complex tensors yield real views high priority oncall: distributed triaged module: complex module: data parallel,high priority oncall: distributed triaged module: complex module: data parallel
[RFC] Extend Autocast to CPU/CUDA with BF16 data type feature triaged module: bfloat16 module: amp (automated mixed precision),feature triaged module: bfloat16 module: amp (automated mixed precision)
bool_tensor.sum(dtype=torch.int32) creates int32-copy of the original int8 tensor  triaged module: boolean tensor module: reductions,triaged module: boolean tensor module: reductions
it is lack of compatibility of CUDA_HOME (or other env) module: cpp-extensions module: cuda triaged,module: cpp-extensions module: cuda triaged
torch.allclose does not allow different types for comparison triaged module: numpy module: type promotion,triaged module: numpy module: type promotion
simple matrix multiplication yields wrong result on Ampere (3080) triaged module: cublas module: correctness (silent) module: tf32,triaged module: cublas module: correctness (silent) module: tf32
[RFC] Plan to deduplicate test_c10d and distributed_test  oncall: distributed triaged better-engineering,oncall: distributed triaged better-engineering
Tests should be runnable without run_test.py triaged module: testing,triaged module: testing
CI not surfacing some failures (-Werror?) on PR module: ci triaged,module: ci triaged
Split test/cpp_extensions/setup.py per orthogonal extension module and/or do Python build from ninja too module: build module: cpp-extensions triaged,module: build module: cpp-extensions triaged
nn.DataParallel should raise error when provided with list of tensors triaged module: data parallel,triaged module: data parallel
SIGSEGV at at::is_vulkan_available() invocation on Android triaged module: android,triaged module: android
Build errors with USE_VULKAN=ON when cross-compiling for Android module: build triaged module: android,module: build triaged module: android
endpoint=False for torch.linspace and torch.logspace triaged enhancement module: numpy module: tensor creation,triaged enhancement module: numpy module: tensor creation
Poor error message when trying to jit a function instead of a module (RuntimeError: Cannot insert a Tensor that requires grad as a constant.) oncall: jit,oncall: jit
Small model occupies too much GPU in CUDA11.1 + Torch1.8.1 but is normal in Torch 1.6 + CUDA10.1 module: performance module: cuda module: memory usage triaged,module: performance module: cuda module: memory usage triaged
Improve CUDA extension building experience module: binaries module: cuda triaged enhancement,module: binaries module: cuda triaged enhancement
Batched multi_dot / chain_matmul + let it accept a tensor instead of tuple triaged enhancement module: linear algebra,triaged enhancement module: linear algebra
Cuda memory leak check is somehow unstable with repeat_test_for_types module: memory usage triaged module: testing,module: memory usage triaged module: testing
Profile Optionals and Optional[Tensor] specifically oncall: jit triaged,oncall: jit triaged
[RFC] Model Sharding for distributed training oncall: distributed triaged,oncall: distributed triaged
[FX] to_folder breaks with qualified type name module: bootcamp triaged module: fx,module: bootcamp triaged module: fx
"Consider adding meta:1, meta:2 devices triaged enhancement module: structured kernels module: meta tensors",triaged enhancement module: structured kernels module: meta tensors
"`torch.tensor(..., device='meta')` doesn't work module: bootcamp triaged",module: bootcamp triaged
Isolate CPU tests from GPU tests module: tests triaged actionable,module: tests triaged actionable
Internal assert failed: `iter.device(arg).is_cuda()` module: error checking triaged,module: error checking triaged
Better syntax for OpInfo module: tests triaged better-engineering,module: tests triaged better-engineering
multiprocessing function cannot pass cuda objects use when calling inside from a DDP process oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
Better handling of OpInfo.sample_inputs high priority module: tests triaged better-engineering needs design ezyang's list,high priority module: tests triaged better-engineering needs design ezyang's list
Parameterless model still has extra inputs when exported to ONNX + insufficient checking of argument export_params module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Add suport to torch.gather for negative indices triaged enhancement module: advanced indexing,triaged enhancement module: advanced indexing
LR scheduler step() behaviour with and without epoch parameter  module: optimizer triaged,module: optimizer triaged
Cusolver handle may decrease MAGMA performance on GPU module: performance module: cuda triaged module: linear algebra module: magma,module: performance module: cuda triaged module: linear algebra module: magma
Why isn't pip finding the correct pytorch dependency? module: binaries triaged,module: binaries triaged
Alternative backend fallback like mechanism which has higher precedence than operator-specific composite implementations module: internals triaged,module: internals triaged
Python dispatch key and backend fallback triaged module: __torch_function__,triaged module: __torch_function__
Make it possible to skip only one hop of __torch_function__ override triaged module: __torch_function__,triaged module: __torch_function__
[JIT][Testing] Split up test_jit.py and test_jit_py3.jit oncall: jit triaged,oncall: jit triaged
[FR] Safe softmax module: nn triaged enhancement,module: nn triaged enhancement
16 bit accumulation for Convolution module: convolution triaged enhancement,module: convolution triaged enhancement
High CPU using torch.stack/torch.cat on Windows module: performance module: windows triaged,module: performance module: windows triaged
DataLoader performance drop on 4-channel images? needs reproduction module: dataloader triaged,needs reproduction module: dataloader triaged
Add Minimal Gated Unit (MGU) module: nn triaged enhancement,module: nn triaged enhancement
[TensorExpr] Provide a safe API to request the N-th loop for a given Buf. module: bootcamp triaged NNC,module: bootcamp triaged NNC
Accept objects with `__float__` wherever regular `float`s are accepted feature triaged module: half,feature triaged module: half
test_quantize_fx.py test_resnet_18_dpp test failure oncall: quantization low priority triaged module: fx,oncall: quantization low priority triaged module: fx
CUBLAS_STATUS_EXECUTION_FAILED error on torch >= 1.8.0 and CUDA 11.1 needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
[1.8.x] Build from source with USE_VULKAN fails (requires C++20 instead of C++14) module: build triaged,module: build triaged
Results surprisingly differ on different backends triaged module: numerical-reproducibility,triaged module: numerical-reproducibility
Structured kernels have increased TensorIterator overhead. module: performance module: bootcamp triaged,module: performance module: bootcamp triaged
Optimizations to TORCH_CHECK change inlining behavior. module: performance module: bootcamp triaged,module: performance module: bootcamp triaged
Regression in Python arg parser performance. module: performance module: bootcamp triaged,module: performance module: bootcamp triaged
[tests] `cumprod` OpInfo tests take long time to run (around 1min) module: ci module: tests triaged,module: ci module: tests triaged
FX tracer doesn't support returning a new instance of `object` subclass from module forward triaged module: fx,triaged module: fx
Out variants for Convolution ops module: convolution triaged,module: convolution triaged
Allow CUDA build without requiring a physical GPU device. module: build module: cuda triaged enhancement,module: build module: cuda triaged enhancement
[fx] forward references in annotations do not produce correct source code module: bootcamp triaged module: fx,module: bootcamp triaged module: fx
Default generated meta functions for inplace operations don't report errors triaged,triaged
Often the documentation does not match the actual signature of the function module: docs triaged module: __torch_function__,module: docs triaged module: __torch_function__
Get a thread safe copy of torch::nn::Sequential object module: cpp module: nn triaged,module: cpp module: nn triaged
Not possible to save dataloader in C++ module: dataloader triaged enhancement,module: dataloader triaged enhancement
Occured error in loss.backward() when using sparse=True in Embedding layer module: sparse triaged module: embedding,module: sparse triaged module: embedding
Inconsistent behaviour with `weight` argument in CrossEntropyLoss and BCEWithLogitsLoss module: loss triaged,module: loss triaged
[testing] test_reference_numerics_extremal_clamp_cpu_bfloat16 fails on ci build with GCC5.4 and Python 3.6 module: tests triaged module: bfloat16,module: tests triaged module: bfloat16
c++ libtorch config mismatch with python version module: build triaged,module: build triaged
Import error needs reproduction triaged,needs reproduction triaged
Per channel weight observer for ConvTranspose oncall: quantization low priority triaged,oncall: quantization low priority triaged
TensorExpr `LoopNest.get_loops_for` misbehaved after loop distribution transformation module: cpp triaged,module: cpp triaged
[package] Detection of dependencies introduced by torch.ops.load_library  triaged,triaged
"Vertices=torch.matmul(vertices.unsqueeze(0), rotations_init), RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched in CentOS needs reproduction module: cuda triaged module: numpy",needs reproduction module: cuda triaged module: numpy
[FX] Cannot symbolically trace class method triaged module: fx,triaged module: fx
"RuntimeError: ""log2"" ""_vml_cpu"" not implemented for 'Half' triaged module: numpy module: half",triaged module: numpy module: half
torch.sqrt for negative values should either return complex tensors or clearly throw a domain error/warning triaged module: complex module: numpy,triaged module: complex module: numpy
[Feature request] Add batched matrix support for torch.diag triaged enhancement module: numpy module: batching module: linear algebra,triaged enhancement module: numpy module: batching module: linear algebra
JIT prim ops v.s. dispatcher oncall: jit module: dispatch oncall: mobile,oncall: jit module: dispatch oncall: mobile
Segmentation fault in PyTorch dataloader needs reproduction module: dataloader triaged,needs reproduction module: dataloader triaged
Test test_large_cumprod_cuda_float16 gets killed due to (probably) OOM  module: memory usage module: tests triaged,module: memory usage module: tests triaged
qsize not implemented error module: multiprocessing triaged module: macos,module: multiprocessing triaged module: macos
Optional enhanced logging for operator calls for backend implementor debugging triaged module: backend,triaged module: backend
torch.jit.trace() fails on a GCN with sparse inputs and dense layers oncall: jit,oncall: jit
torch-tensor-repr gdb macro doesn't seem to work in opt mode triaged,triaged
torch.pow returns incorrect value for 0^0j triaged module: complex module: numpy,triaged module: complex module: numpy
"torch.pow(tensor, tensor) throws RuntimeError for dtype bool  triaged module: numpy",triaged module: numpy
Dispatcher TLS to bypass loads of dispatch key from tensor arguments module: performance triaged enhancement module: dispatch,module: performance triaged enhancement module: dispatch
Exceptions thrown in JIT interpreter always are translated into RuntimeError oncall: jit,oncall: jit
if cannot be symbolically traced triaged module: fx,triaged module: fx
Various issues in derivatives.yaml module: autograd triaged actionable,module: autograd triaged actionable
Make searchsorted and bucketize API consistent module: docs triaged module: numpy module: deprecation module: ux,module: docs triaged module: numpy module: deprecation module: ux
cublasSgemmStridedBatched failure when calling grad of grad needs reproduction module: crash module: cuda triaged,needs reproduction module: crash module: cuda triaged
"Add OpInfo metadata for ""is_torch_functional"" and a skip for these ops in TestOperatorSignatures.test_get_torch_func_signature_exhaustive module: tests triaged module: fx",module: tests triaged module: fx
why single pytorch process is displayed on each GPUs with memory 0MB module: cuda triaged,module: cuda triaged
Let backends specify a schema version when registering kernels module: internals triaged module: backend module: codegen,module: internals triaged module: backend module: codegen
[ROCm] test failures during 4.1 upgrade high priority module: dependency bug module: rocm triaged,high priority module: dependency bug module: rocm triaged
Store created by dist.new_group doesn't appear to respect timeout high priority triage review oncall: distributed triaged better-engineering,high priority triage review oncall: distributed triaged better-engineering
torch.fx may have problem for annotations triaged module: fx,triaged module: fx
Caught an integer overflow or wraparound in torch.nn.MaxPool1d triaged module: pooling,triaged module: pooling
Caffe2 has undeclared dependencies caffe2 triaged,caffe2 triaged
"RuntimeError: iter.device(arg).is_cuda() INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1607370116979/work/aten/src/ATen/native/cuda/Loops.cuh"":94, please report a bug to PyTorch.  needs reproduction module: cuda triaged module: TensorIterator",needs reproduction module: cuda triaged module: TensorIterator
add tests to cover DDP on mixed dense-sparse models oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
BERT model from torchbenchmark fails inference after freezing. triaged jit-backlog,triaged jit-backlog
[JIT] prim::ModuleContainerIndex doesnt work with freezing oncall: jit days,oncall: jit days
Some Numba tests are failing high priority module: ci module: tests triaged module: numba,high priority module: ci module: tests triaged module: numba
It is strange that PyTorch is slow on RTX 3090 module: performance module: cuda triaged,module: performance module: cuda triaged
test_clamp fails on ppc64le oncall: jit triaged module: POWER NNC,oncall: jit triaged module: POWER NNC
Half Normal Log_Prob not defined for 0  module: numerical-stability module: distributions triaged module: random module: half,module: numerical-stability module: distributions triaged module: random module: half
"torch.lerp and torch._foreach_lerp should support uint8 inputs (for vision), int16 (for audio), int32/int64 (for generality)  without (up)casting to float for  and/or dtype argument  triaged enhancement module: interpolation",triaged enhancement module: interpolation
"in WINDOWS, CUDA Out of Memory error but CUDA memory is almost empty module: windows module: cuda module: memory usage triaged",module: windows module: cuda module: memory usage triaged
[proposal] Integrate autograd graph visualizer for displaying crashed locations during detect_anomaly feature module: autograd triaged,feature module: autograd triaged
RMSProp documentation is confusing module: docs module: optimizer triaged enhancement,module: docs module: optimizer triaged enhancement
[package] Heuristics for extern-ing common dependencies triaged,triaged
[package] Clear and minimal interface for what a package provides triaged,triaged
[package] Buck integration for dependency analysis triaged,triaged
Generate delegates for all non-structured kernels triaged module: dispatch,triaged module: dispatch
Numpy.float64 vs native python float breaks DDP triaged module: ddp,triaged module: ddp
Missing tests for gradcheck module: autograd module: tests triaged,module: autograd module: tests triaged
RuntimeError: polar does not support automatic differentiation for outputs with complex dtype. triaged complex_autograd,triaged complex_autograd
Unit testing failures when porting PyTorch wheel module: binaries module: build triaged,module: binaries module: build triaged
build broken when USE_TENSORPIPE=OFF and USE_DISTRIBUTED=ON oncall: distributed module: build triaged module: tensorpipe,oncall: distributed module: build triaged module: tensorpipe
[discussion] Support torch.matmul: strided x sparse in addition to sparse x strided module: sparse triaged,module: sparse triaged
cuDNN error upon the backward method with a sliced tensor output from nn.GRU. module: cudnn module: rnn module: cuda triaged,module: cudnn module: rnn module: cuda triaged
Split VariableTypeManual.cpp triaged module: dispatch,triaged module: dispatch
test_randperm is failing on CPU-only build module: tests triaged,module: tests triaged
[Static Runtime] static_runtime_benchmark compile-time error module: build triaged,module: build triaged
[Static Runtime] StaticRuntime.EmbeddingBag test case broken module: tests triaged,module: tests triaged
SummaryWriter add_image() docs do not state that it is expecting images in a certain format triaged module: tensorboard oncall: visualization,triaged module: tensorboard oncall: visualization
DistributedDataParallel: `DDP(model)` hangs on non-master node oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
Provide half-away-from-zero rounding mode on Tensor::round module: cpp triaged enhancement function request,module: cpp triaged enhancement function request
OSS Mobile Errors Point to fburls triaged enhancement better-engineering,triaged enhancement better-engineering
OSS build doesn't hard fail on nodiscard triaged module: build warnings better-engineering,triaged module: build warnings better-engineering
Implement Truly Parallel Ensemble Layers feature module: nn triaged module: batching,feature module: nn triaged module: batching
[Torchscript] Error detected in torch::jit::(anonymous namespace)::DifferentiableGraphBackward on custom RNN model oncall: jit,oncall: jit
as_tensor returns CPU copy of a CUDA buffer object implementing CUDA Array Interface module: cuda triaged module: numba,module: cuda triaged module: numba
Support Array Interface (__array_interface__ attribute) triaged enhancement module: numpy,triaged enhancement module: numpy
torch.kron of a transposed input error module: error checking triaged function request,module: error checking triaged function request
torch.put is divergent from np.put triaged module: numpy,triaged module: numpy
TorchVision and TorchAudio wheels for AArch64 absent from https://download.pytorch.org/whl/torch_stable.html module: build triaged module: vision module: arm,module: build triaged module: vision module: arm
cdist skip for manual backward is kind of janky triaged module: dispatch better-engineering module: distance functions,triaged module: dispatch better-engineering module: distance functions
Dispatch table for linalg_norm is fishy triaged module: dispatch better-engineering module: norms and normalization,triaged module: dispatch better-engineering module: norms and normalization
[FX] Add guards in files in `test/fx` to make sure they're not run directly triaged module: fx,triaged module: fx
Many advanced indexing operations have untested large tensor branches module: tests triaged module: advanced indexing,module: tests triaged module: advanced indexing
c++ convert from std::vector<Tensor> to c10::List<optional<Tensor>> module: docs module: cpp triaged,module: docs module: cpp triaged
Wrong initialisation gain for SNNs/SELU module: nn triaged module: initialization,module: nn triaged module: initialization
RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED in pytorch lightning module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
Web support? feature triaged module: language binding,feature triaged module: language binding
[FR] Configurable gain value for kaiming_normal_ / kaiming_uniform_ in torch.nn.init module: nn triaged enhancement module: initialization,module: nn triaged enhancement module: initialization
Support scatter/gather for NCCL backend triaged module: c10d,triaged module: c10d
enable parallel test execution for GPU tests module: ci module: tests triaged,module: ci module: tests triaged
[codegen] Change public C++ to accept out/mut arguments by const& triaged module: codegen,triaged module: codegen
"hope to support something like  ""torch::manual_seed_for_mulit_thread"" module: cpp triaged enhancement module: random",module: cpp triaged enhancement module: random
[Feature Request] Optionally specify batch dimension in DataLoader and collate_fn module: dataloader triaged enhancement,module: dataloader triaged enhancement
Is pytorch 1.8.0 incompatible with cuda 11.2 or what is the reason for this error? module: cuda triaged,module: cuda triaged
[Pipe] supporting None and non-Tensors in forward's input/output  triaged pipeline parallelism,triaged pipeline parallelism
Only tensor can be proxied in builtin operators with `__torch_function__` triaged module: __torch_function__ module: fx,triaged module: __torch_function__ module: fx
SubgraphRewriter doesn't handle tensor constants in replacement graph  triaged module: fx,triaged module: fx
[JIT] TorchScript infers incorrect type for `or` in non-boolean context oncall: jit,oncall: jit
There should be a CI build with OpenBLAS in addition to MKL module: ci triaged,module: ci triaged
[torch.vmap] Support second order grads module: autograd triaged enhancement module: vmap,module: autograd triaged enhancement module: vmap
Have the possibility to reduce a tensor with median on more than one specified dimension triaged enhancement module: numpy has workaround module: reductions,triaged enhancement module: numpy has workaround module: reductions
"terminate called after throwing an instance of 'c10::Error'   what():  isTuple() INTERNAL ASSERT FAILED at ""/home/wenda/libtorch/include/ATen/core/ivalue_inl.h"":927, please report a bug to PyTorch. Expected Tuple but got GenericList needs reproduction module: crash module: cpp triaged",needs reproduction module: crash module: cpp triaged
Check Archive.org for Failed Dataset Downloads module: dependency bug triaged enhancement,module: dependency bug triaged enhancement
GoogleNet pretrained/scratch bug? triaged module: vision,triaged module: vision
Linear algebra GPU library function bug tracking issue [magma/cusolver/cublas] module: cuda triaged module: linear algebra,module: cuda triaged module: linear algebra
[JIT] Bad interaction between if-else-style and assert-style type refinement oncall: jit,oncall: jit
Add comprehensive subgroup testing to torch.distributed oncall: distributed triaged module: c10d module: ddp,oncall: distributed triaged module: c10d module: ddp
"After the source code is compiled, an error will be reported fvcore oncall: jit",oncall: jit
Lowering MHA C++/Python to ATen triage review module: nn oncall: transformer/mha,triage review module: nn oncall: transformer/mha
[JIT] torch.jit.optimized_execution(True) greatly slows down some operations in PyTorch 1.8.0  oncall: jit,oncall: jit
foreach ops in autograd  high priority module: autograd triaged,high priority module: autograd triaged
[fx] GraphModule __init__ when provided a Module w/ inconsistent buffer handling triaged module: fx,triaged module: fx
Support for one-hot of dtypes besides torch.int64 module: nn triaged enhancement,module: nn triaged enhancement
ZeroDivisionError: float division by zero in Adam (bias_correction1 is zero) module: numerical-stability module: optimizer triaged,module: numerical-stability module: optimizer triaged
TorchScript cannot handle declared module attributes of Python types oncall: jit,oncall: jit
TorchScript divison by zero fails to error out  oncall: jit,oncall: jit
[FX] Allow customizing code generation for certain Nodes triaged module: fx,triaged module: fx
PyTorch on Embedded Hardware module: cuda triaged,module: cuda triaged
Poor support of `Optimizer.add_param_group` module: optimizer triaged,module: optimizer triaged
Ninja recompiles certain unchanged source files in cpp extension since torch 1.8.0 module: build triaged,module: build triaged
torch.load with Exception module: serialization triaged,module: serialization triaged
`test_reductions` ignoring some tests module: tests triaged module: reductions,module: tests triaged module: reductions
Support GPU/CPU communication in RPC oncall: distributed triaged module: rpc module: tensorpipe,oncall: distributed triaged module: rpc module: tensorpipe
[FX] Regression from 1.8: FX can no longer trace functions where the first element of an int list is a Proxy triaged module: fx,triaged module: fx
torch.distributed with NCCL can hang if the first operation is a barrier oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
vmap gradgradcheck test fails for unfold operation module: autograd triaged module: vmap,module: autograd triaged module: vmap
torch.unsafe_chunk and torch.unsafe_split 's documentation does not render on the website. module: docs triaged module: viewing and reshaping,module: docs triaged module: viewing and reshaping
torch.arange() issue oncall: distributed module: multiprocessing triaged module: regression,oncall: distributed module: multiprocessing triaged module: regression
test/distributed/test_c10d.py::DistributedDataParallelTest only passes if run before other test cases oncall: distributed triaged,oncall: distributed triaged
Properly design manual_cpp_binding (make it less error prone) module: cpp triaged module: language binding,module: cpp triaged module: language binding
How to delete Module from GPU? (libtorch C++) module: cpp-extensions module: cpp triaged,module: cpp-extensions module: cpp triaged
[FX] Ability to wrap functions in other modules for symbolic tracing triaged module: fx,triaged module: fx
Improve memory format testing module: tests triaged module: memory format,module: tests triaged module: memory format
Very slow backward speed when using gather with small-range indices module: performance triaged,module: performance triaged
Show methods of the class in the right sidebar module: docs triaged,module: docs triaged
RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)` triaged module: cublas,triaged module: cublas
After upgrade to 1.8.0 memory leak in libTorch after using torch::jit::load() oncall: jit oncall: mobile,oncall: jit oncall: mobile
libtorch reports bad_alloc when used together with darknet needs reproduction triaged,needs reproduction triaged
Second order derivatives for F.grid_sample() triaged enhancement,triaged enhancement
Model loaded from full model state checkpoint does not run on multiple GPUs (device mismatch) triage review oncall: distributed module: nn module: cuda module: serialization,triage review oncall: distributed module: nn module: cuda module: serialization
IRSimplifier performs unsafe transformation on mod: m*n%m -> 0 triaged NNC,triaged NNC
"Conjugate gradient Descent, and Linear operator are not present in pytorch. module: sparse triaged module: complex module: linear algebra complex_autograd function request module: lazy",module: sparse triaged module: complex module: linear algebra complex_autograd function request module: lazy
Add complex support for torch.unique triaged module: complex module: numpy complex_autograd function request module: python array api,triaged module: complex module: numpy complex_autograd function request module: python array api
Pipe should automatically build appropriate partitions. triaged pipeline parallelism,triaged pipeline parallelism
Backward compute time in DDP logging may not be accurate  oncall: distributed triaged,oncall: distributed triaged
[bug][JIT] Error when tracing model with partial hooks in PyTorch v1.8.0 oncall: jit,oncall: jit
"torch.matmul doesn't handle zero-sized inputs in some cases, leading to batched grad failures module: autograd module: tests triaged module: vmap",module: autograd module: tests triaged module: vmap
When I ran the 'Get Started with Data Data Parallel' tutorial the code got 'RuntimeError '!There are no hints triaged module: data parallel,triaged module: data parallel
[bug] GaussianNLLLoss: Variance not broadcast to input shape module: docs module: nn triaged,module: docs module: nn triaged
torch.empty_strided doesn't test if strides are negative module: error checking triaged,module: error checking triaged
Default pytest is truncating stacktrace (somehow!!!) triaged,triaged
[bug] torch.cumsum: functional and method variant promotes all ints to Long but inplace don't module: docs triaged module: type promotion,module: docs triaged module: type promotion
[bug] torch.cumsum: behaviour for `bool` input module: cuda module: cpu triaged module: boolean tensor,module: cuda module: cpu triaged module: boolean tensor
OpInfo mechanism to test for nondeterminism module: tests triaged module: determinism,module: tests triaged module: determinism
CUDA error: an illegal memory access was encountered when updating model weights using GradScaler module: nn module: cuda triaged module: amp (automated mixed precision),module: nn module: cuda triaged module: amp (automated mixed precision)
Should we remind users not to use the dataset  on GPU when it's the argument of DataLoader? module: dataloader triaged,module: dataloader triaged
Find a good namespace home for torch._assert_async triaged module: numpy module: testing,triaged module: numpy module: testing
Different gradients in torch.matmul depending on input shape module: numerical-stability triaged,module: numerical-stability triaged
Dimension argument names in torch.diag_embed/diagonal vs. transpose/transpose_ module: docs triaged module: numpy,module: docs triaged module: numpy
Inconsistent `out=` behaviour on advanced indexing operations. triaged module: complex enhancement module: advanced indexing,triaged module: complex enhancement module: advanced indexing
`index_copy_`  test fails on PyTorch/XLA module: tests triaged module: xla,module: tests triaged module: xla
Relative performance of histc vs bincount module: performance triaged module: sorting and selection,module: performance triaged module: sorting and selection
[nnc] Prevent user from scheduling after prepareForCodegen triaged NNC,triaged NNC
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
[NIT] Unpickler persistent_load monkey patch should fall back to original persistent_load module: serialization triaged,module: serialization triaged
Exceedingly different F.conv2d outputs on cuda/cpu high priority module: cudnn module: cuda module: convolution triaged module: correctness (silent),high priority module: cudnn module: cuda module: convolution triaged module: correctness (silent)
doxygen and pytorch documentation module: docs module: cpp triaged,module: docs module: cpp triaged
Structured kernels for operators that don't have out= variants triaged module: structured kernels,triaged module: structured kernels
Lint rule to forbid bare assert() in cuda module: cuda module: lint triaged,module: cuda module: lint triaged
Deprecate and remove torch.set_default_tensor_type triaged module: deprecation,triaged module: deprecation
[fx] Handle models with Optional inputs triaged module: fx,triaged module: fx
ModuleList not recognised as reversible by mypy module: nn module: typing triaged,module: nn module: typing triaged
Reset mask for torch.cumsum? triaged enhancement module: numpy module: reductions,triaged enhancement module: numpy module: reductions
group conv in amp too slower module: performance module: cuda triaged module: amp (automated mixed precision),module: performance module: cuda triaged module: amp (automated mixed precision)
'Tensor' object has no attribute 'astype' triaged module: numpy function request,triaged module: numpy function request
[NNC] Simplify after each transformation triaged NNC,triaged NNC
Ensure that nn.MHA and functional multi-head attention is supported by FX graph mode quantization oncall: quantization low priority triaged module: fx,oncall: quantization low priority triaged module: fx
[codegen] Make it easier to codegen call to API triaged enhancement module: codegen,triaged enhancement module: codegen
lr_scheduler _triangular2_scale_fn calculation is overflowed needs reproduction module: optimizer triaged,needs reproduction module: optimizer triaged
Tracker: pytest-related test improvements module: tests triaged tracker,module: tests triaged tracker
Add numerically stable methods to torch.distributions (e.g. logcdf) module: distributions triaged,module: distributions triaged
[JIT] Provide a way to access a flattened optimized graph from Profiling Executor oncall: jit module: bootcamp triaged,oncall: jit module: bootcamp triaged
profile pure C++ process  module: cpp triaged enhancement oncall: profiler,module: cpp triaged enhancement oncall: profiler
[TorchScript Usability]  More intuitive error message when passing wrong types to `torch.jit.script()` oncall: jit,oncall: jit
Broadcasting behaviour for linear algebra solvers triaged module: linear algebra,triaged module: linear algebra
Export to ONNX with all tensor shapes included module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Create a standalone python execution viewer like livepython feature module: logging triaged,feature module: logging triaged
LR Scheduler load_state_dict does not properly update scaling module: optimizer triaged,module: optimizer triaged
"`embedding_bag(..., include_last_offset=True)` should always error if `offsets[-1] != input.size()` module: nn triaged module: embedding",module: nn triaged module: embedding
How to skip the images in a custom dataset and deal with None values? module: dataloader triaged,module: dataloader triaged
[feature request] F.interpolate_as module: nn triaged function request module: interpolation,module: nn triaged function request module: interpolation
Create CI test checker to ensure # of test ran is the same as # of test reported in xmlrunner module: ci triaged enhancement,module: ci triaged enhancement
[nnc] Grouped convolutions triaged NNC,triaged NNC
Basic feature missing module: autograd triaged,module: autograd triaged
Sort torch.topk() output following input index order triaged enhancement,triaged enhancement
Gradient checkpointing support in C++ API module: cpp feature triaged enhancement,module: cpp feature triaged enhancement
torch.searchsorted issues module: docs triaged module: numpy module: sorting and selection,module: docs triaged module: numpy module: sorting and selection
[OpInfo] Improvements for sparse ops tests module: sparse module: tests triaged,module: sparse module: tests triaged
Creating torch tensor as a function of index value triaged module: numpy function request,triaged module: numpy function request
Discrepancy between CPU->GPU and GPU->CPU data transfer speeds module: performance module: cuda triaged,module: performance module: cuda triaged
Fix `torch.norm`'s backward to match forward for ord=+/-inf module: autograd triaged module: linear algebra,module: autograd triaged module: linear algebra
Refactor `torch.linalg.norm` triaged module: linear algebra,triaged module: linear algebra
[TensorExpr] Loop Transformations: `compute_inline` should use `Let` for inlined expressions triaged NNC,triaged NNC
[BE] Consolidating scripts/ and tools/ folder module: docs triaged,module: docs triaged
Add Quantized{CPU|CUDA} support to Structured Kernels module: internals triaged module: structured kernels,module: internals triaged module: structured kernels
[feature request] Introduce TORCH_USE_CUDA=OFF runtime environment variable to faithfully/completely disable cuda (torch.cuda.is_available() <- False and torch.cuda.get_device_count() <- 0) and driver loading/interaction module: cuda triaged enhancement small,module: cuda triaged enhancement small
code linking to libtorch cannot use thrust/cub functions high priority module: cpp module: cuda triaged,high priority module: cpp module: cuda triaged
Fix `torch.linalg.vector_norm`'s backward to avoid needing `at::abs()` call in forward for ord=+/-inf  triaged module: linear algebra,triaged module: linear algebra
[FR] torch.load's map_location supports int  module: cuda triaged enhancement,module: cuda triaged enhancement
CUDA `linalg.norm` matrix order +/-2 and nuclear norm for extreme values may be incorrect high priority module: cuda triaged module: NaNs and Infs module: linear algebra,high priority module: cuda triaged module: NaNs and Infs module: linear algebra
Add slicing to distributions module: distributions triaged enhancement,module: distributions triaged enhancement
Torchaudio 0.8.0.dev nightly requires torch 1.9.0.dev which is missing module: build triaged,module: build triaged
torch::jit::Node::isNondeterministic() does not include aten::feature_dropout or inline kinds oncall: jit,oncall: jit
CUDA error: no kernel image is available for execution on the device When convert cpu tensor to gpu tensor module: build module: cuda triaged,module: build module: cuda triaged
Custom scatter function for DataParallel triaged enhancement module: data parallel,triaged enhancement module: data parallel
pickle is a security issue module: pickle module: serialization triaged module: hub topic: security,module: pickle module: serialization triaged module: hub topic: security
[TensorExpr] Add IR Verifier module: bootcamp triaged NNC,module: bootcamp triaged NNC
version1.7.0 is ~1.3x slower than 1.4.0 on ResNet18 module: performance triaged,module: performance triaged
Error: â€˜strâ€™ is not a member of â€˜c10â€™; did you mean â€˜c10::aten::strâ€™? while using libtorch module: cpp triaged,module: cpp triaged
"RTX 2080s performs better than RTX 3080 in Semantic Segmentation inference process(Libtorch,win10),why? module: performance module: cuda triaged",module: performance module: cuda triaged
Omitting mutability annotations in op schemas can lead to subtle errors  module: internals triaged,module: internals triaged
Submodules not rewritten during AST rewrite triaged module: fx,triaged module: fx
Cumulative integration? triaged module: numpy function request,triaged module: numpy function request
Helper methods not preserved during AST rewrite triaged module: fx,triaged module: fx
torch.bmm incorrect with pytorch 1.7.1 and cuda 11 needs reproduction module: cuda triaged module: cublas,needs reproduction module: cuda triaged module: cublas
link failed when using custom build pytorch on Android triaged module: undefined reference module: android module: arm,triaged module: undefined reference module: android module: arm
Dispatcher documentation needs update module: docs triaged module: dispatch,module: docs triaged module: dispatch
"Could you please increase the wheel compiled Linux (aarch64) GPU, thank you very much! triaged module: arm",triaged module: arm
all reduce hangs and does not throw exception when CUDA_VISIBLE_DEVICES is not set properly using the NCCL backend oncall: distributed module: nccl,oncall: distributed module: nccl
[nnc][perf] Performance decrease with CPU fusion on `freeze(script(pytorch_mobilenet_v3))` module: performance triaged NNC,module: performance triaged NNC
Add autograd tests to verify correctness for R -> C cases module: tests triaged module: complex complex_autograd,module: tests triaged module: complex complex_autograd
[proposal] Way to export/display the convolution algorithm found with cudnn.benchmark = True + allow to override algo choice with conv/matmul module/function algo/hint arguments module: cudnn feature module: convolution triaged,module: cudnn feature module: convolution triaged
[NNC] NNC usability list oncall: jit triaged NNC,oncall: jit triaged NNC
torch.autograd.Function doesn't support non-Tensor outputs module: autograd triaged,module: autograd triaged
autograd.functional.vjp with function that mutates inputs in-place can cause confusion module: autograd triaged,module: autograd triaged
Normal_like operator triaged enhancement module: numpy,triaged enhancement module: numpy
Reciprocals of complex tensors with infinities are different from NumPy. triaged module: complex module: numpy module: NaNs and Infs,triaged module: complex module: numpy module: NaNs and Infs
"TorchConfig.cmake entries of if(ON) are not accepted by CMake as ""true"", to be replaced by if(1) module: build triaged",module: build triaged
Implementation of many complex functions is fast but inaccurate in libc++ module: numerical-stability triaged module: complex enhancement module: numpy,module: numerical-stability triaged module: complex enhancement module: numpy
[JIT] __getitem__ on scripted ModuleDict returns unscripted module oncall: jit days,oncall: jit days
Model produces same outputs for different inputs when using GPU needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
[FR} Implement Skellam distribution module: distributions triaged,module: distributions triaged
Printing a Tensor that is being vmap'ed over in C++ raises an error triaged module: vmap,triaged module: vmap
"""LayerNormKernelImpl"" not implemented for 'Half' - CPU module: nn module: cpu triaged enhancement module: half",module: nn module: cpu triaged enhancement module: half
DataParallel module fails to handle data with size not divisible by number of GPUs triaged module: data parallel,triaged module: data parallel
[docs] Official apex -> torch.cuda.amp migration guide module: docs feature triaged module: amp (automated mixed precision),module: docs feature triaged module: amp (automated mixed precision)
Port unary elementwise ops to structured kernels triaged module: structured kernels,triaged module: structured kernels
GPU (+CPU/RAM?) self-check script or instructions in core module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
'TypeError: expected CPU (got CUDA)' when placing a CUDA tensor on a class that is inheriting torch.Tensor triaged module: tensor creation,triaged module: tensor creation
tolist called on torch scalar does not return a list => proposal to support new arg force = True to provide a work around module: docs triaged module: numpy,module: docs triaged module: numpy
Unlucky batch->device split by DataParallel causes an exception triaged module: data parallel,triaged module: data parallel
CTCLoss gradient is incorrect module: docs module: autograd module: loss triaged,module: docs module: autograd module: loss triaged
CosineAnnealingWarmRestarts LR scheduler fails when lash_epoch != -1 module: optimizer triaged,module: optimizer triaged
Removing mutation on block input should print any meaning message. module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Conv3D error : CUDNN_STATUS_INTERNAL_ERROR module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
C++ standard library confusion module: build triaged,module: build triaged
Wasserstein metric module: nn triaged needs research,module: nn triaged needs research
from_numpy function rounds float values on Jetson NX triaged module: correctness (silent) module: arm module: jetson,triaged module: correctness (silent) module: arm module: jetson
Allow F.pad(mode = 'reflect') when shape == pad module: nn triaged enhancement module: numpy module: padding,module: nn triaged enhancement module: numpy module: padding
"`torch.load(..., weights_only=True)` currently raises a Deprecation warning + [proposal] `weights_only=True` should become default for safe legacy-loading pickles high priority feature module: serialization triaged module: hub topic: security",high priority feature module: serialization triaged module: hub topic: security
CMake errors building project that uses PyTorch module: build triaged,module: build triaged
Pointer passed where number is expected for PYTORCH_CUDA_FUSER_JIT_OPT_LEVEL leading to crash oncall: jit,oncall: jit
Trying to initialise CUDA twice in a process with no visible devices hangs the process and terminal permanently module: cuda triaged module: deadlock,module: cuda triaged module: deadlock
[proposal] Parameter dim for F.linear (and maybe nn.Linear) module: nn triaged enhancement,module: nn triaged enhancement
Build using Py_LIMITED_API and then build wheels with the stable ABI abi3 tag module: binaries module: performance oncall: releng triaged,module: binaries module: performance oncall: releng triaged
"Unable to build: subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '4']' returned non-zero exit status 1. module: build triaged",module: build triaged
at::numeric_limits is misleading module: cuda module: rocm triaged better-engineering actionable,module: cuda module: rocm triaged better-engineering actionable
common_jit.check_against_reference() should check gradients w.r.t. weights oncall: jit module: autograd triaged days,oncall: jit module: autograd triaged days
Detect OpenMP Loop and this application may hang warnings oncall: jit module: tests module: multithreading days,oncall: jit module: tests module: multithreading days
device (+dtype) arguments for torch.stack / torch.cat module: performance module: cuda triaged enhancement,module: performance module: cuda triaged enhancement
torch.distributed.TCPStore doesn't work with dual IPv4/IPv6 network interface oncall: distributed triaged,oncall: distributed triaged
Automatic splitting of batch during model feed-forwarding to avoid memory errors caused by the too big batch_size feature module: memory usage triaged needs research,feature module: memory usage triaged needs research
[FX] Interpreter doesn't GC values triaged module: fx,triaged module: fx
CPU eval BatchNorm2d is not threaded module: performance module: nn triaged module: norms and normalization,module: performance module: nn triaged module: norms and normalization
[FX] Modules called with constants should return constants and not proxies. triaged module: fx,triaged module: fx
Overwritten method has wrong self when JIT'ed oncall: jit,oncall: jit
Performance bugs of transpose2d on A100 module: performance module: cudnn triaged,module: performance module: cudnn triaged
torch.nn.functional.grid_sample outputs NaN module: nn triaged module: NaNs and Infs,module: nn triaged module: NaNs and Infs
torch.nn.functional.binary_cross_entropy(_with_logits) outputs NaN module: nn triaged module: NaNs and Infs,module: nn triaged module: NaNs and Infs
Can't reassign a child module instance to a lambda module: nn triaged,module: nn triaged
[cuda jit extentions] global shared destination conflicts with virtual envs oncall: jit,oncall: jit
JIT Module forward pre-hooks do not support default arguments oncall: jit,oncall: jit
Tensor.nonzero tries to allocate huge amount of memory for tensors on GPU with num_elements close to INT_MAX module: dependency bug module: cuda module: memory usage triaged,module: dependency bug module: cuda module: memory usage triaged
Tensor.nonzero fails on GPU for tensors containing more than INT_MAX elements module: cuda triaged enhancement,module: cuda triaged enhancement
discrepancies between TorchConfigVersion.cmake to git branch module: build triaged,module: build triaged
TestVectorizedMemoryAccess.CopyKernel starting to fail after driver to 460.39 high priority module: cuda module: ci triaged,high priority module: cuda module: ci triaged
Assignment target is transposed when using jit.script and avanced indexing oncall: jit good first issue OSS contribution wanted days,oncall: jit good first issue OSS contribution wanted days
"Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /pytorch/caffe2/serialize/inline_container.cc:132) triaged module: assert failure",triaged module: assert failure
.numpy() array failes to keep original storage around triaged module: numpy module: tensor creation,triaged module: numpy module: tensor creation
TypeError: can't pickle _thread.lock objects when using spawn / forkserver module: pickle triaged better-engineering,module: pickle triaged better-engineering
methods decorated with torch.jit._overload_method are not accessible oncall: jit,oncall: jit
torch.fx.symbolic_trace fails on torch.arange with input-dependent size triaged module: fx,triaged module: fx
[discussion] [python] Generic way for generating module-wrappers for stateless parametrized functions feature module: nn triaged better-engineering,feature module: nn triaged better-engineering
Libtorch lock thread module: abi triaged,module: abi triaged
Proposal for export TensorBoard tracing files triaged module: tensorboard,triaged module: tensorboard
Assertion on the existence of RRef user-facing methods' docstrings oncall: distributed triaged better-engineering module: rpc,oncall: distributed triaged better-engineering module: rpc
Analytical metric package module: distributions feature triaged module: data,module: distributions feature triaged module: data
BatchNorm3d error : CUDNN_STATUS_NOT_SUPPORTED module: cudnn triaged,module: cudnn triaged
torch.nn.functional.ctc_loss crash(segfault)  module: crash module: nn module: loss triaged,module: crash module: nn module: loss triaged
Support for 64-bit (ILP64) LAPACK module: build triaged module: linear algebra,module: build triaged module: linear algebra
Inconsistent result between NumPy and PyTorch for 0^(z) when Re(z) < 0 triaged module: complex module: NaNs and Infs,triaged module: complex module: NaNs and Infs
Weird behavior in Conv2d padding when changed to 'reflect' module: docs module: nn module: convolution triaged module: padding,module: docs module: nn module: convolution triaged module: padding
[JIT/Futures] Future callbacks/then() APIs should throw the correct exception oncall: jit module: bootcamp days,oncall: jit module: bootcamp days
Torch DataLoader fails to reraise error from sqlalchemy. module: dataloader triaged,module: dataloader triaged
"Segmentation Fault: Garbage collector, cuda memory module: cuda triaged",module: cuda triaged
[docs] FractionalMaxPool2d : _random_samples arg is present in signature but undocumented. module: docs module: nn triaged module: pooling,module: docs module: nn triaged module: pooling
"`fractional_max_pool{2, 3}d` inconsistent between CUDA and CPU triaged module: pooling",triaged module: pooling
[RPC] rref.get_type() should have default return type of future oncall: distributed module: rpc,oncall: distributed module: rpc
[TorchScript Usability] Type-checker too strict for Optional type initialization pattern oncall: jit,oncall: jit
torch.reshape fails to keep the memory format triaged module: memory format module: viewing and reshaping,triaged module: memory format module: viewing and reshaping
Why isn't there a pip-installable PyTorch for Raspberry Pi? module: binaries feature triaged module: arm,module: binaries feature triaged module: arm
Python-3.9: Importing numba and torch triggers internal error module: build triaged,module: build triaged
Massive Performance bottlenecks in some of the Reduce operations. module: performance triaged module: TensorIterator module: reductions,module: performance triaged module: TensorIterator module: reductions
Add branch predictor hints to prefer `Context::deterministicAlgorithms() == false` module: performance triaged module: determinism,module: performance triaged module: determinism
Make TorchBind object appear in state_dict feature module: nn module: serialization triaged,feature module: nn module: serialization triaged
torch::jit::Module cannot be forward declared and used into a `unique_ptr`  oncall: jit,oncall: jit
Optional tensor type feature triaged,feature triaged
torch.median slower than torch.sort on cpu module: performance module: cpu triaged module: sorting and selection module: reductions,module: performance module: cpu triaged module: sorting and selection module: reductions
"dist.init_process_group('nccl', ...) hangs in some combinations of pytorch+python+cuda version oncall: distributed triaged",oncall: distributed triaged
Different results on RTX 2060s vs GTX 1060 with gpt model triaged module: numerical-reproducibility,triaged module: numerical-reproducibility
Pytorch num_worker>0 code worked first time and then it never worked with same setting again module: dataloader triaged,module: dataloader triaged
Caffe2 RNN Cell Names not rectified caffe2 triaged,caffe2 triaged
C++ load model error oncall: jit,oncall: jit
[numpy] `round` and `trunc` not supported for Integral Type while Python and NumPy supports them triaged module: numpy,triaged module: numpy
max_pool2d CPU forward performance is poor module: performance triaged module: pooling,module: performance triaged module: pooling
DataParallel copies the model onto GPUs sequentially module: performance triaged enhancement module: data parallel,module: performance triaged enhancement module: data parallel
TorchScript class compilation failure with pybind11-2.6.2 oncall: jit,oncall: jit
Run time error module: dependency bug module: cuda triaged module: cublas module: wsl,module: dependency bug module: cuda triaged module: cublas module: wsl
Cant compile from source 1.8.0a0 because 'magma_v2.h' file not found and undefined references to `magma_*' high priority module: build module: cuda triaged module: linear algebra module: magma,high priority module: build module: cuda triaged module: linear algebra module: magma
Support torch::jit::script::Module converting to torch::nn::Module. oncall: jit weeks,oncall: jit weeks
"Pytorch not working properly (I don't know how to summarize it, see below) module: nn triaged",module: nn triaged
"""make install"" skips some required libraries for static build module: build triaged",module: build triaged
result_type doesn't take dtypes and doesn't match numpy triaged module: numpy function request,triaged module: numpy function request
Bind tensor.unflatten to torch.unflatten namespace (torch.flatten exists but not torch.unflatten) triaged,triaged
"[ux] Proposal to have t() === transpose(-1, -2), since batches are very frequent triaged module: numpy module: ux",triaged module: numpy module: ux
Profiler is stuck for DDP training triaged oncall: profiler,triaged oncall: profiler
Error of running needs reproduction triaged,needs reproduction triaged
[search] duplicated entries in results module: docs triaged,module: docs triaged
macOS error building 1.7.1 from source on Catalina 10.15.7 module: build triaged module: macos,module: build triaged module: macos
std::type_index doesn't provide reliable equality for getCustomClassTypeMap module: internals triaged,module: internals triaged
`torch.cuda.device` should look like it is inherited from `torch.device` module: cuda triaged enhancement,module: cuda triaged enhancement
"Complex->Real cast is a warning, calling `real` or `imag` on non-complex tensors is an Error. triaged module: complex module: ux",triaged module: complex module: ux
Initialize NCCL backend with MPI oncall: distributed triaged module: mpi enhancement module: nccl,oncall: distributed triaged module: mpi enhancement module: nccl
[numpy] torch.ceil and torch.floor don't support integer inputs while Numpy does triaged module: numpy,triaged module: numpy
Make it clearer when a tensor was quantized symmetrically oncall: quantization low priority triaged,oncall: quantization low priority triaged
-fno-omit-frame-pointer by default in our builds module: build triaged enhancement,module: build triaged enhancement
[TorchScript Usability] Non-intuitive error messages for user errors  oncall: jit,oncall: jit
[TorchScript Usability] tensor printout format differs from eager mode oncall: jit good first issue OSS contribution wanted TSUsability TSRootCause:PyTorchParityGap,oncall: jit good first issue OSS contribution wanted TSUsability TSRootCause:PyTorchParityGap
[TorchScript Usability] Earlier type checking needed for interface type oncall: jit,oncall: jit
[TorchScript Usability] error message when invoking __init__ of module types inside TorchScript oncall: jit,oncall: jit
segmentation fault in torch.nn.ReplicationPad3d/2d when padding is large module: crash module: nn triaged module: padding,module: crash module: nn triaged module: padding
TestLinalgCPU.test_cholesky_solve_autograd_cpu_float64 fails on different random seed module: autograd triaged module: linear algebra module: correctness (silent),module: autograd triaged module: linear algebra module: correctness (silent)
"torch.tensor(x) fails inconsistently, assumes 0 index exists in 'Series' triaged module: tensor creation",triaged module: tensor creation
dyld: Symbol not found: _PyBaseObject_Type   Referenced from module: build triaged module: macos module: static linking,module: build triaged module: macos module: static linking
apex internal assert failed module: dependency bug triaged module: assert failure module: regression,module: dependency bug triaged module: assert failure module: regression
RFC: Private CUDA memory pools feature module: cuda triaged module: cuda graphs,feature module: cuda triaged module: cuda graphs
 error: â€˜PyFrameObject {aka struct _frame}â€™ has no member named â€˜f_lastiâ€™ module: build triaged,module: build triaged
Native code assert or hang on process exit with a torch.jit.script model on multi-GPU triage review oncall: jit,triage review oncall: jit
Class-based structured kernels instruction count regression module: performance module: internals triaged module: structured kernels,module: performance module: internals triaged module: structured kernels
CuDNN 8 with benchmark=True takes minutes to execute for certain configurations module: performance module: cudnn triaged,module: performance module: cudnn triaged
Status of pip wheels with _GLIBCXX_USE_CXX11_ABI=1 high priority module: binaries module: cpp module: abi triaged needs design,high priority module: binaries module: cpp module: abi triaged needs design
The pooling code is ignoring sliding windows that start within the right padding triaged module: pooling,triaged module: pooling
arm64-v8a not compiling due to libpytorch_jni.so module: build triaged module: android module: arm,module: build triaged module: android module: arm
len of ModuleList is incorrect for jitted models oncall: jit,oncall: jit
/usr/local/include/google/protobuf/stubs/pbconfig.h:3:37: note: expanded from macro 'GOOGLE_PROTOBUF_HASH_MAP_H' module: build triaged module: macos,module: build triaged module: macos
[pipeline] gpu util + peak mem reporting to tune partitions and chunks  oncall: distributed feature triaged pipeline parallelism,oncall: distributed feature triaged pipeline parallelism
DataLoader is slow in spawned processes module: performance module: multiprocessing module: dataloader triaged,module: performance module: multiprocessing module: dataloader triaged
Consider adding context manager support for torch.set_printoptions module: printing triaged enhancement,module: printing triaged enhancement
Scalar/Tensor arg type for op schemas triaged module: codegen,triaged module: codegen
Add option for Convolutions to operate over NTC and NHWC tensors feature triaged module: memory format,feature triaged module: memory format
Dispatch-less structured wrapper / composite / alias kernels module: internals triaged module: structured kernels,module: internals triaged module: structured kernels
libpytorch macos build: static library eigen_blas_LIBRARY-NOTFOUND not found module: build triaged,module: build triaged
Questions about amp module: cuda triaged module: amp (automated mixed precision),module: cuda triaged module: amp (automated mixed precision)
SequentialSampler getting slower as time passing by module: dataloader triaged,module: dataloader triaged
dataloader bug need help module: dataloader triaged,module: dataloader triaged
[JIT] Type refinement failure in if-elif-else structure oncall: jit TSRootCause:TypeRefinement TSUsability,oncall: jit TSRootCause:TypeRefinement TSUsability
THCCachingAllocator::cuda_free_mutex has no effect module: cuda triaged module: nccl,module: cuda triaged module: nccl
"torch.unique(x, dim=1, return_inverse=True) returns inverse for only the last sub-tensor along dim module: docs triaged module: numpy module: sorting and selection",module: docs triaged module: numpy module: sorting and selection
Does pytorch support cuda 11.1 module: build triaged,module: build triaged
Unexpected slow dropout in stacked RNN/LSTM/GRU high priority module: cudnn module: rnn triaged,high priority module: cudnn module: rnn triaged
Segmentation Fault when importing torch on macOS Big Sur high priority needs reproduction module: binaries triaged module: macos module: openmp,high priority needs reproduction module: binaries triaged module: macos module: openmp
[TorchScript Performance Deep-dive] problems discovered in TS performance deep-dive module: performance triaged,module: performance triaged
Insufficient shared memory (shm) while training module: memory usage triaged,module: memory usage triaged
Dataloader Prefetch data to GPU by cudaMemPrefetchAsync module: dataloader triaged enhancement,module: dataloader triaged enhancement
Pull upstream cmake's change in FindCUDA/select_compute_arch module: build module: cuda triaged,module: build module: cuda triaged
pin_memory leads to GPU memory being used but it doesn't initialize CUDA module: cuda triaged,module: cuda triaged
NCCL_BLOCKING_WAIT=1 makes training extremely slow (but if not set then OOM on one device will hang training) oncall: distributed triaged,oncall: distributed triaged
Segmentation fault encountered when using nn.MultiheadAttention with v1.7.1 module: crash triaged module: regression oncall: transformer/mha,module: crash triaged module: regression oncall: transformer/mha
pytorch DDP hangs at .backward() call needs reproduction triaged module: deadlock module: ddp,needs reproduction triaged module: deadlock module: ddp
Link to `torch.einsum` in `torch.tensordot` module: docs triaged,module: docs triaged
Implement torch.pow for float16 and bfloat16 on CPU triaged module: half function request,triaged module: half function request
pytorch is built without _GLIBCXX_USE_CXX11_ABI and can cause std::regex crashes (probably) needs reproduction module: build triaged,needs reproduction module: build triaged
[TorchScript Usability] Incorrect default types oncall: jit,oncall: jit
"Improve error message for ""Could not run 'aten::record_stream' with arguments from the 'CPU' backend. ""  triaged enhancement module: dispatch",triaged enhancement module: dispatch
Batched grad coverage rollup module: autograd triaged module: vmap,module: autograd triaged module: vmap
Adding visualizations for indexing and other functions module: docs triaged module: advanced indexing module: scatter & gather ops,module: docs triaged module: advanced indexing module: scatter & gather ops
[PyTorch Mobile] Can't use Vulkan backend triaged module: android oncall: mobile module: vulkan,triaged module: android oncall: mobile module: vulkan
dependency_links is deprecated  module: build module: docs triaged,module: build module: docs triaged
ignore_index for nn_mse_loss module: nn module: loss triaged needs design function request,module: nn module: loss triaged needs design function request
Multiple ProcessGroup C++ Tests are flaky oncall: distributed triaged module: flaky-tests module: c10d,oncall: distributed triaged module: flaky-tests module: c10d
CosineAnnealingWarmRestarts lacks verbose functionality module: optimizer triaged,module: optimizer triaged
RandomSampler is very slow with huge dataset module: dataloader triaged,module: dataloader triaged
Launching two processes causes hanging module: multiprocessing triaged,module: multiprocessing triaged
build from source fail in jeston tx2 Python3.8 module: build triaged module: arm,module: build triaged module: arm
Training slowdown from 1.6 to 1.7.1 module: performance triaged module: __torch_function__,module: performance triaged module: __torch_function__
Use unified type for distributions.constraint API module: distributions triaged,module: distributions triaged
Discourage slow gradchecks module: autograd module: tests triaged,module: autograd module: tests triaged
Enhance supported fill value type for constant pad in functional.pad module: nn triaged enhancement needs design module: padding,module: nn triaged enhancement needs design module: padding
test_fn_grad_fft_fftn_cpu_complex128 and test_fn_grad_fft_rfftn_cpu_float64 are failing under TSAN module: tests triaged module: sanitizers module: fft,module: tests triaged module: sanitizers module: fft
Channels last doesn't improve speed when using SyncBatchNorm module: performance triaged module: memory format,module: performance triaged module: memory format
Unable to compile CUDAExtension with Pytorch 1.5 or above module: bc-breaking module: cpp-extensions module: cuda triaged topic: bc breaking,module: bc-breaking module: cpp-extensions module: cuda triaged topic: bc breaking
Huge file sizes for libtorch and dependencies? module: binaries module: build triaged,module: binaries module: build triaged
Make use of eps for numerical stability more consistent module: numerical-stability triaged,module: numerical-stability triaged
linker error when trying to use Metal backend in PyTorch Mobile module: build triaged module: arm,module: build triaged module: arm
[RFC] Make RRef proxy APIs non-blocking triaged module: rpc,triaged module: rpc
Standalone OSS RPC benchmark oncall: distributed triaged better-engineering module: rpc,oncall: distributed triaged better-engineering module: rpc
libtorch gpu set id bug module: cpp module: cuda triaged,module: cpp module: cuda triaged
out variant of many loss functions are not consistent with non-out variant when reduction is not none module: cpp module: nn module: loss triaged module: reductions,module: cpp module: nn module: loss triaged module: reductions
Provide a set of C++ foreach APIs that will take tensor pointers as an input module: cpp triaged,module: cpp triaged
error: conversion from â€˜std::vector<at::Tensor>â€™ to non-scalar type â€˜at::Tensorâ€™ requested triaged module: numpy,triaged module: numpy
for CNN in fp16 execution time depends on input scale module: performance module: nn module: cuda triaged module: half,module: performance module: nn module: cuda triaged module: half
torch.tril_indices is incompatible with np.tril_indices triaged module: numpy module: deprecation module: ux,triaged module: numpy module: deprecation module: ux
how to save weights when using RPC framework oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
torch.special tracking issue triaged module: numpy module: special tracker,triaged module: numpy module: special tracker
NumPy Compatibility tracking issue high priority triaged module: numpy tracker,high priority triaged module: numpy tracker
RFC: identify analogous NumPy operators when documenting PyTorch operators module: docs triaged module: numpy,module: docs triaged module: numpy
torch.any and torch.all map uint8 -> uint8 but should map uint8 -> bool module: bc-breaking triaged module: numpy module: reductions topic: bc breaking,module: bc-breaking triaged module: numpy module: reductions topic: bc breaking
Interpolation tracking issue triaged module: numpy module: interpolation tracker,triaged module: numpy module: interpolation tracker
Function request: scipy.interpolate.InterpolatedUnivariateSpline triaged module: numpy function request module: interpolation,triaged module: numpy function request module: interpolation
Function request: scipy.interpolate.griddata triaged module: numpy function request module: interpolation,triaged module: numpy function request module: interpolation
Function request: scipy.interpolate.RegularGridInterpolator triaged module: numpy function request module: interpolation,triaged module: numpy function request module: interpolation
Function request: scipy.interpolate.RectBivariateSpline triaged module: numpy function request module: interpolation,triaged module: numpy function request module: interpolation
Function Request: scipy.ndimage.zoom triaged module: numpy function request module: interpolation,triaged module: numpy function request module: interpolation
Function Request: scipy.interpolate.interp1d triaged module: numpy function request module: interpolation,triaged module: numpy function request module: interpolation
Function Request: np.interp triaged module: numpy function request module: interpolation,triaged module: numpy function request module: interpolation
Function Request: scipy.stats.pearsonr triaged module: numpy function request,triaged module: numpy function request
torch.Tensor.random_ is divergent from NumPy's np.random.random triaged module: numpy module: random module: deprecation,triaged module: numpy module: random module: deprecation
RTX3090 performs no better than 1080ti module: performance triaged,module: performance triaged
running a job on multiple gpus with qsub  oncall: distributed triaged,oncall: distributed triaged
Way to solve GPU Host Thread Contention? oncall: distributed triaged,oncall: distributed triaged
Implement dunder getattribute instead of dunder getattr for nn.Module module: bc-breaking module: nn triaged topic: bc breaking,module: bc-breaking module: nn triaged topic: bc breaking
torch.meshgrid is divergent from np.meshgrid triaged module: numpy module: deprecation,triaged module: numpy module: deprecation
torch.transpose is divergent from np.transpose triaged module: numpy module: deprecation,triaged module: numpy module: deprecation
torch.equal is divergent from np.equal triaged module: numpy module: deprecation module: testing,triaged module: numpy module: deprecation module: testing
torch.cross is divergent from np.cross triaged module: numpy module: deprecation,triaged module: numpy module: deprecation
CUDA error when using `binary_cross_entropy_with_logits` module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
ModuleNotFoundError: No module named '__torch__' needs reproduction module: multiprocessing module: serialization triaged,needs reproduction module: multiprocessing module: serialization triaged
FusedKernelCPU failed to delete generated dll files on Windows  oncall: jit module: windows,oncall: jit module: windows
More general MultiHeadAttention and Transformer modules triaged enhancement oncall: transformer/mha,triaged enhancement oncall: transformer/mha
"Tracing with autocast failed with error: ""Cannot insert a Tensor that requires grad as a constant"" oncall: jit module: amp (automated mixed precision)",oncall: jit module: amp (automated mixed precision)
`requires_grad_` in `no_grad` context returns incorrect value with tensor subclasses module: autograd triaged,module: autograd triaged
torch.float_power out= and inplace variant errors on non-matching output dtype instead of casting triaged module: type promotion,triaged module: type promotion
"Pytorch 1.7.1 hangs with multi-gpu, while Pytorch 1.6.0 works correctly triaged module: deadlock module: data parallel",triaged module: deadlock module: data parallel
Build Fail with vulkan on ARM64 and wayland drivers module: build triaged,module: build triaged
Find the joint eigenvalue of two matrices.  feature triaged needs research module: linear algebra,feature triaged needs research module: linear algebra
Excessive memory usage caused by Samplers storing lists of indices module: dataloader module: memory usage triaged,module: dataloader module: memory usage triaged
deterministic implementation for adaptive_avg_pool2d_backward_cuda  module: cuda triaged module: determinism module: pooling function request,module: cuda triaged module: determinism module: pooling function request
GRU and LSTM fail for seq_len = 0 module: rnn triaged,module: rnn triaged
Calling emptyCache in Conv_v7.cpp causes performance degradation module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
CUDACachingAllocator is not GC-aware module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
[FX] Static specialization of callsites (e.g. chunk) triaged module: fx,triaged module: fx
Automated Mixed Precision not documented to work with nn.DataParallel triaged module: data parallel,triaged module: data parallel
FX call_function does not work if target is a class member function triaged module: fx,triaged module: fx
Inconsistent Variable Naming in FindTorch.cmake module: build module: cpp triaged,module: build module: cpp triaged
"nn.InstanceNorm should warn user if input channel is inconsistent with num_features, even when affine=False module: nn triaged needs design module: norms and normalization",module: nn triaged needs design module: norms and normalization
JIT does not correctly compile custom classes derived from torch.Tensor oncall: jit,oncall: jit
[proposal] Pseudo-functions to support common gradient patch use-cases like replacing inf / nan or clipping / gradient reversal module: autograd triaged needs design,module: autograd triaged needs design
torch.where scalar/tensor documentation is unclear and not formatted module: docs triaged module: sorting and selection,module: docs triaged module: sorting and selection
need a clear guide for when and how to use torch.cuda.set_device() module: docs module: cuda triaged needs design,module: docs module: cuda triaged needs design
Errors when coercing complex numbers of various sizes triaged module: complex module: half,triaged module: complex module: half
Cannot print 32-bit complex tensors module: docs module: printing triaged module: complex module: half,module: docs module: printing triaged module: complex module: half
Broken LAPACK links in the documentation module: docs triaged module: linear algebra,module: docs triaged module: linear algebra
batch_isend_irecv: the receiving end cannot receive large tensors from the sending end correctly oncall: distributed triaged,oncall: distributed triaged
Torch native_layer_norm OP out-of-bounds access triaged module: norms and normalization,triaged module: norms and normalization
Magma functions that don't have queue argument create cublas handles for each call module: performance triaged module: linear algebra,module: performance triaged module: linear algebra
pytorch_windows_vs2019_py36_cuda11.1_test1 intermittently fails high priority module: windows module: cuda module: ci triaged,high priority module: windows module: cuda module: ci triaged
Torch  _remove_batch_dim OP out-of-bounds access module: crash triaged module: vmap,module: crash triaged module: vmap
Torch quantized_lstm_cell op out-of-bounds access module: crash oncall: quantization low priority triaged,module: crash oncall: quantization low priority triaged
channels_last format convolution is slower than normal NCHW module: performance module: cuda triaged module: memory format,module: performance module: cuda triaged module: memory format
Multinomial without replacement produces samples that have zero probability module: distributions triaged module: numpy module: random module: ux,module: distributions triaged module: numpy module: random module: ux
torch.Tensor.repeat is divergent from np.repeat high priority triaged module: numpy module: deprecation,high priority triaged module: numpy module: deprecation
torch.split is divergent from np.split high priority triaged module: numpy module: deprecation,high priority triaged module: numpy module: deprecation
torch.var and torch.std are not compatible with np.var and np.std high priority triaged module: numpy module: deprecation module: reductions,high priority triaged module: numpy module: deprecation module: reductions
"[docs] nn.modules pages should mention corresponding functional versions, e.g. for nn.Hardshrink module: docs module: nn triaged small",module: docs module: nn triaged small
[RFC] Speed up python function and arg serialization in RPC APIs oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
Test functionality to detect extra cross device synchronizations module: cuda module: tests triaged,module: cuda module: tests triaged
A significant overhead when running fastrnns with autograd.profiler module: performance triaged oncall: profiler,module: performance triaged oncall: profiler
Support DDP find_unused_parameters=True mode when combined with Pipe oncall: distributed triaged module: rpc module: ddp,oncall: distributed triaged module: rpc module: ddp
GEMM with int8 datatype throws RuntimeError on GPU triaged module: linear algebra needs design function request,triaged module: linear algebra needs design function request
LR scheduler `get_lr()` bug module: optimizer triaged,module: optimizer triaged
nll_loss2d: t >= 0 && t < n_classes assertion is not checked when using GPU tensors and reduction='none' module: nn module: loss module: cuda good first issue module: error checking triaged,module: nn module: loss module: cuda good first issue module: error checking triaged
The same tensor requires more memory on RTX3090 high priority module: cuda module: memory usage triaged,high priority module: cuda module: memory usage triaged
Tensorboard Error needs reproduction triaged,needs reproduction triaged
torch.solve on Jetson is slower than humans module: build triaged module: linear algebra,module: build triaged module: linear algebra
Pytorch1.7.1 Dataloader bugs on win10 module: windows module: dataloader triaged,module: windows module: dataloader triaged
Advanced Indexing does not trace correctly for tensor shape that has leading 1s oncall: jit module: advanced indexing days,oncall: jit module: advanced indexing days
[Feature Request] SuperLoss (NeurIPS 2020) module: nn module: loss triaged function request,module: nn module: loss triaged function request
[dataloader] RuntimeError: Too many open files when yielding integers module: dataloader triaged,module: dataloader triaged
torch init_process_group always hangs awaiting response (this tag is deprecated) oncall: distributed triaged,awaiting response (this tag is deprecated) oncall: distributed triaged
BCEWithLogitsLoss gives out nan with -inf logits module: loss triaged module: NaNs and Infs,module: loss triaged module: NaNs and Infs
RuntimeError: CUDA error: unspecified launch failure needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
Ensure inplace views are properly handled module: autograd triaged better-engineering,module: autograd triaged better-engineering
from_blob segfaults when given CUDA pointer module: crash module: cpp module: cuda triaged,module: crash module: cpp module: cuda triaged
Undefined reference to `cv::imread` module: build triaged has workaround,module: build triaged has workaround
ninja: build stopped: subcommand failed. module: build triaged,module: build triaged
Caching for autoregressive decoding of Transformer module: performance feature module: nn triaged oncall: transformer/mha,module: performance feature module: nn triaged oncall: transformer/mha
Compatibility with Raspberry Pi module: build feature triaged,module: build feature triaged
c10::scalar_to_tensor(...) uses should be audited for performance and type promotion impact high priority module: performance module: cuda triaged,high priority module: performance module: cuda triaged
IndexError: Input _features.0.weight is undefined! caffe2 triaged module: vision,caffe2 triaged module: vision
RandomSampler generator created in every iteration module: dataloader triaged,module: dataloader triaged
Wrong error is raised for property of nn.Module (again) module: nn triaged,module: nn triaged
cure unnecessary NaN values that arise from Â±âˆž arguments module: numerical-stability module: bootcamp module: autograd triaged module: NaNs and Infs,module: numerical-stability module: bootcamp module: autograd triaged module: NaNs and Infs
Flattening nn.Parameters while maintaining gradients from neural network forward pass module: nn module: optimizer triaged enhancement,module: nn module: optimizer triaged enhancement
Two independent future chains in PowerSGD cannot be kicked off asynchronously oncall: distributed triaged,oncall: distributed triaged
That define multiple models can influence the convolution parameter update. needs reproduction module: nn triaged,needs reproduction module: nn triaged
"File ""setup.py"", line 773:subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '8']' returned non-zero exit status 2 module: build module: cuda triaged",module: build module: cuda triaged
Model passed through queue hangs while loading in sub process  module: multiprocessing triaged,module: multiprocessing triaged
"While training the model, GPU util reaches 100% but no progress happends. needs reproduction module: cuda triaged module: deadlock",needs reproduction module: cuda triaged module: deadlock
Use rlibm for faster and more accurate floating point operations triaged enhancement module: bfloat16,triaged enhancement module: bfloat16
"torch.cuda.amp, example with 20% memory increase compared to apex/amp triaged module: amp (automated mixed precision)",triaged module: amp (automated mixed precision)
Annotating submodule with correct type fails scripting oncall: jit TSRootCause:InvalidCustomClass TSUsability,oncall: jit TSRootCause:InvalidCustomClass TSUsability
Turn deprecation warnings into errors in CI module: ci module: tests triaged,module: ci module: tests triaged
Bug in CosineAnnealingWarmRestarts module: optimizer triaged,module: optimizer triaged
Support scope in fx Node triaged module: fx,triaged module: fx
Improve error on missing/problematic Module member type annotation oncall: jit days TSRootCause:TypeChecking TSUsability,oncall: jit days TSRootCause:TypeChecking TSUsability
[JIT] Serialization forward compatibility tests oncall: jit weeks,oncall: jit weeks
Improve AssertionError for torch.nn.functional.pad 'replicate' module: bootcamp module: nn triaged enhancement better-engineering,module: bootcamp module: nn triaged enhancement better-engineering
jit tracer doesn't work with unflatten layer oncall: jit weeks,oncall: jit weeks
Pytorch mobile Vulkan API bug during call to ReLU oncall: mobile oncall: java,oncall: mobile oncall: java
Segfault in torch.bincount module: crash triaged,module: crash triaged
Enable torch.nn.modules.pooling typechecks during CI module: typing triaged,module: typing triaged
Bug report: INTERNAL ASSERT FAILED needs reproduction triaged,needs reproduction triaged
rank-0 gpu consume too high memory oncall: distributed triaged,oncall: distributed triaged
Internal symbols are leaking from torch.nn.functional triaged,triaged
Callbacks may not be automatically synchronized in a single NCCL future chain oncall: distributed triaged,oncall: distributed triaged
Use METH_FASTCALL protocol in Python bindings feature triaged module: pybind,feature triaged module: pybind
Write link in README.md module: docs triaged,module: docs triaged
Pytorch 1.5+: Kernel dies / Segmentation Fault with torch.cuda.mermory_allocated() and torch.cuda.mermory_reserved() module: cuda triaged module: regression,module: cuda triaged module: regression
Libtorch: Segmentation fault when running torch::jit::load oncall: jit weeks,oncall: jit weeks
 This error occurs occasionally during the run module: crash module: cudnn triaged,module: crash module: cudnn triaged
Impossible to run tests target with LibTorch as dependency inside a cocoapods. triaged module: ios,triaged module: ios
[RFC] DataLoader architecture updates and TarDataset implementation feature module: dataloader triaged,feature module: dataloader triaged
clip_grad_norm_ performance regression module: performance triaged module: norms and normalization,module: performance triaged module: norms and normalization
Inserting named tensor into other fails triaged enhancement module: named tensor,triaged enhancement module: named tensor
[doc] missing torch.pdist entry module: docs module: nn triaged small module: distance functions,module: docs module: nn triaged small module: distance functions
A few years after #701 and PyTorch is still using implicit __all__ imports. module: typing triaged module: pybind better-engineering module: codegen,module: typing triaged module: pybind better-engineering module: codegen
Add global gradcheck setting module: autograd triaged better-engineering,module: autograd triaged better-engineering
Bitcode enable for iOS module: binaries triaged module: ios,module: binaries triaged module: ios
PyTorch 1.7.1 on (macOS/python 3.9/conda) links libtorch_global_deps.dylib with libomp.dylib instead of libiomp5.dylib module: binaries module: build triaged module: macos,module: binaries module: build triaged module: macos
Can we add try-except for list/slice indices even when auto_collation is True? triaged enhancement module: data,triaged enhancement module: data
Apparent Memory Leak with torch.as_tensor module: memory usage triaged module: numpy quansight-nack module: tensor creation,module: memory usage triaged module: numpy quansight-nack module: tensor creation
Uninitialized variable was not detected in ASAN CI config high priority module: ci triaged,high priority module: ci triaged
"PyEval_SaveThread: the function must be called with the GIL held, but the GIL is released triaged module: pybind",triaged module: pybind
[pre-commit hook] fails on `BLK100 Black would make changes.` oncall: distributed triaged,oncall: distributed triaged
global_pruning costs memory after model is trained module: cuda module: memory usage triaged module: pruning,module: cuda module: memory usage triaged module: pruning
Backward hangs with DDP during training. oncall: distributed triaged module: ddp,oncall: distributed triaged module: ddp
how could I print the log in source code module: cpp module: logging triaged,module: cpp module: logging triaged
is this a typo in optimizer.pyi ? it says `statue` instead of `state` module: optimizer triaged,module: optimizer triaged
[question] How hard would it be to implement 4-bit precision training? module: internals triaged,module: internals triaged
add inverse cdf for Chi-square Distribution  module: distributions feature triaged,module: distributions feature triaged
TensorBoard SummaryWriter to remote storage unreasonably slow triaged module: tensorboard,triaged module: tensorboard
Bug when combining AMP with channels_last memory format module: cuda triaged module: memory format module: amp (automated mixed precision),module: cuda triaged module: memory format module: amp (automated mixed precision)
[FX] Update `placeholder` docs to better reflect the role of `node` and `target` triaged module: fx,triaged module: fx
Differentiate between objects with a true `Tensor` type and objects with a default `Tensor` type oncall: jit,oncall: jit
Methods that solve systems of linear equations are memory-inefficient (batch-wise broadcasting) module: memory usage triaged module: linear algebra,module: memory usage triaged module: linear algebra
Looking for a more convenient way to getter value of the upper/lower triangular matrix into 1D module: bc-breaking triaged module: numpy needs design topic: bc breaking,module: bc-breaking triaged module: numpy needs design topic: bc breaking
Torch abs op crashes on -128 int8 tensor with ASAN module: error checking triaged,module: error checking triaged
Bug in profiler on CI machines when profiling NCCL distributed calls oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Provide a mechanism to limit the workspace size of cudnn convolution module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
torch.utils.data.DistributedSampler allow uneven inputs module: dataloader triaged enhancement module: data,module: dataloader triaged enhancement module: data
[NNC] Some ops have type promotion logic which adds extra casts & does compute in different dtype than eager module: bootcamp triaged NNC,module: bootcamp triaged NNC
Investigate `torch.linalg.norm` performance module: performance triaged module: linear algebra,module: performance triaged module: linear algebra
Implement fill_out on complex triaged enhancement,triaged enhancement
softmax fail needs reproduction oncall: jit module: autograd module: nn module: viewing and reshaping,needs reproduction oncall: jit module: autograd module: nn module: viewing and reshaping
"Hello everyone, why does libtorch half precision float 16 leak memory?å¤§å®¶å¥½ï¼Œä¸ºä»€ä¹ˆlibtorchåŠç²¾åº¦float16ä¼šå†…å­˜æ³„æ¼ï¼Ÿ module: memory usage triaged",module: memory usage triaged
Unknown failure for test_reference_numerics_sinc_cpu_complex64  triaged module: complex module: trigonometric functions,triaged module: complex module: trigonometric functions
[RFC] Integrate DeepSpeed CUDA kernels feature triaged,feature triaged
NCCL Error when training with 2x 3090s. module: multi-gpu module: cuda triaged,module: multi-gpu module: cuda triaged
swa_utils.bn_update is too opinionated in how it calls the model feature module: optimizer triaged,feature module: optimizer triaged
Add List Type Data through add_hparam method in torch.utils.tensorboard.SummaryWriter feature triaged,feature triaged
Make the  forward function of a nn.Module and/or a certain function to work with only float32 when used with autocast module: nn triaged module: amp (automated mixed precision),module: nn triaged module: amp (automated mixed precision)
Automatically rerun tests with CUDA_LAUNCH_BLOCKING=1 when they fail with CUDA errors in CI module: cuda module: tests triaged,module: cuda module: tests triaged
Torch1.2 can't be downloaded! triaged,triaged
[Poll] Support DistributedDataParallel (DDP) in PyTorch C++ API (libtorch) oncall: distributed feature triaged module: ddp,oncall: distributed feature triaged module: ddp
CUDA error: illegal memory access Conv3d module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
Domain Specific Batch Normalization triaged enhancement,triaged enhancement
"When multiple GPUs run multiple processes, it is found that any process not running in GPU 0 will have some more memory (such as 200m) in GPU 0. What is the cause of this?ï¼ˆå¤šä¸ªGPUè·‘å¤šè¿›ç¨‹æ—¶å€™ï¼Œå‘çŽ°åªè¦ä¸åœ¨0å·GPUè·‘çš„è¿›ç¨‹éƒ½ä¼šåœ¨0å·GPUå¤šå‡ºä¸€äº›å†…å­˜(å¦‚200M)ï¼Œè¯·é—®è¿™æ˜¯ä»€ä¹ˆæƒ…å†µå¯¼è‡´çš„ï¼Ÿï¼‰ oncall: distributed",oncall: distributed
addmm with out= argument returns incorrect result high priority module: docs triaged module: partial aliasing module: linear algebra module: correctness (silent) module: structured kernels,high priority module: docs triaged module: partial aliasing module: linear algebra module: correctness (silent) module: structured kernels
is_non_overlapping_and_dense() does not error out for sparse tensors  module: sparse triaged,module: sparse triaged
Libtorch:  nvrtc: error: invalid value for --gpu-architecture (-arch) triaged,triaged
memory_format kwarg doesn't work on most factory functions module: bootcamp triaged enhancement module: memory format,module: bootcamp triaged enhancement module: memory format
cudnn cannot be pickled by cloudpickle module: cudnn module: cuda module: serialization triaged,module: cudnn module: cuda module: serialization triaged
inception_v3 is not symbolically traceable triaged module: fx,triaged module: fx
Weight_decay in torch.Adam module: docs module: optimizer triaged,module: docs module: optimizer triaged
[bug][libtorch] torch::load seeks to the start of the input stream before reading module: serialization triaged needs design,module: serialization triaged needs design
RTX 3090 setup vs 2x RTX 2080TI setup slower? Help.. module: performance triaged module: data parallel,module: performance triaged module: data parallel
test_inverse setup is flaky using MKL>=2020.1 on certain CPUs and fails on CUDA module: cuda triaged module: mkl shadow review,module: cuda triaged module: mkl shadow review
provide example for distributed training with iterative dataloaders  oncall: distributed module: dataloader triaged,oncall: distributed module: dataloader triaged
Remove remaining native:: references from non-native ATen module: internals triaged,module: internals triaged
Operator registration doesn't work with noexcept functions on some compilers (c10::guts::is_function_type rejects noexcept) module: internals triaged,module: internals triaged
Confused on project website pointer. module: docs triaged,module: docs triaged
Can sublist a nn.Sequential subclass module: nn triaged,module: nn triaged
"[bug] `torch.{sinh, cosh}`: Incorrect values for vectorized path module: cpu triaged module: sleef module: NaNs and Infs module: vectorization module: trigonometric functions",module: cpu triaged module: sleef module: NaNs and Infs module: vectorization module: trigonometric functions
Missing super().__init__() call in nn.Module module: nn triaged,module: nn triaged
[FR] loss reduction argument accepts None module: loss triaged,module: loss triaged
[NNC] Bugs Exposed in Binary Op Testing module: bootcamp triaged NNC,module: bootcamp triaged NNC
[feature request] Give `torch.cholesky` an optional fallback to test whether the tensor is positive definite triaged module: linear algebra,triaged module: linear algebra
torch.save() fails when attempting to save to mounted drives needs reproduction module: serialization triaged,needs reproduction module: serialization triaged
test_fs_pool fails needs reproduction module: multiprocessing module: tests triaged,needs reproduction module: multiprocessing module: tests triaged
[FR] torch.load should support loading directly to pinned/shared memory high priority module: multiprocessing feature module: serialization triaged,high priority module: multiprocessing feature module: serialization triaged
[FR] tensor ctors should support directly creating in shared memory high priority feature triaged module: tensor creation,high priority feature triaged module: tensor creation
segment fault for Image model training by four GPUs needs reproduction module: crash triaged,needs reproduction module: crash triaged
at::size documentation conflict module: docs module: cpp triaged,module: docs module: cpp triaged
CARU: A Content-Adaptive Recurrent Unit for the Transition of Hidden State in NLP feature module: nn module: rnn triaged,feature module: nn module: rnn triaged
C++ optimizer check for duplicate parameters module: optimizer triaged,module: optimizer triaged
Ambiguous RuntimeError raised when device of input PackedSequence does not match with nn.LSTM's device module: nn module: rnn triaged,module: nn module: rnn triaged
Inconsistent type of property stride of Conv1d and MaxPool1d module: nn triaged needs design module: ux,module: nn triaged needs design module: ux
SegmentationFault when pytorch is installed from source. module: crash module: build triaged,module: crash module: build triaged
[complex] torch.abs: does not match numpy  triaged module: complex module: numpy module: NaNs and Infs,triaged module: complex module: numpy module: NaNs and Infs
Add wheels for all cuda versions on pypi module: binaries triaged,module: binaries triaged
Vulkan Api Backend Build Error with GCC module: build triaged,module: build triaged
[master] new default signature: op=<ReduceOp.SUM: 0>from `op=<ReduceOp.SUM> oncall: distributed module: docs triaged,oncall: distributed module: docs triaged
compile error immintrin.h module: build triaged,module: build triaged
After updating to Xcode 12 and LibTorch to 1.7.0. Facing issue when running unit test. oncall: jit oncall: mobile module: ios module: arm,oncall: jit oncall: mobile module: ios module: arm
"Qt 5.14.2, libtorch1.7.0 cuda10.2ï¼ŒError after using header file triaged module: static linking",triaged module: static linking
cudnn convolution modifies the input Tensor metadata inplace when it tries to `.resize_()` it module: cudnn module: convolution triaged module: memory format,module: cudnn module: convolution triaged module: memory format
pin_memory=True in DataLoader converts a tuple to list automatically module: dataloader triaged,module: dataloader triaged
conv_transpose3d returns different result when the input and kernel are mkldnn tensors high priority module: dependency bug module: convolution triaged module: mkldnn,high priority module: dependency bug module: convolution triaged module: mkldnn
"Error occurred when trying to call ""torch::jit::load""method for loading "".pt"" file on IOS oncall: jit oncall: mobile",oncall: jit oncall: mobile
hangs indefinitely at os.waitpid() high priority module: docs module: multiprocessing module: error checking triaged module: deadlock,high priority module: docs module: multiprocessing module: error checking triaged module: deadlock
Management of Gpu memory to avoid memory errors feature module: memory usage triaged,feature module: memory usage triaged
Add Go to high level interface feature triaged,feature triaged
Custom exception for out of memory high priority feature triaged better-engineering,high priority feature triaged better-engineering
[FR] bool tensor should support basic arithmetics triaged module: numpy module: type promotion module: boolean tensor,triaged module: numpy module: type promotion module: boolean tensor
torch.float128 datatype feature triaged module: numpy,feature triaged module: numpy
[JIT] Constant Propagation Shouldn't Run on Values that Escape the Graph oncall: jit days,oncall: jit days
[RFC] CUDA-aware future for distributed oncall: distributed triaged module: rpc module: c10d,oncall: distributed triaged module: rpc module: c10d
pytorch is not linked with support for cuda devices module: build module: cuda triaged,module: build module: cuda triaged
"[doc] Error ""You've reached a dead end"" when opening torch.__config__ in the docs module: docs triaged module: doc infra",module: docs triaged module: doc infra
Batched inplace mm changes stride when out size is correct triaged module: memory format module: linear algebra module: correctness (silent),triaged module: memory format module: linear algebra module: correctness (silent)
"Pytorch streams API don't execute concurrently, However Same code in CUDA does.  module: performance module: cuda triaged",module: performance module: cuda triaged
torch.multinomial example is incorrect module: distributions module: docs triaged,module: distributions module: docs triaged
torch.eye(d) is slow and hogs cpu for d >= 182 module: performance good first issue module: cpu triaged module: multithreading,module: performance good first issue module: cpu triaged module: multithreading
"RuntimeError: ""threshold_cpu"" not implemented for 'Half' module: cpu triaged enhancement module: half",module: cpu triaged enhancement module: half
torch.save writes file even when pickle fails module: serialization triaged,module: serialization triaged
Softplus differs depending on number of elements in tensor module: nn triaged,module: nn triaged
Modifying values() tensor of COO tensor requiring grad throws an odd error message module: sparse module: autograd triaged,module: sparse module: autograd triaged
Document torch.distributed.destroy_process_group() oncall: distributed module: docs triaged module: c10d,oncall: distributed module: docs triaged module: c10d
"""Found no NVIDIA driver"" produces too much log module: cuda triaged",module: cuda triaged
[JIT] Contained Module Attributes should be recursively compiled oncall: jit,oncall: jit
Support for oneDNN / MKL-DNN on AArch64 triaged module: mkldnn module: arm,triaged module: mkldnn module: arm
[libtorch] Build shared library with libtorch and compile a static library triaged module: static linking,triaged module: static linking
nn.functional.interpolate backward in fp16 is extremely slow module: performance module: nn module: cuda triaged module: half module: interpolation,module: performance module: nn module: cuda triaged module: half module: interpolation
More robust list comprehension  oncall: jit triaged weeks,oncall: jit triaged weeks
quantization - document get_default_qconfig oncall: quantization triaged,oncall: quantization triaged
[JIT][PE] Profiling Executor should re-profile `DifferentiableGraph` oncall: jit,oncall: jit
[JIT] Constant folding on no-op `aten::mul` oncall: jit,oncall: jit
[dataloader] Worker threads to print the signal they received before they die module: dataloader triaged enhancement,module: dataloader triaged enhancement
CPU Tensor with Python MP Freezing in Docker Container module: multiprocessing triaged module: docker,module: multiprocessing triaged module: docker
[JIT] Saving and Loading JIT Model Does Not Preserve __dir__ oncall: jit days,oncall: jit days
FAILED: test_api/CMakeFiles/test_api.dir/dataloader.cpp.o  module: build triaged,module: build triaged
"[NNC] Improve ""UNSUPPORTED DTYPE"" error messages  oncall: jit module: bootcamp NNC",oncall: jit module: bootcamp NNC
Missing -fopenmp when used in torchvision module: build module: cpp-extensions triaged enhancement module: build warnings module: openmp,module: build module: cpp-extensions triaged enhancement module: build warnings module: openmp
[complex] torch.{exp}: does not match numpy triaged module: complex module: numpy module: correctness (silent),triaged module: complex module: numpy module: correctness (silent)
Inconsistent complex results with NumPy when computing non-positive power of 0 triaged module: complex module: numpy module: NaNs and Infs,triaged module: complex module: numpy module: NaNs and Infs
Torch DDP seems not prefer IPv6 over IPv4? high priority triage review oncall: distributed triaged,high priority triage review oncall: distributed triaged
GPU Vendor-Agnosticism via Vulkan feature module: cuda module: rocm triaged module: vulkan,feature module: cuda module: rocm triaged module: vulkan
"build pytorch from source on ubuntu, building error from fbgemm::SparseAdaGradSignature module: build module: collect_env.py triaged",module: build module: collect_env.py triaged
Is nn.Conv2d equivalent with Unfold + Matrix Multiplication + Fold ? needs reproduction module: convolution triaged module: correctness (silent),needs reproduction module: convolution triaged module: correctness (silent)
Linear algebra GPU backend tracking issue [magma/cusolver/cublas] high priority module: performance module: cuda triaged module: linear algebra,high priority module: performance module: cuda triaged module: linear algebra
Unknown DispatchKey when indexing tensor with named dimensions triaged module: dispatch module: named tensor,triaged module: dispatch module: named tensor
Function request: Sparse matrix inverse module: sparse triaged module: linear algebra function request,module: sparse triaged module: linear algebra function request
LibTorch cannot load PyTorch exported model module: docs module: serialization triaged,module: docs module: serialization triaged
Hash mismatch for METADATA file module: binaries triaged,module: binaries triaged
sparse filter layers (more specifically convolutions) module: sparse triaged needs research function request,module: sparse triaged needs research function request
Code becomes more than x20 slower after upgrading torch version module: performance triaged,module: performance triaged
The speed of pytorch with cudatoolkit 11.0 is slower than cudatoolkit 10.2 module: performance module: cuda triaged,module: performance module: cuda triaged
amp for custom op triaged module: custom-operators module: amp (automated mixed precision),triaged module: custom-operators module: amp (automated mixed precision)
Memory leak in nn.MaxPool2d layer when run on iOS module: memory usage triaged module: ios,module: memory usage triaged module: ios
"Get forward output, grad, hessian all at once module: autograd triaged enhancement",module: autograd triaged enhancement
"ONNX export of a scripted submodule fails with ""Modules that are called during a trace must be registered as submodules of the thing being traced"" oncall: jit module: onnx onnx-triaged",oncall: jit module: onnx onnx-triaged
NCCL 2.7.8 errors on PyTorch distributed process group creation oncall: distributed triaged,oncall: distributed triaged
`.modules()` is not callable in TorchScript when any submodule is a module interface type (i.e. a class decorated by `@torch.jit.interface`) oncall: jit,oncall: jit
QAT with DDP should have documentation oncall: quantization triaged,oncall: quantization triaged
Incorrect output loss value under specific CUDA version high priority module: dependency bug module: numerical-stability module: cuda triaged module: linear algebra module: tf32,high priority module: dependency bug module: numerical-stability module: cuda triaged module: linear algebra module: tf32
Factorial & Binomial Coefficient triaged module: numpy function request,triaged module: numpy function request
Error during distributed training needs reproduction oncall: distributed triaged,needs reproduction oncall: distributed triaged
"DDP error, when I use ""if"" branch in forward function. module: autograd triaged module: ddp",module: autograd triaged module: ddp
Eliminate redundant device guards in generic dispatch key kernel wrappers module: performance triaged module: dispatch,module: performance triaged module: dispatch
" Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2 module: serialization triaged",module: serialization triaged
Make libtorch modular module: binaries module: build triaged module: selective build,module: binaries module: build triaged module: selective build
Cudnn header files should be copied into build package as well  module: windows triaged windows-triaged,module: windows triaged windows-triaged
Conda package should install libtorch in the standard location module: binaries module: build triaged,module: binaries module: build triaged
GPU acceleration for Apple's M1 chip? module: performance triaged,module: performance triaged
DCNv2 Layers module: nn triaged function request,module: nn triaged function request
pyre not able to infer the type of torch.jit.script oncall: jit,oncall: jit
ARM Mac 16-core Neural Engine feature triaged,feature triaged
How to install Pytorch on AIX7.2 without internet access? module: build triaged,module: build triaged
Pytorch 1.7.0 with cuda 11.1.1 and cudnn 8.0.5 module: cuda oncall: releng module: ci triaged enhancement,module: cuda oncall: releng module: ci triaged enhancement
DataParallel support for scripted modules in C++ oncall: jit triaged,oncall: jit triaged
error: reference to __host__ function 'parallel_for<thrust::cuda_cub::for_each_f...' in __host__ __device__ function module: build module: cuda triaged,module: build module: cuda triaged
Allow add_embedding to have dict for metadata triaged module: tensorboard,triaged module: tensorboard
Input dimension check for `torch.gather` module: error checking triaged,module: error checking triaged
Error while trying to load a pretrained ResNet34 VTN (Video Transformer Network) model in Android triaged oncall: mobile,triaged oncall: mobile
PyTorch1.3.1 Can not using namespace torch::indexing module: cpp triaged,module: cpp triaged
Can we apply Weight normalization hook to a method other than `forward`? module: nn triaged enhancement module: norms and normalization,module: nn triaged enhancement module: norms and normalization
torch.nn.Module.apply cyclical references unbounded module: nn triaged,module: nn triaged
Failed to get generated_cpp list module: build triaged,module: build triaged
Handling multiple large-scale datasets efficiently  module: dataloader triaged,module: dataloader triaged
Minumul LR is never reached in  module: optimizer triaged,module: optimizer triaged
Ninja-built CUDAExtension build process ignores changed #include dependencies module: build triaged,module: build triaged
per channel observer to work for weights of groupwise conv transpose oncall: quantization low priority triaged,oncall: quantization low priority triaged
ReduceLROnPlateau fails for negative input module: docs module: optimizer triaged,module: docs module: optimizer triaged
Hackable python-only autograd engine module: autograd triaged,module: autograd triaged
[fx] scripting a model with tensor list as input fails triaged module: fx,triaged module: fx
Reciprocal and reciprocal square root instructions are too inaccurate on ARM64 triaged module: correctness (silent) module: arm,triaged module: correctness (silent) module: arm
"thread blocked when moving a tensor from GPU to CPU, by calling the function .cpu() in pytorch. This kind of block can be stop by any window event like mouse moving/clicking or keyboard pressing. module: cuda module: cpu triaged",module: cuda module: cpu triaged
how to use torch.utils.checkpoint + gru with variable length sequence? module: rnn triaged,module: rnn triaged
unbalanced gpu memory when using DistributedDataParallel oncall: distributed triaged,oncall: distributed triaged
[Discussion] Use the unicode variant of the Windows API module: windows module: internals triaged,module: windows module: internals triaged
torch.vmap giving INTERNAL ASSERT FAILED error triaged module: vmap,triaged module: vmap
FX quantization: we should preserve original model class name through the quantization passes oncall: quantization low priority triaged,oncall: quantization low priority triaged
Conv1D formula in docs is wrong module: docs module: nn triaged,module: docs module: nn triaged
My demo app uses other package name will crash but org.pytorch.demo oncall: mobile,oncall: mobile
ConvTranspose1d groups=channels is very slow!!! module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
Quantized modules should properly implement __getstate__ and __setstate__ (copy.deepcopy doesn't work on quantized model) oncall: quantization triaged,oncall: quantization triaged
ProcessGroupNCCL NCCL lib version mismatch module: binaries oncall: distributed triaged,module: binaries oncall: distributed triaged
Error: invalid use of register module: build triaged,module: build triaged
DDP doesn't work with retain_graph = True high priority triage review oncall: distributed triaged,high priority triage review oncall: distributed triaged
[JIT] Alias Annotations for Native Schema List Ops Is Subtly Wrong high priority oncall: jit days weeks,high priority oncall: jit days weeks
[RFC] Add test execution time analysis CI workflow module: ci triaged,module: ci triaged
torch.save has a maximum size regardless of RAM module: docs module: serialization triaged function request,module: docs module: serialization triaged function request
[feature request] Show warning if optimizer.zero_grad() was not called before optimizer.step() module: nn module: optimizer triaged module: ux,module: nn module: optimizer triaged module: ux
Grad strides do not match bucket view strides. oncall: distributed module: memory format module: ddp,oncall: distributed module: memory format module: ddp
segfault during shutdown with torch1.7 module: dependency bug module: crash triaged module: pybind module: third_party,module: dependency bug module: crash triaged module: pybind module: third_party
Allow all torch.nn modules to accept arbitrary batch dimensions module: nn triaged enhancement module: batching module: ux,module: nn triaged enhancement module: batching module: ux
ctx.save_for_backward doesn't save torch.Tensor subclasses fully module: docs module: autograd triaged enhancement needs design module: __torch_function__,module: docs module: autograd triaged enhancement needs design module: __torch_function__
trying to Installing pytorch on python3.9 via pip results in an non-descript error high priority module: binaries oncall: releng triaged,high priority module: binaries oncall: releng triaged
THCudaShutdown should be called before THCState_free module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
Pytorch 1.7/Cuda 11.1 binaries module: binaries module: cuda triaged,module: binaries module: cuda triaged
Training slows down and memory usage increases when upgrading from PyTorch 1.6 to 1.7 module: performance triaged module: regression module: amp (automated mixed precision),module: performance triaged module: regression module: amp (automated mixed precision)
"[types] torch.utils.data.{Dataset, Sampler} are not Sized triaged enhancement needs research module: data",triaged enhancement needs research module: data
Activation functions for complex tensors module: nn triaged module: complex function request,module: nn triaged module: complex function request
Tensor subclasses lose type when pickling module: serialization triaged enhancement module: __torch_function__,module: serialization triaged enhancement module: __torch_function__
"Change ""_next_index()"" in DataLoader to a public and stable API module: dataloader triaged needs research",module: dataloader triaged needs research
torch.autograd.backward() fails to sync with other stream module: autograd triaged,module: autograd triaged
Fix the way imports are done to be more correct for static type checkers module: typing triaged enhancement,module: typing triaged enhancement
"torch.sum(tensor(2.), dim=0) (and probably other reduction functions) doesn't make sense module: docs triaged module: reductions",module: docs triaged module: reductions
[JIT] scripting torchaudio.transforms.MFCC() error  oncall: mobile,oncall: mobile
Yolov5 detect.py(ingerence) Error:Torch.nn.modules.module.ModuleAttributeError: 'Hardswish' object has no attribute 'inplace' module: nn module: pickle triaged,module: nn module: pickle triaged
Pooling code does not allow sliding window starting in right padded region triaged module: pooling,triaged module: pooling
Jacobians computed by autograd.functional.jacobian with compute_graph sometimes set requires_grad True module: autograd triaged,module: autograd triaged
Authentication for RPC oncall: distributed triaged module: rpc module: tensorpipe,oncall: distributed triaged module: rpc module: tensorpipe
How to use clang as a cuda compiler instead of nvcc? module: build triaged enhancement,module: build triaged enhancement
CUDA out of memory when using torch.load module: cuda triaged,module: cuda triaged
Support deleting a parameter/buffer by name feature module: nn triaged module: ux,feature module: nn triaged module: ux
[META] DO NOT DELETE THIS LABEL triaged open source fb-exported Merged,triaged open source fb-exported Merged
Mixing Numpy's arrays and PyTorch tensors triaged module: numpy needs design module: ux,triaged module: numpy needs design module: ux
C++ Optimizer: remove warning on Optimizer::size method module: optimizer triaged better-engineering,module: optimizer triaged better-engineering
simple v *=  v_scale error module: autograd triaged actionable,module: autograd triaged actionable
Specify input dimensions where they are not obvious module: docs triaged,module: docs triaged
[cudatoolkit 11.0] segfaults module: build module: cuda triaged,module: build module: cuda triaged
Compile Pytorch with MAGMA Issue needs reproduction module: build triaged,needs reproduction module: build triaged
About 2 minor bug fixes on CUDA macOSX 10.13.6 module: build module: cuda triaged module: macos module: nnpack,module: build module: cuda triaged module: macos module: nnpack
"DataLoader gives ""Broken pipe"" error on Linux platform module: multiprocessing module: dataloader triaged",module: multiprocessing module: dataloader triaged
Clearer error messages for 'different devices' error in the RNN module triaged better-engineering,triaged better-engineering
Unable to compile from code v1.6.0 module: build triaged,module: build triaged
How to define a new data type in  native_functions.yaml? module: internals triaged,module: internals triaged
Memory leak when creating new tensors inside nn.DataParallel on multiple GPUs oncall: distributed triaged,oncall: distributed triaged
Quantization - we need a better solution for tracking quantization backend settings in a model oncall: quantization triaged,oncall: quantization triaged
torch.utils.tensorboard.SummaryWriter.add_embedding fails for some label_img sizes triaged module: tensorboard,triaged module: tensorboard
PROBLEM WITH INSTALLATION PYTORCH FROM SOURCE module: build triaged,module: build triaged
The documentation for c10::Dict is completely empty. module: docs module: internals module: cpp triaged,module: docs module: internals module: cpp triaged
A random split function that return the datasets following specific target (label) distribution. feature triaged module: data,feature triaged module: data
Excluding image processing operators due to no opencv module: build triaged,module: build triaged
Future returned by RPC should print a warning message on destruction if it's not waited oncall: distributed module: bootcamp triaged enhancement module: rpc,oncall: distributed module: bootcamp triaged enhancement module: rpc
Trying to compile PyTorch for sm_30 fails with `error: identifier â€œ__ldgâ€ is undefined` module: build module: cuda triaged,module: build module: cuda triaged
Autograd support for the tensor multiplication of sparse tensors module: sparse module: autograd triaged function request,module: sparse module: autograd triaged function request
Loss functions for complex tensors  module: nn triaged module: complex complex_autograd,module: nn triaged module: complex complex_autograd
error in bazel build //... module: build triaged module: bazel,module: build triaged module: bazel
torch.concat doesn't raise an error in a quantized model oncall: quantization low priority triaged,oncall: quantization low priority triaged
Clean up request_callback_no_python.cpp triaged better-engineering module: rpc,triaged better-engineering module: rpc
Exception raised in rpc_async context is silently handled oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
PyTorch NaN behavior and API design triaged module: numpy module: NaNs and Infs needs design module: ux,triaged module: numpy module: NaNs and Infs needs design module: ux
`torch.igamma` error and gives wrong results on float64 ROCm high priority module: rocm triaged module: correctness (silent),high priority module: rocm triaged module: correctness (silent)
[Request] Batched Dataset->DataLoader interface feature module: dataloader triaged,feature module: dataloader triaged
ModuleNotFoundError when installing PyTorch via pip on aarch64 environment module: binaries triaged module: arm,module: binaries triaged module: arm
Batchnorm support for tracking buffer statistics when using gradient accumulation  triaged enhancement needs research module: norms and normalization,triaged enhancement needs research module: norms and normalization
Cannot script when a static method or cross module function calls custom op oncall: jit module: cpp-extensions internals module: torchbind,oncall: jit module: cpp-extensions internals module: torchbind
cannot call rpc.init_rpc twice within a single process high priority oncall: distributed triaged module: rpc,high priority oncall: distributed triaged module: rpc
Advanced indexing: allow combining Boolean & integer index triaged module: numpy module: advanced indexing function request,triaged module: numpy module: advanced indexing function request
[META] Assert that expected libraries appear in libtorch triaged module: static linking enhancement,triaged module: static linking enhancement
C++ API: Training and inference of torchscript modules on multiple GPU oncall: jit,oncall: jit
send_object/recv_object APIs for c10d oncall: distributed triaged module: c10d,oncall: distributed triaged module: c10d
Custom ops get stuck in multiprocess data loader under certain environments module: dataloader triaged module: deadlock module: custom-operators,module: dataloader triaged module: deadlock module: custom-operators
Make Undefined reprensentable in DispatchKeySet.  triaged enhancement,triaged enhancement
"GPU memory leak when registering a forward hook with ""self"" access module: cuda module: memory usage triaged has workaround",module: cuda module: memory usage triaged has workaround
"Code asserts when register new aten operations' implementation thru ""c10::RegisterOperators::op"" API.  module: internals triaged",module: internals triaged
[Feature] Imbalanced batch scattering in th.nn.DataParallel triaged enhancement module: data parallel,triaged enhancement module: data parallel
"Reduce ""reserved"" memory by PyTorch. module: cuda module: memory usage triaged",module: cuda module: memory usage triaged
JIT: error in LSTM with `flatten_parameters` oncall: jit,oncall: jit
Complex Number support for torch.nn modules module: nn triaged module: complex,module: nn triaged module: complex
[JIT] Inconsistent behavior of torchScript parser in python and c++ oncall: jit,oncall: jit
[chore] Test pybind11 2.6.0 (RC 2?) module: build module: ci triaged enhancement,module: build module: ci triaged enhancement
torch.distributions.half_normal.HalfNormal.cdf returns negative values module: distributions triaged,module: distributions triaged
A race problem of JIT cpp extensions in distributed setting oncall: jit days,oncall: jit days
Add distributed examples into PyTorch CI tests oncall: distributed module: bootcamp module: tests triaged,oncall: distributed module: bootcamp module: tests triaged
Remove header declarations for CPUType/TypeDefault module: internals triaged,module: internals triaged
"Compile error when use c10::RegisterOperators::Options::kernel with the return type ""std::tuple<Tensor&, Tensor&>"" module: internals triaged",module: internals triaged
ProcessGroup::Work API changes module: bc-breaking feature triaged module: c10d topic: bc breaking,module: bc-breaking feature triaged module: c10d topic: bc breaking
test_distributed_* does not show error details from the subprocess oncall: distributed module: tests triaged better-engineering pt_distributed_rampup,oncall: distributed module: tests triaged better-engineering pt_distributed_rampup
test_distributed_* does not work with run_test.py -i option oncall: distributed module: tests triaged better-engineering pt_distributed_rampup,oncall: distributed module: tests triaged better-engineering pt_distributed_rampup
Enable named tensor inputs to `torch.linalg.norm` triaged enhancement module: named tensor,triaged enhancement module: named tensor
"""distributed"" NCCL tests fail when having more than 3 GPUs oncall: distributed module: tests triaged module: nccl module: ddp",oncall: distributed module: tests triaged module: nccl module: ddp
ubuntu16.04+vscode compile libtorch error module: build triaged,module: build triaged
"Support ""symmetric"" reflection padding module: nn triaged module: numpy function request module: padding module: tensorflow",module: nn triaged module: numpy function request module: padding module: tensorflow
torch.mode when input has nans module: docs triaged module: numpy module: NaNs and Infs module: sorting and selection module: reductions,module: docs triaged module: numpy module: NaNs and Infs module: sorting and selection module: reductions
Use better tempfile creation mechanism to avoid skip windows test module: tests triaged,module: tests triaged
pytorch not uploading data to tensorboard page triaged module: tensorboard,triaged module: tensorboard
Better support for operators that return (named) tuples of tensors triaged module: numpy module: deprecation module: ux,triaged module: numpy module: deprecation module: ux
redesign of Dropout feature module: nn triaged,feature module: nn triaged
"torch.cuda.amp!  when I use @autocast() on DCN(DeformConv), the error ""RuntimeError:expect scalar type Float but Half "" module: cuda triaged module: amp (automated mixed precision)",module: cuda triaged module: amp (automated mixed precision)
BucketSampler for easy variable-length input batching feature module: dataloader triaged,feature module: dataloader triaged
Squared 2-norm pdist (as available in SciPy / Faiss) triaged module: numpy function request module: norms and normalization module: distance functions,triaged module: numpy function request module: norms and normalization module: distance functions
[discussion] In-place gradient (grad_input) computation for better memory utilisation module: autograd module: memory usage triaged needs research,module: autograd module: memory usage triaged needs research
Error with DistributedDataParallel with specific model high priority triage review oncall: distributed triaged,high priority triage review oncall: distributed triaged
Deprecate spmm and dsmm functions module: sparse triaged open source module: linear algebra module: deprecation,module: sparse triaged open source module: linear algebra module: deprecation
No improvement gain between sm_86 (cuda 11.1) and sm_80 (cuda 11.0) on 3090 or 3080 GPUs. module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
Ability to disable cusolver module: build module: cuda triaged module: linear algebra,module: build module: cuda triaged module: linear algebra
Complex backward returns NaN values high priority module: cuda triaged module: complex complex_autograd,high priority module: cuda triaged module: complex complex_autograd
Getting this error while installing torch and torchvision! needs reproduction triaged,needs reproduction triaged
seeding does not work when I initialize a linear model even if I do not use it in the code module: nn triaged,module: nn triaged
"Policy CMP0012   is not set, CMake, Building For C++ With PyTorch CUDA module: build triaged better-engineering",module: build triaged better-engineering
test_cat_cuda (__main__.TestTensorExprFuser) fails module: tests triaged shadow review,module: tests triaged shadow review
"Spawn wrapper should catch BaseException, not Exception module: multiprocessing triaged better-engineering",module: multiprocessing triaged better-engineering
"In PyTorch Tutorials, RuntimeError: CUDA error: out of memory happen module: memory usage triaged",module: memory usage triaged
NCCL watchdog thread should log warnings about long-running GPU operations instead of silently hanging module: bootcamp triaged pt_distributed_rampup module: c10d,module: bootcamp triaged pt_distributed_rampup module: c10d
TestXNNPACKConv1dTransformPass.test_conv1d_with_relu_fc takes 2+ min to finsh module: ci module: tests triaged,module: ci module: tests triaged
TestDataLoader.test_proper_exit takes 2.5min to finish module: dataloader module: ci module: tests triaged,module: dataloader module: ci module: tests triaged
Support undispatched ops in codegen triaged module: codegen,triaged module: codegen
"Error installing on source: ""src.cxx:1:10: fatal error: glog/stl_logging.h: No such file or directory"" module: build triaged",module: build triaged
Custom attention triaged oncall: transformer/mha,triaged oncall: transformer/mha
Padding mode for ConvTransposeNd module: convolution triaged function request,module: convolution triaged function request
Reproducibility breaks down with weighted Cross Entropy loss module: loss module: cuda triaged module: numerical-reproducibility module: determinism,module: loss module: cuda triaged module: numerical-reproducibility module: determinism
Backpropagation for sparse matrix indexing is problematic (colab provided) module: sparse triaged,module: sparse triaged
Improve distributed documentation for NCCL_BLOCKING_WAIT  oncall: distributed triaged pt_distributed_rampup module: ddp,oncall: distributed triaged pt_distributed_rampup module: ddp
with torch.cuda.amp.autocast() get out of memory error when using with torch.no_grad() during validation module: docs module: autograd triaged module: amp (automated mixed precision),module: docs module: autograd triaged module: amp (automated mixed precision)
Add support for reading the whole file in from_file module: serialization triaged enhancement module: numpy,module: serialization triaged enhancement module: numpy
Unify matrix multiplications operations module: sparse triaged open source module: linear algebra,module: sparse triaged open source module: linear algebra
Do we have plan to offer C++ binding for prune related features.  module: cpp triaged enhancement module: pruning,module: cpp triaged enhancement module: pruning
The same function can have different signatures in  the torch and torch.nn.functional namespaces module: nn triaged,module: nn triaged
Error with DataParallel and dataclass triaged module: data parallel,triaged module: data parallel
Support torch.mean for BoolTensors and other integer tensor inputs (without manual upcasting and hopefully without hidden upcasting) triaged module: numpy module: type promotion module: reductions,triaged module: numpy module: type promotion module: reductions
"Traceback for ""Warning: Mixed memory format inputs detected while calling the operator."" triaged module: memory format",triaged module: memory format
F.conv2d() causes RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR  module: cudnn triaged,module: cudnn triaged
`defineMethodsInModule` no longer exists module: docs triaged,module: docs triaged
resize_(0) is very expensive module: performance triaged,module: performance triaged
Complex Number support for distributed oncall: distributed module: bootcamp triaged module: complex pt_distributed_rampup module: c10d,oncall: distributed module: bootcamp triaged module: complex pt_distributed_rampup module: c10d
Low shared memory module: dataloader triaged,module: dataloader triaged
Better documentation of vec256 API module: docs triaged module: vectorization internals,module: docs triaged module: vectorization internals
Complex and real results do not agree when computing reciprocal or pow(-1) of 0 triaged module: complex,triaged module: complex
"[fx] symbolic trace ""is None"" and ""is not None"" checks triaged module: fx",triaged module: fx
[FX] Configurability of nested module representation triaged module: fx,triaged module: fx
[FX] Allow customization of the behavior of Proxy triaged module: fx,triaged module: fx
fx graph mode quantization tutorials oncall: quantization low priority triaged,oncall: quantization low priority triaged
Why is RTX3080 slower than RTX2020-Ti? module: performance module: cuda triaged,module: performance module: cuda triaged
Build Failed Ubuntu Cuda. CUDAHooks.cpp:97:15: error: â€˜struct cudaPointerAttributesâ€™ has no member named â€˜typeâ€™ return attr.type == cudaMemoryTypeHost; module: build triaged,module: build triaged
Distutils Error in torch.hub Load() triaged module: hub,triaged module: hub
Resolving overloads and default arguments from within __torch_function__ feature triaged module: __torch_function__,feature triaged module: __torch_function__
Android: allow preallocation of output buffers feature triaged module: android oncall: mobile,feature triaged module: android oncall: mobile
Can't initialize NCCL/GLOO process group if default process group is MPI high priority triage review oncall: distributed module: bootcamp triaged module: nccl pt_distributed_rampup,high priority triage review oncall: distributed module: bootcamp triaged module: nccl pt_distributed_rampup
Why does NLLLoss and CrossEntropyLoss require type long? feature triaged,feature triaged
"[FX] Make tracer return a Graph, not a GraphModule triaged module: fx",triaged module: fx
[JIT] JIT Autograd is saving more results than are necessary for bprop like the result of RELU when it is an intermediate oncall: jit,oncall: jit
Tensorboard makes logger handlers (except the 1st one) DISAPPEAR ! triaged module: tensorboard,triaged module: tensorboard
"torch.nonzero(t, as_tuple=...) does not work with the JIT because the as_tuple signatures are not exposed properly oncall: jit triaged module: numpy module: python array api",oncall: jit triaged module: numpy module: python array api
Build LibTorch for cpu using OpenBLAS: Had to manually remove a path in Caffe2Targets.cmake module: build triaged,module: build triaged
Combining add_scalar with add_hparams with different frequencey  triaged module: tensorboard,triaged module: tensorboard
Backward for sparse tensor item select does not work module: sparse module: autograd triaged,module: sparse module: autograd triaged
CuDNN version not found module: build module: cudnn triaged,module: build module: cudnn triaged
Retire usages of CUDA_tensor_apply helpers in ATen module: internals module: cuda triaged better-engineering,module: internals module: cuda triaged better-engineering
Using operator[] in GenericPackedTensorAccessor impossible in cpu code in .cu files (tensor on cpu) module: build triaged,module: build triaged
Use libtorch for online inference: an illegal memory access was encountered awaiting response (this tag is deprecated) module: cuda triaged,awaiting response (this tag is deprecated) module: cuda triaged
Module __call__ typing module: typing triaged,module: typing triaged
Autocast but there are no kernels registered for this dispatch key. The operator is pï¿½8ßŽU Aborted (core dumped) triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
torch.nn.functional.one_hot should gracefully skip negative and out-of-range indices module: nn triaged enhancement,module: nn triaged enhancement
Memory leak when using share_memory_ on cuda device module: memory usage triaged,module: memory usage triaged
Blocked version of Cholesky backward module: autograd triaged module: linear algebra,module: autograd triaged module: linear algebra
Adaptive usage of memory during training inference feature module: nn module: cuda triaged needs research,feature module: nn module: cuda triaged needs research
"Training fast with small dataset, slow with large dataset needs reproduction module: performance triaged module: data parallel module: amp (automated mixed precision)",needs reproduction module: performance triaged module: data parallel module: amp (automated mixed precision)
Functional interface for optimizers module: cpp module: optimizer triaged,module: cpp module: optimizer triaged
FX should preserve type annotations and not break TorchScript-ability oncall: jit triaged module: fx,oncall: jit triaged module: fx
Adding @torch.no_grad() to forward () causes undefined value in torch.jit.script()  oncall: jit,oncall: jit
Staged backend boxed fallback (per-operator precomputation / precompute) triaged module: dispatch,triaged module: dispatch
DataLoader with cv2 and some numpy/cv2 import order causes workers to not work high priority needs reproduction module: dataloader triaged module: multithreading module: regression,high priority needs reproduction module: dataloader triaged module: multithreading module: regression
"In profiler, record_function event's total CPU time can be less than the contained ops triaged oncall: profiler module: correctness (silent)",triaged oncall: profiler module: correctness (silent)
Mention accessor/data_ptr for raw memory access in Libtorch index API document and discuss performance implications module: performance module: cpp triaged,module: performance module: cpp triaged
Multiple torch.load in one file high priority module: serialization triaged module: regression,high priority module: serialization triaged module: regression
Inconsistent wheel name in https://download.pytorch.org/whl/torch_stable.html module: build triaged,module: build triaged
Boolean indexing of an ndarray with a torch.tensor mask breaks for size=1  triaged module: numpy module: advanced indexing,triaged module: numpy module: advanced indexing
Standardized Distributions module: distributions triaged,module: distributions triaged
(prototype) Graph Mode Dynamic Quantization on BERT failure on quantize_dynamic_jit(...) call triaged oncall: mobile,triaged oncall: mobile
Pytorch report INTERNAL ASSERT FAILED at ..\\torch\\csrc\\jit\\ir.cpp:1529 when use torch.jit.script to convert to model oncall: jit,oncall: jit
Compilation errors on power-pc module: build triaged module: POWER,module: build triaged module: POWER
about benchmark issue module: cudnn triaged,module: cudnn triaged
Custom Datatypes in Tensors triaged module: numpy needs research,triaged module: numpy needs research
torch rpc cannot handle UnsupportedNodeError exception oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
"torch.distributed launch.py is hanged.  (pid, sts) = os.waitpid(self.pid, wait_flags) oncall: distributed triaged",oncall: distributed triaged
Median / quantile / mode / rank / percentile pooling module: performance triaged module: sorting and selection module: pooling,module: performance triaged module: sorting and selection module: pooling
"Unite/unify tensor.unfold with F.unfold and make them more performant (zero-copy or little-copy with stride tricks, as in NumPy) module: nn triaged module: ux",module: nn triaged module: ux
"Support unfold for integral types (long, byte etc) tensors feature module: nn triaged actionable",feature module: nn triaged actionable
Tensordot does not support Bool triaged enhancement module: numpy module: boolean tensor,triaged enhancement module: numpy module: boolean tensor
[feature request] dtype argument for torch.sign triaged enhancement module: numpy,triaged enhancement module: numpy
copy.deepcopy not working properly for jit.TopLevelTracedModule oncall: jit days,oncall: jit days
Clarify use of GLog/GFlags module: dependency bug module: build triaged small module: build warnings better-engineering,module: dependency bug module: build triaged small module: build warnings better-engineering
In pytorch 1.6ã€‚Run model with input no contiguous tensor will become very slow. module: performance module: cuda triaged,module: performance module: cuda triaged
DataParallel on CPU triaged enhancement module: data parallel,triaged enhancement module: data parallel
Issue while writing scalars o tensorboard using writer.add_scalars(....) triaged module: tensorboard,triaged module: tensorboard
Incorrect documentation of SGD momentum module: docs module: optimizer triaged,module: docs module: optimizer triaged
for loop can't be symbolically traced triaged module: fx,triaged module: fx
Don't query current device on stream construction module: bootcamp triaged,module: bootcamp triaged
Incorrect processing of autograd profiler outputs in torch.utils.bottleneck module: autograd triaged oncall: profiler,module: autograd triaged oncall: profiler
RuntimeError: Caught RuntimeError in replica 1 on device 1. needs reproduction triaged module: data parallel,needs reproduction triaged module: data parallel
fx: Python range function used on tensor shape is not symbolically traceable triaged module: fx,triaged module: fx
[RFC] Pipeline Parallelism in PyTorch oncall: distributed triaged needs research module: rpc,oncall: distributed triaged needs research module: rpc
ComplexHelper.h contains non-inline functions triaged module: complex,triaged module: complex
LNK2019 error after static compilation of Libtorch module: build triaged,module: build triaged
Simple functions shouldn't go through dispatcher triaged module: dispatch better-engineering,triaged module: dispatch better-engineering
Unreliable CPU times in torch.autograd.profiler.profile(use_cuda=True) when using CUDA module: cuda triaged oncall: profiler,module: cuda triaged oncall: profiler
Hybrid Memory module: cuda module: memory usage triaged needs research,module: cuda module: memory usage triaged needs research
Timed out RRef can still be used in subsequent RPCs triaged module: rpc,triaged module: rpc
cblas_gemv is not being used for gemv on complex on CPU triaged module: complex,triaged module: complex
[tools.codegen] Remove byte-for-byte compatibility code triaged module: codegen,triaged module: codegen
Pytorch Installation from source fails (GCC-8.4/CUDA-10.2/RHEL7) and (GCC-7.5/CUDA-10.2/RHEL7) module: build triaged,module: build triaged
16-bit input + AMP breaks AdaptiveLogSoftmaxWithLoss triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
Get rid of copy_from module: internals triaged,module: internals triaged
404 Page not found - inference_api.md & management_api.md triaged module: doc infra,triaged module: doc infra
AMP support for libtorch/c++ triaged enhancement module: amp (automated mixed precision),triaged enhancement module: amp (automated mixed precision)
Error message concerning dtype GRU not identical between devices module: error checking triaged,module: error checking triaged
Make quantized::prepack_fp16 op just do prepacking oncall: quantization low priority triaged,oncall: quantization low priority triaged
[RFC] Integrate profiler with torch.distributed APIs for profiling of distributed models oncall: distributed triaged oncall: profiler,oncall: distributed triaged oncall: profiler
Use macro to define `DispatchKey::toString` method triaged module: dispatch,triaged module: dispatch
torch.sparse improvements - tracking issue high priority module: sparse triaged,high priority module: sparse triaged
Multi-process Dataloader and multi-parameter exceptions module: dataloader triaged enhancement,module: dataloader triaged enhancement
CUDA RuntimeError unrecoverably bricks session module: cuda triaged,module: cuda triaged
MacOS CPU torch.tan and torch.tanh do not compute some values properly module: cpu triaged module: complex module: macos,module: cpu triaged module: complex module: macos
building torchscript extension results in INVALID TYPE: Only int64_t and bool are supported as an integral argument type  custom_class oncall: jit,oncall: jit
[Feature] Fused Matmul & Min/Max/Sum/Prod module: performance triaged enhancement needs research module: linear algebra,module: performance triaged enhancement needs research module: linear algebra
PyTorch 1.6 DataParallel causes CUDNN_STATUS_BAD_PARAM in backward pass module: cudnn module: cuda triaged module: data parallel,module: cudnn module: cuda triaged module: data parallel
 floating point exception (core dumped) in training 10^4 steps module: cuda triaged,module: cuda triaged
too large data in Queue cause dead lock in Multiprocessing  module: multiprocessing triaged,module: multiprocessing triaged
torch.distributed server back-connects to clients on random ports leading to firewall problems oncall: distributed triaged,oncall: distributed triaged
â€œdoxygenfunction: Unable to resolve multiple matches for function ...â€ in C++ documentation module: docs module: cpp triaged,module: docs module: cpp triaged
DataLoader consumes extremely large shared memory (shm) in its initialization. module: dataloader triaged,module: dataloader triaged
"LR scheduler throws warning when using scaler.step instead of optimizer.step, and when saving optimizer state triaged module: amp (automated mixed precision)",triaged module: amp (automated mixed precision)
FP16 inference latency after sleeping module: performance triaged,module: performance triaged
for_each doesn't support integer input to float output type promotion triaged,triaged
RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input. module: cudnn module: cuda module: convolution triaged,module: cudnn module: cuda module: convolution triaged
Error in the converting to pytorch mobile.. oncall: mobile,oncall: mobile
[jit] class type optional type annotation not working properly  oncall: jit,oncall: jit
Support for CUDA matrix multiplication on long (and other integer) tensors high priority module: cuda triaged module: cublas module: linear algebra needs design function request,high priority module: cuda triaged module: cublas module: linear algebra needs design function request
[tools.codegen] Rename api.legacy_dispatcher to api.native module: internals triaged,module: internals triaged
Batchnorm2d in PT1.6 couldn't turn off track_running_stats module: docs module: nn triaged,module: docs module: nn triaged
Investigate SyncBatchNorm cleanup with NCCL Async Error Handling oncall: distributed triaged better-engineering,oncall: distributed triaged better-engineering
Support for NVIDIA UVM technology feature module: cuda triaged,feature module: cuda triaged
Libtorch CMakeList.txt config help module: build triaged,module: build triaged
Asynchronous Execution on CPU feature module: cpu triaged,feature module: cpu triaged
Consider cache effects in Timer module: bootcamp triaged enhancement,module: bootcamp triaged enhancement
[FX] Schema normalization tooling  module: internals triaged module: fx,module: internals triaged module: fx
`Undefined symbol: torch::kNearest` when building App iOS triaged oncall: mobile module: ios,triaged oncall: mobile module: ios
PyTorch wheel's own OpenMP library clashing with system-wide OpenMP library at runtime triaged module: openmp,triaged module: openmp
How to use CUDA Dynamic Parallelism in PyTorch CPP extension? module: cuda triaged,module: cuda triaged
Slower speeds when using half(). module: performance triaged,module: performance triaged
Additional clutter memory allocated on GPU 0 when training on GPU 1+ module: cuda triaged,module: cuda triaged
torch.mv with sparse matrix gives internal assert on cuda module: cuda triaged module: assert failure,module: cuda triaged module: assert failure
caffe2: Python 3 deprecation warnings about inspect.getargspec caffe2 triaged,caffe2 triaged
Support keep stride for neg with requires_grad=False module: memory usage triaged enhancement module: numpy,module: memory usage triaged enhancement module: numpy
REINFORCEMENT LEARNING (DQN) TUTORIAL is not working anymore due to matplotlib.pyplot.imshow() module: docs triaged,module: docs triaged
build from scratch failed because of the expansion of macro module: build triaged,module: build triaged
Deadlock with RPC and dist.barrier() for TensorPipeAgent and NCCL. triaged module: nccl module: rpc module: tensorpipe,triaged module: nccl module: rpc module: tensorpipe
CUDA memory leak in multi-processing module: performance module: cuda module: memory usage triaged,module: performance module: cuda module: memory usage triaged
Item seems to affect the backward process of DataParallel module: cuda triaged module: data parallel,module: cuda triaged module: data parallel
[RFC] Manage CUDA Stream in TensorPipe RPC Agent feature triaged module: rpc module: tensorpipe,feature triaged module: rpc module: tensorpipe
[jit] Better type refinement for class attributes which are class types oncall: jit weeks months TSRootCause:TypeRefinement TSUsability,oncall: jit weeks months TSRootCause:TypeRefinement TSUsability
[FX] Can't symbolically trace callsites to ScriptFunction triaged module: fx,triaged module: fx
Immutable (read-only) tensors triaged,triaged
Make it so that leading underscore operators are truly private and can be changed without worry for BC triaged,triaged
bytes(byte_tensor) gives strange error + [feature request] support memoryview(tensor) module: error checking triaged module: advanced indexing,module: error checking triaged module: advanced indexing
torch.cuda.synchronize Influence distributed training module: performance module: cudnn module: cuda triaged module: data parallel,module: performance module: cudnn module: cuda triaged module: data parallel
Documentation and `torch.sparse` alias for `torch.bmm` sparse-dense module: sparse triaged,module: sparse triaged
Insert at specific index/key in nn.Sequential module: nn triaged enhancement,module: nn triaged enhancement
PackedSequence objects that are created within jit.script are not PackedSequence oncall: jit TSRootCause:InvalidCustomClass TSUsability,oncall: jit TSRootCause:InvalidCustomClass TSUsability
"In profiler, calling `key_averages()` unexpectedly changes CPU time returned for profiled events triaged oncall: profiler",triaged oncall: profiler
"In profiler, recorded block's total time can be less than the operators within the block triaged oncall: profiler",triaged oncall: profiler
torch.bincount beyond 1d arrays triaged enhancement,triaged enhancement
Multi-machine multi-gpu training won't start on CNGrid needs reproduction oncall: distributed triaged,needs reproduction oncall: distributed triaged
Move randperm() to DistributionTemplates triaged module: random csprng,triaged module: random csprng
JIT fails sanity checks during tracing torch.rand_like oncall: jit,oncall: jit
Error : lib/libstdc++.so.6: version `CXXABI_1.3.11 not found module: binaries module: build triaged,module: binaries module: build triaged
OSError: /lib64/libc.so.6: version `GLIBC_2.14' not found  module: binaries triaged,module: binaries triaged
"Using Dataparallel with multi input error.  Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0 triaged module: data parallel",triaged module: data parallel
named tensor INTERNAL ASSERT FAILED when indexing with a list. triaged module: named tensor,triaged module: named tensor
Static Linking of PyTorch didn't statically link CUDA module: binaries triaged module: static linking,module: binaries triaged module: static linking
Syncbatchnorm and DDP  oncall: distributed module: cuda triaged,oncall: distributed module: cuda triaged
[FX] List unpacking is broken triaged module: fx,triaged module: fx
"Some largeCUDATensorTest fails with OOM when running with the entire test suit, but not when running standalone module: tests triaged",module: tests triaged
Make `torch.Generator` picklable feature module: serialization triaged,feature module: serialization triaged
test_doc_template is not working correctly module: tests triaged,module: tests triaged
Exploring supporting some cases of `__bool__` on Proxy triaged module: fx,triaged module: fx
c10::string_view pybind11 custom type caster. triaged enhancement module: pybind,triaged enhancement module: pybind
[FR] hub uses default github branch triaged enhancement module: hub,triaged enhancement module: hub
[FR] Raise an exception when constructing non-empty index and empty values sparse tensor module: sparse module: cuda triaged,module: sparse module: cuda triaged
Type hints from _VariableFunctions and elsewhere clash module: typing triaged,module: typing triaged
ImportError: libtorch_cpu.so: cannot open shared object file: No such file or directory module: build triaged,module: build triaged
"For the same complex dtype and same value, comparing a PyTorch tensor with a NumPy array results in False triaged module: complex",triaged module: complex
`torch.svd()` CUDA gives incorrect results when input contains `nan` triaged module: NaNs and Infs module: linear algebra,triaged module: NaNs and Infs module: linear algebra
[RFC] RPC BENCHMARK triaged module: rpc,triaged module: rpc
FP16 gives NaN loss when using pre-trained model module: cuda triaged module: amp (automated mixed precision),module: cuda triaged module: amp (automated mixed precision)
JIT tracing incorrectly records some slice bounds high priority triage review oncall: jit,high priority triage review oncall: jit
"NCCL WARN Your program may be hanging, this may be caused by a collective mismatch around rank 2. Please check your collective calls at and around this rank. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs oncall: distributed module: cuda triaged module: nccl",oncall: distributed module: cuda triaged module: nccl
AMP much worse performance with groupped Conv2d than fp32  module: dependency bug module: performance module: cuda triaged module: amp (automated mixed precision),module: dependency bug module: performance module: cuda triaged module: amp (automated mixed precision)
"How do I debug ""RuntimeError: trying to initialize the default process group twice!"" oncall: distributed triaged",oncall: distributed triaged
Make busy waiting time in DDP (kSynchronizeBusyWaitMillis) a configurable environment variable oncall: distributed triaged,oncall: distributed triaged
torch.quantization.quantize_dynamic document refers `module` as a parameter  module: docs triaged,module: docs triaged
Useless Exception traces when DataSet timing out module: bootcamp module: dataloader triaged,module: bootcamp module: dataloader triaged
Implement a set_printoptions method in libtorch  module: cpp triaged enhancement,module: cpp triaged enhancement
pybind11_object_dealloc error module: crash triaged module: pybind,module: crash triaged module: pybind
Data loader struct pack issue(overflow)? module: multiprocessing module: dataloader triaged,module: multiprocessing module: dataloader triaged
Trying to build PyTorch from source with LLD 8 fails module: build triaged module: static linking,module: build triaged module: static linking
CUDAGuard might not create CUcontext module: cuda triaged,module: cuda triaged
Training Large Neural Networks with Constant Memory using a New Execution Algorithm module: nn module: memory usage triaged needs research,module: nn module: memory usage triaged needs research
Inference performance regression caused by hacky_wrapper_for_legacy_signatures module: performance module: internals triaged module: dispatch,module: performance module: internals triaged module: dispatch
Add support of random state generator objects to nn.init module module: nn triaged enhancement module: initialization,module: nn triaged enhancement module: initialization
Switch C10_EXPORT_CAFFE2_OP_TO_C10 to new operator registration API caffe2 module: bootcamp,caffe2 module: bootcamp
ModuleDict does not preserve order of initializing dictionary module: nn triaged,module: nn triaged
"Abort message: â€˜terminating with uncaught exception of type c10::Error: _ivalue_ INTERNAL ASSERT FAILED at ../torch/csrc/jit/api/object.cpp:19, please report a bug to PyTorch. (_ivalue at ../torch/csrc/jit/api/object.cpp:19) oncall: jit oncall: mobile",oncall: jit oncall: mobile
OffsetCalculator.cuh(and THCIntegerDivider.cuh) should be available with PyTorch cpu-only binaries module: internals triaged better-engineering csprng,module: internals triaged better-engineering csprng
[JIT] Using Any type variable inside can produce incorrect results due to type unification problems oncall: jit weeks,oncall: jit weeks
"torch.load(.., map_location='cpu') fails when unserializing cuda tensors on a cpu-only device serialized with pickle module: pickle module: serialization triaged",module: pickle module: serialization triaged
"Remove warning, and update documentation. module: docs module: optimizer triaged",module: docs module: optimizer triaged
backward for dense+sparse does not work module: sparse module: autograd triaged,module: sparse module: autograd triaged
CUDA error: an illegal memory access was encountered (laugh_kernel at ...../cuda/CUDALoops.cuh:112) module: cuda triaged module: cublas,module: cuda triaged module: cublas
Cannot find CUDA devices when the machine stays idle for a while module: cuda triaged,module: cuda triaged
OSError: could not find class definition when export torchscript module oncall: jit,oncall: jit
RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. oncall: distributed triaged module: data parallel,oncall: distributed triaged module: data parallel
Adding entropy function analogous to SciPy feature triaged module: numpy,feature triaged module: numpy
Vec256<int64_t> does not handle LONG_MAX on minimum module: cpu triaged module: NaNs and Infs module: vectorization module: reductions,module: cpu triaged module: NaNs and Infs module: vectorization module: reductions
Reduce number of stack frames used up by dispatcher module: internals module: bootcamp triaged,module: internals module: bootcamp triaged
Support for Multi-Categorical in torch.distributions module: distributions feature triaged,module: distributions feature triaged
[JIT] Cannot iterate over nn.ModuleList in JITted code when accessed through one level of indirection oncall: jit days,oncall: jit days
mp.spawn 'args' are not clear module: docs module: multiprocessing triaged,module: docs module: multiprocessing triaged
Expose counters for torch.distributed to report communication overhead within a block. oncall: distributed triaged,oncall: distributed triaged
Support controlling output delay for CTC loss. module: loss triaged enhancement,module: loss triaged enhancement
model trace error caffe2 triaged,caffe2 triaged
one_hot tensors are channels_last but marked as contiguous triaged module: memory format,triaged module: memory format
Do not call nullptr deleter in at::fromDLPack (dlpack) module: cpp module: bootcamp triaged,module: cpp module: bootcamp triaged
"Pytorch ""Import torch"" lead to ""core dump"" module: binaries module: protobuf triaged",module: binaries module: protobuf triaged
[jit] need a better way to handle mix CPU/GPU (Inference/Training) for tracing oncall: jit,oncall: jit
Accessing elements of tensor with multi-dimensional index results `IndexError` triaged module: numpy module: advanced indexing,triaged module: numpy module: advanced indexing
"Non-conforming variable identifiers in JIT code errors / printouts: e.g. ""Tensor 0"" oncall: jit",oncall: jit
Add CutMix transform to torchvision.transforms triaged module: vision,triaged module: vision
"When modified the model python file, the pytorch will raise the KeyError of this file triaged module: data parallel",triaged module: data parallel
Accessing tensor by element is super slow module: performance triaged,module: performance triaged
torch.multinomial with replacement=True produces inaccurate results for large number of categories module: numerical-stability module: distributions triaged,module: numerical-stability module: distributions triaged
"out kwargs are sometimes inconsistent with returned named tuple field name (and in some cases, cannot be made consistent) triaged module: codegen",triaged module: codegen
Missing caffe2_pybind11_state in pip install after cmake. module: build triaged module: pybind,module: build triaged module: pybind
New function for CTC decoding module: nn triaged enhancement needs design,module: nn triaged enhancement needs design
Enable torch.optim typechecks during CI  module: typing triaged,module: typing triaged
LSTM::permute_hidden breaks Liskov substitution principle module: rnn module: typing triaged,module: rnn module: typing triaged
torch.dot throws an error for input tensors of different dtypes triaged enhancement module: numpy module: type promotion,triaged enhancement module: numpy module: type promotion
undefined symbol: FLAGS_caffe2_keep_on_shrink caffe2 triaged,caffe2 triaged
No vectorization for int8 and uint8 triaged module: vectorization better-engineering,triaged module: vectorization better-engineering
BUILD_PYTHON switched to OFF at second run of CMake module: build triaged,module: build triaged
Implement map-style caching DataSet as PyTorch build-in DataSet. module: bootcamp module: dataloader triaged,module: bootcamp module: dataloader triaged
`torch.distributions.Categorical` crashes with illegal instruction module: crash module: distributions triaged,module: crash module: distributions triaged
Enable torch.quantization.fuse_modules typechecks during CI  module: typing triaged,module: typing triaged
Enable torch.testing._internal typechecks during CI  module: typing triaged,module: typing triaged
[feature request] Faster specialized int16->float32 conversions to match speed with NumPy module: performance module: cpu triaged,module: performance module: cpu triaged
TorchBind C++ Enum Class triaged enhancement module: torchbind,triaged enhancement module: torchbind
Verify TorchBind works with nested class type triaged enhancement module: torchbind,triaged enhancement module: torchbind
[jit] std::exception_ptr can't be torchbind/convert to ivalue oncall: jit,oncall: jit
Legacy Python2 and early Python3 leftovers triaged better-engineering shadow review OSS contribution wanted,triaged better-engineering shadow review OSS contribution wanted
Build problems caffe2 -- pytorch from source & CUDA 11.0 module: build module: cuda triaged,module: build module: cuda triaged
"Add typing information into variable declarations for JIT script "".code"" output oncall: jit feature triaged days",oncall: jit feature triaged days
Avoid dynamic isCustomClassRegistered() checks in kernel call paths triaged module: dispatch,triaged module: dispatch
Unable to call `super` method with TorchScript triage review oncall: jit TSRootCause:DynamicBehaviors TSUsability TSRootCause:ModuleInheritance,triage review oncall: jit TSRootCause:DynamicBehaviors TSUsability TSRootCause:ModuleInheritance
Support for defining/saving custom operators with dynamic schema oncall: jit triaged,oncall: jit triaged
Tensorpipe requires setting both GLOO_SOCKET_IFNAME and TP_SOCKET_IFNAME oncall: distributed triaged,oncall: distributed triaged
TensorOptions extensibility has rusted shut triaged module: codegen,triaged module: codegen
Can not get pytorch working with tensorboard triaged module: tensorboard,triaged module: tensorboard
"prelu_backward, hardshrink_backward shouldn't be a method triaged module: dispatch",triaged module: dispatch
[RFC] DeepSpeed + PT Distributed Integration oncall: distributed feature triaged,oncall: distributed feature triaged
Add support for user defined types in serialization in libtorch module: cpp feature module: serialization triaged,module: cpp feature module: serialization triaged
Documentation mistake of Adam in v1.6.0? module: docs module: optimizer triaged,module: docs module: optimizer triaged
"RuntimeError: ""triangular_solve_cuda"" not implemented for 'Half' module: cuda triaged module: half module: amp (automated mixed precision)",module: cuda triaged module: half module: amp (automated mixed precision)
Undocumented parameters in the Variables section of the recurrent bidirectional layers regarding the backward pass module: docs triaged,module: docs triaged
torch/library.h doc rendering issue module: docs triaged,module: docs triaged
Can't registered boxed kernel for operator that doesn't support boxing triaged module: dispatch,triaged module: dispatch
Add a guide for what to do when you think there's a memory leak high priority module: docs module: memory usage triaged,high priority module: docs module: memory usage triaged
checkpoint_sequential breaks backpropagation module: checkpoint module: autograd triaged,module: checkpoint module: autograd triaged
build error redefinition of â€˜struct c10::complex<T>â€™ with release v1.6.0 module: build triaged,module: build triaged
Unbuffered operation triaged enhancement module: numpy function request,triaged enhancement module: numpy function request
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:27 for spectral norm triaged module: cublas,triaged module: cublas
distributed training with c10d does not work with layerdrop in pytorch > 1.4 high priority oncall: distributed triaged,high priority oncall: distributed triaged
Build fail on Ubuntu 18.04: caffe2/CMakeFiles/init_test.dir/build.make:106: recipe for target 'bin/init_test' failed module: build triaged,module: build triaged
libtorch 1.5 crashes when used on macs when using torch::max without AVX support module: crash module: cpu triaged module: macos,module: crash module: cpu triaged module: macos
[quant] Quantized AdaptivePool3d is much slower for ChannelsLast3d. module: performance oncall: quantization low priority triaged module: memory format,module: performance oncall: quantization low priority triaged module: memory format
Missing API reference for SWALR and AveragedModel module: docs triaged,module: docs triaged
Mismatch in docs and behavior of align_corners for nn.functional.interpolate module: docs module: nn triaged,module: docs module: nn triaged
How to build libtorch static libraries on Windows? module: binaries module: build module: windows triaged windows-triaged,module: binaries module: build module: windows triaged windows-triaged
torch.distributed.rpc package not work well with generator and lambda triaged module: rpc,triaged module: rpc
How can I specify NumPy while building PyTorch? module: build triaged,module: build triaged
"Unexpected behavior of ""to"" method inside a torch.jit.script decorated function oncall: jit days TSRootCause:DefaultTypes TSUsability",oncall: jit days TSRootCause:DefaultTypes TSUsability
[discussion] Support __round__ magic triaged enhancement module: numpy,triaged enhancement module: numpy
Optimizer support via Libtorch C++ on Android module: cpp module: optimizer triaged oncall: mobile,module: cpp module: optimizer triaged oncall: mobile
Why Conv3D is slower than Conv2D when its flops is smaller than Conv2D module: performance module: cudnn module: convolution triaged,module: performance module: cudnn module: convolution triaged
"When one distributed test fails in CI, the next one can fail spuriously oncall: distributed module: ci module: tests triaged",oncall: distributed module: ci module: tests triaged
RuntimeError: each element in list of batch should be of equal size  module: dataloader triaged,module: dataloader triaged
onnx export failed when output size are not factor of input size for adaptive_avg_pool2d module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
F.grid_sample produces weird results on single-pixel images module: nn triaged,module: nn triaged
Logsigmoid as chaining operation triaged function request,triaged function request
support LSTM for quantization aware training oncall: quantization low priority triaged,oncall: quantization low priority triaged
Lower performance when forwarding JIT model with libTorch 1.6 compared with 1.5 oncall: jit,oncall: jit
Build script complains that gcc is < 6 even though gcc version is 8 needs reproduction triaged,needs reproduction triaged
CuDNN RNN bindings are basically all deprecated in cudnn 8 module: cudnn module: rnn triaged module: deprecation,module: cudnn module: rnn triaged module: deprecation
torch rpc lose device information between rpc calls triaged module: rpc,triaged module: rpc
renorm dim argument is extremely confusing module: docs triaged better-engineering module: ux,module: docs triaged better-engineering module: ux
Python3 segfaults or doesn't load torch after installation module: build triaged,module: build triaged
[proposal] batch mode for randperm triaged enhancement module: random,triaged enhancement module: random
pytorch tests failed module: tests triaged,module: tests triaged
Support recursive data type in TorchScript high priority oncall: jit triaged weeks,high priority oncall: jit triaged weeks
[Macos][CircleCI] Test failed in test_dataloader.py module: ci triaged module: macos,module: ci triaged module: macos
Using hooks with `DataParallel` gets `autograd` error triaged module: data parallel,triaged module: data parallel
Training with AMP gives overflow warning module: cuda triaged module: amp (automated mixed precision),module: cuda triaged module: amp (automated mixed precision)
[JIT] Runtime error when backpropagating through input gradient oncall: jit triaged,oncall: jit triaged
`device-side assert triggered` - when probability > 1.0 module: crash module: cuda triaged better-engineering,module: crash module: cuda triaged better-engineering
OneCycleLR argument `pct_start` used as a proportion not percentage module: docs module: optimizer triaged,module: docs module: optimizer triaged
torch.random.randperm stuck in multiprocess module: multiprocessing triaged module: random,module: multiprocessing triaged module: random
How to use libtorch on Jetson TX2 module: binaries triaged module: arm,module: binaries triaged module: arm
KeyError: xxxxxxxxxx when calling optimizer.state_dict() module: optimizer triaged,module: optimizer triaged
./scripts/build_android.sh need support nn::module nn::Functional nn::Linear feature triaged module: android oncall: mobile,feature triaged module: android oncall: mobile
torch.distributions.multinomial.Multinomial cannot be used in batch module: distributions feature triaged,module: distributions feature triaged
slow data loading in VisionDataset  - need to allow batch loading. module: performance feature module: dataloader triaged module: vision,module: performance feature module: dataloader triaged module: vision
Reading data speed slower than tensorflow module: performance module: dataloader triaged,module: performance module: dataloader triaged
RFC: torch.vmap triaged module: vmap,triaged module: vmap
Stop implementing JIT stack as a std::vector oncall: jit triaged weeks,oncall: jit triaged weeks
libpytorch_jni is not provided in conda builds module: build triaged,module: build triaged
nn::Sequential Link error in Android version module: android oncall: mobile,module: android oncall: mobile
An error occurred deploying CRNN using libtorch oncall: jit module: rnn module: cuda triaged,oncall: jit module: rnn module: cuda triaged
Generate unique `worker_id` for each node in the RPC framework for the store. triaged module: rpc,triaged module: rpc
Audit quantization functions to ensure proper argument sizes oncall: quantization low priority triaged,oncall: quantization low priority triaged
RuntimeError: ProcessGroupNCCL does not support recv (torch-nightly) oncall: distributed feature triaged module: nccl,oncall: distributed feature triaged module: nccl
Instance norm annotation is incorrect high priority triage review oncall: jit days,high priority triage review oncall: jit days
Proper testing of `nn.Module` loading backward compatibility module: nn module: tests module: serialization triaged,module: nn module: tests module: serialization triaged
Inconsistent results when trying to enable Tensor Cores on NVIDIA T4 module: cudnn triaged module: memory format module: amp (automated mixed precision),module: cudnn triaged module: memory format module: amp (automated mixed precision)
Is it possible that one-version pytorch supports two gpu (GTX TITAN X and TITAN) using at the same time? module: cuda triaged,module: cuda triaged
KeyError: 'track_running_stats' in batchnorm.extra_repr module: printing module: serialization triaged,module: printing module: serialization triaged
Cannot load certain function from dumped Torchscript file high priority triage review oncall: jit days,high priority triage review oncall: jit days
"`torch.tensor([1, 2], dtype=torch.int).fmod(torch.tensor(0, dtype=torch.float))` leads to ""RuntimeError: result type Float can't be cast to the desired output type Int"" triaged module: type promotion",triaged module: type promotion
Implementing packbits feature triaged module: numpy,feature triaged module: numpy
Eliminate warning when cloning a tensor using `torch.tensor(x)` module: docs triaged,module: docs triaged
Skip all reduce globally unused parameters in DDP oncall: distributed triaged enhancement module: data parallel,oncall: distributed triaged enhancement module: data parallel
"RuntimeError: test_autograd failed! Build success, test failed on IBM POWER9 module: autograd triaged module: POWER",module: autograd triaged module: POWER
BusError memory-mapped tensor module: internals triaged,module: internals triaged
Slow `index_add_` on `torch.long` tensors  module: performance module: cuda triaged,module: performance module: cuda triaged
[RFC] Add Windows support to torch.distributed package oncall: distributed module: windows triaged windows-triaged,oncall: distributed module: windows triaged windows-triaged
Setting threads number to the number of default by torch.set_num_threads is faster than not setting it triaged module: multithreading,triaged module: multithreading
To have single cuda context across multiple processes module: cuda triaged,module: cuda triaged
Memory usage of torch.nn.functional.interpolate increased with v1.5.0 when run on numpy input module: nn module: cuda module: memory usage triaged module: numpy,module: nn module: cuda module: memory usage triaged module: numpy
Function request: np.copy (alias of clone?) triaged module: numpy function request,triaged module: numpy function request
"Code hangs when using `set_start_method('spawn', force=True)` in `torch.multiprocessing.pool` module: multiprocessing triaged",module: multiprocessing triaged
Segmentation fault (core dumped) when running optimize_for_mobile oncall: mobile,oncall: mobile
Support additional arguments in nn.Identity.forward module: nn triaged,module: nn triaged
EMNIST looks different to MNIST triaged module: vision,triaged module: vision
PyBind11 submodule required even when pybind11_PREFER_third_party=OFF module: build triaged,module: build triaged
torch.random.fork tries to initialize cuda even when no cuda devices are available module: cuda triaged module: random,module: cuda triaged module: random
can not use nn::Functional(torch::softmax(-1)) in Sequential module: cpp module: nn triaged,module: cpp module: nn triaged
[docs] Clarify behavior of torch.cuda.device_count() when torch compiled without CUDA (cpu-only) or when CUDA is not available module: docs module: cuda triaged,module: docs module: cuda triaged
Installing pytorch from source on Power9 (PPC64LE) + CUDA 10.2 + RHLE7  module: build triaged module: POWER,module: build triaged module: POWER
torch.exp() cannot be modified by an inplace operation module: docs module: autograd triaged,module: docs module: autograd triaged
Failure to compile Eigen on Power with clang module: build triaged module: third_party,module: build triaged module: third_party
Compilation on Power fails with clang due to vec_xl module: build triaged module: POWER,module: build triaged module: POWER
ASAN build breaks when using third_party/protobuf module: build module: protobuf triaged module: POWER,module: build module: protobuf triaged module: POWER
ASAN build broken when using USE_ASAN=1 module: build triaged module: POWER,module: build triaged module: POWER
Automating the various Installation processes for Linux module: build feature triaged,module: build feature triaged
Inconsistencies on ModelLayer caffe2-op triaged better-engineering,caffe2-op triaged better-engineering
[C++] adding type checking or type casting to torch::PackedTensorAccessor indexing module: cpp triaged enhancement,module: cpp triaged enhancement
TorchScript pack_padded_sequence and  pad_packed_sequence run time error oncall: jit,oncall: jit
Difference in inference time between CUDA 10.0 & 10.2 module: performance module: cudnn module: cuda triaged,module: performance module: cudnn module: cuda triaged
test_bottleneck_cuda fails on Power module: tests triaged module: POWER,module: tests triaged module: POWER
"Error occurs when adding ""max_unpool2d"" to onnx module: onnx triaged enhancement",module: onnx triaged enhancement
bitwise_or / bitwise_and /... reductions across dim (+ multidim / keepdim for consistency) triaged enhancement,triaged enhancement
install path can set prefix install dir  module: build triaged,module: build triaged
out-variant for tensor.bitwise_and (exists for torch.bitwise_and) + bitwise_friends triaged enhancement module: numpy needs research module: ux function request,triaged enhancement module: numpy needs research module: ux function request
Caffe2 GPU Inference not working on pytorch nightly version caffe2 triaged,caffe2 triaged
[C++] Libtorch error in Release build only  module: abi triaged,module: abi triaged
General reduction mode selection for in-place and out-variants for wider range (hopefully all) of ops triaged function request module: reductions,triaged function request module: reductions
Improve overload resolution order rules for `invokeOperatorFromPython` high priority oncall: jit,high priority oncall: jit
Torch.multiprocessing.spawn can deadlock module: multiprocessing triaged,module: multiprocessing triaged
'torch::jit::script::ErrorReport' from 'torch::jit::load('modelpath') in Visual Studio 2017 oncall: jit,oncall: jit
TypeError: cannot create 'generator' instances triaged module: data parallel,triaged module: data parallel
"../caffe2/perfkernels/common_avx2.cc:17:2: error: #error ( ""You found a build system error: __AVX2__ is defined (via e.g. -mavx2) "" ""but CAFFE2_PERF_WITH_AVX2 is not defined.""); module: build triaged",module: build triaged
[Feature Request] named tensor support for `tensordot`(tensor contract) triaged enhancement module: named tensor,triaged enhancement module: named tensor
libgcc_s.so.1 must be installed for pthread_cancel to work high priority module: binaries triaged,high priority module: binaries triaged
Potential bug in net_printer caffe2,caffe2
Error with minimal hogwild test (multiprocessing shared memory) module: multiprocessing triaged,module: multiprocessing triaged
Can we have a way to reset a scheduler back to epoch -1 module: optimizer triaged enhancement,module: optimizer triaged enhancement
New Feature : A very fast algorithm for computing matrix rank triaged enhancement module: numpy module: linear algebra,triaged enhancement module: numpy module: linear algebra
"torch.cuda.BoolTensor uses 8 bits per element, not 1 bit as reported by element_size() module: docs triaged",module: docs triaged
[RPC] Should we support users _not_ calling rpc.shutdown()? triaged module: rpc,triaged module: rpc
momentum in BatchNorm module: docs module: nn triaged needs research,module: docs module: nn triaged needs research
Test failure in test_shared_allgather_nccl: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:537 oncall: distributed triaged module: POWER,oncall: distributed triaged module: POWER
Parallel computation of the diagonal of a Jacobian feature module: autograd triaged,feature module: autograd triaged
"torch.nn.functional.grid_sample()is doing bilinear interpolation when the input is 5D, i think the mode should add 'trilinear' module: nn triaged",module: nn triaged
c++ indexing vs python module: cpp triaged,module: cpp triaged
nn.MultiheadAttention causes gradients to become NaN under some use cases high priority module: nn triaged module: NaNs and Infs oncall: transformer/mha,high priority module: nn triaged module: NaNs and Infs oncall: transformer/mha
Make torch.iinfo/torch.finfo torchscriptable triage review oncall: jit days,triage review oncall: jit days
"LSTMs leak memory in CPU PyTorch 1.5.1, 1.6, and 1.7 on Linux high priority module: rnn module: cpu module: memory usage triaged",high priority module: rnn module: cpu module: memory usage triaged
Implement backend fallback for Tracer module: internals triaged,module: internals triaged
Add a done() API to torch.futures.Future and ProcessGroup::Work oncall: distributed triaged,oncall: distributed triaged
[jit] Support NamedTuple in tracing oncall: jit module: bootcamp days,oncall: jit module: bootcamp days
test_jit.py fails on Power oncall: jit triaged,oncall: jit triaged
Bottleneck when publishing the model using flask about 3 times slower. module: performance module: cpu triaged,module: performance module: cpu triaged
"About the description of the mathematical formula of nn.RNN, I think it is wrong module: docs triaged",module: docs triaged
Large overhead (7 microseconds) for PyTorch operation module: performance triaged,module: performance triaged
[RFC] Device Placement API for RPC  feature triaged module: rpc module: tensorpipe,feature triaged module: rpc module: tensorpipe
Distributed tests fail with pytest oncall: distributed triaged,oncall: distributed triaged
Reference the randomness issue in DataLoader & Dataset documentation. module: docs module: dataloader triaged,module: docs module: dataloader triaged
torch.nn.parallel.scatter_gather.gather can't gather outputs that are dataclasses oncall: distributed triaged module: data parallel,oncall: distributed triaged module: data parallel
torch.combinations() - Tried to allocate 7869836414.81 GiB module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
[discussion] Comparison operator chaining triaged enhancement module: boolean tensor,triaged enhancement module: boolean tensor
SGD documentatiuon detail on g_{t+1} module: optimizer triaged,module: optimizer triaged
ConvTranspose1d layer behaviour under different channel numbers module: docs module: convolution triaged,module: docs module: convolution triaged
"[RFC, Tracker] DataLoader improvements module: dataloader triaged",module: dataloader triaged
List[Any] support in TorchScript high priority triage review oncall: jit weeks,high priority triage review oncall: jit weeks
add a reparameterized version of inverse Gaussian distribution module: distributions triaged enhancement,module: distributions triaged enhancement
Add nn.functional.swish alias for nn.functional.silu module: nn triaged,module: nn triaged
torch.abs(complex) is divergent from NumPy on vectorized NaN values  triaged module: complex module: NaNs and Infs,triaged module: complex module: NaNs and Infs
torch.sign is divergent from numpy.sign on NaN triaged module: numpy,triaged module: numpy
torch.tan(complex) on CUDA doesn't handle nonfinite values properly module: cuda triaged module: complex,module: cuda triaged module: complex
[discussion] Unite nn modules nn.Something*d to just nn.Something with appropriate options module: nn triaged,module: nn triaged
"test_backward_deadlock fails with ""Directory not empty"" oncall: jit module: POWER",oncall: jit module: POWER
"""out of scope"" variable `cmd` used oncall: distributed triaged better-engineering",oncall: distributed triaged better-engineering
Cleanup git branches triaged small better-engineering actionable,triaged small better-engineering actionable
"The name ""Contiguous"" in `torch::MemoryFormat::Contiguous` can mislead users into assuming contiguous memory high priority triaged module: memory format",high priority triaged module: memory format
libTorch cpp docs missing for Tensor::item() module: docs module: cpp triaged,module: docs module: cpp triaged
test_upsampling_not_recompute_scale_factor fails with Eigen/OpenBLAS module: tests triaged,module: tests triaged
test_autograd failures on Power module: autograd module: cuda module: tests triaged module: POWER module: linear algebra,module: autograd module: cuda module: tests triaged module: POWER module: linear algebra
DLPack cannot accept empty CUDA tensor module: cuda triaged,module: cuda triaged
Failure to build on Power9 due to FXDIV_SOURCE_DIR not being set module: build triaged module: POWER,module: build triaged module: POWER
The new version of the libtorch become slow module: performance oncall: jit module: cpp triaged,module: performance oncall: jit module: cpp triaged
Advanced indexing gradient is extremely slow when there are many duplicate indices module: performance triaged has workaround,module: performance triaged has workaround
eager mode quantization should remove qconfig in the (non-leaf module of) quantized model oncall: quantization low priority triaged,oncall: quantization low priority triaged
libtorch cmake should PUBLIC ally advertise it as requiring C++14 module: build triaged,module: build triaged
Memory bug for backward on torch.sparse.mm?  module: sparse module: autograd triaged,module: sparse module: autograd triaged
TORCH_CUDA_ARCH_LIST deprecation warning module: build triaged enhancement,module: build triaged enhancement
DDP should provide an option for not touching grad of locally unused params oncall: distributed feature triaged,oncall: distributed feature triaged
max_pool2d always compute indices even when it's not required module: nn triaged enhancement module: pooling,module: nn triaged enhancement module: pooling
Add a function to convert SyncBatchNorm layers back to BatchNorm Layers module: bootcamp feature module: nn triaged,module: bootcamp feature module: nn triaged
Cannot define v_dim in MultiheadAttention module: nn triaged oncall: transformer/mha,module: nn triaged oncall: transformer/mha
Human-readable names and operations for TorchScript model graphs triage review oncall: jit TSUsability TSRootCause:PoorIRVisibility,triage review oncall: jit TSUsability TSRootCause:PoorIRVisibility
out= resizing (and restriding) behavior is confusing module: bc-breaking triaged module: numpy module: safe resize topic: bc breaking,module: bc-breaking triaged module: numpy module: safe resize topic: bc breaking
Is this a bug? The values calculated according to the document isn't equal to the values calculated by framework module: optimizer triaged,module: optimizer triaged
Difference between allocated and reserved CUDA memory module: memory usage triaged,module: memory usage triaged
'_mm256_extract_epi64' was not declared in this scope when compiling on Debian 32-bit module: build triaged,module: build triaged
[FR] NCCL and bool type triaged module: nccl small module: data parallel,triaged module: nccl small module: data parallel
Add a launching script for RPC feature triaged module: rpc,feature triaged module: rpc
len of dataloader when using iterable dataset does not reflect batch size module: nn module: dataloader triaged,module: nn module: dataloader triaged
The values calculated according to the document isn't equal to the values calculated by framework module: docs module: optimizer triaged,module: docs module: optimizer triaged
Regarding graphs page on site module: docs triaged,module: docs triaged
Pytorch 1.4 compilation hangs on AMD Epyc module: build triaged module: vectorization,module: build triaged module: vectorization
Inconsistent behaviour when parameter appears multiple times in parameter list module: docs module: optimizer triaged,module: docs module: optimizer triaged
Will the model run slower when deployed using libtorch ? module: performance module: cpp triaged,module: performance module: cpp triaged
[RFC] [RPC] Automatic retries of all requests in TensorPipe agent triaged module: rpc module: tensorpipe,triaged module: rpc module: tensorpipe
MultiheadAttention set(-inf) cause 'Nan' in loss computing triaged module: NaNs and Infs oncall: transformer/mha,triaged module: NaNs and Infs oncall: transformer/mha
Vectorized torch.eig() module: performance triaged enhancement module: vectorization,module: performance triaged enhancement module: vectorization
[FR] torch.floor/ceil should support output int dtype triaged enhancement module: type promotion,triaged enhancement module: type promotion
Improve error messaging when using dictionaries as inputs to a trace oncall: jit triaged enhancement,oncall: jit triaged enhancement
JIT fuser result of dropout doesn't fully match eager mode  oncall: jit triaged,oncall: jit triaged
Allow TLS to keep distributed autograd context alive feature triaged module: rpc,feature triaged module: rpc
[JIT] Infer type of argument foo = None to be Optional[Tensor] high priority triage review oncall: jit,high priority triage review oncall: jit
Option to allow loading state dict with mismatching shapes. module: nn module: serialization triaged enhancement,module: nn module: serialization triaged enhancement
Build pytorch with cuda11 module: build module: cuda triaged,module: build module: cuda triaged
emit_nvtx context manager is very slow module: performance module: autograd triaged oncall: profiler,module: performance module: autograd triaged oncall: profiler
Encoding dimension argument for F.one_hot module: bootcamp triaged enhancement module: memory format,module: bootcamp triaged enhancement module: memory format
Incremental version of pca_lowrank feature module: cuda triaged function request,feature module: cuda triaged function request
Enhance supported types of functional.pad  module: nn triaged enhancement module: numpy,module: nn triaged enhancement module: numpy
Sparse Convolutional support module: sparse triaged,module: sparse triaged
Quantization support for F.Softmax oncall: quantization low priority triaged,oncall: quantization low priority triaged
RuntimeError: rois.device().is_cpu() ASSERT FAILED at /vision/torchvision/csrc/cpu/ROIAlign_cpu.cpp:386 triaged module: vision,triaged module: vision
torch::autograd::Function should set AutoNonVariableTypeMode when running forward module: autograd triaged,module: autograd triaged
Custom c++ extension build process doesn't preserve color from compiler module: cpp-extensions module: cpp triaged better-engineering,module: cpp-extensions module: cpp triaged better-engineering
libtorch: macros in logging_is_not_google_glog.h have very common names like CHECK or LOG module: logging triaged better-engineering,module: logging triaged better-engineering
[JIT] Tracing BCE loss throws error when using weight oncall: jit triaged,oncall: jit triaged
Segmentation fault in forward pass using DataParallel and multiple GPUs module: cuda triaged module: data parallel,module: cuda triaged module: data parallel
[typing] overly restrictive List[int] module: typing triaged,module: typing triaged
[JIT] Recursive compilation doesn't apply for types used in type expressions but not value expressions oncall: jit triaged,oncall: jit triaged
RuntimeError: broken pipe from NCCL oncall: distributed triaged module: nccl,oncall: distributed triaged module: nccl
Suppress scientific notation in libtorch module: cpp feature triaged,module: cpp feature triaged
Inconsistent behavior between numpy.exp and torch.exp on CPU for complex numbers triaged module: complex module: numpy,triaged module: complex module: numpy
Libtorch C++ multiple GPU performance slower than single GPU module: performance module: cpp triaged module: data parallel,module: performance module: cpp triaged module: data parallel
Generalized CPU vector reductions module: performance module: cpu triaged module: reductions,module: performance module: cpu triaged module: reductions
Converting NumPy dtype to Torch dtype when using `as_tensor` triaged module: numpy,triaged module: numpy
"If all parameters are unused by forward pass in a process, backward will not work with DDP. oncall: distributed triaged",oncall: distributed triaged
Stop registering kernels that use DispatchStub as catch all module: internals triaged,module: internals triaged
"Compiling PyTorch with 11.0, V11.0.167 module: build triaged",module: build triaged
Cannot manually assign a tensor to .grad from TorchScript triage review oncall: jit days,triage review oncall: jit days
RemoteModule enhancements module: bootcamp triaged enhancement module: rpc pt_distributed_rampup,module: bootcamp triaged enhancement module: rpc pt_distributed_rampup
"Embedding with DataParallel can return ""incomplete"" results module: cuda triaged module: data parallel",module: cuda triaged module: data parallel
SyncBatchNorm for JIT and a list of not supported operations oncall: jit triaged enhancement,oncall: jit triaged enhancement
Mixed precision causes NaN loss triaged module: NaNs and Infs module: amp (automated mixed precision),triaged module: NaNs and Infs module: amp (automated mixed precision)
Learning rate change is not applied at designated iteration with a scheduler module: optimizer triaged,module: optimizer triaged
torch.autograd.functional.* for models module: autograd triaged enhancement,module: autograd triaged enhancement
please add 'tensor.astype(dtype_string)' syntax for numpy interoperability triaged enhancement module: numpy,triaged enhancement module: numpy
jit's default dtype is different in sandcastle and test_jit.py oncall: jit module: tests triaged,oncall: jit module: tests triaged
DataParallel with Torch 1.5 high priority triaged module: regression module: data parallel,high priority triaged module: regression module: data parallel
Print values (but not strings) when STRIP_ERROR_MESSAGES is defined module: internals module: bootcamp triaged,module: internals module: bootcamp triaged
"[feature request] batch_apply, a general-purpose device-agnostic batch iterator triaged module: batching",triaged module: batching
"Can we make torch.inverse FP16? - RuntimeError: ""inverse_cuda"" not implemented for 'Half' module: cuda triaged enhancement has workaround module: linear algebra module: amp (automated mixed precision)",module: cuda triaged enhancement has workaround module: linear algebra module: amp (automated mixed precision)
Segment Fault after use cusolverDnDestroy() with torch1.5 module: dependency bug needs reproduction module: crash module: cuda triaged,module: dependency bug needs reproduction module: crash module: cuda triaged
Add a `like` argument to creation ops triaged module: tensor creation function request,triaged module: tensor creation function request
Missing explanation in torch.utils.tensorboard.add_histogram() module: docs triaged module: tensorboard,module: docs triaged module: tensorboard
Cannot re-initialize CUDA in forked subprocess oncall: distributed module: dataloader module: cuda triaged,oncall: distributed module: dataloader module: cuda triaged
New ProcessGroups created with dist.new_group may leak memory oncall: distributed module: memory usage triaged better-engineering,oncall: distributed module: memory usage triaged better-engineering
[discussion] Expressing tensor dimension semantics / constraints through typing / constraints blocks. Constraints block could be scripted/traced and help for tracing/script execution and codegen module: internals feature triaged,module: internals feature triaged
Tracking output dimensions of the convolutional layers module: internals triaged,module: internals triaged
Too many labels in the repo triaged module: infra,triaged module: infra
Refactor the adaptive avg pool code oncall: quantization good first issue low priority triaged module: pooling,oncall: quantization good first issue low priority triaged module: pooling
from torch._C import default_generator ImportError: cannot import name 'default_generator' module: build module: rocm triaged,module: build module: rocm triaged
Observer in Quantization throws Warning oncall: quantization low priority triaged,oncall: quantization low priority triaged
Conv3d with specific kernel size outputs inconsistent results between FP16 and FP32 in V100 GPU module: dependency bug module: cudnn module: cuda module: convolution triaged,module: dependency bug module: cudnn module: cuda module: convolution triaged
caffe2 error during compilation of PyTorch with ROCm on Archlinux caffe2,caffe2
CUDA error: out of memory when running tensorpipe test_cuda triaged module: rpc module: tensorpipe,triaged module: rpc module: tensorpipe
Include expanded TensorOptions version of op in at:: namespace triaged module: codegen,triaged module: codegen
Move torch cpp Errors to c10::Error module: internals module: error checking triaged enhancement better-engineering,module: internals module: error checking triaged enhancement better-engineering
Move all torch.Tensor methods to codegen module: internals triaged module: pybind,module: internals triaged module: pybind
C++ API for torch.autograd.functional.jacobian module: cpp module: autograd triaged,module: cpp module: autograd triaged
SyncBatchNorm doesn't work when I set track_running_stats False oncall: distributed triaged,oncall: distributed triaged
Improve RPC test debugability high priority triaged better-engineering module: rpc,high priority triaged better-engineering module: rpc
About torch.backends.cudnn.deterministic issue module: cudnn triaged,module: cudnn triaged
Why does Libtorch use more memory than Pytorch does in Pythonï¼Ÿ needs reproduction module: memory usage triaged,needs reproduction module: memory usage triaged
ppc64le: test cpp_extensions/rng_extension.cpp failure (without altivec override) triaged module: random module: POWER,triaged module: random module: POWER
[RFC] Add a RPC context manager that collects/waits for all RPC futures created in scope module: bootcamp feature triaged module: rpc,module: bootcamp feature triaged module: rpc
Updating learning rate with Libtorch 1.5 and optimiser options module: cpp module: optimizer triaged,module: cpp module: optimizer triaged
Bump up NCCL to 2.7.3 oncall: distributed triaged module: nccl,oncall: distributed triaged module: nccl
Make Scaling in BatchNorm optional  triaged enhancement,triaged enhancement
TestListwiseL2rOps::test_lambda_rank_loss fails caffe2 module: tests triaged,caffe2 module: tests triaged
[JIT][to-backend] Finalize the PyTorchBackendInterface API/naming for external sharing oncall: jit triaged,oncall: jit triaged
[JIT][to-backend] Support type refinement for container types in the generated code oncall: jit triaged,oncall: jit triaged
Simplify checks that generator has next normal sample cache methods in normal_distribution triaged module: random,triaged module: random
Doing optimizing compilations in a separate thread triaged jit-backlog,triaged jit-backlog
matchTensor tests. Add tests to make sure that `isSubtypeOf` and `matchTensor` return equivalent results triaged jit-backlog,triaged jit-backlog
[Android Pytorch] TorchScript traced model returns inconsistent output tensors on each run oncall: jit triaged oncall: mobile,oncall: jit triaged oncall: mobile
torch::jit::load -> Unhandled exception  oncall: jit triaged,oncall: jit triaged
Caffe2 operation switches current CUDA stream caffe2 module: cuda triaged,caffe2 module: cuda triaged
How do I derive weights for CrossEntropy Loss on my custom dataset? triaged module: vision,triaged module: vision
libtorch 1.5 macos crash when loading on some mac module: build module: cpp triaged module: mkldnn,module: build module: cpp triaged module: mkldnn
Port old registration API to new one triaged better-engineering,triaged better-engineering
Different max_pool2d cpp signatures due to indices.  triaged better-engineering module: pooling,triaged better-engineering module: pooling
[Caffe2]  compilation in c++ undefined reference to `caffe2 module: build caffe2 triaged,module: build caffe2 triaged
 build QT program use libtorch-cxx11-abi-shared-with-deps-1.5.0+cu101 ok with CPU but error with cuda GPU module: build triaged,module: build triaged
Parallelize arguments serde for RPC with TorchScript functions.  module: performance oncall: jit triaged module: rpc,module: performance oncall: jit triaged module: rpc
Shared file-system initialization in pytorch distributed is slow  module: performance oncall: distributed triaged,module: performance oncall: distributed triaged
Simplify layers of optionals in `VaryingShape` and `Stride` oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
Tensor.size() API not correctly documented? module: docs triaged,module: docs triaged
max_unpool2d and max_unpool3d cpp signature should be similar triaged enhancement module: pooling,triaged enhancement module: pooling
[JIT] exported dunder methods are ignored oncall: jit triaged,oncall: jit triaged
User Objects aren't recursively scripted as nn.Module attributes oncall: jit triaged,oncall: jit triaged
Do not modify global random state module: docs triaged module: random,module: docs triaged module: random
[distributed] calling nccl reduce with inconsistent dst hangs oncall: distributed triaged module: nccl module: deadlock,oncall: distributed triaged module: nccl module: deadlock
[Quantization] Output tensor type is lost after serializing and loading back a quantized model oncall: jit,oncall: jit
Run `clang-tidy` on the `aten` folder? module: lint triaged module: build warnings better-engineering,module: lint triaged module: build warnings better-engineering
CUDA cannot be found module: build module: cuda triaged module: arm,module: build module: cuda triaged module: arm
[BatchNorm] Add boolean flags to choose the stats for normalization module: nn triaged function request module: norms and normalization,module: nn triaged function request module: norms and normalization
TestTorchDeviceTypeCPU.test_float_to_int_conversion_finite_cpu_uint8 is broken on PowerPC module: tests triaged module: POWER,module: tests triaged module: POWER
Force JIT to do type inference even when mypy annotated oncall: jit module: typing triaged,oncall: jit module: typing triaged
Better err msg for tensor ctor from sequence triaged enhancement better-engineering,triaged enhancement better-engineering
[RFC][distributed] RFC: c10d ProcessGroup extension and C++ API change oncall: distributed triaged,oncall: distributed triaged
Broadcasting for torch.cross feature triaged module: numpy,feature triaged module: numpy
Installation from source fails on macOS (No CUDA) module: build module: cuda triaged module: macos,module: build module: cuda triaged module: macos
F.affine_grid dispatch async issue module: performance module: nn module: cuda triaged,module: performance module: nn module: cuda triaged
Switching from CPU build to CUDA build in conda environments is a bit tricky module: docs triaged,module: docs triaged
friend constexpr in templated struct loses constexpr-ness in nvcc triaged,triaged
Dataloader._shutdown_workers hangs module: multiprocessing module: dataloader triaged,module: multiprocessing module: dataloader triaged
[JIT] OrderedDict doesn't support custom objects triage review oncall: jit,triage review oncall: jit
[JIT] Print out mutation in IR Dumps triage review oncall: jit,triage review oncall: jit
Segfault = docker + tensorboard + pytorch triaged module: tensorboard,triaged module: tensorboard
test_nn_module_tests should run less tests module: nn module: tests triaged,module: nn module: tests triaged
Valgrind leak checking flags losses in libtorch module: cpp triaged,module: cpp triaged
pytest suppresses stderr from Python startup by default module: logging module: tests triaged,module: logging module: tests triaged
Misannotation of layer_norm parameters causes internal assert failure oncall: quantization low priority triaged,oncall: quantization low priority triaged
JIT test suite has dependencies across tests oncall: jit triaged,oncall: jit triaged
Torch hub: object has no attribute nms triaged module: docker,triaged module: docker
PyTorch multiprocessing.spawn seems slow with list of tensors module: performance module: multiprocessing triaged,module: performance module: multiprocessing triaged
error when specifying sparse=True in embedding module: sparse triaged,module: sparse triaged
BCEWithLogitsLoss() not equal to BCELoss() with sigmoid() module: loss triaged,module: loss triaged
Format issue in `torch.quantization.add_quant_dequant`  documentation parameter section module: docs triaged,module: docs triaged
torch::jit::script::Module::to(torch::kDouble) also casts buffers oncall: jit triaged,oncall: jit triaged
Allow to_here to interrupt blocking wait in the case of rpc.remote timeout feature triaged module: rpc pt_distributed_rampup,feature triaged module: rpc pt_distributed_rampup
Morphological operations feature triaged,feature triaged
"RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:514, unhandled system error, NCCL version 2.4.8 module: crash triaged module: nccl",module: crash triaged module: nccl
Pickling a Tensor or a Storage is not deterministic module: pickle triaged,module: pickle triaged
Draw quantized tensors in tensorboard triaged enhancement module: tensorboard,triaged enhancement module: tensorboard
Attributes not removed by freeze module oncall: jit triaged,oncall: jit triaged
"The conversion of Inplace aten::append to outplace is incomplete, creating incorrect subgraphs oncall: jit triaged",oncall: jit triaged
SyncBatchNorm doesn't scale well across multiple nodes on large data sizes module: performance oncall: distributed triaged,module: performance oncall: distributed triaged
Enable scripting modules with recursive functions oncall: jit feature triaged,oncall: jit feature triaged
Torch script to support dictionary with keys of type tuple oncall: jit triaged,oncall: jit triaged
Clean up GIL that used to guard deleted RRef destructions  triaged better-engineering module: rpc,triaged better-engineering module: rpc
Clean up RPC request callback implementation high priority triage review oncall: distributed triaged better-engineering module: rpc,high priority triage review oncall: distributed triaged better-engineering module: rpc
send a Tensor to Cuda very slow module: performance module: cuda triaged,module: performance module: cuda triaged
Make torch.cross `dim` parameter work intuitively module: bc-breaking triaged module: numpy topic: bc breaking,module: bc-breaking triaged module: numpy topic: bc breaking
[JIT] nn.ModuleList loses None objects inside it after scripting triage review oncall: jit,triage review oncall: jit
Compilation errors module: build triaged module: mkldnn,module: build triaged module: mkldnn
Let future.wait() take in an optional timeout triaged enhancement module: rpc,triaged enhancement module: rpc
Current implementation of c10::complex does not support being used in shared memory triaged module: complex,triaged module: complex
Try Address Sanitizer in MSVC builds module: windows triaged,module: windows triaged
Negative stride values in `as_strided` module: internals module: error checking triaged module: memory format,module: internals module: error checking triaged module: memory format
Add support for rsample to MixtureSameFamily Distribution module: distributions triaged enhancement,module: distributions triaged enhancement
Crossentropy inconsistent results depending on tensor order module: nn triaged module: determinism,module: nn triaged module: determinism
`torch.backends` undocumented module: docs triaged,module: docs triaged
"Problems implementing complex support for acosh, asinh, tanh triaged module: complex",triaged module: complex
[TensorPipe] Errors in pipeWrite should clear out the future in pendingResponseMessage triaged module: rpc module: tensorpipe,triaged module: rpc module: tensorpipe
The MacOS compiler is generating illegal instruction for the division of c10::complex triaged module: complex,triaged module: complex
Build issue when installing pytorch with USE_FFMPEG=1 USE_OPENCV=1  module: build triaged,module: build triaged
Add cpack support to CMakeLists.txt module: build module: cpp triaged,module: build module: cpp triaged
Some @slowtests are never run in CI high priority module: tests triaged quansight-nack,high priority module: tests triaged quansight-nack
Failure when loading quantized pre-trained weights partially oncall: quantization low priority triaged,oncall: quantization low priority triaged
PyTorch Issue w/ GPU needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
Cap DDP total number of buckets oncall: distributed triaged,oncall: distributed triaged
Missing OneCycleLR and MultiplicativeLR in lr_scheduler.pyi module: typing triaged,module: typing triaged
[doc] document cuda.nccl module: docs module: cuda triaged module: nccl,module: docs module: cuda triaged module: nccl
[FR] API consistency for cuda.comm and distributed oncall: distributed module: docs module: cuda triaged,oncall: distributed module: docs module: cuda triaged
[FR] [distributed] coalesced primitives oncall: distributed triaged,oncall: distributed triaged
runtime error while using default arguments in add_graph() function triaged module: tensorboard oncall: visualization,triaged module: tensorboard oncall: visualization
Use general ATen dispatch mechanism module: internals triaged,module: internals triaged
ATen registrable operator list module: internals triaged,module: internals triaged
ATen operator API versioning module: internals triaged,module: internals triaged
[JIT] jit can not recognize the imported function oncall: jit triaged,oncall: jit triaged
torch.lobpcg always breaks for autograd module: autograd triaged module: linear algebra,module: autograd triaged module: linear algebra
[FR] Support SyncBatchNorm in DataParallel triaged enhancement module: nccl module: data parallel,triaged enhancement module: nccl module: data parallel
[FR] DataParallel arg rename device_ids->devices triaged enhancement module: data parallel,triaged enhancement module: data parallel
[FR] cuda.comm.broadcast/reduce_add support `out=` oncall: distributed feature module: cuda triaged,oncall: distributed feature module: cuda triaged
Memory Leak with Docker GPU high priority module: cudnn module: cuda module: memory usage triaged module: regression,high priority module: cudnn module: cuda module: memory usage triaged module: regression
Override `__call__` instead of `forward` module: typing triaged,module: typing triaged
[JIT] named_(parameters | buffers) do not support recursive iteration.  oncall: jit triaged,oncall: jit triaged
Can not compile GridSamplerKernel.cpp with gcc-9.3 module: build triaged,module: build triaged
Can't compile QuantizedOpKernels.cpp using gcc-9.3 module: build triaged,module: build triaged
Weight decay in AdamW module: optimizer triaged,module: optimizer triaged
Convergence issues when using pytorch's native AMP triaged module: amp (automated mixed precision),triaged module: amp (automated mixed precision)
SPMG in DDP does not have gradient computation and communication overlap? oncall: distributed triaged,oncall: distributed triaged
weight_decay in Adam is not an L2 Penalty module: optimizer triaged,module: optimizer triaged
LibTorch 1.5.0 not supporting GLIBC < 2.23 module: build module: cpp triaged,module: build module: cpp triaged
test_float_to_int_conversion_finite_cpu_int16 is failing on MacOS high priority module: tests triaged module: numpy,high priority module: tests triaged module: numpy
[jit] `jit.annotate` and multiple compilation of functions with different types triage review oncall: jit,triage review oncall: jit
Building NVCC (Device) failed when building from source module: build triaged,module: build triaged
NaN Loss for FasterRCNN on Multiclass Object Detection on Custom Dataset COCO triaged module: vision,triaged module: vision
torch.as_tensor(np_array) is sometimes much faster than torch.tensor(np_array) module: performance triaged module: numpy,module: performance triaged module: numpy
TestCase.assertEqual does not distinguish Python builtin types and single-element Tensor module: tests triaged,module: tests triaged
Add CUDA callback to Python API feature module: cuda triaged module: rpc,feature module: cuda triaged module: rpc
Unify CPU/CUDA exponential transformation formula triaged module: random,triaged module: random
Implement torch.erf for complex dtypes triaged module: complex,triaged module: complex
torch while loading weighs found runtime error on storage has wrong size: expected 4254413747647032608 got 1024  module: serialization triaged,module: serialization triaged
Synchronization problem in torch.distributed with MPI on CUDA  oncall: distributed triaged,oncall: distributed triaged
Maxunpool seems to give a weird error message triaged,triaged
Difference between using a python list and nn.ModuleList module: docs module: nn triaged,module: docs module: nn triaged
Torchvision error TypeError: _resolve_type_from_object() triaged module: vision,triaged module: vision
Wheels not manylinux1 compliant module: binaries triaged,module: binaries triaged
Providing CUDA tensor to model on CPU causes a crash module: crash module: rnn module: cuda triaged,module: crash module: rnn module: cuda triaged
"Can you add NMS,RoIAlign,RoIPool for libtorch? triaged module: vision",triaged module: vision
RuntimeError: num_gpus <= 16 INTERNAL ASSERT FAILED  module: multi-gpu module: cuda triaged,module: multi-gpu module: cuda triaged
Investigate using log1p instead of log in transformation functions(TransformationHelper.h) module: distributions triaged module: random,module: distributions triaged module: random
Investigate exponential distribution improvements module: distributions triaged module: random,module: distributions triaged module: random
Investigate using -cospi(u) / sinpi(u) instead of tan(pi * (u - 0.5)) in transformation::cauchy module: distributions triaged module: random,module: distributions triaged module: random
[quantization] Version support for quantization BC tests oncall: quantization low priority triaged,oncall: quantization low priority triaged
tensorboard projector mode with custom metadata_header with only one label name triaged module: tensorboard,triaged module: tensorboard
"When I use cuda(), wg = th.matmul(extra_obs, extra_obs.transpose(-2, -1)) take a mistake triaged module: cublas",triaged module: cublas
Returning a tensor instead of a list in split and chunk feature triaged has workaround shadow review,feature triaged has workaround shadow review
Add Distributed LR Scheduler to RPC module: bootcamp feature triaged module: rpc,module: bootcamp feature triaged module: rpc
Make rebuildBucket() to be async in c10d reducer oncall: distributed triaged module: data parallel,oncall: distributed triaged module: data parallel
"build libtorch problem: Configuring incomplete, errors occurred!  _mm256_abs_epi16 module: build triaged",module: build triaged
[DISCUSSION] RPC server-side ThreadLocalState oncall: jit triaged module: multithreading module: rpc,oncall: jit triaged module: multithreading module: rpc
expected scalar type Half but found Float with torch.cuda.amp and torch.nn.DataParallel triaged module: data parallel module: amp (automated mixed precision),triaged module: data parallel module: amp (automated mixed precision)
TopK implementation slower than a custom divide and conquer implementation module: performance module: cuda good first issue module: cpu triaged module: sorting and selection,module: performance module: cuda good first issue module: cpu triaged module: sorting and selection
Scale parameter downcasted and rounded down in pytorch.distributions.Normal module: distributions triaged,module: distributions triaged
[torch.jit.trace] torch.jit.trace fixed batch size CNN  oncall: jit triaged,oncall: jit triaged
[Feature] Option to have zeros/ones/full output tensor with zero strides triaged enhancement module: tensor creation function request,triaged enhancement module: tensor creation function request
Named Tensor and Indexing triaged module: named tensor,triaged module: named tensor
JitTest.testAutogradProfiler is broken in test_misc.cpp oncall: jit module: tests triaged oncall: profiler,oncall: jit module: tests triaged oncall: profiler
[JIT] torch.tensor needs a Tensor overload oncall: jit module: bootcamp triaged small,oncall: jit module: bootcamp triaged small
No MKL Compatible Conda installation for PyTorch 1.5 module: binaries triaged module: mkl,module: binaries triaged module: mkl
4D tensor support for torch.nn.functionnal.fold() (col2im) module: nn triaged function request,module: nn triaged function request
"Can I add more ""Project Documentation"" on the PYPI webpage?  module: build module: docs triaged small",module: build module: docs triaged small
Provide an issubdtype API triaged module: numpy module: type promotion,triaged module: numpy module: type promotion
Support alternate casting rules triaged module: numpy module: type promotion,triaged module: numpy module: type promotion
c10/macros/cmake_macros.h not exists triaged oncall: mobile,triaged oncall: mobile
Compiling errors when trying to cross-compile the C++ API for RTOS (QNX) module: build module: cpp triaged,module: build module: cpp triaged
Adding model metadata in TorchScript model file triage review oncall: jit module: serialization,triage review oncall: jit module: serialization
"PyTorch's rot90 returns a new tensor, inconsistent with NumPy's returning a view triaged module: numpy",triaged module: numpy
"PyTorch's flip returns a new tensor, but NumPy's flip returns a view triaged module: numpy",triaged module: numpy
Make torch.rpc accept store as optional parameter feature triaged module: rpc,feature triaged module: rpc
SIGXCPU at test_cholesky_solve with AMD EPYC 7742 64-Core Processor module: tests triaged,module: tests triaged
RuntimeError: arg_types.size() == param_names.size() - (moduleSelf_ ? 1 : 0) INTERNAL ASSERT FAILED oncall: jit module: typing triaged,oncall: jit module: typing triaged
Failed to link torch_library using cmake module: build module: cpp triaged,module: build module: cpp triaged
When TorchScripted module has bad type annotation you get bad error message oncall: jit module: bootcamp triaged,oncall: jit module: bootcamp triaged
Python autograd engine threads never terminate in Python 3.5-3.8 module: crash module: autograd module: ci triaged,module: crash module: autograd module: ci triaged
pybind11::gil_scoped_release: crash on exit with daemon threads high priority module: dependency bug module: crash triaged,high priority module: dependency bug module: crash triaged
"Please help, building failing with MAGMA support module: build triaged",module: build triaged
nn.Module Abstract Class attribute overrides Child. triaged,triaged
Loops are very slow compared to tensorflow triage review oncall: jit,triage review oncall: jit
RESOLVED: Disable zero-dim CUDA tensors interacting with CUDA tensors on other devices module: bc-breaking module: cuda triaged enhancement,module: bc-breaking module: cuda triaged enhancement
missing dependency `protobuf` for caffe2 module: build triaged,module: build triaged
IndexError reports the wrong dimension when fancy indexing module: error checking triaged,module: error checking triaged
Why no `torch.randperm_like`? triaged enhancement module: tensor creation,triaged enhancement module: tensor creation
Can't Build Pytorch0.4.1 From Source module: build triaged,module: build triaged
test_ReplicationPad3d (test_nn.TestNN) takes too long to run module: tests triaged,module: tests triaged
test_LocalResponseNorm_3d_custom_params (test_nn.TestNN) takes too long to run module: tests triaged,module: tests triaged
test_interpolate_nearest_scale_3d in test_nn takes too long to run module: tests triaged,module: tests triaged
valgrind says libtorch has memory leak module: memory usage triaged,module: memory usage triaged
"OneCycleLR  raises ""Tried to step { step_num + 1 } times"" after the value is more than expected. triaged",triaged
[JIT] Aliased Python references to script::Object escape IValue reference counting oncall: jit triaged,oncall: jit triaged
"TorchScript to support == None, != None oncall: jit triaged",oncall: jit triaged
test_cpp_warnings_have_python_context_cpu fails under some build configurations module: tests triaged,module: tests triaged
Pybind11 cpp extensions broken with pytorch v1.5.0 high priority module: build module: docs module: cpp triaged module: regression,high priority module: build module: docs module: cpp triaged module: regression
Add helpers to save/load RPC-based models feature triaged module: rpc,feature triaged module: rpc
ImportError: libtorch_cpu.so: cannot open shared object file: No such file or directory module: build triaged,module: build triaged
DNNL's backward pass much slower when using nn.grad.conv2d_input and nn.grad.conv2d_weight module: autograd module: nn triaged module: mkldnn,module: autograd module: nn triaged module: mkldnn
Div by zero error not triggered and inf not returned when dividing by 0 for some dtypes module: numerical-stability triaged module: type promotion,module: numerical-stability triaged module: type promotion
Quantile Regression Loss module: nn triaged OSS contribution wanted,module: nn triaged OSS contribution wanted
Support Slicing of ModuleList during JIT model tracing/scripting  oncall: jit module: bootcamp triaged large medium,oncall: jit module: bootcamp triaged large medium
"torch.cuda.nccl.init_rank does not handle ""uid"" properly, causing runtime error module: cuda triaged module: nccl",module: cuda triaged module: nccl
Resnet Model always predicting same label triaged module: vision,triaged module: vision
Inconsistent Documentation about Optimizer.step(closure) module: docs module: optimizer triaged,module: docs module: optimizer triaged
"New padding size format for F.pad, allowing named tensors and more clear syntax overall feature module: nn triaged",feature module: nn triaged
unsqueeze support for named tensors triaged enhancement module: named tensor,triaged enhancement module: named tensor
JIT compilation of NamedTuple Containing a NamedTuple fails oncall: jit triaged,oncall: jit triaged
Special methods on torchscript custom class oncall: jit triaged,oncall: jit triaged
torch.save incompatible with lzma file module: serialization triaged,module: serialization triaged
CUDA sources are not cached with sccache module: build module: windows module: cuda module: ci triaged module: regression,module: build module: windows module: cuda module: ci triaged module: regression
torch.addmv can't take as input tensors with different dtypes triaged module: type promotion,triaged module: type promotion
Support @property decorator in TorchScript triage review oncall: jit,triage review oncall: jit
"[RFC] Don't install CI dependencies in build scripts, install them in underlying docker images module: ci triaged",module: ci triaged
Deprecate type() and type_as() call triaged module: deprecation module: ux,triaged module: deprecation module: ux
autograd engine callbacks don't respect non-default cuda streams module: autograd module: cuda triaged,module: autograd module: cuda triaged
llvmlite version issue when upgrading CI docker image to python 3.8 module: ci triaged,module: ci triaged
[DRAFT] Channels Last + AMP support plan for 1.6 release triaged module: memory format,triaged module: memory format
DistributedDataParallel does not support Modules that take no inputs. triaged module: data parallel,triaged module: data parallel
"Add Numpy-like ""order"" argument to reshape feature triaged module: numpy has workaround",feature triaged module: numpy has workaround
Guard Gloo and TensorPipe related code in RPC with #ifdef module: bootcamp triaged module: rpc module: tensorpipe,module: bootcamp triaged module: rpc module: tensorpipe
CMake Documentation Issue module: docs triaged,module: docs triaged
Documentation of _CtxMethodMixin: must be tensors? module: docs module: autograd triaged,module: docs module: autograd triaged
torch.cdist returns inconsistent result module: numerical-stability module: docs triaged module: distance functions,module: numerical-stability module: docs triaged module: distance functions
Variational Dropout In RNN module: rnn triaged enhancement,module: rnn triaged enhancement
Builtin FusedLayerNorm is slower than apex one module: performance module: cuda triaged,module: performance module: cuda triaged
"Add Compound key, make custom ops default to it (but keep internal users using CatchAll) triaged internals",triaged internals
LSTMCell consumes x1.5 more memory on CUDA on pytorch >=1.3 comparing to pytorch 1.2 module: rnn module: cuda module: memory usage triaged,module: rnn module: cuda module: memory usage triaged
Remove everything from a GPU (including drivers) module: cuda triaged enhancement,module: cuda triaged enhancement
logging_is_not_google_glog.h:24:11: error: 'const int ERROR' redeclared as different kind of symbol  (v1.3.0) module: build triaged,module: build triaged
NCCL fails to find cuda include dir oncall: distributed triaged module: nccl,oncall: distributed triaged module: nccl
"test_qnnpack_sigmoid failing on old CPU, built pytorch 1.5.0 module: binaries module: build triaged",module: binaries module: build triaged
Unable to use torch.det() inside nn.DataParallel with multiple gpus module: cuda triaged module: data parallel module: linear algebra,module: cuda triaged module: data parallel module: linear algebra
"torch.cartesian_prod(*tensors) error when you have tensors with [x,y] triaged module: linear algebra",triaged module: linear algebra
Add name to Class Parameter() oncall: distributed module: nn triaged enhancement module: rpc,oncall: distributed module: nn triaged enhancement module: rpc
Are there any differences in kernel memory between RX2080's and Quadro RTX4000?   module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
Implement generic function scheduler in c10/util triaged module: rpc,triaged module: rpc
Improve visibility in test suite timings module: ci module: tests triaged better-engineering,module: ci module: tests triaged better-engineering
"Cannot build pytorch with linker arguments in C{,XX}FLAGS module: build triaged",module: build triaged
undefined reference to pthreadpool_compute* module: build triaged module: xnnpack,module: build triaged module: xnnpack
Log-linear version of cumsum and cumprod module: performance feature triaged,module: performance feature triaged
No speedup from channels_last with DataParallel module: performance triaged module: data parallel module: memory format,module: performance triaged module: data parallel module: memory format
[RuntimeError] Tensor creation using storage fails module: error checking triaged,module: error checking triaged
RuntimeError: CUDA error: an illegal memory access was encountered with channels_last high priority module: dependency bug module: binaries module: cudnn module: cuda triaged,high priority module: dependency bug module: binaries module: cudnn module: cuda triaged
Multi-Process Single-GPU is bad oncall: distributed triaged module: data parallel,oncall: distributed triaged module: data parallel
torch.clamp_max clamp_min shouldn't be there triaged better-engineering,triaged better-engineering
Tensor.as_strided_ is not documented module: docs triaged,module: docs triaged
Tensor.is_same_size not documented module: docs triaged,module: docs triaged
rsub incorrectly exposed in torch triaged better-engineering,triaged better-engineering
"Questions about ""torch.utils.tensorboard.add_graph"": Could I use it to see network graph's compute time and memory?  module: docs triaged module: tensorboard",module: docs triaged module: tensorboard
Compatibility of subset dataset with disabled batch sampling module: dataloader triaged,module: dataloader triaged
torch.cdist() implementation without using contiguous() calls module: performance triaged enhancement module: distance functions,module: performance triaged enhancement module: distance functions
3D grouped & depthwise convolution very slow on backward pass module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
Error running trace on Pytorch Crowd Counting model oncall: jit triaged,oncall: jit triaged
Move bernoulli_() to DistributionTemplates triaged module: random,triaged module: random
Efficient handling special gradient values in the autograd module: performance feature module: autograd triaged,module: performance feature module: autograd triaged
[JIT] Not support for MaskRCNNPredictor needs reproduction oncall: jit triaged,needs reproduction oncall: jit triaged
qnnpack's quantized-add gives wrong result oncall: quantization triaged oncall: mobile,oncall: quantization triaged oncall: mobile
Why do we not use TorchScript to build graph for tensorboard feature triaged module: tensorboard oncall: visualization,feature triaged module: tensorboard oncall: visualization
Concerning default confiugration for distribution packages module: binaries module: build triaged,module: binaries module: build triaged
Multiprocessing: model shared between processes hangs during copy.deepcopy module: multiprocessing module: cuda triaged,module: multiprocessing module: cuda triaged
equal_nan keyword not implemented for complex torch.isclose triaged module: complex module: numpy,triaged module: complex module: numpy
Data caching module a la `Sampler` feature triaged,feature triaged
python setup.py install error module: build triaged,module: build triaged
"intermittent failures of ""test_remote_script_module"" high priority triage review oncall: jit module: flaky-tests",high priority triage review oncall: jit module: flaky-tests
Inconsistency between GPU memory usage in torch.cuda.memory_summary and nvidia-smi module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
`max_norm` parameter on nn.Embedding will fail inside nn.DataParallel module: nn triaged module: data parallel,module: nn triaged module: data parallel
RuntimeError: Tried to instantiate class __file__.__file__ but it does not exist! Ensure that it is registered via torch::jit::class_ needs reproduction oncall: jit triaged,needs reproduction oncall: jit triaged
Make it harder to make SIOF bugs for torchbind classes referenced by schemas triaged better-engineering,triaged better-engineering
torch.distributions bug in RelaxedOneHotCategorical.log_prob  module: distributions triaged,module: distributions triaged
Enhanced operator context when reporting errors triaged enhancement module: shape checking,triaged enhancement module: shape checking
Per-cluster biases in AdaptiveLogSoftmaxWithLoss feature module: nn triaged needs research,feature module: nn triaged needs research
CPU out of bound memory access in CUDA reduction kernel config module: cuda triaged module: TensorIterator,module: cuda triaged module: TensorIterator
Cuda profiler + DataParallel + manual profiling start = strange profiling overhead pattern oncall: distributed triaged,oncall: distributed triaged
Propagation of channels-last layout leads to massive slowdowns in 1.5 compared to 1.4 module: performance module: internals triaged module: memory format,module: performance module: internals triaged module: memory format
[Design][RFC] RemoteModule API Design oncall: jit triaged module: rpc,oncall: jit triaged module: rpc
pip install torch==1.4.0 is broken when using CUDA 10.1 module: build triaged,module: build triaged
[JIT] Use of global value creates confusing error message oncall: jit triaged small,oncall: jit triaged small
Legacy fuser doesn't do remainder consistently with `aten::remainder` oncall: jit triaged,oncall: jit triaged
nn.Bilinear cannot be used inside nn.Sequential feature module: nn triaged,feature module: nn triaged
Result parameters from Single-Process Multi-GPU DDP training on RNN do not match local training oncall: distributed triaged,oncall: distributed triaged
can't wrap two models in the same class needs reproduction triaged,needs reproduction triaged
Saved model behaves differently on same data needs reproduction module: serialization triaged,needs reproduction module: serialization triaged
Deprecated mask fill mask type can causes pages and pages of repeated messages triaged,triaged
Misleading documentation in torch.nn.functional.fold module: docs module: nn triaged,module: docs module: nn triaged
CUDA error: device-side assert triggered @ model.cuda() needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
copy_ slowness module: performance module: cuda triaged,module: performance module: cuda triaged
Converting to Torch Script: cpp_module does not match nn_module high priority triage review oncall: jit triaged,high priority triage review oncall: jit triaged
Make it an error to def() an operator multiple times module: bootcamp triaged,module: bootcamp triaged
"RuntimeError: NCCL error in ProcessGroupNCCL.cpp:290, unhandled system error module: dependency bug oncall: distributed triaged module: nccl",module: dependency bug oncall: distributed triaged module: nccl
Add a CI configuration to test USE_DISTRIBUTED=0 oncall: distributed module: ci module: tests triaged,oncall: distributed module: ci module: tests triaged
[docs] Explain active_bytes in torch.cuda.memory_stats and Cuda Memory Management module: docs triaged,module: docs triaged
pytorch latest update(1.4) broke CosineAnnealingWarmRestarts: T_cur is not define module: optimizer triaged,module: optimizer triaged
Pull hacked twins out of prim ops oncall: jit triaged,oncall: jit triaged
JIT string ops and other miscellaneous ops probably shouldn't be in aten namespace triage review oncall: jit triaged,triage review oncall: jit triaged
Multiprocess DataLoader with DLPack conversion sometimes corrupts memory module: multiprocessing module: serialization triaged module: xnnpack,module: multiprocessing module: serialization triaged module: xnnpack
"jit trace failed due to ""failed to differentiate `prim::ListConstruct`"" oncall: jit triaged",oncall: jit triaged
Problem with c10/utils/variant.h module: cpp triaged,module: cpp triaged
jit.script leads to RuntimeError: attribute lookup is not defined on python value of type in some cases triage review oncall: jit triaged small,triage review oncall: jit triaged small
pytorch and c++ inference disagree module: cpp triaged,module: cpp triaged
Memory leak upon exception for interactive consoles module: memory usage triaged,module: memory usage triaged
Stochasticity for DistributedDataParallel on CPU but not on GPU high priority needs reproduction oncall: distributed triaged module: determinism module: data parallel,high priority needs reproduction oncall: distributed triaged module: determinism module: data parallel
c++: error: unrecognized command line option '-Wthread-safety' module: build triaged,module: build triaged
Clearer guidance on when to define an operator as a method on torchbind'ed class versus standalone function triaged,triaged
When you try to register a kernel that make boxed from unboxed functor doesn't support you get a horrible error message triaged,triaged
return_index option for torch.unique triaged enhancement module: numpy function request,triaged enhancement module: numpy function request
Custom class type name is very wordy oncall: jit triaged,oncall: jit triaged
Custom Generators don't work in JIT triaged,triaged
[1.4.1] Cuda build fails needs reproduction module: build triaged,needs reproduction module: build triaged
variable name N_ conflicts with an internationalization macro in glib oncall: jit module: build triaged,oncall: jit module: build triaged
Reduction for `torch.int8` is super slow on CUDA module: performance module: cuda triaged module: TensorIterator,module: performance module: cuda triaged module: TensorIterator
state_dict and load_state_dict methods for DataLoader and Sampler to continue training at specific epoch and batch feature module: dataloader triaged,feature module: dataloader triaged
"RuntimeError: Expected cuda::check_device({sparse_, r_, t, dense}) to be true, but got false.   needs reproduction triaged module: data parallel",needs reproduction triaged module: data parallel
Poor elmenetwise_kernel performance becomes critical on small mini-batch sizes module: performance module: cuda triaged module: TensorIterator,module: performance module: cuda triaged module: TensorIterator
TorchScript Support for Named Tensors triage review oncall: jit triaged,triage review oncall: jit triaged
Add load_state_dict and state_dict() in C++ module: cpp triaged,module: cpp triaged
One confusion about the CompilationUnit destructuring process in torch/jit/__init__.py  oncall: jit triaged,oncall: jit triaged
[1.4.1] cmake3 not found module: build triaged,module: build triaged
Libtorch build error when setting both `USE_GLOO` and `USE_SYSTEM_NCCL` to `ON` oncall: distributed module: build module: docs triaged module: nccl,oncall: distributed module: build module: docs triaged module: nccl
Allow `__array__` to automatically detach and move to CPU feature module: cuda triaged module: numpy,feature module: cuda triaged module: numpy
Typecasting issue in MSELoss module: loss triaged module: type promotion,module: loss triaged module: type promotion
"Drop _stacklevel from argspecs of F.softmax, F.softmin, F.log_softmax (for implicit dim has been long deprecated) module: docs triaged",module: docs triaged
Restructure test_c10d.py and test_distributed.py oncall: distributed triaged better-engineering,oncall: distributed triaged better-engineering
XNNPACK operators are not actually registered under xnnpack namespace triaged better-engineering module: xnnpack,triaged better-engineering module: xnnpack
DDP should divide bucket contents by the number of global replicas instead of world size oncall: distributed triaged,oncall: distributed triaged
Quantized _out functions don't follow same conventions as other out functions in the codebase oncall: quantization low priority triaged better-engineering,oncall: quantization low priority triaged better-engineering
Wrong results for multiplication of non-finite complex numbers with real numbers triaged module: complex module: numpy,triaged module: complex module: numpy
Comparison ops for Complex Tensors triaged module: complex module: numpy,triaged module: complex module: numpy
Issue when linking C++ code with libtorch_cpu: cuda not detected module: build module: cpp triaged,module: build module: cpp triaged
"After `create_graph=True`, calculating `backward()` on sparse Tensor fails triaged enhancement",triaged enhancement
Any reference to LPPool2d module: docs triaged,module: docs triaged
Unable to build wheel from RC3 module: build triaged,module: build triaged
Docs of distributed module do not include the full documentation for torch.distributed.launch module: docs triaged,module: docs triaged
CMake targets wrongly forward unknown options to NVCC (v1.5+) module: build triaged,module: build triaged
Jit doesn't match schema like eager mode does oncall: jit triaged,oncall: jit triaged
Memory leak issue still exists in CI  module: build module: ci triaged,module: build module: ci triaged
Illegal memory access / CUDNN_STATUS_MAPPING_ERROR on Quadro 8000 module: cudnn triaged,module: cudnn triaged
Updating a ModelDict instance with another ModelDict instance generates error. module: nn triaged,module: nn triaged
[discussion] Refactor spectral_norm to use the newly merged lowrank solvers and proposal for Linear Algebra Cookbook page triaged module: linear algebra,triaged module: linear algebra
"torch.ceil, torch.floor should accept a dtype argument triaged module: numpy module: ux",triaged module: numpy module: ux
 Rectify docs for MultiLabelSoftMarginLoss module: docs module: nn triaged,module: docs module: nn triaged
[JIT] support self.named_buffers and self.named_parameters in TorchScript triage review oncall: jit triaged enhancement,triage review oncall: jit triaged enhancement
New dtype ComplexPolarFloat (phasor) low priority triaged module: complex enhancement,low priority triaged module: complex enhancement
Decouple DDP from CUDA oncall: distributed feature module: cuda triaged,oncall: distributed feature module: cuda triaged
Seg Fault: import vaex with torch module: binaries module: crash triaged,module: binaries module: crash triaged
Inconsistent ProcessGroupMPI work data structure for send/recv and collectives module: performance oncall: distributed triaged better-engineering,module: performance oncall: distributed triaged better-engineering
build Pytorch-1.5.0-rc1 from source fail module: build triaged,module: build triaged
Allow grid_sample to accept pixel units (absolute coordinates) proposal accepted module: nn triaged module: vision,proposal accepted module: nn triaged module: vision
"In AutogradContext, get_saved_variables() should be renamed to get_saved_tensors() module: cpp triaged",module: cpp triaged
[quantization] torch.quantized_lstm and torch.quantized_gru not documented module: docs oncall: quantization low priority triaged,module: docs oncall: quantization low priority triaged
Add Specific Warning/Error For Unsupported GPU or Systems module: cuda triaged enhancement,module: cuda triaged enhancement
RuntimeError CUDA error despite CUDA available and GPU supported module: binaries module: cuda module: error checking triaged,module: binaries module: cuda module: error checking triaged
[JIT] support list of nn.Module in torchscript triage review oncall: jit triaged enhancement,triage review oncall: jit triaged enhancement
Half type promotion with Numpy arrays is incorrect triaged module: numpy,triaged module: numpy
[DISCUSSION] Better user experience for debugging on Windows module: build module: windows triaged,module: build module: windows triaged
[JIT] Huge delay (1274s vs 0.031s) when running scripted model high priority oncall: jit triaged,high priority oncall: jit triaged
torch.multinomial is misnamed.  module: distributions triaged module: ux,module: distributions triaged module: ux
C++ tensor print doesn't show requires_grad and grad_fn like Python tensor print module: printing module: cpp triaged,module: printing module: cpp triaged
JITed GRU too slow high priority triage review oncall: jit triaged,high priority triage review oncall: jit triaged
"[JIT] Tensor method API behavior discrepancy, Tensor.detach(..) needs reproduction oncall: jit triaged",needs reproduction oncall: jit triaged
TensorOptions shouldn't provide default values triaged better-engineering,triaged better-engineering
"Copy activations from one parts to another part in tensor, but report error triaged",triaged
Build PyTorch-1.4.0 from source failed module: build triaged,module: build triaged
MKLDNN_conv2d 2X slower than the native TH implementation module: performance module: cpu triaged module: mkldnn,module: performance module: cpu triaged module: mkldnn
Error when loading jit traced FasterRCNN model in C++ oncall: jit triaged,oncall: jit triaged
torch.autograd.set_detect_anomaly(True) does not exist in C++? module: cpp triaged,module: cpp triaged
CUDNN_STATUS_INTERNAL_ERROR with GPU RTX 8000 needs reproduction module: cudnn module: cuda triaged,needs reproduction module: cudnn module: cuda triaged
Linking error after marking an op `manual_kernel_registration: True` module: build triaged,module: build triaged
[JIT] Tracer bakes List[Tensor] attribute as constant oncall: jit triaged,oncall: jit triaged
Hook on input tensor not called when using autograd.grad() module: autograd triaged actionable,module: autograd triaged actionable
Dropout on sparse tensors module: sparse module: nn triaged enhancement,module: sparse module: nn triaged enhancement
Backward function causes device error in C++ when changing module's device repeatly. module: cpp module: autograd triaged,module: cpp module: autograd triaged
Python 3 build PyTorch got core dump needs reproduction module: crash triaged,needs reproduction module: crash triaged
[jit] Structural typing shouldn't work oncall: jit triaged,oncall: jit triaged
Unexpected behaviour for affine_grid and grid_sample with 3D inputs module: numerical-stability module: nn triaged,module: numerical-stability module: nn triaged
BatchNormFuncOptions object cant be printed in C++ module: cpp module: nn triaged,module: cpp module: nn triaged
Clarify tensor storage communication behavior in RPC module: docs feature triaged module: rpc,module: docs feature triaged module: rpc
how to do 3d data augmentation in parallel on the gpu? module: dataloader module: cuda triaged,module: dataloader module: cuda triaged
UserRRef should store error if it sees any and prevent subsequent usage feature triaged module: rpc,feature triaged module: rpc
Expected object of scalar type Double but got scalar type Float for argument #2 'mat2' in call to _th_mm triaged,triaged
pytorch 1.4.0 hangs when using with CUDA   >=  10.1 needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
Cannot JIT functions with custom backwards (e.g. swish) oncall: jit module: autograd triaged,oncall: jit module: autograd triaged
"Simple C++ custom autograd function code throws error ""CUDA error: driver shutting down"" module: cpp module: autograd triaged",module: cpp module: autograd triaged
LibTorch API on Mobile module: cpp triaged oncall: mobile,module: cpp triaged oncall: mobile
libtorch for Windows. MNIST example does no work. module: cpp triaged,module: cpp triaged
TensorBoard add_scalars throws error when dict has keys of type int triaged,triaged
USE_AVX/USE_AVX2 does not affect __AVX2__ macro defition triaged module: build warnings internals,triaged module: build warnings internals
allgather_coalesced for tensors of different types seems to be broken oncall: distributed triaged,oncall: distributed triaged
[discussion] Generic solutions for too-small-epsilon in FP16 training module: docs module: autograd triaged enhancement module: half,module: docs module: autograd triaged enhancement module: half
"nn.LSTM gives nondeterministic results with dropout and multiple layers, OR cuDNN version mismatch module: cudnn module: cuda triaged module: determinism",module: cudnn module: cuda triaged module: determinism
Wrong conv2d output on GPU when kernel has many zeros module: dependency bug module: numerical-stability module: cuda module: convolution triaged,module: dependency bug module: numerical-stability module: cuda module: convolution triaged
Caffe2 utility_ops_gpu_test fails on Windows module: tests triaged,module: tests triaged
Caffe2 ReshapeOpGPUTest crashes on Windows module: tests triaged,module: tests triaged
Caching Support for class Dataset module: dataloader triaged enhancement,module: dataloader triaged enhancement
"Dimension reducing variants of bitwise operations (bitwise_or, bitwise_and, bitwise_xor) triaged function request module: reductions",triaged function request module: reductions
How do you change Adam learning rate since the latest commits ? module: cpp triaged,module: cpp triaged
Caffe2 generate_proposals_op_gpu_test crashes on Windows module: tests triaged,module: tests triaged
Integration of Large Model Support in PyTorch module: internals feature low priority module: memory usage triaged,module: internals feature low priority module: memory usage triaged
Randomly error reports awaiting response (this tag is deprecated) triaged,awaiting response (this tag is deprecated) triaged
"Could not find any similar ops to ""foo..."" in the `Libtorch` triage review oncall: jit triaged",triage review oncall: jit triaged
Performance bug with convolutions with weights and inputs of similar spatial size module: performance module: cudnn module: convolution triaged,module: performance module: cudnn module: convolution triaged
Increased memory usage in repetitive torch.jit.trace calls high priority oncall: jit triaged,high priority oncall: jit triaged
libtorch_global_deps.so not found. module: build triaged module: regression,module: build triaged module: regression
caffe2 `DEPTHWISE3x3.Conv` test is broken caffe2 module: tests triaged,caffe2 module: tests triaged
Apple Review Rejected. ITMS-90338: Non-public API usage In Pytorch For iOS triaged module: ios,triaged module: ios
Fail to load torch script model oncall: jit triaged,oncall: jit triaged
Support bool input tensors for argmax / argmin / sort / topk and other functions triaged enhancement module: reductions,triaged enhancement module: reductions
torch.bernoulli and torch.rand and torch.randint to support dtype=torch.bool kwarg (and operation of tensor.uniform_ / tensor.random_ on bool tensors) module: distributions triaged,module: distributions triaged
RuntimeError during converting Reformer model to TorchScript triage review oncall: jit triaged,triage review oncall: jit triaged
TorchScript can't use lists in conditionals oncall: jit triaged enhancement,oncall: jit triaged enhancement
Support tensor.cumsum() for 1-dim tensors triaged module: numpy,triaged module: numpy
Multiprocessing map gets stuck if doing inference on loaded model module: multiprocessing triaged,module: multiprocessing triaged
Shared-QK transformer for the transformer (nn.activation.MultiheadAttention) module in PyTorch? triaged needs research oncall: transformer/mha,triaged needs research oncall: transformer/mha
[JIT] dropout fails on legacy mode oncall: jit triaged,oncall: jit triaged
"torch::normal only supports (double, double), but at::normal supports (double, double) / (double, Tensor) / (Tensor, double) / (Tensor, Tensor) module: cpp triaged",module: cpp triaged
PyTorch / libtorch executables fail when built against libcuda stub library high priority module: binaries module: build triaged,high priority module: binaries module: build triaged
"MacOS Error: subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '12']' returned non-zero exit status 1. module: build module: cuda triaged module: macos",module: build module: cuda triaged module: macos
torch.jit.script works for x*sigmoid(x) but not for x*sin(x) oncall: jit triaged,oncall: jit triaged
runtime error when loding heavy dataset needs reproduction triaged,needs reproduction triaged
NCCL version upgrade for PyTorch module: build triaged,module: build triaged
Method/constructor which takes as input angle and magnitude and returns a complex tensor triaged module: complex,triaged module: complex
ONNX export support for sparse tensors module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Why C++ version libtorch so slow oncall: jit triaged,oncall: jit triaged
Using profiler to profile distributed autograd code can lead to misleading results triaged module: rpc oncall: profiler,triaged module: rpc oncall: profiler
Raspberry Pi Zero build fails module: build triaged,module: build triaged
The BatchNorm error in `DataParallel` module: nn triaged module: data parallel,module: nn triaged module: data parallel
hasSpecialCase INTERNAL ASSERT FAILED: We don't have an op for aten::to but it isn't a special case. oncall: jit triaged,oncall: jit triaged
Documentation doesn't cover MWE using launch.py script todo oncall: distributed triaged,todo oncall: distributed triaged
Segfault when using misaligned data pointer (from joblib) module: crash triaged module: numpy,module: crash triaged module: numpy
Autograd Engine leaks reentrant threadpool threads on deletion module: autograd triaged better-engineering,module: autograd triaged better-engineering
Customize batch size based on  gpu id triaged enhancement module: data parallel,triaged enhancement module: data parallel
JIT does not support class instance attribute type annotation oncall: jit triaged enhancement days,oncall: jit triaged enhancement days
"zip(list, tuple) throws an non-actionable error message oncall: jit triaged days TSRootCause:TypeChecking TSUsability",oncall: jit triaged days TSRootCause:TypeChecking TSUsability
undefined symbols when using  libtorch and ITK module: build triaged,module: build triaged
Generator C++ API should match Python API module: cpp triaged module: random,module: cpp triaged module: random
"[JIT] If a python function type comment is referring to a wrong type, JIT frontend gives a not helpful error message oncall: jit triaged",oncall: jit triaged
Options for printing the shape with print(tensor) module: printing triaged,module: printing triaged
Make operator registrations truly commutative using priority triaged module: dispatch,triaged module: dispatch
Deprecate and remove RegisterOperators triaged module: dispatch,triaged module: dispatch
Rename Dispatcher::findSchema to Dispatcher::findOperator triaged module: dispatch,triaged module: dispatch
How install old version pytorch 1.2.0 from source? module: build triaged,module: build triaged
Refactor record_function_ops.cpp to not use cpp_custom_type_hack oncall: jit triaged oncall: profiler,oncall: jit triaged oncall: profiler
`conv2d` is slow with specific shapes of channels_last tensors module: performance module: cudnn triaged module: memory format,module: performance module: cudnn triaged module: memory format
"[feature request] Sparse (hybrid sparse-dense) output option for topk, min, max module: sparse triaged",module: sparse triaged
Equality operator for torch.distribution.* module: distributions triaged enhancement,module: distributions triaged enhancement
TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function needs reproduction oncall: jit triaged,needs reproduction oncall: jit triaged
Issues with DataParallel on Multiple GPUs triaged module: data parallel,triaged module: data parallel
Enable OpaqueTensor to possess Storage then allow it to view from CPUTensor module: internals triaged module: mkldnn,module: internals triaged module: mkldnn
[JIT]torch.jit.export invaild for pytorch1.4 needs reproduction oncall: jit triaged,needs reproduction oncall: jit triaged
libtorch memory leak module: cpp module: memory usage triaged,module: cpp module: memory usage triaged
Expanded tensors don't work as registered buffers module: nn module: error checking triaged module: partial aliasing,module: nn module: error checking triaged module: partial aliasing
Set the number of CUDAStream feature module: cuda triaged,feature module: cuda triaged
Extend nn.functional.softmax for arbitrary dimensions module: nn triaged function request,module: nn triaged function request
Flaky test test_ctc_loss_cuda on Windows high priority module: autograd module: cuda triaged module: flaky-tests,high priority module: autograd module: cuda triaged module: flaky-tests
[jit] `__hash__` magic method is missing oncall: jit low priority,oncall: jit low priority
[torch.jit.script] Allow `range` to index into Tensor oncall: jit triaged enhancement,oncall: jit triaged enhancement
[torch.jit.script] Support tensor indexing after an Ellipsis oncall: jit triaged enhancement,oncall: jit triaged enhancement
[JIT] Tensor.add_ incorrect schema matching on overloads oncall: jit triaged,oncall: jit triaged
"Semantic differences between forward and backward of upsampling2d/3d in channels last (NHWC, NDHWC) format. triaged module: memory format",triaged module: memory format
Is it possible to run an object detection android app by using Pytorch Mobile? triaged module: android,triaged module: android
keepdim for Tensor.select? triaged function request module: viewing and reshaping,triaged function request module: viewing and reshaping
"Unsupported ONNX op (Upsample 3D, bicubic) contrary to documentation module: nn triaged module: vision",module: nn triaged module: vision
Add ability to return a copy to Module's state_dict member function. module: nn triaged enhancement,module: nn triaged enhancement
General Purpose Faulty RPC Agent oncall: distributed triaged better-engineering module: rpc,oncall: distributed triaged better-engineering module: rpc
CPU softmax performance poor when dim is not the last dimension module: performance module: cpu triaged,module: performance module: cpu triaged
Maybe gpu_kernel shouldn't ASSERT_HOST_DEVICE_LAMBDA triaged,triaged
torch.jit.script report error when using index to subscript nn.ModuleList triage review oncall: jit,triage review oncall: jit
Pytorch report INTERNAL ASSERT FAILED at ..\\torch\\csrc\\jit\\ir.cpp:1529 when use torch.jit.script to convert to model needs reproduction oncall: jit triaged,needs reproduction oncall: jit triaged
RuntimeError: derivative for grid_sampler_2d_backward is not implemented triaged module: interpolation,triaged module: interpolation
[feature request]Support dilation parameter for unfold2d_* function (slow cpu maxpool2d #28733) module: performance module: nn triaged module: mkldnn module: pooling function request,module: performance module: nn triaged module: mkldnn module: pooling function request
Confusing error message of tensor constructor when passing a storage triaged,triaged
"Support creating a CPU tensor from ctypes pointer in Python / from_blob(ptr, shape, strides, dtype) module: docs module: memory usage triaged enhancement module: numpy",module: docs module: memory usage triaged enhancement module: numpy
A better way to show users all build options module: build triaged,module: build triaged
Eigen version for PyTorch ? module: build triaged,module: build triaged
Multiple CMake target errors ever since commit 0e52627358 module: build triaged,module: build triaged
Significant speed difference between P100 and V100 module: performance module: cuda triaged module: cublas,module: performance module: cuda triaged module: cublas
Restructure `multi_head_attention_forward` triaged oncall: transformer/mha,triaged oncall: transformer/mha
Backward operations not decorated with stashed seq marker triaged oncall: profiler,triaged oncall: profiler
Strange behaviour of F.interpolate with bicubic mode. module: nn triaged,module: nn triaged
Expose chunk_sizes for DataParallel module: nn triaged enhancement module: data parallel,module: nn triaged enhancement module: data parallel
JIT: Tracing faster than scripting oncall: jit triaged,oncall: jit triaged
[Feature request] Query padding mask for nn.MultiheadAttention module: nn triaged oncall: transformer/mha,module: nn triaged oncall: transformer/mha
Mac build from source error  module: build triaged oncall: mobile module: xnnpack,module: build triaged oncall: mobile module: xnnpack
error in pytorch Docs: https://pytorch.org/docs/stable/distributions.html module: docs triaged,module: docs triaged
PyTorch build with Ideep support? module: build triaged module: mkldnn,module: build triaged module: mkldnn
.detach() behaves differently for dense tensors vs sparse tensors module: sparse triaged,module: sparse triaged
Better testing of the autograd engine module: autograd triaged better-engineering,module: autograd triaged better-engineering
[JIT legacy executor] device propagation regression oncall: jit triaged,oncall: jit triaged
[jit] Hook support tracking high priority triage review oncall: jit triaged,high priority triage review oncall: jit triaged
PyTorch GPU memory allocation module: cuda triaged,module: cuda triaged
Inconsistent semantics of converting inf/-inf to long module: docs triaged,module: docs triaged
`torch.norm` is 113x slower than `torch.sqrt(a**2 + b**2)` module: performance triaged module: linear algebra,module: performance triaged module: linear algebra
CustomFromMask pruning stores a copy of the user-provided mask module: nn triaged module: pruning,module: nn triaged module: pruning
RuntimeError with torch.unique: radix_sort: failed on 2nd step: invalid argument module: cuda triaged module: sorting and selection,module: cuda triaged module: sorting and selection
[docs] Unclear arg spec for torch.full module: docs triaged module: numpy,module: docs triaged module: numpy
END_HANDLE_TH_ERRORS_PYBIND prevents pybind11 Exception translation triaged module: pybind,triaged module: pybind
ExceptionWrapper cannot handle exceptions with more than one argument module: dataloader module: error checking triaged,module: dataloader module: error checking triaged
torch::jit::script::Object::attr should throw AttributeError instead of RuntimeError oncall: jit triaged,oncall: jit triaged
autograd with TorchScript does not match finite differences high priority triage review oncall: jit,high priority triage review oncall: jit
Autograd deep copy avoidance optimization unsound in the presence of views module: autograd triaged,module: autograd triaged
RPC API Changes for TensorPipes triaged module: rpc,triaged module: rpc
[C++ API Parity] Incorrect documentation for optim initialization in serialization docs module: docs module: cpp module: optimizer triaged,module: docs module: cpp module: optimizer triaged
pip --requirement installs incorrect CUDA version module: dependency bug module: binaries oncall: releng triaged,module: dependency bug module: binaries oncall: releng triaged
libtorch.so file size is very large high priority module: binaries module: cpp triaged oncall: mobile quansight-nack,high priority module: binaries module: cpp triaged oncall: mobile quansight-nack
Model loaded in C++ runtime is not thread safe module: cpp triaged module: sanitizers,module: cpp triaged module: sanitizers
_broadcast_coalesced_reshape doesn't respect zero-size tensor triaged small module: data parallel,triaged small module: data parallel
TensorPipes RPC Agent Default Args/Result Device Mapping triaged module: rpc,triaged module: rpc
TensorPipes RPC Agent Multiple Placement Retries triaged module: rpc,triaged module: rpc
TensorPipes RPC Agent Message Acknowledgements triaged module: rpc,triaged module: rpc
TensorPipes RPC Agent Listener shortcut triaged module: rpc,triaged module: rpc
Initialize TensorPipe RPC Agent Transport triaged module: rpc,triaged module: rpc
TensorPipes RPC Agent CUDA Support module: cuda triaged module: rpc,module: cuda triaged module: rpc
Docs for uniform_ don't make any sense module: distributions triaged,module: distributions triaged
Libtorch segfault when used with libqpOASES  module: build triaged,module: build triaged
[jit] Python arg parser / TorchScript incompatibilities oncall: jit triaged,oncall: jit triaged
Torchscript incompatible with torch.cat for tensor lists oncall: jit triaged,oncall: jit triaged
cuDNN batchnorm with non-contiguous running mean silently discards updates module: cudnn triaged,module: cudnn triaged
Memory leak in embedding layer and LSTM module: memory usage triaged,module: memory usage triaged
Possibility to support int4 data type feature triaged,feature triaged
Unclear output for Pytorch Profiler module: docs module: autograd triaged,module: docs module: autograd triaged
[jit] Returning different types with `Any` segfaults oncall: jit triaged,oncall: jit triaged
"Models saved in C++ LibTorch with torch::save, cannot be loaded in python using torch.load module: cpp module: serialization triaged",module: cpp module: serialization triaged
Can't get module gradient in autograd.Function's custom backward when DataParallel is used triaged module: data parallel,triaged module: data parallel
Add `start_process_in_context` to `torch.multiprocessing` module: multiprocessing triaged enhancement,module: multiprocessing triaged enhancement
Deepcopy fails with nn.parallel.replicate triaged module: data parallel,triaged module: data parallel
[JIT] BroadcastingList annotations don't work with ignore'd functions oncall: jit triaged,oncall: jit triaged
Support pipelining the backward pass and optimizer.step() for distributed autograd. oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
Tensor.random_ is not implemented for bfloat16 on CPU(but implemented on CUDA) module: cpu triaged module: random,module: cpu triaged module: random
JIT tracing check fails with boolean tensor modifications triage review oncall: jit actionable,triage review oncall: jit actionable
Some module has incorrect scope in complex naming situations in tensorboard graph export triaged module: tensorboard,triaged module: tensorboard
Decouple Lifetime of Local RRef and RPC oncall: distributed triaged,oncall: distributed triaged
[dev] `RecursiveScriptModule` does not expose `jit.ignore`d methods oncall: jit triaged,oncall: jit triaged
Some module info is missing in nested graph for tensorboard triaged module: tensorboard,triaged module: tensorboard
"/usr/bin/x86_64-linux-gnu-ld: warning: libcusparse.so.10.0, needed by /pytorch_master/build/lib/libtorch_cuda.so, not found (try using -rpath or -rpath-link) module: build module: cuda triaged",module: build module: cuda triaged
torch.rand() not having same values on using torch.manual_seed(0) triaged module: random,triaged module: random
master build error caffe2,caffe2
Remove .data subset1 for fixathon module: autograd triaged enhancement better-engineering actionable fixathon,module: autograd triaged enhancement better-engineering actionable fixathon
empty_sparse shouldn't be called with memory layout but is module: sparse triaged module: memory format module: tensor creation,module: sparse triaged module: memory format module: tensor creation
"When a Node fails to resolve to an Operator, print out the types of arguments, and all ""close matches"" in known operators oncall: jit triaged",oncall: jit triaged
jit.trace checker fails for LSTM  oncall: jit,oncall: jit
Distributed Data Parallel for computation graphs that make RPCs in forward() oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
Make ArrayRef::size() return int64_t rather than size_t module: internals triaged,module: internals triaged
torch._C.Node.scopeName() missing in pytorch 1.4 oncall: jit triaged,oncall: jit triaged
It seems nn.Sequential.add_module() could take duplicate names module: nn triaged,module: nn triaged
IterableDataset with num_workers > 0 and drop_last=True drops more instances than expected feature module: dataloader triaged,feature module: dataloader triaged
PyTorch 1.4.0 does not support using `Module` or `ModuleList` in attribute annotations in ScriptModule oncall: jit triaged,oncall: jit triaged
It is not good to separate the steps of modules making and forward computation feature triaged,feature triaged
Sobol engine generates out-of-bounds samples after drawing too many samples triaged module: random,triaged module: random
Illegal instruction: 4 - OSX 10.13.6 install from source module: build triaged module: macos,module: build triaged module: macos
torch.clamp_ not inplace during backward module: autograd module: memory usage triaged,module: autograd module: memory usage triaged
Expose `internal::GRAIN_SIZE` through Python API. triaged enhancement module: multithreading,triaged enhancement module: multithreading
Make it easier to add new messages in RPC layer triaged better-engineering module: rpc,triaged better-engineering module: rpc
Dropout of attention weights in function F.multi_head_attention_forward() breaks sum-to-1 constraint module: nn triaged,module: nn triaged
Long torchscript warmup times can be problematic for production serving oncall: jit triaged,oncall: jit triaged
[jit] `Node` callstack is incorrect oncall: jit triaged,oncall: jit triaged
 malloc(): memory corruption (fast) triaged module: data parallel,triaged module: data parallel
how-to-adjust-learning-rate using libtorch triaged,triaged
Creating Torch tensors slows OpenCV video reading a lot module: performance triaged module: multithreading,module: performance triaged module: multithreading
torch::var_out and dimnames module: cpp triaged module: named tensor,module: cpp triaged module: named tensor
Don't take TensorOptions by reference module: cpp triaged,module: cpp triaged
[feature request] torch.expand to not require unsqueeze and match -1 to existing dimensions if tensor.shape.count(-1) == tensor.ndim triaged enhancement,triaged enhancement
Training got stuck due to timeout from dataloader module: performance module: dataloader triaged,module: performance module: dataloader triaged
iOS libtorch superpoint model bug module: macos oncall: mobile module: ios shadow review,module: macos oncall: mobile module: ios shadow review
"Uninitialised value was created by a stack allocation, reported by valgrind triage review oncall: jit triaged",triage review oncall: jit triaged
Core dumps being created when running test_c10d.py and test_multiprocessing_spawn.py module: tests triaged,module: tests triaged
PyTorch 1.4.0 CUDA initialization error with CPU-only (multiprocessing) on Python 3.7.5 module: autograd module: cuda triaged,module: autograd module: cuda triaged
[v1.5] Python/C++ API parity master tracking task module: cpp triaged,module: cpp triaged
[feature request] [dataloader] Introduce Dataset.__collate__ module: dataloader triaged enhancement needs research,module: dataloader triaged enhancement needs research
TensorIterator does not work with different input/output types triaged enhancement module: vectorization module: TensorIterator,triaged enhancement module: vectorization module: TensorIterator
Support multiple-build-type generators for CMake module: windows triaged,module: windows triaged
Python package using CMake module: build low priority triaged enhancement,module: build low priority triaged enhancement
[feature request] make torch.multinomial behaviour compliant with rnn output dimension module: distributions module: bc-breaking feature triaged,module: distributions module: bc-breaking feature triaged
[RFC] Add ability to get all remote parameters when constructing DistributedOptimizer. triaged module: rpc,triaged module: rpc
Move the custom pass execution back to the beginning of runNondiffOptimization oncall: jit triaged,oncall: jit triaged
InlineAutodiffSubgraphs in JIT inlines non-differentiable custom groups unexpectedly. oncall: jit module: custom-operators actionable,oncall: jit module: custom-operators actionable
test_baddbmm_cpu_float32 fails locally for me when built with DEBUG=1 module: tests triaged small,module: tests triaged small
F.max_pool*d/F.min_pool*d should support integer dtypes and bool tensors triaged module: pooling function request,triaged module: pooling function request
not able to import *  from fastai.vision  in Google collab triaged,triaged
DataParallel gives different gradients when using LSTMs triaged module: determinism module: data parallel,triaged module: determinism module: data parallel
Recover from CUDA runtime error module: cuda triaged module: third_party,module: cuda triaged module: third_party
Warning when link libtorch and opencv4.2.0 together module: build triaged better-engineering,module: build triaged better-engineering
torch.nn.functional import grid_sample needs reproduction triaged,needs reproduction triaged
bytearray(tensor) behaves very differently from bytearray(tensor.numpy()) module: printing triaged module: numpy,module: printing triaged module: numpy
[debatable] Better infer dtype in torch.as_tensor triaged module: tensor creation,triaged module: tensor creation
Support batch linear transformation module: nn triaged module: batching function request,module: nn triaged module: batching function request
Scripting fails to preserve attribute aliasing oncall: jit triaged,oncall: jit triaged
Torch not compiled with CUDA enabled awaiting response (this tag is deprecated) module: binaries module: cuda triaged,awaiting response (this tag is deprecated) module: binaries module: cuda triaged
Upper/Lower attributes for named dimensions for proper Ricci notation and to generalize matrix operations triaged module: named tensor,triaged module: named tensor
matmul: no warning when contracting differently named dimensions triaged module: named tensor,triaged module: named tensor
torch batchwise max with indices triaged module: batching function request module: reductions,triaged module: batching function request module: reductions
[jit] Dict set item type mismatch error doesn't say the type was inferred oncall: jit triaged,oncall: jit triaged
[JIT] pytorch 1.4 breaks torch.jit.script(LSTM/GRU) triage review oncall: jit triaged has workaround,triage review oncall: jit triaged has workaround
"which pytorch do i install for running this project in my windows.link for this is ::  https://github.com/xiaojunxu/SQLNet  .IN this they have used python 2.7 ,but i am unable to install pytorch on python 2.7 environment .help me with this module: binaries triaged",module: binaries triaged
Operation Registration Error triaged module: dispatch,triaged module: dispatch
Connect timeout feature do not work in DDP with TCPStore oncall: distributed triaged,oncall: distributed triaged
"Torchscript used to work, but now it fails with VariableTensorId error oncall: jit triaged module: android oncall: mobile",oncall: jit triaged module: android oncall: mobile
Interpolate in the â€œbicubicâ€ mode with the same shape outputs zeros from second sample onwards high priority triaged,high priority triaged
 from torch._C import * (ImportError: DLL load failed) triaged,triaged
Optional seq_len argument to torch.nn.utils.rnn.pad_sequence triaged function request module: padding,triaged function request module: padding
Some questions Concerning Intel's DNNL(MKLDNN) support in Pytorch (adding support for Intel Processors GPUs) by transitioning to DNNL triaged module: mkldnn,triaged module: mkldnn
[RFC] Nested scopes in autograd profiler should support RPC calls properly. module: autograd triaged module: rpc,module: autograd triaged module: rpc
CUDNN_STATUS_EXECUTION_FAILED module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
Confusing error messages of tensor.scatter_ on both CPU and CUDA module: docs module: error checking triaged module: scatter & gather ops,module: docs module: error checking triaged module: scatter & gather ops
__cuda_array_interface__ conversion does not support readonly arrays module: internals module: cuda triaged module: numba,module: internals module: cuda triaged module: numba
"[feature request] np.packbits / np.unpackbits, general BitTensors (maybe can be just tensors with dtype torch.bits8 or have a new dtype torch.bits introduced) and bit packed tensors utilities for saving memory / accesses, support for BitTensors wherever BoolTensors are used high priority feature triaged module: boolean tensor",high priority feature triaged module: boolean tensor
Silent failing of batch_sampler when the data points are lists of tensors. module: dataloader triaged module: nestedtensor,module: dataloader triaged module: nestedtensor
Can we add support for Enum in scripted models? oncall: jit triaged,oncall: jit triaged
MKLDNN doesnt work and is slower than normal cpu mode triaged module: mkldnn,triaged module: mkldnn
Error tracing custom autograd.Function oncall: jit triaged,oncall: jit triaged
Implement Backend-Agnostic RPC functionality in RpcAgent triaged better-engineering module: rpc,triaged better-engineering module: rpc
Comments Separating Class Methods from Different Classes in C++ Files triaged better-engineering,triaged better-engineering
Error while trying to build pytorch from source in conda environment module: build triaged,module: build triaged
Documentation is not loaded by IDEs module: docs triaged,module: docs triaged
JIT performance discrepancies module: performance oncall: jit triaged,module: performance oncall: jit triaged
RPC mock mode for unit tests. triaged module: rpc,triaged module: rpc
PyTorch freezes on second call to scripted densenet model from torchvision oncall: jit triaged module: vision,oncall: jit triaged module: vision
Torch serialization does not restore tensors properly when custom __reduce__ is defined module: serialization triaged,module: serialization triaged
Add Julia Bindings to Torch backend  feature low priority triaged module: language binding,feature low priority triaged module: language binding
torch.tensordot has inconsistent signature with torch script oncall: jit triaged,oncall: jit triaged
ninja: build stopped: subcommand failed. module: build triaged,module: build triaged
How to customize build torchscript model to be used in end devices codebase oncall: jit triaged oncall: mobile,oncall: jit triaged oncall: mobile
Building PyTorch ignores my current Python version module: build triaged,module: build triaged
Second-order gradient cause segfault over time needs reproduction triaged,needs reproduction triaged
[Tensorboard] Problem with subfolders from SummaryWriter triaged module: tensorboard,triaged module: tensorboard
changed format of trace graph in torch 1.4.0 oncall: jit triaged,oncall: jit triaged
TORCH_CUDA_API export failure on torch::cuda::nccl::detail::throw_nccl_error(ncclResult_t) high priority module: binaries oncall: releng triaged module: nccl quansight-nack,high priority module: binaries oncall: releng triaged module: nccl quansight-nack
Logical AND and OR for Tensors in C++ API. module: cpp triaged function request,module: cpp triaged function request
[FYI] MultiheadAttention / Transformer proposal accepted module: nn triaged oncall: transformer/mha,proposal accepted module: nn triaged oncall: transformer/mha
User-friendly handling of types and devices triaged module: numpy module: ux,triaged module: numpy module: ux
Segmentation Fault (core dumped) with 1.4.0 needs reproduction triaged,needs reproduction triaged
SGD optimizer with deprecation warning module: optimizer triaged module: deprecation,module: optimizer triaged module: deprecation
Document different gradient wrt to jax when nesting module: docs module: autograd triaged,module: docs module: autograd triaged
Interrupting a DDP worker while using the CUDA MPS server causes CUDA to hang until reboot oncall: distributed module: cuda triaged,oncall: distributed module: cuda triaged
[feature request] Out-variant and dtype argument for torch.argmax / torch.argmin / torch.argsort (and friends) module: memory usage triaged module: sorting and selection function request module: reductions,module: memory usage triaged module: sorting and selection function request module: reductions
Reduce RPC branches for Python/Built-inOp/TorchScript oncall: jit triaged module: rpc,oncall: jit triaged module: rpc
MultivariateNormal.rsample: use eigen-decomposition when Cholesky fails module: distributions triaged enhancement module: linear algebra,module: distributions triaged enhancement module: linear algebra
"""Tried to register multiple operators with the same name and the same overload name"" error is confusing triaged module: dispatch better-engineering",triaged module: dispatch better-engineering
[jit] Use `typing.get_type_hints` instead of parsing types manually oncall: jit triaged,oncall: jit triaged
Batched torch.eig() and gradient of torch.eig() for real eigenvalues module: autograd triaged module: batching module: linear algebra,module: autograd triaged module: batching module: linear algebra
Bug in add_param - functionality for tensorboard (scatter matrix view) module: tensorboard oncall: visualization,module: tensorboard oncall: visualization
Pytorch 1.4 does not detect gpu module: cuda triaged,module: cuda triaged
TorchScript C++ API Tracking Issue oncall: jit triaged,oncall: jit triaged
Tensor.random_ is not implemented for bool on CUDA(but implemented on CPU) module: cuda triaged module: random,module: cuda triaged module: random
LibTorch operates very slowly on data blobs from GPU module: performance module: cuda triaged,module: performance module: cuda triaged
Don't unnecesarily send cleanup dist autograd context RPCs to other nodes module: bootcamp triaged better-engineering module: rpc,module: bootcamp triaged better-engineering module: rpc
"RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch. oncall: distributed triaged",oncall: distributed triaged
TracerWarning When Using Tensor Size in Torchscript Trace oncall: jit triaged,oncall: jit triaged
[JIT] Make `torch.jit.script` work on all objects which we can represent as IValues oncall: jit triaged,oncall: jit triaged
DataParallel does not work with sparse parameters triaged module: data parallel,triaged module: data parallel
Segmentation fault in lazyInitCUDA -> CUDAHooks::initCUDA -> THCMagma_init -> magma_init needs reproduction module: cuda triaged module: third_party,needs reproduction module: cuda triaged module: third_party
[C++] Don't use DeprecatedTypeProperties in torch::utils::reorder_tensors_like module: internals triaged,module: internals triaged
Segmentation fault in C++ API torch::from_blob(...).clone() module: cpp triaged,module: cpp triaged
MAGMA libraries module: binaries module: build triaged,module: binaries module: build triaged
Is there a way to use SYSTEM_INSTALLED 3rdparty libraries? module: build triaged,module: build triaged
DistributedStreamSampler: support stream sampler in distributed setting oncall: distributed module: cpp triaged,oncall: distributed module: cpp triaged
Build without MKL is not possible when MKL is installed module: build triaged module: mkl,module: build triaged module: mkl
KL divergence for diagonal Gaussian distributions module: distributions feature triaged,module: distributions feature triaged
How to accelerate the compiling of pytorch  module: build triaged,module: build triaged
torchvision triaged module: vision,triaged module: vision
[JIT] Compile group of functions/modules oncall: jit triaged,oncall: jit triaged
Make _rpc_sync_torchscript and _rpc_async_torchscript work with autograd profiler oncall: jit triaged,oncall: jit triaged
Migrate processGroup::async_work to use Future oncall: distributed triaged,oncall: distributed triaged
Backward `Functional.conv3d` is slow when cuDNN is enabled module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
[jit] Calling `torch.jit.script` on `@staticmethod`s which in turn call other `@staticmethod`s results in a `TypeError` oncall: jit triaged,oncall: jit triaged
Possible ProcessGroup::Work::abort correctness issue oncall: distributed triaged module: rpc,oncall: distributed triaged module: rpc
torch.bartlett_window not jitable oncall: jit triaged,oncall: jit triaged
Pytorch JIT Compilation Does Not Finish (Infinite Loop?) for Deeper Models oncall: jit triaged,oncall: jit triaged
"Problem with multiprocessing, custom __getstate__ with Tensors and forkserver module: multiprocessing module: serialization triaged",module: multiprocessing module: serialization triaged
Save LocalResponceNorm as a single aten node when convert a pytorch model to jit model oncall: jit triaged,oncall: jit triaged
deadlock when using mp.spawn multiprocessing module: multiprocessing triaged,module: multiprocessing triaged
[FR] bincount along arbitrary dimension triaged module: sorting and selection function request,triaged module: sorting and selection function request
[FR] histc should (optionally) return a long tensor triaged module: sorting and selection function request,triaged module: sorting and selection function request
[jit] Various problems calling `@staticmethod`s in 1.4.0 oncall: jit triaged,oncall: jit triaged
Truncated normal distribution module: distributions feature triaged,module: distributions feature triaged
Allow range in dim argument of reducing operations such as sum triaged module: ux function request module: reductions,triaged module: ux function request module: reductions
Python 3.8 Windows JIT test failure oncall: jit triaged,oncall: jit triaged
Support strategy to train large model that exceeds GPU mem and DRAM mem feature triaged module: rpc,feature triaged module: rpc
Numpy array functionality in torchscript. oncall: jit triaged,oncall: jit triaged
torch.nn.functional.threshold not work with LongTensor module: nn triaged enhancement,module: nn triaged enhancement
models after prepare/prepare_qat contains `qconfig` which prevents pickling oncall: quantization low priority triaged,oncall: quantization low priority triaged
" 0 INTERNAL ASSERT FAILED at /pytorch/c10/util/intrusive_ptr.h:348, please report a bug to PyTorch. triaged module: assert failure",triaged module: assert failure
Pytorch compilation error module: build triaged,module: build triaged
"dist.send/recv ""IndexError: map::at"" error when bool tensors are used with mpi backend triaged module: boolean tensor",triaged module: boolean tensor
pytorch_linux_xenial_cuda10_1_cudnn7_py3_slow_test triggered on all PRs module: ci triaged,module: ci triaged
Can't import torch on latest master module: build triaged,module: build triaged
"C++ randint returns float32, python returns int64 module: cpp triaged",module: cpp triaged
Serialization inconsistency with pickling tensors breaks caching module: serialization triaged,module: serialization triaged
Using Tensor.to(device) after distributed all_reduce intermittently causes deadlock with NCCL  oncall: distributed triaged module: nccl,oncall: distributed triaged module: nccl
pin_memory may change the type of instance returned by collate_fn. module: dataloader triaged,module: dataloader triaged
We should change DeprecationWarnings to UserWarnings in 27361 module: docs triaged,module: docs triaged
torch.nn.functional.normalize epsilon too small for half precision module: numerical-stability module: nn triaged module: half module: norms and normalization,module: numerical-stability module: nn triaged module: half module: norms and normalization
Slighty out of tolerance for `test_mv` and `test_cholesky_solve_batched_cuda_float64` module: tests triaged,module: tests triaged
Idempotency Keys for RPC Retry triaged module: rpc,triaged module: rpc
Inconsistent linking flags result in error when building lib/libc10_cuda.so module: build module: cuda triaged,module: build module: cuda triaged
TensorRT: CheckDims() need adjustment for EXPLICIT_BATCH? caffe2 triaged,caffe2 triaged
Improve cuda OOM message module: docs module: bootcamp module: cuda triaged,module: docs module: bootcamp module: cuda triaged
torch.gt ge lt le triaged module: type promotion,triaged module: type promotion
logsumexp with subtraction feature triaged module: reductions,feature triaged module: reductions
logical_not for Boolean tensor has peculiar behavior triaged module: boolean tensor,triaged module: boolean tensor
Batch convolutional layer with 5d weight tensor that is not contiguous feature triaged,feature triaged
build pytorch from source fauled: undefined reference to `cusparseGetErrorString(cusparseStatus_t)' module: build triaged,module: build triaged
Reduce degrees of freedom in strides so that they unambiguously specify layout permutation triaged enhancement,triaged enhancement
Sobol point implementation triaged module: random,triaged module: random
Internal assert failure for user IValue unwrapping oncall: jit triaged,oncall: jit triaged
Internal assert failed using multiple GPUs with DataParallel module: cuda triaged module: assert failure module: data parallel,module: cuda triaged module: assert failure module: data parallel
Extend `ConcatDataset` to return dataset index module: dataloader triaged enhancement small,module: dataloader triaged enhancement small
Dockerfile for people to quick-start contributing triaged enhancement module: docker,triaged enhancement module: docker
Inaccurate batched GRU results on CPU module: nn module: rnn module: cpu triaged module: numerical-reproducibility,module: nn module: rnn module: cpu triaged module: numerical-reproducibility
DistributedDataParallel non-floating point dtype parameter with requires_grad=False oncall: distributed triaged,oncall: distributed triaged
gradients inside gradient checkpoint module: checkpoint module: autograd triaged enhancement,module: checkpoint module: autograd triaged enhancement
Trying to disable cuda to run torch on OSX run it on CPU module: build module: cuda triaged,module: build module: cuda triaged
repeat_interleave Performance Issue module: performance good first issue triaged module: tensor creation,module: performance good first issue triaged module: tensor creation
Reuse spawned subprocesses in RPC tests triaged better-engineering module: rpc,triaged better-engineering module: rpc
JIT tests are linked directly into libtorch and register operators even when unused oncall: jit triaged,oncall: jit triaged
"Function request: logerfc, logerfcx special functions module: numerical-stability triaged module: numpy function request module: special",module: numerical-stability triaged module: numpy function request module: special
error when building pytorch 1.1.0 from source module: build triaged,module: build triaged
Support sparse inputs for torch.block_diag module: sparse triaged module: tensor creation function request,module: sparse triaged module: tensor creation function request
RPC and dist_autograd should respect no_grad mode feature triaged module: rpc,feature triaged module: rpc
I'm not able to build pytorch with tensorrt (current master) module: build triaged,module: build triaged
`index_add_` with multidimensional index triaged module: advanced indexing function request,triaged module: advanced indexing function request
Compile libtorch by source code failed. module: binaries module: cpp triaged,module: binaries module: cpp triaged
"torch.masked_select out argument can easily be misused, because output shape is dynamically computed high priority triaged module: numpy module: safe resize",high priority triaged module: numpy module: safe resize
"The dependency target ""nccl_external"" of target ""gloo_cuda"" does not exist. oncall: distributed module: build triaged",oncall: distributed module: build triaged
Example cmakelists for custom cuda operator? oncall: jit triaged,oncall: jit triaged
[Feature Request] reduce CUDA runtime size by selectively compiling PyTorch GPU kernels high priority module: binaries module: build module: cuda triaged,high priority module: binaries module: build module: cuda triaged
The model training time is increasing between runs if the same DataLoader reused to train multiple models. module: dataloader triaged,module: dataloader triaged
Negative indices in chunk could cause Out of Range access on loss.backward in JIT oncall: jit triaged,oncall: jit triaged
Fix softplus clampling issues using logsigmoid module: nn triaged,module: nn triaged
Support rpc/remote torch script call with script class/module name and class/module method name oncall: jit triaged module: rpc,oncall: jit triaged module: rpc
logsumexp: two little-impact perf suggestions module: performance module: cpu triaged module: reductions,module: performance module: cpu triaged module: reductions
Libtorch's files conflict with glog's file? module: build triaged,module: build triaged
How to distinguish different layers in hookï¼Ÿ module: nn triaged,module: nn triaged
ImportError: /home/xx/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so: undefined symbol: _ZNK5torch3jit5Graph8toStringE oncall: jit triaged,oncall: jit triaged
Inaccurate ValueError reporting in nn/functional.py module: nn triaged,module: nn triaged
CI test should use PR commit instead of pulling the latest master module: ci triaged,module: ci triaged
torch.poisson returns floating point tensor module: distributions triaged,module: distributions triaged
Documentation for `scatter` incorrectly states that index values must be unique module: bc-breaking module: docs triaged module: scatter & gather ops,module: bc-breaking module: docs triaged module: scatter & gather ops
Support Python builtins on iterators in JIT oncall: jit triaged,oncall: jit triaged
Allow using digits in names of named tensors triaged enhancement module: named tensor,triaged enhancement module: named tensor
DataLoader: Segmentation Fault (core dumped) high priority needs reproduction module: crash module: dataloader triaged,high priority needs reproduction module: crash module: dataloader triaged
Need a launch utility for Distributed RPC framework. triaged better-engineering module: rpc,triaged better-engineering module: rpc
sparse tensor eliminate_zeros module: sparse feature triaged,module: sparse feature triaged
AttributeError: module 'torch.distributed' has no attribute 'init_process_group' on torch 1.3 aarch64 oncall: distributed triaged,oncall: distributed triaged
Computing dot product of columns sliced from large matrix causes illegal memory access in CUDA module: dependency bug module: cuda triaged module: 64-bit module: cublas,module: dependency bug module: cuda triaged module: 64-bit module: cublas
the cmake problem with build from source ? module: binaries triaged,module: binaries triaged
DataLoader does not consider default floating point type module: dataloader triaged,module: dataloader triaged
Parallelization: more balanced work distribution among workers module: cpp feature triaged module: data parallel,module: cpp feature triaged module: data parallel
What is the significance of torchvision._is_tracing()?  triaged module: vision,triaged module: vision
Cuda error 59 : device-side assert triggered needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
Use of Sequence collections for abstract classes in Dataset feature module: dataloader triaged,feature module: dataloader triaged
"update embedding at indices, other than those passed as input, in the case of sparse tensors module: sparse module: nn triaged enhancement",module: sparse module: nn triaged enhancement
'torch.load' report 'bad pickle data' module: serialization triaged,module: serialization triaged
No auto-suggest capacity for Transformer triaged enhancement,triaged enhancement
ATen not compiled with MKL support module: build triaged module: mkl,module: build triaged module: mkl
pytorch forward hangs in multiprocess environment module: multiprocessing triaged,module: multiprocessing triaged
Option to apply weights to gradients when using DistributedDataParallel oncall: distributed feature module: autograd triaged module: data parallel,oncall: distributed feature module: autograd triaged module: data parallel
attribute and register_buffer are not the same on gpu  oncall: jit triaged,oncall: jit triaged
Bug in ForkingPickler for multiprocessing spawn context for shared storages on Linux module: multiprocessing triaged,module: multiprocessing triaged
Can't pin storage memory module: docs module: cuda triaged,module: docs module: cuda triaged
Force libtorch to use CUDA context oncall: jit triaged,oncall: jit triaged
More dynamic PyTorch APIs high priority triaged module: ux,high priority triaged module: ux
TorchBind broken on rocm oncall: jit triaged,oncall: jit triaged
"c++ PReLUFuncOptions declared, not used or valid triaged",triaged
[Feature Request] Make torch.solve output NaN for singular matrix triaged module: linear algebra,triaged module: linear algebra
IndexExpressions (or slice) for jit.script functions oncall: jit triaged,oncall: jit triaged
cuCtxGetDevice error and seg fault with DDP and OpenMPI oncall: distributed module: cuda triaged,oncall: distributed module: cuda triaged
DataParallel has different tensor copy behavior if batch size = 1 triaged module: data parallel,triaged module: data parallel
Slow clip_grad_norm_ because of .item() calls when run on device module: performance module: cuda triaged module: norms and normalization,module: performance module: cuda triaged module: norms and normalization
Mnasnet0_5 first layer shape incorrect triaged module: vision,triaged module: vision
Distributed hangs on process termination with world_size=1 oncall: distributed module: bootcamp triaged,oncall: distributed module: bootcamp triaged
Make RRef.to_here() non-blocking triaged module: rpc,triaged module: rpc
DataParallel doesn't properly handle kwargs module: nn triaged module: data parallel,module: nn triaged module: data parallel
Integrate `torch.xxx` and `Tensor.xxx` triaged module: ux,triaged module: ux
[jit] `del` with slices doesn't work oncall: jit triaged,oncall: jit triaged
cuDNN convolution does not handle empty input tensor module: cudnn module: cuda triaged small,module: cudnn module: cuda triaged small
Add option in LSTM layer to access all cell states of all time steps feature module: nn module: rnn triaged,feature module: nn module: rnn triaged
Support DataParallel with PackedSequence oncall: distributed module: bootcamp triaged enhancement module: data parallel,oncall: distributed module: bootcamp triaged enhancement module: data parallel
torch.scatter_logsumexp triaged function request module: scatter & gather ops,triaged function request module: scatter & gather ops
"Error ""builtin cannot be used as a value"" when add Python snippets in C++ oncall: jit low priority triaged",oncall: jit low priority triaged
[jit] Python type hints in TorchScript classes don't work oncall: jit triaged,oncall: jit triaged
TestMomentumSGD.test_fp16momentum_sgd (caffe2) is flaky high priority triage review caffe2 module: flaky-tests,high priority triage review caffe2 module: flaky-tests
`clip_grad_norm` allows negative `max_norm` values module: nn triaged enhancement,module: nn triaged enhancement
sklearn and pytorch incompatibility issue needs reproduction module: crash triaged,needs reproduction module: crash triaged
DDP/MP not yielding nontrivial speedup module: performance module: multiprocessing triaged module: data parallel,module: performance module: multiprocessing triaged module: data parallel
Optimizing DLRM for CPU module: performance feature module: cpu triaged,module: performance feature module: cpu triaged
Memory management is inefficient which limits performance module: performance module: cuda module: memory usage triaged,module: performance module: cuda module: memory usage triaged
quantization - Missing operations needed for object detection oncall: quantization triaged,oncall: quantization triaged
Why Keras behave better and faster than Pytorch under the same network configuration? module: performance module: windows triaged,module: performance module: windows triaged
[mac] Failure to import torch triaged module: macos,triaged module: macos
torch runtime error when manual link libmkldnn.so module: build triaged module: mkldnn,module: build triaged module: mkldnn
Intel OMP multiprocessing assertion failure: Assertion failure at z_Linux_util.cpp(2338) module: dependency bug module: multiprocessing triaged,module: dependency bug module: multiprocessing triaged
Failed to config caffe2_rocksdb in cmake module: build triaged,module: build triaged
pinned memory requires DeviceGuard in multi-process envs module: docs module: multiprocessing module: cuda triaged,module: docs module: multiprocessing module: cuda triaged
Script method can't call a scripted function when it is decorated with `@torch.no_grad` oncall: jit triaged,oncall: jit triaged
[feature request] Better handling for CUDA Out of Memory module: cuda triaged,module: cuda triaged
Default shuffle behavior of DistributedSampler oncall: distributed module: dataloader triaged,oncall: distributed module: dataloader triaged
[RPC] Support nn.Module pickling with share memory triaged module: rpc,triaged module: rpc
[JIT] Slice with optional not supported oncall: jit triaged,oncall: jit triaged
Retain Subgraph or Save Intermediate Grad support? module: autograd triaged,module: autograd triaged
torch::nn::functional::interpolate crash module: cpp triaged,module: cpp triaged
[JIT] tensor(device=...) and tensor.to(device = ...) does not work properly in traced functions and modules oncall: jit triaged,oncall: jit triaged
[jit] Python objects as arguments are not mutated oncall: jit low priority triaged,oncall: jit low priority triaged
torch.nn.Softplus threshold argument bug? module: nn triaged,module: nn triaged
Add torch.version.nccl oncall: distributed module: bootcamp triaged enhancement module: nccl,oncall: distributed module: bootcamp triaged enhancement module: nccl
Common lookup of generic types across full script and mobile parsers feature triaged,feature triaged
float[] unsupported in native_functions.yaml triaged enhancement,triaged enhancement
MathJax too small in Firefox module: docs triaged,module: docs triaged
Pytorch 1.3.0 on RTX cards: CUDA error: an illegal memory access was encountered module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
"GPU version of minimal example for libtorch fails with ""no kernel image is available..."" module: cuda triaged",module: cuda triaged
The inference speed of the torch compiled manually is slower than the torch build from official binaries? needs reproduction module: binaries module: performance triaged,needs reproduction module: binaries module: performance triaged
Remove `.data` module: autograd triaged enhancement better-engineering actionable,module: autograd triaged enhancement better-engineering actionable
Categorical.sample too slow module: distributions triaged,module: distributions triaged
JIT breaks with postponed annotations oncall: jit triaged,oncall: jit triaged
How can I add masks to parameters module: nn triaged,module: nn triaged
error C3203: â€œtemplated_iteratorâ€: æœªä¸“ç”¨åŒ–çš„ ç±» æ¨¡æ¿ ä¸èƒ½ç”¨ä½œ æ¨¡æ¿ å˜é‡ï¼Œè¯¥å˜é‡å±žäºŽ æ¨¡æ¿ å‚æ•°â€œ_Ty1â€ï¼Œåº”ä¸º real ç±»åž‹ needs reproduction module: internals triaged,needs reproduction module: internals triaged
false CHECK FAILED at ../aten/src/ATen/core/function_schema_inl.h module: internals triaged,module: internals triaged
Wrong initialization with kaiming_uniform_ module: nn triaged,module: nn triaged
Spurious negative output in convolution of positive tensors module: docs module: convolution triaged,module: docs module: convolution triaged
How to set not to build libtorch_cpu.so and libmkl_*.so dependencies? module: build triaged module: mkl,module: build triaged module: mkl
Don't ship protoc in wheels module: binaries triaged,module: binaries triaged
CUDA error: initialization error (multiprocessing) with Python 3.7  needs reproduction module: multiprocessing triaged,needs reproduction module: multiprocessing triaged
Adding max_norm constraint to an Embedding layer leads to an error module: nn triaged,module: nn triaged
"FATAL_ERROR ""Failed to determine the source files for the regular expression backend"" module: build triaged module: third_party",module: build triaged module: third_party
ReduceLROnPlateau detects a plateau during a steady decrease after a spike module: optimizer triaged,module: optimizer triaged
save model definition file feature module: serialization triaged,feature module: serialization triaged
Torch getting stuck transfering model to GPU in multiple GPU setting needs reproduction module: multiprocessing module: cuda triaged,needs reproduction module: multiprocessing module: cuda triaged
Sending sparse tensors over RPC not yet supported module: serialization triaged module: rpc,module: serialization triaged module: rpc
Pytorch openmp thread number tuning option for CPU trainning module: performance module: cpu triaged enhancement module: multithreading,module: performance module: cpu triaged enhancement module: multithreading
Master Task: JIT/C++ Parity With Pytorch Python API oncall: jit triaged,oncall: jit triaged
C++ / JIT Parity for ops in `torch/functional.py` and `torch/tensor.py` oncall: jit triaged TSUsability TSRootCause:PyTorchParityGap,oncall: jit triaged TSUsability TSRootCause:PyTorchParityGap
Remove Ops bound in Python Layer for Legacy Reasons module: internals triaged module: pybind,module: internals triaged module: pybind
Natively Declarable Fast-path Functions  high priority module: internals triaged module: dispatch,high priority module: internals triaged module: dispatch
Page for `torch.__config__` 404s module: docs triaged,module: docs triaged
`TestDocCoverage.test_torch` error messages could be clearer module: docs triaged,module: docs triaged
"JIT, nn.utils.weight_norm and {save,load}_state_dict produce wrong results oncall: jit triaged",oncall: jit triaged
Issues in linking libtorch c++  module: build triaged,module: build triaged
Distributed Package asynchronous send/receive not working as expected (Gloo) oncall: distributed triaged,oncall: distributed triaged
Method to broadcast parameters/buffers of DDP model oncall: distributed triaged enhancement,oncall: distributed triaged enhancement
Sorting in embedding_dense_backward_cuda takes very long time module: performance module: cuda triaged module: embedding,module: performance module: cuda triaged module: embedding
[FR] multidim squeeze and flatten triaged function request module: viewing and reshaping,triaged function request module: viewing and reshaping
"Dedicated inverse AdaptiveMaxPool1d operation (e.g., AdaptiveMaxUnpool1d) module: nn triaged module: vision module: pooling function request",module: nn triaged module: vision module: pooling function request
[TensorBoard] The different order of import SummaryWriter may cause Segmentation fault needs reproduction oncall: visualization,needs reproduction oncall: visualization
Cannot use torch.jit.script with nn.DataParallel oncall: jit triaged module: data parallel,oncall: jit triaged module: data parallel
Remove RPC internal helper that overrides the default pickler triaged module: rpc,triaged module: rpc
Refactor/consolidate code for generating test tensors module: tests triaged module: type promotion,module: tests triaged module: type promotion
Overlapping strides not supported by cublas triaged module: cublas,triaged module: cublas
weight norm missing p= module: nn triaged enhancement,module: nn triaged enhancement
"run ./android/run_tests.sh --warning-mode all   ,  show error. triaged module: android oncall: mobile",triaged module: android oncall: mobile
`index_select` with multidimensional `index` feature triaged module: advanced indexing,feature triaged module: advanced indexing
"Assertion `index >= -sizes[i] && index < sizes[i] && ""index out of bounds""` failed. module: cuda module: error checking triaged",module: cuda module: error checking triaged
`F.interpolate` returns unexpected result when dealing with output size `1` module: nn triaged module: vision module: interpolation,module: nn triaged module: vision module: interpolation
[FR] nn.init.* accept None as input tensor module: nn triaged enhancement,module: nn triaged enhancement
Document memory characteristics of in-place ops module: sparse module: docs triaged enhancement module: type promotion,module: sparse module: docs triaged enhancement module: type promotion
Version 1.3 no longer supporting Tesla K40m? module: binaries module: docs module: cuda triaged,module: binaries module: docs module: cuda triaged
Warn about driver CUDA mismatch in torch.cuda.is_available() module: cuda triaged enhancement,module: cuda triaged enhancement
"LibTorch, Error in 'xxx': free(): invalid pointer needs reproduction module: cpp triaged",needs reproduction module: cpp triaged
Adding a function to change the process_group in DistributedDataParallel oncall: distributed triaged,oncall: distributed triaged
Torchscript Precision Issue with PyTorch/vision pretrained model inception v3 oncall: jit triaged,oncall: jit triaged
Add single input/output tensor scatter/gather to ProcessGroup base class oncall: distributed triaged enhancement,oncall: distributed triaged enhancement
cuda support module: build triaged,module: build triaged
`torch.multiprocessing.spawn` fails when `join=False` module: multiprocessing triaged,module: multiprocessing triaged
[TensorBoard] Graph with objects other than torch.nn.Module can not be visualized. triaged module: tensorboard,triaged module: tensorboard
add method to make tensor constant for debug purposes module: autograd triaged,module: autograd triaged
Expose bhistc to python triaged enhancement,triaged enhancement
Redundant counter in batchnorm impl module: nn triaged,module: nn triaged
[jit] Module references aren't preserved oncall: jit triaged,oncall: jit triaged
More detailed information about TensorType in error messages triaged enhancement,triaged enhancement
nn.functional should maintain API parity with nn where possible module: nn triaged enhancement,module: nn triaged enhancement
Codegen refactoring master task triaged,triaged
affine_grid CUDA / cuDNN support for Half removed in 1.3.x module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
There is no support for `weight_decay`/`momentum` in SGD for sparse tensors. module: sparse triaged enhancement,module: sparse triaged enhancement
Unexpected difference torch.multiprocessing.manager.queue and torch.multiprocessing.queue module: multiprocessing module: cuda triaged,module: multiprocessing module: cuda triaged
Conv2d: Inconsistent results on Raspberry Pi 3B  module: convolution triaged,module: convolution triaged
Increasing memory usage on CPU module: performance module: cpu module: memory usage triaged,module: performance module: cpu module: memory usage triaged
Exponentiated gradient descent feature module: optimizer triaged,feature module: optimizer triaged
Refactor (a bit) `torch.hub(.load)` triaged enhancement module: hub,triaged enhancement module: hub
TorchScript Performance: 150x gap between TorchScript and Native Python triage review oncall: jit triaged,triage review oncall: jit triaged
pairwise_dist eps argument is confusing module: numerical-stability module: bc-breaking module: docs triaged,module: numerical-stability module: bc-breaking module: docs triaged
Add scripts for comprehensive benchmark TensorIterator module: performance feature triaged,module: performance feature triaged
CUDA masked_select uses way too much memory module: performance module: cuda triaged,module: performance module: cuda triaged
Wrong substitution of aten::to  oncall: jit triaged oncall: mobile,oncall: jit triaged oncall: mobile
c10:Error: could not unlink the shared memory file module: multiprocessing module: dataloader triaged better-engineering,module: multiprocessing module: dataloader triaged better-engineering
[quantization][graph mode] SubgraphRewriter discards SourceRanges oncall: jit triaged,oncall: jit triaged
Support struct that is initializable / mutable in CPP oncall: jit triaged,oncall: jit triaged
Transform caffe2 to trt failed caffe2 caffe2-op triaged,caffe2 caffe2-op triaged
I have  reconstructed LSTM model and tested by mnist data but the loss is not changed (loss=2.3) module: rnn triaged,module: rnn triaged
Unable to register custom JIT Operator with AliasAnalysisKind::CONSERVATIVE  oncall: jit triaged,oncall: jit triaged
CPU Memory Leak for JIT ScriptModule on DataParallel oncall: jit triaged,oncall: jit triaged
Move the attributes of a module to the given device oncall: jit triaged,oncall: jit triaged
Indexing into tensor order of magnitude slower than numpy high priority module: performance module: autograd triaged module: advanced indexing,high priority module: performance module: autograd triaged module: advanced indexing
Add support for integer matrix multiplication (particularly for dtype = torch.int8 ) feature module: cuda triaged,feature module: cuda triaged
Get wrong precision when multi nodes run in docker needs reproduction oncall: distributed triaged,needs reproduction oncall: distributed triaged
 n-dimensional non-constant padding functional module: nn triaged enhancement module: padding,module: nn triaged enhancement module: padding
torch.save/load shows raw path on the pickle_module arg module: docs module: serialization triaged,module: docs module: serialization triaged
Fold DispatchStub into c10 dispatcher triaged module: dispatch,triaged module: dispatch
torch.distributions.normal.Normal is not JIT supported triage review oncall: jit feature triaged,triage review oncall: jit feature triaged
[jit] Traced `cat` on GPU doesn't support negative indexing oncall: jit triaged,oncall: jit triaged
AdamSparse fails to run module: sparse triaged,module: sparse triaged
SGD fails on sparse matrix module: sparse module: optimizer triaged,module: sparse module: optimizer triaged
Parallel data loader performance degradation for IterableDataset with num_workers > 1 (but not for Dataset). module: dataloader triaged,module: dataloader triaged
android run build_pytorch_android.sh error triaged oncall: mobile,triaged oncall: mobile
[doc] Tensor.mean: dtype kwarg is not documented module: docs triaged module: reductions,module: docs triaged module: reductions
[FR] general nll_loss and cross_entropy along arbitrary dimension module: loss triaged,module: loss triaged
[jit] Printing the graph doesn't include function calls oncall: jit triaged,oncall: jit triaged
"[build] gcc 7.4 needs CMAKE_CXX_FLAGS=""-std=gnu++11"" module: dependency bug module: build triaged",module: dependency bug module: build triaged
Tensor.nbytes() returns itemsize * numel for sparse tensors module: sparse module: autograd triaged actionable fixathon,module: sparse module: autograd triaged actionable fixathon
Slow (20-50x) RNN tutorial/example when torch is installed using pip comp. to conda installation module: binaries module: performance module: rnn triaged module: mkl,module: binaries module: performance module: rnn triaged module: mkl
Provide a mechanism to set global state per test in thread-safe manner module: tests triaged enhancement,module: tests triaged enhancement
CPU and CUDA error messages are divergent in type promotion module: tests triaged module: type promotion,module: tests triaged module: type promotion
[FR] F.pad support syntax sugars for specifying the padding amount module: convolution triaged enhancement,module: convolution triaged enhancement
Reliable way to identify RuntimeErrors (CUDA) module: cuda triaged enhancement,module: cuda triaged enhancement
MultiStepLR does not return good lr after load_state_dict module: optimizer triaged,module: optimizer triaged
[jit] Dict construction fails at runtime oncall: jit triaged,oncall: jit triaged
Improved detection of repeated observers oncall: quantization low priority triaged,oncall: quantization low priority triaged
[ONNX] Exported ONNX module with for loop + scatter operation on tensor seems to be incorrect module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
error: â€˜struct torch::jit::RegisterOperatorsâ€™ has no member named â€˜opâ€™ oncall: jit triaged,oncall: jit triaged
Compiled functions can't take variable number of arguments oncall: jit feature triaged,oncall: jit feature triaged
[jit] Script class attributes aren't automatically added oncall: jit triaged,oncall: jit triaged
[feature request] Print some measure of fragmentation at CUDA out-of-memory module: cuda triaged enhancement,module: cuda triaged enhancement
`print` uses lots of GPU memory module: printing module: cuda triaged enhancement,module: printing module: cuda triaged enhancement
Dispatch key reorganization module: internals triaged,module: internals triaged
Tensorboard GPU Problems module: cuda triaged module: tensorboard,module: cuda triaged module: tensorboard
Half precision cdist module: cuda triaged module: half function request module: distance functions,module: cuda triaged module: half function request module: distance functions
Indexing with torch tensors and NumPy arrays is different triaged module: numpy,triaged module: numpy
RuntimeError: CUDA error: invalid device ordinal module: cuda triaged,module: cuda triaged
torch.stack: bad shape error message module: docs module: error checking triaged,module: docs module: error checking triaged
"""malloc(): memory corruption (fast)"", action=3) at malloc.c needs reproduction module: crash triaged",needs reproduction module: crash triaged
NLLLoss reduce=True returning nan in float16 module: nn triaged,module: nn triaged
"IValue can't be constructed from one of `int`, `long`, or `long long`. oncall: jit triaged",oncall: jit triaged
[RFC] RPC timeout triaged module: rpc,triaged module: rpc
[feature request] torch.kthvalue to support a new argument largest triaged module: sorting and selection,triaged module: sorting and selection
Support out= parameters with autograd feature module: autograd module: nn triaged,feature module: autograd module: nn triaged
[jit] Saving a `ScriptFunction` to a buffer doesn't work oncall: jit triaged,oncall: jit triaged
backward_compatibility_check_test doesn't play well with reverts module: ci triaged,module: ci triaged
torch.std() returns nan for single item tensors. triaged module: numpy small,triaged module: numpy small
nn.Conv(n)d constructor doesn't check for the number of kernel dimensions module: nn module: convolution triaged,module: nn module: convolution triaged
Redo our library structure module: build triaged,module: build triaged
Add instructions for building torch.distributed on macOS oncall: distributed module: docs triaged,oncall: distributed module: docs triaged
RuntimeError: CUDA out of memory with available GPU memory module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
How to run two different jit models in two GPUs respectively in one scrip? oncall: jit triaged,oncall: jit triaged
[FR] trace_module traces both eval and train graph oncall: jit triaged enhancement,oncall: jit triaged enhancement
Modules without copying  in multiprocess module: multiprocessing triaged,module: multiprocessing triaged
"torch.sum(tensor, dim=()) is different from np.sum(arr, axis=()) high priority module: bc-breaking triaged module: numpy module: TensorIterator module: deprecation module: reductions",high priority module: bc-breaking triaged module: numpy module: TensorIterator module: deprecation module: reductions
JIT should respect SKIP_PYTHON_BINDINGS and SKIP_PYTHON_BINDINGS_SIGNATURES triage review oncall: jit triaged,triage review oncall: jit triaged
_compared_saved_loaded doesn't work with torch.tensor constants triage review oncall: jit triaged,triage review oncall: jit triaged
torch.masked_fill missing out argument feature triaged module: ux,feature triaged module: ux
"crash when call dist.new_group(ranks=local_ranks, backend='gloo') oncall: distributed triaged",oncall: distributed triaged
First element in data passed to `torch.*Tensor` constructors cannot be a tensor triaged module: nestedtensor,triaged module: nestedtensor
a retrained and saved  jit module could not be reload. oncall: jit triaged,oncall: jit triaged
"""Unknown type constructor"" error in TorchScript oncall: jit triaged jit-backlog",oncall: jit triaged jit-backlog
error: 'SO_REUSEPORT' was not declared in this scope module: build triaged module: third_party,module: build triaged module: third_party
[FR] script returns subclass of the original module class triage review oncall: jit triaged TSUsability TSRootCause:ModuleInheritance,triage review oncall: jit triaged TSUsability TSRootCause:ModuleInheritance
Trace of torch.tensor is impressively convoluted oncall: jit triaged,oncall: jit triaged
Docker issue for Pytorch 1.3  module: binaries module: build triaged module: docker,module: binaries module: build triaged module: docker
Add Wishart and inverse-Wishart distributions module: distributions feature triaged,module: distributions feature triaged
Support clang+cuda builds module: build triaged enhancement,module: build triaged enhancement
RuntimeError: !t.is_cuda() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:591 module: sparse triaged module: assert failure,module: sparse triaged module: assert failure
Named Tensors: Slicing based on name triaged module: advanced indexing module: named tensor,triaged module: advanced indexing module: named tensor
Named Tensors: size of a named dimension module: docs triaged module: named tensor,module: docs triaged module: named tensor
"torch.mean(x, dims=[]) has incorrect gradient in 1.2 high priority module: autograd triaged quansight-nack module: reductions",high priority module: autograd triaged quansight-nack module: reductions
Unsafe use of `at::parallel_for` in current codebase high priority module: performance module: internals module: autograd triaged module: multithreading,high priority module: performance module: internals module: autograd triaged module: multithreading
[feature request] Some activations modules missing inplace argument module: nn triaged enhancement,module: nn triaged enhancement
RuntimeError: CUDA error: unknown error needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
ConnectionResetError when using dataLoader with pin_memory=True needs reproduction module: dataloader triaged,needs reproduction module: dataloader triaged
The `@` operation uses too much memory on GPU needs reproduction module: memory usage triaged,needs reproduction module: memory usage triaged
Cannot select version in the tutorials page module: docs triaged,module: docs triaged
Add support for multidimensional input to `at::tensor` module: cpp triaged,module: cpp triaged
torch::tensor(scalar) behaves differently from at::tensor(scalar) module: cpp triaged module: pybind,module: cpp triaged module: pybind
"Provide rpc, remote and dist autograd C++ APIs and register them as Prim::ops module: cpp module: autograd triaged enhancement module: rpc",module: cpp module: autograd triaged enhancement module: rpc
Font used in documentation is not always sharp module: docs triaged,module: docs triaged
Why attn_mask is not 3D tensor in nn.MultiheadAttention? module: nn triaged enhancement oncall: transformer/mha,module: nn triaged enhancement oncall: transformer/mha
Support RRef[T].__call__(*args) which invokes T.__call__(*args) on owner triaged module: rpc,triaged module: rpc
Command for downloading torch 1.12.0 CUDA 10 linked to cu92 version module: binaries triaged,module: binaries triaged
Remove bce_with_logits in derivatives.yaml triaged enhancement better-engineering,triaged enhancement better-engineering
Implement operator<< for bfloat16 triaged module: bfloat16,triaged module: bfloat16
torch.cuda.close request to be able to reset communications feature module: cuda triaged,feature module: cuda triaged
torch.from_file not documented module: docs triaged,module: docs triaged
[CUDA-MEMCHECK] TestTorchDeviceTypeCUDA.test_pin_memory_from_constructor_cuda fails module: cuda triaged,module: cuda triaged
RPC couldn't match torch.ones with requires_grad=True high priority triage review oncall: distributed module: internals triaged better-engineering module: rpc,high priority triage review oncall: distributed module: internals triaged better-engineering module: rpc
'import torch` fails with Illegal instruction module: build triaged,module: build triaged
cudnn.determinstic=True causes dilated convolution to be >10x slower module: cudnn triaged,module: cudnn triaged
torch.tensor() is very slow when it is passed an h5py Dataset. module: performance triaged module: tensor creation,module: performance triaged module: tensor creation
cmake allows both MKL and MKLDNN to be OFF;  aten/src/ATen/CMakeLists.txt then ignores c++ sources module: build triaged,module: build triaged
ABI backwards compatibility module: abi triaged,module: abi triaged
super().__init__() not called in torch.nn.Module.__init__ module: nn triaged,module: nn triaged
Problem when installing Pytorch from source on CentOS 7.4 module: build triaged,module: build triaged
Sampler for IterableDataset module: dataloader triaged,module: dataloader triaged
torch.as_tensor fails to create named tensors triaged module: named tensor module: tensor creation,triaged module: named tensor module: tensor creation
CPU MaxPool2d is very slow module: performance module: bootcamp module: cpu triaged module: pooling,module: performance module: bootcamp module: cpu triaged module: pooling
[docs] Unclear input/output format for TransformerEncoderLayer module: nn triaged oncall: transformer/mha,module: nn triaged oncall: transformer/mha
Cmake fails due to bad python call module: build triaged,module: build triaged
Can't successfully install pytorch with python3.6 on my pi 4  module: build triaged has workaround,module: build triaged has workaround
Channels Last (NHWC) support plan. triaged module: memory format,triaged module: memory format
[BUG] Can't Deepcopy module with weightnorm module: nn triaged,module: nn triaged
`Tensor.__reversed__` breaks protocol for reversible objects module: internals triaged module: pybind,module: internals triaged module: pybind
We should suggest using as_strided_ instead of set_ module: docs triaged,module: docs triaged
Doing rendezvous twice can cause hangs oncall: distributed triaged,oncall: distributed triaged
Expose DifferentiableGraphBackward to python oncall: jit feature module: autograd triaged,oncall: jit feature module: autograd triaged
TestTorch.test_c10_layer_norm fails if you run it on a build of PyTorch with BUILD_CAFFE2_OPS=0 triaged module: dispatch,triaged module: dispatch
Unified management of thread local variables high priority module: performance module: internals feature triaged module: multithreading,high priority module: performance module: internals feature triaged module: multithreading
[jit] C++ Documentation oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
[JIT] __repr__ support for ScriptModules oncall: jit low priority triaged jit-backlog,oncall: jit low priority triaged jit-backlog
Get rid of libc10.so module: build triaged,module: build triaged
Problem when installing pytorch 1.4 from source on Centos 6.3 module: build triaged,module: build triaged
Problem installing from source on CentOS 6.5 module: build triaged module: nccl,module: build triaged module: nccl
gradient of Dirichlet.log_prob gives nan module: distributions triaged,module: distributions triaged
Python/C++ API Parity: torch.optim optimizers module: cpp triaged,module: cpp triaged
Patch: Fix for using `clang` to compile CUDA module: build module: cuda triaged,module: build module: cuda triaged
torch.multinominal ignores elements from cumulative distribution module: cpu triaged module: random,module: cpu triaged module: random
LinearOperator Abstraction / Structure-Exploiting LazyTensors for Linear Algebra feature triaged module: linear algebra module: lazy,feature triaged module: linear algebra module: lazy
No in-place version of where() triaged OSS contribution wanted actionable module: sorting and selection function request,triaged OSS contribution wanted actionable module: sorting and selection function request
Unable to install pytorch with cuda 10.0 using conda module: build triaged,module: build triaged
Seg-fault in LayerNormKernelImpl needs reproduction module: crash module: cuda triaged,needs reproduction module: crash module: cuda triaged
`torch.nn.Module._load_state_dict` catch-all error message can be misleading triaged,triaged
jit script fails with `AttributeError: 'str' object has no attribute 'lineno'` oncall: jit triaged,oncall: jit triaged
[jit] scripted module and user defined methods oncall: jit triaged,oncall: jit triaged
nn.parallel.replicate in v1.1+ is much slower than v1.0 module: performance triaged module: data parallel,module: performance triaged module: data parallel
Problem with jit TorchScript while copying data between GRUs triage review oncall: jit triaged,triage review oncall: jit triaged
[jit] TorchScript classes don't work in notebooks oncall: jit triaged,oncall: jit triaged
PyTorch RPC should expose critical metrics to the application. feature triaged module: rpc,feature triaged module: rpc
ProcessGroupMPI reports incorrect world size oncall: distributed triaged,oncall: distributed triaged
support class annotations in __init__ oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
torch native functions cannot be used with inspect.signature triaged module: pybind module: language binding,triaged module: pybind module: language binding
Allow to disable polling for CUDA synchronization high priority module: performance module: cuda triaged enhancement,high priority module: performance module: cuda triaged enhancement
Convert manually bound `cuda` `cpu` `byte` `float` operators to native_functions triaged module: ux,triaged module: ux
TorchScript doesn't support torch.channels_last or any other memory format constants oncall: jit triaged module: memory format,oncall: jit triaged module: memory format
TorchScript custom ops like `cuda` `byte` etc. doesn't support memory_format argument oncall: jit triaged,oncall: jit triaged
"index_sub, index_mul and index_div triaged enhancement OSS contribution wanted",triaged enhancement OSS contribution wanted
reflective padding for 5D tensor triaged function request module: padding,triaged function request module: padding
[JIT] Allow user to provide aliasing information on input tensors and model parameters triage review oncall: jit triaged,triage review oncall: jit triaged
Named tensor: align_to align_as  error messages triaged module: named tensor,triaged module: named tensor
Named Tensor: Support -1 in `unflatten`. triaged module: named tensor,triaged module: named tensor
[discussion] Smarter version of torch.reshape (can avoid realloc in some cases) triaged function request module: viewing and reshaping,triaged function request module: viewing and reshaping
Traced resnet101 leaks memory during `forward` module: memory usage triaged,module: memory usage triaged
TestTorch.test_doc should be in TestDocCoverage module: docs triaged module: doc infra,module: docs triaged module: doc infra
"Sending CUDA tensors via queue between processes, memory of Consumer process grows infinitely  module: multiprocessing module: cuda module: memory usage triaged",module: multiprocessing module: cuda module: memory usage triaged
can't load model on cuda after call cudaDeviceReset functions oncall: jit triaged,oncall: jit triaged
how to use libtorch library in cuda file with nvcc compiler(c++)? module: cpp triaged,module: cpp triaged
Specifying `pos_weight` in F.binary_cross_entropy_with_logits leads to RuntimeError: class size not match module: nn module: error checking triaged,module: nn module: error checking triaged
`num_batches_tracked` update in `_BatchNorm` forward should be a single scalar update on host regardless of the residence of the layer module: performance module: nn module: cuda triaged enhancement,module: performance module: nn module: cuda triaged enhancement
Be able to build torch.distributed documentation easier oncall: distributed triaged module: doc infra,oncall: distributed triaged module: doc infra
caffe2 install VS2019 CUDA 10.1 lib\\torch.lib : fatal error LNK1248: image size (10028FA9F) exceeds maximum allowable size (FFFFFFFF)  caffe2,caffe2
RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 24 and 195 in dimension 0 at /opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/TH/generic/THTensor.cpp:689 oncall: jit module: nn triaged,oncall: jit module: nn triaged
Improve the error message when trying to install in a 32-bit Python environment triaged enhancement,triaged enhancement
docs for torch.cuda.reset_max_memory_reserved don't exist module: docs module: cuda triaged,module: docs module: cuda triaged
torch.cuda.default_generators documentation referenced but don't exist. high priority module: docs module: cuda triaged,high priority module: docs module: cuda triaged
Test `make html-stable` target in CI triaged module: doc infra,triaged module: doc infra
Version number is still duplicated in a bunch of places module: binaries triaged better-engineering,module: binaries triaged better-engineering
Scripting torchvision.models.detection.maskrcnn_resnet50_fpn triaged module: vision,triaged module: vision
Documentation makefile should include torchvision module: docs triaged module: doc infra,module: docs triaged module: doc infra
Deployment training model at C + + end needs reproduction oncall: jit module: docs module: cpp triaged,needs reproduction oncall: jit module: docs module: cpp triaged
`nn.Sequential.__setattr__` appends to the execution list module: docs low priority triaged,module: docs low priority triaged
I can't set gpu is 1 it always use gpu 0 needs reproduction module: cpp module: cuda low priority triaged,needs reproduction module: cpp module: cuda low priority triaged
Avoid RTTI in DistEngine triaged better-engineering module: rpc,triaged better-engineering module: rpc
ðŸš€ Graceful RPCAgent termination in multi-driver scenario high priority triaged better-engineering module: rpc,high priority triaged better-engineering module: rpc
Test re-entrant backward works with torch.distributed.autograd.backward() module: autograd triaged module: rpc,module: autograd triaged module: rpc
torch.distributed.autograd.backward() should populate .grad field on Tensors by default. module: autograd triaged module: rpc,module: autograd triaged module: rpc
[feature request] [dataloader] Pad variable-sized tensors in default_collate module: dataloader triaged enhancement,module: dataloader triaged enhancement
scatter_add allows index tensor that doesn't match input size in forward pass but fails on backward pass high priority module: crash triaged,high priority module: crash triaged
Unify warning logging mechanism module: internals triaged enhancement better-engineering,module: internals triaged enhancement better-engineering
RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
Support Anomaly detection for distributed autograd. module: autograd triaged module: rpc,module: autograd triaged module: rpc
Make topk sort stable triaged enhancement module: determinism OSS contribution wanted module: sorting and selection,triaged enhancement module: determinism OSS contribution wanted module: sorting and selection
CTCLoss cuda backend large batch handling takes up to 1.8x more memory module: loss module: cuda module: memory usage triaged enhancement,module: loss module: cuda module: memory usage triaged enhancement
Allow explicit gradients in torch.distributed.autograd.backward() API triaged module: rpc,triaged module: rpc
[JIT] ndimension not supported oncall: jit triaged,oncall: jit triaged
[JIT] List Comprehensions With Ifs not Supported  oncall: jit triaged,oncall: jit triaged
TSAN failure related to mkldnn oncall: jit triaged module: mkldnn,oncall: jit triaged module: mkldnn
[JIT] Figure out how to easily investigate memory usage issues issues oncall: jit triaged,oncall: jit triaged
[JIT] script::Module API parity with nnmodule oncall: jit triaged,oncall: jit triaged
batch_norm_elemt_cuda_template does not use its argument epsilon module: bc-breaking module: nn module: cuda triaged,module: bc-breaking module: nn module: cuda triaged
[dataloader] Sampler abstract constructor API minor proposal module: dataloader triaged,module: dataloader triaged
Installer not setting rpath for MAGMA (OS X w/ GPU) module: build module: cuda triaged module: macos,module: build module: cuda triaged module: macos
[jit] String frontend doesn't support default arg values oncall: jit triaged,oncall: jit triaged
Re-enable test_EmbeddingBag_per_sample_weights_and_no_offsets_cuda module: nn module: ci triaged module: flaky-tests,module: nn module: ci triaged module: flaky-tests
[ONNX export] UpSample with scale_factor should map to UpSample op with scale  module: onnx triaged enhancement onnx-triaged,module: onnx triaged enhancement onnx-triaged
[jit] `ScriptFunction`s are loaded as `ScriptModule`s oncall: jit triaged,oncall: jit triaged
torch.Tensor.mean erroneously documented as sometimes returning a tuple module: docs triaged module: reductions,module: docs triaged module: reductions
"Tensorboard add image with boxes, labels, and confidence scores. triaged module: tensorboard",triaged module: tensorboard
CUDA error: device-side assert triggered(insert_events at /pytorch/c10/cuda/CUDACachingAllocator.cpp:569) module: cuda triaged,module: cuda triaged
pytorch data_parallel oom on gpu:0 module: memory usage triaged module: data parallel,module: memory usage triaged module: data parallel
Easier way to create tensors with names triaged enhancement module: named tensor,triaged enhancement module: named tensor
Stop binding in-place methods to `torch.*` triaged module: ux,triaged module: ux
torch::jit::script::Module has no zero_grad() oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
CUDAPytorchToCaffe2.MutualResizes is flaky caffe2-op module: cuda module: tests triaged module: flaky-tests,caffe2-op module: cuda module: tests triaged module: flaky-tests
Support implicit RRef type conversion triaged module: rpc,triaged module: rpc
[c10] c10 dispatch doesn't support tracing of scalars module: internals triaged module: dispatch,module: internals triaged module: dispatch
[jit] Document what types can be traced oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
RNG for torch.randn_like triaged enhancement module: random,triaged enhancement module: random
torch::NoGradGuard no_grad get wrong  when I use batchsize!=1 module: cpp triaged,module: cpp triaged
The Gather problem in DataParallel: dimension are not matched. triaged module: data parallel,triaged module: data parallel
RuntimeError:[enforce fail at context.h:48] option.device_type() ==PROTO_CPU. 1vs0 caffe2 triaged,caffe2 triaged
Tensorboard logging image WITH LABEL triaged module: tensorboard,triaged module: tensorboard
Is it an incompleted dst tensor synchronization in CUDA device to device copy ? module: cuda triaged,module: cuda triaged
Cmake warnings during build triaged module: build warnings,triaged module: build warnings
Missing bin and include when building with torchvision on CentOS module: build module: cpp triaged module: vision,module: build module: cpp triaged module: vision
[JIT] Script doesn't preserve builtin torch named tuples upon python return oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
Lint rule to test for creation of tensor in native/ without options() module: build module: lint triaged better-engineering,module: build module: lint triaged better-engineering
AnyValueTest.CorrectlyAccessesIntWhenCorrectType UBSAN failure: owncast of address 0x60300105d750 which does not point to an object of type 'Holder<const int>' Sep 27 00:01:03 0x60300105d750: note: object is of type 'torch::nn::AnyModule::Value::Holder<int>' module: build module: cpp triaged,module: build module: cpp triaged
Batched Dataloader feature module: dataloader triaged,feature module: dataloader triaged
conv2d Memory usage is too largeï¼› pytorch 1.1.0 module: dependency bug module: cudnn module: memory usage module: convolution triaged,module: dependency bug module: cudnn module: memory usage module: convolution triaged
Statically checked tensor shapes module: internals feature triaged,module: internals feature triaged
[BUG Report]Integrate libtorch to ffmpeg but memory leak happened! module: build triaged module: vision,module: build triaged module: vision
Google Summer of Code triaged,triaged
setup.py install error module: build triaged,module: build triaged
Importing tensorboard jams CUDA device selection triaged module: tensorboard,triaged module: tensorboard
Matrix corresponding to convolution by a 2D kernel (convmtx2) feature module: nn triaged,feature module: nn triaged
[RFC] RRef Protocol triaged module: rpc,triaged module: rpc
[jit] Default args don't work with TorchScript classes oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
[jit] Bad error when instantiating TorchScript class with incorrect types oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
Support FPGA Xilinx triaged module: backend,triaged module: backend
torch::nn::Sequential not compatible with torch::nn::RNN module: cpp module: nn triaged,module: cpp module: nn triaged
Make `torch.save` serialize a zip file module: serialization triaged enhancement,module: serialization triaged enhancement
No way to disable mse_loss broadcasting warning module: nn triaged enhancement,module: nn triaged enhancement
MKLDNN+AMD BLIS path for PyTorch  feature module: cpu triaged module: mkldnn,feature module: cpu triaged module: mkldnn
Multilinear map module: nn triaged function request,module: nn triaged function request
[libtorch]Same model in CUDA and CPU got different result? module: cpp module: nn triaged,module: cpp module: nn triaged
parallel_for may hang when called in main process and then on daemon process module: multiprocessing triaged module: deadlock,module: multiprocessing triaged module: deadlock
Avoid sending zero grads over the wire in distributed autograd backward pass triaged module: rpc,triaged module: rpc
Remove TensorOptions logic from generated code module: internals triaged,module: internals triaged
Generated file not getting cleaned up by clean module: build triaged,module: build triaged
RuntimeError: tensor.ndimension() == static_cast<int64_t>(expected_size.size()) INTERNAL ASSERT FAILED needs reproduction module: multi-gpu module: cuda triaged,needs reproduction module: multi-gpu module: cuda triaged
NNPACK condition should be changed (ARM processors) module: convolution triaged enhancement module: nnpack,module: convolution triaged enhancement module: nnpack
DistAutogradContext should be cleaned up in case of node failures. module: autograd module: memory usage triaged enhancement module: rpc,module: autograd module: memory usage triaged enhancement module: rpc
torch.tensor / torch.as_tensor not working with list of tensors module: error checking triaged module: numpy module: tensor creation,module: error checking triaged module: numpy module: tensor creation
The performance of multiplication of two matrices is different between window and linux module: performance module: windows triaged,module: performance module: windows triaged
How to get rid of zombie processes using torch.multiprocessing.Pool? module: dependency bug oncall: distributed module: multiprocessing triaged,module: dependency bug oncall: distributed module: multiprocessing triaged
CPU version of PyTorch on PyPI module: build feature oncall: releng module: cpu triaged,module: build feature oncall: releng module: cpu triaged
Behavior of F.dropout in eval mode module: docs low priority triaged,module: docs low priority triaged
"Supporting ""cdf"" for Student-T distribution module: distributions low priority triaged enhancement",module: distributions low priority triaged enhancement
Inplace and out arguments for BatchNorm (and other norm layers: InstanceNorm / LayerNorm / GroupNorm ...) module: performance triaged function request module: norms and normalization,module: performance triaged function request module: norms and normalization
"""git describe"" shows incorrect version 1.0 instead of 1.2 module: build triaged",module: build triaged
Higher dimension support for `MultiLableSoftMarginLoss` module: nn module: loss triaged enhancement,module: nn module: loss triaged enhancement
Default adam epsilon to 1e-7 when on fp16 module: numerical-stability module: optimizer triaged enhancement module: half,module: numerical-stability module: optimizer triaged enhancement module: half
Dispatch Tracing/Debugging triaged enhancement internals module: dispatch,triaged enhancement internals module: dispatch
Memory leak in multithreading environment when loading checkpoint high priority needs reproduction module: multiprocessing module: memory usage triaged module: multithreading,high priority needs reproduction module: multiprocessing module: memory usage triaged module: multithreading
Tests for pytorch_macos_10_13_cuda9_2_cudnn7_py3_build fail module: ci triaged module: macos,module: ci triaged module: macos
c10 List API hard to use module: internals triaged,module: internals triaged
There should be gating around BFloat16 module: performance triaged enhancement module: bfloat16,module: performance triaged enhancement module: bfloat16
Process fails with assertion error in magma-cuda100 module: dependency bug needs reproduction module: crash triaged module: linear algebra,module: dependency bug needs reproduction module: crash triaged module: linear algebra
Make GloballyUniqueId a common type for both rpc and dist autograd triaged better-engineering module: rpc,triaged better-engineering module: rpc
Tracing non-constant shapes is broken oncall: jit triaged,oncall: jit triaged
Access data_ptr in RNN.cpp module: rnn triaged,module: rnn triaged
Support the AVX512 runtime dispatch feature low priority triaged,feature low priority triaged
[RFC] TensorBoard extensions and improvements for PyTorch triaged module: tensorboard,triaged module: tensorboard
[Feature Request] Trace / Script C++ models oncall: jit module: cpp triaged,oncall: jit module: cpp triaged
DataLoader workers fail to die module: dataloader triaged module: deadlock,module: dataloader triaged module: deadlock
sccache stats can cause whole build to fail module: build triaged,module: build triaged
Why doc building isn't failing us for referring to a non-existent method? module: docs triaged,module: docs triaged
[C++] `Module::pretty_print` is broken module: cpp module: nn triaged,module: cpp module: nn triaged
Provide a way to select SVD algorithm in PyTorch? triaged module: linear algebra function request,triaged module: linear algebra function request
TestAutograd.test_deep_reentrant fails with SIGBUS on macOS module: autograd module: tests triaged module: macos,module: autograd module: tests triaged module: macos
Python hang after using torch.exp() needs reproduction triaged module: deadlock,needs reproduction triaged module: deadlock
load_state_dict on CPU first module: serialization triaged,module: serialization triaged
Forward/backward hooks for C++ torch::nn modules module: cpp module: autograd triaged,module: cpp module: autograd triaged
Python/C++ API Parity: torch.nn modules and functional module: cpp module: nn good first issue triaged,module: cpp module: nn good first issue triaged
Incorrect lable read with ImageInput Op of Caffe2 caffe2 triaged,caffe2 triaged
Conv2D 2x~20x slower than Tensorflow when channel count is small module: performance module: cudnn module: cuda module: convolution triaged,module: performance module: cudnn module: cuda module: convolution triaged
Pytorch master can not build with computer capability 3.0 under Mac OS X 10.13.16 with Nvidia GT 750m module: cuda low priority triaged module: macos,module: cuda low priority triaged module: macos
(PyTorch1.1 and 1.2) RuntimeError: Can't detach views in-place. Use detach() instead module: autograd module: optimizer triaged enhancement,module: autograd module: optimizer triaged enhancement
[jit] `random` module support oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
[Feature request] modified Cholesky decomposition high priority triaged enhancement module: linear algebra,high priority triaged enhancement module: linear algebra
Detaching a distribution's `log_prob` to block gradients only w.r.t its parameters module: distributions triaged,module: distributions triaged
"Model parallel with DDP get `Socket Timeout` error when using NCCL, while GLOO works fine oncall: distributed triaged",oncall: distributed triaged
torch.cuda.empty_cache() write data to gpu0 module: cuda triaged,module: cuda triaged
Int32 overflow in bincount indexing module: cuda triaged module: 64-bit,module: cuda triaged module: 64-bit
Custom sampler for Seq2Seq models to avoid padding module: dataloader triaged,module: dataloader triaged
Backtrace prints many <unknown function> module: build triaged better-engineering,module: build triaged better-engineering
[dataloader] Hang because of too many open files (and probably some process dead) module: dataloader triaged,module: dataloader triaged
finfo operator not bound into JIT oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
"Torch.jit.trace unexpected error with `torch.cat(â€¦, dim=-1)`  oncall: jit triaged",oncall: jit triaged
libtorch forward memory leak module: cpp module: memory usage triaged,module: cpp module: memory usage triaged
[distributed] all_gather on a List of Tensors directly oncall: distributed module: bootcamp triaged enhancement,oncall: distributed module: bootcamp triaged enhancement
[C++] Support negative index in `torch::TensorAccessor::size()` module: cpp triaged,module: cpp triaged
Avoid non-POD data in thread_local module: performance triaged module: multithreading better-engineering,module: performance triaged module: multithreading better-engineering
IValue pickle does not work properly if an empty tensor table is not provided oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
Remote memory access similar to MPI one-sided in pytorch oncall: distributed feature triaged module: mpi,oncall: distributed feature triaged module: mpi
Usage of DDP on a module that doesn't require gradients oncall: distributed triaged enhancement,oncall: distributed triaged enhancement
The inference speed of the torch c++ dynamic library compiled manually is slower than the torch library officially provided module: binaries module: performance module: cuda triaged,module: binaries module: performance module: cuda triaged
[Proposal] Pin Windows SDK and MSVC compiler versions in LibTorch module: build module: windows triaged enhancement,module: build module: windows triaged enhancement
[dataloader] Problem in exception reraise mechanism module: dataloader triaged,module: dataloader triaged
pytorch c++ api cannot call operator() on torch::nn::Sequential module: cpp module: nn triaged,module: cpp module: nn triaged
Assign torch.cuda.FloatTensor to List tensor module: crash module: cuda triaged has workaround,module: crash module: cuda triaged has workaround
Multithreaded backpropagation with custom autograd.Functions feature triaged module: data parallel,feature triaged module: data parallel
clang-tidy job merges with master which can lead to hard to understand errors module: ci triaged,module: ci triaged
[feature request] symmetric matrix square root triaged module: linear algebra function request,triaged module: linear algebra function request
Vectorize bool operations triaged enhancement module: boolean tensor,triaged enhancement module: boolean tensor
Delete TensorOptions::operator== module: internals triaged enhancement small,module: internals triaged enhancement small
Build link not right module: build module: cpp triaged,module: build module: cpp triaged
[jit] NamedTuples don't respect `__new__` oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
[DRAFT] Auto-casting in JIT - Automatic mixed precision oncall: jit triaged,oncall: jit triaged
regarding builtin_function_or_method feature low priority triaged,feature low priority triaged
[jit] Bad error for incorrect container type oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
ConstQuantizerPtr is misnamed oncall: quantization triaged,oncall: quantization triaged
Handling of packed_sequence by activation functions and linear layers module: nn module: rnn triaged,module: nn module: rnn triaged
torch.as_tensor(bytearray(...)) seems to leak memory module: dataloader module: cuda module: memory usage triaged,module: dataloader module: cuda module: memory usage triaged
JIT leaks memory when I change the max sequence length oncall: jit triaged has workaround,oncall: jit triaged has workaround
Ability to tell whether a tensor might be changed in TH/Aten impl high priority module: internals triaged,high priority module: internals triaged
Error in python3: double free or corruption (fasttop) needs reproduction module: cudnn module: multiprocessing module: dataloader module: cuda triaged quansight-nack,needs reproduction module: cudnn module: multiprocessing module: dataloader module: cuda triaged quansight-nack
BCEWithLogitsLoss expects wrong shape of weight (#classes instead of batch size) module: nn module: loss triaged,module: nn module: loss triaged
Incorrect Validation Accuracy Due to Distributed Sampler oncall: distributed module: dataloader triaged,oncall: distributed module: dataloader triaged
euclidean distance between two tensors triaged function request module: distance functions,triaged function request module: distance functions
Problems with install python from source module: build triaged,module: build triaged
Serialization does not work for quantized modules module: serialization triaged quantization_release_1.3,module: serialization triaged quantization_release_1.3
[RFC] NestedTensor - 0.0.2 triaged,triaged
[FR] Dropout modules/functions should take in generator= module: nn triaged enhancement module: random,module: nn triaged enhancement module: random
No way to correctly reset weights of a model with spectral norm module: nn triaged has workaround,module: nn triaged has workaround
JavaScript (Web Assembly) target for trained models triaged enhancement,triaged enhancement
Generator objects should not always use the same seed triaged module: random,triaged module: random
[FR] torch.(Generator|random).seed allows specifying the seed value triaged enhancement module: random,triaged enhancement module: random
[FR][jit] torch.jit.script as a class decorator oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
ctc_loss computes different losses and gradients on batched utterances vs. individual utterances module: loss triaged,module: loss triaged
ScriptModule and nn.Module parameter ordering difference oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
[Distance functions] F.pdist backward CUDA invalid configuration module: cuda triaged module: distance functions,module: cuda triaged module: distance functions
Add new interpolation modes to `grid_sample` module: nn triaged function request module: interpolation,module: nn triaged function request module: interpolation
"make add_module accept tuples as well or change containers(ModuleList, Sequential, etc) to allow this module: nn triaged enhancement",module: nn triaged enhancement
Benchmark cuDNN affine_grid_generator vs native module: performance module: cudnn module: nn module: cuda triaged,module: performance module: cudnn module: nn module: cuda triaged
[C++] Call find_package(Torch REQUIRED) more than one time in downstream project causes CMake configuration error module: build triaged,module: build triaged
Data worker should fetch a sample instead of a batch. module: dataloader triaged enhancement,module: dataloader triaged enhancement
DataLoader slow down when `pin_memory=False` module: performance module: multiprocessing module: dataloader triaged,module: performance module: multiprocessing module: dataloader triaged
torch.cuda.synchronize blocks CUDA execution on other threads using other devices. module: cuda triaged,module: cuda triaged
[RPC] Fix logging initialization warning in ProcessGroupAgent module: internals triaged,module: internals triaged
[RPC] Make ProcessGroupAgent send task non-blocking todo triaged module: rpc,todo triaged module: rpc
"Consider not checking in autogenerated core/{Tensor.h,TensorMethods.h} module: build module: cpp triaged",module: build module: cpp triaged
Successive Layer Normalization in nn.Transformer module: nn triaged oncall: transformer/mha,module: nn triaged oncall: transformer/mha
Shared Dataset Functionality module: dataloader triaged better-engineering,module: dataloader triaged better-engineering
tensorboard add_graph error triaged module: tensorboard,triaged module: tensorboard
Export torch.cat to ONNX with Dynamic shape does not work on GPU caffe2 triaged,caffe2 triaged
RuntimeError on PyTorch 1.2 under NVIDIA Nsight Systems module: cuda triaged module: third_party,module: cuda triaged module: third_party
Recommendations for Grid Sample/Affine Grid/Displacement Fields/Optical Flow proposal accepted triaged module: interpolation,proposal accepted triaged module: interpolation
Improve binary release for PyTorch domain library module: binaries triaged better-engineering,module: binaries triaged better-engineering
Gloo scatter gives wrong result for stride != 1 oncall: distributed module: bootcamp triaged,oncall: distributed module: bootcamp triaged
PyTorch 1.2 'module' object has no attribute 'BFloat16StorageBase' triaged module: undefined reference module: vision,triaged module: undefined reference module: vision
subprocess.CalledProcessError: Compile source in NVIDIA TX2 module: build module: cuda triaged,module: build module: cuda triaged
Transformer Lack of Embedding Layer and Positional Encodings high priority module: docs feature module: nn triaged needs design oncall: transformer/mha,high priority module: docs feature module: nn triaged needs design oncall: transformer/mha
"Problematic handling of NaN and inf in grid_sample, causing segfaults, corrupted CUDA memory, and incorrect results high priority module: crash module: cuda triaged module: interpolation",high priority module: crash module: cuda triaged module: interpolation
"Doesn't install the python module ""torch"" module: build triaged",module: build triaged
Installs empty directories under Python's sitelibdir module: build triaged,module: build triaged
[jit] Dict iterator invalidation doesn't match Python oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
Failed to compile PyTorch on IBM Power 9 architecture with CUDA 10 module: build triaged,module: build triaged
Migrate CPU_tensor_apply to TensorIterator in aten/src/ATen/native/TensorCompare.cpp:30 triaged better-engineering module: CPU_tensor_apply,triaged better-engineering module: CPU_tensor_apply
Port CPU_tensor_apply functions to TensorIterator (umbrella issue) triaged better-engineering module: CPU_tensor_apply,triaged better-engineering module: CPU_tensor_apply
Benchmark cudnn version of grid sampler module: performance module: cudnn triaged,module: performance module: cudnn triaged
[Tensorboard] Write summaries to S3 or GCS bucket feature triaged module: tensorboard,feature triaged module: tensorboard
Crashes on torch.cuda.memory_allocated(device) module: error checking triaged,module: error checking triaged
Build PyTorch 1.2.0 occur `recipe for target bin/test_parallel' failed module: build triaged,module: build triaged
Use a ScriptModule on GPU that was saved from CPU oncall: jit module: serialization triaged,oncall: jit module: serialization triaged
Label tracking meta-issue (edit me to get automatically CC'ed on issues! cc bot) triaged,triaged
Building Python bits separate from C++ bits and making one play well with the other module: build triaged enhancement,module: build triaged enhancement
1.0rc0-6216 installs empty directories under include and duplicates under / module: build triaged,module: build triaged
"When running model forward with large batch size, it reports the error: THCudaTensor sizes too large for THCDeviceTensor conversion module: cuda triaged",module: cuda triaged
Significantly slower in latest version than in 0.4.0 needs reproduction module: performance module: cuda triaged,needs reproduction module: performance module: cuda triaged
SyncBatchNorm error when using model.eval() with DistributedDataParallel needs reproduction oncall: distributed module: autograd triaged,needs reproduction oncall: distributed module: autograd triaged
Default warning handler in C++ doesn't seem to unique warnings module: error checking triaged,module: error checking triaged
Visual Studio Code not providing autosuggestions for submodules triaged,triaged
CI Standardization for Domain APIs module: binaries module: build module: ci triaged better-engineering,module: binaries module: build module: ci triaged better-engineering
Unified representation for enum types oncall: jit triaged,oncall: jit triaged
Consider changing the behavior of Tensor.__contains__(Tensor) to make more sense triaged module: numpy module: ux,triaged module: numpy module: ux
Auto tuner takes too much time in serialized model oncall: jit triaged,oncall: jit triaged
Port `masked_fill` operator from the TH code to Aten module: cuda module: cpu triaged module: porting better-engineering,module: cuda module: cpu triaged module: porting better-engineering
Check PyTorch version when initializing process groups oncall: distributed module: bootcamp triaged enhancement,oncall: distributed module: bootcamp triaged enhancement
"TensorIterator ""builder"" options should be documented. module: internals triaged",module: internals triaged
"""PyTorch core"" thread local flag module: internals triaged enhancement",module: internals triaged enhancement
Loading custom Torchscript C++ operators in python segfaults due to ABI compatibility issue between pytorch and libtorch oncall: jit triaged,oncall: jit triaged
hasSideEffects INTERNAL ASSERT FAILED when using .split method with JIT high priority oncall: jit triaged,high priority oncall: jit triaged
torch.utils.tensorboard.SummaryWriter fails to flush at program exit triaged module: tensorboard,triaged module: tensorboard
Confusing error message for Custom Class type mismatch oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
Allow forward method to be defined with .define() in new TorchScript API oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
deprecate cuda arch 3.5/3.7 in nightlies module: binaries triaged,module: binaries triaged
TensorIterator stubs are designed for merge conflicts. module: internals triaged,module: internals triaged
Pin flake8 version in CI module: ci triaged,module: ci triaged
[feature request] Subset of eigenvalues/eigenvectors  module: performance feature module: cpu triaged,module: performance feature module: cpu triaged
torch.fft crash when used with nn.DataParallel module: cuda triaged module: data parallel,module: cuda triaged module: data parallel
torch.unique is inconsistent with NumPy's unique triaged module: numpy module: correctness (silent),triaged module: numpy module: correctness (silent)
Failed to build pytorch with NanoPi M4 module: build low priority triaged,module: build low priority triaged
"""To compact weights again call flatten_parameters()"" is printed every step for every GPU module: nn module: rnn triaged module: data parallel",module: nn module: rnn triaged module: data parallel
sccache crashes when building `Distribution.cu` on Windows module: build triaged module: build warnings,module: build triaged module: build warnings
Allow incompatible shapes in load_state_dict(strict=False) module: serialization triaged enhancement,module: serialization triaged enhancement
[RPC] Add type annotations for RPC-related Python files triaged better-engineering module: rpc,triaged better-engineering module: rpc
[JIT] script doesn't convert dtypes back to torch.dtype from long oncall: jit triaged,oncall: jit triaged
`suggest_memory_format` has ambiguity & cannot represent intended layout format for corner cases module: internals triaged,module: internals triaged
torch.nn.functional.grid_sample with 'circular' border conditions module: nn triaged,module: nn triaged
Multi-gpu example freeze and is not killable module: dependency bug module: multi-gpu module: multiprocessing module: cuda triaged module: deadlock has workaround module: data parallel quansight-nack,module: dependency bug module: multi-gpu module: multiprocessing module: cuda triaged module: deadlock has workaround module: data parallel quansight-nack
[Caffe2] build android in v1.1.0 with headfile error caffe2 triaged,caffe2 triaged
"torch.{save,load} data corruption when serializing a Module with __{get,set}state__ high priority module: serialization triaged quansight-nack",high priority module: serialization triaged quansight-nack
tensor.var_mean variant for existing torch.var_mean (and same for std_mean) triaged function request module: reductions,triaged function request module: reductions
Tests do not pass with the latest protobuf module: protobuf caffe2 triaged,module: protobuf caffe2 triaged
fractional_max_pool2d_with_indices silently ignores output_ratio if output_size is provided module: error checking triaged module: pooling,module: error checking triaged module: pooling
Refactor CircleCI config for version 2.1 module: ci triaged better-engineering,module: ci triaged better-engineering
Multiplying a very large CUDA tensor with another tensor yields unexpected result module: dependency bug module: cuda triaged,module: dependency bug module: cuda triaged
Using  `torch.utils.checkpoint.checkpoint_sequential` and `torch.autograd.grad` breaks when used in combination with `DistributedDataParallel` oncall: distributed module: checkpoint feature triaged,oncall: distributed module: checkpoint feature triaged
Error out during compilation if USE_FBGEMM=1 is ignored module: build module: cpu module: ci triaged better-engineering,module: build module: cpu module: ci triaged better-engineering
[jit] Python @property's not supported in TorchScript oncall: jit triaged quantization_release_1.3 jit-backlog,oncall: jit triaged quantization_release_1.3 jit-backlog
We should run clang-tidy on all of master module: ci triaged better-engineering,module: ci triaged better-engineering
Improve the performance of linear algebra operations in CUDA for small problem sizes module: performance module: cuda triaged module: linear algebra,module: performance module: cuda triaged module: linear algebra
torch.nn.DataParallel causes incorrect gradients oncall: distributed module: autograd triaged,oncall: distributed module: autograd triaged
Remove USE_C10D flag oncall: distributed module: build triaged,oncall: distributed module: build triaged
[JIT] Can't use ndim in script oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
model->to(device) costs over a millisecond when doing nothing module: performance triaged,module: performance triaged
CUDA: THTensor code complains about devices not matching when creating tensor from blob module: cpp triaged,module: cpp triaged
Fan out calculation broken for group (depthwise) convolution module: convolution triaged module: initialization,module: convolution triaged module: initialization
[quantization] jit::class_ for packed weights oncall: jit triaged quantization_release_1.3 jit-backlog,oncall: jit triaged quantization_release_1.3 jit-backlog
Make MultiProcessTestCase pickable oncall: distributed module: tests triaged better-engineering,oncall: distributed module: tests triaged better-engineering
Better version of chrome://tracing module: docs triaged small,module: docs triaged small
Construction of MultivariateNormal much slower on GPU than CPU module: performance module: distributions module: cuda triaged,module: performance module: distributions module: cuda triaged
SummaryWriter doesn't read comment if log_dir precised triaged module: tensorboard,triaged module: tensorboard
Better documentation about PyTorch's dependencies module: build module: docs triaged module: third_party,module: build module: docs triaged module: third_party
[feature request] Core API for invertible/inplace and flow-like ops + memory-saving (hookless?) reversible sequential container for RevNets to allow for much larger batch-sizes in academic setting high priority module: distributions feature module: nn triaged needs design,high priority module: distributions feature module: nn triaged needs design
Wrong device in graph - Tensorboard SummaryWriter  triaged module: tensorboard,triaged module: tensorboard
nn.Module.forward signature with **kwargs module: checkpoint module: nn triaged enhancement,module: checkpoint module: nn triaged enhancement
Failed to build pytorch ... module: build triaged,module: build triaged
Using PyTorch on AWS EFA network module: dependency bug oncall: distributed triaged,module: dependency bug oncall: distributed triaged
segmentation faults when using multiprocessing_context='spawn' with large number of processes oncall: distributed module: multiprocessing module: dataloader triaged,oncall: distributed module: multiprocessing module: dataloader triaged
"Bogus ""Your compiler (clang++) is not compatible"" message module: build triaged module: macos",module: build triaged module: macos
Accelerate PyTorch just-in-time compilation using MKL-DNN oncall: jit feature triaged module: mkldnn,oncall: jit feature triaged module: mkldnn
TorchScript GPU Fuser Doesn't Handle In-Place Operations module: performance oncall: jit triaged,module: performance oncall: jit triaged
model use dilated conv backward in v1.1.0 is ~3x slower than in v0.4.1 on 1080Ti  module: performance module: cudnn module: cuda module: convolution triaged,module: performance module: cudnn module: cuda module: convolution triaged
Error from PyTorch when finalizing Python embedded in C++ triaged module: pybind,triaged module: pybind
MultiheadAttention output changes if input order is not exactly same needs reproduction module: nn triaged,needs reproduction module: nn triaged
Build reconfiguration should consistently honor env variables module: build triaged,module: build triaged
Enable PyTorch Bfloat16 for CPU and add MKL-DNN bfloat16 optimization for Cooper Lake module: performance module: cpu triaged,module: performance module: cpu triaged
Build error due to unintended include path /usr/include module: build triaged,module: build triaged
einsum equation with conditional mask works in numpy but not in PyTorch triaged module: type promotion module: linear algebra function request,triaged module: type promotion module: linear algebra function request
[Feature request] Let DistributedSampler take a Sampler as input oncall: distributed feature module: dataloader triaged has workaround,oncall: distributed feature module: dataloader triaged has workaround
Unreachable code in tanh caffe2 triaged,caffe2 triaged
Hanging on when one gpu node return zero as loss in the context of distributed data parallel training oncall: distributed triaged,oncall: distributed triaged
Tensor from mmaped storage loads the entire file into memory module: memory usage triaged,module: memory usage triaged
[jit] add named tuple as output type to the tracer oncall: jit triaged,oncall: jit triaged
QNNpack tests should be skipped on ppc64le (not enabled there) module: tests triaged better-engineering,module: tests triaged better-engineering
make torch.utils._download_url_from_file public and add a docstring module: docs triaged module: hub module: ux,module: docs triaged module: hub module: ux
Error while using Libtorch + OpenCV + Qt Creator module: cpp triaged,module: cpp triaged
Add gatherv/allgatherv primitives to support non-equal contribution oncall: distributed module: bootcamp feature triaged,oncall: distributed module: bootcamp feature triaged
UBSAN failure in test_simple_model (__main__.TestTensorBoardNumpy): runtime error: call to function pybind11::class_<caffe2::GradientWrapper>::dealloc(pybind11::detail::value_and_holder&) through pointer to incorrect function type 'void (*)(pybind11::detail::value_and_holder &)' module: tests triaged,module: tests triaged
"Unofficial ARMv6, ARMv7, ARMv8 builds module: build module: docs triaged",module: build module: docs triaged
Consolidate multiprocessing helpers in distributed tests oncall: distributed triaged,oncall: distributed triaged
Libtorch with deeplabv3_resnet101 will not forward. module: crash module: cpp triaged,module: crash module: cpp triaged
"Can't `torch.sum(tensor, dim)` where `dim >= 64` module: error checking triaged module: TensorIterator module: reductions",module: error checking triaged module: TensorIterator module: reductions
Deterministic mode for scatter_add operation triaged module: determinism function request module: scatter & gather ops,triaged module: determinism function request module: scatter & gather ops
[RFC] RPC Based Distributed Model Parallel feature triaged module: rpc,feature triaged module: rpc
Test utility for non-contiguous tensors module: tests triaged enhancement,module: tests triaged enhancement
Versioning for libtorch nightlies module: binaries module: build triaged better-engineering,module: binaries module: build triaged better-engineering
Inconsistent axis argument names in torch.diagonal and torch.transpose module: docs low priority triaged,module: docs low priority triaged
[c++] torch::conv2d() expected output_padding to be a single integer value or a list of 3 values  module: docs module: cpp module: nn low priority module: convolution triaged,module: docs module: cpp module: nn low priority module: convolution triaged
creation of a tensor from a numba.cuda array feature low priority triaged module: numba,feature low priority triaged module: numba
The speed of `torch.einsum` and `torch.matmul` when using `fp16` is slow module: performance module: cuda triaged module: linear algebra,module: performance module: cuda triaged module: linear algebra
ConcatDataset returns different error messages setting out of range plus index and minus index. module: docs low priority triaged,module: docs low priority triaged
Add automatic tuning flags to utils.data.dataloader feature module: dataloader low priority triaged,feature module: dataloader low priority triaged
Proposal: Optional AutogradMeta for Variable module: autograd triaged,module: autograd triaged
BatchNorm1d fails on first run through GPU module: nn module: cuda triaged,module: nn module: cuda triaged
performance much worse on 2080ti than 1080ti module: performance module: cuda triaged,module: performance module: cuda triaged
Support serializing IValue to bytes (and deserialize from bytes) oncall: jit triaged,oncall: jit triaged
[data loader] Graceful data loader threads exit on KeyboardInterrupt needs reproduction module: dataloader triaged,needs reproduction module: dataloader triaged
Unify tensor shape formatting in shape checks module: error checking module: convolution triaged enhancement,module: error checking module: convolution triaged enhancement
JIT trace parameter sharing error if Module attributes happen to be the same oncall: jit triaged,oncall: jit triaged
Difference between dropout2d and dropout3d module: nn triaged,module: nn triaged
Make it easier to bisect on PyTorch module: build module: ci triaged better-engineering,module: build module: ci triaged better-engineering
CRITICAL:root:Cannot load caffe2.python. Error: DLL load failed: The specified module could not be found. caffe2,caffe2
TensorImpl de-virtualization high priority module: dependency bug module: internals triaged quansight-nack,high priority module: dependency bug module: internals triaged quansight-nack
Port `fmod` operator from the TH code to Aten module: cuda module: cpu triaged module: porting better-engineering,module: cuda module: cpu triaged module: porting better-engineering
How to use mpi backend without CUDA_aware triaged module: mpi,triaged module: mpi
Pytorch deadlock from distributed multiprocessing oncall: distributed triaged,oncall: distributed triaged
Getting cuda runtime error (48) with Jetson TX2 when running simple program module: cuda triaged,module: cuda triaged
In-place updating the original value tensor should also update version counter of sparse tensor's values_ tensor module: sparse module: autograd triaged,module: sparse module: autograd triaged
assert_no_internal_overlap should pass const char* module: internals triaged,module: internals triaged
[RFC] InstanceNorm default affine value module: docs triaged,module: docs triaged
torch.fill_() exists and modifies the input tensor: Expected or bug? module: bc-breaking triaged module: deprecation module: ux,module: bc-breaking triaged module: deprecation module: ux
DispatchStub should report what operator it failed to find kernel for module: internals module: cpu triaged,module: internals module: cpu triaged
CMAKE_PARSE_IMPLICIT_LINK_INFO Function invoked with incorrect arguments module: build triaged module: flaky-tests has workaround,module: build triaged module: flaky-tests has workaround
The training always freezes after some epochs. needs reproduction module: cuda triaged module: deadlock,needs reproduction module: cuda triaged module: deadlock
BFloat16 numeric limits should contain more info triaged enhancement module: bfloat16,triaged enhancement module: bfloat16
"""CrossEntropyLoss"" should mention in its name that it takes softmax for target module: nn module: loss triaged",module: nn module: loss triaged
Autograd profiler memory leak when use_cuda=True needs reproduction module: autograd triaged,needs reproduction module: autograd triaged
[dataloader] Mysterious error when using spawn start_method  module: dataloader triaged,module: dataloader triaged
Pytorch compilation error on Mac OS module: build triaged,module: build triaged
torch.gels runs 100 time slower on gpu than on cpu module: performance triaged,module: performance triaged
Double backward 3 times slower for conv2d with padding = 1 module: dependency bug module: performance module: double backwards module: nn module: convolution triaged has workaround quansight-nack,module: dependency bug module: performance module: double backwards module: nn module: convolution triaged has workaround quansight-nack
Batched symeig and qr are very slow on GPU module: performance module: cuda triaged module: linear algebra,module: performance module: cuda triaged module: linear algebra
CPU torch.exponential_ function may generate 0 which can cause downstream NaN triaged module: random,triaged module: random
Eigen Tensor library for convolutions on CPU module: performance module: cpu module: convolution triaged module: arm function request,module: performance module: cpu module: convolution triaged module: arm function request
make[2]: *** No rule to make target 'libtorch/lib/libc10.so' module: build module: cpp triaged,module: build module: cpp triaged
libtorch new op module: docs module: cpp triaged module: custom-operators,module: docs module: cpp triaged module: custom-operators
Know which function is used by conv and force to use a function module: nn module: convolution triaged,module: nn module: convolution triaged
No assertion when using scatter_ on a non-contiguous tensor module: error checking triaged module: partial aliasing module: scatter & gather ops,module: error checking triaged module: partial aliasing module: scatter & gather ops
`binary_linux_libtorch_2.7m_cu100_devtoolset3_build` times out after running for 5 hours module: build triaged better-engineering,module: build triaged better-engineering
Error in equation module: docs triaged,module: docs triaged
"""Floating point exception"" after trying the method from the issue #22382 oncall: jit triaged",oncall: jit triaged
"Move csrc/distributed/c10d/{comm,reducer} to libtorch.so oncall: distributed triaged",oncall: distributed triaged
Tracing an RNN does not support torch.nn.utils.rnn.PackedSequence as input oncall: jit triaged,oncall: jit triaged
Weak Symbols Resolution Causes Segmentation Fault in External Libraries triaged,triaged
Add support for serializing Mkldnn Tensor module: serialization triaged module: mkldnn,module: serialization triaged module: mkldnn
CPU random number generator is slow module: performance triaged module: random,module: performance triaged module: random
Build failure with setup.py module: build triaged,module: build triaged
PyTorch Tensor subclasses and protocols for NumPy interoperability high priority feature triaged module: numpy,high priority feature triaged module: numpy
Storage operation failing on second GPU module: cuda triaged,module: cuda triaged
scatter_ supporting different reduction modes high priority module: sparse module: internals triaged enhancement module: scatter & gather ops,high priority module: sparse module: internals triaged enhancement module: scatter & gather ops
Label.dim Enforcement Check in AccuracyOp caffe2 triaged,caffe2 triaged
returned non-zero exit status 2. module: build triaged,module: build triaged
"When I run python setup.py install to install Caffe2, I have an error: ""No such file or directory: 'nvcc': 'nvcc'"" caffe2 triaged",caffe2 triaged
Illegal instruction (core dumped) when running in qemu high priority module: crash module: cpu triaged module: vectorization,high priority module: crash module: cpu triaged module: vectorization
Integer division by Zero giving large number results instead of NaN/inf on Windows module: cpu triaged,module: cpu triaged
Training CNNs with deconvolution module: convolution triaged module: vision function request,module: convolution triaged module: vision function request
Handle all IntArrayRef expansions in ATen module: nn triaged enhancement,module: nn triaged enhancement
second derivatives of unfold triaged enhancement module: derivatives,triaged enhancement module: derivatives
using multi thread lead to gpu stuck with GPU-util 100% high priority needs reproduction module: cuda triaged quansight-nack,high priority needs reproduction module: cuda triaged quansight-nack
[libtorch] header warning suppression triaged module: build warnings,triaged module: build warnings
[FYI] Introducing Quantized Tensor module: docs triaged,module: docs triaged
[RFC] NestedTensor - 0.0.1 triaged module: batching,triaged module: batching
[jit] Optional type refinement on non-named expressions oncall: jit triaged TSRootCause:TypeRefinement TSUsability,oncall: jit triaged TSRootCause:TypeRefinement TSUsability
[dataloader] SIGCHLD handler should poll the queue for exception first module: dataloader triaged,module: dataloader triaged
Automatic rank selection when using file:// initialization method oncall: distributed triaged enhancement,oncall: distributed triaged enhancement
ASSERT FAILED at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:12721 triaged module: assert failure,triaged module: assert failure
[ONNX] BUG for Upsample operator export re-used by caffe2 caffe2,caffe2
Cannot update part of the parameters in DistributedDataParallel. oncall: distributed triaged,oncall: distributed triaged
Have a different way to check if gradient was computed in the optimizer (not checking for None) feature module: autograd module: optimizer triaged,feature module: autograd module: optimizer triaged
Consolidate definition of operators/gradients where possible module: internals triaged better-engineering,module: internals triaged better-engineering
nn.modules.functional.h does not support optional arguments module: cpp module: nn triaged,module: cpp module: nn triaged
Mysterious Tensor Indexing Problem high priority module: error checking triaged module: numpy module: advanced indexing module: ux,high priority module: error checking triaged module: numpy module: advanced indexing module: ux
nn.init.orthogonal_ doesn't work with multiprocessing module: dependency bug module: multiprocessing module: nn triaged module: initialization,module: dependency bug module: multiprocessing module: nn triaged module: initialization
contradictory output values module: cpp triaged,module: cpp triaged
Crash when using tensor.set_data() function in libtorch on windows module: cpp triaged,module: cpp triaged
nn.TransformerLayer feature module: nn triaged oncall: transformer/mha,feature module: nn triaged oncall: transformer/mha
Batch Normalization axis triaged module: batching function request module: norms and normalization,triaged module: batching function request module: norms and normalization
Logging mode for saying when tensor broadcast occurs module: internals feature module: molly-guard triaged,module: internals feature module: molly-guard triaged
torch::zeros is slow for small tensors (C++) module: performance module: cpp triaged,module: performance module: cpp triaged
How about add torch::end for slicing in c++ frontend module: internals feature triaged,module: internals feature triaged
C++ ABI - Coupling different libraries issue module: build triaged,module: build triaged
Linker errors when building project with OpenCV module: build triaged,module: build triaged
cumsum cuda numerical instability module: numerical-stability module: cuda triaged,module: numerical-stability module: cuda triaged
"Use of word ""elements"" in `torch.utils.data` samplers module: docs triaged",module: docs triaged
typo and missing return statements module: build triaged module: third_party,module: build triaged module: third_party
[FR] faster reduce sum on expanded/unfolded tensors module: performance triaged module: linear algebra function request,module: performance triaged module: linear algebra function request
torch.save also saves docstrings into pickle for some reason module: serialization triaged,module: serialization triaged
symbol lookup error: libmkl_intel_lp64.so: undefined  symbol: mkl_blas_dsyrk (binaries built with static linking -DBUILD_SHARED_LIBS=OFF fail due to dynamic linker problem) module: build triaged module: static linking module: third_party has workaround,module: build triaged module: static linking module: third_party has workaround
Improve multithreaded random number generation (RNG) module: cpu triaged module: random module: multithreading,module: cpu triaged module: random module: multithreading
(LLD 8.0.0) ld: error: can't create dynamic relocation R_X86_64_DTPOFF32 against symbol: ideep::utils::computation_cache module: build triaged module: static linking module: mkldnn,module: build triaged module: static linking module: mkldnn
Batched Conv2d for sequence data module: convolution triaged module: batching function request,module: convolution triaged module: batching function request
The cuda problem in caffe2 caffe2,caffe2
C++ module API footgun: assigning to parameter doesn't update `parameters()` list module: docs triaged,module: docs triaged
[FR] Diagonal Transform for Distributions module: distributions feature triaged,module: distributions feature triaged
Batch Dataloader and Dataset feature module: dataloader triaged,feature module: dataloader triaged
Strange latency overhead of F.conv2d module: performance module: cpu triaged,module: performance module: cpu triaged
RuntimeError: cublas runtime error  triaged module: cublas,triaged module: cublas
torch.bernoulli's parameter generator not documented module: docs triaged module: random,module: docs triaged module: random
Pytorch hangs when dataloader multiprocessing workers are killed module: dataloader triaged,module: dataloader triaged
Feature Request: beta cdf module: distributions feature triaged,module: distributions feature triaged
"[jit] In pickler, don't memoize if not necessary oncall: jit triaged jit-backlog",oncall: jit triaged jit-backlog
`attn_mask` in nn.MultiheadAttention is additive module: docs module: nn triaged oncall: transformer/mha,module: docs module: nn triaged oncall: transformer/mha
Not obvious how to install torchvision with PyTorch source build triaged module: vision,triaged module: vision
No continuous integration coverage for Python 2 CUDA module: ci triaged,module: ci triaged
"Slow convolution with large kernels, should be using FFT module: performance module: cudnn module: convolution triaged module: fft",module: performance module: cudnn module: convolution triaged module: fft
RuntimeError: cublas runtime error  triaged module: cublas,triaged module: cublas
downsampling with grid_sample doesn't match interpolate triaged module: vision module: interpolation,triaged module: vision module: interpolation
[JIT] Memory Leak during tracing? oncall: jit triaged,oncall: jit triaged
[JIT] kwarg with default doesn't work for class instantiation oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
Implementation of Group equivariant convolutions feature module: nn low priority triaged,feature module: nn low priority triaged
how libtorch can work with  tensor data as same as  pytorch module: docs module: cpp low priority triaged,module: docs module: cpp low priority triaged
collect_env ignores conda environment module: build module: docs low priority module: collect_env.py triaged,module: build module: docs low priority module: collect_env.py triaged
"Undefined symbols for architecture x86_64: ""testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith(void const*)"" on Mac OS X module: build module: internals low priority triaged module: macos",module: build module: internals low priority triaged module: macos
[caffe2] check Range Operator inputs with bug caffe2,caffe2
weight_norm is not supported in TorchScript oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
IsType<T>() ASSERT FAILED [Detectron e2e_mask_rcnn_R-50-C4_1x.yaml] caffe2 module: assert failure,caffe2 module: assert failure
Failed to install pytorch from source on ubuntu. needs reproduction module: build triaged,needs reproduction module: build triaged
Adding a method called `T` in native_functions causes undefined behavior on Windows module: windows triaged,module: windows triaged
Getting Access to Blob/Tensor reference in jit::script::Module oncall: jit triaged,oncall: jit triaged
Data Parallel Implementation Improvements oncall: distributed triaged,oncall: distributed triaged
[cmake build] can't build pytorch with install mkl library  module: build triaged,module: build triaged
[RFC] Adding MKL-DNN Int8 functions to PyTorch/Aten/JIT backend oncall: jit oncall: quantization triaged module: mkldnn,oncall: jit oncall: quantization triaged module: mkldnn
[JIT] Expose subgraph execution for intermediate output extraction oncall: jit enhancement TSUsability TSRootCause:PoorIRVisibility,oncall: jit enhancement TSUsability TSRootCause:PoorIRVisibility
Misleading Error when doing Large Batch Matrix Multiplication module: cuda triaged,module: cuda triaged
output values not same and much slower than Python API module: cpp triaged,module: cpp triaged
Zero-dim Tensors (scalars) should be printed at full precision module: printing triaged,module: printing triaged
free(): invalid pointer Aborted (core dumped) needs reproduction triaged,needs reproduction triaged
"Stop using ""AAA"" prefix for builds module: ci triaged",module: ci triaged
[utils.bottleneck] throws initialization error for cuda profiling module: dataloader triaged,module: dataloader triaged
How to use Infiniband for cpu-cluster with backend gloo? oncall: distributed triaged,oncall: distributed triaged
"Error when creating new caffe2::Predictor(_initNet, _predictNet) caffe2",caffe2
Track running stats regardless of track_running_stats=False module: nn triaged,module: nn triaged
official libtorch static build zip file error module: binaries module: build triaged,module: binaries module: build triaged
Import warning when using the wrong version of CUDA module: cuda triaged,module: cuda triaged
convert Onnx Slice operator to caffe2 failed caffe2 triaged,caffe2 triaged
"[Proposal] Data reading framework for PyTorch (Hive, MySQL, S3 etc.)  feature module: dataloader triaged needs research",feature module: dataloader triaged needs research
LibTorch :About torch.jit.trace generate model.pt needs reproduction oncall: jit triaged,needs reproduction oncall: jit triaged
[JIT] Better Python String Support oncall: jit jit-backlog,oncall: jit jit-backlog
cmake for Torch unusable in archlinux module: dependency bug module: build triaged,module: dependency bug module: build triaged
Segmentation fault when use torch::from_blob module: crash module: cpp module: abi triaged,module: crash module: cpp module: abi triaged
[jit] set up views for Autodiff and autograd hooks  oncall: jit triaged,oncall: jit triaged
Remove unpack() in torch/csrc/nn/type_checks.h and its caller functions in the codebase module: internals module: nn good first issue triaged,module: internals module: nn good first issue triaged
In-source build causes repeating filename annotations (Windows doesn't support out-of-source build) module: build module: windows triaged,module: build module: windows triaged
[distribution] Support for various domain for AffineTransform module: distributions triaged,module: distributions triaged
[JIT] List python builtin has wrong casting behavior oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
nn.Embedding backwards slow under high row contention module: performance module: nn module: cuda triaged,module: performance module: nn module: cuda triaged
Main page broadcasting (?) example image bug module: docs triaged,module: docs triaged
profiler seems not print all op calls triaged oncall: profiler,triaged oncall: profiler
@ignore annotation for user defined type oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
NVRTC_ERROR unknown when using self-built libtorch module: build triaged,module: build triaged
Performance issue when accessing an extremely large (10GB) longtensor module: cudnn module: cuda triaged,module: cudnn module: cuda triaged
torch.norm produces incorrect results triaged module: numerical-reproducibility module: norms and normalization,triaged module: numerical-reproducibility module: norms and normalization
c++ torch::nn::Sequential increments count on name errors module: cpp triaged,module: cpp triaged
Statically make `__setstate__` set all attributes/parameters  high priority module: serialization triaged,high priority module: serialization triaged
RoiAlignTest.CheckCPUGPUEqual is still flaky caffe2 triaged module: flaky-tests better-engineering,caffe2 triaged module: flaky-tests better-engineering
Lint rule to prevent direct use of #pragma omp module: lint triaged,module: lint triaged
Dataloader's memory usage keeps increasing during one single epoch. module: dataloader triaged,module: dataloader triaged
pos_weight argument in torch.nn.BCELoss module: nn triaged enhancement,module: nn triaged enhancement
Class based Sampler for Class Incremental/Continual Learning research feature module: dataloader triaged,feature module: dataloader triaged
String in tensor module: internals triaged,module: internals triaged
CosineAnnealingLR has unexpected behavior with large step module: optimizer triaged,module: optimizer triaged
Better documentation / molly-guards around use of multiprocessing with spawn in Jupyter/ipython notebooks module: docs triaged enhancement,module: docs triaged enhancement
Overhead performance regression over time umbrella issue. high priority module: performance module: internals module: cuda module: cpu triaged quansight-nack,high priority module: performance module: internals module: cuda module: cpu triaged quansight-nack
"Define portable M_PI replacement, use it instead of non-standard M_PI in math.h module: internals triaged",module: internals triaged
torch.distributions.Binomial.sample() uses a massive amount of memory module: distributions module: memory usage triaged,module: distributions module: memory usage triaged
TensorIterator resizes output to a scalar if there are no inputs module: internals triaged,module: internals triaged
libtorch+opencv Mat result error: different from the python ones module: cpp triaged,module: cpp triaged
Redundantly saving sizes of SavedVariables in autograd Function module: bootcamp module: autograd low priority triaged,module: bootcamp module: autograd low priority triaged
Sparse tensors can't be used in DataLoader running many workers module: sparse module: multiprocessing feature triaged,module: sparse module: multiprocessing feature triaged
ProcessGroupMPI tests in test_c10d.py oncall: distributed module: tests triaged,oncall: distributed module: tests triaged
[jit] torch.tensor doesn't support list of tuples oncall: jit low priority triaged,oncall: jit low priority triaged
Failed to build with system protobuf module: build module: protobuf triaged,module: build module: protobuf triaged
"SyncBatchNorm should support 2D input (B, C) oncall: distributed module: nn triaged enhancement",oncall: distributed module: nn triaged enhancement
Unify Caffe2 and PyTorch OpenMP initialization triaged module: multithreading,triaged module: multithreading
torch.nn.threshold cannot accept tensor as a threshold module: nn triaged enhancement,module: nn triaged enhancement
RuntimeError: Given input size: (2048x1x1). Calculated output size: (2048x-5x-5). Output size is too small at /pytorch/aten/src/THNN/generic/SpatialAveragePooling.c:48 module: docs triaged module: tensorboard has workaround,module: docs triaged module: tensorboard has workaround
Advanced indexing with uint8 tensor versus int64 tensor is inconsistent module: bc-breaking triaged module: advanced indexing module: boolean tensor,module: bc-breaking triaged module: advanced indexing module: boolean tensor
Inconsistant values of lr_scheduler.get_lr and lr in optimizer.param_groups module: optimizer triaged,module: optimizer triaged
"ERROR: Command ""python setup.py egg_info"" when dockerfile build needs reproduction module: build triaged",needs reproduction module: build triaged
[JIT] Source highlighting doesn't line up when tabs are used for indentation oncall: jit triaged enhancement jit-backlog,oncall: jit triaged enhancement jit-backlog
Split libtorch binary build CI job into separate variants module: ci triaged,module: ci triaged
LSTM forget bias must be initialized properly module: bc-breaking module: nn module: rnn triaged module: initialization,module: bc-breaking module: nn module: rnn triaged module: initialization
CosineAnnealingLR giving unexpected learning rates on PyTorch 1.1. module: optimizer triaged,module: optimizer triaged
Change devtoolset7 CUDA 9.0 nightlies to use a lower devtoolset module: ci triaged,module: ci triaged
Implement noise_shape keyword for Dropout layers feature module: nn triaged,feature module: nn triaged
Creation of too big multidimensional array returns empty tensor. module: error checking triaged module: tensor creation,module: error checking triaged module: tensor creation
Precision of sparse float embeddings differs from dense embeddings on CPU module: nn triaged module: numerical-reproducibility,module: nn triaged module: numerical-reproducibility
"RuntimeError: invalid argument 10: ldb should be at least max(1, 0), but have 0 at ../aten/src/TH/generic/THBlas.cpp:36 module: internals triaged",module: internals triaged
install error from source proposal accepted module: internals triaged,proposal accepted module: internals triaged
[JIT] traced model with optimization shows no performance improvement oncall: jit triaged,oncall: jit triaged
Performance difference between 0.4.1 and 1.1.0 module: performance triaged,module: performance triaged
"BatchNorm1d does not support batchsize>65535 in eval mode with 3 dimension (NxCxL), raise CUDNN_STATUS_NOT_SUPPORTED module: dependency bug module: cudnn triaged small",module: dependency bug module: cudnn triaged small
vectorized convert_to_int_of_same_size <int64_t> can't handle nan module: cpu module: error checking triaged module: NaNs and Infs,module: cpu module: error checking triaged module: NaNs and Infs
Performance issue master (a25b79531) module: performance module: cpu triaged module: multithreading,module: performance module: cpu triaged module: multithreading
Importing open3d after PyTorch causes free(): invalid pointer module: crash triaged module: pybind,module: crash triaged module: pybind
Test failure for depthwise3x3_conv caffe2,caffe2
Bad overload order for zeros_like triaged module: pybind module: tensor creation,triaged module: pybind module: tensor creation
View in Sequential - reasoned case for it module: nn triaged enhancement,module: nn triaged enhancement
torch.set_flush_denormal not working on some (old) OSX machines triaged module: macos,triaged module: macos
Importing matlab.engine after torch causes bad_alloc module: internals triaged module: assert failure,module: internals triaged module: assert failure
"[jit] Traced {zeros,empty}()/{zeros,empty}_like() calls do not respect device args. oncall: jit triaged",oncall: jit triaged
[jit] nn.LSTM errors in nn.ScriptModule oncall: jit triaged,oncall: jit triaged
cudnn conv doesn't check batch_size > 0 module: cudnn module: cuda module: error checking triaged,module: cudnn module: cuda module: error checking triaged
[jit] Do aten::values dispatch at build time instead of runtime oncall: jit triaged,oncall: jit triaged
[doc] Document general guidelines to work with CUDA async copying and streams high priority module: docs feature module: cuda triaged better-engineering,high priority module: docs feature module: cuda triaged better-engineering
why my personal compiled libtorch is so slow?  2~3 times slower than caffe module: performance module: windows triaged,module: performance module: windows triaged
failed to load model which is saved as text format(pickle_protocol=0) instead of binary format module: pickle module: serialization triaged,module: pickle module: serialization triaged
[jit] Bad error when calling `ScriptModule`s with attributes/parameters oncall: jit triaged,oncall: jit triaged
C++ API 'nn::Sequential' has inconsistent behavior with python conterpart module: cpp triaged,module: cpp triaged
How to load PyTorch model with LSTM using C++ api module: cpp triaged,module: cpp triaged
The `unary_kernel` call should be pushed into CopyKernel.cpp and it should completely replace the current `copy_kernel`. module: cpu triaged,module: cpu triaged
[caffe2]Reloading model gives segmentation fault caffe2,caffe2
Massive memory overhead over NumPy module: internals module: memory usage triaged,module: internals module: memory usage triaged
[jit] requires_grad in JIT constructor/factories high priority triage review oncall: jit,high priority triage review oncall: jit
Different behavior when trace model. oncall: jit triaged,oncall: jit triaged
CUDA optimization: using `__restrict__` whenever possible module: cuda triaged,module: cuda triaged
Multiprocessing on distributed Multi-nodes shutdown error: â€˜spawnâ€™ on slave node leads to semaphore_tracker leaked oncall: distributed triaged,oncall: distributed triaged
torch.distributed.broadcast should default to current stream oncall: distributed triaged,oncall: distributed triaged
GCP Base Image Wrong CUDA Version triaged module: doc infra,triaged module: doc infra
add stable distribution in torch.distributions module: distributions feature low priority triaged,module: distributions feature low priority triaged
Different behavior of torch.nn.MultiMarginLoss on CPU/GPU Tensors module: cuda module: error checking triaged,module: cuda module: error checking triaged
"Deprecate torch.add(tensor, value, other) module: bc-breaking triaged module: numpy module: deprecation module: ux",module: bc-breaking triaged module: numpy module: deprecation module: ux
index_put_ take min when there are repeated indices low priority triaged enhancement module: advanced indexing,low priority triaged enhancement module: advanced indexing
8 tests in test_c10d fail when running all tests in one command oncall: distributed module: tests triaged,oncall: distributed module: tests triaged
[Feature Request] Common constants in the torch.* namespace triaged enhancement module: numpy,triaged enhancement module: numpy
LayerNorm is very slow (almost frozen) in CPU of multiprocessing module: cpu triaged,module: cpu triaged
Suggest model.eval() in torch.no_grad (and vice versa) module: docs triaged,module: docs triaged
Improve unit test coverage of torch.unique module: tests triaged module: sorting and selection,module: tests triaged module: sorting and selection
Make operators like logsumexp and cumsum operate over dimension 0 by default (or at least for 1D arrays) module: docs triaged enhancement module: numpy module: reductions,module: docs triaged enhancement module: numpy module: reductions
Support memoryview() method on torch.Tensor feature triaged module: numpy,feature triaged module: numpy
pytorch.version.cuda is None when compiling with CUDA support needs reproduction module: build module: cuda triaged,needs reproduction module: build module: cuda triaged
weight_norm doesn't support eta and returns nan for zero weights module: nn triaged module: NaNs and Infs module: norms and normalization,module: nn triaged module: NaNs and Infs module: norms and normalization
Multi-gpu via torch::nn::parallel::data_parallel oncall: distributed module: cpp feature triaged,oncall: distributed module: cpp feature triaged
Embedding layer does not check input range module: cuda module: error checking triaged,module: cuda module: error checking triaged
[jit] Can't `torch.jit.script` a lambda oncall: jit low priority triaged jit-backlog,oncall: jit low priority triaged jit-backlog
"Performance issue with torch.jit.trace(), slow prediction in C++ (CPU) triage review needs reproduction module: performance oncall: jit module: cpp",triage review needs reproduction module: performance oncall: jit module: cpp
[RFC] Memory format (aka layout aka NHWC) support module: internals triaged module: mkldnn,module: internals triaged module: mkldnn
C++ custom module not thread safe needs reproduction module: cpp triaged,needs reproduction module: cpp triaged
Python math module support oncall: jit triaged enhancement jit-backlog,oncall: jit triaged enhancement jit-backlog
Clean up and consolidate DDP tests oncall: distributed module: tests triaged,oncall: distributed module: tests triaged
torch.from_PIL() Request ? feature triaged module: vision,feature triaged module: vision
ninja: build stopped: subcommand failed. module: build triaged,module: build triaged
Performance issue of unique on CPU module: performance module: cpu triaged module: sorting and selection,module: performance module: cpu triaged module: sorting and selection
Completion of error handling module: internals low priority module: error checking triaged,module: internals low priority module: error checking triaged
[c10d] CUDA tests for C++ reducer oncall: distributed triaged,oncall: distributed triaged
Provide option to use alias method in Categorical.sample() module: distributions triaged,module: distributions triaged
[FR] torch.dist along a dimension triaged function request module: reductions,triaged function request module: reductions
CI with >8G CUDA memory module: cuda module: ci triaged module: 64-bit,module: cuda module: ci triaged module: 64-bit
Allow tracing of models which output `None` oncall: jit feature triaged,oncall: jit feature triaged
More efficient STFT on CUDA module: performance feature module: cuda triaged,module: performance feature module: cuda triaged
Running custom operator tests manually is too difficult module: cpp-extensions module: tests triaged enhancement,module: cpp-extensions module: tests triaged enhancement
[Feature Request] Flattened indices option for max pooling module: nn triaged enhancement module: pooling,module: nn triaged enhancement module: pooling
Value of torch.backends.cudnn.benchmark Baked into JIT-Traced Modules ( 150x slowdown on ConvTranspose2d() ) [jit] [libtorch] [cudnn]  oncall: jit triaged,oncall: jit triaged
Numerical instability KL divergence RelaxedOneHotCategorical module: numerical-stability module: distributions triaged,module: numerical-stability module: distributions triaged
MaxPool with n-dimensional tensors module: nn triaged enhancement module: pooling,module: nn triaged enhancement module: pooling
Add build tests for feature environment vars todo module: ci triaged,todo module: ci triaged
[FR] Warn in cuda init if cuda < 10 is used with RTX cards module: cuda module: molly-guard triaged,module: cuda module: molly-guard triaged
Memory not being deallocated in backward() module: autograd module: memory usage triaged quansight-nack,module: autograd module: memory usage triaged quansight-nack
Speed-up torch.cat on CPU module: performance module: cpu triaged,module: performance module: cpu triaged
FP32 depthwise convolution is slow in GPU high priority module: dependency bug module: performance module: cudnn module: cuda module: convolution triaged,high priority module: dependency bug module: performance module: cudnn module: cuda module: convolution triaged
documentation for C++ / libtorch autograd profiler module: docs module: cpp triaged,module: docs module: cpp triaged
Make it easier to figure out what CuDNN convolution algorithm we actually chose module: cudnn module: logging triaged,module: cudnn module: logging triaged
Jit fail with TracingCheckError with tracing model with layers created after init. oncall: jit,oncall: jit
download_mnist.py causes flaky tests oncall: releng module: ci module: tests triaged module: flaky-tests better-engineering,oncall: releng module: ci module: tests triaged module: flaky-tests better-engineering
Tests on CI are not printing exceptions as they occur low priority module: ci module: tests triaged,low priority module: ci module: tests triaged
UnpicklingError when trying to load multiple objects from a file module: pickle module: serialization triaged,module: pickle module: serialization triaged
improve jit error message for legacy constructor module: docs triaged,module: docs triaged
Port SpatialConvolutionMM and VolumetricConvolutionMM to ATen triaged module: porting,triaged module: porting
Add a RandomBatchSampler ? module: dataloader triaged enhancement,module: dataloader triaged enhancement
[FR] support default size (scalar) in torch.randint triaged module: numpy module: tensor creation function request,triaged module: numpy module: tensor creation function request
NCCL backend fails when calling broadcast from different threads high priority oncall: distributed triaged module: nccl,high priority oncall: distributed triaged module: nccl
Doubly freed pointer in torch::cat error handling when called via pybind11. module: cpp module: error checking triaged,module: cpp module: error checking triaged
Add support for tuple type deduction in C++ custom operators module: internals module: bootcamp triaged enhancement,module: internals module: bootcamp triaged enhancement
[JIT] bitwise NOT does not handle tensor shapes correctly under JIT oncall: jit triaged,oncall: jit triaged
Rename ignore_index to ignore_target in CrossEntropyLoss module: nn low priority triaged enhancement small,module: nn low priority triaged enhancement small
Update weight initialisations to current best practices high priority module: bc-breaking feature module: nn triaged quansight-nack,high priority module: bc-breaking feature module: nn triaged quansight-nack
[docs] How to achieve high-order derivation in my .cpp? module: docs module: cpp module: autograd triaged actionable,module: docs module: cpp module: autograd triaged actionable
JIT does not batch linear layers in an ensemble oncall: jit triaged,oncall: jit triaged
copy.copy not working for ScriptModule oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
torch.flip is inconsistent with np.flip and also uses `dims` arg instead of `dim` triaged module: numpy function request,triaged module: numpy function request
[JIT Script] Need support on distributions.Categorical. oncall: jit triaged,oncall: jit triaged
cuDNN error when using 3d convolutions module: dependency bug module: cudnn triaged,module: dependency bug module: cudnn triaged
caffe2 Segmentation fault (core dumped) caffe2,caffe2
Support 'bytes' type in torchscript oncall: jit triaged jit-backlog,oncall: jit triaged jit-backlog
"""unknown builtin op"" error with static library module: cpp triaged",module: cpp triaged
Binary not operator causes crash when Jit module is executed on different device oncall: jit triaged,oncall: jit triaged
torch.cuda.is_available()  returns misleading value module: cuda triaged module: ux,module: cuda triaged module: ux
JIT torch.ones_like with dtype starts failing on master oncall: jit,oncall: jit
Build compact libtorch from source with cmake  module: build module: cpp triaged,module: build module: cpp triaged
Conjugate gradient method feature triaged module: derivatives function request,feature triaged module: derivatives function request
should disable AVX on 32bit x86 / refine AVX availability tests module: build triaged,module: build triaged
CUDA large matrix-vector product (torch.mv) causes illegal memory access module: dependency bug module: cuda triaged module: 64-bit module: cublas,module: dependency bug module: cuda triaged module: 64-bit module: cublas
Caffe2 building failure caffe2,caffe2
C++ nn::Sequential push_back() copies module if the module is concrete type module: cpp module: nn triaged,module: cpp module: nn triaged
[C++ Frontend] ONNX export module: onnx triaged onnx-triaged,module: onnx triaged onnx-triaged
Caffe2 building failure when turning off USE_OPENMP caffe2,caffe2
Build fails for caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkldnn/Conv.cpp.o; possibly MKL-DNN issue caffe2,caffe2
Tensor::options() returns false for requires_grad when it is true module: cpp-extensions triaged,module: cpp-extensions triaged
PyPy support module: binaries feature triaged,module: binaries feature triaged
Feature Request: deterministic CUDA torch.nn.CTCLoss feature module: nn module: loss triaged module: determinism,feature module: nn module: loss triaged module: determinism
Non-coherent result for C++ with multithreading and GPU oncall: jit module: cpp triaged,oncall: jit module: cpp triaged
[Caffe2] install error caffe2,caffe2
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED when batch size is too large module: cudnn triaged,module: cudnn triaged
"distributed data parallel, gloo backend works, but nccl deadlock oncall: distributed triaged module: nccl module: deadlock",oncall: distributed triaged module: nccl module: deadlock
"collate_fn returns subclass of torch.Tensor, but DataLoader transforms back to torch.Tensor module: dataloader triaged module: __torch_function__",module: dataloader triaged module: __torch_function__
Training hangs when using DistributedDataParallel in two pod on two nodes  oncall: distributed triaged,oncall: distributed triaged
Improve save() method in torch.jit.ScriptModule oncall: jit low priority triaged jit-backlog,oncall: jit low priority triaged jit-backlog
Building pytorch on ARM failed module: build triaged,module: build triaged
C++ API: Crash in cudnnDestroy() when deconstructing module: cudnn module: cpp module: abi triaged,module: cudnn module: cpp module: abi triaged
TracedModule 'to' attribute doesn't work for tensors created on forward. oncall: jit triaged,oncall: jit triaged
"Not depend on third_party submodules, but self-built libraries? module: build triaged",module: build triaged
Document whether it is possible to train TorchScript modules oncall: jit module: docs module: cpp triaged,oncall: jit module: docs module: cpp triaged
The documentation to Module._version isn't visible. module: docs module: nn module: serialization triaged,module: docs module: nn module: serialization triaged
Add ASSERT for calling accessor on a GPU tensor module: cpp module: molly-guard triaged,module: cpp module: molly-guard triaged
"torch.Tensor.cpu talks about the object being ""on the correct device"" module: docs module: bootcamp triaged small",module: docs module: bootcamp triaged small
cmake fails with -DBUILD_PYTHON=OFF -DUSE_NNPACK=OFF module: build oncall: releng triaged,module: build oncall: releng triaged
RuntimeError: storage_.IsType<T>() ASSERT FAILED awaiting response (this tag is deprecated) caffe2 triaged,awaiting response (this tag is deprecated) caffe2 triaged
"improved assert message in the case of ""CUDA error: device-side assert triggered"" module: bootcamp module: cuda module: molly-guard triaged",module: bootcamp module: cuda module: molly-guard triaged
testConvnetBenchmarks intermittently segfaults triaged module: flaky-tests better-engineering,triaged module: flaky-tests better-engineering
"torch.nn.CrossEntropyLoss with ""reduction"" sum/mean is not deterministic on segmentation outputs / labels module: cuda triaged",module: cuda triaged
Code review on .circleci/ module: ci triaged,module: ci triaged
Implement Adaptive Input Representations for Neural Language Modeling feature module: nn triaged,feature module: nn triaged
Use standard docker image for XLA build oncall: releng triaged module: xla module: docker,oncall: releng triaged module: xla module: docker
Generic object to tensor dispatching feature triaged,feature triaged
Generated `__init__.pyi` contains invalid default values triaged module: codegen,triaged module: codegen
Proposal: Add __tensor_wrap__ method similar to numpy __array_wrap__ module: internals triaged module: numpy,module: internals triaged module: numpy
[feature request] build and move distributions w/ device and/or dtype module: build module: distributions triaged,module: build module: distributions triaged
Group Norm Error When using FP16 needs reproduction triaged module: type promotion module: half module: norms and normalization,needs reproduction triaged module: type promotion module: half module: norms and normalization
Deadlock with multiprocessing (using fork) and OpenMP / PyTorch should warn after OMP and fork that multithreading may be broken high priority module: docs module: multiprocessing triaged,high priority module: docs module: multiprocessing triaged
Global Second Order Pooling todo feature triaged module: pooling,todo feature triaged module: pooling
Better API / conversion for c++ objects to correct IValue objects oncall: jit low priority jit-backlog,oncall: jit low priority jit-backlog
Error in converting pytorch model to caffe2 using onnx framework  caffe2,caffe2
Conv2d layers should accept 4-tuple for padding argument module: nn module: convolution triaged enhancement,module: nn module: convolution triaged enhancement
Distributed training jobs do not terminate properly if there is a crash oncall: distributed triaged,oncall: distributed triaged
"Be able to use ""@pytorchbot retest this please"" to re-run both CircleCI and Jenkins jobs feature module: ci triaged",feature module: ci triaged
Errors running distributed example oncall: distributed triaged,oncall: distributed triaged
Download speed issues with the pytorch conda channel module: dependency bug triaged,module: dependency bug triaged
Hardshrink for Sparse Tensors module: sparse feature triaged,module: sparse feature triaged
torch.multiprocessing.pool.Pool broken module: multiprocessing triaged small,module: multiprocessing triaged small
Seg fault with test_rnn_retain_variables on ppc64le module: crash triaged module: POWER,module: crash triaged module: POWER
Multiple CPU processes using same GPU model for inference module: windows module: multiprocessing triaged,module: windows module: multiprocessing triaged
Issue with dataloader using pin_memory = True module: dataloader triaged,module: dataloader triaged
Implement `numpy.random.choice` equivalent high priority module: bootcamp feature triaged module: numpy,high priority module: bootcamp feature triaged module: numpy
`nn.Linear` allows 1d input tensors module: docs triaged,module: docs triaged
Allow building C++ custom ops that imports another custom ops module: cpp-extensions triaged,module: cpp-extensions triaged
Allow to build pytorch for a *specific* architecture module: build triaged,module: build triaged
cstddef not found when compiling C++ Extension - macOS module: cpp-extensions triaged,module: cpp-extensions triaged
Wrong description of positive class weight in BCEWithLogitsLoss module: docs triaged,module: docs triaged
"Should be a way to unpickle an object with a torch cuda tensor on a CPU-only machine when using plain ""pickle"" todo feature module: serialization triaged",todo feature module: serialization triaged
torch.save overwrite module: docs triaged enhancement,module: docs triaged enhancement
Performance regression on CPU from 0.4.1 to 1.0.0 on ResNet inference module: performance module: cpu triaged module: single threaded,module: performance module: cpu triaged module: single threaded
culibos linker errors on binary_linux_conda_3.6_cu90_build module: build triaged,module: build triaged
Better include path when compiling mkldnn module: build triaged,module: build triaged
Very poor Uniform() sampling near floating 0.0 triaged module: random,triaged module: random
split_with_sizes should accept a LongTensor as the split_sizes parameter feature triaged module: numpy,feature triaged module: numpy
[feature request] Store accumulated gradients in separate GPU or on CPU memory triaged module: data parallel,triaged module: data parallel
Check whether _cudnn_rnn_flatten_weight can avoid changing the TensorImpl or Storage pointer of tensors in `weight_arr` module: cudnn triaged module: assert failure,module: cudnn triaged module: assert failure
Reduce fragmentation with CUDA caching allocator when using many streams module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
new_zeros is not traced correctly oncall: jit good first issue triaged,oncall: jit good first issue triaged
ROCm 2.1: test_gamma_gpu_sample test fails module: rocm triaged,module: rocm triaged
"CUDA cache doubles on the second batch and causes OOM, `empty_cache` doesn't empty it module: dependency bug module: cudnn triaged",module: dependency bug module: cudnn triaged
Error while installing pytorch module: build caffe2,module: build caffe2
[JIT] NVRTC unknown error needs reproduction oncall: jit has workaround,needs reproduction oncall: jit has workaround
Make sure `data_ptr` for non-zero-size input tensors stays the same after the VariableType dispatch module: autograd module: molly-guard triaged module: assert failure,module: autograd module: molly-guard triaged module: assert failure
Error with setting tensors to use cpu in packed_padded_sequence when CUDA tensor is set as default module: rnn module: cuda triaged,module: rnn module: cuda triaged
TorchConfig.cmake always sets _GLIBCXX_USE_CXX11_ABI module: build triaged,module: build triaged
caffe2 softmaxwithloss problem caffe2,caffe2
TensorRTOpTest.test_vgg19 is flaky triaged module: flaky-tests,triaged module: flaky-tests
A bug in parallel.data_parallel when module_kwargs is not None module: cuda module: error checking triaged module: batching module: data parallel,module: cuda module: error checking triaged module: batching module: data parallel
Flip is much slower than advanced indexing module: performance triaged module: viewing and reshaping,module: performance triaged module: viewing and reshaping
error on cmake_version from tools/build_pytorch_libs.py module: build triaged,module: build triaged
error with nccl when distributed training on caffe2 caffe2,caffe2
Python-bound C++ frontend modules don't handle attributes well module: cpp triaged,module: cpp triaged
support `unique_indices` option for `unique` todo feature triaged module: numpy,todo feature triaged module: numpy
Should torch.arange take a layout parameter? module: sparse triaged module: tensor creation module: ux,module: sparse triaged module: tensor creation module: ux
We're binding a bunch of crap to 'torch' namespace which shouldn't be there triaged better-engineering,triaged better-engineering
Pytorch with CUDA aware OpenMPI for Infiniband not working with HCOLL and MXM oncall: distributed triaged module: c10d distributed-backlog,oncall: distributed triaged module: c10d distributed-backlog
caffe2::onnx::OnnxExporter::Caffe2OpToOnnxNodes failure caffe2,caffe2
ProcessGroupGlooTest.test_gather_stress is flaky oncall: distributed triaged module: flaky-tests module: c10d,oncall: distributed triaged module: flaky-tests module: c10d
Unexpected behavior of jit.trace when PYTORCH_JIT=0 oncall: jit low priority,oncall: jit low priority
Confusing documentation with distributions.Categorical about logits module: distributions module: docs triaged,module: distributions module: docs triaged
support for multiple torch.cuda.max_memory_allocated() counters todo feature module: cuda triaged,todo feature module: cuda triaged
[JIT] Support C++ front end module and JIT interop oncall: jit,oncall: jit
why check ArgumentInfo is_pod? suffer bugs module: docs low priority triaged,module: docs low priority triaged
Sparse matrix multiplication is too slow module: sparse triaged,module: sparse triaged
CategoricalCrossEntropy Loss runs with wrong tag module: nn module: loss triaged,module: nn module: loss triaged
[Caffe2] resize5d stride/dim issue caffe2,caffe2
build failure with NNAPI enabled (Caffe2 path but under pytorch umbrella) module: build caffe2 triaged,module: build caffe2 triaged
More data type support for gather_map module: nn triaged enhancement,module: nn triaged enhancement
ProcessGroupGlooTest.test_scatter_stress_cuda is flaky oncall: distributed triaged module: flaky-tests module: c10d,oncall: distributed triaged module: flaky-tests module: c10d
Broadcasting and Additional Dimensions for pairwise_distance module: nn triaged function request module: distance functions,module: nn triaged function request module: distance functions
RuntimeError: [enforce fail at pybind_state.cc:1111] success. Error running net train caffe2,caffe2
Deformable Convolution feature triaged module: vision,feature triaged module: vision
DataLoader with option to re-use worker processes high priority feature module: dataloader triaged,high priority feature module: dataloader triaged
computing  entropy of a tensor  high priority triaged function request,high priority triaged function request
AvgPool2d doesn't test if kernel is smaller than input size module: bootcamp module: error checking triaged better-engineering module: pooling,module: bootcamp module: error checking triaged better-engineering module: pooling
No test coverage for kwargs of AvgPool2d and AvgPool3d module: bootcamp module: nn triaged module: pooling,module: bootcamp module: nn triaged module: pooling
Possible regression in incremental build module: build triaged,module: build triaged
Implicit conversion error in caffe2 caffe2,caffe2
Beta Distribution values wrong for a=b---> 0 module: distributions triaged,module: distributions triaged
"Nn.dataparallel with multiple output, weird gradient result None oncall: distributed triaged",oncall: distributed triaged
[Caffe2] Internal compiler error for CUDA. caffe2,caffe2
Feature request: transposed locally connected layer todo module: nn triaged enhancement,todo module: nn triaged enhancement
documentation for adding a new type via C++ extensions module: docs triaged module: complex module: bfloat16,module: docs triaged module: complex module: bfloat16
LibTorch: include cmake files for all distributed headers module: cpp triaged,module: cpp triaged
Improve one_hot module: nn triaged function request,module: nn triaged function request
JIT is not compatible with data parallel oncall: jit,oncall: jit
F.grid_sample doesn't respect padding_mode when height of inputs is 1 module: docs triaged module: interpolation,module: docs triaged module: interpolation
Projective Transformation grid generator todo triaged enhancement,todo triaged enhancement
Proposal for build system under many system configuration testing module: build module: ci triaged,module: build module: ci triaged
AttributeError: module 'caffe2.python._import_c_extension' has no attribute 'get_cudnn_version' when Caffe2 is not built with CuDNN caffe2,caffe2
libtorch without cmake  module: build module: cpp triaged,module: build module: cpp triaged
CMake Error at CMakeLists.txt:10 (find_package) in C++ module: build triaged,module: build triaged
Momentum problem (1-momentum is correct?) in BatchNorm2d todo module: docs module: nn triaged,todo module: docs module: nn triaged
manylinux2014 compatible wheels module: build triaged,module: build triaged
linked error of Pytorch 1.0 release  module: build triaged module: macos,module: build triaged module: macos
[caffe2] Installation problem on OSX caffe2,caffe2
possible unsafety in torch.distributions.kl_divergence for Bernoullis module: distributions triaged module: NaNs and Infs,module: distributions triaged module: NaNs and Infs
index_add_ with scalar values instead of tensors triaged enhancement module: advanced indexing,triaged enhancement module: advanced indexing
MultiGPU for gru module: rnn triaged module: data parallel,module: rnn triaged module: data parallel
err:torch.nn.CrossEntropyLoss module: docs triaged,module: docs triaged
[Feature Request] cdist: pairwise distances between two sets of tensors with batch mode triaged module: batching function request module: distance functions,triaged module: batching function request module: distance functions
Gather backward is faster than integer indexing on GPU module: performance triaged module: determinism,module: performance triaged module: determinism
cudnn not found module: build module: cudnn triaged actionable,module: build module: cudnn triaged actionable
Maybe a bug when using DataParallel triaged module: data parallel,triaged module: data parallel
Use std::variant to represent C++ side enumerations (with binding support) module: cpp triaged,module: cpp triaged
Update third_party/googletest - Ability to skip tests in GTEST module: tests triaged module: third_party,module: tests triaged module: third_party
torch.save does not work if nn.Module has partial JIT. oncall: jit,oncall: jit
In-place operations on `.data` or `.detach()` of sparse tensor doesn't update the original tensor module: sparse triaged,module: sparse triaged
[JIT] jit.trace fails with custom GRUs and CUDA when the sequence is longer oncall: jit,oncall: jit
as_tensor does not use the device of the default tensor type module: docs module: cuda triaged,module: docs module: cuda triaged
get/set device in c++ module: cpp triaged small,module: cpp triaged small
[Feature request] create sparse coo matrix w/o index check module: performance module: sparse triaged enhancement,module: performance module: sparse triaged enhancement
Missing dilation from several pooling modules (AvgPool) hackamonth triaged module: pooling function request,hackamonth triaged module: pooling function request
[discussion] Recommend a different file extension for models (.PTH is a special extension for Python) triaged,triaged
Negative indexing for nn.Embedding inputs module: nn triaged needs design function request,module: nn triaged needs design function request
Test OpenCV4 in CI module: ci triaged,module: ci triaged
Modern interface for Storage module: internals triaged,module: internals triaged
Tensor.copy_() seems to work improperly with numpy/list indices module: docs triaged,module: docs triaged
pytorch_doc_push is racing with itself triaged,triaged
CI: Flaky download from download.pytorch.org module: ci triaged module: flaky-tests better-engineering,module: ci triaged module: flaky-tests better-engineering
[Feature Request] linux distribution friendly build system module: build triaged enhancement,module: build triaged enhancement
Advanced indexing slower than numpy module: performance triaged module: numpy module: advanced indexing,module: performance triaged module: numpy module: advanced indexing
[Caffe2] Exception when creating gradient for [Cast] SquaredL2Distance as output layer of CNN network caffe2,caffe2
[ONNX CI] TestCaffe2End2End.test_squeezenet occasional error caffe2,caffe2
[c10d] Check that allgather/gather output tensors point to different storage oncall: distributed feature triaged,oncall: distributed feature triaged
libtorch exports protobuf symbols high priority module: build module: protobuf module: cpp module: abi triaged,high priority module: build module: protobuf module: cpp module: abi triaged
pin_memory/is_pinned API is too CUDA-centric high priority triaged module: memory format needs design,high priority triaged module: memory format needs design
"Batch matmul with sparse matrix, dense vector todo module: sparse triaged",todo module: sparse triaged
"building bundled nccl fails in (caffe2, cuda 8, cudnn 7) CI environment caffe2",caffe2
Flaky download from files.pythonhosted.org when installing botocore triaged module: flaky-tests better-engineering,triaged module: flaky-tests better-engineering
"Add a debug mode which is -O0 for framework code, but -O for kernels module: build triaged",module: build triaged
"Optimizer warning when parameters ""change"" module: optimizer triaged enhancement",module: optimizer triaged enhancement
The speed of scatter is influenced by the data size while using nn.DataParallel module: performance module: dataloader triaged module: data parallel,module: performance module: dataloader triaged module: data parallel
[Caffe2] Error protos.protos_size() == OutputSize() when loading dataset created by regular Caffe (datum) caffe2,caffe2
[JIT] Tracing a script function/module where not all args are Tensors oncall: jit,oncall: jit
Use of STL templates in cpu/ directory (compiling with different AVX settings) is silently hazardous module: build triaged,module: build triaged
[c10d] Configurable timeout per operation for MPI backend oncall: distributed feature triaged distributed-backlog,oncall: distributed feature triaged distributed-backlog
[Caffe2] Check failed: output->size() == values_.size() output size: 1 given size: 1563551 caffe2,caffe2
Error: DeviceGuardImpl for cpu is not available (static linking PyTorch) module: build triaged module: static linking has workaround,module: build triaged module: static linking has workaround
[libtorch] Catkin_make compilation error needs reproduction module: build triaged has workaround,needs reproduction module: build triaged has workaround
[Caffe2] Error when loading a leveldb dataset using brew.db_input (Error protos.protos_size() == OutputSize().) caffe2,caffe2
Provide Protobuf library if libtorch was built with included version module: build module: protobuf module: cpp triaged,module: build module: protobuf module: cpp triaged
[bug] inconsistent behavior of indexing  triaged module: advanced indexing,triaged module: advanced indexing
Caffe2->ONNX conversion issue due to spatial-bn caffe2,caffe2
C++ model load error oncall: jit triaged,oncall: jit triaged
Failed to run 'bash ../tools/build_pytorch_libs.sh --use-nnpack --use-mkldnn --use-qnnpack caffe2' module: build triaged,module: build triaged
Failed to run 'bash ../tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn --use-qnnpack caffe2' module: build triaged,module: build triaged
running caffe2 float16 tensors results in aten runtime error caffe2,caffe2
[feature request] bincount along specified dimension(s) triaged module: numpy module: sorting and selection function request,triaged module: numpy module: sorting and selection function request
Make torch.multiprocessing.SpawnContext usable module: multiprocessing feature triaged,module: multiprocessing feature triaged
[tracking task] FBGEMM guarding AVX2 properly module: build triaged module: third_party,module: build triaged module: third_party
Pytorch very slow to convert list of numpy arrays into tensors high priority module: performance triaged enhancement module: numpy has workaround,high priority module: performance triaged enhancement module: numpy has workaround
[c10d] Coordinated file truncation for FileStore oncall: distributed feature triaged distributed-backlog,oncall: distributed feature triaged distributed-backlog
test_spectral_norm: Backward is not reentrant module: autograd triaged module: data parallel module: norms and normalization,module: autograd triaged module: data parallel module: norms and normalization
Feature request: von Mises-Fisher distribution module: distributions triaged,module: distributions triaged
PyTorch streams are not cuda-memcheck clean module: cuda triaged,module: cuda triaged
torch.linspace does not check for infinity and nan module: error checking triaged module: tensor creation,module: error checking triaged module: tensor creation
Error in building Caffe2 on Windows (experimental operators) caffe2,caffe2
depthwise convolution are slow on cpu  module: performance module: cpu module: convolution triaged,module: performance module: cpu module: convolution triaged
[feature request] Singular values and spectral norm for convolutional layers  module: convolution triaged module: linear algebra module: fft function request,module: convolution triaged module: linear algebra module: fft function request
[jit] restrict promotion of single-element arguments to lists oncall: jit,oncall: jit
Support gathering nested lists in DataParallel  triaged module: data parallel,triaged module: data parallel
batch_norm doesn't bump version counter of running stats high priority module: autograd module: nn triaged actionable fixathon,high priority module: autograd module: nn triaged actionable fixathon
Use a dill-based multiprocessing library and serialization module: multiprocessing triaged,module: multiprocessing triaged
Non-Zero Padding in Convolution Module module: nn module: convolution triaged,module: nn module: convolution triaged
DataLoader num_workers > 0 causes CPU memory from parent process to be replicated in all worker processes high priority module: dependency bug module: multiprocessing module: dataloader module: molly-guard module: memory usage triaged,high priority module: dependency bug module: multiprocessing module: dataloader module: molly-guard module: memory usage triaged
torch.utils.checkpoint is not compatible with nn.DataParallel module: checkpoint triaged module: data parallel,module: checkpoint triaged module: data parallel
Caffe2: Causes error when using flag remove_legacy_pad while converting from caffe to caffe2 caffe2,caffe2
Memory inefficient in batched matmul when requiring gradients triaged module: linear algebra,triaged module: linear algebra
[pytorch] [feature request] Error out if the needed GPU device capability is absent in runtime module: cuda module: error checking triaged,module: cuda module: error checking triaged
cudnn explicit paths and GCC multilib suffixes prevents detection of good cudnn headers module: build triaged,module: build triaged
circular module reference raises RecursionError module: nn triaged,module: nn triaged
"arm64 port for PyTorch, libtorch module: ci triaged enhancement module: arm",module: ci triaged enhancement module: arm
"warning: attribute namespace ""clang"" is unrecognized; High Sierra / Fedora compilation with clang results in spurious clang errors in nvcc module: build module: cuda triaged module: build warnings",module: build module: cuda triaged module: build warnings
Tests that download from internet should retry on failure  module: ci triaged,module: ci triaged
Backward pass over torch.nn.functional.pad is extremely slow with half tensors module: performance module: cuda triaged module: half,module: performance module: cuda triaged module: half
Generalized Data Class module: dataloader triaged module: data,module: dataloader triaged module: data
[caffe2]How can I export init_net.pb and predict_net.pb files on my own? caffe2 triaged,caffe2 triaged
Python dataloader Improvements module: dataloader triaged,module: dataloader triaged
Improved performance for torch.multinomial with small batches module: performance module: cpu triaged,module: performance module: cpu triaged
Fail to Run Caffe2 with Successful Build: This caffe2 python run does not have GPU support caffe2,caffe2
Installing pytorch from source on labs.cognitiveclass.ai triaged module: POWER,triaged module: POWER
Come up with a better strategy for noticing BC-breaking attribute additions to serializable classes module: bc-breaking module: molly-guard triaged,module: bc-breaking module: molly-guard triaged
RelaxedOneHotCategorical not implementing entropy (and other abstract methods) module: distributions triaged enhancement,module: distributions triaged enhancement
Massive initial memory overhead GPU module: cuda module: memory usage triaged,module: cuda module: memory usage triaged
[CAPI] Increase of memory usage when exporting a Adam optimzer module: cpp module: optimizer module: memory usage triaged,module: cpp module: optimizer module: memory usage triaged
Test that (cd build && ninja) immediately after build is no-op in CI module: build module: ci triaged,module: build module: ci triaged
BUILD_BINARY is a lie module: build triaged,module: build triaged
Initialization error when moving data to the GPU module: cuda triaged,module: cuda triaged
tutorial_blob ERROR caffe2,caffe2
Port dragon4_scientific for pretty float tensor print. module: printing triaged enhancement,module: printing triaged enhancement
C++ frontend: how to debug nan gradients module: cpp triaged enhancement,module: cpp triaged enhancement
"Build the docker image from source, but torch.cuda.is_available()==false triaged module: docker",triaged module: docker
cdf in torch.distributions.bernoulli throws NotImplementedError todo module: distributions triaged,todo module: distributions triaged
pack_padded_sequence throws IndexError when only kwargs are specified module: rnn triaged,module: rnn triaged
Error occuring while converting mnist or cifar model from caffe2 to onnx caffe2,caffe2
How to run a pytorch-onnx-caffe2 model on GPU? caffe2 triaged,caffe2 triaged
How can I build caffe2_gtest_main under pytorch/caffe2/test/ folder? caffe2 triaged,caffe2 triaged
[feature request] ignore_index and size_average in nn.AdaptiveLogSoftMaxWithLoss module: nn triaged,module: nn triaged
Move collate_fn functionality / responsibility into Dataset object module: dataloader triaged,module: dataloader triaged
Differentiation through Module parameters updates feature module: autograd module: nn module: optimizer triaged,feature module: autograd module: nn module: optimizer triaged
"pytorch/torch/utils/cpp_extension.py ignores compiler setting,  module: cpp-extensions triaged",module: cpp-extensions triaged
Install Jetson TX2 Max Regcount Error needs reproduction module: build triaged module: jetson,needs reproduction module: build triaged module: jetson
Request for stripped down / inference only pytorch wheels module: build triaged,module: build triaged
[CMake] Linking against Intel OpenMP module: build triaged module: mkldnn module: openmp,module: build triaged module: mkldnn module: openmp
Provide better documentation for torch.Size module: docs triaged,module: docs triaged
"The text design (color, type) makes it hard to read todo triaged",todo triaged
Support calculating grad for dense in sparse @ dense  module: sparse triaged,module: sparse triaged
CuDNN convolution on some CUDA devices will not preserve NaN weights (upstream bug) module: dependency bug module: cudnn low priority triaged,module: dependency bug module: cudnn low priority triaged
Certain operations cause implicity sync-points module: cuda triaged,module: cuda triaged
"input_device, output_device, devices_used properties module: nn triaged",module: nn triaged
"Could not find a package configuration file provided by ""Torch"" with any of   the following names: module: cpp-extensions triaged",module: cpp-extensions triaged
`pstrf` on positive semi-definite matrices triaged module: linear algebra function request,triaged module: linear algebra function request
[Caffe2] Segmentation fault (core dumped) while import caffe2.python.core caffe2,caffe2
nn.functional.linear() for sparse tensor module: sparse module: nn triaged,module: sparse module: nn triaged
[Caffe2] Relink error after installing Caffe2 from conda caffe2,caffe2
.cuda() changes a module's behavior when there are registered buffers with requires_grad=True module: autograd module: nn triaged,module: autograd module: nn triaged
clip_grad_norm_ does not work on grads of different types todo module: nn triaged,todo module: nn triaged
Misleading step method in lr_scheduler.ReduceLROnPlateau todo module: optimizer triaged,todo module: optimizer triaged
"Error: Internal Compiler Error (codegen): ""there was an error in verifying the lgenfe output!"" module: build module: cuda triaged internals",module: build module: cuda triaged internals
Stop using make_intrusive directly; provide some make_tensor module: internals triaged,module: internals triaged
cuda test hangs if GPUs in Exclusive Process mode todo module: cuda triaged,todo module: cuda triaged
[feature request] Publish wheels with debug symbols module: binaries triaged,module: binaries triaged
[Enhancement] Increase user-friendliness of dataset.random_split triaged enhancement module: data,triaged enhancement module: data
Specify out= argument to convolution module: convolution triaged enhancement,module: convolution triaged enhancement
Jit cannot trace autograd for certain operator oncall: jit,oncall: jit
dtype mismatch error messages can be misleading todo module: error checking module: molly-guard triaged,todo module: error checking module: molly-guard triaged
[feature request] Kumaraswamy distribution module: distributions feature triaged,module: distributions feature triaged
torch.bmm doesn't support CUDA uint8 (byte) tensor todo module: bootcamp module: cuda triaged enhancement module: linear algebra,todo module: bootcamp module: cuda triaged enhancement module: linear algebra
caffe2 argmax and argmin documentation incorrect for output type caffe2,caffe2
[caffe2] Bug for softmaxwithloss operator caffe2,caffe2
error: â€˜array_sizeâ€™ is not a class template module: build module: cpp triaged,module: build module: cpp triaged
DataParallel: Parallel_apply assert len(modules) == len(inputs) AssertionError oncall: distributed triaged,oncall: distributed triaged
set num_workers on the dataloader make the jupyter kernel crash at the almost end of the epoch  module: dataloader triaged,module: dataloader triaged
Mysterious error due to num_workers: 1 module: multiprocessing triaged,module: multiprocessing triaged
Semaphore leaks in dataloader module: dataloader triaged,module: dataloader triaged
One GPU is more memory efficient than Multiple GPUs module: multi-gpu triaged module: data parallel,module: multi-gpu triaged module: data parallel
DataLoader: Could not wrapper a exception in threads module: dataloader module: error checking triaged,module: dataloader module: error checking triaged
Add min mode to embedding bags module: nn triaged enhancement,module: nn triaged enhancement
Request to import pytest in test/*.py module: tests triaged,module: tests triaged
"CrossEntropyLoss, ignore_index does not prevent back-prop if the logits are -inf module: docs module: nn module: loss triaged",module: docs module: nn module: loss triaged
"[CLEANUP] Context functions should return TypeExtendedInterface, not Type triaged better-engineering",triaged better-engineering
[JIT][tracer] Slicing shape is specialized to tensor rank oncall: jit,oncall: jit
at::Device makes it very easy to write buggy code triaged better-engineering,triaged better-engineering
[feature request] - Allow sequences lengths to be 0 in PackSequence module: nn module: rnn triaged enhancement,module: nn module: rnn triaged enhancement
"Tests with ""."" in the name cannot be run standalone module: tests triaged",module: tests triaged
[caffe2] caffe2 openmp linking error with xcode 9.0(AppleClang 9.0) on mac caffe2,caffe2
High leverage TH operations to port to ATen triaged module: porting better-engineering module: tensor creation,triaged module: porting better-engineering module: tensor creation
[Feature request] Intuitive error message when input to Linear is not cudarized module: error checking triaged,module: error checking triaged
Views created in no_grad block still have requires_grad=True high priority module: docs module: autograd triaged,high priority module: docs module: autograd triaged
[distributions] Torch distribution samplers slow on expanded parameters todo module: distributions triaged,todo module: distributions triaged
Suggest: DataLoader add device parameter  todo module: dataloader triaged enhancement,todo module: dataloader triaged enhancement
Better user experience for using Generator object todo module: docs module: cuda triaged module: random,todo module: docs module: cuda triaged module: random
Why does DistributedDataSampler not use default RNG? oncall: distributed triaged,oncall: distributed triaged
Ubuntu 16.04 setup.py error - undefined reference to elfLink_Get_FatBinary_From_Object' /usr/lib/x86_64-linux-gnu/libcuda.so: undefined reference to elf32_section_header' module: build triaged module: undefined reference,module: build triaged module: undefined reference
[pytorch][feature request] Cosine distance / simialrity between samples of own tensor or two tensors module: nn triaged module: numpy function request module: distance functions,module: nn triaged module: numpy function request module: distance functions
[Caffe2] Error C2492: data with thread storage duration may not have dll interface caffe2,caffe2
use_system_nccl flag does not work? module: build triaged,module: build triaged
[...] operator for masked select does not broadcast anymore todo triaged module: sorting and selection,todo triaged module: sorting and selection
undefined reference to 'caffe2::Caffe2FlagsRegistry[abi:cxx11]()' module: build triaged,module: build triaged
"non-shuffling data loaders can affect random states, thus the results of shuffling data loaders. todo module: dataloader triaged",todo module: dataloader triaged
 A bug in roi_align.cc? [Caffe2] caffe2,caffe2
Multiprocess Deadlock when using np.transpose and torch.stack  module: multiprocessing triaged,module: multiprocessing triaged
Error when building  caffe2,caffe2
Where could I see all of the caffe2 operators and their arguments in python API? caffe2,caffe2
[feature request] padding for torch.cat  triaged enhancement module: viewing and reshaping,triaged enhancement module: viewing and reshaping
Better dev docs for writing native CPU kernels with Vec256 triaged module: vectorization,triaged module: vectorization
Cannot run torch in different sub-interpreters todo needs reproduction module: cpp triaged,todo needs reproduction module: cpp triaged
[Caffe2] Model work Alright in Python but Failed in C++ caffe2,caffe2
Cannot find operator schema for 'ATen' Caffe2 Ios caffe2 triaged,caffe2 triaged
LNK2019 error when linking with MSVC [Caffe2] caffe2,caffe2
[distributed] Synchronization on CUDA side with MPI backend oncall: distributed module: docs triaged module: c10d distributed-backlog,oncall: distributed module: docs triaged module: c10d distributed-backlog
Unexpected Behavior when Pointwise Operations Write to Expanded Tensors module: internals good first issue triaged module: partial aliasing,module: internals good first issue triaged module: partial aliasing
[Caffe2] Failed to build dispatch_test. Error LNK2001: unresolved external symbol caffe2,caffe2
Have all C++ modules expose a __file__ attribute module: docs triaged enhancement,module: docs triaged enhancement
size mismatch when trying to reconstruct predifined network triaged module: vision,triaged module: vision
Build system doesn't prevent ATen/core from including non-core files module: build triaged,module: build triaged
make install errorï¼š [third_party/gloo/gloo/CMakeFiles/gloo.dir/all] Error 2 module: build triaged module: third_party,module: build triaged module: third_party
Tensor.register_hook is not passing the tensor object to the hook function module: bc-breaking triaged enhancement,module: bc-breaking triaged enhancement
[Caffe2] Error importing ConvTranspose2d to Caffe2 with ONNX caffe2,caffe2
Request for better memory management feature module: memory usage triaged,feature module: memory usage triaged
"Bilinear interpolation behavior inconsistent with TF, CoreML and Caffe triaged module: interpolation",triaged module: interpolation
[BUG]: unstable happend in saving model. module: serialization triaged,module: serialization triaged
[Caffe2] Error C2375 when building DLL.  caffe2,caffe2
Reduce code duplication in interpolate and make it more generic feature triaged module: interpolation,feature triaged module: interpolation
[feature request] batch_first option in torch.utils.data module: dataloader triaged enhancement,module: dataloader triaged enhancement
"Sending CUDA tensor to process, and then back, does not work module: bootcamp module: multiprocessing module: cuda triaged",module: bootcamp module: multiprocessing module: cuda triaged
[feature request] Runtime warning for inappropriate labels (among others) module: loss module: error checking triaged,module: loss module: error checking triaged
[Caffe2] Build failure on Ubuntu 16.04 caffe2,caffe2
"[Feature request] Batch eig/symeig functions (for small matrices, with CUDA) triaged module: batching module: linear algebra function request",triaged module: batching module: linear algebra function request
build error caffe2,caffe2
torch.utils.data.random_split() returns dataset index as tensor module: docs module: dataloader triaged,module: docs module: dataloader triaged
error when building caffe2 from source with gcc 7 & cuda gcc 5.5 caffe2,caffe2
"[Caffe2] MNIST Tutorial LMDB Error ""Cannot open db""  caffe2",caffe2
Sparse tensor use cases module: sparse feature triaged,module: sparse feature triaged
RNN gradients in eval mode in pytorch 0.4 module: nn module: rnn triaged,module: nn module: rnn triaged
[feature request] Add matrix functions triaged module: numpy module: linear algebra function request,triaged module: numpy module: linear algebra function request
python caffe2/python/operator_test/activation_ops_test.py  Segmentation fault (core dumped) caffe2,caffe2
Stop passing inplace/out arguments as (non-const) Tensor& to functions module: internals module: cpp triaged enhancement,module: internals module: cpp triaged enhancement
A serious problem when installing caffe2. Can anyone  help me? caffe2,caffe2
Stop ifdef'ing out scatter/gather (comm) in libtorch module: cpp module: cuda triaged,module: cpp module: cuda triaged
CRITICAL:root:Cannot load caffe2.python. Error: DLL load failed: A dynamic link library (DLL) initialization routine failed. caffe2,caffe2
interaction with FindCUDA causes spurious re-cmakes module: build triaged,module: build triaged
[doc] functionalities not documented module: docs good first issue triaged,module: docs good first issue triaged
Unintuitive reduction of mini-batch loss for NLLLoss module: docs module: nn module: loss triaged,module: docs module: nn module: loss triaged
"Pytorch is slow when only using CPU, and cannot utilize multicore of CPU module: performance module: cpu triaged module: multithreading",module: performance module: cpu triaged module: multithreading
Build torch as a submodule with static linking doesn't work (CAFFE2_PERF_WITH_AVX2 is not defined) caffe2 module: static linking,caffe2 module: static linking
dataloader stuck at sched_yield =0 module: dataloader triaged,module: dataloader triaged
[request] speed-up multidim slicing backward todo module: performance module: autograd triaged,todo module: performance module: autograd triaged
[Build error] libcudnn.so: error adding symbols: File in wrong format caffe2,caffe2
[feature request] Support for 0-length sequences in packed_sequences module: nn module: rnn triaged,module: nn module: rnn triaged
Remove BatchNorm layers once the training is completed. feature module: nn triaged module: norms and normalization,feature module: nn triaged module: norms and normalization
The state of sparse Tensors module: sparse triaged,module: sparse triaged
WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode. CRITICAL:root:Cannot load caffe2.python. Error: libcaffe2.so: cannot open shared object file: No such file or directory caffe2,caffe2
[feature request] More options for Fractional Max Pooling triaged module: pooling,triaged module: pooling
batch_sampler/test_worker_seed intermittently fails with address already in use on OS X todo module: serialization triaged module: flaky-tests,todo module: serialization triaged module: flaky-tests
Incorrect term in _LRScheduler. todo module: optimizer triaged,todo module: optimizer triaged
[feature request] Rename `Subset` -> `Resample` to reflect wider use todo module: dataloader triaged,todo module: dataloader triaged
WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode. WARNING:root:Debug message: libcurand.so.9.0: cannot open shared object file: No such file or directory Segmentation fault (core dumped) caffe2,caffe2
Windows CPU version much slower than Unix versions module: windows triaged,module: windows triaged
[doc] many losses still mention size_average in formula module: docs module: nn module: loss triaged module: deprecation,module: docs module: nn module: loss triaged module: deprecation
/usr/bin/ld: cannot find -lpthreads module: build triaged has workaround,module: build triaged has workaround
Accumulate into accreal instead of real for CPU loss functions module: nn module: loss triaged,module: nn module: loss triaged
[gradcheck] warn about the case that mulitple inputs share storage module: autograd triaged,module: autograd triaged
[Caffe2] Cannot load caffe2.python. Error: libcaffe2.so: cannot open shared object file: No such file or directory caffe2,caffe2
[Caffe2] compiling error with gcc-6 caffe2,caffe2
[pytorch] Make dtype second positional argument of tensor factory methods triaged module: numpy module: pybind module: ux,triaged module: numpy module: pybind module: ux
Mismatch in behaviour of WeightedRandomSampler and other samplers todo module: dataloader triaged,todo module: dataloader triaged
[Caffe2] Error running net train when running resnet50 caffe2,caffe2
[feature request] freeze() for nn.Module module: nn low priority triaged enhancement,module: nn low priority triaged enhancement
Lint check for non-Unicode characters in diffs / Unicode characters without coding module: lint triaged better-engineering,module: lint triaged better-engineering
cmake error caffe2,caffe2
[Caffe2] segmentation fault caffe2,caffe2
Check failed: error == cudaSuccess unspecified launch failure [caffe2] caffe2,caffe2
Multiprocessing Self Test Error todo module: multiprocessing triaged,todo module: multiprocessing triaged
Issues with dynamically created grad_fn for views module: autograd triaged module: viewing and reshaping,module: autograd triaged module: viewing and reshaping
Use target_compile_options to set warning flags module: build triaged,module: build triaged
Todo functions and autograd supports for Sparse Tensor module: sparse triaged,module: sparse triaged
Inconsistency in implementation of _LRScheduler  module: nn module: optimizer triaged needs research,module: nn module: optimizer triaged needs research
Storages still use legacy printing module: printing triaged,module: printing triaged
Cannot allocate memory Error from operator caffe2,caffe2
Come with a better strategy for TensorArg (error reporting) module: performance module: internals triaged,module: performance module: internals triaged
Update tests to no longer spew debug info module: tests triaged better-engineering,module: tests triaged better-engineering
cleanup BLAS detection module: build triaged module: linear algebra,module: build triaged module: linear algebra
[JIT] Interleaved C++-Python execution loses inner Python stacks oncall: jit,oncall: jit
[Caffe2] Runtime error while using a pre-trained style_transfer model caffe2,caffe2
An error occurred while creating a new notebook.  caffe2,caffe2
Use CMAKE_<LANG>_COMPILER_LAUNCHER module: build triaged,module: build triaged
batchnorm2d  track_running_stats module: serialization triaged,module: serialization triaged
Properly release NCCL resources triaged module: nccl,triaged module: nccl
[Feature request] Add torch.multiprocessing.Pipe module: bootcamp module: multiprocessing triaged enhancement small,module: bootcamp module: multiprocessing triaged enhancement small
[Installation]: Support conda/pip install with ppc64le(power8) module: binaries triaged enhancement module: POWER,module: binaries triaged enhancement module: POWER
Different behavior of LSTM and LSTMCell implementation  module: numerical-stability module: rnn triaged module: numerical-reproducibility,module: numerical-stability module: rnn triaged module: numerical-reproducibility
error when import caffe2.python.onnx.backend caffe2,caffe2
[Caffe2] Successive in-place operators cause RuntimeError of gradient operator versions caffe2,caffe2
GRU is implementation of GRU v1 draft rather than final GRU paper algo module: docs module: rnn triaged,module: docs module: rnn triaged
PyTorch multiprocessing using single CPU core todo module: multiprocessing triaged,todo module: multiprocessing triaged
Export CC is ignored when I build pytorch module: build triaged,module: build triaged
load_state_dict unexpectedly does not load Tensor to buffers that currently have None value module: bc-breaking module: docs module: nn triaged,module: bc-breaking module: docs module: nn triaged
 Please provide wheel package for windows on PyPI module: binaries triaged,module: binaries triaged
manager.cpp:64: undefined reference to `shm_open (when building with GCC 5.x (sic)) module: build triaged,module: build triaged
Document torch.acos() behavior near -1 and 1 module: docs triaged,module: docs triaged
import problem caffe2,caffe2
CrossEntropyLoss mishandles weights module: nn module: loss triaged,module: nn module: loss triaged
[PyTorch] Windows CI CUDA mem leak check on BN tests are flaky module: memory usage triaged module: flaky-tests,module: memory usage triaged module: flaky-tests
[PyTorch] EmbeddingBag comparison vs Embedding fails w/ small max_norm on CUDA triaged module: norms and normalization,triaged module: norms and normalization
Build error : mpi/mpi_gpu_test.cc.o:  undefined reference to symbol '_ZN3MPI8Datatype4FreeEv' caffe2,caffe2
Better error message in DataChannelTCP::_receive triaged module: backend,triaged module: backend
Deprecate torch.Tensor triaged module: deprecation module: tensor creation,triaged module: deprecation module: tensor creation
[feature request] batch_first of RNN hidden weight for Multi GPU training module: nn triaged enhancement module: data parallel,module: nn triaged enhancement module: data parallel
[PyTorch] weight tensor dimension assumption module: nn triaged,module: nn triaged
Feature Request: Logistic Distribution module: distributions feature triaged,module: distributions feature triaged
torch.Tensor.new() disappeared in 0.4 doc module: docs triaged module: deprecation module: tensor creation,module: docs triaged module: deprecation module: tensor creation
LMDB read error for Mnist caffe2,caffe2
OOM Exception when using torch.nn.grad.conv2d_weight (apparently because CuDNN backwards is not used) module: performance module: cudnn module: memory usage module: convolution triaged has workaround,module: performance module: cudnn module: memory usage module: convolution triaged has workaround
Checkpointing is slow on nn.DataParallel models module: performance module: checkpoint triaged,module: performance module: checkpoint triaged
[feature request] Add cudaification API for distributions module: distributions triaged enhancement,module: distributions triaged enhancement
detectron net create error caffe2,caffe2
[Caffe2] Fail to build after upgrading to cuda 9.2 caffe2,caffe2
Conv3D can be optimized for cases when kernel is spatial (probably) module: performance module: convolution triaged,module: performance module: convolution triaged
Inserting a tensor into a python dict causes strange behavior todo module: nn triaged,todo module: nn triaged
Only one thread is used on macOS (super slow on CPU) triaged module: macos module: multithreading,triaged module: macos module: multithreading
"checkpoint(function, *args) should have the same requires_grad as function(*args) module: checkpoint module: autograd triaged",module: checkpoint module: autograd triaged
Inconsistent interactions of PyTorch tensors and NumPy ops triaged module: numpy,triaged module: numpy
[Caffe2] Build broken on macOS High Sierra: can't find sys headers in /usr/local/include. caffe2,caffe2
[Bug] Dilated max-pooling fails due to padding check todo triaged module: pooling,todo triaged module: pooling
[feature request] Global GPU Flag feature module: cuda triaged,feature module: cuda triaged
Make c10d/FileStore cache file descriptor newcomer oncall: distributed feature triaged,newcomer oncall: distributed feature triaged
[feature request] torch.nn.DataParallel should work nicely both for cpu and gpu devices triaged enhancement module: data parallel,triaged enhancement module: data parallel
Multi queue for dataloader when workers > 1 module: dataloader triaged enhancement,module: dataloader triaged enhancement
nn.DataParallel fills None grads with 0 triaged module: data parallel,triaged module: data parallel
DataParallel on list inputs triaged module: data parallel,triaged module: data parallel
"Windows MAGMA binary requires explicit linking against MKL LAPACK, or it will silently give  incorrect results module: build module: cuda triaged",module: build module: cuda triaged
Check for F2C convention (for blas) at runtime module: build triaged,module: build triaged
[memory leak] [PyTorch] .backward(create_graph=True) module: autograd module: memory usage triaged,module: autograd module: memory usage triaged
Issue when importing both retro (from OpenAI) and torch triaged module: pybind,triaged module: pybind
"Optional modifiers (e.g., Tensor?) are not checked for non-dispatched native functions triaged module: dispatch",triaged module: dispatch
[proposal] [discussion] Refactor pruning/weight_norm using new Reparametrization functionality + actually deprecate old impl of SpectralNorm module: nn triaged,module: nn triaged
[Caffe2]Install problem caffe2,caffe2
Do not put system paths in RPATH module: build triaged,module: build triaged
LBFGS contribution  awaiting response (this tag is deprecated) module: optimizer triaged,awaiting response (this tag is deprecated) module: optimizer triaged
CuDNN version not supported todo module: build module: cudnn module: cuda triaged,todo module: build module: cudnn module: cuda triaged
[Caffe2] compilation linker error `libtbb.so.2: undefined reference to std::__exception_ptr` module: crash module: build caffe2,module: crash module: build caffe2
[Compilation] how to disable caffe2?  module: build triaged,module: build triaged
[feature request] norm argument for RNNCells module: rnn triaged enhancement,module: rnn triaged enhancement
[pytorch] Not handling python reload properly todo module: crash triaged,todo module: crash triaged
[Caffe2] cudnn versions compatibility issue. module: cudnn caffe2,module: cudnn caffe2
Autogenerate code example / tutorial outputs in documentation todo module: docs good first issue triaged module: doc infra,todo module: docs good first issue triaged module: doc infra
[Caffe2] CUDNN_STATUS_BAD_PARAM Error with the LRN layer while trying to run the code using CUDA. The training works fine on CPU caffe2,caffe2
"ã€Train issue with caffe2 detectronã€‘Aborted at 1523501813 (unix time) try ""date -d @1523501813"" if you are using GNU date *** PC: @     0x7fa35103733a (unknown) caffe2",caffe2
[Caffe2] Onnx to Caffe2 Blob error caffe2,caffe2
[feature request] dropout1d todo module: nn triaged enhancement,todo module: nn triaged enhancement
Examine contiguity requirements for gradInput triaged,triaged
[utils.bottleneck] List of improvements todo triaged module: bottleneck,todo triaged module: bottleneck
[caffe2] Run resnet50_trainer.py error between 2 machines using GLOO/Redis and ibverbs caffe2,caffe2
worker assignments in torch.utils.dataloader.py module: dataloader triaged,module: dataloader triaged
[feature request] Include libomp support (macOS) module: build triaged module: macos,module: build triaged module: macos
Multithreading Scaling Issue with MKL caffe2,caffe2
"[feature request] adding a nonzero element ""in-place"" in sparse tensor module: sparse triaged",module: sparse triaged
Note about unusual stride situations in dev docs / make it easier to test for this in the library module: docs triaged module: memory format,module: docs triaged module: memory format
Test suite should test implementations module: tests triaged,module: tests triaged
"Well documented, safe method to deserialize model parameters from untrusted sources feature module: pickle module: serialization triaged onnx-triaged topic: security",feature module: pickle module: serialization triaged onnx-triaged topic: security
Repeated 'python setup.py install' with clang leads to -lcpuinfo not found module: build triaged,module: build triaged
"LBFGS always give nan results, why needs reproduction module: numerical-stability module: optimizer triaged",needs reproduction module: numerical-stability module: optimizer triaged
Install doesn't work with spaces in directory todo module: build triaged has workaround,todo module: build triaged has workaround
Conv-RNN combination slow in backward pass module: performance module: nn triaged,module: performance module: nn triaged
"Build Fails on Gentoo with CUDA 9.1, GCC 6.4, Python 3.5 module: build triaged",module: build triaged
Change THCudaCheck to suggest that device-side asserts likely mean that you have out of bound indices module: cuda module: error checking triaged module: assert failure small,module: cuda module: error checking triaged module: assert failure small
Add hookable weights module: nn triaged enhancement,module: nn triaged enhancement
"[feature request] F.interpolate to support integral data types: bool, int8, int32, int16, int64 ||| support uint8 on CUDA todo triaged",todo triaged
TestNN.test_data_parallel takes 10G of memory module: memory usage module: tests triaged,module: memory usage module: tests triaged
"torch.jit.trace(network, data) fails if data is an OrderedDict oncall: jit module: bootcamp days",oncall: jit module: bootcamp days
Gaussian Sampling feature module: nn triaged,feature module: nn triaged
RuntimeError: $ Torch: not enough memory: you tried to allocate 72GB. Buy new RAM! module: memory usage triaged,module: memory usage triaged
scatter_add_ should support scalar source (including Python scalar) triaged module: scatter & gather ops,triaged module: scatter & gather ops
Perf regression: indexing 1-d tensor module: performance in progress triaged,module: performance in progress triaged
Handle python_arg_parser dtype constants better todo feature triaged,todo feature triaged
Bugs: Score Function approach in REINFORCE for PONG todo module: crash module: loss module: cuda module: memory usage triaged,todo module: crash module: loss module: cuda module: memory usage triaged
TestMultiprocessing.test_fd_sharing hangs with ASAN module: tests triaged,module: tests triaged
BatchNorm1d raises RuntimeError (CUDNN_STATUS_BAD_PARAM) on 3D input. module: cudnn triaged,module: cudnn triaged
Unsafe out= keyword argument with tensors sharing storage triaged module: numpy module: safe resize module: correctness (silent),triaged module: numpy module: safe resize module: correctness (silent)
[feature request] Stratified splits in random_split function module: dataloader triaged,module: dataloader triaged
Weird error message in torch.split_size_or_sections triaged module: numpy,triaged module: numpy
ASAN detected leaks on python -c 'import torch' module: memory usage triaged,module: memory usage triaged
TakeBackward taking a significant portion of backward time module: performance module: autograd triaged,module: performance module: autograd triaged
BCELoss - weight parameter shape incorrect module: nn module: loss triaged,module: nn module: loss triaged
Saving model with runtime code changes module: serialization triaged,module: serialization triaged
[feature request]Add an env variable to cover different pathes when testing code with openmp module: tests triaged better-engineering,module: tests triaged better-engineering
NVIDIA_DRIVER_CAPABILITIES env variable is missing in pytorch docker images triaged module: docker,triaged module: docker
Delete obsolete `THCDeviceTensor::downcastOuter` / `THCDeviceTensor::downcastInner` functions module: bootcamp module: cuda triaged small better-engineering,module: bootcamp module: cuda triaged small better-engineering
[Feature Request] Calculating FLOPs for computational graph operations high priority module: performance feature triaged quansight-nack,high priority module: performance feature triaged quansight-nack
MultiGPU hangs Titan Xp in multiprocessing/queue.py module: multiprocessing module: cuda triaged module: macos,module: multiprocessing module: cuda triaged module: macos
Speed up data loading for `TensorDataset` if the underlying dataset supports index by a list of indices module: performance module: dataloader triaged,module: performance module: dataloader triaged
[feature request] Type-1 Multi-layer bidirectional RNN module: cudnn module: rnn triaged function request,module: cudnn module: rnn triaged function request
"Rebuild from no-CUDA to CUDA leads to: error: #error ""Expected GLOO_USE_CUDA to be defined"" module: build low priority triaged has workaround",module: build low priority triaged has workaround
 [Feature Request] clip_grad_norm for sparse gradients module: sparse triaged,module: sparse triaged
Clang color diagnostics don't work with ninja module: build triaged,module: build triaged
Consider disallowing Variables that require grad in NCCL/comm functions module: autograd triaged module: nccl actionable,module: autograd triaged module: nccl actionable
Compilation issue: problem with GPU capability check module: build module: cuda triaged,module: build module: cuda triaged
Very slow on CPU module: performance module: rnn module: cpu triaged,module: performance module: rnn module: cpu triaged
Better header hygiene in ATen module: internals triaged,module: internals triaged
CUDNN_STATUS_INTERNAL_ERROR when training with conv3d module: cudnn module: convolution triaged,module: cudnn module: convolution triaged
Protect user from No module named _C import error module: error checking triaged module: pybind,module: error checking triaged module: pybind
`from` keyword in `random_` gives error module: distributions triaged,module: distributions triaged
Mixed Tensor/TensorList arguments in ATen functions with explicit derivatives module: autograd triaged enhancement,module: autograd triaged enhancement
Met 'cudnnDestroyDropoutDescriptor' while run multiply gpu-based models in multiply processes module: multi-gpu module: cudnn triaged,module: multi-gpu module: cudnn triaged
Carefully audit contiguity requirements of code module: cudnn triaged,module: cudnn triaged
Bind in Python _backward ATen functions module: autograd triaged,module: autograd triaged
Assert that some tests must not be skipped under certain CI configurations high priority module: ci module: tests triaged quansight-nack,high priority module: ci module: tests triaged quansight-nack
DistributedDataParallel doesn't converge well when using MPI oncall: distributed triaged,oncall: distributed triaged
Installation Optimise For Chinese Users Who Behind the Wall module: docs triaged,module: docs triaged
"Cache CuDNN benchmark selection, turn it on by default, use it across PyTorch runs module: cudnn triaged",module: cudnn triaged
Invoking MKL in multiprocessing with importing torch causes blocking module: multiprocessing triaged module: mkldnn module: mkl,module: multiprocessing triaged module: mkldnn module: mkl
torch.cuda.device_count() returns 1 using 4 TitanX setup. needs reproduction module: cuda triaged,needs reproduction module: cuda triaged
Feature request: sparse matrix max(axis) module: sparse triaged enhancement,module: sparse triaged enhancement
single-gpu works but multi-gpu hangs module: cudnn triaged module: data parallel,module: cudnn triaged module: data parallel
Fused RNN refactor plan module: cudnn triaged,module: cudnn triaged
Variable outputs of stochastic functions should never require grad module: distributions triaged,module: distributions triaged
ATen explicitly differentiated native function resolution hazard (call is ambiguous) module: internals triaged,module: internals triaged
[Proposal] Consistent `batch_first` effect for RNN modules module: docs module: nn module: rnn triaged,module: docs module: nn module: rnn triaged
x.grad should be 0 but get NaN after x/0 module: docs module: autograd triaged module: NaNs and Infs has workaround needs design,module: docs module: autograd triaged module: NaNs and Infs has workaround needs design
Use the int64 version of MKL calls module: internals triaged module: mkl,module: internals triaged module: mkl
Make the generator tools data model more explicit triaged module: codegen,triaged module: codegen
GridSampler behaviours module: cudnn triaged,module: cudnn triaged
Feature request: Correlation module triaged,triaged
Raise an error when using magma built against wrong version of cuda module: binaries module: build triaged,module: binaries module: build triaged
Suppress hidden state output of RNNs? module: memory usage triaged enhancement,module: memory usage triaged enhancement
Sparse matrices in dataloader error module: sparse triaged,module: sparse triaged
Wrap Cephes library for mathematical special functions feature triaged module: numpy,feature triaged module: numpy
"[Feature Request] Implement ""same"" padding for convolution operations? high priority module: nn module: convolution triaged enhancement needs design",high priority module: nn module: convolution triaged enhancement needs design
Considerable slowdown in Adam.step after a number of epochs with multiple losses awaiting response (this tag is deprecated) needs reproduction module: performance module: optimizer triaged,awaiting response (this tag is deprecated) needs reproduction module: performance module: optimizer triaged
Fuse bias to CuDNN convolution triaged enhancement,triaged enhancement
Memory leak when doing backward with grad as yourself module: autograd module: memory usage triaged quansight-nack,module: autograd module: memory usage triaged quansight-nack
Make pytest stop printing docstrings in its default diagnostic output module: tests triaged enhancement,module: tests triaged enhancement
Deprecate inplace argument in torch.nn.functional module: bc-breaking feature module: nn triaged module: deprecation,module: bc-breaking feature module: nn triaged module: deprecation
Exposing CuDNN benchmark strategy selection  module: cudnn module: bootcamp feature triaged,module: cudnn module: bootcamp feature triaged
Proposal: combine requires_grad and retain_grad() module: autograd triaged,module: autograd triaged
Multiprocessing with torch.solve hangs module: multiprocessing triaged module: linear algebra,module: multiprocessing triaged module: linear algebra
making .cuda() falls back to an identity function when gpu is not available module: cuda triaged,module: cuda triaged
Feature Request: Distributed send arbitrary objects oncall: distributed feature module: pickle module: serialization triaged,oncall: distributed feature module: pickle module: serialization triaged
improve performance of common CPU clone / contiguous calls with HPTT module: cpu triaged,module: cpu triaged
CUDA topk is slow for some input sizes module: performance module: cuda triaged module: sorting and selection,module: performance module: cuda triaged module: sorting and selection
High CPU use by clock_gettime syscall module: performance module: cuda triaged,module: performance module: cuda triaged
type of torch.bernoulli and torch.multinomial inconsistent module: distributions triaged,module: distributions triaged
Data sampling seems to be more complicated than necessary module: dataloader triaged,module: dataloader triaged
"DataLoader ""casting"" non statndard objects to lists module: dataloader triaged",module: dataloader triaged
Add safety checks in `index_add`/`scatter_add` triaged,triaged
Sparse tensor .new(size) can be confusing module: sparse triaged,module: sparse triaged
BN slows down double-backprop enormously module: performance triaged,module: performance triaged
Autograd profiler should omit CUDA time columns on CPU profiler triaged oncall: profiler,triaged oncall: profiler
Add the new lr_scheduler which called poly module: optimizer triaged,module: optimizer triaged
Learning rate scheduler have different APIs module: optimizer triaged,module: optimizer triaged
Proposal: simplify overloaded Tensor function signatures triaged,triaged
support grid_sample with batch=1 but supprting batch affine parameters triaged module: vision module: interpolation,triaged module: vision module: interpolation
DataLoader gets stuck after model initialization module: dataloader triaged,module: dataloader triaged
Have ppc64le docker images?  triaged module: POWER,triaged module: POWER
Feature Request: Support grad of grad in fused RNNs module: double backwards feature module: autograd module: nn triaged,module: double backwards feature module: autograd module: nn triaged
BatchNorm{1-2-3}d are redundant module: nn triaged,module: nn triaged
CUDA multinomial is limited to 2^24 categories high priority module: distributions module: cuda triaged module: 64-bit function request,high priority module: distributions module: cuda triaged module: 64-bit function request
ImportError: dlopen: cannot load any more object with static TLS module: crash module: build triaged module: assert failure has workaround,module: crash module: build triaged module: assert failure has workaround
Counter-intuitive Patience & Cooldown of ReduceLROnPlateau todo module: optimizer triaged,todo module: optimizer triaged
"DataLoader converts cuda FloatTensor into cpu DoubleTensor when shape is (n,) needs reproduction module: dataloader triaged",needs reproduction module: dataloader triaged
detach_() variant that affects all past uses too feature module: autograd triaged,feature module: autograd triaged
"""Shared memory manager connection has timed out"" needs reproduction module: multiprocessing triaged",needs reproduction module: multiprocessing triaged
Autograd test failure on ppc64le triaged module: POWER,triaged module: POWER
Hard-negative mining using __getitem__ directive in Dataset class module: dataloader triaged,module: dataloader triaged
DataParallel is not compatible with pack_padded_sequence awaiting response (this tag is deprecated) triaged module: data parallel,awaiting response (this tag is deprecated) triaged module: data parallel
Discrepancy in BCEWithLogitsLoss and ClassNLLLoss module: loss module: cuda module: cpu triaged,module: loss module: cuda module: cpu triaged
[Feature request] truncated normal initializer(sampler) triaged enhancement module: initialization,triaged enhancement module: initialization
Naming inconsistencies module: nn triaged enhancement module: ux,module: nn triaged enhancement module: ux
Feature Request: ReLU on LSTMs and GRUs feature module: nn module: rnn triaged,feature module: nn module: rnn triaged
[feature request] time-distributed layers for application of normal layers to sequence data module: nn triaged,module: nn triaged
Factorized Output Layer todo feature triaged,todo feature triaged
Feature request: reverse_padded_sequence module: rnn triaged enhancement actionable,module: rnn triaged enhancement actionable
Add support for colors (and maybe other attributes) to NVTX API newcomer module: cuda triaged,newcomer module: cuda triaged
In-place bernoulli_ has more functionality than torch.bernoulli with output parameter module: distributions triaged,module: distributions triaged
Expose optimizer options as attributes when there's a single param group module: bootcamp module: optimizer triaged enhancement,module: bootcamp module: optimizer triaged enhancement
Pad PackedSequences to original batch length hackamonth triaged module: nestedtensor,hackamonth triaged module: nestedtensor
[feature request] Support tensors of different sizes as batch elements in DataLoader feature module: dataloader triaged module: nestedtensor,feature module: dataloader triaged module: nestedtensor
Cannot find Intel MKL module: dependency bug module: build triaged module: mkl,module: dependency bug module: build triaged module: mkl
avg_pool functions hold input for backward feature triaged module: pooling,feature triaged module: pooling
"Change sparse_mask to take indexing mask, rather than entire sparse tensor module: sparse feature triaged",module: sparse feature triaged
BatchNorm should use Bessel's correction consistently module: nn triaged module: norms and normalization,module: nn triaged module: norms and normalization
"""Sparsified"" mathematical operations module: sparse low priority triaged",module: sparse low priority triaged
Unhelpful CrossEntropyLoss dimension error message module: loss module: cuda module: error checking triaged,module: loss module: cuda module: error checking triaged
Dice Loss PR module: loss triaged enhancement Stale,module: loss triaged enhancement Stale
Batched sparse QR factorizations and solves with cusolver module: sparse feature triaged module: linear algebra Stale,module: sparse feature triaged module: linear algebra Stale
dataloader parallels over elements vs over batches todo feature module: dataloader triaged,todo feature module: dataloader triaged
BCELoss doesn't accept LongTensor targets feature module: loss triaged Stale,feature module: loss triaged Stale
Feature Request: NegativeSampling and HierarchicalSoftmax loss functions feature module: nn module: loss triaged Stale module: primTorch,feature module: nn module: loss triaged Stale module: primTorch
Keyword arguments passed to module's __call__ aren't forwarded to the hooks module: nn low priority triaged enhancement,module: nn low priority triaged enhancement
expose backend selection and cudnn settings to the end user module: cudnn feature triaged Stale,module: cudnn feature triaged Stale
