Issue Title,Issue Body,Is Bug,BERT Embedding
TF 2.0: tf.stack can cause a segmentation fault TF 2.0,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- Mobile device: N/A
- TensorFlow installed from: binary
- TensorFlow version: 2.0.0-alpha0, 2.0.0-dev20190319
- Python version: 3.6.7
- Bazel version: N/A
- GCC/Compiler version: N/A
- CUDA/cuDNN version: 10.1 / 7.4.2
- GPU model and memory: GeForce GTX 1080 Ti (11 GB)

**Describe the current behavior**
The Python interpreter crashes with SIGSEGV (Segmentation Fault); according to gdb the fault occurs in ```EagerTensor_CheckExact(_object const*) ()```.

**Describe the expected behavior**
No segmentation fault.
Ideally a stacked tensor returned (I was adapting code I developed interactively in eager execution mode, where it worked, in a Jupyter notebook for addition to a Keras based model), or an error that the argument cannot be a tensor (this is the TF1 behavior):
```
TypeError: Expected list for 'values' argument to 'pack' Op, not <tf.Tensor 'input_1:0' shape=(?, 128, 128, 1) dtype=float32>.
``` 

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.keras.layers import Input

print(tf.__version__)

input_ = Input((128, 128, 1), dtype='float32')
print(input_)
output = tf.stack(input_, axis=1)
```",True,"[-5.06160975e-01 -5.97922683e-01 -2.82181025e-01 -2.10945845e-01
 -7.49241635e-02 -4.06392425e-01 -1.05549656e-01  1.71502322e-01
 -5.84677160e-02 -7.85584152e-02 -6.67011365e-02  2.06091255e-03
 -8.18508714e-02  1.12074524e-01 -4.17627364e-01  3.14364314e-01
 -3.17935757e-02 -2.78715909e-01  3.46832097e-01 -1.05590180e-01
 -3.14215451e-01 -3.55492607e-02 -2.13890895e-01  3.33823055e-01
  1.17650934e-01 -1.08410488e-03 -9.58038867e-02  8.07498023e-03
 -1.53594375e-01 -2.13923436e-02  1.25903457e-01  4.34263945e-01
 -7.30868131e-02  5.78355044e-02 -1.01313606e-01  2.03918785e-01
 -2.88009167e-01 -1.62753463e-01 -2.54471540e-01 -2.14132279e-01
  1.30661316e-02 -1.66407913e-01  1.10457502e-01 -1.38511986e-01
  8.34728107e-02  1.50366290e-03  1.39352381e-01  1.78162739e-01
 -1.50546283e-01 -6.23299628e-02  9.36636254e-02  1.12630188e-01
 -1.51632562e-01 -3.39178026e-01  1.53858483e-01  4.77522612e-02
  2.15397358e-01  1.19927183e-01  1.33664757e-01  3.91958624e-01
  1.61588401e-01 -2.23540261e-01  8.03441778e-02  1.45859569e-01
  2.03983203e-01  3.68373871e-01  2.87492275e-01 -3.74664307e-01
  1.51197508e-01 -1.56358868e-01 -3.85744460e-02 -7.01142475e-02
 -9.04963613e-02 -4.62073162e-02  1.38972312e-01 -9.50731561e-02
 -3.96191776e-02  1.61877111e-01  1.47292361e-01 -1.43459037e-01
  9.77525115e-02  5.83115257e-02  1.10253006e-01 -2.90603101e-01
 -3.06867585e-02 -1.02987833e-01  3.49628210e-01  5.93731701e-02
  2.10855395e-01 -1.44912988e-01  3.99554491e-01  1.68895930e-01
  1.78217404e-02  1.65957376e-01  2.38547653e-01  1.43029541e-03
  1.11989677e-03 -7.55354390e-02 -5.82208484e-03  1.40486538e-01
 -8.12542737e-02 -1.28760606e-01 -9.44383144e-02  1.73096597e-01
 -3.87584642e-02 -4.57279570e-02  2.86425054e-01 -1.06965907e-01
 -1.37411594e-01  1.23800449e-01  9.72146690e-02  1.79149091e-01
  1.17739275e-01 -1.78152800e-01  4.13460881e-02 -4.55644913e-02
 -1.06209867e-01  7.10402876e-02  2.05489397e-01  2.90036678e-01
 -2.38032818e-01 -3.08837533e-01 -6.18730895e-02  5.31799123e-02
  3.80683899e-01  5.02231419e-02 -2.18705565e-01  1.24426372e-01
  8.47717524e-02 -7.39166886e-02  9.69230682e-02  1.40735358e-01
 -1.20836169e-01 -1.07801989e-01 -6.95941523e-02  2.55290240e-01
 -2.39580631e-01 -1.97762877e-01 -2.04857081e-01 -2.62338519e-01
  2.10305825e-02  2.63199508e-02  1.64393023e-01 -2.89327979e-01
  3.50775421e-02  3.70016024e-02 -2.44487703e-01  1.63387075e-01
 -1.71945453e-01  1.68561757e-01  6.72365874e-02 -1.31047992e-02
  1.45765722e-01  3.68649423e-01  3.94101962e-02  8.97608846e-02
  3.87064457e-01 -1.07448414e-01  3.01116779e-02 -3.68578494e-01
 -3.40793394e-02  1.41517848e-01 -1.84565708e-02 -6.68459013e-03
  8.93895179e-02  1.52854905e-01 -3.01409006e-01 -2.58734405e-01
  4.43681777e-02  1.58210486e-01 -2.04626620e-01 -9.24280435e-02
 -1.03513636e-01  2.13865981e-01  2.73491770e-01 -1.88750833e-01
  1.72852457e-01 -3.30203116e-01  5.50716463e-03  1.34570569e-01
 -4.16997001e-02 -1.40424073e-01  1.37757540e-01  1.24064162e-02
 -8.13868940e-02 -1.85624138e-02  3.44217777e-01  1.62971571e-01
 -8.40828121e-02 -1.23975664e-01 -5.24683237e-01 -8.27408284e-02
  5.82950786e-02 -1.42613739e-01 -1.76278979e-01 -8.70683044e-02
  2.98409939e-01  7.74265155e-02  1.23226494e-01  1.54714122e-01
 -5.63880391e-02  2.49699354e-02 -1.29174981e-02  1.58264548e-01
 -6.95553496e-02 -1.33385062e-01 -2.56599277e-01 -3.50189693e-02
 -1.26206219e-01  4.83532175e-02  2.86594987e-01 -4.21441317e-01
 -1.88145280e-01 -5.94766140e-02 -2.67463565e-01 -1.34090737e-01
 -9.97683257e-02  7.28294253e-06 -2.39960268e-01  3.31353545e-01
  2.41091073e-01  9.42515954e-03  1.91210449e-01 -3.48993152e-01
 -6.42739385e-02 -5.57326106e-03 -2.17732877e-01  1.19102143e-01
 -8.53491127e-02  1.09715328e-01 -2.51352414e-02  1.30166739e-01
 -1.01134600e-02  1.26183346e-01  6.83103800e-02 -1.51198348e-02
 -1.30946666e-01 -2.41028070e-01  1.95061982e-01 -4.61638439e-04
 -3.05438936e-01 -9.56321582e-02 -1.19650669e-01 -1.10991634e-01
  1.88984722e-01  1.33396834e-01 -9.19766873e-02  9.93125215e-02
 -6.98019266e-02  5.25309443e-01 -1.65409952e-01  3.32867503e-02
  1.96514755e-01  9.22256634e-02  8.68009627e-02  5.25931343e-02
  7.32500553e-02 -3.84055451e-03  3.29265177e-01 -5.04484028e-02
  1.81999698e-01  1.28411323e-01  2.58379094e-02  4.31217283e-01
  2.96073169e-01  4.87905592e-01 -2.46306285e-02  3.04444820e-01
 -1.70217276e-01 -1.29419386e-01 -4.23471481e-02  3.73923220e-02
  3.83930475e-01 -3.83548319e-01  8.21064711e-02 -1.71088010e-01
  6.08996511e-01 -9.67879072e-02  3.62083763e-02  4.69538718e-01
  1.65294111e-01  2.17640132e-01  1.79985985e-02  7.15922713e-02
  1.08635783e-01 -2.95588285e-01 -3.79879661e-02 -4.93565977e-01
 -9.34387594e-02 -4.75439988e-02 -1.01746656e-01 -1.63718909e-02
  1.90659210e-01  8.76548141e-02 -1.02770813e-01  1.94210708e-02
 -3.84485275e-02 -5.25334068e-02  1.81487389e-02 -2.05510914e-01
 -1.59290731e-01 -5.16066439e-02 -1.30562615e-02 -5.91541864e-02
 -1.20544881e-02  5.54284081e-03  3.52778018e-01  3.24096113e-01
  2.11508423e-01 -2.51203507e-01  5.20502806e-01 -9.27783996e-02
 -8.29336494e-02  2.12850213e-01 -7.70935416e-02  1.16035670e-01
 -3.32163870e-01  3.39750409e-01  1.72168285e-01 -1.58788189e-01
  6.85689375e-02 -3.74383509e-01 -3.56262714e-01  1.30959719e-01
  1.13029659e-01 -1.34547472e-01  2.23883055e-02 -7.36033842e-02
  3.40325851e-03  1.24084562e-01 -1.28803790e-01 -1.90005004e-01
 -3.19906443e-01  8.68619755e-02 -7.63699114e-02  1.03250556e-01
 -4.09262717e-01  1.30904138e-01 -6.73225895e-02 -8.59678686e-02
 -2.15597264e-02 -2.69726783e-01 -1.27337009e-01 -1.71453923e-01
 -4.18453626e-02 -3.27603891e-02  1.49258733e-01  4.13110107e-01
  2.51845062e-01 -6.53550923e-02 -4.33148220e-02 -9.32661165e-03
 -2.55734801e-01 -5.70956543e-02 -6.47857264e-02  2.69673884e-01
 -3.20666492e-01 -6.10786080e-02  3.35901886e-01  2.23980606e-01
 -1.47990081e-02  8.28812458e-03 -6.51214123e-02  1.10144302e-01
  5.29325232e-02 -1.53499618e-01 -2.86971450e-01  1.11581482e-01
  1.87132373e-01  2.70243287e-01 -2.08023656e-02  3.45669806e-01
 -1.36127248e-01  1.57913595e-01  5.27141094e-01 -3.15202624e-01
  5.44266850e-02 -9.01317745e-02 -4.37917262e-02 -1.75857067e-01
 -1.25243813e-01 -5.52065298e-02  6.54345378e-02 -5.68452328e-02]"
[2.0] tf.numpy_function logs deprecation warning type:docs-bug TF 2.0,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: `2.0.0.dev20190311`
- Python version: 3.6.6
- CUDA/cuDNN version: 10.0

**Describe the current behavior**

When using `tf.numpy_function`, a warning is logged about `tf.py_func` being deprecated.

**Describe the expected behavior**

As a V2 symbol, `tf.numpy_function` should not produce a deprecation warning.

**Code to reproduce the issue**

```python
import tensorflow as tf
tf.numpy_function(lambda x: x, [tf.zeros([5])], [tf.float32])
```

```text
W0315 11:05:55.860109 139695358637824 deprecation.py:323] From /home/klein/dev/OpenNMT-tf/envv2/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py:476: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
```",True,"[-4.62258488e-01 -5.94015241e-01 -1.23574078e-01 -3.64383385e-02
  3.72785568e-01 -3.23685646e-01  1.83050096e-01  1.14950791e-01
 -2.94703484e-01  2.99653597e-02 -1.31186336e-01 -1.31246120e-01
 -8.50775391e-02  3.13958585e-01 -2.92921424e-01  1.96157575e-01
 -3.68367396e-02 -3.20696980e-01  2.04472870e-01 -1.51743114e-01
 -1.33242771e-01  1.45572461e-02 -2.36437261e-01  4.85897213e-01
  7.93284625e-02  1.58785313e-01 -1.93166494e-01 -1.66150346e-01
 -7.57038593e-02  2.15007350e-01  2.05693632e-01  2.22223073e-01
 -2.36053690e-01 -1.09672800e-01  1.34402186e-01  2.44198024e-01
 -3.13411430e-02 -1.31671742e-01 -2.77891129e-01 -2.89281942e-02
  3.88774015e-02 -2.57798016e-01 -6.25688536e-03 -1.07029742e-02
 -1.46753173e-02 -1.27196282e-01 -3.24570388e-02 -2.17695124e-02
 -2.25526944e-01  7.02788122e-04  1.65353775e-01  2.01022774e-01
 -2.03217983e-01 -3.48612428e-01  1.99705243e-01  7.53655285e-02
  8.50229338e-02  1.78764597e-01 -4.77062166e-02  2.83874780e-01
  2.67884791e-01 -1.07547507e-01  3.55462357e-02  2.39153594e-01
  3.27196002e-01  3.55464458e-01  1.31277084e-01 -2.34405711e-01
  3.30768347e-01 -1.30522940e-02 -1.05772600e-01 -1.02387272e-01
 -1.15564652e-01 -2.14954078e-01  1.06442265e-01  4.30029184e-02
  4.93764952e-02  2.63105482e-01  1.57028034e-01 -1.65969104e-01
 -3.62978168e-02 -3.50707024e-03  2.36726984e-01 -2.24987864e-01
 -2.25943178e-01 -6.05193526e-02  2.57038713e-01 -1.01246625e-01
  4.57470119e-01 -2.97531724e-01  4.75196689e-01 -2.57221088e-02
 -1.54598802e-01  9.40254852e-02  3.07040840e-01 -1.09225571e-01
 -1.84370846e-01  2.44266823e-01 -9.99925733e-02  7.83107430e-02
 -3.99562065e-04 -1.47305667e-01 -5.08099794e-02  8.60983059e-02
  2.03128353e-01 -8.78601372e-02  1.69707656e-01 -1.51944518e-01
  2.34071799e-02  1.23274410e-02  1.83976144e-01 -6.25681579e-02
  2.50096768e-01  6.32934943e-02  5.97525686e-02  1.65721148e-01
 -2.36369520e-02  1.79535061e-01  1.43665031e-01  6.96767569e-01
 -1.09859668e-02 -2.10209176e-01  2.51255892e-02  1.14399968e-02
  1.90693676e-01  9.39790606e-02 -1.60471618e-01  1.54121071e-02
  1.39680415e-01  3.11841667e-02  1.07075281e-01  4.49579358e-02
 -2.18927085e-01  1.84416562e-01 -8.93073976e-02  6.23267293e-02
 -1.56732529e-01 -1.69271287e-02 -1.42129421e-01 -6.04145676e-02
 -1.82913780e-01  4.62654233e-02  3.64210419e-02 -2.69323111e-01
 -9.74404141e-02  9.06482786e-02 -5.61879463e-02  3.37855816e-01
 -5.97275421e-02  2.18668371e-01 -3.96900959e-02  6.04041927e-02
  1.65194869e-01  3.84265989e-01  5.09316009e-03  7.30985627e-02
  3.65404248e-01 -2.06693828e-01 -5.44043118e-03 -4.13074642e-01
  6.05698004e-02  3.31137598e-01 -1.00990780e-01 -2.55851686e-01
  1.19281463e-01  3.20148021e-01 -4.49811935e-01 -2.05532447e-01
 -1.24486089e-01  9.87307578e-02  3.97687256e-02 -4.92919758e-02
 -7.55338967e-02  1.54312938e-01  3.67686331e-01 -2.17562824e-01
  4.55361158e-01 -6.73937678e-01  6.48238510e-02  5.10116741e-02
 -1.59171849e-01  2.88759917e-03  8.41254890e-02 -1.94039885e-02
 -1.17788687e-01 -3.33859250e-02  2.68885009e-02  2.30949093e-02
  1.45123675e-01 -1.71635509e-01 -3.00726533e-01 -3.27670515e-01
  2.41265774e-01 -1.09291911e-01  1.58146452e-02  1.04574680e-01
  2.00486079e-01 -2.68561523e-02  1.04354545e-01  3.90512943e-01
 -1.99446902e-01 -4.85298559e-02  1.03264399e-01  5.02150804e-02
  2.66521633e-01 -1.44083858e-01 -1.64201885e-01 -3.42937291e-01
 -8.94099325e-02 -1.84287697e-01  5.19976802e-02 -2.32385978e-01
 -1.24145389e-01 -1.95524395e-01 -3.90002251e-01  1.01951234e-01
 -1.81683987e-01  6.06229492e-02 -1.19064964e-01  3.20840895e-01
  2.39044338e-01  2.68421862e-02 -9.75752920e-02 -3.94192606e-01
 -1.88600451e-01  1.40434355e-01 -3.82463962e-01 -3.64433751e-02
 -2.49685749e-01  2.35322848e-01  5.41292056e-02  7.17831776e-02
  1.62891343e-01  8.66215378e-02  2.21729055e-02 -1.05495468e-01
 -2.01764569e-01 -1.79000705e-01 -8.49501193e-02  5.92192002e-02
 -4.45062608e-01 -3.54238376e-02 -1.90904617e-01 -1.79477319e-01
  2.56530821e-01  2.43167132e-01 -1.97514370e-01 -2.78937705e-02
 -2.42016464e-01  4.82263237e-01  2.31051259e-02 -6.53233305e-02
  1.20566867e-01 -1.05706789e-02  2.14962795e-01  2.75215030e-01
 -3.23586762e-02  7.16687068e-02  2.02130690e-01  1.11173037e-02
  2.34345347e-01  2.57467151e-01 -1.68724842e-02  4.00947094e-01
  2.75970638e-01  4.27373648e-01 -1.21030584e-03  2.28177145e-01
 -2.20210180e-01 -7.81793147e-02 -9.61161107e-02 -3.04374635e-01
  3.25248539e-01 -2.40519345e-01  1.17653832e-01 -4.22272146e-01
  5.31991005e-01 -1.40423387e-01 -1.70964569e-01  2.08872110e-01
  5.88384382e-02  2.69830942e-01 -1.51989937e-01  1.07361883e-01
  7.49248788e-02  5.47593795e-02 -2.12315172e-01 -2.88867056e-01
 -2.74736911e-01 -5.87522909e-02 -3.20481271e-01  5.68649881e-02
  2.28540912e-01 -6.24883398e-02 -4.48753908e-02 -8.24429188e-03
 -1.78537145e-01 -3.47741768e-02  8.59972686e-02  3.45814005e-02
 -1.81055278e-01 -2.04789549e-01  1.44612610e-01  4.01164591e-02
 -1.79370027e-03  1.16498880e-02  3.27941954e-01  2.04484135e-01
  2.85782456e-01 -2.75688142e-01  3.55175108e-01  4.31480594e-02
  3.46212834e-03  2.94349104e-01 -4.84022498e-03  2.96930432e-01
 -4.68679547e-01  4.69202489e-01  1.91388011e-01 -1.61837190e-01
 -3.72158363e-02 -4.74038720e-01 -4.06961977e-01  1.61885247e-02
  2.71710813e-01  5.29899299e-02 -5.35952076e-02 -5.07494509e-02
 -1.89060375e-01 -2.79944334e-02 -2.36390695e-01 -2.31752157e-01
 -3.50798815e-01 -1.60217118e-02 -1.75038159e-01  8.45162477e-03
 -2.75916845e-01  1.62873507e-01  5.41867875e-02 -2.50215918e-01
 -9.88210589e-02 -1.90375939e-01 -1.79318726e-01 -2.68912256e-01
 -1.34959772e-01 -2.91556537e-01  4.07051474e-01  5.47851145e-01
 -1.69138238e-03  3.09444796e-02  1.42381698e-01  1.19700611e-01
 -2.48734236e-01 -5.29094040e-03 -1.27628073e-01  3.48747671e-01
  7.02429563e-02  8.27573799e-03  1.57816559e-01  4.77363735e-01
 -1.38060823e-01  1.95859559e-02 -1.50650531e-01  1.44101279e-02
  8.57648104e-02 -2.63695985e-01 -2.62526989e-01  7.62786195e-02
  3.18852782e-01  2.28063464e-01 -1.03498712e-01  1.95274398e-01
 -1.78162634e-01  4.61520553e-01  3.90702844e-01 -3.93315524e-01
 -1.23521537e-01 -2.57317331e-02  1.45407408e-01 -1.27640083e-01
 -8.28206390e-02  2.22077239e-02  2.12852716e-01 -1.83906667e-02]"
Compilation error of TensorFlow Lite on Windows ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): I don't use python since building TensorFlow Lite
- Python version: See above
- Bazel version (if compiling from source): See above
- GCC/Compiler version (if compiling from source): gcc (Rev2, Built by MSYS2 project) 8.3.0
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**

Compilation error

```
tensorflow/lite/experimental/c/c_api.cc:80:29: error: function 'void TFL_InterpreterOptionsSetErrorReporter(TFL_InterpreterOptions*, void (*)(void*, const char*, va_list), void*)' definition is marked dllimport
 TFL_CAPI_EXPORT extern void TFL_InterpreterOptionsSetErrorReporter(
                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
```

**Describe the expected behavior**

Success to compile

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

See tensorflow/lite/experimental/c/c_api.cc This is broken C syntax when TFL_CAPI_EXPORT is `__declspec(dllimport)`

This is small step to way to reproduce on Windows.

foo.cc
```
__declspec(dllimport) extern void foo() {
}
```
```
$ g++ foo.cc
foo.cc:1:35: error: function 'void foo()' definition is marked dllimport
 __declspec(dllimport) extern void foo() {
```

**Other info / logs**

See above

I created pull-request https://github.com/tensorflow/tensorflow/pull/26615
",True,"[-2.71045864e-01 -5.82778394e-01 -1.34851933e-01  1.01781674e-01
  9.78149101e-02 -1.87937438e-01 -3.29809755e-01 -1.84891954e-01
 -4.32697982e-02 -2.56759435e-01 -4.32283618e-02  2.09411591e-01
 -3.76867242e-02 -2.10677553e-02 -2.49486901e-02  2.30752736e-01
  5.86902276e-02 -2.18938272e-02  7.53435940e-02 -2.00267211e-02
  1.42621636e-01 -7.71767944e-02  5.17898519e-03  2.09989399e-01
  2.64575392e-01  8.71469378e-02 -2.54279166e-01 -2.11971611e-01
  1.89350359e-03  1.65091027e-02  3.75066042e-01 -3.21076438e-02
  1.46257162e-01 -1.05566785e-01  2.13924587e-01  1.25783756e-01
  4.15615588e-02 -2.09222108e-01 -5.43467440e-02 -1.26103252e-01
  1.42495543e-01  4.78197262e-02  1.04076236e-01  9.55194682e-02
 -9.07881111e-02  2.85194069e-02  7.70919677e-03 -1.85537577e-01
 -2.37890050e-01 -1.83459193e-01 -1.00035965e-01 -1.28113553e-01
 -3.95359844e-01 -2.52365053e-01 -8.36137980e-02 -1.66252747e-01
  8.96289945e-02  7.60793835e-02  2.06378073e-01  2.30562896e-01
  1.51396647e-01  3.62985767e-02  1.28406212e-01  1.01808339e-01
  5.85270599e-02 -9.31770355e-03  1.21970244e-01 -1.84563845e-02
  4.52964187e-01 -2.63263583e-01  2.49798074e-01 -1.21612340e-01
 -2.08724171e-01  5.61995916e-02  3.21823210e-02 -4.91524115e-05
 -1.56339526e-01  2.91058004e-01  2.59312868e-01 -2.16701329e-01
  1.31165594e-01 -1.92865044e-01  2.59780645e-01 -6.27926737e-03
  3.40238839e-01 -9.15032029e-02  2.22242892e-01  1.40949309e-01
  2.12727070e-01 -6.74645416e-03  2.90183872e-01 -3.38496380e-02
  1.01680644e-01  1.80340946e-01  4.82222140e-01  2.09018122e-02
 -9.44892317e-02  1.64531380e-01 -1.83251768e-01 -3.05750102e-01
 -1.12014644e-01 -1.52262211e-01  1.67493336e-02  1.61077246e-01
 -3.74265853e-03 -1.20648235e-01  2.69580871e-01  2.11348549e-01
  6.27687126e-02  2.35235132e-02  2.41378471e-01 -1.66648060e-01
  1.56761020e-01 -7.92452395e-02 -1.24726169e-01  5.19273877e-02
 -1.18411414e-01 -6.44156784e-02 -2.55005181e-01  5.23898661e-01
 -9.36834663e-02 -1.81219772e-01  1.08168937e-01  2.17651367e-01
  2.72323012e-01  2.85792369e-02 -4.73233581e-01 -3.93074453e-02
  2.21702866e-02  6.52929544e-02 -4.85585257e-02  1.07045576e-01
  4.75624576e-02  1.39868539e-02  1.92711890e-01 -2.25745976e-01
 -3.00366938e-01 -1.40986949e-01 -8.86787474e-02 -2.36476324e-02
 -1.38351351e-01  2.73933470e-01  5.05019873e-02 -2.69023240e-01
 -5.86514063e-02  4.80951220e-02 -9.14689153e-04  1.79251343e-01
  3.26683521e-02 -2.24209502e-01 -3.33759397e-01  2.09773749e-01
 -3.22129488e-01  5.87720215e-01  1.37851343e-01  5.61234802e-02
  1.33649066e-01 -5.11629209e-02 -8.30769613e-02 -3.03827822e-01
  1.05830923e-01  3.08582962e-01 -3.74753103e-02 -2.55807545e-02
  7.27142096e-02  5.37884608e-02 -5.49352348e-01  4.60858792e-02
 -9.81275216e-02  1.48643762e-01 -6.17263503e-02  8.68242607e-02
  1.75151527e-01  5.10612503e-02  3.90284479e-01  7.81954527e-02
  6.89440727e-01 -3.64032447e-01 -2.27274463e-01  2.60284960e-01
  4.26898181e-01  5.78576326e-02  1.57014981e-01  5.71438521e-02
  8.77958238e-02  1.17892861e-01  9.53918174e-02  2.65339136e-01
 -2.53188670e-01  8.37301463e-02 -2.47138768e-01  4.17379737e-02
  1.63516760e-01 -7.01427534e-02 -2.29524583e-01  2.77941346e-01
  7.91090578e-02  6.66684359e-02 -8.98537505e-03  2.02982426e-02
 -5.44990227e-03 -5.20030558e-02  6.54493719e-02 -1.49855718e-01
  1.73254222e-01 -1.40853703e-01 -3.27944458e-01 -2.99404740e-01
 -4.31870192e-01 -1.14813797e-01 -2.85887830e-02 -2.65906096e-01
 -9.69354138e-02  1.90883845e-01 -3.60842854e-01  2.70209402e-01
  2.48482674e-01  1.34131521e-01  3.58394757e-02  9.76735204e-02
  1.61120519e-02 -8.49855840e-02 -1.74186185e-01 -2.04070389e-01
 -1.88663423e-01 -7.58282244e-02 -1.41030505e-01  2.06203058e-01
 -6.23680241e-02  2.90757626e-01 -2.53898263e-01 -2.69429654e-01
  3.33817422e-01  2.47929692e-02  1.03782840e-01 -1.07858256e-01
 -2.37006649e-01 -1.00546464e-01 -3.11104834e-01  1.75961927e-01
 -2.25712776e-01 -3.64597261e-01  3.65289301e-02 -4.77408059e-02
  3.74739096e-02  1.08987659e-01  6.98374361e-02  9.45106819e-02
 -1.71258867e-01  3.82327080e-01  3.67593691e-02 -2.46375818e-02
  3.78494561e-01  1.99416146e-01  2.98393101e-01  1.46390468e-01
 -1.86659485e-01  4.27956730e-02  6.95410520e-02  1.26686752e-01
  3.01724523e-01  2.69627035e-01 -2.32092023e-01  3.69209677e-01
  1.16485670e-01  1.77365363e-01 -3.79236937e-01 -5.14841788e-02
  5.12661412e-02  2.15068460e-02  1.89801633e-01 -2.40493879e-01
  1.62879437e-01 -3.18666160e-01 -6.15679398e-02 -1.62533775e-01
  3.43257070e-01 -1.90702260e-01 -1.97973341e-01 -7.53820762e-02
  6.31945878e-02  3.17134500e-01 -2.27513403e-01  1.18002392e-01
  2.84559548e-01 -6.13958016e-03  2.38783255e-01 -5.79721093e-01
 -3.23296845e-01  4.72160615e-03 -1.14632249e-01  1.35154165e-02
  8.70270729e-02  3.35282683e-01 -3.12053245e-02  4.02593762e-02
 -1.56024367e-01 -3.66336331e-02  2.53875256e-01  3.25318635e-01
 -5.47440946e-02  7.49934744e-03  5.70786521e-02 -1.52805120e-01
  3.59045379e-02  1.53565884e-01  1.87043995e-02 -7.97439069e-02
  4.24608231e-01 -3.20266426e-01  7.90115595e-02  2.63550341e-01
 -7.14923292e-02  2.75068551e-01 -1.40881669e-02  8.82194936e-02
 -2.70870149e-01  5.43877602e-01 -2.37983912e-02 -1.04814850e-01
 -5.66982813e-02 -7.07108974e-02 -2.83249050e-01 -6.29603118e-02
  6.80094659e-02  8.62969011e-02 -3.68911117e-01 -1.66060850e-01
 -2.86279827e-01 -1.68466717e-02 -7.03856647e-02 -1.18820615e-01
 -3.12778831e-01 -3.90613973e-02 -2.04908535e-01 -1.13795459e-01
 -1.16878033e-01  4.21852797e-01  1.52175762e-02 -3.41104209e-01
 -5.90654090e-03 -4.52915318e-02  2.04063118e-01 -2.88153589e-01
 -1.40159905e-01 -1.04770012e-01  4.50372010e-01  4.04689908e-01
 -1.33692086e-01 -1.39278263e-01 -9.18234885e-02  1.50869146e-01
 -3.66451085e-01 -3.53048742e-03 -1.61645129e-01  1.13329984e-01
 -2.66109645e-01  8.20056126e-02  1.23422846e-01  4.21769917e-01
 -1.94358170e-01 -9.64018703e-02 -2.22695649e-01 -2.40282640e-01
  7.10531250e-02  3.62708420e-03 -3.13139945e-01 -5.93258217e-02
  2.38251299e-01  5.12190223e-01 -9.40987021e-02  9.21867322e-03
 -5.00201225e-01  1.76972538e-01  3.65368336e-01 -1.17433876e-01
 -4.82583456e-02 -8.86057969e-03  2.28469282e-01 -1.02044865e-01
 -1.81444287e-01 -1.00022383e-01  1.29547507e-01 -1.20633706e-01]"
"[Bug report] wrong container setting in OpsTestBase::AddResourceInput, easily fix type:bug comp:ops","File: tensorflow/core/kernels/ops_testutil.h
Class: OpsTestBase
Function: [AddResourceInput](https://github.com/wendy2003888/tensorflow/commit/18b867eabd1bb8f3540ee56a8f47f96d9f3bc20d)

resource container set to empty when using default container.

Due to CLA problem,  I can't contribute rn. 
Please review pull reques [#26428](https://github.com/tensorflow/tensorflow/pull/26428)  from ppwwyyxx. 

Thank you

<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (python3 pip)
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source):  5.4.0
- CUDA/cuDNN version: 10.0 
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",True,"[-3.92854214e-02 -2.66549736e-01 -2.30503470e-01 -8.33172426e-02
  2.34999865e-01 -4.39889789e-01  6.74375147e-03  7.78017640e-02
 -4.63058054e-01 -3.32989067e-01  2.73036599e-01  1.65944010e-01
 -1.70162916e-01  1.93896577e-01 -3.16728354e-01 -4.06949967e-02
  1.95827767e-01 -2.94397712e-01  6.28525689e-02  1.28293872e-01
  4.50861752e-02 -8.12711269e-02 -4.76863459e-02 -8.68194252e-02
  9.88349617e-02  2.88277447e-01 -3.22838366e-01  1.45044848e-01
 -5.28658219e-02  6.41932786e-01  6.63472056e-01  1.16216347e-01
 -3.13161254e-01  1.57917917e-01  4.94689822e-01  1.33862108e-01
 -2.50452340e-01 -1.99229494e-01 -4.92865890e-02 -1.58531711e-01
  2.94498503e-01  1.84110776e-01 -2.86116242e-01  9.15436149e-02
 -1.71627820e-01 -1.49343848e-01 -6.92888647e-02 -3.34291607e-02
  7.37496242e-02 -3.59978080e-01 -7.70228952e-02 -5.90570271e-05
 -2.58010328e-01 -5.02564192e-01  2.70597756e-01 -1.70391083e-01
  8.18738788e-02  5.24417162e-01 -1.88878566e-01  1.27785861e-01
  1.92724884e-01 -4.08361793e-01  2.37264007e-01 -1.23600699e-02
  2.04945475e-01 -4.33595814e-02  5.61818779e-01 -1.19091302e-01
  1.56248227e-01 -2.84293950e-01  6.87388331e-02  1.12823397e-01
 -3.64692569e-01 -2.05677096e-02  2.67812669e-01  1.79128617e-01
 -4.66103435e-01 -3.53044197e-02  1.42698243e-01  4.88886647e-02
 -6.16750121e-02 -1.44848153e-02  9.49522853e-02  1.31109446e-01
 -1.73538908e-01  2.19450682e-01 -3.80907655e-02  1.95455365e-02
  2.78875232e-01 -2.62718529e-01  3.15582484e-01  1.53978229e-01
 -2.51711488e-01 -1.11432746e-06  3.94686490e-01  6.59136772e-02
  1.73490047e-01  1.95942819e-01 -4.79874194e-01 -5.77502251e-02
 -2.09857374e-01 -4.95746732e-01 -2.24541545e-01  7.84575045e-02
 -2.13086486e-01 -1.49430960e-01  1.33802727e-01  2.18202710e-01
  1.41117096e-01  1.63609818e-01  2.40470037e-01  2.07869753e-01
 -1.63416132e-01 -8.35012347e-02 -1.69678569e-01  8.62008631e-02
 -1.47550300e-01  5.58847040e-02 -2.13196278e-01  3.49989474e-01
 -2.04278335e-01 -1.09795183e-01 -1.33706510e-01  9.00920667e-03
  2.53264695e-01 -1.88180506e-01  4.35434617e-02  3.13450396e-01
  4.43919823e-02 -3.21794033e-01  1.03415951e-01 -6.08570948e-02
 -1.39397206e-02 -1.47223413e-01  3.41129929e-01  1.40429005e-01
  1.35336429e-01 -3.06142747e-01 -2.61679471e-01 -4.41398978e-01
 -2.25153267e-01  2.00168520e-01 -1.85205251e-01 -4.35242772e-01
  3.01003940e-02  4.26439047e-01 -1.91921040e-01  1.50237292e-01
 -1.78310037e-01  5.26096001e-02  1.75898507e-01  1.00937476e-02
  1.06430165e-01  6.04099751e-01 -2.49644578e-01  3.25661361e-01
  3.17891061e-01  1.66847818e-02  1.61377519e-01 -2.36010730e-01
 -1.94949046e-01  5.83852947e-01 -1.23973265e-02 -5.29055521e-02
  2.62268662e-01 -2.43571445e-01 -5.68228662e-01 -5.39711833e-01
 -1.67523623e-01  2.49582127e-01 -2.57089794e-01  6.83799610e-02
 -6.26806468e-02 -1.74254745e-01  5.68028316e-02  9.46898013e-03
  3.35580438e-01 -2.67078370e-01  4.69114333e-02  8.00876617e-02
  2.46028006e-01 -1.35037052e-02 -5.38496077e-02  1.95212830e-02
 -3.74669991e-02  5.42032905e-02  2.29268130e-02 -4.31333333e-02
 -2.75960475e-01 -3.23498905e-01 -5.68762302e-01  1.85567945e-01
  3.84844482e-01  3.84164639e-02 -9.58246067e-02 -2.60237813e-01
  6.30562723e-01  4.52777803e-01  3.99951518e-01  7.55800959e-03
 -3.55089866e-02  3.22862864e-01  1.84941128e-01  2.17670023e-01
  3.79364014e-01  6.40931129e-02 -2.61804044e-01 -4.13712859e-01
 -5.69884516e-02 -3.66052091e-02 -5.03365621e-02 -4.63206731e-02
 -1.93681270e-01 -3.30407768e-01 -1.72472775e-01  1.28732249e-01
 -3.76344658e-02  2.25699544e-01  1.00966886e-01 -8.95918012e-02
 -2.40266606e-01 -1.81293249e-01 -2.65531957e-01 -3.68842781e-01
  1.04195729e-01 -4.42750975e-02 -3.62315327e-01  3.41685526e-02
  1.00202732e-01  3.39747012e-01 -3.33386719e-01 -9.24741849e-03
  2.89165288e-01 -1.13215581e-01  5.09350263e-02  8.23392645e-02
  4.97587621e-02  4.66940086e-03 -1.73442990e-01  1.59246981e-01
 -1.20169759e-01 -5.30267656e-01  1.22150570e-01  1.88090175e-01
  3.04986000e-01  9.11981985e-02  1.45374641e-01  2.92369891e-02
 -1.42617643e-01  1.06145151e-01 -9.19846743e-02 -3.91512483e-01
  5.82304671e-02 -4.71030474e-02  2.82362878e-01  4.06516612e-01
  1.37586400e-01 -2.81652175e-02  5.71966916e-02 -2.28901073e-01
  1.45297021e-01  1.74117461e-01  1.83719009e-01  3.94361377e-01
  1.59299046e-01  5.34240752e-02 -3.32220197e-01  3.08147609e-01
 -6.85629621e-02 -3.09079528e-01  6.93745732e-01 -2.82576233e-01
  3.30330193e-01 -2.55367160e-01  8.61523077e-02 -1.97376519e-01
  3.59142661e-01 -3.41118276e-01  7.81939477e-02  4.98730540e-01
  4.09136191e-02  7.72358850e-02 -1.55487001e-01  3.56174186e-02
  5.45947015e-01 -2.63984501e-01 -9.19132382e-02 -9.75444093e-02
 -3.68393362e-01 -9.42332670e-04 -4.48747985e-02 -6.02504387e-02
  9.81987640e-02  1.29780665e-01 -3.16101521e-01  1.60489440e-01
  2.05791295e-01 -1.04324549e-01  2.34378844e-01  3.12968567e-02
 -2.42322415e-01 -1.03616923e-01 -4.75663319e-02 -1.62266403e-01
 -2.30428100e-01  1.06431089e-01 -1.46637976e-01 -1.00750893e-01
  6.48592293e-01 -5.53149819e-01  3.73866379e-01  3.06347579e-01
 -4.00672138e-01  2.59832501e-01 -2.04412952e-01 -2.28517205e-02
 -2.89101243e-01  6.13890767e-01 -2.76679099e-01 -1.40925214e-01
  2.13720351e-01  4.79272492e-02 -3.33289117e-01  2.72559822e-01
  2.10161179e-01 -1.02428719e-01 -5.52927077e-01  2.21865967e-01
 -6.49744123e-02  9.26376954e-02 -9.54688191e-02  6.07934147e-02
 -2.57362545e-01 -1.73869684e-01  9.72184315e-02 -1.86818957e-01
 -2.62845337e-01  2.63341129e-01 -8.05165768e-02 -3.97594243e-01
  3.25468183e-01 -2.53352344e-01  1.12741455e-01 -3.35626423e-01
 -9.52925086e-02 -1.89586520e-01  2.17239812e-01  3.57798547e-01
  2.49052227e-01  2.41050020e-01  2.28510678e-01 -5.13972640e-02
 -1.28176361e-01  1.42208859e-02 -2.90541053e-01  4.37958956e-01
 -2.77536035e-01  1.37303934e-01  9.69122201e-02  4.57502276e-01
 -3.00500631e-01  4.50240169e-03 -2.81574726e-01 -2.80669272e-01
  3.28663997e-02  2.47548163e-01 -6.41577840e-02  2.23775506e-01
  1.08279631e-01  2.88878411e-01 -1.87400073e-01  2.90035065e-02
 -4.26760763e-01 -2.22156756e-03  3.94340575e-01 -5.10982014e-02
  6.66189939e-02 -1.71786070e-01  1.37023419e-01  7.15966225e-02
 -9.98997018e-02  2.42901295e-01  1.60390124e-01  4.00661469e-01]"
Dataset iterator is stalled indefinitely with corrupt TFRecord despite using `ignore_errors` stat:awaiting response type:feature comp:data,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 9/7
- GPU model and memory: n/a

Creating this issue per @mrry's [suggestion](https://github.com/tensorflow/tensorflow/issues/12701#issuecomment-462799443) with minimal code to reproduce the issue.

cc @guillaumekln You're [right](https://github.com/tensorflow/tensorflow/issues/13463#issuecomment-462654013), what I'm seeing may not have to do with `OutofRangeError`, but the execution stalls indefinitely when it encounters corrupt data within a TFRecord, despite using `tf.data.experimental.ignore_errors()`.

#### Example Data (for snippets below)
- [clean.tfrecord](https://drive.google.com/uc?export=view&id=1Y2yV67jAvYhqz8OJLPV7v0BN2w1Feps9)
- [corrupt.tfrecord](https://drive.google.com/uc?export=view&id=1Fh7Vah0i_XaGqdb724NIhUIZpR_t5YiG)

#### With `clean.tfrecord`:
```python3
import tensorflow as tf

filenames = ['/data/clean.tfrecord']
#filenames = ['/data/corrupt.tfrecord']

dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.apply(tf.data.experimental.ignore_errors())
dataset = dataset.batch(64)
dataset = dataset.repeat(1)

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
    i = 0
    while True:
        try:
            print(i, sess.run(next_element).shape)
            i = i + 1
        except tf.errors.OutOfRangeError:
            print(""Dataset complete"")
            break
```

#### Output:
```
0 (64,)
1 (64,)
2 (64,)
3 (64,)
4 (64,)
5 (64,)
6 (64,)
7 (64,)
8 (64,)
9 (64,)
10 (64,)
11 (64,)
12 (64,)
13 (64,)
14 (64,)
15 (64,)
16 (64,)
17 (64,)
18 (64,)
19 (35,)
Dataset complete
```

#### With `corrupt.tfrecord`:
```python3
import tensorflow as tf

#filenames = ['/data/clean.tfrecord']
filenames = ['/data/corrupt.tfrecord']

dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.apply(tf.data.experimental.ignore_errors())
dataset = dataset.batch(64)
dataset = dataset.repeat(1)

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
    i = 0
    while True:
        try:
            print(i, sess.run(next_element).shape)
            i = i + 1
        except tf.errors.OutOfRangeError:
            print(""Dataset complete"")
            break
```

#### Output:
```
0 (64,)
1 (64,)
2 (64,)
3 (64,)
4 (64,)
5 (64,)
6 (64,)
7 (64,)
8 (64,)
9 (64,)
10 (64,)
11 (64,)
12 (64,)
(execution stalled indefinitely here)
```

Not sure why the corrupt files within the TFRecord aren't being ignored with `tf.data.experimental.ignore_errors()`. @mrry any idea?",True,"[-0.23490533 -0.37835258 -0.22542125 -0.4051851   0.28533858 -0.21613511
 -0.1414331   0.08016969 -0.35271528  0.06364688  0.14054976  0.13225989
 -0.02717331 -0.00178379 -0.3897335   0.16457136  0.04908878 -0.25345045
  0.32320923 -0.01115075 -0.13448901 -0.06254053 -0.25586247  0.27094075
  0.008078    0.19236538 -0.06106049 -0.08968427  0.00500776  0.06545693
  0.08268999  0.14736485 -0.01640946  0.21090822 -0.10669643 -0.07450417
 -0.37493467 -0.01605727 -0.15140584 -0.18118325 -0.01041982 -0.19569491
 -0.01519961  0.01524739  0.03780606  0.2540915  -0.03440683  0.18377493
 -0.24303305 -0.0824611   0.14851946  0.13707595 -0.19928205 -0.27903682
 -0.15219949 -0.01038019  0.17326055  0.19540161  0.15630029  0.3754826
  0.26125652 -0.10054551 -0.10823756  0.07237447  0.14566526  0.18797499
  0.23464508 -0.18903057  0.46751714 -0.03734028  0.07431865 -0.07627665
 -0.22847277  0.05385849 -0.04967096  0.20922461 -0.06857324  0.13040316
  0.20906627 -0.08805257  0.01877452  0.04163297  0.11463469  0.01966512
  0.05458987 -0.25011605  0.12143129  0.2544433   0.09707953 -0.14124736
  0.56193686  0.4674161  -0.01696473  0.15541464  0.1921655   0.21599159
 -0.02116925  0.21713026 -0.08757547  0.07213854  0.02466252 -0.06611346
 -0.27748236  0.28083614  0.01609511 -0.03451148  0.03739854 -0.3401869
  0.02473426  0.13551074  0.092951   -0.0188617  -0.07381245 -0.10506907
 -0.04247174 -0.08135191 -0.17690969  0.2373612   0.02638279  0.16695833
 -0.01769519 -0.34953558  0.05302244  0.07092581  0.5204251   0.02885699
 -0.13836858  0.02768292  0.03981522 -0.10093722  0.19768718 -0.05908706
 -0.40989432 -0.21234281 -0.04251588  0.0936816  -0.13319764  0.12418009
 -0.3020038  -0.08380523 -0.15537375  0.2640429   0.21374463 -0.04680831
  0.11586644  0.02730467 -0.11931552  0.12990725 -0.01843963  0.07353117
  0.04785628 -0.15605277  0.11944658  0.37946808 -0.09885401  0.15023646
  0.35473734  0.02279716 -0.03044071 -0.47841713  0.02985702  0.14702788
  0.00838888 -0.10192064  0.13197535  0.1985307  -0.22070563 -0.14969456
  0.09831586  0.25228268 -0.08923467 -0.05722607  0.01937509  0.13800937
  0.17960094 -0.13513601  0.3681594  -0.30736476 -0.08834269 -0.02184459
  0.06171556 -0.24030404  0.04474279  0.02265936 -0.05258549  0.04457134
  0.29199457  0.11119135 -0.2618541  -0.0811776  -0.19683328 -0.1546243
  0.11748852  0.08813484 -0.22746834 -0.28291523  0.12387356 -0.11120939
  0.15033108  0.14091885 -0.0365756  -0.03371487 -0.04748641  0.00817969
  0.02183198 -0.12724514 -0.404854   -0.29471898 -0.21277988 -0.06912863
  0.12883034 -0.15383805  0.04195183  0.0012484  -0.15771028 -0.16701156
 -0.17974547 -0.02413702 -0.30231202  0.23664221  0.089472   -0.05015067
  0.26406115 -0.27101922 -0.16956523 -0.11048034 -0.03145941 -0.06884318
  0.08136211  0.1506413  -0.16049841  0.10241837 -0.06678198  0.12750158
 -0.02106605 -0.18300205  0.08409949 -0.2712507  -0.10516483  0.3231615
 -0.38128394 -0.20435631 -0.17084496 -0.05156534  0.11588351  0.0464528
 -0.02057509  0.1335271  -0.12859598  0.31352675 -0.2110663  -0.11174425
  0.3063193   0.24020493  0.13611987  0.06885457  0.04921312  0.07330573
  0.2796796  -0.04391008  0.07544797  0.13104796  0.05047014  0.46191835
  0.4159376   0.38144583 -0.16248228  0.20568906 -0.02973433 -0.42408
 -0.13114025  0.03833903  0.32101822 -0.29639018  0.1357424  -0.02214482
  0.57841957  0.08889066  0.07659279  0.29228044  0.25545216  0.10234438
 -0.21457013  0.00622525  0.17004317 -0.247297   -0.13280018 -0.46365476
 -0.19509676  0.02782843  0.00818671  0.0361548   0.00571124  0.10390852
 -0.03496863 -0.08707751 -0.10625584 -0.16033697  0.19462115  0.15917
 -0.11933412  0.09253347  0.06267133  0.05447381 -0.0540904  -0.11988366
  0.41949314  0.16501358  0.298972   -0.20646961  0.21615243 -0.013099
  0.13005862  0.3234473   0.03037662  0.01928698 -0.30762738  0.42851442
  0.01607797 -0.04871272  0.12379712 -0.3321947   0.10627666  0.00396187
  0.07296829  0.09805275  0.00227123 -0.12288275  0.12831762  0.11528923
 -0.04785794 -0.22175589 -0.1667943  -0.03096866 -0.09257201 -0.13081181
 -0.22670251  0.30068648 -0.02083788 -0.07018736 -0.17957526 -0.22954546
 -0.18561931 -0.42648566  0.05399875  0.00461718  0.24101257  0.3594962
  0.05834677  0.13146946 -0.08297875 -0.24854645  0.08729433 -0.05308846
 -0.09152822  0.16584994 -0.02773686 -0.10600583  0.03679735  0.04894324
 -0.16349718  0.15516582 -0.30349767 -0.11350073  0.05888548 -0.31816912
 -0.1747743   0.04235711  0.04220553  0.2774837  -0.22829273  0.28743172
 -0.33563092 -0.02270807  0.37269855 -0.29209936 -0.02161145 -0.01564906
  0.0090261  -0.12554921 -0.16870295 -0.22702895 -0.02387614  0.08392444]"
Segfault when using VLOG with with MKL on 1.13rc1 comp:mkl,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): from source with MKL
```
bazel build --config=mkl --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx2 --copt=-mavx --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --copt=""-DEIGEN_USE_VML"" //tensorflow/tools/pip_package:build_pip_package
```
- TensorFlow version (use command below): 1.13 rc1
- Python version: 3.6
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): 5.3
- CUDA/cuDNN version: 10.0, 7.4.1
- GPU model and memory: Nvidia V100, 16GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Segfault when running an estimator test

**Describe the expected behavior**
No crash

**Code to reproduce the issue**
```git clone https://github.com/tensorflow/models && cd models/samples/core/get_started && TF_CPP_MIN_VLOG_LEVEL=1 python premade_estimator.py```
https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py

**Other info / logs**
```
#0  0x00007fff474a1e57 in tensorflow::OptimizationPassRegistry::RunGrouping(tensorflow::OptimizationPassRegistry::Grouping, tensorflow::GraphOptimizationPassOptions const&) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#1  0x00007fff4bb9b6dc in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::string, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, long long*) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007fff4bb9c19c in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys> >*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo> >*, tensorflow::DirectSession::RunStateArgs*) () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fff4bb9e478 in tensorflow::DirectSession::GetOrCreateExecutors(absl::Span<std::string const>, absl::Span<std::string const>, absl::Span<std::string const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fff4bb9fad4 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fff49b36ad5 in tensorflow::SessionRef::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fff49d4c1ff in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) [clone .constprop.654] () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fff49d4ca79 in TF_SessionRun () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007fff49b31ec1 in tensorflow::TF_SessionRun_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007fff49b31fa2 in tensorflow::TF_SessionRun_wrapper(TF_Session*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007fff49ae633c in _wrap_TF_SessionRun_wrapper () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```",True,"[-0.30404085 -0.37012625 -0.12515302 -0.3441813  -0.00941844  0.01273115
 -0.12161573 -0.02399426 -0.00979819 -0.01594119 -0.19307521  0.50107014
  0.21570426  0.01356867  0.13108183  0.31209904 -0.05321011 -0.14261827
 -0.05716834  0.01770372  0.04123702  0.08466576  0.02875108  0.10931692
  0.00930195 -0.01387649 -0.39095724 -0.09062517 -0.050207   -0.06383763
  0.17350936  0.20523252  0.07942892 -0.14152017  0.21932429  0.22442256
 -0.00587575 -0.14047384 -0.07580139  0.03881937  0.04268248 -0.1800034
 -0.00060876 -0.08062001  0.00167252 -0.12907577 -0.09463197  0.11422807
 -0.22087316  0.17283356 -0.19891256 -0.21441898 -0.39547715  0.04597351
  0.29868525  0.00898514 -0.14898676  0.1818189   0.28071323  0.1605426
  0.21260446  0.08003055  0.12895994  0.18614912  0.16283688  0.01448569
  0.2056755  -0.11644381  0.3709633   0.17399196  0.10648209 -0.19747955
 -0.1043684  -0.1454679  -0.04345107  0.00597969 -0.06486425  0.0768526
  0.4184003  -0.23883203  0.016829    0.11035191  0.198926   -0.18012008
  0.03572837 -0.10436171  0.3995484   0.21146873  0.53117955  0.0639222
 -0.01911987 -0.25255388  0.12018931  0.15627497  0.38672465  0.16307504
 -0.18748504  0.09172387 -0.25582874 -0.27679837 -0.06627708 -0.2036354
  0.24964021  0.06270304  0.10186719  0.02705207  0.20080934  0.30985856
  0.03076039  0.09829627  0.23084494  0.19235773  0.12424335 -0.20767257
 -0.21812846  0.39547238 -0.00726529 -0.16871856 -0.36885047  0.56500936
  0.11202284 -0.24789722  0.04057922  0.22748266  0.2529912   0.322302
 -0.4167783  -0.08946303  0.00663844  0.11100718  0.10005496  0.07860367
  0.03222761  0.2958303  -0.07666612 -0.01196494 -0.47048685  0.04894369
 -0.0198796  -0.15771928 -0.22398211 -0.00571474  0.14690854 -0.11886309
 -0.06254609 -0.0578024  -0.06283842  0.20475152 -0.09749821 -0.18783566
 -0.44126663  0.15479968 -0.2692206   0.57595915  0.08723949  0.02911874
  0.15372398 -0.06585841  0.12284523 -0.16196805 -0.09390157  0.09135888
  0.12350343 -0.04761463  0.21762218  0.13302362 -0.4188807  -0.08182568
 -0.28624427 -0.04010628 -0.26426667  0.12613401  0.05791564 -0.11766455
  0.3401405  -0.20038053  0.42052835 -0.26415008 -0.10185765  0.22100797
  0.3905306   0.12948409  0.15993604 -0.04560222 -0.1052946  -0.07396258
 -0.05631043  0.13068989 -0.07141913 -0.1627115  -0.10535572  0.03559567
  0.3091611  -0.14975123 -0.07341443  0.06063601  0.09307939  0.1120681
  0.2693355   0.07870953  0.13426915  0.03230121 -0.15026873 -0.11711902
  0.16459845  0.2843939  -0.2830739  -0.20983402 -0.34849483 -0.15332022
 -0.03930803 -0.12004311 -0.16468227 -0.15928414 -0.27862108  0.22329277
  0.09565476  0.0586034   0.29270926  0.1702342   0.02584331 -0.0017506
 -0.08498157 -0.1710501  -0.26738536  0.11705461 -0.09871326  0.01997161
 -0.17232287  0.04727209 -0.05830941 -0.18562089  0.18731292 -0.07818544
 -0.0347432   0.02277626 -0.35105377  0.0056575  -0.02236897 -0.06586839
 -0.10474375 -0.38390768 -0.12517722 -0.22589973  0.29133534  0.077269
 -0.09943447  0.39697343 -0.11092164  0.3481288   0.14104499 -0.06770508
  0.05902365  0.07631665  0.10185647  0.13315278 -0.12775329  0.1164228
  0.19826779 -0.19949698  0.05259355  0.2853933  -0.13561743  0.18826525
  0.18827029  0.16293076 -0.43153405  0.00430543 -0.19447437 -0.20188357
  0.41577694 -0.26882833  0.57549536 -0.08190589 -0.0704868  -0.09766254
  0.2772573  -0.36555457 -0.22708416  0.03124296  0.0467305   0.12473735
  0.00208427  0.20428428  0.0557051  -0.10444956  0.01196394 -0.45979458
 -0.18276751 -0.22813438 -0.17218247  0.03475777  0.05036417  0.1725865
  0.08788528  0.0176905  -0.04369505  0.00121187  0.30169213  0.00204246
  0.00240789  0.00072485  0.07658149 -0.03810263 -0.01720902  0.11839654
 -0.05903383  0.13015702  0.21484281 -0.02658809  0.1555331   0.16235967
 -0.33460376  0.12548244 -0.10084505  0.28422076 -0.37445205  0.31795794
 -0.09752376  0.10439735 -0.147583   -0.40498388 -0.05029085  0.00698333
  0.31284046  0.22994503 -0.41964626  0.03993832 -0.3576496   0.11074119
 -0.12913576 -0.08229722 -0.1301769  -0.09190345 -0.19616741 -0.16793855
 -0.25538796  0.31787527 -0.04852072 -0.20244747 -0.14907387 -0.09949428
  0.05036496  0.03114924 -0.34890866 -0.25617325  0.22948456  0.32655033
 -0.05380815 -0.11933225  0.08467644  0.26857758 -0.31056345 -0.09182075
 -0.48471174  0.1580115  -0.02352329  0.11476922 -0.01537839  0.42941874
 -0.05814288 -0.1303519   0.03211653 -0.13053401 -0.06798429 -0.07212149
  0.07676648 -0.15102708  0.35884833  0.07537183 -0.17173047 -0.02423407
 -0.2553612   0.2665096   0.13626152 -0.21086785  0.16535023  0.0527894
  0.28602064 -0.03989337 -0.40777045 -0.31231987  0.00728467 -0.3262354 ]"
Can't build tensorflow master with CUDA & MPI enabled type:build/install subtype: ubuntu/linux subtype:bazel,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.22.0 (tried with 0.21.0, 0.20.0, 0.19.0)
- GCC/Compiler version (if compiling from source): 5.4.0
- Nvidia Driver: 410.48
- CUDA/cuDNN version: 10.0.130 / 7.4.2
- GPU model and memory: Tesla V100

I use nvidia-docker/docker to build and test the master branch of tensorflow. Since a few days, I consistently get the same error over and over.

```shell
Execution platform: @bazel_tools//platforms:host_platform
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In lambda function:
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:125:49: error: no matching function for call to 'tensorflow::TensorResponse::InitPartial(const tensorflow::RecvTensorResponse&)'
           tr.InitPartial(mpi_response.response());
                                                 ^
In file included from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:31:0:
./tensorflow/core/distributed_runtime/tensor_coding.h:79:8: note: candidate: void tensorflow::TensorResponse::InitPartial(const tensorflow::RecvTensorResponse&, const tensorflow::AllocationAttributes&)
   void InitPartial(const RecvTensorResponse& response,
        ^
./tensorflow/core/distributed_runtime/tensor_coding.h:79:8: note:   candidate expects 2 arguments, 1 provided
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/contrib/mpi/mpi_utils.h:25,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:34,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:467:54:   required from here
./tensorflow/core/util/tensor_format.h:441:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
                             ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:441:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 555.689s, Critical Path: 278.37s
INFO: 7384 processes: 7384 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```

I build using the following:

```dockerfile
ENV LD_LIBRARY_PATH='/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64/:/usr/local/cuda/extras/CUPTI/lib64' \
    BLAS_INCLUDE='/usr/local/cuda/targets/x86_64-linux/include' \
    BLAS_LIB='/usr/local/cuda/targets/x86_64-linux/lib' \
    CPLUS_INCLUDE_PATH='/usr/local/cuda/$CPLUS_INCLUDE_PATH' \
    CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \
    GCC_HOST_COMPILER_PATH=""/usr/bin/gcc"" \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=5.2,6.0,6.1,7.0,7.5 \
    TF_CUDA_VERSION=10.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_ENABLE_XLA=1 \
    TF_NEED_CUDA=1 \
    TF_NEED_JEMALLOC=1 \
    TF_NEED_HDFS=1 \
    TF_NEED_MPI=1 \
    TF_NEED_VERBS=0 \
    TF_NEED_OPENCL=0 \
    TF_NEED_GDR=0 \
    TF_NEED_GCP=0 \
    TF_NEED_S3=0 \
    TF_NEED_TENSORRT=0

# Get the TF branch for later build
RUN rm -rf /opt/tensorflow && \
    git clone --branch=$TF_BUILD_BRANCH --depth=1 $TF_REPO /opt/tensorflow && \
    cd /opt/tensorflow && \
    rm -f /usr/local/cuda/lib64/stubs/libcuda.so.1 && \
    ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 && \
    echo ""/usr/local/cuda/lib64"" > /etc/ld.so.conf.d/cuda.conf && \
    echo ""/usr/local/cuda/targets/x86_64-linux/lib/stubs"" > /etc/ld.so.conf.d/cuda-stubs.conf && \
    bazel clean && \
    ldconfig && \
    tensorflow/tools/ci_build/builds/configured GPU \
    bazel build -c opt --copt=-mavx --config=cuda --verbose_failures \
        --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
        --action_env=LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \
        tensorflow/tools/pip_package:build_pip_package && \
    mkdir /opt/tensorflow/pip_pkg && \
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /opt/tensorflow/pip_pkg --gpu && \
    pip --no-cache-dir install --upgrade /opt/tensorflow/pip_pkg/tensorflow_*.whl && \
    rm -f /usr/local/cuda/lib64/stubs/libcuda.so.1 && \
    rm -rf /root/.cache && \
    rm -rf opt/tensorflow/pip_pkg
```

My script is perfectly working for the branches:  `r1.12` and `r1.13`. and fails on `master`. I guess that you have updated something, because it seems that your CI builds are still passing.",True,"[-0.437972   -0.83968866 -0.44582745  0.21585342  0.08800489 -0.23632126
 -0.23328874 -0.04916397 -0.24862492 -0.22928163 -0.01095351 -0.02830924
 -0.26000553  0.08142802 -0.17138952  0.2714292   0.00167079 -0.2612394
  0.3798325   0.07515079 -0.16308844 -0.00754987 -0.17313361  0.24507362
  0.5039597   0.18974698 -0.17696942 -0.2029388   0.20612623 -0.18843192
  0.61759627  0.00513693  0.11776459  0.00656838 -0.07259738  0.22866756
 -0.03603359 -0.053953   -0.06762005 -0.05896425  0.14189653  0.02439015
  0.21031201 -0.04701171  0.27313936 -0.16474688  0.15777722 -0.05354468
 -0.05738394 -0.28901538 -0.13088816 -0.1686287  -0.00321211 -0.19132051
 -0.03036601  0.05275457  0.1344434   0.15317464 -0.18824212  0.27351296
  0.10745262  0.17962715 -0.13989623  0.04172992 -0.04230598  0.18161465
  0.17916253 -0.40498257  0.5986808  -0.05154824  0.16151743 -0.1065223
 -0.28604567 -0.18178253 -0.2613568   0.16719447  0.2852895   0.31853575
  0.10769264 -0.01671624  0.02000625 -0.05428745  0.2952323  -0.08833289
 -0.01174533  0.19008876  0.24314612  0.14677912  0.0648689  -0.19392169
  0.44051445  0.30788267 -0.06010975  0.10088874  0.14458784  0.06612683
  0.03738062  0.2516442   0.04931125 -0.18702695 -0.30768847 -0.15482287
 -0.0639879   0.19326153 -0.12410599 -0.22920397  0.3043754   0.1533214
 -0.02167943 -0.15088612  0.22070582 -0.12869257  0.2248416   0.12132071
  0.22548445 -0.08167505 -0.48604846  0.09639586 -0.08301201  0.783697
 -0.33821794 -0.06192374  0.21377996 -0.09323995  0.4108165   0.07905015
 -0.09478103 -0.21787445  0.08131164  0.2234335   0.17244357  0.1568606
 -0.21009101  0.2796048   0.0044137   0.08502646 -0.28319848 -0.04181094
 -0.0609248  -0.09653713 -0.09442646  0.35080212 -0.07922679 -0.25870365
 -0.08634368  0.01554533 -0.07543474 -0.05524115  0.01178409  0.02489395
 -0.5101914  -0.06738023 -0.12802233  0.2674585   0.24535328  0.38412488
  0.24811628 -0.0657272  -0.0819538  -0.5744878  -0.05662487  0.37761557
 -0.26058733 -0.05176778 -0.38546926  0.14241657 -0.3081351  -0.30982998
  0.27381232  0.3423315  -0.15165028 -0.18037067  0.07826439  0.19015902
  0.16966103 -0.13867462  0.2875175  -0.4983467   0.03383883  0.2566709
  0.02097368  0.19657096  0.20962691  0.00098359  0.15958205  0.14416775
  0.17085963  0.08524997 -0.06658679  0.08722578 -0.36138391 -0.07953559
  0.16713512 -0.08292553 -0.26045835  0.22704121  0.2736339   0.13484848
  0.00953617  0.19653797 -0.42265514  0.09254389  0.12107332  0.09722769
 -0.18524677 -0.2992112  -0.11225114  0.17219761 -0.44175404  0.04378117
 -0.02071451 -0.24738634 -0.0263249   0.09765293 -0.30133915  0.02272126
  0.24395405 -0.13893318 -0.20561856  0.2723773   0.27309993 -0.16182636
  0.0209851  -0.1999051   0.06310207 -0.26549625  0.08207247 -0.0890729
  0.01181553  0.3456539  -0.13264135  0.1456667   0.27752227  0.19224128
  0.05150175  0.01975804  0.15284476 -0.04109567 -0.24993497  0.15489426
 -0.50606954 -0.16075452  0.04576024  0.03635833  0.04393371 -0.00752345
 -0.01095514 -0.06103564 -0.18534464  0.21973386  0.03332183 -0.19778612
  0.16980824  0.2030829   0.21621332  0.11321241 -0.28923154  0.0110411
 -0.01092323  0.04086712  0.07070429  0.4199031  -0.30382144  0.20925768
 -0.02390983  0.4002552  -0.38053307  0.24863371  0.03132136 -0.09585652
  0.18917973  0.01439635  0.17619348 -0.359667   -0.26934603 -0.06798851
  0.31531435  0.03935265 -0.02423254 -0.26798946  0.23071714  0.07015343
 -0.18934515 -0.20309806 -0.00343228 -0.3530523  -0.03568193 -0.45675832
 -0.32435498  0.16566557 -0.08691449  0.36187977 -0.02330183 -0.0970637
 -0.20754698 -0.14455281  0.26189938  0.23713614  0.00981769  0.18962511
 -0.32788157 -0.05785028  0.13605526 -0.17106286  0.04395337 -0.05275553
  0.12996513  0.11009911  0.5271395  -0.06645828  0.20494843 -0.26363322
 -0.1376782   0.28079903  0.02054808  0.11650044 -0.15684631  0.49873817
  0.38333964 -0.19603306 -0.16933732 -0.335159   -0.34474882 -0.07021692
  0.08517509 -0.05117936 -0.33496922 -0.32535994 -0.17051026  0.34939057
 -0.12171823 -0.08326562 -0.07823145  0.25963843 -0.24226148  0.05969014
 -0.07992795  0.0518821   0.01065824 -0.29198545  0.13917482 -0.11553191
  0.115646   -0.40743673 -0.2766901  -0.08269808  0.2718191   0.34423453
 -0.29668683 -0.04620534 -0.0035664   0.33080155 -0.22232129 -0.02938552
  0.12179238  0.06446858  0.17929408  0.05563224  0.19416766  0.4124381
 -0.04040818 -0.16035965 -0.19426304 -0.27356273  0.2549842  -0.22537047
 -0.271186   -0.01670529 -0.01190216  0.42092782  0.00784469  0.23751372
 -0.07047512  0.26237562  0.660649   -0.2213006  -0.49430788  0.11540138
 -0.05012108  0.05770905 -0.00432736 -0.1533251   0.4657661  -0.24238743]"
Bad broadcasting when multiplying sparse and dense tensors ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
- Python version: 3.6.7 Anaconda 64bit
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**

When multiplying (`*` operator) a sparse tensor `S` with rank `N` with a dense tensor `D` with rank `M`, where `N > M` and `S` and `D` have different but broadcast-compatible shapes, then TensorFlow attempts to perform the multiplication producing and incorrect sparse tensor as a result with the shape of `S`.

**Describe the expected behavior**

TensorFlow should refuse to perform a sparse-dense multiplication that requires broadcasting. This is the case when both tensors have the same rank or the rank of the sparse tensor is less than the rank of the dense tensor, but not when the rank of the sparse tensor is greater.

**Code to reproduce the issue**

```py
import tensorflow as tf

with tf.Graph().as_default(), tf.Session() as sess:
    a = tf.reshape(tf.range(12), [3, 4, 1])
    b = tf.SparseTensor([[0, 0, 1, 0], [0, 0, 3, 0]], [10, 20], [1, 1, 4, 2])
    c = a * b
    print(c.shape)
    # (1, 1, 4, 2)
    print(sess.run(tf.sparse.to_dense(c)))
    # [[[[ 0  0]
    #    [10  0]
    #    [ 0  0]
    #    [60  0]]]]
```

**Other info / logs**
NA",True,"[-0.20012845 -0.41438088 -0.1261115  -0.31473875  0.2209292  -0.3501492
 -0.16651787  0.00829732 -0.08920698  0.02465438  0.08830322  0.12280395
 -0.14623448  0.04870507 -0.11698559  0.10682209  0.13699837 -0.09691614
  0.07979359  0.07927175 -0.05471262 -0.24305144 -0.18310383  0.07912771
  0.29513705  0.11052604 -0.00198487 -0.13089237  0.17998374 -0.10384533
  0.3640748   0.23954862  0.2496011   0.00463808 -0.30926543  0.09349351
 -0.08069091 -0.21703845 -0.2763166  -0.17373887 -0.04995931  0.17620455
  0.10010664  0.20263201 -0.09644759  0.16043529  0.03903616  0.05562799
 -0.10952457 -0.2411297   0.04207331  0.12387204 -0.38524222 -0.08597507
 -0.20470026 -0.24138165  0.00246925  0.07512388  0.03110792  0.18733957
  0.07998622 -0.15797892  0.01542907  0.05147946  0.14162903  0.19002444
  0.28653735 -0.13045475  0.31614074 -0.02077277 -0.06952078  0.04830038
 -0.13906103  0.2543648   0.05886265  0.18685548  0.18635389  0.22055562
  0.2762441  -0.04024795  0.2508216  -0.1816592   0.21802932 -0.22512135
  0.22690953 -0.12005929  0.15273577  0.24463445 -0.01547559 -0.27543712
  0.15460709  0.19465682 -0.08981718  0.04179289  0.1028643   0.21014315
  0.06674188 -0.08491579 -0.01450419  0.15187518 -0.07178362 -0.0557564
 -0.08367787  0.20917326 -0.04973797 -0.18287675  0.1924394  -0.05635928
 -0.0512383   0.0544734  -0.04642722 -0.01081183 -0.03047967 -0.02689258
  0.02759599 -0.06784772 -0.10733199  0.22417083  0.00105876  0.3273248
 -0.23539406 -0.23798423  0.08219054 -0.02774481  0.40997064  0.22457147
 -0.17375477 -0.06267139 -0.11745068 -0.03929775  0.0737906   0.06902464
 -0.05586506  0.023946   -0.04557829  0.06532726  0.03402057 -0.01769268
 -0.24032158 -0.05496853  0.07544532  0.3445105   0.16212893 -0.13372251
 -0.01505378 -0.05748501  0.09876487  0.17936584  0.03275851 -0.13478822
  0.01433916 -0.2023021   0.1441865   0.11456582  0.16177167  0.07957481
  0.2678991  -0.02215362 -0.15684293 -0.45100167  0.05741235 -0.00099122
 -0.17472069 -0.076563    0.01073503  0.15759334 -0.15462832 -0.14121804
 -0.04836346  0.21437114 -0.3171688  -0.02015587 -0.10282102  0.31567532
  0.17426522 -0.2977517   0.09258552 -0.39044264 -0.1518427  -0.04587378
 -0.06346318 -0.20142034  0.20133442  0.00765185 -0.15610622 -0.01834111
  0.17805493  0.03787671 -0.16031247 -0.08535324 -0.34855986 -0.05495276
  0.11993073 -0.1654464  -0.07122529 -0.08450349  0.00810397 -0.09550393
 -0.12920184  0.1893934  -0.00454412 -0.2605266   0.09042317  0.14972407
  0.11766955 -0.3072392  -0.23951249 -0.00693213 -0.36534035  0.09847658
  0.10858275 -0.23255152 -0.17985061  0.30981272 -0.22783637 -0.1784397
 -0.10705139  0.06544743 -0.32888013  0.32423496  0.1296843  -0.13648736
  0.08638646 -0.23129936 -0.39547265 -0.09699985 -0.31388736  0.05966987
 -0.08247992  0.39472318  0.11503845  0.1923109   0.04663439 -0.07364115
  0.01432452 -0.12718567 -0.11957611 -0.12734026 -0.08796291  0.24365532
 -0.454412   -0.00978518  0.10527179  0.07251493 -0.05874542  0.18945004
 -0.0526973   0.25594556  0.0021655   0.28990075 -0.09004977  0.05391721
  0.34721705  0.30342168  0.20401171 -0.19538784  0.02842011  0.08710369
  0.25943816  0.18220007  0.23189834  0.05401035 -0.13306707  0.16807503
  0.3219442   0.42057186 -0.25722975  0.1952002   0.11825833 -0.22588527
 -0.16614543  0.01706231  0.20051685 -0.4807822   0.07971656 -0.05520014
  0.288757    0.17337707 -0.21101174  0.08617177  0.07449688  0.07404985
  0.04993126  0.08824142 -0.00593061 -0.4086256  -0.29144597 -0.52891517
 -0.04271898 -0.00621332  0.11974841  0.13100389  0.21301901  0.28061187
 -0.14597812 -0.02527912  0.00227127 -0.04658906  0.09047152  0.2219071
 -0.02315874  0.01859771  0.01649164  0.08199899 -0.10206542  0.075371
  0.23256624  0.10797867  0.39089304 -0.09642158  0.30036387  0.05227083
  0.07269855  0.18008651 -0.15690295  0.13920264 -0.29957616  0.56073797
  0.1871239  -0.06634855  0.05644503 -0.31132668  0.02065745 -0.0534628
  0.00654334 -0.03466039  0.0271701  -0.36293495  0.077378   -0.02571012
 -0.08980043 -0.38068533 -0.29763934  0.25326538  0.10453686  0.10433314
 -0.2580901   0.27434742 -0.03984733 -0.07036455 -0.10040223 -0.03915219
  0.00562039 -0.16727047  0.07650334 -0.12844074  0.4102566   0.1682074
  0.03387219  0.05623115 -0.36615723  0.07991724 -0.19121702 -0.14898787
 -0.1840368   0.16817065  0.04803097 -0.18625423  0.07924524  0.34806448
 -0.10524198 -0.21405643 -0.16770571 -0.07613508  0.2685445  -0.12727821
 -0.04442016 -0.35528287  0.1512447   0.31201154  0.23021951  0.18469718
 -0.16733512  0.20399985  0.32663    -0.34251973 -0.32689792  0.10774899
 -0.1309913  -0.00109119 -0.1736755   0.13216202 -0.05515101 -0.01020008]"
tf.contrib.image.transform lead to a ValueError in new releases of tensorflow ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro 18.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): python-tensorflow-opt-cuda from manjaro repositories 
- TensorFlow version (use command below): 1.11
- Python version: 3.6/3.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.0.130-2 /   7.3.0-1
- GPU model and memory: 1080Ti 11GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

After the release that Allow a different output shape from the input in `tf.contrib.image.transform` code that applied this function stopped working with a value error. For exampleon previous versions, using eager execution this worked:

`image = tf.contrib.image.translate(image, random_translation, 'NEAREST') `

But after this change I get a `ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`` on `tf.contrib.image.transform` on the condition of the line 273-275 of `tensorflow/tensorflow/contrib/image/python/ops/image_ops.py`, where this condition is triggered (caused by empty output shape call in tf.contrib.image.translate:

```
if output_shape is None:
      output_shape = tensor_util.constant_value(
          array_ops.shape(images)[1:3]) or array_ops.shape(images)[1:3]
```

**Describe the expected behavior**

If instead of `tf.contrib.image.transform` I run it with `output_shape` argument:

`random_transformations = tf.contrib.image.translations_to_projective_transforms(random_shifts)
images = tf.contrib.image.transform(image, random_transformations, 'NEAREST',                                       output_shape=tf.convert_to_tensor(images.numpy().shape[1:3], dtype=np.int32))`

everything goes as expected. So I guess that the issue in on passing output_shape=None in line 122-126 of `tensorflow/tensorflow/contrib/image/python/ops/image_ops.py:

```
def translate(images, translations, interpolation=""NEAREST"", name=None):
  """"""Translate image(s) by the passed vectors(s).
  Args:
    images: A tensor of shape (num_images, num_rows, num_columns, num_channels)
        (NHWC), (num_rows, num_columns, num_channels) (HWC), or
        (num_rows, num_columns) (HW). The rank must be statically known (the
        shape is not `TensorShape(None)`.
    translations: A vector representing [dx, dy] or (if images has rank 4)
        a matrix of length num_images, with a [dx, dy] vector for each image in
        the batch.
    interpolation: Interpolation mode. Supported values: ""NEAREST"", ""BILINEAR"".
    name: The name of the op.
  Returns:
    Image(s) with the same type and shape as `images`, translated by the given
        vector(s). Empty space due to the translation will be filled with zeros.
  Raises:
    TypeError: If `image` is an invalid type.
  """"""
  with ops.name_scope(name, ""translate""):
    return transform(
        images,
        translations_to_projective_transforms(translations),
        interpolation=interpolation)

```",True,"[-0.41801932 -0.57635367 -0.10128747 -0.08925433  0.06776263 -0.30740473
 -0.08464295 -0.17844324 -0.18490636 -0.11293782 -0.18274269  0.16188604
  0.00428139  0.02687519 -0.14772463  0.4310052  -0.01474415 -0.0552379
  0.10928507 -0.04614541  0.01259904 -0.0021619   0.09955336  0.27914122
  0.05216362  0.06577025 -0.02284213 -0.06093575 -0.01959902 -0.06928301
  0.21075012  0.04492948 -0.00163602 -0.05063593  0.03912923  0.2709447
 -0.24985862 -0.05929985  0.0270895  -0.04502301  0.05969492 -0.04174598
  0.1462662   0.18643753 -0.14110912  0.07480736  0.03195019 -0.04360291
 -0.23740342 -0.09920444  0.08370614 -0.15131351 -0.50298053 -0.1311867
  0.03419397  0.04829741  0.16541146  0.05940302  0.2359333   0.08911779
  0.10695165  0.12674056  0.2458337   0.21265441  0.0301904   0.16083728
  0.14366198 -0.09336921  0.2922325  -0.35781008  0.1561516  -0.09749906
 -0.15667069 -0.05675934  0.01840103 -0.08819757 -0.1116973   0.32404023
  0.32134044 -0.20509417  0.20490216 -0.10006474  0.14351578 -0.00633229
  0.19476765 -0.05877021  0.29349872  0.04892472  0.350547   -0.03387853
  0.36745232 -0.02445496 -0.03204633  0.10649589  0.42867327  0.18952341
 -0.09500741  0.2008756  -0.12582688 -0.17996705 -0.13121563 -0.30303937
 -0.20045301  0.03375204  0.00772675 -0.03054744  0.1830146   0.21382776
  0.01146238 -0.03522567  0.12719497  0.05776627  0.14400347 -0.06644788
 -0.01841953 -0.02226984 -0.06476396  0.0306151  -0.18361586  0.24120775
  0.21442641 -0.15597871 -0.03722738  0.0485491   0.34969625  0.16910039
 -0.3518313  -0.00809955  0.1462176   0.1372479   0.09924443  0.14282751
 -0.07487541  0.05167519  0.03684386 -0.15851629 -0.5319823  -0.05803249
 -0.07243667 -0.14918977 -0.2284761   0.30478388  0.01924594 -0.1832698
 -0.02093528  0.08791403 -0.11259618  0.2530746  -0.24310315  0.00595873
 -0.34659255  0.11758562 -0.1944342   0.54324067  0.09694572  0.1602606
  0.04701333 -0.12161572 -0.08558287 -0.3182329   0.03662976  0.18541402
 -0.16916004  0.02195693  0.06167464  0.1731101  -0.4270293  -0.01065356
  0.00333765  0.18383965 -0.16172248  0.04180707  0.13268766 -0.05065016
  0.40868777  0.03831537  0.4011702  -0.2705375  -0.29683724  0.1044884
  0.42480296 -0.07441357  0.01065023 -0.02446203  0.0743371  -0.11980405
  0.06866131  0.1915853  -0.29048845  0.04808015 -0.29551947  0.19982685
  0.25385046  0.03855871 -0.12495379  0.11246498  0.15519413  0.17949282
 -0.00658488  0.30181313 -0.10140291 -0.16878983 -0.06818935 -0.02453226
  0.054471   -0.01443227 -0.24056071 -0.34997803 -0.4326735   0.01775899
 -0.09740758 -0.19298598  0.10901698  0.06434523 -0.2302812   0.02561634
  0.02411101 -0.00858452  0.05030528  0.07496013  0.04359305 -0.08944935
  0.21062568 -0.34682554 -0.18896684  0.05638564 -0.15405796  0.2456832
  0.00480751  0.12504776 -0.06555261 -0.12905443  0.34591228  0.04902366
 -0.00681872  0.09294543 -0.36090672 -0.30798015 -0.02776339  0.04877035
 -0.20436814 -0.35183173 -0.07419877 -0.1902552   0.13616756  0.28553283
 -0.00845428  0.08318333 -0.20479122  0.46616215 -0.1165912  -0.02439727
  0.3413649   0.11957052  0.127901    0.05667692 -0.01694385  0.2790854
  0.04709546 -0.01042419  0.24267022  0.10750712  0.04502615  0.559224
  0.07017965  0.15278204 -0.23075838 -0.00679241 -0.24520314 -0.09490202
  0.06958781 -0.16644877  0.4497137  -0.16556758  0.10442972 -0.1086168
  0.29432264 -0.1034492  -0.15617734  0.13724881  0.0509662   0.26751173
 -0.02892607  0.27632165  0.15325546  0.03920787  0.01054418 -0.3825143
 -0.3422094  -0.12777586 -0.16077697  0.11794955  0.16671266  0.11825074
  0.01780903  0.03890295 -0.10193767 -0.09678774  0.1592027   0.0297574
  0.03260023  0.023373    0.12809622 -0.01191744 -0.082191    0.0679436
  0.09341004  0.00351846  0.23750691 -0.16730335 -0.01125101  0.12738815
 -0.14386213  0.14760776 -0.2043841   0.21409768 -0.15443507  0.33062267
 -0.12848836 -0.06839445 -0.04137848 -0.08034306 -0.23600972  0.050092
  0.07801543 -0.02528496 -0.19754238  0.06456967 -0.19272847  0.06269175
 -0.12178481 -0.09834105 -0.1601341   0.03684749 -0.27959722 -0.00126807
 -0.15986586  0.25630334 -0.06764601 -0.24259914 -0.15227453 -0.124415
 -0.07608475 -0.31781134 -0.33762562 -0.17014146  0.2120167   0.49176034
 -0.02633125  0.14794932  0.1562853   0.11638498 -0.37857333  0.08122571
 -0.14733043  0.11667022 -0.14459348 -0.01488641  0.1275575   0.4251541
 -0.21508518 -0.13049164 -0.14972287 -0.10560106 -0.0387693  -0.19186148
 -0.09186979 -0.19825369  0.21975303  0.30343786  0.06146417  0.13156885
 -0.30872235  0.20993972  0.37390232 -0.07881838  0.12749605  0.19380292
  0.2765856  -0.14469099 -0.23563576 -0.1863244   0.00791027 -0.15537673]"
Incorrect masking in keras.backend.rnn ,"**System information**
```
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.12.0-rc0-2215-g6735dd4799', '1.13.0-dev20181109')
```

**Describe the current behavior**
For correct masking of `output` it is required that `output == states[0]` for  `keras.backend.rnn(..., unroll=False)`, not so with `unroll=True` or in latest version of [keras-team/keras](https://github.com/keras-team/keras). See [this line](https://github.com/tensorflow/tensorflow/blob/d703f64e452f49603a244978b13239ab7aac0168/tensorflow/python/keras/backend.py#L3496) for the reason.

**Describe the expected behavior**
As per this [PR](https://github.com/keras-team/keras/pull/11499) (and discussions in linked issues) it has been established that output should be independent of states. That is, it should _not_ be assumed that `output == states[0]` for `ouput, states = step_function(inputs, previous_states)` in the `keras.backend.rnn` implementation. For details see test below.

**Code to reproduce the issue**
See test below (basically same as [this test added keras-team/keras](https://github.com/keras-team/keras/pull/11499/files#diff-e942014d73bf67b47a3b55f7f7041797R829))

```python
import numpy as np
from tensorflow import keras

def test_rnn_output_and_state_masking_independent():
    num_samples = 2
    num_timesteps = 4
    state_and_io_size = 5
    mask_last_num_timesteps = 2  # for second sample only

    # a step function that just outputs inputs,
    # but increments states +1 per timestep
    def step_function(inputs, states):
        return inputs, [s + 1 for s in states]

    inputs_vals = np.random.random(
        (num_samples, num_timesteps, state_and_io_size))
    initial_state_vals = np.random.random((num_samples, state_and_io_size))
    # masking of two last timesteps for second sample only
    mask_vals = np.ones((num_samples, num_timesteps))
    mask_vals[1, -mask_last_num_timesteps:] = 0

    # outputs expected to be same as inputs for the first sample
    expected_outputs = inputs_vals.copy()
    # but for the second sample all outputs in masked region should be the same
    # as last output before masked region
    expected_outputs[1, -mask_last_num_timesteps:] = \
        expected_outputs[1, -(mask_last_num_timesteps + 1)]

    expected_state = initial_state_vals.copy()
    # first state should be incremented for every timestep (no masking)
    expected_state[0] += num_timesteps
    # second state should not be incremented for last two timesteps
    expected_state[1] += (num_timesteps - mask_last_num_timesteps)

    # verify same expected output for `unroll=true/false`
    inputs = keras.backend.variable(inputs_vals)
    initial_states = [keras.backend.variable(initial_state_vals)]
    mask = keras.backend.variable(mask_vals)
    for unroll in [True, False]:
        last_output, outputs, last_states = keras.backend.rnn(
            step_function,
            inputs,
            initial_states,
            mask=mask,
            unroll=unroll,
            input_length=num_timesteps if unroll else None)

        np.testing.assert_allclose(
            keras.backend.eval(outputs), expected_outputs,
            err_msg=""Unexpected output for unroll={}"".format(unroll))
        np.testing.assert_allclose(
            keras.backend.eval(last_states[0]), expected_state,
            err_msg=""Unexpected state for unroll={}"".format(unroll))
```
Gives:
```
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0
Unexpected output for unroll=False
(mismatch 25.0%)
 x: array([0.116687, 0.622734, 0.210443, 0.662715, 0.720813, 0.654062,
       0.936728, 0.451018, 0.471044, 0.560336, 0.133492, 0.228378,
       2.081284, 2.415464, 2.081284, 2.415464], dtype=float32)
 y: array([0.116687, 0.622734, 0.210443, 0.662715, 0.720813, 0.654062,
       0.936728, 0.451018, 0.471044, 0.560336, 0.133492, 0.228378,
       0.133492, 0.228378, 0.133492, 0.228378])
```
**Other info / logs**
There are a few further implications of current implementation addressed by the tests added to keras-team/keras [here](https://github.com/keras-team/keras/pull/11499/files#diff-e942014d73bf67b47a3b55f7f7041797R829))",True,"[-3.30131501e-01 -3.89123142e-01 -5.32206714e-01  2.35405024e-02
 -1.45597696e-01 -3.29470247e-01  3.59919965e-01 -2.04411492e-01
 -3.41006815e-01 -1.06454324e-02  8.75010900e-03 -5.29389307e-02
  6.09034412e-02 -4.05620635e-02 -2.26778477e-01  7.85715040e-03
  3.80540043e-02  8.24425444e-02  2.67627202e-02 -3.44458431e-01
  6.35018647e-02 -1.22718304e-01 -1.12542562e-01  4.42278862e-01
 -1.37590496e-02  1.92120820e-01  3.44562382e-02 -2.16346681e-01
  7.62214959e-02 -9.48229432e-02 -1.03540175e-01 -7.67382234e-02
 -1.40939534e-01  7.20866024e-02 -1.49002582e-01  2.56687999e-01
 -1.70175388e-01 -1.94593847e-01 -9.51164067e-02 -1.89755246e-01
  7.69028887e-02  1.61181033e-01 -1.52515620e-02 -1.95228457e-01
  1.09328136e-01 -1.48617357e-01 -8.53961259e-02 -1.49741814e-01
 -1.87911287e-01 -3.33633333e-01 -2.15714142e-01  2.91217744e-01
 -4.98340309e-01 -4.95371670e-01  2.03047469e-01 -2.32201934e-01
  2.03203425e-01  1.37140751e-01 -9.79427472e-02 -2.11272612e-01
  6.99085966e-02 -1.55680314e-01  4.90572564e-02  1.58252060e-01
  4.10464644e-01 -6.47135749e-02  1.58140566e-02 -1.25825107e-02
  4.02947217e-01 -1.07196696e-01 -1.04465326e-02  1.21528432e-01
 -3.64971220e-01  9.09273624e-02  6.39667921e-03  2.86075652e-01
  9.65213776e-02  1.14078000e-01  2.46315181e-01 -2.86916375e-01
  1.45551518e-01 -1.00664459e-01  9.20795426e-02 -4.69999611e-02
  1.88912660e-01 -3.77470292e-02  9.52305049e-02 -8.20875317e-02
  4.40938562e-01  7.89692029e-02  4.95760769e-01 -1.57483727e-01
 -1.39606446e-01  3.46568972e-01  3.38954628e-01 -2.72248209e-01
  1.46982044e-01  3.80937696e-01 -8.29530582e-02  8.43799114e-03
  3.07466816e-02 -3.04237545e-01 -9.11314934e-02  1.63016971e-02
  2.68167377e-01  1.62692696e-01  1.79841816e-02 -2.19079167e-01
  1.24495193e-01  5.35591133e-02  1.69572338e-01  1.21291175e-01
 -1.09349996e-01  6.24088049e-02 -1.34538427e-01  1.40344471e-01
  4.94086519e-02 -9.30482596e-02 -1.80895597e-01  5.56663632e-01
 -9.74382609e-02 -2.16883197e-02  9.06948149e-02  3.88570428e-01
  1.10192239e-01  1.58603072e-01  1.27923451e-02 -8.91714990e-02
 -6.96181953e-02  5.54463863e-02  8.39117169e-02 -5.64254448e-02
 -6.13853857e-02  1.36940420e-01  1.04368500e-01 -3.88622023e-02
 -2.59975761e-01 -1.19824402e-01 -2.67410368e-01 -1.21312983e-01
 -2.22548157e-01 -2.30977032e-02  5.70142567e-02 -2.41715655e-01
  3.06683868e-01  2.37437010e-01 -3.19010645e-01  8.18003714e-02
 -8.76861438e-02  2.99827047e-02 -1.45596251e-01 -1.36672050e-01
 -3.19711745e-01  1.59481391e-01 -1.96191482e-02 -2.11916864e-03
 -4.66079116e-02 -1.18864886e-01  9.50574875e-02 -2.07114339e-01
  1.10193774e-01  3.07234883e-01 -5.58159724e-02 -5.29869571e-02
  4.31979716e-01  2.06024259e-01 -1.71837911e-01 -3.42071474e-01
 -1.00212917e-01  1.30360663e-01 -7.37856030e-02  2.88684785e-01
 -9.16321874e-02 -2.13742897e-01  3.15566301e-01 -2.28302747e-01
  2.53941361e-02 -2.93611705e-01  3.24517637e-02  3.24746311e-01
  5.96998632e-02  8.17515701e-02  6.00942820e-02 -5.19187301e-02
 -1.70980804e-02  4.95939329e-03  3.23358834e-01  1.97716549e-01
 -1.02777079e-01  1.23100892e-01 -6.02707081e-03 -2.51482725e-01
  1.05132535e-01 -3.02076228e-02  1.21549247e-02 -1.27528310e-01
  1.87372208e-01 -4.27412912e-02  1.04705989e-01  2.29578465e-02
 -3.18312764e-01 -3.02365243e-01 -1.07743800e-01 -2.67951768e-02
  1.66171178e-01 -2.93108195e-01 -1.41555563e-01 -3.25319707e-01
 -2.53752321e-02  6.39854968e-01  9.49510932e-02 -3.97130400e-01
 -1.00569189e-01 -7.91159719e-02 -8.46042335e-02 -1.49542034e-01
  9.63450596e-02  2.77983993e-01 -2.03518290e-02  2.41141453e-01
  1.99853778e-01 -2.03258127e-01  8.40979517e-02 -1.46175355e-01
  1.75906658e-01  1.37238860e-01 -7.16507733e-02  1.14894897e-01
 -4.89799589e-01  6.64006826e-03  2.67012388e-01 -2.05507830e-01
  2.70366549e-01 -8.68140906e-03  1.65562034e-01 -2.07521558e-01
  9.82928723e-02  7.08482713e-02  4.40959707e-02 -1.61574125e-01
 -3.72135848e-01  2.08484784e-01 -5.87345064e-02 -3.73043925e-01
 -8.11782926e-02 -3.46804112e-02 -8.86029974e-02  1.51188761e-01
 -5.15544772e-01  2.19308615e-01 -2.34179676e-01  1.24071389e-01
  1.32735282e-01  4.36896086e-03  2.31258720e-01  1.60455525e-01
  1.47117674e-01  1.39185727e-01  1.63433980e-02 -1.43329889e-01
  2.63824761e-01  4.02687490e-01  9.84432399e-02  4.19196159e-01
  1.27938211e-01  2.14726120e-01 -4.28749442e-01  3.92521799e-01
 -1.73235402e-01 -3.00573707e-01  8.48658103e-03 -7.36074522e-02
  3.52140903e-01 -4.42486495e-01  3.29981625e-01 -5.62901795e-01
  3.25347364e-01 -3.73430513e-02 -1.96139321e-01  4.01116610e-01
 -4.01149616e-02  2.42908627e-01 -3.15632850e-01  2.79020965e-01
  3.88190985e-01 -3.01842690e-01  2.81295832e-02 -6.19394779e-01
  5.01962006e-02 -7.06433356e-02 -4.04452067e-03 -6.47169054e-02
  4.47125137e-01 -1.98077798e-01  3.46633673e-01  8.75357240e-02
 -9.06003565e-02  2.07020104e-01  9.19314548e-02  1.79728061e-01
 -3.10336828e-01  1.00927040e-01  2.44306743e-01  2.22783595e-01
  1.48769468e-05 -9.58074257e-02  1.52531594e-01  3.34721029e-01
  5.26803851e-01 -1.25526071e-01  4.53435808e-01  1.59780681e-03
 -2.10964441e-01  3.33936512e-01 -5.49280234e-02  3.17852318e-01
  2.45188773e-02  3.79841536e-01  1.13612987e-01  1.56254202e-01
 -2.23161355e-01 -4.78325486e-01 -3.18054259e-01  6.61666393e-02
 -9.94727612e-02 -1.41692460e-01 -5.36861718e-02 -6.29190087e-01
 -1.32891357e-01 -1.73285156e-01 -2.84877270e-01 -1.34785414e-01
 -2.58545607e-01  4.09908593e-02 -7.72971101e-03  1.67439133e-02
 -2.78908819e-01  4.16363388e-01 -3.48270983e-02 -2.46383220e-01
 -3.00668001e-01 -1.17648140e-01 -1.07756965e-01 -1.62723899e-01
 -1.68503180e-01 -2.48296872e-01  2.17999518e-01  9.13358808e-01
  1.69154748e-01 -1.34406179e-01  1.71891212e-01  1.18580803e-01
 -7.21599311e-02  1.19049564e-01 -3.34585495e-02  1.36163950e-01
 -8.03171992e-02 -1.71442538e-01  4.99314815e-02  4.44132447e-01
 -2.73861915e-01 -2.65610784e-01 -2.32490376e-01 -5.51831946e-02
 -9.96994674e-02 -6.16160594e-02  2.96476260e-02  9.15536135e-02
 -4.75320294e-02  3.53520036e-01  8.44943598e-02  2.98977077e-01
 -2.96412230e-01  3.84461462e-01  2.94330806e-01 -3.25423330e-01
  7.83032104e-02 -1.82498768e-01  1.99552029e-01  1.98523086e-02
 -1.54490173e-02  1.03224844e-01  7.21438527e-02 -5.79403639e-02]"
tf.GradientTape() not support to MaxPool3D comp:ops,"**System information**
- Have I written custom code: **yes**
- OS Platform and Distribution: **Linux Ubuntu 16.04**
- TensorFlow installed from (source or binary): **binary, pip3 install**
- TensorFlow version (use command below): **1.10**
- Python version: **3.5**
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: **CUDA Version 9.1.85**
- GPU model and memory: **TITAN V, 12066MB**

**Describe the current behavior**
I'm trying to create a simple cnn3d network, which contains a convolutional layer in 3d, a max pooling in 3d and a fully connected layer, all this using eager mode.
When trying to calculate the gradients with an optimizer I get this error:
`TypeError: 'NoneType' object has no attribute '__getitem__'`
Then, when trying to remove the MaxPool3D layer or replace with AveragePooling3D, the gradient calculation works without problems.

**Describe the expected behavior**
Calculate the gradients using the eager mode with tf.GradientTape() in a network with a Conv3D layer and a MaxPool3D layer.

**Code to reproduce the issue**
```python3
from __future__ import absolute_import, division, print_function

import tensorflow as tf

# enable eager mode
tf.enable_eager_execution()
tf.set_random_seed(0)
np.random.seed(0)


x = tf.random_uniform((10,5,10,10,3))
y = tf.random_uniform((10, 5))


class MyModel(tf.keras.Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv3d_1 = tf.keras.layers.Conv3D(filters=6,
                                           kernel_size=(3,3,3))
    self.max_pool_1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2))
    self.flatten = tf.keras.layers.Flatten()
    self.dense_1 = tf.layers.Dense(5)

  def call(self, input, training=False):
    """"""Run the model.""""""
    model = self.conv3d_1(input)
    model = self.max_pool_1(model)
    model = self.flatten(model)
    model = self.dense_1(model)
    
    return model

model = MyModel()

def loss(model, x, y):
  logits = model(x)
  return tf.losses.softmax_cross_entropy(onehot_labels=y, logits=logits), logits

def grad(model, inputs, targets):
  with tf.GradientTape() as tape:
    loss_value, logits = loss(model, inputs, targets)
  return loss_value, logits, tape.gradient(loss_value, model.trainable_variables)

# Optimize the model
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
global_step = tf.train.get_or_create_global_step()
loss_value, logits, grads = grad(model, x, y)
optimizer.apply_gradients(zip(grads, model.variables), global_step)
```

**Other info / logs**
The traceback error:

```python3
TypeErrorTraceback (most recent call last)
<ipython-input-4-56cdb9bbe4e5> in <module>()
     44 # Optimize the model
     45 optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
---> 46 loss_value, logits, grads = grad(model, x, y)
     47 optimizer.apply_gradients(zip(grads, model.variables), global_step)

<ipython-input-4-56cdb9bbe4e5> in grad(model, inputs, targets)
     40   with tf.GradientTape() as tape:
     41     loss_value, logits = loss(model, inputs, targets)
---> 42   return loss_value, logits, tape.gradient(loss_value, model.trainable_variables)
     43 
     44 # Optimize the model

/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc in gradient(self, target, sources, output_gradients)
    899         nest.flatten(target),
    900         flat_sources,
--> 901         output_gradients=output_gradients)
    902 
    903     if not self._persistent:

/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/imperative_grad.pyc in imperative_grad(tape, target, sources, output_gradients)
     62       target,
     63       sources,
---> 64       output_gradients)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)
    115     return [None] * num_inputs
    116 
--> 117   return grad_fn(mock_op, *out_grads)
    118 
    119 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.pyc in _MaxPool3DGrad(op, grad)
    180   return gen_nn_ops.max_pool3d_grad(
    181       op.inputs[0],
--> 182       op.outputs[0],
    183       grad,
    184       ksize=op.get_attr(""ksize""),

TypeError: 'NoneType' object has no attribute '__getitem__'
```

",True,"[-5.64277470e-01 -6.69410527e-01 -4.21010822e-01 -1.15483820e-01
  6.48274124e-02 -3.43260355e-02 -1.09168254e-01 -2.05201320e-02
 -4.50894415e-01 -1.25586778e-01 -7.77241439e-02 -2.00905800e-01
 -2.39970341e-01  1.98395044e-01 -2.19019175e-01  3.29674840e-01
  1.63882598e-01 -1.63471699e-01  2.98114240e-01 -2.70951450e-01
 -1.78719819e-01 -5.48915714e-02 -1.55870140e-01  4.43343252e-01
  2.82274604e-01  1.93492189e-01 -1.80392876e-01 -9.16967243e-02
 -6.69416934e-02  5.42258509e-02  2.62749135e-01  2.15020359e-01
 -4.96599376e-02 -4.33437824e-02 -6.48391694e-02  3.43626797e-01
 -1.06103569e-01 -1.97909594e-01 -2.67530560e-01 -9.23176259e-02
  3.68711073e-03 -2.13842671e-02 -8.13366771e-02  9.89742130e-02
  5.37039340e-03 -3.25051658e-02  7.01457858e-02 -6.12432286e-02
 -3.43827665e-01  4.76360098e-02 -1.72407672e-01 -4.01798934e-02
 -2.64625072e-01 -2.59025186e-01  1.35989383e-01 -1.37773871e-01
  3.05792987e-01 -4.53815386e-02  5.37600443e-02  4.98950183e-02
 -7.97745138e-02 -1.81892693e-01 -1.12326615e-01  2.50715256e-01
 -2.15340555e-02  1.65492028e-01  2.69135416e-01 -3.33480269e-01
  2.60029465e-01 -1.75199524e-01 -2.66354084e-01 -2.81759471e-01
 -1.96700454e-01 -1.82298973e-01 -9.60773677e-02 -1.92072876e-02
  1.78242773e-01  2.34908342e-01  5.27760386e-02 -2.52292395e-01
  3.58907402e-01 -1.02689534e-01  1.95778877e-01 -1.66577511e-02
 -8.34029168e-02  6.56156093e-02  2.28523970e-01  2.48194903e-01
  2.46318936e-01 -1.97002023e-01  4.34116244e-01  2.58735299e-01
 -1.04622558e-01  1.81402713e-01  1.51917845e-01  2.63774157e-01
 -1.32207684e-02  1.40035361e-01  9.46712196e-02 -4.14716229e-02
 -5.25896102e-02 -2.26179719e-01 -1.03456780e-01  4.06647287e-02
  4.19394337e-02 -2.31709667e-02  1.96789280e-01  1.51063293e-01
  7.08320290e-02 -1.26210563e-02  1.01809338e-01  7.43745193e-02
  1.96304440e-01 -3.33625436e-01  1.69489622e-01  8.52739364e-02
 -1.00319326e-01  2.08797157e-01  8.91026258e-02  6.69392288e-01
  1.77725852e-02 -1.43444210e-01  3.81809473e-01  1.88162982e-01
  5.98202407e-01  4.51081991e-03  3.41025591e-02  1.29697472e-01
  2.50536919e-01  3.37982774e-02  2.31234297e-01  1.69495374e-01
 -2.91406214e-01  2.28787810e-01  4.27399836e-02  2.90380232e-02
 -3.24149966e-01 -1.79500040e-02 -2.62133449e-01 -1.93847388e-01
 -2.26220697e-01  4.43754137e-01 -1.82115406e-01 -3.02071750e-01
 -1.03385024e-01  3.20541598e-02 -2.18188807e-01  2.54987150e-01
 -1.29170537e-01  2.18054265e-01 -3.45161438e-01  1.16573140e-01
 -5.90920895e-02  1.88110232e-01  1.58787340e-01  2.72527874e-01
  2.81737506e-01  5.67520969e-02 -2.52997838e-02 -6.38301313e-01
 -1.60933912e-01  5.22738695e-02 -1.76451504e-01  1.01246633e-01
 -2.72167981e-01  9.17854458e-02 -4.44930494e-01 -2.11481631e-01
  2.04359427e-01  2.23661065e-01 -2.13610083e-01 -1.54992521e-01
  1.22812688e-02  2.98539221e-01  1.90974563e-01 -5.60640767e-02
  1.51051641e-01 -4.20167983e-01 -9.07832198e-03  2.49718532e-01
  1.59829974e-01 -2.57917680e-02  1.29172206e-01 -1.24688171e-01
  2.06276812e-02  1.69930458e-01  4.18381333e-01  3.14321458e-01
 -3.15854371e-01  6.58370182e-03 -4.52969313e-01 -2.08418071e-01
  1.09007701e-01 -1.57444283e-01 -3.07133883e-01  7.21960142e-02
  2.74379611e-01  1.00018129e-01 -6.11774139e-02  3.39452624e-01
 -2.24476352e-01 -1.44692823e-01  1.57235175e-01  6.20936155e-02
 -1.38807178e-01 -1.36486307e-01 -2.11432636e-01 -1.54715419e-01
 -6.19806945e-01  1.80745944e-01  9.97145772e-02 -5.92360020e-01
  1.63867883e-02 -1.92633439e-02 -2.74750501e-01  1.82155408e-02
  1.39406160e-01 -2.87575662e-01 -6.79613128e-02  8.27453509e-02
  2.34061182e-01 -4.01075929e-02  1.54719383e-01 -5.41372895e-01
 -7.80415088e-02 -9.04544592e-02 -2.97378004e-01  3.38804722e-01
 -2.72731613e-02  2.91826814e-01 -1.32086128e-02 -7.94100910e-02
  1.24692798e-01  1.06552370e-01  3.25879574e-01 -2.12298296e-02
  1.31611764e-01 -3.28645974e-01  1.64266348e-01  1.99608386e-01
 -4.41116691e-01 -2.10273921e-01  9.58652571e-02  5.04509173e-02
  2.90867627e-01 -2.23395340e-02 -3.01558048e-01  1.32576048e-01
 -6.50444105e-02  6.69044197e-01 -7.39384145e-02 -1.39946312e-01
  2.57518470e-01  4.23686296e-01  2.86135286e-01  9.43228975e-02
 -2.75422513e-01  1.31494373e-01  3.51618707e-01 -1.02709651e-01
 -1.14748143e-02  4.59437609e-01 -2.50846535e-01  5.46322882e-01
  2.04691783e-01  2.17730224e-01 -2.31204629e-01  4.72526848e-01
 -1.50424376e-01 -1.26039892e-01  2.02747419e-01 -2.20643982e-01
  2.75360346e-01 -2.66582787e-01 -8.62688124e-02  7.32347071e-02
  5.53710461e-01  7.20796362e-02  9.99847800e-02  2.58871436e-01
 -2.21500322e-02  2.08182186e-01 -8.78835917e-02  3.66437584e-02
 -2.47498542e-01 -1.75203890e-01 -5.03382348e-02 -4.43620980e-01
 -1.28381610e-01 -7.77019653e-03 -1.54651580e-02  1.72206461e-02
 -8.94701388e-03  7.45303631e-02 -6.94569945e-02  6.81350082e-02
  1.33533046e-01  6.06567785e-03  1.22126877e-01 -3.27806845e-02
  1.66763365e-01  1.99072748e-01 -3.49843614e-02  1.24528795e-01
  3.24700251e-02 -4.63394485e-02  6.08784795e-01  1.12647310e-01
  3.80470514e-01 -2.05728173e-01  1.47760421e-01 -1.75408721e-01
  6.62077889e-02  1.87281087e-01 -1.33219898e-01  1.64921418e-01
 -3.06755602e-01  3.17154109e-01  3.27246606e-01 -2.52059519e-01
  2.68046141e-01 -2.38094911e-01 -3.32794785e-01  1.18365958e-01
  7.50970468e-02 -9.53006595e-02 -6.62021339e-02 -2.50696957e-01
 -1.59024402e-01  1.50327176e-01  1.90610319e-01 -1.82521492e-01
 -2.22493172e-01  4.13080640e-02 -1.44396620e-02  3.55360433e-02
 -2.27127492e-01  2.69137502e-01 -8.70872736e-02 -9.61230993e-02
 -7.53222704e-02 -1.22392684e-01  1.59723550e-01 -2.98052132e-01
 -2.70076096e-01 -1.20932117e-01  2.58825719e-01  2.04741523e-01
  1.13981955e-01  1.13515124e-01  3.26637737e-02  3.33761647e-02
 -5.16876280e-01  3.10394913e-04 -7.22012371e-02  1.00380585e-01
 -1.49049610e-03 -1.73339192e-02  9.52763855e-02  5.27473569e-01
 -1.29355788e-01 -1.28919944e-01 -1.92766979e-01 -2.00325191e-01
  7.13435933e-02 -3.07098389e-01 -2.03837335e-01 -2.92023599e-01
 -5.48094362e-02  3.48609686e-01 -3.28825042e-02  4.39116806e-01
 -3.11908364e-01  4.66478467e-01  6.87391758e-01 -2.49952078e-01
 -1.76329091e-01 -3.22627723e-02 -1.09466851e-01 -2.74483800e-01
  7.15416074e-02 -9.60429907e-02  1.59712434e-01  2.33678445e-02]"
Bug in image_captioning_with_attention.ipynb  ,"**System information:** ***Running the notebook on Colab.***

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): Colab ('1.12.0-rc2')
- Python version: Colab (3.6)
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: Colab
- GPU model and memory: Colab

**Describe the current behavior**

Error in Cell 27 (Training): 

`InvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/`

**Describe the expected behavior**

If `tokenizer.word_index[tokenizer.oov_token] = top_k + 1` in Cell 10 is commented out, the notebook runs end-to-end on Colab. 

**Code to reproduce the issue**

The reason this happens could be because in Cells 9 and 10, Tokenizer is defined with `oov_token=""<unk>""` in Cell 9, which assigns ""\<unk\> : 1"" in the token dictionary, and then `tokenizer.word_index[tokenizer.oov_token] = top_k + 1` in Cell 10 assigns ""\<unk\>: top_k + 1"" which makes the number \<unk\> was previously assigned to non-existent in the dictionary and causes trouble during lookup. 

The following demonstration explains why we can get an error when we refer to the dictionary later on. 

```
import tensorflow as tf
tf.enable_eager_execution()
import re
import numpy as np

sent = ""<start> Alice in wonderland. <end>""
sent2 = ""<start> When suddenly Alice saw a White Rabbit. <end>""
x_train = [sent, sent2]

top_k = 5
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k, 
                                                  oov_token=""<unk>"", 
                                                  filters='!""#$%&()*+.,-/:;=?@[\]^_`{|}~ ')
tokenizer.fit_on_texts(x_train)
train_seqs = tokenizer.texts_to_sequences(x_train)
tokenizer.word_index = {key:value for key, value in tokenizer.word_index.items() if value <= top_k}
print(tokenizer.word_index)
idx = tokenizer.word_index['<unk>'] #saving the original index of '<unk>'
tokenizer.word_index[tokenizer.oov_token] = top_k + 1 # the problematic line
print(tokenizer.word_index)
index_word = {value:key for key, value in tokenizer.word_index.items()}
index_word[idx]
```

`Output:`
```
{'<unk>': 1, '<start>': 2, 'alice': 3, '<end>': 4, 'in': 5}
{'<unk>': 6, '<start>': 2, 'alice': 3, '<end>': 4, 'in': 5}
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-11-55bbbd305835> in <module>()
     11 print(tokenizer.word_index)
     12 index_word = {value:key for key, value in tokenizer.word_index.items()}
---> 13 index_word[idx]

KeyError: 1
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-58-03bc9960ded7> in <module>()
     19             for i in range(1, target.shape[1]):
     20                 # passing the features through the decoder
---> 21                 predictions, hidden, _ = decoder(dec_input, features, hidden)
     22 
     23                 loss += loss_function(target[:, i], predictions)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    755       if not in_deferred_mode:
    756         self._in_call = True
--> 757         outputs = self.call(inputs, *args, **kwargs)
    758         self._in_call = False
    759         if outputs is None:

<ipython-input-54-b844d20e3fc2> in call(self, x, features, hidden)
     16 
     17     # x shape after passing through embedding == (batch_size, 1, embedding_dim)
---> 18     x = self.embedding(x)
     19 
     20     # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    755       if not in_deferred_mode:
    756         self._in_call = True
--> 757         outputs = self.call(inputs, *args, **kwargs)
    758         self._in_call = False
    759         if outputs is None:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/embeddings.py in call(self, inputs)
    175     if dtype != 'int32' and dtype != 'int64':
    176       inputs = math_ops.cast(inputs, 'int32')
--> 177     out = embedding_ops.embedding_lookup(self.embeddings, inputs)
    178     return out
    179 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in embedding_lookup(params, ids, partition_strategy, name, validate_indices, max_norm)
    311       name=name,
    312       max_norm=max_norm,
--> 313       transform_fn=None)
    314 
    315 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/embedding_ops.py in _embedding_lookup_and_transform(params, ids, partition_strategy, name, max_norm, transform_fn)
    131     if np == 1 and (not transform_fn or ids.get_shape().ndims == 1):
    132       with ops.colocate_with(params[0]):
--> 133         result = _clip(array_ops.gather(params[0], ids, name=name),
    134                        ids, max_norm)
    135         if transform_fn:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in gather(***failed resolving arguments***)
   2671     # TODO(apassos) find a less bad way of detecting resource variables without
   2672     # introducing a circular dependency.
-> 2673     return params.sparse_read(indices, name=name)
   2674   except AttributeError:
   2675     return gen_array_ops.gather_v2(params, indices, axis, name=name)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in sparse_read(self, indices, name)
    756         tape.variable_accessed(self)
    757       value = gen_resource_variable_ops.resource_gather(
--> 758           self._handle, indices, dtype=self._dtype, name=name)
    759     return array_ops.identity(value)
    760 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py in resource_gather(resource, indices, dtype, validate_indices, name)
    611       else:
    612         message = e.message
--> 613       _six.raise_from(_core._status_to_exception(e.code, message), None)
    614 
    615 

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: indices[37,0] = 5001 is not in [0, 5001) [Op:ResourceGather] name: rnn__decoder_1/embedding_1/embedding_lookup/
```",True,"[-3.14258069e-01 -3.45483065e-01 -2.66752481e-01 -1.96218327e-01
  3.66354913e-01 -3.26870441e-01 -1.32446885e-01  1.78812265e-01
 -2.45384932e-01 -1.66227520e-02  7.38499090e-02  2.18063686e-02
 -1.12380385e-01  1.10892937e-01 -3.47622931e-02  3.44950676e-01
 -6.91345111e-02 -2.56661177e-01  2.13179827e-01  1.37528121e-01
  1.47304296e-01 -1.80143729e-01  1.45023808e-01 -1.05441883e-02
 -3.78933772e-02  7.02249110e-02  1.29955485e-01 -1.45888358e-01
  7.33689964e-02 -5.58004901e-02  3.64736259e-01  2.83500344e-01
 -6.50044531e-04 -2.10398007e-02  1.50878668e-01  2.11049080e-01
 -4.34528321e-01 -4.87732515e-02 -9.88188908e-02 -2.22704470e-01
  1.33639678e-01  9.95975062e-02  3.30951095e-01  6.58966154e-02
  3.55328210e-02  2.54611187e-02 -7.38476217e-02  1.53649122e-01
 -1.19943075e-01 -2.52455503e-01  1.26890957e-01 -6.01149239e-02
 -3.07984501e-01 -2.21739709e-01 -1.11498073e-01  4.32387963e-02
 -1.53320193e-01 -6.78335279e-02  1.16857402e-01  1.89836740e-01
  1.43996179e-01  9.46410596e-02 -1.48390993e-01  8.84347185e-02
  3.66962194e-01  1.56732112e-01  2.02807203e-01 -1.60848320e-01
  2.57924676e-01  8.54686648e-02  9.71566886e-02 -7.95123074e-03
 -2.11897880e-01  7.05351084e-02  2.12858647e-01  8.07932764e-02
 -1.44813269e-01  2.39644825e-01 -2.04269662e-02  1.24265626e-03
 -9.94963571e-03 -1.87951133e-01  2.99006611e-01 -3.90515104e-02
 -9.02161598e-02  1.03986025e-01  4.42084223e-02  7.41179883e-02
 -1.62924021e-01 -3.24923515e-01  2.34479427e-01  1.99341834e-01
  4.84972112e-02  1.36188745e-01  1.05371416e-01  1.67110443e-01
 -3.15435193e-02 -2.99776420e-02 -3.65171641e-01 -2.11739734e-01
  4.02898714e-02 -9.34176147e-02 -1.29545197e-01  1.49584264e-01
 -1.18016049e-01 -1.78094864e-01  1.82379022e-01  8.51343274e-02
 -1.52722239e-01  1.26723692e-01  2.56290603e-02  1.49133712e-01
 -2.06579447e-01  1.04513563e-01  2.82499522e-01  6.92252889e-02
 -1.46503448e-01 -1.83614746e-01 -1.90188497e-01  3.72641444e-01
 -1.94900930e-01  3.11202891e-02 -8.59667212e-02  3.72741697e-03
  9.12648290e-02  2.13024661e-01 -1.37786921e-02  1.48708418e-01
 -1.40654862e-01 -8.63845795e-02  8.91759545e-02  1.49805889e-01
  3.34778316e-02 -9.00028348e-02  1.11715831e-01  1.67487592e-01
  2.29664911e-02  1.83692947e-01 -1.36416346e-01 -7.14007020e-02
  1.71600208e-02  3.09303939e-01  1.70416236e-01 -1.18487701e-01
 -9.77119356e-02 -1.83803603e-01  5.66030182e-02  2.03738868e-01
 -8.82211626e-02 -1.75239533e-01  8.81904438e-02  2.73680873e-02
  3.19459438e-02  5.58126211e-01  2.59328745e-02  4.29203391e-01
  2.14642808e-01  7.93410093e-02  3.90326791e-02 -2.65526175e-01
 -1.67982802e-01  5.69977984e-02  4.28133756e-02 -9.01295617e-02
 -1.31553888e-01 -7.77276084e-02 -2.31255263e-01 -5.19466959e-02
 -2.74481058e-01  1.63930029e-01 -3.07949543e-01  3.00038420e-02
 -7.40593970e-02  1.55210957e-01  4.11342792e-02 -2.17931047e-01
  2.28004590e-01 -2.18610853e-01 -3.37051183e-01  2.79764626e-02
 -2.35394947e-02 -1.14268094e-01  2.72998512e-01 -2.79766560e-01
 -8.47890824e-02  1.36574805e-01  2.83359528e-01  4.63480651e-02
  1.22987084e-01  9.47949104e-03 -2.21808255e-01  7.48204142e-02
  1.15534902e-01  8.10554922e-02 -1.21438801e-01 -1.42873302e-01
  2.43030638e-01 -1.01133555e-01  5.00627011e-02 -1.61492880e-02
  1.68906689e-01  1.04302704e-01 -7.40317907e-03  2.99023420e-01
  1.81617811e-02 -8.14474747e-02 -1.94249377e-01 -4.34978679e-02
 -2.22236395e-01 -4.88374457e-02  5.78509122e-02 -2.26363376e-01
 -1.54508561e-01 -7.47735947e-02 -1.31888300e-01 -1.05087563e-01
 -1.21753603e-01  1.59826279e-01 -1.27132088e-01  2.85180330e-01
 -1.03398167e-01 -1.09347075e-01  2.27177236e-02 -3.39251995e-01
 -1.99398220e-01  1.00988761e-01 -1.89366102e-01 -1.78100199e-01
 -1.03178024e-01  1.51052803e-01 -7.01666400e-02  1.81435570e-02
  3.38869810e-01 -6.48680180e-02 -1.14726104e-01  2.81092882e-01
 -3.19658399e-01 -1.18084431e-01 -8.89922976e-02  2.40169048e-01
 -1.85265467e-01 -1.84145987e-01 -8.60730782e-02  9.11499783e-02
  2.08585383e-03  1.19974799e-01  7.75197074e-02  1.72027767e-01
 -8.86488240e-03  1.51736528e-01  3.91104519e-02 -1.53433099e-01
  3.33149195e-01 -2.95224730e-02  1.62103742e-01  7.25344792e-02
  2.11865053e-01  7.36645311e-02  1.13827676e-01 -1.98897086e-02
  2.28511527e-01  2.22514588e-02 -9.31158476e-03  1.31369501e-01
  3.41418594e-01  3.09342623e-01 -3.46031129e-01 -4.90708053e-02
  2.79857993e-01 -2.56291211e-01 -1.15236379e-02 -2.54974246e-01
  3.66401374e-01 -1.71978414e-01  1.25985727e-01 -1.41978830e-01
  5.21959186e-01 -7.64624774e-03 -1.01692021e-01  2.12962672e-01
  3.87314141e-01  7.45898262e-02  3.96699458e-01 -4.24570590e-02
  1.11470714e-01 -3.75941038e-01 -1.89809397e-01 -7.20898449e-01
 -2.81005859e-01 -3.49123478e-01  6.81308806e-02  2.54019015e-02
  4.37581837e-01  2.04776704e-01 -2.70789862e-01  1.14390254e-01
  9.11404043e-02  1.07071817e-01 -9.81177986e-02  3.43586355e-01
 -1.76707089e-01 -1.69159174e-01  7.21849576e-02 -4.18468714e-02
 -2.50411391e-01  2.46267036e-01  1.08074553e-01 -8.12963024e-02
  6.54477060e-01 -2.90293097e-01  1.70174450e-01 -9.30567086e-02
 -1.92599997e-01 -5.96633367e-02 -9.74475220e-02  1.85711056e-01
 -2.86027193e-01  6.53950214e-01  3.88277054e-01  2.50117332e-02
 -7.48133566e-03 -2.72357821e-01 -1.18173584e-01  8.99139717e-02
  8.02147612e-02 -2.50134654e-02 -1.79935157e-01 -1.18321404e-01
  7.05141649e-02  1.21880256e-01  1.12583518e-01  1.12808999e-02
 -1.11492507e-01  2.84702390e-01 -3.56528163e-01  3.01570306e-03
 -2.27887541e-01  2.90281534e-01 -9.79020894e-02 -1.41423583e-01
  6.32623658e-02 -2.60451436e-01 -5.30542200e-03 -3.45706135e-01
 -2.22163185e-01 -2.96038449e-01  3.78543258e-01  2.97155857e-01
 -1.06879070e-01 -1.23559861e-02 -4.11849022e-02 -3.82465497e-02
  5.40093333e-02 -2.94999592e-03 -3.83476853e-01  1.36486948e-01
  8.92486870e-02 -1.47570655e-01  2.44493484e-01  3.39715660e-01
 -3.20271969e-01 -2.37401098e-01 -2.70691633e-01 -1.02214754e-01
  1.86855882e-01 -1.11913726e-01  2.99572833e-02 -1.10145152e-01
  1.50844678e-02  1.78281248e-01 -1.83305323e-01 -9.53103602e-03
 -1.44515559e-01  1.27389148e-01  3.96434337e-01 -2.93211132e-01
 -1.19011067e-01  1.51727498e-01 -1.38195958e-02 -6.20631836e-02
 -8.01836178e-02  3.96558363e-03  2.03666072e-02 -2.51307994e-01]"
`tf.keras.layers.Bidirectional` does not work under eager execution mode type:bug comp:eager,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.12.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): BINARY
- TensorFlow version (use command below): v1.12.0-rc0-17-g7b08198113 1.12.0-rc1
- Python version: 3.6.5
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**

A very simple model with `Bidirectional` layer will stop work after we enable the eager execution mode.

**Describe the expected behavior**

Code should work with eager execution mode.

**Code to reproduce the issue**

The following code will be able to reprorduce this problem:

```py
import tensorflow as tf
import numpy as np

tf.enable_eager_execution()

x = tf.keras.layers.Input(shape=(2, 1))
y = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(1, return_state=True)
)(x)

model = tf.keras.Model(inputs=x, outputs=y)

data = np.array([0.1, 0.2]).reshape((1, -1, 1))
print(model.predict(data))
```
However, if we comment out the `tf.enable_eager_execution()`, then it will work as expected:

**Other info / logs**

The error message(with eager execution enabled) is:

```
Traceback (most recent call last):
  File ""src/t.py"", line 11, in <module>
    )(x)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 473, in __call__
    return super(Bidirectional, self).__call__(inputs, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 769, in __call__
    output_shapes = self.compute_output_shape(input_shapes)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py"", line 149, in wrapper
    output_shape = fn(instance, input_shape)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 444, in compute_output_shape
    input_shape).as_list())
AttributeError: 'list' object has no attribute 'as_list'
```

After we disable the eager execution, then the above code works as expected.

```
[array([[0.01535364, 0.04371894]], dtype=float32), array([[0.01535364]], dtype=float32), array([[0.03121366]], dtype=float32), array([[0.04371894]], dtype=float32), array([[0.09061633]], dtype=float32)]
```",True,"[-0.37379533 -0.424186   -0.19110496 -0.22274837  0.06734701 -0.36240217
 -0.09594955 -0.12030524 -0.17189214  0.05296642  0.08265528  0.02430843
 -0.19608942  0.02793721 -0.04180722  0.29777205  0.05225664 -0.28358176
  0.23413676 -0.03182184  0.20638222 -0.14487255 -0.07532907  0.12888113
  0.1551281   0.11530441 -0.02816145 -0.14258508  0.03022084 -0.04458718
  0.22006553  0.14164627 -0.16889408 -0.06713168 -0.01517664  0.21029055
 -0.19324604 -0.10700498 -0.35015178 -0.23950003  0.10169652 -0.01170189
 -0.04620844  0.08724806  0.07793686  0.11907325  0.10795313  0.00954198
 -0.19494829 -0.16632283  0.01419882  0.04655781 -0.23778802 -0.15877202
 -0.10389847  0.0618043  -0.0589605   0.16160306  0.08851717  0.15667883
 -0.09781519  0.01911448 -0.19421348  0.28826684  0.26414788  0.03591255
  0.30659676  0.01369788  0.3255577  -0.06371175  0.01813951 -0.03784265
 -0.37922275  0.20389366  0.24869436  0.19131592 -0.14379731  0.14279738
  0.20116365 -0.16179056  0.18983117 -0.13385022  0.11946934 -0.01976063
  0.03564709 -0.11352258  0.20360716  0.06384158  0.11551827 -0.06523214
  0.32889992  0.12771119 -0.12174961  0.07555357  0.33198082 -0.07217547
  0.0837401   0.16592649 -0.21361676  0.08116648 -0.03146186 -0.08839296
 -0.19560719  0.18788573  0.09751526 -0.12584443  0.06035212 -0.20009388
  0.06393268  0.06847455  0.0176613   0.03099314 -0.06014941 -0.16833426
  0.08387205 -0.10150386 -0.15040536  0.05241533  0.05612896  0.26619315
 -0.14680791 -0.33629125 -0.08893076  0.19635972  0.30195028  0.2233952
 -0.06741849  0.15757462  0.04162868  0.02086158  0.15622872  0.02207505
 -0.15918867  0.00079603  0.16951467  0.15146762 -0.22137922  0.04088919
 -0.41792926 -0.21609217 -0.03053461  0.36278355  0.09149954 -0.18142335
  0.14537506 -0.07424683  0.00142758  0.12249336 -0.04769813 -0.04395439
 -0.14276893 -0.10493819  0.00098196  0.19103423  0.1840604   0.1850617
  0.12116958  0.02512332 -0.1479606  -0.41203943  0.07592229  0.08075989
 -0.11999513  0.05195682  0.22233313  0.01517631 -0.2565282  -0.20139843
 -0.06340931  0.18407057 -0.16025977  0.03689296 -0.19082215  0.26021278
  0.28730005 -0.07043831  0.28995603 -0.38800102 -0.10512227 -0.0260465
  0.12971306 -0.01365774  0.08912799  0.09281412 -0.25190386  0.00690104
  0.17041335  0.13027003 -0.2977897   0.05459823 -0.3897112  -0.02195202
  0.11165937  0.08673272  0.05343578 -0.0187009   0.22173405 -0.35589856
 -0.1610668   0.20507798  0.03808566 -0.30283654 -0.10401988  0.084834
 -0.05160198 -0.21157819 -0.24104688 -0.01222674 -0.19890022  0.0523747
  0.2160829  -0.17493987  0.09172496  0.07540727 -0.1988108  -0.09474623
  0.00224808  0.04136026 -0.29908857  0.27696115  0.23204947 -0.0367268
  0.06306542 -0.2534451  -0.18310308 -0.37587798 -0.23569867  0.23154128
 -0.05381934  0.3138172  -0.09946227  0.01476982  0.18527177 -0.02276732
 -0.06625589 -0.09154857  0.12880194 -0.26745397 -0.11401865  0.09142543
 -0.68480283 -0.08928448  0.06122683 -0.02756956 -0.14141986  0.16524172
 -0.04462772  0.11196605 -0.12199691  0.28239536 -0.03001893  0.04742192
  0.21171865  0.2491284   0.11842182  0.06503922 -0.10586169  0.05093228
  0.18246947  0.35179895  0.29157645  0.32213888  0.12828961  0.48183656
  0.24420488  0.3225742  -0.25828132  0.18586808 -0.02162124 -0.12278359
 -0.2293928  -0.25595888  0.20366475 -0.11773409  0.06474826 -0.06373563
  0.3496225   0.00828664 -0.01812671  0.17504397  0.07575945  0.10278467
 -0.00805802  0.16977482  0.17896529 -0.35768637 -0.091433   -0.6455013
 -0.00834879 -0.09828679  0.13580827 -0.00852979  0.28529036  0.01476096
 -0.09022636 -0.05708948  0.05202897  0.00148056  0.28116646  0.1794163
 -0.02840485 -0.02881378 -0.0270635   0.1314562  -0.04987214  0.05408312
  0.3279735   0.1464189   0.39596123 -0.00279676  0.21654029  0.20007089
 -0.02605303  0.18320152 -0.10372324  0.09590355 -0.12569404  0.48795885
  0.02326485  0.01313155  0.1709746  -0.19437295 -0.141223   -0.05069169
 -0.09128274 -0.0075148  -0.19818105 -0.14258674 -0.01947653 -0.14583433
 -0.06355446 -0.21957862 -0.23118852  0.21158564 -0.15050308 -0.04294707
 -0.25276244  0.40870726 -0.19377014 -0.23516801 -0.03159398  0.05285437
 -0.08959576 -0.18117951  0.00229112 -0.1647892   0.1530684   0.24268284
  0.03472985  0.08539297 -0.3633082   0.03963932 -0.20357162 -0.01006639
 -0.06913413  0.2581059  -0.16149023 -0.06434111  0.1055841   0.31978416
 -0.21928494 -0.07288441 -0.11898871 -0.15654817  0.03721058 -0.03834359
 -0.02077652 -0.10553609  0.12005924  0.3961668  -0.06989574  0.29105675
 -0.325528    0.17702067  0.3722188  -0.2691413  -0.13797331 -0.03242902
  0.06521306  0.00179292 -0.15948689 -0.03198063 -0.199626   -0.01314984]"
EMA's support for DistributionStrategy introduces worse performance comp:dist-strat,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Cent OS 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: None
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: master
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.15
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 7.0
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

**Describe the current behavior**
This [PR](https://github.com/benjamintanweihao/tensorflow/commit/70af5e7ddcef853adb8af6355341bb8a97dcf736) add DistributionStrategy support to moving average APIs. However, **in MirroredStrategy**, this patch's implementation will introduce expensive MEMCPYPtoP cost, as `strategy.reduce(MEAN, value, v)` will eventually call strategy.broadcast(reduced, devices), and this get even worse when there are more devices used. Is this as expected as this implementation is just prepared for GENERAL distribution strategy? Thanks in advance. 


",True,"[-0.3529222  -0.5291858  -0.10739107 -0.13895872  0.02856894 -0.35636014
 -0.2212405  -0.01969729 -0.12027363 -0.25267917  0.2833517   0.28985807
 -0.01340347 -0.07592346 -0.38145065  0.21983454  0.3106483  -0.44297805
  0.21978591  0.15507965 -0.00848883 -0.3481152  -0.14717975  0.03276078
  0.03746708  0.21614815 -0.36730865 -0.19328463  0.12172414 -0.11370823
  0.8014853   0.17945683  0.0699434   0.09158246 -0.14685649  0.17031664
 -0.1356843  -0.21916799 -0.06089522  0.01638222 -0.07403283  0.08190072
 -0.14429179  0.13321003 -0.08928101 -0.01036391  0.16699097 -0.20587604
 -0.29020935 -0.316161    0.2994272   0.09442868 -0.33811495 -0.20373926
 -0.11301775 -0.14871961 -0.02161602  0.02496471 -0.06652714  0.22383463
  0.15199268  0.07658675 -0.31453824  0.14197883  0.08617968  0.21071813
  0.30863038 -0.10398197  0.24877721  0.1516388  -0.2129628  -0.03740515
 -0.24969003  0.09067778  0.13152748  0.5616434   0.16180663  0.08459261
  0.22297667 -0.20868436  0.5565842  -0.28295004  0.09454691 -0.06067784
  0.10423754 -0.06665528  0.2655382   0.12263633  0.1150202  -0.37920544
  0.517388    0.17887484 -0.03884501  0.30371708  0.3043901   0.2944757
 -0.25367877 -0.05579478  0.00666862  0.05054133 -0.12252254  0.0822479
 -0.12945652  0.11842737 -0.26260215 -0.5091945   0.21169595 -0.19372939
 -0.11779888  0.36002636 -0.09366731 -0.0898625  -0.06905327 -0.00991818
 -0.08483752  0.05591038 -0.17374152  0.16451734  0.11094929  0.19536024
 -0.07923714 -0.13309115  0.19718908  0.04466316  0.67589533  0.15211555
  0.04328944 -0.11479207 -0.17199051 -0.18820976  0.1654128   0.03961849
  0.06529815  0.18554406 -0.07120097 -0.09566781 -0.05827547 -0.10398504
 -0.34736454  0.00672684 -0.18232799  0.48037034  0.186243    0.01820005
 -0.05804588  0.05217402  0.06899214  0.01969212 -0.00535498 -0.23608658
 -0.05607313 -0.27483368 -0.02988233  0.21400484  0.14321612  0.1629445
  0.32420617 -0.14434928 -0.03568988 -0.43971696 -0.04963244  0.05970129
 -0.03218243  0.11878303 -0.11362007  0.12907262 -0.30125862 -0.22932912
 -0.08976661  0.39967912 -0.41142815  0.04683845 -0.01473285  0.19955415
  0.36929432 -0.32572046  0.11627679 -0.27486655 -0.22276175 -0.05447235
 -0.08292805 -0.06149293  0.24595967 -0.04328235 -0.02492686 -0.06715871
  0.2877621   0.12271906 -0.22707611 -0.17170191 -0.339956   -0.4255229
  0.04492586 -0.01593037 -0.15338382 -0.16439143  0.16229129  0.30116457
 -0.03432324  0.17783143 -0.07540345 -0.30098343  0.02212323 -0.01705774
  0.15483008 -0.00540534 -0.35130525 -0.00297719 -0.5437709  -0.11129889
  0.10305557 -0.32626417  0.05875907  0.18806209 -0.33528227  0.07239142
  0.07896203  0.09487837 -0.06721535  0.17559075  0.0913626  -0.12327561
  0.21824142 -0.2822118  -0.3484639  -0.0169057  -0.09829383  0.14777079
  0.11078969  0.27794817 -0.03148986  0.29108614  0.10150096  0.19797587
 -0.12309738 -0.06056574  0.17779946 -0.23102102 -0.06183548  0.0050365
 -0.49042347 -0.17451319  0.25411206 -0.17349467  0.0803584   0.4212606
  0.01493743  0.32045686 -0.00201991  0.19528353 -0.22492701 -0.02086117
  0.237347    0.04595123  0.13088346  0.05242906  0.01159127  0.08367421
  0.13677885  0.18171388  0.26865128  0.4792283  -0.20285687  0.33099765
  0.33916837  0.42530972 -0.2522802   0.14861465  0.16577007 -0.17388208
 -0.08676302 -0.36992127  0.4998934  -0.50421464  0.1589136   0.17262462
  0.30356312 -0.10111313  0.18665293  0.16032243  0.03734993 -0.15947455
 -0.15123212  0.19191073  0.07451037 -0.45864618 -0.05863965 -0.44521588
 -0.3624081  -0.0159606   0.17575541  0.13247472  0.0756515   0.3788349
 -0.22140457  0.08619281 -0.09643024 -0.02212326  0.15876679  0.06484876
 -0.23964944  0.05196453 -0.10439791  0.11295682 -0.3844927   0.13914886
  0.11811233  0.01380499  0.26760614 -0.1179477   0.18100375 -0.03983069
 -0.02940025  0.20360386 -0.20860429  0.31628066 -0.33913648  0.6654933
  0.23191252 -0.0370324   0.1586264  -0.17781036  0.10842007  0.09205726
 -0.06352128 -0.07917149 -0.21817346 -0.05843537  0.19103917  0.11592495
  0.12001096 -0.06050713 -0.28838176  0.38142854 -0.08504236  0.20018883
 -0.15626392  0.40360036 -0.07645272 -0.27473918  0.04205469  0.19567095
 -0.01866003 -0.3445089   0.06067713 -0.02783906  0.45923027  0.25453147
 -0.3695256   0.11186543 -0.08400647 -0.12120233 -0.26428866 -0.21752352
 -0.49591118  0.1770927   0.19475436 -0.23228875  0.08341588  0.49668086
  0.08794521 -0.17381613 -0.17610705 -0.02485309 -0.03711371 -0.15406886
 -0.05879893 -0.20522892  0.15282054  0.40943652  0.16823512 -0.02049058
 -0.30813664  0.0264434   0.51700556 -0.59523803 -0.3896323  -0.00930599
 -0.02974821  0.10465732 -0.20883408 -0.27231058  0.16159931  0.13925105]"
"Unable to disable build of AWS, HDFS, Kafka and GCP ","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: any
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: any
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: ./configure

### Describe the problem
When building TensorFlow as a library (and in our case using `--config=monolithic`), we used to remove as much as possible of the not used features. Recently, this was removed from master in https://github.com/tensorflow/tensorflow/commit/d56c298f1ef14b5a738e1e0b7bbc66fcd736be3e for AWS, HDFS, Kafka and GCP.

I understand that that having to deal with multiple optionnally-disabled features can be a huge burden for the future, but is it possible to have some way around ?",True,"[-0.25953898 -0.42502075 -0.34720883 -0.16813543  0.3318419  -0.31007758
 -0.15083036 -0.20440684 -0.35651422  0.01892887  0.15875664  0.18940383
 -0.19886287 -0.22866492 -0.3346976   0.17730522 -0.0891127  -0.33046442
  0.1919859   0.09678598 -0.19499016 -0.09117111 -0.22263691  0.29758945
 -0.04676991  0.24413095 -0.09727154 -0.14295848  0.08887959 -0.08318589
  0.73098564  0.02342679  0.1006611   0.13600823  0.16098753  0.28201813
 -0.02800732 -0.0612068  -0.10826015 -0.20646514  0.05688752 -0.08752652
  0.1636732   0.00401324 -0.07276576  0.07803529 -0.04914416  0.04363137
 -0.07512581 -0.27735716  0.10698794 -0.10447299 -0.37573498 -0.23896821
 -0.17679161 -0.02035042  0.14593786  0.1340112   0.0053251   0.3218474
  0.16059381 -0.06051419  0.04302171  0.15796196  0.12346178  0.3170942
  0.3686602  -0.19231883  0.47341287  0.25665808  0.02340566  0.05627562
 -0.321715    0.1310081   0.15074694  0.4325902   0.00776273  0.15870962
  0.32407925 -0.05389326  0.16705659  0.00670454  0.3404491  -0.19065443
 -0.10868803  0.09167775  0.22569641  0.20209818  0.16705047 -0.2677433
  0.14688978  0.18060166  0.19114384  0.1755617   0.05698999  0.12675545
 -0.14472592  0.061551   -0.15336886 -0.08244034  0.03643053 -0.06360452
 -0.01824402  0.2288177  -0.22482184 -0.08058924  0.2557069   0.0685439
 -0.10098895 -0.09185024  0.05786755  0.06778258 -0.02093516 -0.08921236
 -0.05833116  0.08236938  0.01379367 -0.05657262 -0.19892895  0.40297693
 -0.17832221  0.00244599  0.25802398  0.0318738   0.4181675   0.0494278
 -0.1919367   0.09645921 -0.01194029  0.12094288  0.04661471  0.04182704
 -0.0195065   0.02668235  0.02110874  0.05446709 -0.16030216  0.11320332
  0.01710369  0.20225358  0.03762445  0.19799073  0.18239623 -0.14387433
  0.21440199 -0.09998485  0.23627731  0.08790802  0.0658544  -0.22974542
 -0.09832362 -0.1366949   0.22368835  0.4278739   0.12815897  0.3156337
  0.23589869 -0.05581698 -0.06805293 -0.52386415 -0.28103575  0.08932974
 -0.20497343 -0.01776269 -0.05565339 -0.01909475 -0.23872846 -0.04542338
 -0.20883259  0.19495985 -0.28275502  0.03477907 -0.001769    0.22739111
  0.20332433 -0.12437454  0.32626534 -0.5412451  -0.1604959   0.15972671
 -0.0669734  -0.1092499   0.10797936 -0.22017953  0.04564343  0.01093483
  0.0550417   0.14882496 -0.05804864 -0.15671578 -0.2977903   0.06879352
 -0.08201851 -0.25942165 -0.14830223 -0.01902202 -0.0318274  -0.05340453
 -0.02850304 -0.08904866 -0.00890549 -0.22994219 -0.27616304  0.14922568
 -0.11196341 -0.15352826 -0.44312593  0.13418624 -0.33431017  0.05004527
  0.17037216 -0.2708855  -0.05239576  0.05791375 -0.31203505 -0.15645036
  0.01300977 -0.06103715 -0.30797154  0.34177613  0.36381543 -0.17805776
  0.08405495 -0.31268823 -0.20535728 -0.30126685 -0.16678685  0.01920205
  0.00883688  0.34271747  0.00983212 -0.13377056  0.07324433 -0.06940575
  0.09136636  0.04044116 -0.0753592  -0.38518816 -0.03828174  0.19425875
 -0.52567464 -0.16297823  0.09882508  0.05460934  0.1889858   0.21989468
 -0.17890376  0.46877575 -0.18080975  0.11686165 -0.10223203 -0.15662885
  0.2990834   0.36966735  0.06538139  0.1625601   0.18641251  0.07175206
  0.15034051  0.04582022  0.15367593  0.3938865  -0.03470379  0.3329659
  0.39687273  0.5753443  -0.18793048  0.1831871  -0.08835167 -0.21862149
  0.01577171 -0.10832816  0.28966248 -0.18490537 -0.04841584  0.05247053
  0.39848602  0.05787388 -0.06112925  0.257048    0.11450526 -0.03232351
  0.02417637  0.06108448  0.11335644 -0.4509765  -0.23770405 -0.51207197
 -0.11465463 -0.13277182  0.08717575  0.1376494   0.17611635  0.2495629
 -0.02843949 -0.10551272 -0.07277817  0.2517007   0.03904516  0.25697047
 -0.15612194 -0.00855638  0.16263196  0.0430105  -0.01376722  0.04103319
  0.22198822  0.06489781  0.62165767 -0.20280737  0.0113503  -0.09815388
 -0.06094226  0.31238723 -0.14746088  0.03279442 -0.26119637  0.5159015
  0.04908428 -0.05394037  0.14220315 -0.36768144  0.00747737 -0.01128549
 -0.1907142  -0.15800574 -0.1856501  -0.1423691  -0.07009993  0.10997626
 -0.17262498 -0.15932661 -0.21119328  0.27571175 -0.23380882  0.02160285
 -0.12277307  0.33502224 -0.04090524 -0.13787019 -0.17377771 -0.1377519
 -0.08430868 -0.30055064 -0.09498502 -0.09043692  0.5901725   0.06742987
 -0.11486956  0.34278744 -0.33800057 -0.1356734  -0.3156724  -0.14531599
 -0.31560296  0.19958866  0.13791698 -0.07052496  0.18133101  0.37080476
 -0.19825912 -0.07680888 -0.31974697 -0.18450058  0.03188927 -0.25903726
 -0.15649246 -0.14469184  0.22234523  0.44118845  0.00774481  0.09725221
 -0.2361185   0.25716186  0.49409506 -0.2934562  -0.2291592  -0.03098968
  0.01211922 -0.12326004 -0.07960891 -0.08835007  0.02973855 -0.07420668]"
Failed to build TF from source with MPI support type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04 LTS
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0-rc0
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.17.1
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: 9.2/7.2
- **GPU model and memory**: Asus GTX 1080Ti
- **Exact command to reproduce**: See below

### Describe the problem
I canno't seem to compile tensorflow from source when I try to enable MPI support. The build always fails.
```
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by com.google.protobuf.UnsafeUtil (file:/home/stefan/.cache/bazel/_bazel_stefan/install/28c1d2ace0add449e21862ae9f2d2289/_embedded_binaries/A-server.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of com.google.protobuf.UnsafeUtil
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.17.1 installed.
Please specify the location of python. [Default is /home/stefan/.tmp/venv3/bin/python]: 


Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AttributeError: module 'site' has no attribute 'getsitepackages'
Found possible Python library paths:
  /home/stefan/.tmp/venv3/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/stefan/.tmp/venv3/lib/python3.6/site-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: 
Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: 
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon AWS Platform support? [Y/n]: 
Amazon AWS Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: 
Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with nGraph support? [y/N]: 
No nGraph support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2


Please specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.2


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 


Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:/usr/local/cuda/nccl


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]: 


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: y
MPI support will be enabled for TensorFlow.

Please specify the MPI toolkit folder. [Default is /usr/local]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
Configuration finished
```
This is my configuration. Note that the build succeeds with MPI set to no.
Error message when build fails:
```
ERROR: /home/stefan/.tmp/tensorflow/tensorflow/contrib/mpi/BUILD:60:1: C++ compilation of rule '//tensorflow/contrib/mpi:mpi_rendezvous_mgr' failed (Exit 1)
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:24,
                 from ./tensorflow/core/common_runtime/device.h:43,
                 from ./tensorflow/core/common_runtime/device_mgr.h:24,
                 from ./tensorflow/core/distributed_runtime/worker_session.h:21,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:24,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:35,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                         ~~~~^~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:0:
./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h: In member function 'void tensorflow::MPISendTensorCall::Init(const tensorflow::Rendezvous::ParsedKey&, tensorflow::int64, bool)':
./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:74:36: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'
     mRes_.set_key(parsed.FullKey().ToString());
                                    ^~~~~~~~
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In member function 'virtual void tensorflow::MPIRemoteRendezvous::RecvFromRemoteAsync(const tensorflow::Rendezvous::ParsedKey&, const tensorflow::Rendezvous::Args&, tensorflow::Rendezvous::DoneCallback)':
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:139:38: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'
   mgr->QueueRequest(parsed.FullKey().ToString(), step_id_,
                                      ^~~~~~~~
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In lambda function:
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:261:45: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'
         SendQueueEntry req(parsed.FullKey().ToString().c_str(), std::move(res));
                                             ^~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/contrib/mpi/mpi_utils.h:25,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:34,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 863.199s, Critical Path: 65.99s
INFO: 3852 processes: 3852 local.
FAILED: Build did NOT complete successfully
```
It says that StringPiece has no member ToString.
I installed MPI with:
```
wget https://www.open-mpi.org/software/ompi/v3.0/downloads/openmpi-3.1.2.tar.gz
tar -xvf openmpi-3.1.2.tar.gz
cd openmpi-3.1.2
./configure --disable-mpi-fortran --with-cuda=/usr/local/cuda/ --prefix /usr/local/
make
make install
```
",True,"[-1.79692313e-01 -5.60099661e-01 -2.97442019e-01 -2.75692046e-01
  2.53055573e-01 -2.42136374e-01 -1.67570904e-01 -2.15420015e-02
 -3.14859986e-01  6.28933311e-02  1.39547288e-02  7.34275058e-02
 -1.37647837e-01  1.77297637e-01 -1.64117634e-01  2.80554771e-01
 -7.05684796e-02 -5.57892799e-01  1.81055397e-01  2.60087401e-01
 -2.75152057e-01 -1.29840314e-01 -2.51685023e-01  4.29482907e-01
  5.18656000e-02  2.25503206e-01 -1.70442015e-01 -7.52309430e-03
  1.91193759e-01  2.41158575e-01  5.47946692e-01  8.19078684e-02
 -8.63463357e-02 -2.63884068e-02  9.69795734e-02  2.50040442e-01
 -4.43662256e-02  1.84791312e-02 -2.23693907e-01 -1.38498515e-01
  8.22833478e-02 -6.59881830e-02 -9.69116390e-03  7.26583004e-02
 -1.24363676e-02  1.46333978e-01  1.67505741e-01  7.82758370e-02
 -2.22598061e-01 -1.25099659e-01 -1.21870056e-01  4.28375453e-02
 -4.05452043e-01 -2.38181174e-01 -3.43017280e-02  3.34581137e-02
  2.39143953e-01  2.29332373e-01  1.84737965e-01  3.51889342e-01
  9.03177410e-02 -2.92222705e-02 -9.61280391e-02  1.43161103e-01
 -5.49267158e-02  1.78837106e-01  8.97369310e-02 -1.42857254e-01
  3.56037945e-01 -1.62402213e-01  6.60936236e-02 -1.01518035e-01
 -5.96463606e-02 -5.97766601e-04 -9.75353457e-03  3.07084024e-01
  9.08165872e-02  3.42068195e-01  1.86193123e-01 -2.26936191e-02
  1.94165140e-01 -8.63628983e-02  1.49264246e-01  6.43501207e-02
  8.48120153e-02  1.12225890e-01  2.70953536e-01  3.51216316e-01
  3.33545059e-01 -2.76114076e-01  3.93299699e-01  2.12174565e-01
  1.51938051e-01  1.44814715e-01  3.11118782e-01  2.83590257e-01
 -1.54655501e-01  2.32207581e-01 -1.60428822e-01  3.02535966e-02
 -2.88719423e-02 -3.38269860e-01 -1.74438447e-01  1.28418028e-01
 -2.02543497e-01 -8.39409903e-02  5.56830429e-02  5.58631942e-02
  1.42582327e-01 -1.29376784e-01  1.60405695e-01  9.99126360e-02
 -2.28184685e-02 -1.10361040e-01 -8.56350064e-02  1.17892005e-01
 -1.87688068e-01 -7.23432004e-02  1.27612069e-01  4.09084320e-01
 -8.06794688e-02 -3.32925558e-01  8.47025812e-02 -2.21315637e-01
  4.62031066e-01  1.36127975e-02 -1.99606091e-01 -6.95035234e-03
  8.13528672e-02  6.38288483e-02  2.17368618e-01  2.16441691e-01
 -2.24059477e-01 -3.59944440e-03  9.98904854e-02 -1.45703591e-02
 -2.79761434e-01 -9.98169780e-02 -1.61539719e-01 -2.65959024e-01
 -1.40067935e-01  3.74260306e-01  8.44037160e-02 -1.59921095e-01
  8.89467597e-02 -3.08825076e-02 -7.67770782e-02  2.61455655e-01
 -7.76386857e-02  2.89406255e-03 -1.53657556e-01 -1.16291970e-01
  2.65804619e-01  4.34597909e-01  1.02009781e-01  1.76040828e-01
  2.07935005e-01 -1.06298298e-01 -1.69854894e-01 -6.00504756e-01
 -3.01034808e-01  9.12336931e-02 -2.17245892e-04 -1.18288193e-02
 -2.84931511e-01  2.06796184e-01 -5.54857790e-01 -1.89282939e-01
  1.48446009e-01  2.30927497e-01 -1.07103199e-01 -2.12449953e-03
  5.06661683e-02  2.97368228e-01  2.24840820e-01 -2.05984563e-01
  5.31560898e-01 -4.33488667e-01 -1.34003073e-01  1.03312418e-01
  1.00089222e-01 -9.67244506e-02  1.49095431e-01 -1.38386875e-01
  1.56985279e-02 -6.88163936e-02  2.50653923e-01  1.41800702e-01
 -8.73719230e-02 -1.40892655e-01 -2.65327394e-01  5.00453077e-02
  1.54665470e-01 -2.52129342e-02 -3.19976628e-01 -9.16643888e-02
  2.07672045e-01 -2.40362193e-02  1.92219049e-01  2.28111982e-01
  3.11989728e-02 -1.02710843e-01 -6.42067641e-02  1.09833308e-01
 -5.53824864e-02 -7.77348727e-02 -2.44583279e-01 -1.22470684e-01
 -4.70935881e-01  3.43085565e-02  2.90924370e-01 -2.46244997e-01
  1.78184807e-01 -9.37193185e-02 -1.93921477e-01 -1.60429806e-01
 -1.14855066e-01 -2.86369860e-01 -1.32675320e-01  2.02166826e-01
  5.57641611e-02 -1.68302476e-01 -7.98703283e-02 -3.84579211e-01
 -2.42531121e-01 -1.71180829e-01 -1.96421966e-01 -8.75052586e-02
  3.11314929e-02  3.74132633e-01 -2.71697491e-01 -4.76315804e-02
  7.18715042e-02  6.86068088e-02  2.03336760e-01 -9.76427123e-02
  5.02101071e-02 -2.84080803e-01 -2.50087500e-01  1.49193794e-01
 -3.51204753e-01 -3.28384101e-01 -3.27544734e-02 -1.93020105e-01
  1.57350600e-01  6.62523061e-02 -4.08742242e-02  3.02711368e-01
 -1.75789043e-01  4.45915580e-01 -3.26800048e-02 -1.65844813e-01
  3.03887516e-01  3.56784821e-01  3.43068361e-01  6.40932545e-02
  2.77894624e-02  2.27571666e-01  3.26340675e-01  6.11570626e-02
  1.71496451e-01  4.14645195e-01  1.99845321e-02  4.10837680e-01
  4.03601497e-01  3.91243547e-01 -4.93249372e-02  2.15468571e-01
 -4.12461758e-01 -2.33589739e-01  3.78362089e-02 -8.58915821e-02
  3.86140466e-01 -2.77892321e-01  4.66797911e-02 -1.09184973e-01
  5.28435290e-01 -7.99988303e-03 -8.67801905e-02  1.44556388e-01
  3.45329791e-01  2.49685064e-01 -1.50087446e-01  9.41718072e-02
  9.85220820e-02 -2.59979665e-01 -2.64562011e-01 -6.77299380e-01
 -3.29968989e-01  1.10232852e-01 -8.35177302e-02  1.76729143e-01
 -9.62640792e-02  5.65390289e-02 -7.58803487e-02 -1.52143240e-01
 -7.17367679e-02  9.43885148e-02  3.28453593e-02  8.34009200e-02
 -1.35976508e-01  4.78586145e-02  2.89315283e-01  2.34864384e-01
 -1.02370381e-02 -4.57463041e-02  3.56829286e-01  4.06307243e-02
  5.33430457e-01 -2.85898209e-01  2.25295603e-01 -1.37952238e-01
 -9.41970479e-03  2.61601567e-01 -1.80893540e-02  3.18721592e-01
 -3.74821842e-01  4.35230553e-01  7.01796860e-02 -1.17928728e-01
  1.91434950e-01 -3.77264917e-01 -3.18292618e-01 -9.13142562e-02
 -2.63110757e-01  2.76383571e-02 -8.55085105e-02 -3.17026913e-01
  1.23766124e-01  1.93987474e-01 -2.65492260e-01 -3.29349220e-01
 -1.69492334e-01  2.11428732e-01 -3.40351284e-01 -1.81143522e-01
 -1.99374080e-01  2.37562224e-01 -5.40808365e-02 -2.92140365e-01
  1.58955082e-02 -2.80410826e-01  3.15969288e-02 -2.27755964e-01
 -9.37632918e-02 -1.03726991e-01  2.33336404e-01  1.16035402e-01
  7.97549635e-02  3.19833070e-01 -1.39092520e-01 -6.73445314e-02
 -4.24761891e-01 -7.42090195e-02 -3.88638079e-01  6.83974847e-02
  4.14825715e-02  4.12690006e-02  2.93000609e-01  3.88532430e-01
  1.25168860e-01 -4.70645092e-02 -2.96773255e-01 -2.02204630e-01
  2.09990710e-01 -3.36411178e-01 -3.82998198e-01 -1.55956507e-01
  8.33802074e-02  3.27995062e-01  1.15124121e-01  4.95347917e-01
 -3.24670285e-01  3.70286256e-02  4.99836743e-01 -3.29193234e-01
 -2.38113672e-01  1.86743066e-01  1.02674969e-01 -3.76003206e-01
 -9.27658901e-02 -1.57324165e-01  8.75950456e-02  1.00374661e-01]"
crosstool_wrapper_driver_is_not_gcc failed: error executing command ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.11.0rc1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: 0.17.1
- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 7.3.0-16ubuntu3) 7.3.0
- **CUDA/cuDNN version**: 9.2 / 7.2.1
- **GPU model and memory**: GeForce 940MX
- **Exact command to reproduce**: bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
**When the VERBS support is enabled the tensorflow build fails with the following error message:
~/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command**

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
ERROR: ~/Documents/dev/git/tensorflow/tensorflow/contrib/verbs/BUILD:90:1: C++ compilation of rule '//tensorflow/contrib/verbs:rdma_rendezvous_mgr' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd ~/.cache/bazel/_bazel_blablabla/cf67b2b2e967476eb2b1ee98e33ab5bd/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    NCCL_INSTALL_PATH=/usr/local/nccl_2.2.13-1+cuda9.2_x86_64 \
    PATH=~/bin:/usr/local/sbin:/usr/local/lib:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/linuxbrew/.linuxbrew/bin:/home/linuxbrew/.linuxbrew/sbin:/home/linuxbrew/.linuxbrew/opt/coreutils/libexec/gnubin:/usr/local/cuda/bin:/usr/local/share/apache/hadoop/sbin:/usr/local/share/apache/hadoop/bin:/usr/local/share/apache/spark/sbin:/usr/local/share/apache/spark/bin:/usr/games:/usr/local/games:~/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=5.0 \
    TF_CUDA_VERSION=9.2 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o' '-DGRPC_ARES=0' '-DPB_FIELD_16BIT=1' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DTENSORFLOW_USE_JEMALLOC -DTF_USE_SNAPPY -DTENSORFLOW_USE_VERBS -DTENSORFLOW_USE_GDR -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/k8-opt/genfiles -iquote bazel-out/k8-opt/bin -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote bazel-out/k8-opt/bin/external/protobuf_archive -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote bazel-out/k8-opt/bin/external/bazel_tools -iquote external/grpc -iquote bazel-out/k8-opt/genfiles/external/grpc -iquote bazel-out/k8-opt/bin/external/grpc -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote bazel-out/k8-opt/bin/external/zlib_archive -iquote external/boringssl -iquote bazel-out/k8-opt/genfiles/external/boringssl -iquote bazel-out/k8-opt/bin/external/boringssl -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/jemalloc -iquote bazel-out/k8-opt/genfiles/external/jemalloc -iquote bazel-out/k8-opt/bin/external/jemalloc -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote bazel-out/k8-opt/bin/external/local_config_sycl -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote bazel-out/k8-opt/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote bazel-out/k8-opt/bin/external/jpeg -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/k8-opt/genfiles/external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/curl -iquote bazel-out/k8-opt/genfiles/external/curl -iquote bazel-out/k8-opt/bin/external/curl -iquote external/jsoncpp_git -iquote bazel-out/k8-opt/genfiles/external/jsoncpp_git -iquote bazel-out/k8-opt/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/k8-opt/genfiles/external/aws -iquote bazel-out/k8-opt/bin/external/aws -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem bazel-out/k8-opt/bin/external/protobuf_archive/src -isystem external/grpc/include -isystem bazel-out/k8-opt/genfiles/external/grpc/include -isystem bazel-out/k8-opt/bin/external/grpc/include -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem bazel-out/k8-opt/bin/external/zlib_archive -isystem external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/genfiles/external/grpc/third_party/address_sorting/include -isystem bazel-out/k8-opt/bin/external/grpc/third_party/address_sorting/include -isystem external/boringssl/src/include -isystem bazel-out/k8-opt/genfiles/external/boringssl/src/include -isystem bazel-out/k8-opt/bin/external/boringssl/src/include -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/k8-opt/genfiles/external/jemalloc/include -isystem bazel-out/k8-opt/bin/external/jemalloc/include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem bazel-out/k8-opt/bin/external/gif_archive/lib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/com_google_absl -isystem bazel-out/k8-opt/genfiles/external/com_google_absl -isystem bazel-out/k8-opt/bin/external/com_google_absl -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/k8-opt/genfiles/external/double_conversion -isystem bazel-out/k8-opt/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/k8-opt/genfiles/external/curl/include -isystem bazel-out/k8-opt/bin/external/curl/include -isystem external/jsoncpp_git/include -isystem bazel-out/k8-opt/genfiles/external/jsoncpp_git/include -isystem bazel-out/k8-opt/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/k8-opt/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -mavx -mavx2 -mfma '-mfpmath=both' -msse4.2 -c tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc -o bazel-out/k8-opt/bin/tensorflow/contrib/verbs/_objs/rdma_rendezvous_mgr/rdma_rendezvous_mgr.pic.o)
In file included from ./tensorflow/core/framework/common_shape_fns.h:22:0,
                 from ./tensorflow/core/framework/resource_mgr.h:24,
                 from ./tensorflow/core/common_runtime/device.h:43,
                 from ./tensorflow/core/common_runtime/device_mgr.h:24,
                 from ./tensorflow/core/distributed_runtime/worker_session.h:21,
                 from ./tensorflow/core/distributed_runtime/base_rendezvous_mgr.h:24,
                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:22,
                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In function 'tensorflow::TensorShape tensorflow::ShapeFromFormat(tensorflow::TensorFormat, tensorflow::int64, tensorflow::gtl::ArraySlice<long long int>, tensorflow::int64)':
./tensorflow/core/util/tensor_format.h:501:45: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
     if (format == FORMAT_NHWC_VECT_W && dim == spatial.size() - 1) {
                                         ~~~~^~~~~~~~~~~~~~~~~~~~~
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc: In member function 'virtual void tensorflow::RdmaRemoteRendezvous::RecvFromRemoteAsync(const tensorflow::Rendezvous::ParsedKey&, const tensorflow::Rendezvous::Args&, tensorflow::Rendezvous::DoneCallback)':
tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:66:41: error: 'using StringPiece = class absl::string_view {aka class absl::string_view}' has no member named 'ToString'
   string key(std::move(parsed.FullKey().ToString()));
                                         ^~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/core/lib/core/refcount.h:22,
                 from ./tensorflow/core/platform/tensor_coding.h:21,
                 from ./tensorflow/core/framework/resource_handle.h:19,
                 from ./tensorflow/core/framework/types.h:31,
                 from ./tensorflow/contrib/verbs/verbs_util.h:21,
                 from ./tensorflow/contrib/verbs/rdma.h:30,
                 from ./tensorflow/contrib/verbs/rdma_mgr.h:24,
                 from ./tensorflow/contrib/verbs/rdma_rendezvous_mgr.h:21,
                 from tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetTensorDim(tensorflow::gtl::ArraySlice<T>, tensorflow::TensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:452:47:   required from here
./tensorflow/core/util/tensor_format.h:420:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attributes.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:420:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:461:54:   required from here
./tensorflow/core/util/tensor_format.h:435:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:435:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 7400.912s, Critical Path: 180.41s
INFO: 10108 processes: 10108 local.
FAILED: Build did NOT complete successfully
```",True,"[-3.79381120e-01 -4.02605802e-01 -4.34094667e-01 -9.47435200e-03
  1.83666319e-01 -3.20768446e-01 -8.18962604e-02 -1.20735690e-02
 -3.03541183e-01 -2.44674116e-01  1.75796878e-02  6.29415289e-02
 -1.62716895e-01 -5.91252036e-02  7.08312616e-02  2.04258859e-01
 -1.41918838e-01  5.99123426e-02  7.90801793e-02 -5.51196821e-02
  1.28054306e-01  4.53500301e-02  8.47821608e-02  1.44915879e-01
 -5.34357950e-02  3.99281867e-02 -4.51260209e-02 -2.26527929e-01
 -6.20811544e-02 -3.10783423e-02  5.34760356e-01 -4.99656424e-03
 -3.05412292e-01 -2.60227442e-01  2.53615916e-01  1.56420812e-01
 -1.29275352e-01 -5.05897641e-01 -3.84958014e-02 -1.53226182e-01
  2.65580148e-01  3.87502052e-02  8.13787878e-02  1.84118003e-01
  4.35921699e-02 -5.56224864e-03 -1.17223494e-01 -2.63783429e-02
  1.79662928e-03 -7.67328143e-02 -1.69241175e-01 -2.63381153e-01
 -4.69512582e-01 -2.41929114e-01  1.18248900e-02 -3.09770763e-01
 -9.02038515e-02  4.30432558e-02  1.26011103e-01  1.99164882e-01
  1.03837714e-01 -1.59251362e-01  4.79452051e-02  2.03247279e-01
  4.24617864e-02 -5.11654168e-02  1.92975223e-01 -2.94828355e-01
  2.36780256e-01  7.74986446e-02 -1.15657814e-01 -6.36396408e-02
 -1.83797687e-01 -1.51040912e-01  1.39617324e-01 -6.68405294e-02
 -3.17738831e-01  4.48125303e-01  1.53980121e-01 -2.17112899e-02
  8.07076395e-02  6.98725358e-02  3.40709209e-01 -1.65830672e-01
  2.42634401e-01  1.64291650e-01 -3.25904936e-02  1.20697625e-01
 -4.00443841e-03 -5.27283587e-02  2.33685404e-01  1.02857888e-01
  1.33122727e-01  2.91777968e-01  2.85304308e-01  5.83618432e-02
  2.99748331e-02  5.87827489e-02 -2.43783787e-01 -3.24944586e-01
 -2.06092149e-01 -1.48666546e-01  1.93159461e-01  1.03181534e-01
 -8.12833831e-02  2.95108140e-01  3.14123482e-01  3.65886897e-01
 -9.38293524e-05 -1.84026569e-01  1.70974463e-01  1.97935477e-02
  1.32176187e-02 -7.31838942e-02 -1.41025484e-01  3.23027670e-01
  1.10316284e-01 -2.58324668e-02 -1.09169692e-01  4.16832209e-01
 -1.83608025e-01 -1.44180357e-01  5.14519960e-02  2.14158565e-01
  2.92128742e-01  5.82366884e-02 -1.95302144e-01  1.71781033e-01
 -1.60838217e-01  1.02341212e-01  1.38274729e-01  1.64850399e-01
 -1.91465896e-02  2.33806580e-01 -7.75822103e-02 -9.30685624e-02
 -5.33025026e-01  3.62876765e-02  1.24582900e-02  6.19878396e-02
  8.38278234e-02  2.30280116e-01 -7.12043941e-02 -3.70522916e-01
  6.99267313e-02  8.19501951e-02 -1.55202866e-01 -4.87366542e-02
  6.07990399e-02 -2.13472307e-01 -6.43050745e-02  7.68995564e-03
  6.19470887e-02  6.19410038e-01  7.43140131e-02  1.56689376e-01
  4.48407531e-01  2.95376405e-03 -1.02325827e-01 -5.18672705e-01
 -2.21522339e-02  3.79230678e-01 -2.51262337e-01  2.10253030e-01
  1.13302477e-01 -1.38977200e-01 -2.54997760e-01 -4.93589267e-02
 -1.40432805e-01 -1.44963115e-02 -1.01823717e-01  1.17825717e-01
 -1.75532192e-01 -1.38913184e-01  4.74583268e-01  1.14826664e-01
  4.81683046e-01 -2.82165587e-01 -2.44360626e-01  2.49799654e-01
  1.19239062e-01 -1.12245433e-01  1.15852579e-02  6.37175050e-03
  1.41814008e-01  2.98953891e-01 -2.42747962e-01 -7.17574209e-02
 -1.25777155e-01 -1.32405236e-02 -2.27287740e-01 -1.22466683e-01
  8.56923684e-02  1.50674015e-01 -1.51472196e-01  1.95225179e-01
  1.09797910e-01 -1.09506264e-01 -1.05403692e-01 -3.91275622e-02
  4.44497913e-03  8.15571100e-02 -8.71285349e-02 -6.64801449e-02
 -2.04185918e-01 -1.01101801e-01  1.17779754e-01 -2.16230139e-01
 -2.37184495e-01 -9.15567130e-02  2.43096016e-02 -2.44123310e-01
 -4.44516420e-01  6.22652769e-02 -4.08457190e-01  3.63144949e-02
  1.01108998e-01  1.93886295e-01 -7.51473382e-02  3.41485113e-01
  2.22987294e-01 -1.12591669e-01  1.33035347e-01 -2.13183194e-01
 -2.99934030e-01  9.80172157e-02  1.80317491e-01  2.59567797e-01
 -1.47827283e-01  1.00427084e-01  6.86644763e-02 -2.61032879e-01
  3.78552288e-01  1.98542640e-01 -2.00656895e-03  6.27546459e-02
 -3.84004340e-02 -5.96696325e-02 -3.22417691e-02 -6.37096614e-02
 -1.93600222e-01 -2.41255894e-01  6.69528767e-02 -1.79760635e-01
 -1.98073462e-02  8.37761313e-02  3.67321335e-02  5.00990897e-02
 -1.96730673e-01  3.73253763e-01 -3.82379517e-02 -3.66389632e-01
  4.09108251e-01  1.05927557e-01  1.10370331e-01  5.40265813e-02
  5.81264980e-02  4.88358140e-02  2.43660331e-01  2.92406082e-01
 -7.52739683e-02  2.44095892e-01  4.23554443e-02  4.27190244e-01
  2.86779910e-01  2.82554150e-01 -1.66388065e-01  1.71072572e-01
 -1.92669511e-01 -1.34076163e-01  2.95716971e-02  4.17801552e-03
  1.61113739e-01 -2.07076948e-02 -1.14036098e-01 -7.54912123e-02
  3.31067920e-01 -3.07267290e-02 -6.66339472e-02  2.33379126e-01
  2.16876835e-01  1.28290981e-01 -6.29638433e-02  3.54500972e-02
  8.30016583e-02 -4.65361215e-02  1.08337983e-01 -5.55880785e-01
  1.97274297e-01 -2.72860706e-01 -1.07946917e-01 -1.79561138e-01
  2.98369139e-01  3.10736060e-01  2.98817195e-02  3.14358212e-02
 -5.43060042e-02 -1.30748749e-01  2.28701055e-01  4.07960922e-01
 -1.34457834e-02  8.86957571e-02  6.19149357e-02 -3.29659879e-01
 -1.00539103e-01  9.59600508e-02  1.43296510e-01  8.18598419e-02
  4.96341467e-01 -2.93715000e-01  1.30498871e-01  8.58480111e-02
 -1.55450165e-01  2.96593964e-01 -1.37877792e-01  7.56109431e-02
 -1.69978708e-01  4.99643743e-01 -7.14154020e-02 -5.82853556e-02
  5.94035462e-02 -1.57158762e-01 -2.28869855e-01  4.93426248e-02
  1.00803040e-02  4.51653078e-02 -3.55408847e-01 -2.81296253e-01
 -2.74717450e-01  2.61665195e-01 -2.64380854e-02 -5.48599958e-02
  1.27056777e-01  7.49180019e-02 -3.67219597e-01 -2.04864696e-01
 -1.82326600e-01  4.76409256e-01 -2.72674337e-02 -3.36368054e-01
 -3.69342327e-01 -2.77993739e-01 -1.32101715e-01 -1.46724701e-01
 -1.65720403e-01 -3.72854054e-01  3.89581561e-01  5.28634071e-01
  1.01677580e-02 -1.10323839e-01 -1.74863592e-01  1.35163575e-01
 -2.83759832e-01  9.18510854e-02 -1.80835754e-01  2.42884070e-01
 -1.54101253e-01  1.86308205e-01  1.59348860e-01  3.47037792e-01
  4.60133702e-03  2.11070508e-01 -1.15985624e-01 -5.82489334e-02
 -1.00225359e-02  9.78898108e-02 -2.91282274e-02 -1.43158780e-02
 -2.06015036e-02  3.22463781e-01 -1.68340772e-01 -3.77707295e-02
 -3.00304502e-01  2.42490113e-01  1.98137134e-01 -2.89237738e-01
 -2.05911957e-02  2.07028687e-01 -2.53403410e-02  5.38207889e-02
 -3.82976174e-01 -2.99132943e-01 -3.43781114e-02 -1.07393339e-02]"
Tflite label_image resize bug comp:lite,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
NA
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.10
- **Python version**:
3.5
- **Bazel version (if compiling from source)**:
0.15
- **GCC/Compiler version (if compiling from source)**:
NA
- **CUDA/cuDNN version**:
NA
- **GPU model and memory**:
NA
- **Exact command to reproduce**:
NA

### Describe the problem
In the `label_image.cc` example of TFlite, a resize function present in `bitmap_helpers_impl.h` is being called in line [171](https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/contrib/lite/examples/label_image/label_image.cc#L171) - 
`   resize<float>(interpreter->typed_tensor<float>(input), in.data(),`

In the implementation of the function in `bitmap_helpers_impl.h`, the following [line](https://github.com/tensorflow/tensorflow/blob/4dcfddc5d12018a5a0fdca652b9221ed95e9eb23/tensorflow/contrib/lite/examples/label_image/bitmap_helpers_impl.h#L90) 
```
  auto output_number_of_pixels =
      wanted_height * wanted_height * wanted_channels;
```
assumes that for the input image, the height and width must be the same. And this works for the mobilenet example since the input size is 224 x 224. But, this would cause issues for any network where the expected input size does not have the same width and height. It should instead be - 
```
   auto output_number_of_pixels =
      wanted_width * wanted_height * wanted_channels;
```

### Source code / logs
NA
",True,"[-3.29663217e-01 -5.00978410e-01 -2.42548212e-01 -2.13247895e-01
  2.82451987e-01 -2.33036265e-01  8.77636373e-02  2.76663192e-02
 -2.05383480e-01 -1.28785387e-01  3.04202754e-02  2.32538700e-01
 -1.54293031e-01  2.37025455e-01 -2.70407259e-01  4.05464768e-01
 -1.36458606e-01 -2.33630359e-01  2.09976345e-01 -8.15428048e-03
 -1.11789495e-01 -1.88667148e-01 -1.47230625e-01  3.02987486e-01
  2.71803495e-02  4.27635670e-01 -1.08681887e-01  5.85671589e-02
  1.54536981e-02  1.56815380e-01  1.60395920e-01  3.69185805e-01
 -1.57851547e-01  8.01917762e-02  2.58456487e-02  1.84983641e-01
 -4.88623887e-01  2.82169841e-02 -1.57834560e-01 -1.66998744e-01
  1.09286532e-02  1.26677066e-01  2.49153115e-02  9.08559337e-02
  1.11582443e-01  2.79266179e-01  9.35764983e-02  1.57292008e-01
 -1.85762703e-01 -1.57968834e-01  1.72600314e-01 -1.79635763e-01
 -4.33710158e-01 -2.23794028e-01 -1.19418219e-01  1.65363587e-03
  1.14721209e-01 -1.52443111e-01  2.54027545e-01  1.86074331e-01
  2.41515823e-02 -1.62547641e-02 -4.70142663e-02  1.86641067e-01
  1.40975967e-01  1.62625611e-01  1.64238870e-01 -2.94460326e-01
  1.54279962e-01  1.40821170e-02  9.53992605e-02 -7.56456554e-02
 -9.62722078e-02  2.00457454e-01  9.58433449e-02  2.18336284e-02
 -3.50035019e-02  3.39436710e-01  2.53315330e-01  2.27942206e-02
  2.48427302e-01 -2.76088387e-01  2.22197935e-01 -7.65425786e-02
  1.29548430e-01  1.84088126e-02  2.96538353e-01  2.27922350e-01
  1.27738163e-01 -2.61917770e-01  4.62257266e-01  5.60099125e-01
  8.21149945e-02  2.11627543e-01  2.54714251e-01  1.62838474e-01
 -8.04528967e-02  1.06288359e-01 -1.04595065e-01  6.81344420e-04
 -3.55583504e-02 -3.87186140e-01 -9.68820602e-02  1.58485442e-01
  1.92903932e-02 -1.25750586e-01  2.53116190e-01 -4.13412452e-02
 -2.76057720e-02  1.93984419e-01  1.22384518e-01  2.10324191e-02
  4.28978652e-02 -1.34241074e-01  1.51712494e-03 -1.10272378e-01
  4.73983586e-02  2.56765038e-02  9.72757339e-02  3.94276053e-01
  1.87312718e-02 -1.40640765e-01 -5.33728749e-02  6.70669228e-02
  5.89922071e-01  2.60142356e-01 -2.15517431e-01  1.30115062e-01
  9.45595652e-02  1.67776570e-01 -8.16710070e-02  7.42242336e-02
 -1.89414039e-01  1.14151165e-01 -9.13092345e-02  1.60176888e-01
 -3.36661875e-01  5.51336929e-02 -2.94717610e-01 -1.15629807e-01
 -2.32052207e-01  1.74616843e-01  1.14878148e-01 -1.85017034e-01
  2.47172713e-01  1.24156594e-01 -3.10458004e-01  8.01236331e-02
 -3.52720916e-01  1.64496332e-01 -6.09916598e-02  1.04033783e-01
  8.03997740e-02  5.22330880e-01  5.62228039e-02  1.28106281e-01
  3.24235320e-01 -9.71723795e-02  1.13460176e-01 -4.76888776e-01
 -1.42662615e-01 -3.81147824e-02 -2.57957820e-02 -2.73381919e-03
 -2.35619713e-02  1.03936911e-01 -4.39420253e-01 -1.59310713e-01
  5.65946195e-03  3.52911592e-01 -1.96288675e-01 -1.49583286e-02
  1.58217140e-02  3.65844250e-01  2.00271219e-01 -4.54850495e-02
  3.40136588e-01 -5.08010149e-01 -1.56602919e-01 -3.44596319e-02
 -6.11178428e-02  1.46763623e-02 -8.49611536e-02 -7.10039586e-02
 -8.23988020e-02 -1.57662481e-01  2.83901393e-01  2.34341502e-01
 -3.27138066e-01 -2.54751056e-01 -4.22248125e-01 -6.43486530e-02
  3.48921746e-01 -2.31790841e-02 -7.14946389e-02  1.16382428e-01
  1.14956848e-01 -9.19115096e-02  2.53032707e-02  4.81232181e-02
  5.76654524e-02  9.28640924e-03 -1.69414729e-01 -1.10689573e-01
  8.40809271e-02 -1.15452468e-01 -2.86419421e-01 -1.10114217e-01
 -4.38513756e-01  2.94827558e-02  3.83996755e-01 -4.07939255e-01
 -1.67300105e-01 -2.21861407e-01 -7.82091320e-02 -1.93301752e-01
 -1.36337414e-01 -2.09314644e-01 -2.99846053e-01  3.04639310e-01
  2.60249786e-02 -1.84128657e-01  2.65472978e-02 -4.65080529e-01
 -3.57496083e-01 -1.50940180e-01 -3.26391578e-01  2.39357501e-01
 -1.54873341e-01  3.03148866e-01 -2.04418138e-01  3.28820437e-01
  1.85803205e-01 -3.45065258e-03  9.53527987e-02 -1.18908465e-01
 -1.96461439e-01 -4.07777250e-01 -2.66694278e-01  2.89085209e-01
 -3.03650856e-01 -2.66604722e-01 -2.07136422e-02 -5.03619090e-02
  1.21573001e-01  1.00357905e-01 -1.82829686e-02  2.07123190e-01
 -1.65448010e-01  5.00714064e-01  7.42626190e-03 -5.67432567e-02
  4.67205554e-01  2.36714184e-01  2.30241120e-01 -5.61935129e-04
  1.45043522e-01  1.33065134e-01  2.00491473e-01 -2.50520445e-02
  2.42685795e-01  2.42394954e-01  6.31707311e-02  7.23045826e-01
  4.82689798e-01  3.87744516e-01 -2.30986819e-01  8.68993849e-02
  2.81024612e-02 -3.87363017e-01 -1.25840425e-01  2.31312253e-02
  3.22316676e-01 -3.06722671e-01  1.10100724e-01 -3.16728473e-01
  5.50917029e-01 -1.21023431e-01 -9.24955755e-02  2.34943956e-01
  2.35544756e-01  3.45824100e-03 -3.77849527e-02  2.48610765e-01
  1.80590212e-01 -1.03236772e-01 -2.54848361e-01 -6.45157218e-01
 -2.78542608e-01 -8.04473087e-03 -8.01513344e-02  1.28112033e-01
  7.74350241e-02  1.74008220e-01 -2.17910439e-01  1.91302523e-01
  2.35597230e-02 -1.42914459e-01 -1.14120170e-02  7.24211112e-02
  7.70426989e-02  2.41604194e-01  1.71518266e-01 -8.87491703e-02
 -5.66525757e-02  6.02609105e-03  3.67620409e-01 -1.89388003e-02
  4.84834492e-01 -2.66462445e-01 -2.29677428e-02  6.37201592e-04
 -4.74360548e-02  1.85090840e-01 -1.15419522e-01  3.58766876e-03
 -3.11666965e-01  4.97237533e-01  7.33653903e-02 -1.14716455e-01
  1.23885065e-01 -8.32899883e-02 -1.85431410e-02  8.16751085e-03
 -1.64788067e-02  5.62624121e-03  1.13044545e-01  9.58007947e-03
  4.20761257e-02  6.10154271e-02 -2.75267601e-01 -2.34122723e-01
 -3.14475268e-01  1.12926617e-01 -4.08923745e-01 -9.18687284e-02
 -2.72502363e-01  2.89227217e-01 -1.14805371e-01 -1.11423329e-01
 -1.06697008e-01 -9.18590501e-02 -4.57548760e-02 -3.76679063e-01
 -2.97695249e-02 -1.77273005e-01  1.13848954e-01  3.55505586e-01
  1.30979437e-03 -2.65970435e-02 -1.29641593e-01 -1.02527097e-01
 -1.26076996e-01  1.37949198e-01 -2.01627284e-01  2.07595959e-01
 -2.85472363e-01 -1.78805843e-01  1.59552678e-01  5.18753290e-01
 -1.15184464e-01 -1.02003053e-01 -3.33709300e-01 -1.51935950e-01
  1.10489860e-01 -3.12598288e-01 -1.65900558e-01 -2.29042649e-01
  1.61639720e-01  2.83561587e-01  4.57187854e-02  3.04305911e-01
 -2.72387207e-01  1.28596812e-01  3.28864217e-01 -3.24649990e-01
  1.36822775e-01  2.58976609e-01 -1.22629866e-01 -1.40800133e-01
 -5.30824438e-02 -2.51474738e-01 -2.70388797e-02  1.27250105e-01]"
metrics=['accuracy'] seems to be calculated differently if one uses tf.data inputs instead of numpy arrays for keras model ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: **YES**
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: **Windows 10**
- **Mobile device**:na
- **TensorFlow installed from (source or binary)**: **binary**
- **TensorFlow version (use command below)**:**1.11.0-dev20180907**
- **Python version**:**3.6.3**
- **Bazel version (if compiling from source)**:na
- **GCC/Compiler version (if compiling from source)**:na
- **CUDA/cuDNN version**:na
- **GPU model and memory**:na
- **Exact command to reproduce**:na

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Given the same piece of code for loading mnist data and training a keras model in tensorflow, the metric ""accuracy"" given as argument to keras_model.compile(metrics=[...]) generates very different values (order of 0.10 versus order of 0.90) depending on if you use numpy arrays or tf.data datasets as training inputs. Note that the values of the loss in each case are very close. I suspect that ""accuracy"" is being calculated differently depending on the type of input (numpy or tf.data), or that it is being calculated wrong in one of the cases.
In particular, as an example, using numpy arrays as input, one can get the pair loss: 0.2086 - acc: 0.9389 in one of the steps, while the same loss in with tf.data gives the pair loss: 0.2086 - acc: 0.1024.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

The code below as it is can be run and training with tf.data datasets will be performed. If you comment the block between `#Train with tf.data datasets` and `########################` and uncomment the block between `#Train with numpy arrays` and `########################`, training with numpy arrays as inputs will be performed.

```
import tensorflow as tf
import numpy as np

np.random.seed(1)
tf.set_random_seed(1)
BATCH_SIZE = 32

#Import mnist dataset as numpy arrays
(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()#Import
x_train = x_train / 255.0 #normalizing
y_train = y_train.astype(dtype='float32')
x_train = x_train.astype(dtype='float32')

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1]*x_train.shape[2]))#Reshaping the 2D picture

##############################################################################################
#THIS BLOCK CREATES A DATASET FROM THE NUMPY ARRAYS. IT WILL BE USED FOR THE CASE OF TF.DATA DATASET INPUTS
tfdata_dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))
tfdata_dataset_train = tfdata_dataset_train.batch(BATCH_SIZE).repeat()
##############################################################################################

#Create model
keras_model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(512, activation=tf.nn.relu),
    tf.keras.layers.Dropout(0.2, seed=1),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

#Compile the model
keras_model.compile(optimizer=tf.keras.optimizers.Adam(),
                    loss=tf.keras.losses.sparse_categorical_crossentropy,
                    metrics=['accuracy'])

#Train with numpy arrays
#keras_training_history = keras_model.fit(x_train,
#                y_train,
#                epochs=1
#                )
########################

#Train with tf.data datasets
keras_training_history = keras_model.fit(tfdata_dataset_train,
                epochs=1,
                steps_per_epoch=60000//BATCH_SIZE
                )
########################
```


",True,"[-2.58121490e-01 -5.86932898e-01 -4.12644595e-01 -3.47212441e-02
  2.47203857e-02 -2.72274852e-01 -2.36299664e-01 -1.46847591e-01
 -3.70135307e-02 -1.54052168e-01  5.06436452e-02  1.57268196e-01
 -1.74750574e-02  1.20414898e-01 -2.87583679e-01  1.55866489e-01
 -1.98269740e-01 -1.05480820e-01 -1.30914569e-01  1.42085850e-02
  3.74800935e-02 -8.84361379e-03  1.49143249e-01  1.59021258e-01
  5.22616133e-03 -5.40131330e-02 -7.73822963e-02 -2.01149911e-01
  4.37335819e-02 -3.19083147e-02  3.84778827e-02  1.29681975e-01
 -2.02251226e-01 -1.06303513e-01  4.31774333e-02  1.63626820e-01
  2.71105971e-02 -1.38848528e-01 -2.13506892e-01  4.17996086e-02
  2.26752639e-01 -1.70887232e-01  1.04208030e-01  3.95028666e-02
  1.61410898e-01  6.01585843e-02  6.71268329e-02 -1.20433733e-01
 -1.49764642e-01  2.98266411e-02 -2.40002722e-01 -1.09691590e-01
 -1.90918416e-01 -3.67356509e-01 -1.21666752e-02 -9.05222446e-02
  1.09607711e-01 -5.66757508e-02 -9.69540402e-02 -8.26132447e-02
  2.52508894e-02 -5.59184216e-02  7.37800151e-02  9.45983678e-02
  3.36035848e-01  4.56900373e-02  7.12233484e-02 -1.71295330e-01
  1.86614960e-01  7.27158636e-02  1.25642762e-01 -2.90639754e-02
 -3.17078233e-01  1.27926081e-01  9.94681120e-02  4.83790711e-02
 -1.58924192e-01  2.87055612e-01  4.94024247e-01  1.07814267e-01
  1.16314009e-01  1.00144699e-01  6.63914531e-02 -2.96912901e-02
  1.31577358e-01 -4.21704426e-02  1.52050689e-01  1.40820695e-02
  2.03276008e-01 -1.47236452e-01  4.45392996e-01  1.62110582e-01
 -1.84291050e-01  1.79591671e-01  4.82019484e-01  2.28833497e-01
 -8.45138133e-02  1.70412481e-01 -1.36307418e-01 -8.70471597e-02
 -2.29701713e-01 -1.88711882e-01 -2.15217516e-01  1.54041290e-01
 -1.89879071e-02  7.31104985e-04 -9.57163125e-02  1.00809768e-01
  1.58733070e-01  2.13790629e-02  3.53015125e-01 -3.27358395e-02
  1.27569258e-01  1.30583629e-01 -1.74189448e-01 -8.48990157e-02
  2.39410862e-01  2.93208182e-01 -5.13305590e-02  2.28186190e-01
 -2.66413569e-01 -3.19243670e-01  6.04285114e-02  4.30968963e-03
  2.17226595e-01  9.82474312e-02 -3.27600837e-01  2.24326283e-01
 -1.62751898e-01  1.27320364e-01  1.82976454e-01 -1.25266880e-01
 -1.66174293e-01 -1.82714909e-01 -1.50798678e-01 -2.79331952e-01
 -1.64325878e-01  3.62316445e-02 -2.37312600e-01 -1.76541619e-02
  3.37863714e-02  9.61190015e-02 -3.74168158e-03 -1.64050758e-01
  1.53914616e-01  3.49050999e-01 -2.30969340e-01 -8.01706314e-02
  6.48138225e-02 -1.57949612e-01 -3.61038037e-02 -6.25741854e-02
 -6.47763535e-03  1.07850842e-01 -7.19655771e-03  3.02110575e-02
 -3.19862701e-02 -1.03214182e-01 -7.22001344e-02 -1.85558096e-01
  4.30872515e-02  5.43115810e-02 -6.80927485e-02 -1.11169890e-01
  3.53260815e-01  1.79319203e-01 -2.52967715e-01 -2.16913193e-01
  8.04276988e-02  6.75680488e-02  1.70358479e-01 -2.79430710e-02
 -5.16395867e-02  3.71929519e-02  5.44635653e-01  1.02188580e-01
  1.98073089e-01 -3.69020104e-01 -1.82989255e-01  5.30691035e-02
  3.34218323e-01 -6.05437495e-02  2.70501614e-01  1.08676240e-01
  1.22955717e-01 -1.33801177e-01 -7.49914572e-02  7.22223595e-02
 -2.38756552e-01 -8.13039839e-02 -1.01205856e-01 -2.30183929e-01
  3.34569812e-02  4.57596965e-02 -1.26773089e-01 -9.37854201e-02
  1.77784503e-01 -2.42762834e-01  4.63600084e-02  8.58308673e-02
 -1.50209546e-01 -3.18786129e-04 -9.07871351e-02 -2.52401471e-01
 -1.40512884e-02 -1.88771248e-01  3.11068296e-02 -2.45871797e-01
 -2.08180204e-01  8.46085697e-02  8.39195028e-02 -2.27623791e-01
 -1.79475844e-01  2.74464488e-02 -2.13161096e-01 -4.83045913e-03
 -7.53341168e-02  6.79286942e-03 -2.60479897e-01  4.05052423e-01
 -5.24268560e-02 -1.66951299e-01 -8.28563422e-02 -2.86771417e-01
 -4.37924862e-01  7.77046755e-02  3.77481468e-02  3.23295474e-01
  3.39599401e-02  1.21459082e-01  1.97384447e-01 -1.87994316e-01
  5.32118618e-01  7.98794627e-02 -3.37387696e-02 -1.22556426e-01
 -1.99587926e-01 -2.09604591e-01  4.82173637e-03 -2.57472754e-01
 -4.06823069e-01 -1.43138215e-01 -1.40126243e-01 -2.34316573e-01
 -7.05197453e-04  1.20334983e-01 -5.70516754e-03  1.54956467e-02
 -1.61805958e-01  3.83448422e-01 -2.00909644e-01 -2.88848430e-01
  2.37190336e-01  1.21643990e-02  1.69388056e-01 -8.35378766e-02
  7.83254355e-02  1.13396659e-01  2.16404915e-01  2.53074884e-01
  5.16973853e-01 -5.11591397e-02  1.16508573e-01  6.03482008e-01
  4.05575007e-01  3.28603446e-01 -9.75889340e-02  6.28122985e-02
 -2.79132605e-01 -2.36905083e-01  3.34100984e-02  3.38762105e-02
  3.36406171e-01 -2.47305036e-01  3.46369073e-02 -7.79258534e-02
  3.95537734e-01  1.15415722e-01 -2.45688438e-01 -1.17130261e-02
  1.38208225e-01  2.68010139e-01 -2.36463860e-01  4.63681072e-02
  2.19601393e-02 -9.85969603e-02 -1.18531555e-01 -2.35842586e-01
  9.47168395e-02 -1.25533074e-01 -2.26980224e-02 -6.10239469e-02
  4.08953615e-02  1.59725785e-01  6.42807633e-02 -3.85712981e-02
 -3.36803347e-02 -3.76733989e-02  1.25693828e-01  2.19525024e-01
 -8.55005998e-03  1.76483616e-01  3.12586367e-01 -8.81286040e-02
 -2.11000651e-01  1.31755739e-01  2.72151798e-01  1.90297157e-01
  4.91861731e-01  7.74002373e-02  3.16078812e-01  8.78983214e-02
 -2.01846678e-02  5.66358030e-01 -1.26519911e-02  7.91863576e-02
 -2.87942737e-01  3.83109808e-01 -8.35019257e-03  2.07630098e-02
 -1.16552278e-01 -1.93114072e-01 -2.57494926e-01  1.00345373e-01
 -2.10636109e-02  1.72417402e-01 -1.02868855e-01 -2.92219073e-02
 -1.93331361e-01  2.33719498e-01 -1.51086181e-01 -1.46122262e-01
 -8.97468254e-02  1.45211786e-01 -1.26896769e-01 -2.11791247e-01
 -8.97369012e-02  3.94377619e-01 -8.21499750e-02 -2.27050304e-01
 -5.43312868e-03 -1.80930197e-02 -2.93060958e-01 -2.83086866e-01
 -1.33147696e-02 -1.59385622e-01  2.00428590e-01  6.74608827e-01
  5.88508099e-02  6.96959943e-02  7.24726729e-03  1.57585651e-01
 -1.59102291e-01  1.33824527e-01 -1.07525282e-01  1.48478940e-01
 -1.67641729e-01  2.82037575e-02  2.78560817e-03  3.44795465e-01
 -2.52302200e-01  1.62248611e-01 -1.24830894e-01 -1.71868131e-03
  1.39420420e-01  1.37602955e-01 -1.27136603e-01 -4.34908420e-02
  2.10488468e-01  3.80063295e-01 -1.87242463e-01  1.87513202e-01
 -2.07776815e-01  1.30888661e-02  8.75422508e-02 -3.12809765e-01
 -3.05982772e-02  2.97451317e-02  1.59580514e-01  6.01191968e-02
 -8.93569738e-02 -2.35229999e-01 -2.41475865e-01 -1.07287265e-01]"
ps0 will be OOM using MonitoredTrainingSession when workers too many stat:awaiting response stale,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 1.9
- **GCC/Compiler version (if compiling from source)**: 4.9.2
- **CUDA/cuDNN version**: CPU
- **GPU model and memory**: CPU
- **Exact command to reproduce**: Using MonitoredTrainingSession and thousands of workers.


### Describe the problem
My model is large and complex. Thousands of worker&ps are needed and MonitoredTrainingSession is used. In the stage of initialization, the memory of ps0 will raise up to 100G and then OOM, but other ps work correctly. Op device placement is tf.train.replica_device_setter and no other special strategy.

I open the log_device_placement and found that all ops related with report_uninitialized_xxx are placed on ps0. I think this is the root cause.

I change the code as PR #22136 , and then it works correctly.
",True,"[-0.32524925 -0.2982253  -0.17916206 -0.21508102  0.13397965 -0.39878768
 -0.19574524  0.08653475 -0.2071195  -0.18530792  0.08469918  0.38570914
  0.03594408 -0.23059554 -0.21223918 -0.17252833  0.14754952 -0.3722829
  0.07760338  0.17366329 -0.0318207  -0.4611504  -0.09937897 -0.12530454
 -0.05221884  0.27653843 -0.05784825  0.13616382 -0.05218791  0.20204312
  0.3180095   0.15685494  0.1200518   0.1400742  -0.00504811  0.00235736
 -0.06453746 -0.19766805 -0.4188308   0.0292082   0.07074672 -0.13117592
 -0.06911848  0.23950417  0.13743591  0.13644327 -0.01539748 -0.14948784
 -0.176377   -0.2546697   0.02244386  0.06616041 -0.12021255 -0.09366392
 -0.15594944 -0.2593021   0.16535108  0.07693914  0.19604719  0.30056536
 -0.10968572 -0.10775258  0.02892766 -0.00983839  0.32556698  0.08695593
  0.22553794 -0.21793514  0.30313706  0.23603347  0.04099164  0.01816832
 -0.30020523  0.00376438  0.08886858  0.32538247 -0.31559974 -0.14539325
  0.31657955 -0.4421073   0.25218976 -0.17689523  0.15550803  0.15327804
  0.2504909   0.12427737  0.3084135   0.13133499  0.2881108  -0.01432135
  0.11788645  0.276299    0.07777579 -0.09148782  0.24541405  0.12108264
  0.06835489  0.17528251 -0.32316017 -0.04412206 -0.03208669 -0.22578365
 -0.10298644  0.26323152 -0.2575099  -0.13680114  0.03326149  0.09693538
 -0.22157115 -0.01170479  0.1196921   0.05449874 -0.15091875 -0.17389207
  0.06246153 -0.09444545 -0.19471042  0.06104898 -0.22848098  0.38607252
  0.16463688 -0.14530793 -0.18255095  0.08911148  0.42466512  0.16894048
 -0.0939917   0.27056694 -0.16422734  0.17424974  0.31427264 -0.08547306
 -0.16050224  0.19157493  0.00528251  0.11633089  0.21484199  0.04808827
 -0.3478452   0.1827375   0.22394317  0.2501948   0.19189116 -0.17323263
  0.36497167  0.23612033 -0.15485471  0.22796777  0.19246978 -0.08755339
  0.03460814 -0.14514855 -0.02683381  0.50598454 -0.04804386  0.09174617
  0.12814058  0.09201173 -0.05584313 -0.5233669  -0.12021049  0.11975414
 -0.22470611  0.01428885  0.21477568 -0.07826906 -0.13348266 -0.2602609
 -0.07964346  0.30281216 -0.13905472 -0.10701666 -0.12242587  0.21504068
  0.14510104 -0.25705776  0.04101927 -0.17713194 -0.3707759   0.09661199
 -0.09986441 -0.27808326  0.16045442  0.01828069 -0.15704301  0.01379709
  0.15568702  0.16442147 -0.25621706 -0.11996604 -0.32291177  0.00198605
 -0.08446092  0.00259748 -0.22333765 -0.3465992   0.09443074  0.25622714
 -0.0246567   0.23691396 -0.2644764  -0.03294162 -0.12763394  0.17430925
  0.24879578  0.18096697 -0.63811356 -0.16554277 -0.30661434  0.17407033
  0.06701216 -0.17149618  0.14917596  0.27373168  0.00937182 -0.15313089
 -0.41894132  0.11576818  0.01189644 -0.04256546  0.0853586  -0.07053982
 -0.16189754 -0.3130988  -0.4282944   0.1881252  -0.13609415 -0.14846477
 -0.11839598  0.26024377 -0.00827191  0.08727521  0.07209076  0.17011587
  0.17464265 -0.09833574  0.0575337  -0.2456731  -0.0607325   0.16874954
 -0.46311152 -0.26291814 -0.3245377   0.10217229 -0.06460612  0.31626642
 -0.15268275  0.4417699   0.06513523  0.195459   -0.02999912 -0.300274
  0.17065914  0.14097047  0.03329928  0.06353685  0.01835341  0.28231007
  0.23947388  0.03317587  0.09295231  0.27016354  0.1222046   0.37628767
  0.509       0.1859734  -0.3203941   0.22726071 -0.10756169 -0.23075505
 -0.01943617 -0.1331671   0.3685394  -0.18684618  0.26046747 -0.10726912
  0.42961684 -0.06385934 -0.19350909  0.06536245  0.1980414  -0.00157723
 -0.07961876  0.06737481  0.24029127 -0.42716038 -0.09348106 -0.10964765
 -0.21520466 -0.15830189  0.4068869   0.08278153  0.32761544  0.02627214
 -0.0079042  -0.01230959  0.12452629  0.0950603   0.0892515   0.1336027
 -0.32375836 -0.17789936  0.03231772  0.23701297 -0.38748887 -0.08633272
  0.177405    0.12410869  0.37380385 -0.18859023  0.15057041  0.09754182
  0.0057469   0.13409808 -0.1438205   0.12493944 -0.6211134   0.56198686
 -0.13416182  0.02061727  0.24634019 -0.1935992   0.21830621  0.24672985
 -0.13976529 -0.18225686 -0.2091638  -0.19189006 -0.00601987  0.3368076
  0.14525813 -0.05549076  0.06791247 -0.11202931  0.02150394  0.03382391
 -0.10482377  0.38940293 -0.3142873  -0.12783782 -0.26850146 -0.00826558
  0.07306757 -0.09064358 -0.13699299  0.01579904  0.36080086  0.26853764
 -0.16244434  0.20018792 -0.00172924 -0.15621626 -0.18226624 -0.27904898
 -0.23853603  0.21853071  0.15792228  0.03468157  0.15601166  0.37194335
 -0.29308254 -0.00278715 -0.36603904  0.1352437  -0.01926237 -0.25748307
 -0.04099209 -0.2850414   0.06249351  0.04294732  0.05530228  0.16652447
 -0.1422452   0.11028472  0.37806293 -0.263677   -0.1693267  -0.09811027
 -0.11110688  0.04785585  0.05111013 -0.25780684 -0.1408163   0.00650561]"
[Bug] tf.nn.depthwise_conv2d fails with AttributeError ,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: v1.10.1-0-g4dcfddc5d1
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 9.0/7.1.4
- **GPU model and memory**: GTX 1070

### Describe the problem
`tf.nn.depthwise_conv2d` fails when the shape of the input is not known statically and the data format is 'NCHW' and the dilation rate is larger than 1.

### Source code / logs
Reproducible test case:
```
import tensorflow as tf
sh = tf.placeholder(dtype=tf.int32, shape=[4])
img = tf.ones(sh)
k = tf.ones([1, 1, 1, 1])
t = tf.nn.depthwise_conv2d(img, k, [1, 1, 1, 1], 'VALID', rate=[2, 1], data_format='NCHW')
```
Traceback:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_impl.py"", line 461, in depthwise_conv2d
    op=op)
  File ""/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 364, in with_space_to_batch
    return new_op(input, None)
  File ""/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 520, in __call__
    return self.call(inp, filter)
  File ""/home/manu/.local/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 514, in _with_space_to_batch_call
    output_shape[1] = filter.shape[-1]
AttributeError: 'NoneType' object has no attribute 'shape'
```",True,"[-2.60238856e-01 -8.12218070e-01 -3.95004511e-01 -1.13556981e-01
  1.24666557e-01 -4.88080293e-01  3.19113284e-02 -1.47100240e-01
 -2.60778964e-01 -8.70617628e-02  8.11101645e-02 -2.11723208e-01
 -1.71113610e-01  2.61777967e-01 -3.18111718e-01  3.97219360e-01
 -1.41819209e-01 -2.91114926e-01  6.21354170e-02  4.07739878e-02
 -3.43977094e-01  3.46916616e-02 -3.64966571e-01  4.22015488e-01
  2.06680149e-01  1.18902430e-01 -2.52527386e-01  2.63651870e-02
 -5.22644296e-02  1.52252272e-01  4.62799698e-01  1.19650193e-01
 -2.09372878e-01  6.20405041e-02 -1.28500149e-01  2.79788673e-01
 -2.80722737e-01 -8.44862014e-02 -2.92476505e-01 -3.69262882e-02
  2.34769285e-03  7.84671605e-02 -6.37769103e-02  2.58020516e-02
 -5.36503494e-02 -3.10743675e-02  9.27378237e-02 -8.52652565e-02
  7.31519982e-03 -6.99844956e-02  3.81359830e-02  7.02018887e-02
 -4.53954637e-01 -2.18163490e-01 -3.37526724e-02  5.53966463e-02
  9.23395678e-02  9.40126367e-03  6.07938580e-02  1.67566255e-01
  1.67379394e-01 -1.02361962e-01  3.78775224e-02  1.65178031e-01
  8.68027583e-02  1.34275064e-01  3.50139320e-01 -3.02448213e-01
  2.39548206e-01 -2.32106924e-01  1.17149524e-01 -6.67811409e-02
 -1.19149700e-01 -1.25766322e-01  1.38302088e-01  1.47234857e-01
 -7.91323930e-02  2.52916276e-01  1.49657547e-01 -6.55848235e-02
 -9.06979572e-03 -2.87313554e-02  1.12221844e-01 -7.98772648e-02
 -3.83421406e-02  5.60348630e-02  1.41512811e-01  1.09410018e-01
  1.87162802e-01 -3.01928848e-01  7.19359934e-01  1.19018078e-01
 -1.43719111e-02  1.89269990e-01  4.56222624e-01  2.89993230e-02
 -5.32983355e-02  2.86403358e-01 -1.47884376e-02 -4.96488437e-02
 -2.76573896e-01 -2.09794417e-01 -5.44514135e-02  1.79622099e-01
 -5.90368174e-02  1.13145085e-02  3.97569016e-02 -8.66040401e-03
  1.77193910e-01  1.13866776e-01  3.24564986e-02  7.79175311e-02
  4.17676985e-01 -1.35639086e-02  2.95048237e-01  2.34085768e-01
 -2.49931186e-01  1.81517988e-01 -1.84394360e-01  5.04083216e-01
 -7.47509748e-02  2.98625845e-02  2.70059049e-01  9.37283933e-02
  4.87276196e-01  6.42417893e-02 -4.79799695e-03  5.96090443e-02
  1.48260012e-01 -2.54828595e-02  2.74006158e-01  1.32960275e-01
 -2.59927541e-01  1.36970550e-01 -1.19625531e-01  4.98650707e-02
 -3.86633933e-01 -8.54109228e-02 -1.04308240e-01 -3.22728872e-01
 -2.22295225e-01  2.39954069e-01 -2.43598104e-01 -4.71675813e-01
  1.80573016e-01  3.49903218e-02 -1.66306436e-01  2.62075067e-01
 -1.43713966e-01  3.64791125e-01 -1.46625400e-01  1.40082628e-01
  3.23849693e-02  3.25345218e-01 -6.59469888e-03  4.64716971e-01
  5.22742391e-01 -1.13778651e-01 -1.27159104e-01 -6.24957681e-01
  3.79181765e-02  1.55903608e-01 -6.95850104e-02 -1.23376213e-03
  1.47620797e-01  1.39220551e-01 -5.46533108e-01 -2.27568582e-01
  2.11060256e-01  9.93972644e-02 -1.68047354e-01  2.00278964e-02
 -6.66084215e-02  3.14658195e-01  1.77030921e-01 -1.90861281e-02
  2.72020519e-01 -6.65799022e-01 -9.48797315e-02  1.84096783e-01
 -5.31824343e-02  6.79268269e-03 -2.85167936e-02  5.08035235e-02
  4.51748595e-02 -5.39953895e-02  5.76268248e-02  1.24241561e-01
 -3.74516487e-01 -1.22499913e-01 -3.11863631e-01 -2.54678369e-01
  2.21508309e-01 -1.24206834e-01 -1.44195378e-01 -6.48818985e-02
  3.95414501e-01 -8.81780162e-02  5.43233864e-02  3.07993412e-01
 -2.63776302e-01 -2.67112181e-02  1.39234632e-01  1.47536099e-02
  5.09605277e-04 -3.65470886e-01 -1.06878370e-01 -2.31618181e-01
 -4.84011441e-01  1.75347701e-01  1.02526560e-01 -2.82688737e-01
  7.31162578e-02 -1.71071336e-01 -1.26125947e-01 -6.37060218e-03
  2.25570500e-01 -1.13760315e-01 -1.86267361e-01  2.11266190e-01
  1.57069549e-01 -2.32757747e-01  2.48379722e-01 -4.88011539e-01
 -2.36923724e-01 -1.20021880e-01 -3.73866558e-01  2.90407181e-01
  9.53358561e-02  3.47803593e-01  6.12056553e-02  9.90293473e-02
 -2.36263983e-02 -8.70953724e-02  2.90486664e-01 -6.54323176e-02
 -1.13635302e-01 -2.51916945e-01  2.82065809e-01  2.42821366e-01
 -4.41823930e-01 -1.09691732e-01  7.53930509e-02 -3.85564491e-02
  3.92309874e-01 -2.12371908e-03 -2.33367696e-01 -5.68352677e-02
 -3.45007151e-01  4.61155593e-01 -2.17152402e-01 -1.14971787e-01
  3.63600910e-01  1.61680505e-01  2.92330593e-01  1.07630029e-01
  4.94732112e-02  6.58916533e-02  2.26644039e-01  1.07991420e-01
  2.47540414e-01  1.92411542e-01 -1.68410748e-01  5.66288352e-01
  8.45478922e-02  3.07360739e-01 -1.05449632e-01  3.12697589e-01
 -1.90546125e-01 -1.07411124e-01  1.93733007e-01 -3.02664101e-01
  4.24727857e-01 -2.97248721e-01 -5.84483072e-02 -2.26810426e-01
  4.40366149e-01 -8.16389173e-02  3.46497484e-02  3.41350973e-01
  1.95228189e-01  1.61852419e-01 -7.45967031e-02  4.34993878e-02
  5.75025119e-02 -2.42108926e-02 -2.04979002e-01 -5.08256674e-01
 -2.80142665e-01 -3.84467207e-02 -7.80426115e-02  2.12974638e-01
 -6.29256219e-02 -1.77221954e-01 -1.71155959e-01 -2.87579671e-02
  2.24671245e-01  1.53315425e-01  7.66109079e-02  1.27340063e-01
 -1.10545807e-01  6.41250387e-02  2.05600202e-01 -4.08802181e-04
 -6.01817966e-02  2.72303894e-02  3.79121840e-01  1.03041179e-01
  4.50909078e-01 -2.03190923e-01  4.05765265e-01 -1.33223265e-01
 -7.77912885e-02  3.10606837e-01 -3.91421169e-02  1.45973966e-01
  1.23599723e-01  2.64840752e-01  3.83612692e-01 -1.14421472e-01
  1.86308682e-01 -3.16083848e-01 -5.18043637e-01  1.76295973e-02
  2.35055178e-01 -1.49012640e-01 -5.80914319e-02 -4.56034392e-03
 -2.58374780e-01  2.85916597e-01 -1.94385231e-01 -4.74987477e-02
 -3.66464436e-01  1.41148508e-01 -1.72426850e-01 -4.40759361e-02
 -3.16237330e-01  2.67095298e-01  9.93968919e-03 -4.35721397e-01
  5.17050028e-02 -2.11894825e-01 -1.85153671e-02 -4.91565436e-01
 -2.28566423e-01 -2.43555710e-01  3.04281652e-01  5.10462523e-01
 -1.72848910e-01  2.82098651e-01  3.31860222e-02  1.36441179e-03
 -3.54576379e-01 -9.10731778e-02 -1.81517690e-01  1.83273450e-01
 -1.48951009e-01 -5.34967408e-02  3.45147252e-01  4.05762613e-01
 -2.40661189e-01  6.46879226e-02 -9.58302841e-02 -5.24706356e-02
  1.94125950e-01 -1.56882256e-01 -2.66196251e-01  1.61010958e-03
  2.08690464e-01  2.64392287e-01 -1.55662954e-01  5.21083951e-01
 -3.79280835e-01  1.20579161e-01  6.65274382e-01 -1.94848478e-01
 -3.19944680e-01  2.45025098e-01  5.64742871e-02 -3.83369982e-01
 -1.76259965e-01 -8.96707848e-02  4.77477945e-02 -1.62068337e-01]"
GDR Cannot register memory region ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: no
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.10
- **Python version**: 2.7 
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: K40c 12G
- **Exact command to reproduce**: 
**Worker :** python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=4 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=imagenet-data --all_reduce_spec=pscpu --job_name=worker --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+gdr
**PS :** CUDA_VISIBLE_DEVICES='' python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=0 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=imagenet-data --all_reduce_spec=pscpu --job_name=ps --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+gdr

## Describe the problem
I tried to use GDR, but it seems no performance improvement compared to native RDMA, and logs are the following:

```
$ python benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --local_parameter_device=cpu --num_gpus=4 --batch_size=128 --num_epochs=10 --model=alexnet --variable_update=distributed_replicated --data_dir=/home/shuai/imagenet-data --all_reduce_spec=pscpu --job_name=worker --ps_hosts=10.10.10.6:2222 --worker_hosts=10.10.10.5:3333 --task_index=0 --server_protocol=grpc+gdr
2018-09-05 15:05:58.062100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
name: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:02:00.0
totalMemory: 11.92GiB freeMemory: 11.84GiB
2018-09-05 15:05:58.211502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties:
name: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:03:00.0
totalMemory: 11.92GiB freeMemory: 11.84GiB
2018-09-05 15:05:58.374791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 2 with properties:
name: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:83:00.0
totalMemory: 11.92GiB freeMemory: 11.84GiB
2018-09-05 15:05:58.553152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 3 with properties:
name: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:84:00.0
totalMemory: 11.92GiB freeMemory: 11.84GiB
2018-09-05 15:05:58.553735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0, 1, 2, 3
2018-09-05 15:05:59.918282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-05 15:05:59.918335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1 2 3
2018-09-05 15:05:59.918344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N Y N N
2018-09-05 15:05:59.918349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   Y N N N
2018-09-05 15:05:59.918354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 2:   N N N Y
2018-09-05 15:05:59.918359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 3:   N N Y N
2018-09-05 15:05:59.919609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 11473 MB memory) -> physical GPU (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0, compute capability: 3.5)
2018-09-05 15:06:00.088804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:1 with 11473 MB memory) -> physical GPU (device: 1, name: Tesla K40c, pci bus id: 0000:03:00.0, compute capability: 3.5)
2018-09-05 15:06:00.288448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:2 with 11473 MB memory) -> physical GPU (device: 2, name: Tesla K40c, pci bus id: 0000:83:00.0, compute capability: 3.5)
2018-09-05 15:06:00.487520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:3 with 11473 MB memory) -> physical GPU (device: 3, name: Tesla K40c, pci bus id: 0000:84:00.0, compute capability: 3.5)
2018-09-05 15:06:00.688704: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> 10.10.10.6:2222}
2018-09-05 15:06:00.688752: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3333}
2018-09-05 15:06:00.697485: I tensorflow/contrib/gdr/gdr_memory_manager.cc:254] RDMA server is listening on 10.10.10.5:3333
2018-09-05 15:06:00.697568: I tensorflow/contrib/gdr/gdr_memory_manager.cc:302] Instrumenting CPU allocator cuda_host_bfc
2018-09-05 15:06:00.697588: I tensorflow/contrib/gdr/gdr_memory_manager.cc:302] Instrumenting CPU allocator cpu_pool
2018-09-05 15:06:00.697607: I tensorflow/contrib/gdr/gdr_memory_manager.cc:302] Instrumenting CPU allocator cpu_rdma_bfc
2018-09-05 15:06:00.697830: I tensorflow/contrib/gdr/gdr_memory_manager.cc:95] NUMA node for device: mlx4_0 is 1
2018-09-05 15:06:00.734616: W tensorflow/contrib/gdr/gdr_memory_manager.cc:705] Cannot register memory region
2018-09-05 15:06:00.771893: W tensorflow/contrib/gdr/gdr_memory_manager.cc:705] Cannot register memory region
2018-09-05 15:06:00.771954: I tensorflow/contrib/gdr/gdr_memory_manager.cc:314] Instrumenting GPU allocator with bus_id 2
2018-09-05 15:06:00.772783: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:375] Started server with target: grpc://localhost:3333
```
and here is GPU topology:

```
$ nvidia-smi topo -m
        GPU0    GPU1    GPU2    GPU3    mlx4_0  CPU Affinity
GPU0     X      PHB     SYS     SYS     SYS     0-7,16-23
GPU1    PHB      X      SYS     SYS     SYS     0-7,16-23
GPU2    SYS     SYS      X      PHB     PHB     8-15,24-31
GPU3    SYS     SYS     PHB      X      PHB     8-15,24-31
mlx4_0  SYS     SYS     PHB     PHB      X

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe switches (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing a single PCIe switch
  NV#  = Connection traversing a bonded set of # NVLinks
```

The logs said,"" cannot register memory region"" in GPU 2 and GPU 3. Is it a bug?",True,"[-5.58599271e-02 -6.27922297e-01 -6.48719549e-01 -5.16539454e-01
  3.42725068e-02 -2.58335888e-01 -1.85598433e-01  1.17977627e-01
 -3.63438547e-01 -1.64658293e-01  1.61620110e-01  8.18591118e-02
  1.01973908e-02  8.73503983e-02  5.25059178e-04  2.37533869e-03
 -2.10104644e-01 -3.55289221e-01  1.37909725e-01  5.07489964e-03
 -1.50832534e-02 -3.12380213e-02 -2.00916022e-01  1.35070965e-01
  8.51858556e-02  4.63453054e-01  1.38597069e-02 -3.50466698e-01
  1.62637204e-01 -2.33011693e-02  7.26688385e-01  1.45336568e-01
  1.44804895e-01  2.30850339e-01  3.54996957e-02  2.59102285e-01
 -3.65264893e-01 -5.92570230e-02  7.40195215e-02 -1.23979136e-01
 -5.18206134e-02  1.15607932e-01  4.16932255e-02  1.56876460e-01
  7.18226582e-02  4.58401024e-01  5.32570183e-02  1.81279168e-01
 -7.94056505e-02 -3.04081559e-01  3.68791819e-01 -1.01045914e-01
 -2.48661697e-01 -1.85162589e-01  8.05010833e-03  1.25761144e-02
 -1.06632583e-01 -2.30793655e-01  2.25023739e-02  2.13771492e-01
  1.50808707e-01 -3.65695246e-02 -4.96972799e-02  5.15722856e-02
  4.28847894e-02  1.10921025e-01  5.33210397e-01 -2.58331597e-01
  4.05560672e-01 -4.18434590e-02 -1.57151297e-01  2.12576628e-01
 -4.89841789e-01 -1.07229486e-01  2.79070199e-01  5.42435646e-01
  6.84279799e-02  1.48565590e-01  1.67664513e-01 -2.25327820e-01
  2.78886855e-01  2.24590808e-01  3.60606313e-01 -1.20492011e-01
  1.87733136e-02  3.01655214e-02  2.61780500e-01  8.56884569e-02
  6.55861050e-02 -1.71313137e-01  3.44940215e-01  7.10207149e-02
 -2.99371421e-01  1.04270317e-01  5.50814867e-02  9.08949450e-02
  3.48313004e-02  2.72826850e-03 -8.60163420e-02 -2.13119477e-01
 -5.82541861e-02 -5.85335940e-02 -7.08075613e-03  1.19651869e-01
 -8.91571790e-02 -1.06289171e-01  2.63638973e-01  1.87269345e-01
  3.18991616e-02  2.84408443e-02 -2.23256722e-02  1.91791996e-01
 -5.26142940e-02 -2.55172521e-01 -3.53412777e-02 -9.65320021e-02
 -4.12480384e-01  9.45094526e-02 -3.41573715e-01  4.78766501e-01
 -2.45489329e-01 -9.21056047e-03  4.57174107e-02  2.24618465e-01
  5.19488454e-01  8.69650692e-02 -8.77085477e-02  7.40826279e-02
  2.21607219e-02 -1.20779589e-01  1.66502684e-01 -2.03452297e-02
 -1.37077257e-01  1.30961567e-01 -4.09880839e-02  2.24319130e-01
 -1.04448646e-01  2.83141583e-02 -7.22431615e-02 -2.57894993e-01
 -1.42017454e-01  3.60362947e-01  1.90780103e-01 -1.35627314e-01
  1.40783340e-01 -7.30748475e-02  5.28417081e-02  1.22551568e-01
 -9.36276168e-02  1.24049678e-01 -1.02519970e-02 -6.75448775e-02
 -1.57947421e-01  4.38134193e-01  8.31245035e-02  3.11355233e-01
  3.83733988e-01 -1.07594043e-01 -1.16556302e-01 -5.13871074e-01
 -9.45888907e-02  2.88212776e-01  4.66198996e-02 -1.15739331e-02
 -3.04888561e-02  1.13409638e-01 -1.08255222e-01 -4.25902009e-01
 -9.65252891e-02  4.08440411e-01 -1.99929625e-01  1.29871547e-01
 -6.32469729e-02  3.01744610e-01  2.09465563e-01 -3.21101606e-01
  3.07388306e-01 -3.75730574e-01 -1.21208921e-01  3.11354071e-01
 -3.31623077e-01  2.24495698e-02  1.89415276e-01  5.29530682e-02
 -9.24884081e-02  1.11725092e-01  1.54499590e-01  1.24563724e-01
 -3.39551032e-01 -3.45400125e-02 -2.75855571e-01 -1.04233190e-01
  8.64800215e-02 -1.46708280e-01 -3.01540762e-01 -3.66892338e-01
  2.01231033e-01  1.00998715e-01 -1.33466125e-02  2.84679923e-02
 -7.49834701e-02 -7.31851608e-02 -1.12935849e-01  1.63616925e-01
  1.66626409e-01 -7.71283507e-02 -5.39315522e-01  7.21804798e-04
 -4.02404189e-01  2.83901487e-02  2.21438874e-02 -1.96651548e-01
 -9.88144130e-02 -1.08285332e-02 -3.26962918e-02 -2.59850353e-01
  5.49333207e-02 -1.64551049e-01 -3.40842128e-01  1.52241781e-01
  7.94552490e-02 -1.96620464e-01 -4.60802205e-02 -3.46134365e-01
 -1.39923543e-01 -2.63125449e-01 -1.66937068e-01 -1.99629039e-01
 -4.88543138e-03  3.62133533e-01 -9.40483883e-02  2.05631822e-01
 -2.57508606e-01  4.51470912e-02 -3.78029197e-02  2.20057592e-01
  1.20893948e-01  2.82105654e-02 -4.86494005e-02  1.66450292e-01
 -4.82455909e-01 -1.35004818e-02 -2.66200975e-02  3.91330868e-01
  1.78007841e-01  4.11069393e-01  3.95075604e-02  2.68104523e-01
 -3.08167152e-02  2.90512472e-01 -2.49998033e-01  6.32922873e-02
  4.35889781e-01  3.31742316e-02  3.00844848e-01  1.45245809e-02
  1.35037359e-02  1.38573162e-02  2.80673057e-03 -9.32728723e-02
  6.73094988e-02  3.18078279e-01 -2.21957937e-01  1.53409764e-01
  3.92609298e-01  4.19079930e-01 -2.26863295e-01  2.98474133e-01
  1.72018975e-01 -2.11051941e-01  3.19753624e-02 -7.40091801e-02
  2.91492045e-01 -1.06855974e-01  6.04286138e-03 -1.44413829e-01
  1.96197361e-01 -2.18399316e-01  1.12364687e-01  1.61431506e-01
  3.16629350e-01 -9.54521447e-02  1.41092956e-01 -2.52556175e-01
  1.54997796e-01 -4.60272312e-01  2.82935016e-02 -5.02362251e-01
 -3.04963171e-01 -2.25205302e-01  2.01398581e-01  2.15199769e-01
  1.68107823e-01  3.09646547e-01 -1.88702613e-01  7.46146888e-02
  5.50177880e-02  2.09689274e-01  2.13632643e-01  3.53454471e-01
 -2.37602055e-01 -9.30810049e-02 -4.28575231e-03 -8.12160820e-02
 -2.15175524e-01 -2.54472122e-02  4.16257590e-01  1.08321123e-01
  6.65803015e-01 -1.78374827e-01  3.19939017e-01 -2.54175603e-01
 -2.23712921e-01  7.47697800e-02 -1.31449699e-01  2.98909396e-02
 -2.90959954e-01  5.46454728e-01  1.72276020e-01  4.58769500e-03
  6.53849840e-02 -3.25061858e-01 -1.17257401e-01  1.19263977e-01
 -3.36518139e-01 -1.24269120e-01 -1.68795303e-01 -2.39280343e-01
  5.83151281e-02  3.94378245e-01 -4.73080203e-02 -6.18317798e-02
 -2.86777556e-01 -5.91080859e-02 -5.40280640e-01  4.03129682e-02
 -3.83236706e-01  4.00300741e-01 -6.76785186e-02 -3.15651298e-01
 -3.74734789e-01 -9.20839608e-03 -1.29778504e-01 -3.60117167e-01
  1.52440533e-01 -1.64180085e-01  5.09994626e-01  3.34920079e-01
 -2.23825037e-01 -9.29428786e-02 -1.38386086e-01 -5.54162562e-02
  7.81380385e-02 -2.70631135e-01 -2.13835001e-01  5.32594956e-02
 -5.80401123e-02 -8.86775851e-02  4.86310452e-01  4.00552511e-01
 -2.15932548e-01  4.07240316e-02 -1.32568181e-01 -9.74407792e-02
  4.69149500e-02 -1.96659848e-01  1.27312973e-01 -7.58556128e-02
  2.71887124e-01  3.54568541e-01 -1.59405872e-01  2.95105577e-01
 -2.16575995e-01  2.44475245e-01  6.14442348e-01 -2.44157374e-01
 -4.97293055e-01 -4.34139632e-02  1.16103917e-01 -2.60519087e-01
 -3.67106616e-01 -1.10334024e-01 -5.69949411e-02 -8.29575360e-02]"
map_fn gives colocation errors with integer typed input tensors ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
CentOS Linux release 7.4.1708
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
binary (pip3 install tensorflow-gpu==1.9.0)
- **TensorFlow version (use command below)**:
v1.9.0-0-g25c197e023 1.9.0
- **Python version**:
3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.0.176
- **GPU model and memory**:
 GeForce GTX-1080 - 8GB memory
- **Exact command to reproduce**:
`python3 map_fn_GPU_error.py`

### Describe the problem
The `tf.map_fn` function raises the following error if I provide two tensors of input one of which is `tf.float32` and the other `tf.int64`. This error doesn't appear if both of them are `tf.float32`.

```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'map/TensorArray_1': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/device:GPU:0'
Colocation Debug Info:                                                                                                     
Colocation group had the following types and devices:
TensorArrayReadV3: CPU                                                                                                    
Enter: GPU CPU                                                                        
TensorArrayV3: CPU                         
TensorArrayScatterV3: CPU                                                                                                                                      
Placeholder: GPU CPU                                                                                   
                                                           
Colocation members and user-requested devices:                   
  RaggedLengths (Placeholder) /device:GPU:0
  map/TensorArray_1 (TensorArrayV3)                                                                                                                            
  map/TensorArrayUnstack_1/TensorArrayScatter/TensorArrayScatterV3 (TensorArrayScatterV3) /device:GPU:0
  map/while/TensorArrayReadV3_1/Enter (Enter) /device:GPU:0
  map/while/TensorArrayReadV3_1 (TensorArrayReadV3) /device:GPU:0
                                                  
         [[Node: map/TensorArray_1 = TensorArrayV3[clear_after_read=true, dtype=DT_INT64, dynamic_size=false, element_shape=<unknown>, identical_element_shapes=true, tensor_array_name=""""](map/strided_slice)]]
                                                                                                                            
Caused by op 'map/TensorArray_1', defined at:
  File ""map_fn_GPU_error.py"", line 33, in <module>                                                                              
    dtype=tf.float32)                                      
  File ""/my/python/package/path/tensorflow/python/ops/functional_ops.py"", line 420, in map_fn    
    for elem in elems_flat]                    
  File ""/my/python/package/path/tensorflow/python/ops/functional_ops.py"", line 420, in <listcomp>                               
    for elem in elems_flat]                      
  File ""/my/python/package/path/tensorflow/python/ops/tensor_array_ops.py"", line 754, in __init__
    name=name)                                                           
  File ""/my/python/package/path/tensorflow/python/ops/tensor_array_ops.py"", line 160, in __init__         
    self._handle, self._flow = create()            
  File ""/my/python/package/path/tensorflow/python/ops/tensor_array_ops.py"", line 157, in create                                 
    name=scope)   
  File ""/my/python/package/path/tensorflow/python/ops/gen_data_flow_ops.py"", line 7157, in tensor_array_v3
    tensor_array_name=tensor_array_name, name=name)                  
  File ""/my/python/package/path/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)                                                                    
  File ""/my/python/package/path/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)                                                                                                                                             
  File ""/my/python/package/path/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access    

```
This is triggered by the line
`sess.run(tf.global_variables_initializer())` in the code below. This code works perfectly on CPU however, I cannot seem to force placement on the GPU.

### Source code / logs

This is a minimal example to reproduce the above error. (This is the file named `map_fn_GPU_error.py` used in the command above)

```python
import tensorflow as tf
import numpy as np

with_gpu = True
if with_gpu:
    device_name = '/device:GPU:0'
else:
    device_name = '/device:CPU:*'

batch_size = 32
max_ragged_dim = 10
embedding_dim = 20


def ragged_function(ragged_input, length_of_input):
    # length_of_input is not used here but is in the actual function
    # The error is caused whether or not it is used
    return tf.reduce_sum(ragged_input, axis=0)


def final_loss_function(ragged_function_outputs):
    weights = tf.get_variable('LossWeights', dtype=tf.float32,
                              shape=(ragged_function_outputs.shape[1], 1))
    return tf.reduce_sum(tf.matmul(ragged_function_outputs, weights))


with tf.device(device_name):
    input_ragged_mat = tf.placeholder(name='RaggedMatrix', dtype=tf.float32,
                                      shape=(None, max_ragged_dim, embedding_dim))
    input_ragged_lengths = tf.placeholder(name='RaggedLengths', dtype=tf.int64,
                                          shape=(None,))

    ragged_function_outputs = tf.map_fn(lambda x: ragged_function(*x),
                                        [input_ragged_mat, input_ragged_lengths], 
                                        # Note that input_ragged_lengths is of type int64
                                        # This leads to an error on GPU placement
                                        dtype=tf.float32)
    final_loss = final_loss_function(ragged_function_outputs)

    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)
    train_op = optimizer.minimize(final_loss)

# Create input ragged_matrices, with zeros beyond the actual length
ragged_input_mat = np.random.random(size=(batch_size, max_ragged_dim, embedding_dim))
ragged_input_length_mat = np.random.randint(max_ragged_dim, size=(batch_size,)) + 1
for i, rlen in enumerate(ragged_input_length_mat):
    ragged_input_mat[i, rlen:max_ragged_dim] = 0

with tf.Session() as sess:
    feed_dict = {input_ragged_mat: ragged_input_mat,
                 input_ragged_lengths: ragged_input_length_mat}

    sess.run(tf.global_variables_initializer())

    print(""Calculating Final Loss"")
    sess.run([final_loss],
             feed_dict=feed_dict)
    print(""Running Train operation"")
    sess.run([train_op],
             feed_dict=feed_dict)
```",True,"[-3.14325094e-01 -7.38003671e-01 -2.84440666e-01 -4.15661335e-01
 -1.22990645e-02 -4.83994782e-01 -8.80423933e-02 -1.15161784e-01
 -1.54284462e-01 -4.71603796e-02  9.24247690e-03  1.59587294e-01
 -1.05141595e-01  6.26685768e-02 -4.65808600e-01  2.08009735e-01
 -1.98578715e-01 -3.01997900e-01  1.81990325e-01  9.91436839e-02
 -1.83484033e-01 -3.74774560e-02 -1.73749626e-01  1.79989457e-01
 -6.59224465e-02  3.42350066e-01  1.18703723e-01  1.60428919e-02
  7.41244033e-02 -8.57997984e-02  4.90135819e-01  2.07479879e-01
 -7.03235865e-02  2.93268085e-01 -9.91587639e-02  3.38751018e-01
 -1.90918058e-01 -1.84878409e-01 -2.44082846e-02 -1.24946289e-01
  1.03732556e-01  8.07857513e-02  1.96580589e-01  3.12056914e-02
  4.51021165e-01  2.41523564e-01  1.64626930e-02  6.76349103e-02
 -1.83884650e-01 -1.48564681e-01  3.39895904e-01 -1.98909380e-02
 -2.48953551e-01 -9.47064757e-02 -7.86634013e-02 -3.16085964e-02
  5.50860399e-03 -1.68405652e-01  1.14039600e-01  2.16406927e-01
  1.35581389e-01 -1.67868167e-01 -4.86131944e-02 -1.73683353e-02
  2.85144925e-01  1.18509792e-01  2.32320040e-01 -3.44222456e-01
  3.77160549e-01 -2.66067684e-04  2.07716048e-01  6.06760755e-02
 -1.68738574e-01 -6.86140805e-02  3.31177413e-01  3.75113517e-01
 -5.74022252e-03  1.94206774e-01  1.38758257e-01 -8.25220719e-02
  2.06002221e-01  7.51052052e-02  3.98272067e-01  1.91745125e-02
  1.24288343e-01 -7.17348158e-02  2.40835756e-01  3.25814545e-01
  1.47056401e-01 -2.76147783e-01  3.86898965e-01  1.97217464e-01
  5.33003062e-02  9.91113633e-02  3.19296718e-01  3.66044551e-01
 -8.36041868e-02 -8.76526088e-02 -1.20911440e-02 -5.87511882e-02
 -9.61452350e-02 -2.21801549e-01 -1.89985842e-01  6.74576610e-02
 -1.46334201e-01  6.92172647e-02  2.29621738e-01 -1.06088236e-01
 -1.34110719e-01 -1.20869070e-01  8.27335492e-02  1.18534537e-02
  3.38304490e-02 -5.76097667e-02 -6.43032789e-02 -3.62710357e-02
 -8.66587535e-02 -1.03334300e-02 -4.51890416e-02  3.41415286e-01
 -4.21793789e-01 -2.50464141e-01  5.79677001e-02  1.94143923e-03
  2.59675652e-01  9.17113274e-02 -3.20847750e-01 -1.70258224e-01
 -2.92032838e-01  2.29305327e-02  1.31613180e-01 -9.25733447e-02
  2.92137358e-02  6.41512275e-02 -5.16080596e-02 -4.00885120e-02
 -3.71399596e-02  9.24849436e-02 -3.43452454e-01 -1.27672479e-01
  1.41915120e-03  9.24237072e-03  1.44936606e-01  4.03620824e-02
  2.29534268e-01 -2.84404866e-02 -1.63264364e-01  8.67859125e-02
  8.19602795e-03  1.50689423e-01 -7.17377216e-02 -1.11307904e-01
  1.43832028e-01  4.50350761e-01  3.76999974e-02  3.38310897e-01
  4.52926338e-01 -1.97606429e-01 -8.38372707e-02 -4.19391990e-01
 -3.66866626e-02  1.15509652e-01 -1.36397719e-01  2.01815814e-01
  7.44685978e-02  2.17463464e-01 -2.34976590e-01 -2.68305242e-01
 -8.33889246e-02  1.30376071e-01 -4.28189993e-01  8.60957056e-02
 -3.20298523e-02  1.74238324e-01 -1.68292206e-02 -2.86535203e-01
  2.93811649e-01 -4.20218468e-01 -3.35196853e-01  1.72230244e-01
 -9.28363651e-02 -2.08634868e-01  2.88406968e-01 -9.65347141e-02
 -4.17235345e-02  2.55205870e-01  2.02168792e-01  2.73927927e-01
 -1.11213431e-01  6.23074546e-03 -2.40843192e-01 -2.70935111e-02
  1.76597387e-03 -1.67235881e-02 -1.46896988e-01  2.48313770e-02
  2.73552597e-01  1.26185298e-01 -7.99971372e-02  2.76967049e-01
 -1.01655833e-01 -2.62818038e-01 -1.26097530e-01 -8.36805850e-02
 -1.01735726e-01 -3.19992691e-01 -2.40616530e-01 -1.28760949e-01
 -3.49687874e-01  1.99627385e-01  1.10550299e-01 -3.21449548e-01
  8.22812468e-02  9.85925794e-02 -2.65831113e-01 -1.38762563e-01
 -2.20831707e-02 -3.17163914e-02 -9.46130753e-02  2.36286566e-01
 -3.40906382e-02 -1.40819415e-01 -8.28372985e-02 -2.90324509e-01
 -3.28893721e-01 -1.11025572e-01 -3.91287416e-01  1.28257144e-02
 -8.85831714e-02  2.48769112e-02 -7.72158056e-02  1.12830065e-01
  7.44684711e-02 -1.47717753e-02  1.74720854e-01 -9.70423073e-02
  5.29017746e-02 -1.39994845e-01 -1.45015880e-01  2.37216160e-01
 -7.05103397e-01 -2.67360330e-01  5.86680919e-02  6.10701516e-02
  6.14422709e-02  2.59583682e-01 -2.61083953e-02  4.10803676e-01
 -4.66802597e-01  3.34926307e-01 -1.78860500e-03 -1.33587003e-01
  3.88002157e-01  2.21865147e-01  7.05082491e-02  5.13667800e-03
  1.38898388e-01  1.47401392e-01  2.15518624e-01  6.56805858e-02
  3.15032661e-01  3.34633768e-01  3.93978171e-02  4.21499282e-01
  4.10249829e-01  4.49026436e-01 -1.87612236e-01  2.00746208e-01
 -1.22413576e-01 -3.32385838e-01  1.84800744e-01  1.70512691e-01
  4.55651015e-01 -4.04907495e-01  1.76049709e-01 -1.03093252e-01
  4.67237771e-01 -3.68194133e-02 -6.39906228e-02  3.07558626e-01
  1.10027045e-01  5.98657019e-02  6.14355505e-03  1.57882750e-01
  7.95257837e-02 -4.34590310e-01 -1.31586641e-02 -4.62779671e-01
 -1.35422707e-01 -1.25272155e-01 -6.65187240e-02  6.29404485e-02
  2.73841083e-01  4.49464321e-01 -1.40950397e-01  1.48831606e-01
  3.36831287e-02  5.29742166e-02  1.85268044e-01  3.77133220e-01
 -1.25920594e-01 -9.46033895e-02  1.74227715e-01 -4.27678302e-02
 -1.78460449e-01  7.90807009e-02  2.05076247e-01  2.84101129e-01
  5.81611633e-01 -3.65602076e-01  2.88763583e-01 -2.68270403e-01
 -2.47320890e-01  2.25739524e-01  4.12416942e-02  9.35222208e-02
 -4.42391604e-01  4.91870195e-01  7.64565766e-02 -1.33194234e-02
  2.00698618e-03 -3.18457007e-01  1.36522958e-02  3.21165286e-02
 -7.03592449e-02  3.35154720e-02  1.12458527e-01 -1.43402368e-01
 -9.40204412e-02  2.07822740e-01 -2.93491125e-01 -2.25574747e-02
 -2.55084097e-01 -4.28367332e-02 -1.46500379e-01  1.84752252e-02
 -2.74165124e-01  3.62495095e-01 -2.25909892e-03 -1.69618905e-01
 -1.14374459e-01 -1.13947555e-01  1.99221354e-03 -2.78131485e-01
 -3.24306414e-02 -1.86267883e-01  4.60489213e-01  2.12443262e-01
 -2.17302024e-01  1.98808685e-01 -9.78923291e-02 -2.00523779e-01
 -1.44090392e-02 -9.88144204e-02 -4.32650387e-01  1.65424153e-01
 -9.34053585e-02 -7.58430064e-02  2.01444983e-01  3.06189150e-01
 -7.42839277e-02 -3.28184292e-02 -3.88976365e-01 -1.59998789e-01
 -1.41084731e-01 -2.66056716e-01 -1.09663233e-01  6.56636525e-03
  1.50789693e-01  2.36065626e-01  1.29274530e-02  9.21109095e-02
 -1.19900122e-01  9.81666073e-02  5.86188197e-01 -1.62440673e-01
 -2.39493817e-01  1.18464679e-01  2.61593331e-02 -1.55122533e-01
 -2.26571530e-01 -5.75412624e-03 -5.85762560e-02 -9.58012491e-02]"
"""not all arguments converted during string formatting"" in rnn_cell_impl stat:contribution welcome type:bug","I believe there's a mistake in the ```BasicLSTMCell.build``` method:

```python
raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""
                       % inputs_shape)
```
will be called with inputs_shape being a tuple and % formatting will interpret this tuple as multiple formatting arguments. Can we change it to

```python
raise ValueError(""Expected inputs.shape[-1] to be known, saw shape: %s""
                       % (inputs_shape,))
```
?",True,"[ 2.15469711e-02 -1.96229175e-01 -1.85530633e-01 -5.73776923e-02
 -2.30525732e-01 -3.44344705e-01  1.25843212e-01  7.27825612e-02
 -3.81544948e-01 -2.00981498e-02 -1.64478600e-01  1.37905292e-02
  1.42649978e-01 -1.71602853e-02 -5.24850935e-02  3.16672504e-01
 -2.59900153e-01  9.35998112e-02 -1.25757933e-01  1.23784535e-01
 -7.14691207e-02  8.68761390e-02 -9.70044285e-02 -4.33300175e-02
  1.65386856e-01  3.59503962e-02  1.30180687e-01  4.67908494e-02
  2.60606647e-01  3.06649804e-01  7.27903023e-02  6.52443618e-02
 -1.21040925e-01  2.39806309e-01 -2.11583927e-01  1.92825586e-01
 -4.90245879e-01  8.49057436e-02  1.45824105e-01  1.16461724e-01
 -4.93793711e-02 -3.18757407e-02  1.28229242e-02 -5.83921485e-02
 -8.84569660e-02  2.94806808e-03 -2.43390083e-01  2.83079207e-01
 -1.74238950e-01 -1.20242745e-01  3.05376761e-02  2.76575297e-01
 -4.42077637e-01 -3.43133688e-01 -1.17054302e-03 -4.00119349e-02
 -5.98587096e-02  1.58450067e-01 -2.94538755e-02 -3.41640353e-01
 -1.96216047e-01  3.21728736e-02  3.67798001e-01 -8.54001939e-02
 -2.58804202e-01  2.11776525e-01  9.04965177e-02  2.24023938e-01
  3.46045017e-01 -1.98139787e-01  2.79763758e-01  2.72836477e-01
 -5.67251258e-02 -1.46410227e-01  1.52245117e-02  3.71176124e-01
 -3.32930088e-01  9.13443789e-02  8.67740512e-02 -2.83470213e-01
 -4.64613289e-02  1.37992293e-01  1.23681456e-01 -1.26637772e-01
  5.12978554e-01  4.52938825e-02  2.14777648e-01 -1.78099096e-01
  1.42301768e-02  2.85529971e-01  3.06851238e-01 -1.70118287e-01
 -2.39933446e-01  2.74440706e-01 -3.22425067e-02  2.76001811e-01
  4.65329885e-02 -9.14081633e-02 -2.98793018e-01 -4.01981354e-01
  4.64491248e-02  5.52433319e-02 -1.59109652e-01  2.31413677e-01
  5.43495417e-02  5.18715531e-02  3.02751780e-01 -7.56818578e-02
  6.58499971e-02 -1.62621349e-01  1.81510940e-01 -2.07369000e-01
 -8.13635159e-03 -1.29235461e-01 -5.60456887e-02  2.29976714e-01
  1.70391232e-01 -3.78961787e-02 -1.25866622e-01  1.94096476e-01
  1.01506889e-01  2.36759067e-01 -8.84508938e-02  3.62866193e-01
  3.23668942e-02  1.30803153e-01  8.45188871e-02 -1.70348942e-01
 -1.39527798e-01 -1.04099624e-01  2.28950083e-01  1.65085897e-01
 -1.36288658e-01  1.49185374e-01 -2.70997472e-02 -1.10407695e-01
 -9.65664685e-02 -2.31897399e-01 -3.54944110e-01  2.21195631e-02
  1.64861441e-01 -3.65682274e-01 -1.53143466e-01 -3.39812219e-01
  1.24859184e-01  7.95038939e-02 -4.17371035e-01  4.22132313e-02
 -8.56462270e-02  6.57033771e-02 -3.62482443e-02  8.67275149e-02
 -7.24556386e-01  3.38059932e-01  2.27354169e-02  6.64221048e-02
  1.24013200e-01 -1.83619484e-02  1.24652758e-01 -5.31920552e-01
 -1.32098168e-01  1.75882787e-01 -3.39856237e-01 -7.61723295e-02
  1.00909591e-01  1.44332737e-01 -1.16141714e-01  1.41368821e-01
 -3.89194608e-01 -2.10001916e-02 -1.01006411e-01  1.60940170e-01
 -7.79805630e-02 -3.09789658e-01  1.40938133e-01  1.21939801e-01
 -5.27406454e-01 -3.32066715e-01  1.82031780e-01  1.28791332e-01
  5.19508243e-01  3.20350289e-01  7.50656892e-03 -1.08673409e-01
  2.93674320e-03 -3.70384827e-02  1.01656355e-02  2.09416077e-02
  3.03616583e-01  2.68363655e-01 -4.87451367e-02  7.56175071e-02
 -3.17443132e-01  2.51156222e-02  4.42300886e-02 -1.19960099e-01
  5.58831245e-02  9.02619660e-02  8.73171464e-02  1.83018342e-01
 -3.96815896e-01  1.26660183e-01 -4.85318080e-02  7.99270626e-03
  2.67749310e-01 -1.73287764e-01 -1.09095402e-01 -3.65078896e-01
  2.32605767e-02  2.76707649e-01 -1.90676033e-01 -2.81089246e-01
  1.23574033e-01 -2.93417573e-01 -1.15630448e-01 -1.22030064e-01
  1.11522257e-01 -3.06450147e-02 -4.02957872e-02 -2.25464046e-01
 -1.23723403e-01 -6.16074950e-02  3.46019685e-01 -1.71757966e-01
  2.07784116e-01  2.80104280e-01 -1.77856594e-01 -1.22748181e-01
  3.89565006e-02  1.00384735e-01  2.38895833e-01 -1.56170323e-01
  1.02955043e-01 -1.15950897e-01  4.08480782e-03  1.16210282e-01
 -1.79459572e-01 -6.37698695e-02  1.81265086e-01  1.21739745e-01
 -3.89493108e-01  1.46358103e-01 -1.84457563e-02 -4.01585214e-02
 -1.48523152e-01  1.06927611e-01 -2.13953629e-02  1.39443994e-01
 -4.78037059e-01 -1.55194595e-01 -2.12323278e-01 -2.27706641e-01
  7.75128841e-01  1.81217045e-01  2.91506618e-01  1.82376638e-01
  2.02798963e-01  2.32027516e-01 -9.27181691e-02 -1.17856130e-01
  1.37163877e-01 -8.88827145e-02  3.82753275e-02  1.68138444e-01
  1.05901413e-01 -2.40728229e-01 -2.66452283e-01  2.61273563e-01
  3.51704136e-02 -5.86860478e-02  2.62227952e-01  9.37748104e-02
  2.59171426e-01 -3.27893704e-01  3.04584622e-01 -7.17576817e-02
  3.15829873e-01  2.28925288e-01  1.92065269e-01 -2.07682967e-01
 -1.59308583e-01 -8.31913278e-02 -3.18793893e-01  3.17699462e-01
  1.75762445e-01 -1.55774653e-01 -8.59512985e-02 -4.91550863e-02
 -2.41751730e-01  1.20141834e-01  2.80311089e-02  3.71631235e-02
  1.10979877e-01 -2.24719234e-02  1.26570985e-01  2.41743296e-01
 -7.47804046e-02 -1.78013861e-01 -1.42962299e-03  6.13518879e-02
 -8.36950839e-02  9.47417468e-02 -1.15479603e-01  2.17937589e-01
 -4.26506996e-02 -1.59920193e-02 -1.20132744e-01 -7.88094252e-02
 -1.15180477e-01 -3.23082238e-01  1.77702338e-01  3.34145367e-01
 -3.34893972e-01  2.88667768e-01 -1.85260370e-01  4.00635675e-02
  1.47637218e-01  6.20573640e-01  5.06210476e-02  5.04152551e-02
 -4.66585010e-01 -9.71505940e-02  1.87150687e-02  9.47142541e-02
 -1.34265080e-01  5.90460449e-02 -1.64477855e-01 -6.89754263e-04
 -8.22314471e-02 -5.65399788e-02 -8.40040222e-02  1.36092484e-01
 -8.54678452e-02 -1.84142083e-01  1.55091107e-01 -9.72715318e-02
 -1.28115565e-01  2.43478239e-01  8.40792805e-02  1.19990140e-01
 -3.45589630e-02 -6.47345111e-02  1.50300995e-01 -4.22692031e-01
 -1.08430669e-01 -3.55558902e-01  2.03486551e-02  5.33399045e-01
 -4.40054908e-02 -6.65501803e-02  2.44127482e-01 -9.43780839e-02
 -1.19874358e-01  4.35175933e-03 -3.24437857e-01  2.34150633e-01
  1.13311056e-02  1.01851478e-01 -1.09103136e-01  3.20968777e-01
 -5.54949701e-01 -7.37041757e-02 -3.24402928e-01 -1.63472414e-01
  6.29637390e-02 -1.47545755e-01 -4.99938689e-02  4.98379394e-02
  8.75493214e-02  1.40619755e-01 -3.21990103e-02  1.48036271e-01
 -2.91845966e-02  1.83059931e-01  8.21140632e-02  5.27211428e-01
 -1.52652189e-01  3.71194631e-01  1.17854707e-01 -1.35861158e-01
 -1.29428610e-01  3.44285846e-01  4.80079651e-02  1.86764568e-01]"
TensorFlow GDR RDMA verbs compilation failure on 1.10  stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS Linux 7.4.1708
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.10.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.16.1
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: 9.2.1.88
- **GPU model and memory**: nVidia Volta V100 16GB
- **Exact command to reproduce**: bazel build --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

I attempted to compile TensorFlow 1.10.0 using RDMA/VERBS options+GDR options as I'm using an all infiniband EDR network with my Volta V100 HPC environment. The same compile-time strings for Bazel build worked correctly without this error in RC 1.0.8rc1 and this error was not thrown. I am using mvpachi2 + GNU GCC 7.2.0, CUDA 9.2.8.11 + Basel 0.16.0 at compile time. NCCL is at v 2.2.13. The error basel throws after 16xxx objects is as follows:
                                                                                                                                      ^
```
At global scope:
cc1plus: warning: unrecognized command line option '-Wno-self-assign'
ERROR: /opt/ohpc/pub/apps/tensorflow_1.10/tensorflow/BUILD:576:1: Executing genrule //tensorflow:tensorflow_python_api_gen failed (Aborted): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)
2018-08-18 16:25:22.109083: F tensorflow/core/framework/allocator_registry.cc:52] New registration for AllocatorFactory with name=BFCRdmaAllocator priority=101 at location tensorflow/contrib/gdr/gdr_memory_manager.cc:204 conflicts with previous registration at location tensorflow/contrib/verbs/rdma_mgr.cc:277

INFO: Elapsed time: 934.788s, Critical Path: 236.40s
INFO: 13405 processes: 13405 local.
FAILED: Build did NOT complete successfully

```

### Source code / logs
The above is about all I've gotten out of it, thus far. There is no obvious compile time log output from Basel-build that I can see of use. 
",True,"[-0.25087276 -0.5891354  -0.19478106 -0.29722756 -0.03205315 -0.2227444
  0.09240131 -0.01129454 -0.2760979   0.00632233  0.16706222  0.00740297
  0.075891    0.13954267 -0.26926225  0.22477746 -0.11995904 -0.4447251
 -0.02150761  0.1216058  -0.10025103 -0.04950226 -0.19066873  0.17284577
  0.03482164  0.44315004 -0.07475121 -0.12571853  0.19622748  0.16190328
  0.5599513   0.14640382 -0.0110163   0.17429891 -0.00971903  0.31374663
 -0.1823537  -0.06053589  0.05486564 -0.02570073 -0.10816639 -0.32460275
  0.21465172  0.04614498 -0.02512975  0.01631433  0.04987607  0.15653422
 -0.10874476 -0.1257945  -0.02517531 -0.02700896 -0.4565497  -0.15888858
  0.03647091  0.21796444  0.06582795  0.0153115   0.0469759   0.26037154
  0.07916342 -0.08960963 -0.16580282  0.14286987  0.0965145   0.11549714
  0.224708   -0.28669804  0.38549316  0.26690835  0.151073    0.20063683
 -0.40641218 -0.01268613  0.0267931   0.29751801  0.0549223   0.02163482
  0.2694361  -0.27270377  0.08819573  0.10042244  0.40257612  0.08834955
  0.03823829 -0.07058413  0.1289366   0.04051846  0.25887224 -0.30517405
  0.22072592  0.08784892  0.06952314 -0.01756223  0.24452016  0.11443979
 -0.10955197  0.1865972  -0.03170324 -0.01817644 -0.08844517 -0.10446145
 -0.310637    0.05050611 -0.20552102 -0.12709361  0.00573015 -0.03633137
 -0.02449146  0.0194641   0.02597987  0.05634122  0.04031963 -0.10999166
 -0.06286856 -0.00417972 -0.16497509 -0.13207936 -0.2721225   0.30719
 -0.1840795  -0.1492684   0.19806506  0.12865523  0.27458012 -0.08174917
  0.03759173  0.01751592 -0.01966363 -0.05234201  0.31559852  0.03372063
 -0.13184714  0.20917991 -0.2111587  -0.0369029   0.0819381  -0.08816144
 -0.13361004 -0.05082627 -0.18262063  0.15121509  0.26160365 -0.01151123
  0.19393697 -0.08795458 -0.16105032  0.14409567  0.14473891  0.06617945
  0.05507558  0.02351668 -0.02443436  0.27926463  0.32198372  0.3909299
  0.35385948 -0.04650978 -0.15372926 -0.59348416 -0.01744118  0.18429193
 -0.10715362  0.05444635  0.03809578  0.07960483 -0.24356031 -0.19413534
 -0.1471634   0.17584035 -0.3207491   0.05531288 -0.10987565  0.29827553
  0.05806397 -0.19880396  0.198771   -0.50485265 -0.23898329  0.25514758
  0.05811521 -0.09237219  0.23002602 -0.05203974 -0.10878492  0.23687357
  0.04237703  0.11395885 -0.23039877 -0.21547443 -0.37022772  0.11091067
 -0.08076329  0.06958297 -0.06497999 -0.21917349  0.27444205  0.02316316
  0.03023648  0.3335292  -0.16175352 -0.20444518 -0.02433351 -0.06078245
  0.07530902 -0.12955981 -0.5039571  -0.1687638  -0.14153625 -0.08379275
  0.02185572 -0.21880637  0.00916452  0.01901963 -0.23143841 -0.19930732
 -0.2617288   0.08171175 -0.39011753  0.00734186  0.00095854 -0.020086
  0.00962435 -0.31616318 -0.37260938  0.03183413 -0.14174248 -0.02416824
  0.07724451  0.27605808  0.02187325  0.07118277  0.00318753 -0.09137873
 -0.01612306 -0.06342342  0.01447651 -0.1961125  -0.14101085  0.13139047
 -0.5152808  -0.1069825  -0.19123757  0.3060295   0.09580636  0.34287655
 -0.12477814  0.38709548 -0.2180239   0.26879254 -0.12670982 -0.09606415
  0.31814957  0.16120192  0.21327624  0.03468087 -0.01455056 -0.00778729
  0.22932017  0.1278368   0.12806244  0.14838126 -0.04741888  0.35627255
  0.43671313  0.23713782 -0.14505652  0.29089633 -0.01828887 -0.25989765
 -0.19636916 -0.15219375  0.19590451 -0.18682581 -0.0923091  -0.05609294
  0.40728638 -0.08952663 -0.01336595  0.10732658  0.25063094  0.05641483
 -0.04110618  0.19502844 -0.13557711 -0.4686722   0.02767253 -0.34431148
 -0.1584116  -0.1375274   0.04631142  0.3006717   0.26217258  0.20607494
 -0.11021109  0.0572244  -0.08488658  0.08811289  0.1979456   0.2977805
 -0.31393582 -0.06272655  0.13608447  0.07380077 -0.12160815 -0.11414331
  0.53894925  0.157395    0.38601023 -0.2510942   0.4149718  -0.18703587
 -0.20279011  0.47225952 -0.07338697  0.04044003 -0.37284058  0.5321114
  0.04937579  0.12392848  0.07076535 -0.32633    -0.02727281 -0.12755848
  0.1619971  -0.11454743 -0.00241809 -0.10564958 -0.07242059  0.19421777
 -0.1112646  -0.0382032  -0.20284837  0.1538373  -0.19089347  0.06153449
 -0.28385496  0.25657994 -0.05173668 -0.11728072  0.03758696 -0.09473305
 -0.1109646  -0.18139526 -0.10798628 -0.13615441  0.48970455  0.38291204
 -0.18471602  0.3798616  -0.03423015  0.06308664 -0.1498632  -0.24888882
 -0.51640844  0.08779113  0.17625824 -0.09471348  0.34429395  0.2572903
 -0.28134227 -0.04549323 -0.20167568  0.06555064 -0.01974132 -0.11216089
 -0.20290303 -0.17200425  0.18921977  0.15617687 -0.27313614  0.1309902
 -0.2255766   0.1465594   0.643694   -0.38539335 -0.26657665  0.00628339
  0.11088879 -0.15159582 -0.11481892  0.02751302  0.0018596  -0.16348407]"
_UnreadVariable name property fails in TF 1.10-rc1 ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOSX 10.13.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.0-rc0-18-ge5e9a8f4e9 1.10.0-rc1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `tfe.Variable(1).assign(2).name` in eager mode.

### Describe the problem
In eager mode, the result of an assignment is an instance of the class `tensorflow.python.ops.resource_variable_ops._UnreadVariable`. When you call its `name` property, it tries to access `self._parent_op.name`, but its `_parent_op` attribute is `None`, so there is an exception. This is a regression in TF 1.10-rc1, since it worked fine in TF 1.9.0.

### Source code / logs

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
tf.enable_eager_execution()

tfe.Variable(1).assign(2).name
```

Outputs the following exception:

```python
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1082, in name
    return self._parent_op.name
AttributeError: 'NoneType' object has no attribute 'name'
```

Since `str()` and `repr()` use the `name` property, they fail as well. This makes it impossible to call `assign()` in a Python shell, since the shell automatically tries to display the output:

```python
>>> import tensorflow as tf
>>> import tensorflow.contrib.eager as tfe
>>> tf.enable_eager_execution()
>>> tfe.Variable(1).assign(2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 264, in __repr__
    self.name, self.get_shape(), self.dtype.name,
  File ""/Users/ageron/.virtualenvs/ml/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1082, in name
    return self._parent_op.name
AttributeError: 'NoneType' object has no attribute 'name'
```",True,"[-3.05218756e-01 -6.65577412e-01 -4.16889518e-01 -2.31921077e-01
  1.72021329e-01 -3.30467492e-01  8.03517923e-02 -1.66259706e-02
 -7.07115158e-02 -3.99135686e-02  1.54586494e-01 -8.99209604e-02
 -1.12984516e-01  8.13455135e-02 -1.54362798e-01  3.53313446e-01
 -1.56145245e-01 -4.43178833e-01  1.27704963e-01  1.50299907e-01
 -8.33291411e-02 -8.66074786e-02 -1.70862079e-01  3.38721693e-01
 -6.50741160e-04  1.49912745e-01 -9.72744077e-02  1.07042026e-03
  6.13250211e-02  8.84867311e-02  4.41086888e-01  1.02364071e-01
 -1.06532320e-01  1.14395335e-01  4.35148180e-02  3.05940956e-01
 -2.94233978e-01 -1.37612984e-01 -2.45087877e-01 -1.14909828e-01
  5.43463603e-02 -1.11173570e-01  1.19265750e-01  8.92062411e-02
 -6.18552640e-02  1.71523333e-01  6.25013188e-03  3.94771844e-02
 -1.75813705e-01 -7.73107484e-02  2.63427049e-02 -8.13459903e-02
 -5.47749937e-01 -1.99247837e-01 -6.17623031e-02  7.13934749e-02
  3.60320732e-02 -1.54988170e-01  5.46600968e-02  1.37264371e-01
 -9.43459477e-03 -3.91571075e-02 -1.45758182e-01  1.35661870e-01
  1.52250737e-01  2.11945876e-01  1.27747357e-01 -2.29390606e-01
  3.70472908e-01 -8.38562548e-02  2.09880784e-01  2.83588041e-02
 -1.05647705e-01  1.35341957e-01  9.85358730e-02  3.60801905e-01
 -9.45574120e-02  3.00051004e-01  1.08815774e-01  2.49604881e-03
 -2.53426917e-02 -1.19549349e-01  2.49486282e-01 -6.52663857e-02
 -4.69777361e-03  1.94395080e-01  2.80755758e-01 -1.03129759e-01
  1.78934664e-01 -1.16134211e-01  3.99909556e-01  1.39689177e-01
  1.17055560e-02  1.21490233e-01  2.96829998e-01  2.16432542e-01
 -5.31887859e-02  1.72926217e-01 -8.11755583e-02  5.51586598e-02
 -2.11824343e-01 -7.89984912e-02 -9.17800367e-02  1.50140628e-01
  6.67363852e-02  1.45428881e-01  1.17149316e-01 -2.56465018e-01
  9.87612922e-03 -2.30095476e-01 -2.54859254e-02 -9.79428366e-03
  4.38089669e-03 -1.10095069e-01 -1.79708838e-01 -4.79839221e-02
  1.98606968e-01  5.94146699e-02 -6.69112131e-02  2.40954742e-01
 -6.05954342e-02 -1.72473803e-01  5.53790554e-02 -3.80119085e-02
  3.77866030e-01  1.57510519e-01 -1.88486040e-01 -4.36721276e-03
 -1.35526419e-01 -1.37203395e-01  3.02890867e-01  8.66592228e-02
 -3.32568735e-02  1.33035615e-01 -1.15332920e-02  4.61953282e-02
 -3.90589178e-01  1.84755817e-01 -3.06272388e-01  3.24004889e-03
  6.90858066e-02  1.56542093e-01  1.82965606e-01 -6.84549287e-02
  2.60151774e-01 -1.41098544e-01 -1.07863747e-01  1.45702675e-01
 -9.16473120e-02  2.12035328e-01  2.60611288e-02 -4.27979156e-02
  1.66405112e-01  3.21695864e-01 -6.07806779e-02  1.94458753e-01
  2.92494625e-01 -7.22587258e-02  9.30922851e-03 -5.81937671e-01
 -1.17510095e-01  9.24416035e-02 -3.98494974e-02  7.69309551e-02
  2.06222862e-01  1.51941061e-01 -2.70657718e-01 -3.47610079e-02
 -2.76415292e-02 -2.72062495e-02 -2.72842795e-01  9.24911797e-02
 -3.17899734e-01  2.77811170e-01  1.24409147e-01 -1.81292310e-01
  4.14415061e-01 -5.70687175e-01 -1.67456046e-01  1.05120949e-01
  2.29022037e-02 -1.09187439e-01 -9.25405547e-02 -9.21939015e-02
 -1.51837587e-01  7.73609206e-02  1.12822682e-01  3.97657827e-02
 -1.28406882e-01 -1.01055615e-01 -1.46773994e-01 -1.15606003e-02
  2.11430073e-01 -5.56591898e-02 -5.66458404e-02 -1.42679662e-01
  3.31648052e-01 -1.62689999e-01  1.10928349e-01  2.83642411e-01
 -1.15878657e-01 -1.85852066e-01  5.85562289e-02  4.14493605e-02
 -1.13075510e-01 -2.23146111e-01 -9.50276330e-02  5.11523150e-03
 -2.69934267e-01  2.43712455e-01  2.05261260e-01 -9.21566188e-02
 -1.57153189e-01  1.38292804e-01 -1.66996151e-01 -2.89944828e-01
 -1.63694784e-01  1.70326866e-02 -2.66151041e-01  2.85060763e-01
  2.11504191e-01 -6.50208443e-02  9.21208113e-02 -2.41314545e-01
 -3.22967827e-01 -1.60997689e-01 -1.94732562e-01  6.15836047e-02
 -6.30292445e-02  1.74004376e-01 -1.17290303e-01  1.48171127e-01
 -5.05770072e-02  5.16593307e-02  2.53013253e-01 -7.71145299e-02
 -1.31255060e-01 -3.69192034e-01  3.67211178e-02  7.28544593e-02
 -4.97760415e-01 -2.09003687e-01 -7.96600059e-02 -1.01887971e-01
  1.07678644e-01  5.24582863e-02 -1.09418638e-01  1.62795976e-01
 -1.30069122e-01  1.65488943e-01 -1.93050742e-01 -3.20725702e-02
  3.36297214e-01  2.89699733e-01  1.21909663e-01  3.00857097e-01
  3.30284894e-01  2.14918643e-01  1.65901914e-01  1.35873422e-01
  2.33331658e-02  6.08928204e-02  1.93423003e-01  4.03012216e-01
  3.26814502e-01  3.70890260e-01 -2.86895856e-02  2.03295946e-01
 -1.14955261e-01 -3.32781523e-01 -4.88290973e-02  7.18769059e-02
  6.12469196e-01 -3.67284864e-01  2.69559115e-01 -5.16844243e-02
  3.88461441e-01  6.52719010e-03 -1.50006488e-01  2.71847546e-01
  2.60012150e-01 -1.04141243e-01 -7.21627846e-02  1.32456422e-01
  1.02255896e-01 -1.49565458e-01 -1.27234191e-01 -4.08955455e-01
 -1.49312943e-01 -2.14103490e-01 -9.17795449e-02  3.95543091e-02
  1.65656537e-01 -7.13650733e-02 -7.25208372e-02  5.35043925e-02
 -6.61077648e-02 -2.24772915e-02 -8.13806616e-03  3.32876801e-01
 -3.56525145e-02 -3.73689532e-02  2.25447454e-02  3.08228135e-02
 -5.56935742e-02  4.95901182e-02  4.45964873e-01  2.04506695e-01
  3.86228919e-01 -2.80791730e-01  3.24758649e-01 -2.99172282e-01
 -1.95639849e-01  2.48804748e-01 -5.80489542e-03  8.57064128e-02
 -1.56489477e-01  5.41182995e-01  1.28661826e-01  6.87373355e-02
 -6.04625046e-02 -3.84094536e-01 -4.51930650e-02  3.58624719e-02
 -7.96833932e-02  1.34028122e-02  6.67359084e-02 -1.52730972e-01
 -3.66421714e-02  2.31674850e-01 -2.65912175e-01 -1.65272415e-01
 -3.09560508e-01  1.90663576e-01 -3.40794355e-01 -8.92035812e-02
 -2.70886570e-01  3.77047151e-01  8.27373937e-03 -5.22586927e-02
 -8.02338794e-02 -2.45694980e-01 -9.67167988e-02 -3.06042284e-01
 -1.36215106e-01 -3.02512616e-01  1.79163381e-01  3.55493098e-01
  1.66956171e-01  2.57232726e-01  3.67518142e-02 -1.62685767e-01
 -7.15340748e-02  2.32724007e-02 -3.15607071e-01  3.62511843e-01
 -1.28399372e-01 -1.16658069e-01  2.55632661e-02  4.16563451e-01
 -1.06007300e-01 -9.67498273e-02 -2.56628692e-01 -7.35318512e-02
  8.13416392e-02 -1.49431944e-01 -3.42344999e-01 -1.67562693e-01
  2.67136306e-01  2.56376177e-01 -1.38793111e-01  2.66503364e-01
 -2.82470226e-01  1.15762427e-01  3.84358883e-01 -2.13095605e-01
 -1.46493465e-01  5.95480762e-02  5.15084490e-02 -2.56992191e-01
 -1.43212393e-01 -1.73089147e-01 -1.07673980e-01  1.14873108e-02]"
tf.GradientTape.gradient raise error with tf.nn.relu6 stat:contribution welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**: 1.10.0-rc1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
tf.GradientTape.gradient raise error with tf.nn.relu6

### Source code / logs

```python
import tensorflow as tf

tf.enable_eager_execution()
w = tf.contrib.eager.Variable([[1.0]])
with tf.GradientTape() as tape:
    loss = tf.nn.relu6(w * w)
grad = tape.gradient(loss, w)
```

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 858, in gradient
    output_gradients=output_gradients)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py"", line 63, in imperative_grad
    tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 116, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py"", line 387, in _Relu6Grad
    return gen_nn_ops.relu6_grad(grad, op.outputs[0])
TypeError: 'NoneType' object is not subscriptable
```
",True,"[-0.4557613  -0.47933882 -0.3724491  -0.19248372  0.15345079  0.02319925
 -0.03653209 -0.0692012  -0.26106566  0.03457422  0.03512198 -0.07079393
 -0.00881781  0.00823807 -0.3289829   0.56336874  0.01791336 -0.15063053
 -0.1248562  -0.1607473  -0.10031031 -0.28821766 -0.14606476  0.34928676
  0.13135406  0.07820989 -0.16556165 -0.04125574  0.11193042 -0.0460742
  0.27290264  0.41663367 -0.08104746  0.05077747 -0.20219699  0.21356253
 -0.16606188 -0.21642135 -0.23901525 -0.001182   -0.00461398 -0.03333703
 -0.06532082  0.2314412   0.08411577  0.0907554   0.12697709  0.18210655
 -0.34001732 -0.05220416  0.15358427 -0.10359433 -0.41960064 -0.32690358
 -0.1774807   0.12133249  0.20019941  0.01957621  0.35269248  0.29753107
  0.06397684 -0.0391094  -0.03893295  0.18158995  0.14635688  0.14114133
  0.00580558 -0.32163328  0.45924556 -0.03695524  0.12385072 -0.1781693
 -0.09348938 -0.10877253  0.18876582  0.13980272  0.02213402  0.12377441
  0.33429095 -0.16857103  0.42392775 -0.11330946  0.18108827 -0.10153839
  0.16321169 -0.11216816  0.39653826  0.06684084  0.24889225 -0.16838905
  0.47589493  0.04888821 -0.04442293  0.05513746  0.29183426  0.28150016
 -0.40877217 -0.13581617 -0.00204922  0.04437124  0.05961516 -0.22221924
 -0.12203016  0.2941385  -0.19491145 -0.03522271  0.04856458  0.00141498
  0.14348689  0.23637885  0.01501178  0.07963404  0.13044097 -0.00968774
  0.05256029 -0.08735428  0.06735124  0.01621895 -0.12685399  0.41919023
  0.01571876 -0.30163336  0.25020802 -0.07928157  0.4298728   0.11110326
 -0.2398851   0.1794506   0.08581051  0.04059285  0.24661748  0.19050696
 -0.14592847  0.01565332 -0.256484   -0.02207803 -0.24199122  0.12248326
 -0.26272562 -0.05021477 -0.10982323  0.47462583  0.03225841 -0.1382449
  0.02291704 -0.05921248 -0.12473233  0.20621078 -0.14768928 -0.16832083
 -0.25999653 -0.00659115 -0.18790491  0.20509036  0.16653085  0.4631275
  0.30834222 -0.03055382 -0.07378329 -0.59526694  0.02539222 -0.04065433
 -0.0323277   0.00983471  0.18878677  0.11905941 -0.04717041 -0.06028748
  0.13564306  0.255997   -0.05302513  0.01365941 -0.07198656  0.3055389
  0.23997453 -0.27293015  0.12744112 -0.40957832 -0.20179681  0.15571679
  0.12944846 -0.38327962  0.11285314 -0.06787945 -0.0966047   0.06955667
  0.27845445  0.28307664 -0.47629833 -0.09088373 -0.46592146 -0.12683873
  0.0258058   0.09675585 -0.15445948 -0.08010472  0.13464363 -0.10808188
  0.0690953   0.36279625 -0.10312574 -0.26492798 -0.15182796 -0.00457839
 -0.12304742 -0.03964636 -0.16578251 -0.25605318 -0.27133584  0.08431619
  0.19675016 -0.60115314  0.07097426 -0.01717161 -0.1774475  -0.17707269
 -0.00716179 -0.02272684 -0.22036752  0.2172006   0.29167908 -0.01919777
  0.08977509 -0.37315685 -0.531799    0.01135791 -0.23763552  0.17415531
 -0.00645311  0.31162778 -0.00669322  0.21168628 -0.02961105  0.0593948
  0.298926   -0.227796   -0.06252486 -0.37168807  0.10420114  0.04415731
 -0.6182015   0.00962471 -0.21160078 -0.04308781  0.09702863  0.10874923
 -0.35648322  0.37175888 -0.12791684  0.3988486  -0.2182125  -0.11975795
  0.37147748  0.31889367  0.04393592  0.00709575 -0.00998633  0.16741219
  0.40630013  0.10947946  0.1824153   0.29515684 -0.03534208  0.45429134
  0.4207201   0.36808994 -0.02263633  0.21098305 -0.05499891 -0.38916185
 -0.03857279 -0.11712176  0.44459543 -0.2670355   0.07878818  0.12566173
  0.4823038   0.16456695 -0.24861202  0.3038587   0.09058827  0.10044274
 -0.11323092  0.22644708 -0.1518177  -0.18355706 -0.12602538 -0.5801719
 -0.05281989 -0.16113606  0.116354    0.0458466   0.33007115  0.14966692
 -0.00934632  0.1293247   0.05909719  0.00890547  0.06893719 -0.03772042
  0.163611    0.2592012   0.32329547  0.07095781 -0.0014013  -0.02686357
  0.58321214 -0.02233203  0.42672345 -0.28154594  0.10875806 -0.00306461
  0.08117814  0.39840272 -0.24446002 -0.06291019 -0.3476678   0.33790857
  0.17919269  0.00929679  0.22806424 -0.25878462 -0.00505444 -0.02474049
 -0.01032656  0.04837224  0.02370547 -0.27322865 -0.00848125  0.09963587
  0.03133225  0.00064768 -0.18211311  0.13168404 -0.10475739 -0.06067076
 -0.45116943  0.44104898  0.0084451  -0.10986327 -0.2029805  -0.19646907
  0.12776776 -0.27303708 -0.11958957 -0.16944456  0.11540345  0.37379798
  0.09467164  0.3424894  -0.03975701 -0.09923893 -0.36763665 -0.16289935
 -0.32098955  0.12523402 -0.05781145 -0.17036317  0.10601254  0.37295672
 -0.18235818 -0.16813797 -0.16657247 -0.09822579  0.01453673 -0.36618885
 -0.2420337  -0.30367345 -0.02583365  0.23350817 -0.19775595  0.31128305
 -0.42092812  0.11787805  0.4388865  -0.4187338  -0.10621397  0.00978687
 -0.0268066  -0.27446923 -0.07630548 -0.23796219 -0.14525287 -0.10838844]"
Max pooling cause error on empty batch ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 3.10.0-693.2.2.el7.x86_64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8.0
- **Python version**: Python 2.7.14 :: Anaconda
- **Bazel version (if compiling from source)**: None
- **GCC/Compiler version (if compiling from source)**: None
- **CUDA/cuDNN version**: cuda==9.0, cudnn==7.0.4
- **GPU model and memory**: None
- **Exact command to reproduce**: See below

### Describe the problem
When batch_size is 0, max pooling operation seems to produce an unhandled cudaError_t status. It may cause subsequent operations fail with odd error message. That is extremely difficult to debug.

(This corner case bothers us, where we first extract some bounding boxes and then run traditional convolution operations on areas specified by them. The above error occurs in case that no bounding boxes are detected thus batch_size becomes 0. However, the python exception will be randomly thrown at following operation or following session run steps)

```python
import tensorflow as tf
import numpy as np

x = tf.placeholder(dtype=tf.float32, shape=[None, 4, 4, 1])
pool_op = tf.nn.pool(x, pooling_type=""MAX"", window_shape=[2, 2], strides=[1, 1], padding=""SAME"")

y = tf.placeholder(dtype=tf.float32, shape=[None])
other_op = tf.where(tf.equal(y, 1.0))

normal_data = np.zeros([1, 4, 4, 1], dtype=""float32"")
empty_data = np.zeros([0, 4, 4, 1], dtype=""float32"")

# cudaError is thread local, limit thread pool size to make it easy to reproduce
config = tf.ConfigProto()
config.inter_op_parallelism_threads = 1
with tf.Session(config=config) as sess:
    # run other_op success
    print sess.run(other_op, {y: [1.0, 2.0, 3.0, 4.0]})  # [[0]]

    # run pooling on datas success
    print sess.run(pool_op, {x: normal_data}).shape  # (1, 4, 4, 1)
    print sess.run(pool_op, {x: empty_data}).shape  # (0, 4, 4, 1)

    # run other_op now failed
    print sess.run(other_op, {y: [1.0, 2.0, 3.0, 4.0]})  # err
``` 

Above code report error:
tensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true / nonzero indices.  temp_storage_bytes: 1, status: invalid configuration argument

""invalid configuration argument"" seems to be message return by cudaGetError, which indicates a failed kernel launch due to zero or too large number of block threads.

### Source code / logs
![image](https://user-images.githubusercontent.com/7600935/43579974-6dd4967a-9686-11e8-9b22-8288159d155c.png)

",True,"[-0.01232371 -0.65335387 -0.39384788 -0.11695964  0.07670633 -0.36955166
  0.05708315  0.10677552 -0.37924826 -0.11187866 -0.09930772  0.05673726
 -0.25561666  0.06367331 -0.2858345   0.37452748  0.13161491 -0.38980776
 -0.08662669 -0.13152774 -0.15412065 -0.32357556 -0.246328    0.3531962
  0.34492493  0.38296753 -0.28846797 -0.15959764 -0.00898466 -0.0391289
  0.38025686  0.20867509  0.42036176 -0.15260828  0.04396217  0.3577398
 -0.21135038 -0.32914487 -0.2703757   0.04300212  0.11521735  0.16538939
  0.1443506   0.28700596  0.00622669  0.05558614 -0.2122669  -0.01049051
  0.06602711 -0.14331223 -0.03296496  0.1416836  -0.26529643 -0.28930193
 -0.29611903 -0.36759353  0.09268256 -0.39775175 -0.03502728  0.14440235
  0.01047727 -0.02189433  0.01501245  0.05898012  0.17330569 -0.1014235
  0.3376311  -0.18026504  0.440244    0.07239263  0.17263186  0.07749204
 -0.47387084  0.06109582  0.05699069  0.32238093  0.12048301  0.04724826
  0.18989733 -0.2761135  -0.04258162 -0.27045292  0.27216947 -0.35129714
 -0.09863566  0.08319895  0.33380857  0.43441683  0.19493267 -0.02816115
  0.51960206  0.34093583 -0.26845664  0.22148333  0.41720706  0.16397823
  0.10475834  0.21876448 -0.19879459 -0.17304006  0.14254814 -0.01165886
  0.08661242  0.02827809  0.07848033 -0.05010677  0.16111435  0.02445976
  0.18145698 -0.1030934   0.17529991 -0.07055978  0.37256247  0.01258144
  0.07662962  0.19386631 -0.24814758  0.2214605  -0.16548893  0.8041748
 -0.00476883  0.02882761  0.2466917   0.05958818  0.27420425  0.3219386
  0.06381373 -0.08044411  0.04276674 -0.18523598  0.3108561  -0.07187475
 -0.19944927  0.24296746 -0.0259526   0.09144596 -0.22836845  0.01585284
 -0.28978023 -0.08176359  0.04704696  0.17830776 -0.0936681  -0.39853883
  0.03313895  0.19285086 -0.2928654   0.1418595  -0.1166439  -0.1471927
 -0.4132173   0.14700145 -0.09195173  0.12032664  0.0846012   0.27193612
  0.3325205  -0.00398733 -0.03577768 -0.7264163  -0.31465745  0.2160682
 -0.19785938 -0.12225698  0.24256773  0.13018776 -0.4370104  -0.2995658
 -0.14228314  0.41263926 -0.09408988 -0.00200465 -0.1315543   0.1326251
  0.0553415  -0.11065623  0.2237858  -0.57173985 -0.16743158  0.11569531
  0.10892586  0.12476161  0.06585912  0.15284367 -0.1875456   0.06682827
  0.1627792   0.3108083  -0.08104408 -0.01888857 -0.31973997 -0.05748428
  0.41657132 -0.13142467 -0.08184283  0.01747471  0.2970512   0.01288911
 -0.04232369  0.15223137 -0.19859694 -0.2843796   0.10940251  0.08692385
  0.36616445 -0.18596712 -0.44672439 -0.12935993 -0.27921456  0.17988546
  0.05688385 -0.24793626 -0.14176798  0.19706625 -0.30646932 -0.02589322
  0.1984675  -0.17688712 -0.31627673  0.030635    0.31674936 -0.24281323
  0.09974354 -0.3863688  -0.07694969 -0.28215018 -0.2695256   0.03637482
  0.09609643  0.29045004  0.07665044  0.24445024 -0.00154701 -0.16881883
  0.16143581  0.08579201  0.11313532 -0.09955728 -0.2177739   0.29431233
 -0.45248032 -0.22879949  0.29014778  0.11008716  0.03487965  0.20411956
 -0.1318315   0.04182127 -0.17269967  0.22109227 -0.3327219   0.0017054
  0.2510533   0.15964855  0.3553182   0.03187356 -0.08895699  0.32609975
  0.12582462 -0.15558702  0.3263294   0.29144695 -0.10720763  0.65111446
  0.04169343  0.1488975  -0.3427068   0.27486375  0.23040375 -0.1340861
  0.01046338 -0.2448248   0.14012186 -0.2627625  -0.29085636 -0.15062723
  0.50347567  0.19164333 -0.04580116 -0.07039696  0.20745252  0.21923456
 -0.0394541   0.00839445  0.01756247 -0.04934226  0.11114147 -0.57034034
 -0.16771719  0.00826865 -0.01336436  0.17989856  0.15627934  0.02897676
 -0.06857051  0.27693915 -0.00907617  0.2794341   0.09349637  0.34757122
 -0.08518504 -0.03168817  0.1315229   0.04471906 -0.3031635   0.14493853
  0.26564837  0.27980155  0.3972644  -0.21921408  0.29469693 -0.24673001
 -0.22576912  0.13697226 -0.1342256  -0.10000491 -0.06528442  0.67758733
  0.2602944  -0.09170736  0.0193425  -0.20947875 -0.32845047  0.3028813
  0.07794444 -0.09716521 -0.27147436 -0.2734572  -0.23525977  0.12644303
  0.2378608  -0.25802463 -0.33720803 -0.13575637 -0.23685738 -0.26976144
 -0.32221502  0.4174125  -0.06839724 -0.28311944  0.19507843 -0.01733157
  0.278389   -0.43875664 -0.04696547 -0.25514853  0.5795747   0.2624465
 -0.07265255 -0.21352854 -0.10355458  0.05934805 -0.17622691 -0.17300886
 -0.01857799  0.41681254 -0.27015018 -0.06669457  0.06477948  0.22966419
 -0.19861248 -0.19490321 -0.22228576 -0.17867321  0.11322643 -0.2721256
 -0.34867245 -0.04522774  0.3320688   0.1188292  -0.01490878  0.31426606
 -0.12889327  0.5624613   0.5879941  -0.24020697 -0.49846703  0.07223521
  0.07364057 -0.27495235  0.03798798 -0.05917804 -0.08566657 -0.08508711]"
Default floating point values in single_image_random_dot_stereograms_ops.cc cause syntax error stat:contribution welcome,"Have I written custom code: yes
OS Platform and Distribution: Ubuntu 18.04
TensorFlow installed from: origin master
TensorFlow version: 1.9
Bazel version 0.15.2
CUDA/cuDNN version: 9.1/7.1
GPU model and memory:  NVIDIA-SMI 390.48
Exact command to reproduce: use code below in python3 script 
Mobile device: N/A
     
     import tensorflow as tf
      vol = tf.contrib.util.make_tensor_proto([256, 256])

Import of /localhome/local/projects/lme_custom_ops/tensorflow/contrib/image/__init__.py causes syntax error on my system (see issue https://github.com/tensorflow/serving/issues/421).

    Traceback (most recent call last):
    File ""main.py"", line 6, in <module>
        import geometry as geometry
    File ""/localhome/local/projects/deep-iterative-reco/geometry.py"", line 20, in <module>
        volume_origin = tf.contrib.util.make_tensor_proto([-((_volume_xy-1)/2 * _volume_spacing),-((_volume_xy-1)/2 * _volume_spacing)], tf.float32 )
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py"", line 53, in __getattr__
        module = self._load()
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/lazy_loader.py"", line 42, in _load
        module = importlib.import_module(self.__name__)
    File ""/usr/lib/python3.6/importlib/__init__.py"", line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/__init__.py"", line 48, in <module>
        from tensorflow.contrib import image
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/image/__init__.py"", line 70, in <module>
        from tensorflow.contrib.image.python.ops.single_image_random_dot_stereograms import single_image_random_dot_stereograms
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/image/python/ops/single_image_random_dot_stereograms.py"", line 27, in <module>
        ""_single_image_random_dot_stereograms.so""))
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
        ret = load_library.load_op_library(path)
    File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/load_library.py"", line 73, in load_op_library
        exec(wrappers, module.__dict__)
    File ""<string>"", line 28
        def single_image_random_dot_stereograms(depth_values, hidden_surface_removal=True, convergence_dots_size=8, dots_per_inch=72, eye_separation=3, mu=0,333299994, normalize=True, normalize_max=-100, normalize_min=100, border_level=0, number_colors=256, output_image_shape=[1024, 768, 1], output_data_window=[1022, 757], name=None):
                                                                                                                                                                    ^
    SyntaxError: invalid syntax

Works again if I change the default kwargs float arguments to integer values. Reason maybe localization option (I am from Germany which uses , to indicate decimal point. E.g. 1.2 == 1,2 in Germany). Though I think I am using the US standard.

Using this code in /tensorflow/contrib/image/ops/single_image_random_dot_stereograms_ops.cc solves the problem (not using floating values).

    REGISTER_OP( ""SingleImageRandomDotStereograms"" )
        .Attr( ""T: {double,float,int64,int32}"" )
        .Input( ""depth_values: T"" )
        .Output( ""image: uint8"" )
        .Attr( ""hidden_surface_removal: bool = true"" )
        .Attr( ""convergence_dots_size: int = 8"" )
        .Attr( ""dots_per_inch: int = 72"" )
        .Attr( ""eye_separation: float = 3"" )
        .Attr( ""mu: float = 3333"" )
        .Attr( ""normalize: bool = true"" )
        .Attr( ""normalize_max: float = -100.0"" )
        .Attr( ""normalize_min: float = 100.0"" )
        .Attr( ""border_level: float = 0.0"" )
        .Attr( ""number_colors: int = 256"" )


On python2 executing the following...  

      Python 2.7.15rc1 (default, Apr 15 2018, 21:51:34) 
      [GCC 7.3.0] on linux2
      Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
      >>> import locale
      >>> locale.setlocale(locale.LC_ALL, '')
            'LC_CTYPE=en_US.UTF-8;LC_NUMERIC=de_DE.UTF-8;LC_TIME=de_DE.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=de_DE.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=de_DE.UTF-8;LC_NAME=de_DE.UTF-8;LC_ADDRESS=de_DE.UTF-8;LC_TELEPHONE=de_DE.UTF-8;LC_MEASUREMENT=de_DE.UTF-8;LC_IDENTIFICATION=de_DE.UTF-8'

Though I compiled for python3:

    Python 3.6.5 (default, Apr  1 2018, 05:46:30) 
    Type ""copyright"", ""credits"" or ""license"" for more information.

    IPython 5.5.0 -- An enhanced Interactive Python.
    ?         -> Introduction and overview of IPython's features.
    %quickref -> Quick reference.
    help      -> Python's own help system.
    object?   -> Details about 'object', use 'object??' for extra details.

    In [1]: 
    ...: 
    ...: import locale

    In [2]: locale.localeconv()
    Out[2]: 
    {'currency_symbol': '',
    'decimal_point': '.',
    'frac_digits': 127,
    'grouping': [],
    'int_curr_symbol': '',
    'int_frac_digits': 127,
    'mon_decimal_point': '',
    'mon_grouping': [],
    'mon_thousands_sep': '',
    'n_cs_precedes': 127,
    'n_sep_by_space': 127,
    'n_sign_posn': 127,
    'negative_sign': '',
    'p_cs_precedes': 127,
    'p_sep_by_space': 127,
    'p_sign_posn': 127,
    'positive_sign': '',
    'thousands_sep': ''}

I am using tensorflow revision 6d71d3fc659b317a38586f71ae94410ad3261f55 on Ubuntu 18.08. cuDNN 7.1, CUDA 9.1

This seems to be related: https://github.com/tensorflow/tensorflow/issues/2974",True,"[-0.4099164  -0.53635216 -0.23576623 -0.11695345  0.24180655 -0.21369754
 -0.00519355  0.04894428 -0.14703122 -0.19533834  0.05777185 -0.11173782
  0.11684982 -0.18960574 -0.19903943  0.14105177 -0.11011091 -0.07072482
  0.31418246 -0.10379227 -0.05521052 -0.1065257  -0.15055813  0.02476995
  0.22074689  0.12353138 -0.14891948  0.10068136  0.1277582  -0.04586729
  0.36909533  0.24320424  0.23580077  0.10117325 -0.16121602  0.12030251
 -0.2889927  -0.15701224 -0.24839182 -0.03351899  0.06531489  0.12106531
  0.32123512 -0.07432103  0.16467106  0.17412065 -0.02221544  0.04970107
  0.02799228  0.08477639 -0.161542    0.07801656 -0.33688104 -0.25141376
  0.01208385 -0.00389701  0.1582096  -0.02399997  0.00751148  0.05700011
  0.09380911 -0.12260722  0.05651339  0.04956043  0.05282925  0.041369
  0.11225015 -0.19934466  0.35335234 -0.10174564 -0.047164   -0.00439254
 -0.41042858 -0.08105323 -0.00695487  0.23454277 -0.16126712  0.19448864
  0.17185628 -0.31789032  0.04124944 -0.10156728  0.1684632  -0.1517517
  0.16518289 -0.0115778   0.4465275   0.19106047  0.05668198 -0.06345591
  0.39944226  0.34242654 -0.21352997 -0.06392579  0.3799668   0.17017595
  0.13649374  0.02666983  0.07073718  0.05190708 -0.12442216 -0.29851556
 -0.24250738  0.31796235 -0.06059509 -0.03502315  0.15796797  0.03457753
  0.05517283  0.0747001   0.12077926 -0.07042345 -0.07298653 -0.01212803
  0.04275393 -0.07046346 -0.08869183  0.0239864  -0.07869541  0.40090993
 -0.05543239  0.00406066 -0.0122588   0.17121279  0.272164   -0.02566118
  0.04303999 -0.10001849 -0.02656843 -0.14666598  0.21748108 -0.16121623
 -0.15218788  0.0716427   0.01513176  0.01212398 -0.0802426   0.14722711
 -0.23849301 -0.17145795 -0.0614269   0.07532138 -0.19085963 -0.27795756
  0.21353602  0.16497564 -0.19350201  0.11988451 -0.01660428  0.03468936
 -0.05310569 -0.1647806  -0.21358532  0.41544747  0.11256626  0.26862377
  0.22425985  0.07031779  0.03363673 -0.40379402 -0.01789332  0.19419327
 -0.1798307  -0.10897914  0.00607168  0.03851547 -0.30962226 -0.04850551
 -0.16408268  0.28643698 -0.3422608  -0.08701255 -0.05895507  0.14362921
  0.18882282 -0.05986878  0.12947749 -0.20662065 -0.22025624  0.21083374
 -0.027133    0.00755156 -0.0125755  -0.02406813 -0.12308254  0.01322359
  0.15416598  0.02251129 -0.24499762 -0.03524394 -0.4717575   0.06361544
  0.21491748 -0.13978408 -0.03113374 -0.08879843  0.42947245  0.08959854
  0.17998374  0.18770629 -0.29680985 -0.13460897  0.07005233  0.12553127
  0.09771172 -0.17320924 -0.20761308  0.09400311 -0.4329675   0.4245519
 -0.0540912  -0.3767962  -0.01979708  0.15352644 -0.12511143  0.10394816
 -0.2227477  -0.10799758 -0.14431463  0.19789521  0.20609255 -0.1212576
  0.05152094 -0.19539155 -0.21893005  0.07937144 -0.3275228   0.05352442
 -0.13422462  0.2364847   0.09286139  0.1660696   0.38804507  0.14478788
 -0.02474923 -0.00678077 -0.18645218 -0.15096599 -0.15619381 -0.03459681
 -0.27707195 -0.09704296  0.02125334  0.10145797  0.11629216  0.08195918
 -0.13795996  0.16248612 -0.14163002  0.26110107 -0.11645398 -0.1811481
  0.21038699  0.1914325   0.15492928  0.1899839  -0.12811887  0.11003969
  0.15583915 -0.10895972  0.21504653 -0.05666955 -0.13847196  0.5432569
  0.14239003  0.23874757 -0.24905376  0.26453424  0.03612171 -0.24852951
  0.10449843  0.07301997  0.42213762 -0.2697959  -0.03866134 -0.17980298
  0.31780306 -0.11552601 -0.17668399  0.04926868  0.04455449  0.29194582
 -0.19563231  0.03552298 -0.02180898 -0.31197727 -0.11772846 -0.34120244
 -0.26317     0.24280496 -0.07733467  0.15862349  0.0771995   0.05328409
 -0.2617821   0.08216287  0.10574514 -0.17994483 -0.00609904  0.09696818
  0.02499674  0.1967226   0.29354924 -0.08248506 -0.10286818  0.03232678
  0.30304086  0.19136785  0.46503574 -0.25666732  0.21585548 -0.07442921
  0.05215342  0.28517    -0.20500839  0.17880136 -0.23687224  0.38987923
  0.15672997  0.05682242 -0.1522483  -0.30308062 -0.2067171   0.26156068
  0.11564904 -0.01622118 -0.18995257 -0.39334747 -0.16816644  0.12362888
  0.00242898 -0.18906188 -0.19035119 -0.04246651  0.07276484 -0.00563724
 -0.18361743  0.21632686 -0.04565357 -0.21997258 -0.03725462 -0.12551548
  0.01717527 -0.387482   -0.41514355 -0.19527766  0.20336667  0.37351054
  0.07730025 -0.10026822  0.25787255 -0.05976564 -0.11825966 -0.07460885
 -0.0200454   0.05077846 -0.04168624 -0.06946573  0.2128843   0.20830277
 -0.3671435   0.05748831 -0.05836738 -0.27309275  0.08493412 -0.01732701
 -0.13045128 -0.1323402   0.11555637  0.18450622 -0.04080729  0.20074126
 -0.14958179  0.33284605  0.4289146  -0.20115347  0.03689512  0.09661709
  0.075679   -0.05323251 -0.19935335  0.17461129 -0.0278218  -0.16174713]"
[Bug] tf.py_func : Bad tensor -> np.array conversion in py_func (return [] with dtype=int64 is returned as dtype=float64)  stat:contribution welcome type:bug,"### Issue template:

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (minimal example attached inline)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OSX 10.12.6
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Probably
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.5.0-0-g37aa430d84', '1.5.0')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `python py_func_failure.py`

### Describe the problem

This bug occurs when a `tf.py_func` returns an empty python list `[]` which is intended to be a tensor of type `tf.int64` (as defined in the `Tout=` of the `py_func`). 

In the `ops.script_ops.py` the `_convert` static method is unable to tell the `numpy dtype` of the incoming tensor when it is an empty list.  This causes the `_convert` method to execute a `np.asarray([], dtype=None, order=""C"")` which gives us a `array([], dtype=float64)` instead of a `array([], dtype=int64)` which then does not agree with the output tensor description in the py_func. 

### Source code / logs

See gist here for the example that reproduces this issue: https://gist.githubusercontent.com/sabhiram/3f5eaf7e566ef9aefb3ae6e5b8d2edb0/raw/182ab6bc72b5b7b13fd55964c72030cbc53f7cb3/py_func_failure.py

Inlined here:
```
import tensorflow as tf

def test_func(x):
    """""" Builds a list of ints with length `x`.
    """"""
    return [i for i in range(x)],


def main():
    t0 = tf.constant(0, dtype=tf.int64)
    t0 = tf.py_func(test_func, [t0], tf.int64)

    t1 = tf.constant(1, dtype=tf.int64)
    t1 = tf.py_func(test_func, [t1], tf.int64)

    with tf.Session() as sess:
        sess.run(t1)            # OK
        sess.run(t0)            # 0-th value is double, expects int64

if __name__ == ""__main__"":
    print(tf.GIT_VERSION, tf.VERSION)
    main()
```",True,"[-0.29019397 -0.39700508 -0.22456075 -0.14225937 -0.02199089 -0.26304406
  0.04872771  0.13007876 -0.45068455  0.01430833  0.09306903 -0.11881351
 -0.1255523   0.0827771   0.0481649   0.37176043  0.01899631 -0.12361708
  0.2983169   0.08380766  0.05226476  0.10508586 -0.22566962  0.24316077
 -0.16152905 -0.02874338 -0.21048933  0.12759979 -0.00713091  0.04908561
  0.17458364  0.25910538 -0.15595448 -0.04009927 -0.12318143  0.47815195
 -0.24496102 -0.1484049  -0.0753648   0.16673248 -0.00983916 -0.10169437
  0.15360194  0.02897409  0.07300887 -0.09107599  0.03902711  0.07521591
  0.00293724 -0.03126559  0.05019456  0.07973774 -0.39626884 -0.21530478
 -0.05483323  0.02609579 -0.04450747 -0.06113143  0.12024053 -0.11432827
  0.18823443  0.09490135  0.11100882  0.1640808   0.1456893   0.13805048
  0.4235143  -0.09310995  0.37364012 -0.20769332  0.15366274 -0.00250435
 -0.25170517 -0.05900528  0.0329809   0.14696093 -0.12554766  0.1390453
  0.13162167 -0.20241934  0.04953792 -0.17047179  0.03205273 -0.35242486
  0.16138153 -0.11760674  0.30852902 -0.0215253   0.29748303 -0.0673508
  0.5746753   0.18715063 -0.26442856  0.10104728  0.25633645 -0.024028
  0.11280322 -0.05751542 -0.16210377 -0.00521836 -0.03393301 -0.0637347
  0.09157158 -0.13853484  0.02005774  0.14978212  0.1628009  -0.03121718
  0.03189407  0.08014417 -0.14952913  0.00440021  0.139466   -0.06995821
  0.04645829  0.14117657 -0.12474458 -0.05999915 -0.09316716  0.3649306
  0.09840232 -0.08109446  0.06353844  0.5855843   0.1247597   0.19487184
 -0.26222318 -0.05243926  0.08011408  0.02494996  0.3403834  -0.06238241
 -0.14257205  0.25263733 -0.03161282 -0.09510292 -0.40375775 -0.04443976
 -0.2608093  -0.08802114  0.02130826  0.00630833 -0.2644021  -0.3513605
 -0.00858127  0.14419565 -0.14625657  0.3049407  -0.13159615  0.2808901
  0.05827662  0.07324742 -0.3976674   0.5323012  -0.10479082  0.29372114
  0.38345703 -0.1318206  -0.00768384 -0.52401584  0.23391187  0.14323798
  0.15503111 -0.15281045  0.01964743  0.05836347 -0.20189437 -0.01579605
 -0.18195994  0.30266827 -0.1346646   0.31306833 -0.17363252  0.38903874
  0.36047956 -0.20010738  0.19803259 -0.6621537  -0.24018858  0.21251854
  0.00728409  0.08548499 -0.07586145  0.14109889 -0.06128878 -0.09622948
 -0.08000242 -0.17598203 -0.0314011   0.02402713 -0.28548867 -0.36715376
  0.1003089  -0.01764411 -0.0590052  -0.03777739  0.30122888 -0.12723458
  0.2628572   0.24416761 -0.22197738 -0.17346096  0.06548245  0.03590344
  0.17086422 -0.24785168 -0.11925261 -0.3920542  -0.29111204  0.23117599
 -0.24962394 -0.23172744  0.09766012 -0.00260558 -0.32295942  0.03841035
  0.24178185 -0.17621885 -0.07877387  0.0181917  -0.01973359 -0.12473762
  0.22687754 -0.2549069  -0.06883906  0.01600566 -0.2893089   0.06530119
 -0.05030863  0.10203418  0.23419939 -0.06787939  0.2596383   0.00558741
  0.17529687 -0.00633097 -0.31185436 -0.1540367  -0.08052859  0.04485129
 -0.35414815  0.03241862 -0.08717954  0.00099583  0.21332353 -0.03884208
 -0.17921558  0.12801647 -0.30927166  0.31083196 -0.40096545  0.03694103
  0.29886088  0.03792196  0.11481869  0.25745842 -0.0070279  -0.04056792
  0.23094675  0.08616713  0.01257572  0.2788952   0.02890986  0.3458832
  0.13146159  0.14523657 -0.19609058  0.09129387  0.24297628 -0.11052111
  0.04960421 -0.07947969  0.53069454 -0.24098615  0.08644457 -0.3497234
  0.2748887  -0.1503557  -0.06608808  0.12402399 -0.02958427  0.230598
 -0.07356907  0.1736623  -0.14736564 -0.02861694  0.00132653 -0.38931257
 -0.30924946 -0.1041839  -0.0257608  -0.04229927  0.21396208 -0.05930627
  0.00569535  0.22960088 -0.06648378  0.02563216 -0.12188386 -0.05198594
  0.00908049  0.00157359  0.34270555  0.05666795  0.00533444  0.16068666
  0.29850435  0.10232141  0.14330642 -0.22236396  0.30491066 -0.0198948
 -0.37706617  0.21859775 -0.07937546  0.18186477 -0.23416564  0.7092936
  0.19225469  0.06269647 -0.08387452 -0.05399911 -0.20082593  0.15011278
  0.06441572 -0.05442683 -0.10073468 -0.31880203  0.06479956 -0.09246086
 -0.06719653 -0.09987538 -0.20695727 -0.14095366 -0.15494564 -0.19155313
 -0.4320562   0.23203914 -0.04123319 -0.30512974 -0.03738481 -0.09013133
  0.00352086 -0.513013   -0.1417416  -0.31274417  0.33558723  0.5371606
 -0.17645028  0.07811444  0.21269983  0.08172465 -0.30556023 -0.03053666
  0.12519154  0.21718824  0.05136511 -0.01278472  0.04439095  0.51485443
 -0.33466756  0.03839422 -0.23785755 -0.17183547  0.10714084 -0.3053438
 -0.09488567 -0.11233543  0.40213078  0.05877593 -0.17062971  0.14832972
 -0.41506445  0.34371054  0.40291154 -0.2604187  -0.23299897  0.43736467
  0.2114927  -0.28064582  0.01882476 -0.25453424  0.07730296 -0.0338103 ]"
bug about boosted_trees stat:awaiting response,"In tensorflow 1.9 , win10, python36,  I run the example ""tensorflow-master\tensorflow\contrib\boosted_trees\examples\boston.py"", report :

  File ""<ipython-input-1-0c2875deaba9>"", line 6, in <module>
    from tensorflow.contrib.boosted_trees.estimator_batch import custom_export_strategy

  File ""D:\ProgramFiles\Anaconda3\lib\site-packages\tensorflow\contrib\boosted_trees\estimator_batch\custom_export_strategy.py"", line 25, in <module>
    from tensorflow.contrib.boosted_trees.python.training.functions import gbdt_batch

  File ""D:\ProgramFiles\Anaconda3\lib\site-packages\tensorflow\contrib\boosted_trees\python\training\functions\gbdt_batch.py"", line 26, in <module>
    from tensorflow.contrib.boosted_trees.lib.learner.batch import categorical_split_handler

ModuleNotFoundError: No module named 'tensorflow.contrib.boosted_trees.lib'
++++++++++++++++++++++++++++++++++++++++++++++
And when use tensorflow 1.8 ,python 36 ,centos5 or win10, run the same example ""tensorflow-master\tensorflow\contrib\boosted_trees\examples\boston.py"", report :
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 225, in run
    return _execute_schedule(experiment, schedule)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 52, in _execute_schedule
    return task()
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 666, in train_and_evaluate
    self.train(delay_secs=0)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 389, in train
    saving_listeners=self._saving_listeners)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 879, in _call_train
    input_fn=input_fn, steps=steps, max_steps=max_steps, monitors=hooks)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 524, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1080, in _train_model
    scaffold=scaffold)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 421, in __init__
    self._save_path = os.path.join(checkpoint_dir, checkpoint_basename)
  File ""/usr/local/python36/lib/python3.6/posixpath.py"", line 92, in join
    genericpath._check_arg_types('join', a, *p)
  File ""/usr/local/python36/lib/python3.6/genericpath.py"", line 151, in _check_arg_types
    raise TypeError(""Can't mix strings and bytes in path components"") from None
TypeError: Can't mix strings and bytes in path components
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++
and  I run other example again ,""tensorflow-master\tensorflow\contrib\boosted_trees\examples\mnist.py"" ,report:

  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 225, in run
    return _execute_schedule(experiment, schedule)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py"", line 52, in _execute_schedule
    return task()
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 666, in train_and_evaluate
    self.train(delay_secs=0)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 389, in train
    saving_listeners=self._saving_listeners)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py"", line 879, in _call_train
    input_fn=input_fn, steps=steps, max_steps=max_steps, monitors=hooks)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 524, in fit
    loss = self._train_model(input_fn=input_fn, hooks=hooks)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1041, in _train_model
    model_fn_ops = self._get_train_ops(features, labels)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1264, in _get_train_ops
    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1227, in _call_model_fn
    model_fn_results = self._model_fn(features, labels, **kwargs)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/estimator_batch/model.py"", line 116, in model_builder
    logits=logits)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 1085, in create_model_fn_ops
    enable_centered_bias=self._enable_centered_bias)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 669, in _create_model_fn_ops
    batch_size, loss_fn, weight_tensor)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py"", line 1944, in _train_op
    train_op = train_op_fn(loss)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/estimator_batch/model.py"", line 105, in _train_op_fn
    update_op = gbdt_model.train(loss, predictions_dict, labels)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/training/functions/gbdt_batch.py"", line 696, in train
    control_flow_ops.no_op))
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2063, in cond
    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1913, in BuildCondBranch
    original_result = fn()
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/training/functions/gbdt_batch.py"", line 932, in _update_bias_stats
    ensemble_stamp, partition_ids, feature_ids, grads_sum, hess_sum)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/ops/stats_accumulator_ops.py"", line 117, in add
    partition_ids, feature_ids, gradients, hessians))
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/ops/stats_accumulator_ops.py"", line 154, in _make_summary
    partition_ids, feature_ids, gradients, hessians)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/contrib/boosted_trees/python/ops/gen_stats_accumulator_ops.py"", line 1232, in stats_accumulator_tensor_make_summary
    name=name)
  File ""/usr/local/python36/bin/tensor/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 533, in _apply_op_helper
    (prefix, dtypes.as_dtype(input_arg.type).name))
TypeError: Input 'partition_ids' of 'StatsAccumulatorTensorMakeSummary' Op has type int64 that does not match expected type of int32.
",True,"[-7.57049695e-02 -6.06811106e-01 -1.58995256e-01  1.68132633e-01
  4.49105293e-01 -1.57570809e-01 -7.39903469e-03 -6.82147816e-02
 -5.88914871e-01  2.99013197e-01 -6.46687225e-02 -1.37619376e-01
  4.88227233e-02  1.08577222e-01 -1.72897771e-01  5.83608508e-01
 -7.51742125e-02  2.99090415e-01 -1.40220419e-01  8.49246606e-02
  2.51071192e-02 -2.78960526e-01  1.63795725e-02 -1.46974027e-02
  3.19643021e-01  6.24374337e-02 -1.80854663e-01 -2.23534986e-01
 -1.01453535e-01 -7.82392770e-02  6.88762069e-02  1.06484592e-02
 -5.84923476e-03 -6.34327680e-02 -1.13933295e-01  4.44628417e-01
 -3.19598734e-01 -1.07900381e-01 -1.11690700e-01  1.46056339e-01
  6.38134331e-02 -3.23178098e-02  1.14074666e-02  2.31233478e-01
 -2.17820525e-01 -9.68589336e-02 -8.27593450e-03  1.15558848e-01
 -1.28099650e-01 -1.14736602e-01 -4.58898209e-02 -3.22306678e-02
 -4.38786805e-01 -6.08889580e-01 -4.46061492e-02  1.77797884e-01
  3.98045443e-02 -5.93321994e-02  2.46836185e-01  2.40860865e-01
 -1.03973955e-01  1.01874061e-02  1.88777104e-01 -7.48075619e-02
 -6.02794252e-03  7.95048028e-02 -9.64486785e-03  1.59144163e-01
  3.35628301e-01 -1.90614924e-01  1.69922024e-01 -8.90457928e-02
 -3.33686888e-01 -2.76172400e-01  1.65956706e-01  1.26065925e-01
 -3.44071299e-01  3.61474454e-01  2.02211902e-01 -2.53726751e-01
 -1.05368197e-01 -2.43660018e-01 -1.05476350e-01 -2.11229280e-01
  2.53944933e-01 -4.74713147e-02  2.18174621e-01 -4.67560031e-02
  1.16109625e-01  1.82108194e-01  7.36862183e-01 -2.06088319e-01
 -1.12033680e-01  1.14967667e-01  1.87632293e-01  1.26555979e-01
  3.31838839e-02 -5.07060438e-03 -1.00820258e-01 -2.57218361e-01
 -6.79326057e-03 -1.05561323e-01  1.51231200e-01  3.30305636e-01
  6.15719818e-02 -2.80575693e-01 -1.16834886e-01  8.10627490e-02
  1.01487465e-01  2.01816618e-01  4.01580334e-01 -2.03784868e-01
  1.20896220e-01  1.53534114e-01  4.95919585e-02  1.55801415e-01
 -1.27030626e-01 -6.84800893e-02 -1.57566786e-01  5.36189497e-01
 -8.93294662e-02 -1.32319048e-01  4.47954774e-01  1.66567966e-01
 -7.65527710e-02  3.14421020e-02 -3.49140167e-01 -2.94614881e-02
  7.47648478e-02 -7.15330839e-02  2.94178724e-01  2.78279841e-01
 -1.38360023e-01 -1.17971480e-01 -2.25779891e-01 -1.11216143e-01
 -2.73588389e-01 -2.86312491e-01 -5.71012497e-01  2.28426680e-02
 -1.96147069e-01  4.10426050e-01 -1.90883234e-01 -3.24876815e-01
 -5.18993251e-02  1.56518444e-01 -1.60560936e-01  3.05703342e-01
 -6.25690073e-02 -8.28146860e-02 -3.03234041e-01 -1.15147205e-02
 -1.84934765e-01  1.42574385e-01  3.80885184e-01  3.50494206e-01
  4.71091151e-01  6.85519353e-03  8.93102735e-02 -6.13468528e-01
  1.20386094e-01  2.29473740e-01 -6.31904043e-03 -1.62546560e-01
  5.65072000e-02  7.67067075e-02 -3.09476793e-01 -4.36477214e-02
 -1.98517054e-01  1.30032480e-01  1.97144449e-02 -7.51908310e-03
 -1.77906632e-01  3.94636840e-01  4.01528299e-01 -1.42389566e-01
  1.75839573e-01 -7.86197782e-01 -3.04129664e-02  1.70653641e-01
  3.86110574e-01  1.76234990e-01 -1.12042300e-01  1.31282583e-01
 -1.38149470e-01  1.13377281e-01  2.34913692e-01  5.62136807e-03
 -7.69762471e-02  1.21373624e-01 -1.72123998e-01 -1.64263189e-01
  3.24098289e-01  1.84688270e-01 -2.52738651e-02  1.31928504e-01
  2.15390697e-01  1.12064846e-01 -4.91777584e-02  2.92949170e-01
 -1.10332340e-01 -3.29962969e-02  1.43288016e-01  4.33216169e-02
 -1.10199433e-02 -1.80885985e-01 -5.55971026e-01 -3.55280340e-01
 -5.59256971e-01  7.59318396e-02 -1.10504702e-01 -4.46289301e-01
  5.09365574e-02  1.49828747e-01 -4.23055589e-01  2.12029874e-01
  1.24616928e-01 -1.42899647e-01 -2.67673470e-02 -8.22989866e-02
  5.97438291e-02 -4.74095345e-02  3.14886644e-02 -2.65371859e-01
 -6.40146434e-04 -2.01638579e-01  1.13510415e-01  1.06306002e-01
  1.82514220e-01  4.01044488e-01 -7.51187801e-02 -5.53589761e-02
  2.30181277e-01 -4.09437045e-02  1.17010780e-01  7.35532492e-02
  3.44039053e-02 -5.81878424e-02 -1.28878325e-01  4.90939543e-02
 -1.92306042e-01  2.06572294e-01 -8.00015703e-02  1.35497898e-01
 -8.36574882e-02  3.99509929e-02 -3.91360037e-02 -2.08886027e-01
 -4.73113894e-01  3.51712704e-02 -1.31899297e-01 -1.24670066e-01
  3.56480896e-01  9.42435209e-03  2.78190583e-01  2.51598567e-01
 -3.08173299e-01 -5.14929369e-02  1.70430034e-01 -8.75193924e-02
  3.40791583e-01  1.31532058e-01 -9.46142524e-02  7.51159191e-01
  8.51217434e-02  1.19893752e-01 -2.44741216e-01  4.60180759e-01
  6.86832145e-02 -1.39794320e-01 -1.20605603e-02 -3.19365799e-01
  1.41114205e-01 -2.89614677e-01  1.08360164e-01  1.22159064e-01
  2.23848522e-01  1.58872172e-01 -1.83230519e-01 -9.58014280e-02
  2.65639842e-01  4.57310587e-01  1.38108552e-01  3.87786210e-01
 -3.21566582e-01 -8.62787217e-02 -2.18763873e-01 -2.39937305e-01
 -4.18559760e-01 -7.54108727e-02 -1.92005113e-02 -2.80674934e-01
  5.05460501e-01 -1.52638406e-01 -1.22838870e-01 -4.88416068e-02
  1.33293360e-01  3.03965569e-01  2.98548117e-02  1.13774240e-01
  4.87795509e-02  2.10390344e-01  4.86709952e-01 -3.06296349e-01
 -1.29597157e-01 -6.89419582e-02  3.50207448e-01 -9.31174904e-02
  2.55143613e-01 -4.69045371e-01  2.29693145e-01  1.00959547e-01
 -9.28793922e-02  2.16883384e-02 -9.50551033e-02 -6.12479672e-02
 -6.03144057e-02  7.71843076e-01  1.14804924e-01 -6.08124137e-02
 -6.21349104e-02  6.54162019e-02 -3.41050863e-01  2.77860820e-01
  1.06155545e-01  2.40889311e-01 -1.92202657e-01  9.26267952e-02
 -3.09486866e-01 -2.40236342e-01  1.52956858e-01 -6.79338276e-02
 -3.06041956e-01  2.56016731e-01  6.25636280e-02 -2.70643592e-01
 -3.12568069e-01  5.99589527e-01  2.67110705e-01 -2.63924241e-01
  9.09505188e-02 -1.29620776e-01  3.73344690e-01 -2.92066753e-01
  2.38759723e-02 -3.03655267e-01  5.57677224e-02  3.46027255e-01
  2.06023920e-02  5.84098436e-02  4.17731479e-02  1.95925325e-01
 -3.38347673e-01 -1.57640390e-02 -1.14293005e-02  6.01859093e-01
  1.82196349e-02  4.64654118e-02 -2.41690457e-01  5.96699297e-01
 -5.44599444e-03 -1.22313626e-01 -1.76127672e-01 -4.49094415e-01
  1.87731385e-01 -1.08604543e-01 -2.47255117e-01 -3.15785885e-01
  1.96328729e-01  2.23936975e-01 -3.57292965e-02  2.30140358e-01
 -1.71467766e-01  7.35574216e-02  1.13429323e-01 -2.04552203e-01
 -2.53504574e-01  3.81433219e-02  5.31520993e-02 -1.13933489e-01
  8.84132460e-02 -9.74758193e-02 -2.03808248e-01 -2.45552629e-01]"
bug in tf.Print summarized formatting ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.8
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

If you print a tensor of shape [n, 4] with tf.Print, by default (summarize=3 is the default value), you get:

[[9 21 55]...]

which wrongly looks like your tensor is of shape [n, 3].

The correct output should be:

[[9 21 55...]...]

Here is what you get with tf.Print(summarize=10):

[[9 21 55 30][190 -42 236 4][89 -5]...]

Now the vectors of size 4 are visible although the last one is still wrong (looks like a vector of size 2
)",True,"[-3.04267764e-01 -6.83593452e-01 -3.70011568e-01 -1.68973312e-01
  2.17972577e-01 -3.13460737e-01 -5.68967760e-02 -1.17443755e-01
 -3.31654906e-01 -1.29093304e-01 -7.15170056e-03 -9.14670676e-02
 -2.94833422e-01  2.86543071e-01 -1.68699130e-01  2.54702628e-01
 -7.83876777e-02 -3.02397758e-01  1.27463341e-01 -1.61749333e-01
 -3.59369889e-02 -1.23542741e-01 -2.08493039e-01  3.91125023e-01
  1.80894881e-01  2.67186761e-01 -2.10444123e-01  1.34732975e-02
 -4.28151600e-02  9.74982083e-02  1.86888427e-01  2.14980528e-01
  3.07687819e-02  4.33613583e-02  9.40487981e-02  3.67292225e-01
 -1.11709394e-01 -5.39457314e-02 -2.42084026e-01 -1.68581322e-01
  2.67919488e-02 -1.14052594e-01  4.95996624e-02 -7.15896767e-03
  1.42431095e-01 -3.51491235e-02  1.75906997e-02 -1.17154531e-01
 -1.93742841e-01 -5.12985885e-03 -4.71364260e-02  6.36825711e-02
 -4.95385557e-01 -8.35615322e-02 -1.90896504e-02  3.82062271e-02
  9.43871215e-02 -1.31719053e-01  4.84254360e-02  1.71483546e-01
  3.79004627e-02 -1.24415290e-02 -1.71663806e-01  2.29071647e-01
  1.76004529e-01  2.46041626e-01  3.23243678e-01 -2.39602476e-01
  2.84782410e-01 -2.31021523e-01  1.10573769e-01 -9.10517201e-02
 -1.17671795e-01  4.16607559e-02 -7.67154694e-02  1.60286829e-01
  6.06850460e-02  2.15354979e-01  2.45702624e-01 -2.30769515e-02
  6.88699633e-02 -1.77784115e-01  2.41913587e-01 -1.54511437e-01
 -1.23070985e-01 -5.43605760e-02  4.48149323e-01  6.82727154e-03
  2.73835361e-01 -2.13112786e-01  6.09775186e-01  1.06564179e-01
  9.22781974e-02  2.22068250e-01  4.58697379e-01  1.65797621e-01
  4.74972501e-02  4.50699449e-01  6.17638193e-02 -1.03125677e-01
 -1.12393141e-01 -2.38126487e-01  1.75841361e-01 -8.26555789e-02
 -9.02236253e-02 -1.06704801e-01  9.24067870e-02  5.01925945e-02
  1.28459454e-01 -1.59003407e-01  4.94912863e-02  5.92466444e-02
  1.60859615e-01 -1.10207006e-01  1.64768845e-01  1.22087393e-02
  4.60774377e-02  1.20509140e-01  2.86622234e-02  5.74325562e-01
 -8.62135924e-03 -1.36553630e-01  1.39959082e-01  1.55412346e-01
  3.21423233e-01  1.70449659e-01 -1.47156090e-01  8.93986076e-02
  1.54973328e-01 -2.06769779e-02  1.79601878e-01  9.67887118e-02
 -2.26563603e-01  2.13509694e-01 -1.28411859e-01 -1.13062479e-01
 -3.87471437e-01 -8.27957690e-03 -1.08638316e-01 -2.31907144e-01
 -1.41107261e-01  4.84554321e-02 -6.19414523e-02 -3.50867152e-01
  1.27122849e-01  1.25035807e-01 -1.95327714e-01  3.64967734e-01
 -1.93059489e-01  3.54816169e-01 -1.28714085e-01 -1.01186857e-01
  7.85229653e-02  3.81741643e-01 -4.80170511e-02  5.98773127e-04
  2.98285186e-01 -7.53464475e-02 -9.03926119e-02 -5.99300563e-01
 -2.29171053e-01  2.22062141e-01 -6.66940510e-02 -1.94489241e-01
 -7.98973022e-04  7.12199062e-02 -4.51148450e-01 -2.25847289e-01
  1.60112262e-01  3.37485850e-01  6.90700412e-02  6.02851324e-02
 -1.75639801e-02  4.25611377e-01  6.50310069e-02 -1.59018740e-01
  4.67629880e-01 -6.49013519e-01 -7.51463845e-02  2.67546147e-01
  2.00992495e-01 -2.53189597e-02  1.60103470e-01  2.11233553e-03
  9.99124795e-02 -2.59104848e-01  1.63865939e-01  1.64651871e-01
 -1.19692281e-01 -6.57927990e-02 -2.38358974e-01 -1.37935221e-01
  2.97658980e-01 -1.59872752e-02 -3.06173742e-01  2.00468227e-02
  3.04182231e-01 -9.67930853e-02 -3.83055583e-02  1.60543546e-01
 -1.08070523e-01 -9.09796655e-02  6.99850172e-02 -6.06823191e-02
  8.15804750e-02 -2.55213320e-01 -3.11515182e-01 -2.92055339e-01
 -3.45117211e-01  1.42066866e-01  2.27116615e-01 -3.95856708e-01
  1.17706973e-03  8.24715719e-02 -3.26899767e-01  1.09094139e-02
  1.41537577e-01 -1.39385626e-01 -2.45831430e-01  3.16277087e-01
  2.27677464e-01 -1.50805831e-01  7.87639245e-02 -5.31722784e-01
 -1.79079950e-01 -9.62583721e-02 -3.37920249e-01  1.39760464e-01
  2.93864589e-03  2.24748179e-01 -1.32882576e-02  1.51528105e-01
  8.90317634e-02 -2.13210136e-02  2.98415899e-01 -1.26144662e-01
 -9.53895822e-02 -4.16785955e-01 -3.43039453e-01  2.29986250e-01
 -4.20231223e-01 -2.39144832e-01  2.91468129e-02 -5.68221658e-02
  2.43677035e-01 -6.70949928e-03 -1.28340155e-01  1.49350584e-01
 -1.90368563e-01  4.71927881e-01 -2.22603917e-01  5.86922280e-02
  2.99961209e-01  2.20062867e-01  2.57418007e-01  1.05735667e-01
  1.03783786e-01  1.61613107e-01  2.58939564e-01 -4.27042432e-02
  3.41533333e-01  2.44117290e-01 -4.52151224e-02  5.70296168e-01
  2.78021574e-01  3.78745496e-01 -2.07426548e-01  2.38086462e-01
 -6.74601048e-02 -4.57618907e-02  3.32601517e-02 -1.94752842e-01
  3.77746105e-01 -2.77138174e-01 -1.55390799e-03 -2.90589720e-01
  4.58348393e-01 -1.45323098e-01 -1.09558947e-01  2.26914883e-01
  1.49880320e-01  1.46077752e-01 -1.76820576e-01  4.93730754e-02
 -5.06295189e-02 -4.39664088e-02  1.13401160e-01 -7.43643165e-01
 -1.74354076e-01 -4.47626077e-02 -4.11423743e-02 -2.67514158e-02
 -7.60101341e-03 -1.80653691e-01 -5.86547144e-02  4.83657084e-02
 -4.46113944e-02  1.95062503e-01  1.83812976e-01  2.85951078e-01
 -4.35758047e-02 -2.19732791e-01  1.74611747e-01  1.24102518e-01
  4.99653667e-02  5.47191277e-02  3.76390994e-01  1.81728810e-01
  4.72880930e-01 -2.41760939e-01  3.07889462e-01 -7.29030818e-02
 -2.06095219e-01  2.44239241e-01  8.19714963e-02  2.62228131e-01
 -2.36167878e-01  5.58150589e-01  1.90771073e-01 -1.92887753e-01
  7.81067647e-04 -3.42924833e-01 -4.66898859e-01  8.37689787e-02
  9.04096663e-02 -4.87244129e-02  9.77076292e-02 -3.63551587e-01
  1.98009182e-02  5.54609448e-02 -2.50713944e-01 -2.47225180e-01
 -4.91480023e-01 -1.10656284e-02 -2.41116270e-01 -1.29694909e-01
 -3.40563655e-01  2.49873802e-01 -5.36350496e-02 -2.43940905e-01
  9.34566259e-02 -1.13687798e-01 -1.61506101e-01 -3.49290580e-01
 -3.56436111e-02 -2.14378774e-01  3.02386343e-01  5.16590595e-01
 -7.76220858e-02  2.18176261e-01  7.70104527e-02  1.43016711e-01
 -2.15786621e-01 -6.83557689e-02 -2.18041644e-01  1.60136223e-01
  6.98491931e-04  1.45246182e-02  2.24335104e-01  4.74726677e-01
 -1.19091451e-01 -1.75013691e-01 -2.42485732e-01 -1.00497842e-01
  1.75569862e-01 -3.15820217e-01 -3.57770056e-01 -1.11910135e-01
  2.97463804e-01  3.81482482e-01  5.33361658e-02  4.26200509e-01
 -2.65173316e-01  2.85495043e-01  6.34481490e-01 -3.09987187e-01
 -1.45119622e-01  1.89472437e-01  2.24484801e-01 -1.94793224e-01
 -6.14972226e-03 -3.30056787e-01  1.84045047e-01 -1.42894778e-03]"
Character access is not supported in tf.contrib.autograph stat:contribution welcome,"For example this is not working and it returns an error
```
@autograph.convert()
def is_hashtag(x):
  if x[0] == ""#"":
    return True
  return False

with tf.Graph().as_default():  
  with tf.Session() as sess:
    print(sess.run(is_hashtag(tf.constant(""#asdf""))))
```",True,"[-2.66486943e-01 -3.32423359e-01 -2.57347614e-01 -4.35889125e-01
  1.19981356e-01  1.70069821e-02  3.77454132e-01  1.04814775e-01
 -2.58231908e-01  4.04580459e-02 -1.15694620e-01 -2.34312564e-01
  6.78304017e-01  1.09181084e-01  5.36034023e-03  2.37970769e-01
  9.63828415e-02 -7.18439147e-02  1.71678960e-01 -2.08381787e-02
 -1.26278356e-01  2.85308152e-01  8.26509893e-02  1.30021900e-01
 -2.11553887e-01 -3.52765411e-01 -2.51752675e-01 -1.78381745e-02
 -1.04224443e-01  8.69652256e-02 -4.52692509e-02 -1.53087348e-01
 -4.42750335e-01 -2.14932989e-02  1.04909703e-01  2.34429896e-01
 -2.40216196e-01  1.40045241e-01  3.11274659e-02 -8.58152062e-02
  1.01046704e-01 -2.06425861e-01  6.81532174e-02  1.33893088e-01
 -1.74832642e-01  1.12018669e-02  2.88315732e-02  1.63009390e-01
 -2.49653026e-01  3.03152472e-01 -2.31923550e-01  3.23367178e-01
 -4.79837269e-01 -4.33718748e-02 -3.34229058e-04  3.62889141e-01
 -3.06464523e-01  1.14379963e-02  2.41144747e-01 -1.29770905e-01
 -3.72796394e-02  3.24603140e-01  4.99177687e-02  1.15145341e-01
  1.66910440e-01 -1.49757817e-01 -1.84116021e-01  3.37982804e-01
  3.37847382e-01 -2.84406006e-01  1.28584445e-01  1.15133807e-01
 -2.45380163e-01  5.49262539e-02  2.70334810e-01  3.80424671e-02
 -5.43993771e-01  7.37385601e-02 -5.01104770e-03 -1.57957420e-01
 -1.97395235e-01 -5.13732359e-02  1.31553203e-01  8.02294016e-02
  2.49230608e-01 -6.71669990e-02  2.22090855e-01 -1.95631638e-01
  3.75202119e-01  3.46513897e-01  3.06044996e-01 -4.80031744e-02
  1.52119026e-01  2.28004426e-01  1.61964834e-01  9.16320831e-02
 -1.48168340e-01  2.28038922e-01  6.33055195e-02 -1.21909184e-02
 -2.06941605e-01 -1.08524539e-01 -5.99605637e-03  6.21119961e-02
 -1.27425594e-02 -1.93062052e-01 -2.16921240e-01  2.82376617e-01
 -2.34719943e-02  1.02202035e-02 -1.29376665e-01  4.96953502e-02
  2.17739977e-02  3.52463648e-02 -2.01643601e-01  2.25854442e-01
  3.00453812e-01 -1.66291624e-01  1.90428097e-03 -6.30724207e-02
  1.50059044e-01  2.71132886e-01  8.35129991e-03  2.37440526e-01
  2.70230882e-02  2.22957224e-01 -1.65087640e-01 -6.69077411e-02
  7.09073097e-02  9.72826630e-02  3.33820611e-01  2.47486997e-02
 -1.64809927e-01  1.48800313e-01 -7.18889236e-02 -3.66065592e-01
 -4.31934774e-01 -1.64945036e-01 -8.13447163e-02  1.04787715e-01
  7.83189684e-02 -1.83932520e-02 -1.13075636e-01  2.21431926e-01
 -1.13736339e-01 -6.29567131e-02 -1.81332678e-01  3.07340175e-01
 -5.37874848e-02 -1.21831179e-01  2.47574419e-01  2.27395594e-01
 -2.05902919e-01  1.80957735e-01 -1.70601934e-01  1.24263808e-01
  2.47366011e-01 -1.57069668e-01  1.87969550e-01 -3.69820505e-01
  2.25822866e-01  1.33736238e-01  1.39025375e-01 -2.50612050e-01
  3.18801880e-01 -4.62544598e-02 -3.57143320e-02  1.24831915e-01
 -2.00986564e-01 -2.99334805e-02  5.00253355e-03  6.41547665e-02
 -2.49901161e-01 -1.68004602e-01  4.09105748e-01  1.17693000e-01
  2.30104893e-01 -1.57961115e-01 -1.40224934e-01  1.40516415e-01
  1.64263114e-01 -1.79957181e-01 -2.77166545e-01  3.06975245e-01
 -1.46814054e-02  8.84019658e-02  2.91131530e-02 -2.23562136e-01
 -1.91711679e-01 -6.78080544e-02  7.12245256e-02  6.68047518e-02
  7.26012886e-02 -7.37170130e-02  1.49476929e-02 -4.45211202e-01
  2.97777623e-01 -4.68961835e-01  4.28373963e-01  4.32732515e-02
  4.96442523e-03 -5.97010367e-03  1.77571312e-01 -7.65447393e-02
  1.78465024e-01 -1.04184382e-01 -1.50403678e-01 -9.43747014e-02
 -2.28289410e-01 -4.06786837e-02 -3.72036666e-01  1.97149590e-01
  9.30240229e-02 -2.41967887e-01 -7.73651749e-02 -1.35898784e-01
 -2.46768296e-01  4.75160144e-02  2.73268014e-01 -5.17236143e-02
 -3.17967474e-01 -2.02628270e-01  2.68445481e-02 -1.85832694e-01
  1.76756352e-01  2.09596425e-01  8.99945647e-02  2.79821783e-01
  3.87555927e-01 -2.56812751e-01  2.55366415e-01 -6.10702522e-02
  1.81713581e-01  5.44031151e-03  1.16345011e-01  8.50382540e-03
 -1.98261991e-01 -2.14495435e-01 -1.70498088e-01 -1.20981097e-01
  1.10178120e-01  2.55853146e-01 -3.24586809e-01 -2.00424880e-01
  1.12402245e-01 -7.53316842e-03  6.43853471e-02  1.42869607e-01
 -1.98179513e-01 -1.48609892e-01 -4.14488524e-01  1.06266983e-01
 -1.02341183e-01 -1.55613452e-01  1.79107323e-01  3.40640187e-01
  4.71087359e-02  4.64941189e-02  1.11121498e-01  5.94249591e-02
  1.12514526e-01  2.17769653e-01  1.76666394e-01  2.26961076e-01
  1.41815975e-01  6.77844808e-02 -8.21067244e-02 -8.95727351e-02
 -1.92335188e-01 -5.84140904e-02  9.02012810e-02 -9.28242132e-02
  4.12560582e-01  1.82496384e-01  2.23485991e-01  2.30359994e-02
  2.67048061e-01 -3.07974853e-02  5.63289188e-02  2.81502157e-01
  6.82057440e-02  2.01003104e-01 -2.72136867e-01  1.97627708e-01
  1.09986201e-01  9.62619018e-03  4.86733997e-03 -3.49078804e-01
 -2.29480237e-01 -2.29211658e-01 -8.36726502e-02  1.21218108e-01
 -1.22251794e-01 -1.55536696e-01  4.86640725e-03 -1.30842790e-01
  3.95316295e-02 -8.19750279e-02  7.24630728e-02 -4.86425422e-02
  6.20100880e-03 -3.00914254e-02  4.49816644e-01  4.53555658e-02
 -9.33524072e-02  1.29619002e-01  1.23475529e-01 -4.16918918e-02
  4.02909786e-01 -1.79676190e-01  1.36243373e-01  1.31649122e-01
 -2.04766721e-01  2.36797377e-01 -2.17702568e-01  2.39125669e-01
 -5.67306280e-02  2.10174546e-01  2.35175230e-02  2.49820679e-01
 -1.21833712e-01 -2.01818123e-01 -4.00733471e-01 -1.10395022e-01
 -1.09579317e-01  4.36364442e-01 -3.93295996e-02 -3.12864751e-01
  1.26090229e-01  1.65503263e-01  8.39826763e-02 -1.49391174e-01
 -1.53939873e-01 -1.12650871e-01 -3.19105923e-01 -4.54285681e-01
 -5.63604906e-02  2.44926170e-01 -4.05664854e-02 -3.43258381e-01
 -2.07355514e-01 -1.60355359e-01 -1.95641294e-01 -1.90940171e-01
 -2.27129236e-01 -8.82664174e-02 -2.99885571e-01 -9.64236632e-02
  7.96335924e-04  6.15260676e-02  7.09669665e-02  1.05146736e-01
 -9.25986841e-03 -1.81483939e-01 -3.97693306e-01  2.22451612e-01
  1.06643066e-01 -1.84503213e-01 -1.54646397e-01  3.02795917e-01
 -3.17739807e-02 -4.42813970e-02 -1.90810665e-01 -1.20533399e-01
 -2.98727661e-01  5.60370348e-02 -1.43806905e-01  1.24052167e-01
  2.40491197e-01 -1.95581228e-01 -1.68857440e-01  9.61026773e-02
 -2.60647267e-01 -6.09094538e-02  1.42851621e-01  7.01072514e-02
  7.03127235e-02  2.39672631e-01  7.10877597e-01 -2.42425483e-02
 -1.11209430e-01 -7.49108102e-03  6.20693341e-02 -1.82289388e-02]"
please change allocation.cc line 102 copied_buffer_ = std::move(buffer); stat:contribution welcome comp:lite,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
CentOS 6.9 
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.9.0-rc2
- **Python version**: 
3.6.2
- **Bazel version (if compiling from source)**:
0.14.1
- **GCC/Compiler version (if compiling from source)**:
gcc 6.2.0
- **CUDA/cuDNN version**:
cuda 9.0 and cudnn 7.1.1
- **GPU model and memory**:
NVIDIA K80 (12GB/GPU) and P100(16GB/GPU)
- **Exact command to reproduce**:
>   exec env - \
>     CUDA_TOOLKIT_PATH=/apps/cuda/9.0 \
>     CUDNN_INSTALL_PATH=/apps/cudnn/7.1.1-cuda9.0 \
>     GCC_HOST_COMPILER_PATH=/apps/gcc/6.2.0/wrapper/gcc \
>     LD_LIBRARY_PATH=/apps/gcc/6.2.0/lib/gcc/x86_64-pc-linux-gnu/6.2.0:/apps/gcc/6.2.0/lib64:/apps/nccl/2.2.13-cuda9.0/lib:/apps/openmpi/3.1.0/lib:/apps/openmpi/3.1.0/lib/profilers:/apps/intel-ct/17.0.1.132/mkl/lib/intel64:/apps/python3/3.6.2/lib:/apps/binutils/2.25/lib:/apps/java/jdk1.8.0_60/lib:/apps/cudnn/7.1.1-cuda9.0/lib64:/apps/cuda/9.0/extras/CUPTI/lib64:/apps/cuda/9.0/lib64 \
>     NCCL_INSTALL_PATH=/apps/nccl/2.2.13-cuda9.0 \
>     PATH=/apps/gcc/6.2.0/wrapper:/apps/gcc/6.2.0/bin:/apps/openmpi/wrapper/fortran:/apps/openmpi/3.1.0/bin:/apps/python3/3.6.2/bin:/apps/binutils/2.25/bin:/apps/bazel/0.14.1/bin:/apps/java/jdk1.8.0_60/bin:/apps/cuda/9.0/bin:/home/900/yxs900/.local/bin:/opt/bin:/bin:/usr/bin \
>     PWD=/proc/self/cwd \
>     PYTHON_BIN_PATH=/apps/python3/3.6.2/bin/python3 \
>     PYTHON_LIB_PATH=/apps/python3/3.6.2/lib/python3.6/site-packages \
>     TF_CUDA_CLANG=0 \
>     TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0 \
>     TF_CUDA_VERSION=9.0 \
>     TF_CUDNN_VERSION=7 \
>     TF_NCCL_VERSION=2 \
>     TF_NEED_CUDA=1 \
>     TF_NEED_OPENCL_SYCL=0 \
>   external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/contrib/lite/_objs/framework/tensorflow/contrib/lite/allocation.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/contrib/lite/_objs/framework/tensorflow/contrib/lite/allocation.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/k8-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote external/gemmlowp -iquote bazel-out/k8-opt/genfiles/external/gemmlowp -iquote external/flatbuffers -iquote bazel-out/k8-opt/genfiles/external/flatbuffers -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/genfiles/third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem tensorflow/contrib/lite/schema -isystem bazel-out/k8-opt/genfiles/tensorflow/contrib/lite/schema -isystem bazel-out/k8-opt/bin/tensorflow/contrib/lite/schema -isystem external/flatbuffers/include -isystem bazel-out/k8-opt/genfiles/external/flatbuffers/include -isystem bazel-out/k8-opt/bin/external/flatbuffers/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '-D_GLIBCXX_USE_CXX11_ABI=0' '-march=native' -DFARMHASH_NO_CXX_STRING -c tensorflow/contrib/lite/allocation.cc -o bazel-out/k8-opt/bin/tensorflow/contrib/lite/_objs/framework/tensorflow/contrib/lite/allocation.pic.o


### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The build stops because the variables buffer and copied_buffer have different types,  char*& and const char*&, respectively, and std::swap expects identical types of their inputs and code found no candidate for std::swap, see details in the error message below. Simply change line 102 in tensorflow/contrib/lite/allocation.cc {{ copied_buffer_ = std::move(buffer); }} to {{copied_buffer_.reset(const_cast<char const *>(buffer.release()));}} fix the bug.

### Source code / logs
tensorflow/contrib/lite/allocation.cc:102:36:   required from here
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: error: no matching function for call to 'swap(const char*&, char*&)'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:59:0,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/utility:70,
                 from tensorflow/contrib/lite/allocation.cc:27:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:179:5: note: candidate: template<class _Tp> typename std::enable_if<std::__and_<std::is_move_constructible<_Tp>, std::is_move_assignable<_Tp> >::value>::type std::swap(_Tp&, _Tp&)
     swap(_Tp& __a, _Tp& __b)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:179:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   deduced conflicting types for parameter '_Tp' ('const char*' and 'char*')
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:59:0,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/utility:70,
                 from tensorflow/contrib/lite/allocation.cc:27:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:202:5: note: candidate: template<class _Tp, long unsigned int _Nm> typename std::enable_if<std::__is_swappable<_Tp>::value>::type std::swap(_Tp (&)[_Nm], _Tp (&)[_Nm])
     swap(_Tp (&__a)[_Nm], _Tp (&__b)[_Nm])
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/move.h:202:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types '_Tp [_Nm]' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/utility:70:0,
                 from tensorflow/contrib/lite/allocation.cc:27:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:403:5: note: candidate: template<class _T1, class _T2> void std::swap(std::pair<_T1, _T2>&, std::pair<_T1, _T2>&)
     swap(pair<_T1, _T2>& __x, pair<_T1, _T2>& __y)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_pair.h:403:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::pair<_T1, _T2>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/vector:64:0,
                 from ./tensorflow/contrib/lite/allocation.h:22,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_vector.h:1557:5: note: candidate: template<class _Tp, class _Alloc> void std::swap(std::vector<_Tp, _Alloc>&, std::vector<_Tp, _Alloc>&)
     swap(vector<_Tp, _Alloc>& __x, vector<_Tp, _Alloc>& __y)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_vector.h:1557:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::vector<_Tp, _Alloc>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/vector:65:0,
                 from ./tensorflow/contrib/lite/allocation.h:22,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:112:3: note: candidate: void std::swap(std::_Bit_reference, std::_Bit_reference)
   swap(_Bit_reference __x, _Bit_reference __y) noexcept
   ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:112:3: note:   no known conversion for argument 1 from 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}' to 'std::_Bit_reference'
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:120:3: note: candidate: void std::swap(std::_Bit_reference, bool&)
   swap(_Bit_reference __x, bool& __y) noexcept
   ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:120:3: note:   no known conversion for argument 1 from 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}' to 'std::_Bit_reference'
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:128:3: note: candidate: void std::swap(bool&, std::_Bit_reference)
   swap(bool& __x, _Bit_reference __y) noexcept
   ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_bvector.h:128:3: note:   no known conversion for argument 2 from 'char*' to 'std::_Bit_reference'
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/list:63:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:18,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_list.h:1918:5: note: candidate: template<class _Tp, class _Alloc> void std::swap(std::list<_Tp, _Alloc>&, std::list<_Tp, _Alloc>&)
     swap(list<_Tp, _Alloc>& __x, list<_Tp, _Alloc>& __y)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/stl_list.h:1918:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::list<_Tp, _Alloc>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/string:52:0,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/stdexcept:39,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/array:39,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/tuple:39,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/functional:55,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/basic_string.h:5287:5: note: candidate: template<class _CharT, class _Traits, class _Alloc> void std::swap(std::basic_string<_CharT, _Traits, _Alloc>&, std::basic_string<_CharT, _Traits, _Alloc>&)
     swap(basic_string<_CharT, _Traits, _Alloc>& __lhs,
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/bits/basic_string.h:5287:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::basic_string<_CharT, _Traits, _Alloc>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/tuple:39:0,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/functional:55,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/array:275:5: note: candidate: template<class _Tp, long unsigned int _Nm> void std::swap(std::array<_Tp, _Nm>&, std::array<_Tp, _Nm>&)
     swap(array<_Tp, _Nm>& __one, array<_Tp, _Nm>& __two)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/array:275:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::array<_Tp, _Nm>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/functional:55:0,
                 from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/tuple:1546:5: note: candidate: template<class ... _Elements> void std::swap(std::tuple<_Elements ...>&, std::tuple<_Elements ...>&)
     swap(tuple<_Elements...>& __x, tuple<_Elements...>& __y)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/tuple:1546:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::tuple<_Elements ...>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:79:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/functional:2238:5: note: candidate: template<class _Res, class ... _Args> void std::swap(std::function<_Res(_ArgTypes ...)>&, std::function<_Res(_ArgTypes ...)>&)
     swap(function<_Res(_Args...)>& __x, function<_Res(_Args...)>& __y)
     ^~~~
/apps/gcc/6.2.0/include/c++/6.2.0/functional:2238:5: note:   template argument deduction/substitution failed:
In file included from /apps/gcc/6.2.0/include/c++/6.2.0/memory:81:0,
                 from ./tensorflow/contrib/lite/simple_memory_arena.h:19,
                 from ./tensorflow/contrib/lite/allocation.h:25,
                 from tensorflow/contrib/lite/allocation.cc:29:
/apps/gcc/6.2.0/include/c++/6.2.0/bits/unique_ptr.h:614:6: note:   mismatched types 'std::function<_Res(_ArgTypes ...)>' and 'std::__tuple_element_t<0ul, std::tuple<const char*, std::default_delete<const char []> > > {aka const char*}'
  swap(std::get<0>(_M_t), __p);
  ~~~~^~~~~~~~~~~~~~~~~~~~~~~~


",True,"[-4.71637607e-01 -5.39771199e-01 -3.54074925e-01 -3.68740112e-02
  2.14035548e-02 -2.48328596e-01  4.55916524e-02 -1.59431294e-01
 -2.22708687e-01 -6.42848462e-02  1.56646788e-01 -1.66209772e-01
 -2.22071826e-01 -4.65450883e-02 -2.24054337e-01  1.43220112e-01
  1.64450705e-02 -5.09821884e-02  2.83167303e-01  1.43729761e-01
 -1.47622705e-01 -1.57097816e-01 -2.98862129e-01  2.72523314e-01
  4.29936871e-02  3.27959776e-01 -1.44365624e-01 -1.13207117e-01
  9.08384845e-02 -1.87315360e-01  4.73996818e-01 -1.33487254e-01
 -1.03961274e-01  4.18675207e-02 -7.66490549e-02  4.70030546e-01
 -1.10221267e-01 -1.37688935e-01 -7.57583529e-02  1.29927397e-01
  2.94988275e-01  1.00459211e-01  1.59959972e-01 -1.11863956e-01
  2.34709829e-01 -4.25511040e-04 -5.21731526e-02  3.82335521e-02
 -8.64645466e-02 -3.01677108e-01  3.99930850e-02  1.93556175e-01
 -3.76780570e-01 -1.27870321e-01 -4.29989457e-01 -2.55302247e-02
  2.13035017e-01 -1.68232024e-01 -7.81901255e-02  2.00204819e-01
  1.87527090e-01  6.02506250e-02  3.60925309e-03  1.68181047e-01
  4.93666008e-02  6.20755777e-02  3.59339327e-01 -1.63561217e-02
  5.70460200e-01 -2.60322869e-01 -7.46724457e-02  1.08232513e-01
 -3.44479501e-01 -3.79088730e-01  4.43328768e-02  2.16247872e-01
  7.78361224e-03  3.10376823e-01  2.78967202e-01 -3.40072870e-01
  9.28726867e-02 -1.27685636e-01  4.94542830e-02 -1.96872681e-01
 -5.90538979e-02  2.41629958e-01  4.56446320e-01  3.83804142e-02
  2.35452846e-01 -3.57932746e-01  5.98464191e-01  4.49265003e-01
  1.10277459e-02  1.34605020e-01  3.13537896e-01  1.57378748e-01
 -1.00145869e-01  2.10746408e-01  1.11915812e-01  6.93821609e-02
  9.30484608e-02 -1.41038537e-01  8.05146992e-02  6.84040785e-02
 -2.55555985e-03 -2.23408788e-01  9.65562910e-02  1.12791643e-01
  2.90708207e-02 -8.31377357e-02  8.19022208e-02  4.94097993e-02
  3.58402878e-02  6.69093728e-02  1.83237735e-02 -1.46677280e-02
 -1.17434487e-01  3.59269045e-02 -8.53535980e-02  6.25757515e-01
  6.37461431e-03 -2.16069490e-01  5.36622517e-02  3.84174287e-01
  3.82840604e-01  2.87753996e-03 -1.67082295e-01 -4.74516004e-02
 -4.88194488e-02  3.73450592e-02  2.92531431e-01  6.11109510e-02
 -2.86096454e-01  1.04205431e-02 -1.22899026e-01 -1.16158620e-01
 -3.72257739e-01 -8.25978369e-02 -1.11784495e-01 -1.62854940e-01
  3.90826017e-02  3.76717389e-01 -1.92040086e-01 -3.26176703e-01
  5.81572764e-02  2.60729492e-01  1.23951659e-01  2.12792195e-02
 -3.48697394e-01  4.07341480e-01 -1.13919815e-02 -2.78694630e-01
  1.35636568e-01  5.33161871e-02  1.04894936e-01  1.55774951e-01
  2.15889364e-01 -2.44572237e-02  1.30530864e-01 -5.70805073e-01
 -1.55165598e-01  3.80406886e-01  1.10536441e-02 -1.43490851e-01
 -3.44067812e-02  1.10535428e-01 -2.29538634e-01 -2.18244627e-01
  1.34547397e-01  3.09677392e-01 -2.29275525e-01 -5.00641316e-02
 -5.17680123e-02  8.55004489e-02  2.39998996e-01 -2.37728320e-02
  3.17776322e-01 -6.61905348e-01 -1.59807801e-01  1.34683684e-01
 -6.73428625e-02 -9.50600766e-03 -9.68511924e-02 -1.18171215e-01
  9.79470015e-02  3.85212637e-02  1.22110777e-01  1.65097266e-01
 -3.55914444e-01 -7.38848001e-02 -4.51076984e-01 -8.11259821e-02
  1.91678584e-01 -8.89219157e-03 -1.97539896e-01 -5.49216904e-02
  1.20011345e-01  1.17955923e-01 -1.46176845e-01  2.31796399e-01
 -1.63969874e-01 -1.65366352e-01 -1.68030262e-01 -1.79839022e-02
  3.63085344e-02 -1.90797925e-01 -3.69392514e-01 -1.32564053e-01
 -3.41628909e-01  2.02691585e-01 -8.11371058e-02 -4.29683149e-01
 -4.71737981e-02  9.45084728e-03 -2.20806688e-01 -7.13658184e-02
  1.06494218e-01 -6.57020807e-02 -1.22023083e-01  2.45991021e-01
  3.59859854e-01 -1.84127763e-01  3.13223004e-01 -2.67744094e-01
 -2.63410479e-01 -3.67269665e-02 -2.52075493e-01  2.20478162e-01
  2.87953913e-02  1.66025430e-01  1.34919360e-01  2.48459071e-01
  1.72762886e-01  2.44058818e-01 -5.60313091e-03 -1.74374238e-01
  1.14850260e-01 -1.61367074e-01 -2.32660607e-01  1.81507573e-01
 -4.60687011e-01 -3.83081198e-01 -1.32705778e-01 -6.62293956e-02
  5.96597791e-03  2.56024212e-01  1.20046534e-01  3.32436293e-01
 -2.07521766e-01  1.35546908e-01 -2.86559403e-01  3.73908281e-02
  5.47821879e-01  4.33786549e-02 -1.35637030e-01  3.51524115e-01
 -2.04035580e-01  1.18436199e-02  8.89620781e-02 -6.79313540e-02
  2.52130747e-01  4.78761584e-01  9.97448340e-02  5.46878874e-01
  3.41828763e-01  1.18312106e-01 -2.48264939e-01  9.95189473e-02
  1.30773783e-01 -9.17183012e-02  4.72426973e-02 -7.18738288e-02
  1.28319725e-01 -2.52039611e-01  9.74893123e-02 -1.06682509e-01
  3.39752942e-01  1.60198405e-01  9.66688097e-02  2.71718621e-01
  2.31608003e-01  3.57084095e-01 -2.10934401e-01  8.35603625e-02
  5.76233771e-03 -2.54193455e-01 -1.53048456e-01 -3.78920674e-01
 -9.23770070e-02 -1.62484292e-02 -5.45809306e-02 -1.90977037e-01
 -1.04547881e-01 -2.72243991e-02  1.88164879e-02  6.62396103e-02
 -5.01635969e-02 -3.70218046e-03 -8.73939544e-02  1.69450834e-01
  2.86459364e-02 -4.15708721e-01  1.62266269e-01  3.84089738e-01
 -1.99090064e-01  6.42936230e-02  4.31172490e-01  1.77313685e-01
  4.91576463e-01 -1.20666757e-01  2.87866831e-01 -1.05458237e-01
 -1.82356223e-01  3.70890759e-02 -1.09192736e-01 -1.27546461e-02
 -1.54237822e-01  5.56637764e-01  1.94817394e-01 -6.69941306e-02
  1.42845184e-01 -2.93615103e-01 -9.12238657e-03  1.97265297e-01
 -1.31619275e-01 -2.37089157e-01 -1.35257378e-01 -3.48218411e-01
 -2.91834176e-02  1.26455784e-01 -1.00096896e-01 -2.47701079e-01
  9.84414071e-02  1.39589518e-01 -2.02649742e-01 -1.84345037e-01
 -3.67980957e-01  5.70628978e-02 -4.68522571e-02 -3.08048069e-01
  3.67883407e-02 -1.96182206e-01 -2.25394145e-02 -2.65025467e-01
 -2.70142704e-02 -2.92266071e-01  4.51287508e-01  5.74420989e-01
 -1.99801832e-01  1.39445096e-01  1.53132766e-01 -9.85176936e-02
 -1.80176497e-01 -3.57561000e-02 -9.86426473e-02  9.85132605e-02
  1.19547144e-01 -9.79154259e-02  8.46877396e-02  7.21741199e-01
 -8.97241384e-02 -5.45311570e-02 -1.68645501e-01 -1.36772931e-01
  1.64850444e-01 -3.51852119e-01 -2.34176502e-01 -2.44938478e-01
  2.70369291e-01  2.91200936e-01  5.38117848e-02  2.35117853e-01
 -1.50156409e-01  3.19979191e-01  5.99070013e-01 -3.77470374e-01
 -3.95199388e-01 -3.00416127e-02  1.26171529e-01 -8.69138986e-02
 -5.66938743e-02 -8.48423690e-02 -7.20650330e-02 -7.12443665e-02]"
Validate variable dtype before restoring checkpoint stat:contribution welcome,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: [Yes](https://stackoverflow.com/questions/51137417/wrong-output-for-restored-variable-in-tensorflow-graph)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.0-dev20180620
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.0 / 7
- **GPU model and memory**: GeForce GTX 1080 / 8 Gb
- **Exact command to reproduce**: Run the two scripts below in ""Source code/logs""

### Describe the problem
A warning/error should be raised when a variable is loaded with a different dtype as that which was saved. 

In the code below, the variable `global_step` is saved as a `tf.int32`, but treated as a `tf.float32` when restored. When this happens, the `global_step` variable appears to have a value of 7e-45 instead of the expected value of 5. 

### Source code / logs
 [Link to Stackoverflow Post with the code/question](https://stackoverflow.com/questions/51137417/wrong-output-for-restored-variable-in-tensorflow-graph)

[Original post copied here]
**Script that saves the checkpoints:**

    import tensorflow as tf

    a = tf.Variable(3.0, name='a')
    b = tf.Variable(5.0, name='b')
    
    b = tf.assign_add(b, a)
    
    n_steps = 5
    
    global_step = tf.Variable(0, name='global_step', trainable=False)
    
    saver = tf.train.Saver()
    
    with tf.Session() as sess:
    
        sess.run(tf.global_variables_initializer())
        
        for step in range(n_steps):
            print(sess.run(b))
    
            global_step.assign_add(1).eval()
            print(global_step.eval())
    
            saver.save(sess, './my_test_model', global_step=global_step)

**Script that restores checkpoint:**

    import tensorflow as tf
    
    from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file
    
    # List ALL tensors.
    print_tensors_in_checkpoint_file(tf.train.latest_checkpoint('./'), all_tensors=True, tensor_name='')
    
    tf.reset_default_graph()
    
    a = tf.get_variable('a', shape=[])
    b = tf.get_variable('b', shape=[])
    global_step = tf.get_variable('global_step', shape=[])
    
    saver = tf.train.Saver()
    
    with tf.Session() as sess:
            
        ckpt = tf.train.latest_checkpoint('./')
        if ckpt:
            print(ckpt)
        
            saver.restore(sess, ckpt)
        
        else:
            print('Nothing restored')
        
        print(a.eval())
        print(b.eval())
        print(global_step.eval())

**Output**
```
tensor_name:  a
3.0
tensor_name:  b
20.0
tensor_name:  global_step
5
./my_test_model-5
INFO:tensorflow:Restoring parameters from ./my_test_model-5
3.0
20.0
7e-45
```

",True,"[-3.52331221e-01 -6.27682805e-01 -5.73475718e-01 -2.64098167e-01
  9.54136625e-02 -3.65047827e-02  1.27666667e-01 -1.55329164e-02
 -3.85196328e-01 -1.21306770e-01  8.96700025e-02 -4.34683636e-04
  7.34554008e-02 -2.55685449e-01 -4.47905540e-01  2.01246962e-01
  1.47628173e-01 -1.91798404e-01  1.17323339e-01 -2.42761448e-01
 -4.27090079e-01 -2.19914794e-01 -2.50419617e-01  3.20877969e-01
 -1.33119337e-02  2.49431863e-01 -1.27732158e-01  5.64390235e-02
  1.16329491e-01  6.55835867e-03  3.91454965e-01  1.23847663e-01
 -3.25336695e-01  1.13212667e-01 -6.66482598e-02  4.15402144e-01
 -1.78609580e-01  4.29296046e-02 -3.69470865e-02  6.02543205e-02
  2.83696234e-01 -2.13121206e-01  9.31847394e-02  1.64223939e-01
  1.69748679e-01 -1.60985693e-01 -1.80466637e-01 -1.50666153e-02
 -8.41709524e-02 -2.40112737e-01  3.21798086e-01  1.16911180e-01
 -4.11480099e-01 -6.27575070e-03 -3.85451913e-01  9.82246697e-02
  4.19553041e-01 -3.90036553e-02  7.37288296e-02  2.43306115e-01
  1.58069387e-01  2.38554291e-02 -8.97874683e-03  1.18021123e-01
  8.05695727e-02  8.54113791e-03  1.31618828e-01 -7.89269060e-02
  8.96241903e-01  1.19417548e-01  1.63051337e-01  1.44009972e-02
 -2.59109199e-01 -3.34904909e-01  1.00396283e-01  1.66005701e-01
  8.50428082e-03  7.15466812e-02  2.96212792e-01 -5.91576338e-01
  1.75560310e-01 -1.24192528e-01  2.37493664e-01 -2.11319029e-02
  1.51008181e-02 -2.32969113e-02  3.10471445e-01  1.96877480e-01
  3.54185909e-01 -8.33337605e-02  5.03961384e-01  2.86289811e-01
  8.54857862e-02  8.77788663e-02 -6.89126924e-03  1.89930499e-01
 -2.67372616e-02  4.16565806e-01  6.57846630e-02  1.20865852e-01
  8.35965015e-03 -1.52140975e-01 -2.55413860e-01 -1.55873150e-01
 -2.12662697e-01 -1.36738010e-02  4.37022969e-02 -7.36859292e-02
 -4.14638668e-02 -4.38674912e-02  1.58188455e-02 -7.31431395e-02
  1.43884495e-01  3.85869555e-02  2.00344324e-01 -1.47532932e-02
 -6.29885048e-02  6.21103756e-02 -3.45384419e-01  4.25961763e-01
  2.34951332e-01  8.91660154e-02  5.92366219e-01  2.02347636e-01
  4.16886449e-01 -6.42127357e-04  9.22634173e-03  5.23153022e-02
  2.37563401e-02  1.34154931e-01  2.91223735e-01  7.45529905e-02
 -1.51112825e-01  1.52111083e-01 -9.95856524e-02 -2.40422949e-01
 -9.52794701e-02  6.62137615e-03 -2.73344725e-01  3.37831303e-03
 -2.18062371e-01 -8.55236202e-02  7.91034773e-02 -3.87749523e-02
  4.85932976e-02  5.58340922e-04 -1.62929699e-01 -2.31891349e-02
  3.89626250e-02  2.52354473e-01 -1.57584935e-01 -1.38971984e-01
 -2.01127768e-01  3.60472381e-01  1.34748816e-01  5.68686843e-01
  1.06157906e-01 -6.62940089e-03 -2.69691572e-02 -7.04627693e-01
 -6.59937263e-02  3.66266340e-01  9.08170938e-02 -7.03638494e-02
 -9.24284160e-02 -4.93762270e-02 -1.69321030e-01 -7.53946900e-02
  7.08548129e-02  1.89499781e-01 -1.14289321e-01 -1.78976804e-01
 -2.71029651e-01  1.65199012e-01 -1.43382372e-03 -1.99010327e-01
  3.93132508e-01 -5.00544310e-01 -2.90052116e-01  1.77672535e-01
  1.87371582e-01 -3.38229209e-01 -1.54692739e-01  1.60420299e-01
 -1.06868312e-01  1.63419038e-01  7.70178139e-02  5.05365580e-02
 -1.86845496e-01 -9.20092613e-02 -2.81421393e-01  1.07605554e-01
 -4.80537340e-02 -1.41143486e-01 -9.80329514e-02 -1.95902079e-01
  1.37938321e-01 -6.05409145e-02  1.33189067e-01  2.42622584e-01
 -2.63398737e-01 -1.79662868e-01 -7.81745613e-02 -1.26514882e-01
  2.66504228e-01  1.06473431e-01 -2.33970866e-01 -4.04038042e-01
 -3.56211931e-01 -8.57254118e-02 -2.59695709e-01 -2.80600879e-02
  2.94606805e-01 -1.52837247e-01 -1.16385780e-01 -1.93816692e-01
 -2.39227116e-01  3.95950153e-02 -1.93935484e-01 -7.58836716e-02
  9.33868885e-02 -3.03978235e-01  1.17219046e-01 -1.62974447e-01
 -1.85163409e-01  1.42076537e-01 -2.71817207e-01  9.25202668e-02
  6.08519316e-02  2.23213941e-01 -7.63018355e-02  2.94091851e-01
  1.05747081e-01  2.45240688e-01  1.88221425e-01 -1.85967982e-01
  5.31153083e-02 -3.19518983e-01 -3.15910935e-01  2.58438319e-01
 -5.48475027e-01 -9.24318433e-02 -4.49602842e-01  5.00638708e-02
  4.47573364e-02  3.10543150e-01 -4.98423755e-01  3.74738038e-01
 -2.57006109e-01 -1.28192872e-01 -9.17252675e-02 -1.53614774e-01
  2.60757655e-01  1.72270238e-02  2.80536175e-01  9.17488039e-02
 -4.45408747e-02  2.91206121e-01  2.15202674e-01  8.75325501e-02
  5.28056860e-01  3.36833745e-02 -3.19588572e-01  3.29943687e-01
  3.48994970e-01  2.79037952e-01 -3.05308104e-01  2.29830697e-01
  2.23310497e-02 -1.24747768e-01  4.69867885e-02 -6.26391396e-02
  1.02951825e-01  6.68800473e-02 -6.96793720e-02 -2.14905292e-01
  5.75929046e-01  1.36857748e-01 -1.33949593e-01  3.42259705e-01
  2.79605716e-01  3.41516227e-01 -1.36886626e-01  1.04579747e-01
 -9.25099477e-03 -3.85901868e-01  1.48674324e-01 -3.04655671e-01
 -2.20059425e-01 -2.52909958e-01  1.23448968e-01  4.43472899e-02
 -5.75202405e-02 -1.69845864e-01  1.23442456e-01 -4.74158600e-02
 -2.66347490e-02  3.37094590e-02  2.65076011e-01  1.48151472e-01
 -3.00944686e-01 -2.92024076e-01  1.69994712e-01  1.09168567e-01
 -3.38366665e-02 -3.25066924e-01  3.45929831e-01  3.60663682e-01
  4.60742623e-01 -4.03783590e-01  2.15770975e-01 -2.44047403e-01
  2.07752697e-02  2.36004405e-02  1.80786788e-01  7.90411532e-02
 -5.55643082e-01  5.32602310e-01  1.22729819e-02  1.01201586e-01
  2.21306428e-01  4.11638096e-02 -1.83796868e-01 -6.11228570e-02
 -8.52882713e-02  2.23692171e-02  1.74539648e-02 -4.45749521e-01
 -1.87461674e-01  6.00362308e-02 -1.12495832e-01 -7.95907900e-02
 -1.74299702e-02 -2.49166593e-01 -1.42216623e-01 -1.53377712e-01
 -2.95326978e-01  4.45580304e-01 -4.98471875e-03 -2.36313254e-01
 -2.65946276e-02 -1.28235761e-03  1.32493168e-01 -2.74734199e-01
 -5.27301878e-02 -2.16725841e-03  2.12832436e-01  5.80491424e-01
  1.66533068e-01  1.64319843e-01  3.29842508e-01 -5.52754961e-02
  1.07442793e-02 -5.41396514e-02 -3.01265679e-02  1.77044332e-01
  2.94971526e-01  1.62961453e-01  2.73784220e-01  5.32818317e-01
 -2.89504379e-01 -7.64905438e-02 -1.83577701e-01 -9.41524804e-02
 -2.40783468e-02 -2.32885525e-01 -3.71504188e-01 -2.48178124e-01
  1.61797822e-01 -8.65823179e-02 -1.97153464e-01  1.20612353e-01
 -2.42083758e-01  3.04528743e-01  4.92452621e-01 -3.24054182e-01
 -2.84630060e-01  8.28367397e-02  2.35691577e-01 -4.40357387e-01
  1.22980058e-01 -3.37291621e-02 -1.44005984e-01 -9.18889865e-02]"
comparison between signed and unsigned integer stat:contribution welcome,"### System information
No need. It is about comparison warning happening in cc/framework/ops.h
ops.h:153:27: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
       if (t.NumElements() != v.size()) {
                            
cc1plus: all warnings being treated as errors

Unfortunately, due to policy out of my control, 
I cannot change the compiler option to treat warning as an error. 
I hope this to be fixed so that I can compile.

### Describe the problem
Same as above.
### Source code / logs
tensorflow/cc/framework/ops.h
",True,"[-4.42622393e-01  1.83668867e-01 -3.57260406e-01 -9.64095816e-02
  1.39189780e-01 -9.74857956e-02 -2.28697285e-02  2.65863597e-01
 -1.23416543e-01 -4.55545306e-01 -1.58285517e-02  3.66115831e-02
  1.11682825e-01 -3.45281214e-01 -9.71113965e-02 -2.49162719e-01
  2.19142377e-01 -1.53161526e-01  2.08320275e-01  8.38111937e-02
 -2.55244285e-01  3.51114362e-01 -3.06372225e-01  1.02455311e-01
 -2.19750889e-02  1.29162986e-02 -1.29073083e-01  3.69119942e-01
  8.34217817e-02  4.78340276e-02  2.59188175e-01  6.68736547e-02
  7.62434676e-02  5.24629354e-02  9.66568217e-02 -1.09193936e-01
 -6.40198514e-02 -2.25981504e-01 -2.61428773e-01  1.13992348e-01
  5.75991310e-02 -1.63796157e-01  2.33166292e-02 -1.09853387e-01
 -8.37515742e-02 -3.97036225e-02 -2.31197536e-01 -1.15722828e-02
 -7.47374967e-02  1.27842307e-01 -3.48645598e-02  1.58721060e-01
 -7.32536316e-02  8.22715089e-03  8.82071555e-02 -6.18824437e-02
 -3.85537714e-01 -1.57126904e-01 -2.74089247e-01  2.02124983e-01
  3.67562592e-01 -4.68985401e-02  7.83547610e-02 -1.47181451e-01
  3.34078133e-01  1.40797913e-01  3.44305933e-01 -2.52541274e-01
  2.72601306e-01 -7.26563856e-04  2.28712320e-01 -1.66776404e-02
 -5.09589255e-01 -3.11986506e-01  2.45066509e-02  2.12948769e-01
 -5.39614677e-01  2.54188590e-02  2.72531748e-01 -3.68033499e-01
 -2.68115371e-01 -2.79356599e-01 -1.81100950e-01 -2.05194987e-02
  1.97664976e-01 -1.49947882e-01  3.21302712e-01  4.65327501e-03
  1.97762772e-01  2.81426366e-02  3.05925906e-01  7.11878836e-02
 -1.04367889e-01  3.07651982e-02  2.11469308e-01  1.30710274e-01
  9.07405913e-02  8.84955674e-02 -5.75375222e-02  1.86413705e-01
 -1.11517146e-01 -1.08505972e-01 -2.54807830e-01 -1.69060305e-01
 -1.75718218e-04 -1.87237412e-01 -1.06288090e-01 -3.42857353e-02
  1.12489611e-01 -1.46151096e-01  1.96225315e-01  5.04296739e-03
 -5.31530306e-02 -3.57879139e-02 -3.42920497e-02 -1.52483620e-02
  1.15287282e-01  1.41984314e-01  6.20514527e-03  1.39038667e-01
  2.50447959e-01 -3.08380798e-02 -1.15378499e-01  4.36238527e-01
  2.90898055e-01  1.81135729e-01 -3.86010230e-01  6.85008168e-02
 -1.07157618e-01  9.74230543e-02  2.19356433e-01 -1.34854883e-01
 -3.53616327e-02 -2.61859223e-02  5.64096943e-02  1.61688685e-01
 -1.91804856e-01  8.41298327e-02 -1.68051541e-01  1.56526789e-01
  4.17595744e-01 -1.91494733e-01 -9.33245420e-02 -1.81280911e-01
 -3.22567895e-02  5.95129654e-02 -6.48187399e-02  1.91870525e-01
  1.42457634e-01 -1.31220594e-01  9.23378617e-02  1.15618601e-01
 -1.35379732e-01  3.44523311e-01 -3.09693038e-01 -2.65633464e-01
  4.20335561e-01 -1.86705947e-01  4.85496104e-01 -1.51407421e-01
 -6.52425364e-03  3.55194807e-01  2.39415258e-01 -3.57680991e-02
  3.20759088e-01  1.03637233e-01 -8.92810300e-02 -1.52072847e-01
 -5.62045462e-02  3.85702252e-01 -7.47841671e-02 -2.01816559e-01
  1.71832845e-01  1.33295923e-01  2.52664506e-01 -1.21314332e-01
  5.18892467e-01 -1.39857471e-01 -3.85486856e-02  2.32115418e-01
  1.04124531e-01  8.45855698e-02  1.66424662e-01  9.88477618e-02
 -3.28015313e-02  2.30697647e-01 -9.93277803e-02  4.01938736e-01
 -3.59392256e-01 -1.81186609e-02 -2.01384738e-01 -2.16964930e-01
 -2.85803139e-01 -1.19910892e-02 -8.88935775e-02 -4.21352834e-01
  3.78840007e-02 -7.69059733e-03  5.01057088e-01  1.38807565e-01
 -5.20320386e-02  3.69960546e-01 -1.76690400e-01 -3.20163935e-01
  2.55896807e-01  1.37131259e-01  7.03226253e-02 -4.05240804e-01
 -1.77355975e-01  1.57514572e-01 -2.09359378e-01 -1.25785142e-01
  8.23169947e-02 -1.22846022e-01 -2.35471517e-01 -1.05017006e-01
 -2.25731418e-01  1.53486252e-01  2.20421210e-01  2.42799997e-01
 -2.75812335e-02 -6.33817464e-02  2.45741338e-01 -1.36057779e-01
 -2.57567346e-01  6.98222518e-02 -2.39541709e-01 -2.73208823e-02
 -1.59721300e-01 -8.29887167e-02  2.47332081e-01 -1.71123445e-01
  3.22448194e-01 -6.02112859e-02  1.42715573e-01 -1.50859654e-01
 -9.84797627e-02 -2.19042450e-02 -1.10878579e-01 -2.85075486e-01
 -3.05274785e-01 -8.30745995e-02  1.03494942e-01 -1.07389607e-01
  3.11795294e-01  1.60619915e-01  1.43361643e-01 -1.60210766e-02
  4.96571586e-02 -8.90225172e-03 -7.41564929e-02 -8.71850923e-03
  1.50934115e-01 -3.49463731e-01  1.10102318e-01  4.77663338e-01
 -2.40869448e-02  2.96587408e-01 -6.23782612e-02 -1.12247244e-01
  1.23045683e-01  2.14265108e-01  2.84521669e-01  1.34244815e-01
  2.83223420e-01  1.81611747e-01  1.08762294e-01  1.01775832e-01
 -5.45968898e-02 -6.95650280e-02  1.94360211e-01 -1.45676062e-01
  3.38000536e-01 -3.36986840e-01  3.34875673e-01 -2.74275959e-01
  1.89113736e-01 -9.59400013e-02 -1.04734212e-01  7.09348843e-02
 -2.60610972e-02 -1.17791124e-01 -3.33453640e-02  7.08662719e-02
  2.81334400e-01 -1.98874846e-01  1.48097515e-01 -5.25150597e-02
 -2.61388958e-01 -1.03227727e-01  9.02356673e-03 -2.07183212e-01
  9.68980044e-02  5.48267588e-02  8.07704777e-02 -1.00033462e-01
 -2.44283639e-02 -1.29163072e-01 -3.07989195e-02 -6.46500736e-02
 -2.57848918e-01 -6.71572685e-02  3.68012965e-01 -1.32905334e-01
 -4.19728845e-01  9.95571390e-02  1.31068021e-01  1.28475070e-01
  3.50926280e-01 -1.60672650e-01  2.52674729e-01  2.09430024e-01
 -1.07481748e-01  2.29084939e-01 -1.55095309e-01  4.63935137e-01
 -2.95674592e-01  2.76338518e-01  6.11621626e-02 -3.50403413e-02
  6.93595558e-02 -3.45144033e-01 -2.16426536e-01 -1.67639405e-02
  3.00104082e-01  5.57758473e-02 -3.30119044e-01 -4.94084895e-01
 -6.32215515e-02  8.46684799e-02  2.07398623e-01 -3.06715742e-02
 -9.37724411e-02 -2.29231387e-01 -1.44892335e-01 -3.64070296e-01
 -2.20456809e-01  1.05456129e-01 -4.12702039e-02 -6.19110167e-02
 -3.35565269e-01  1.36892229e-01 -3.31125468e-01 -3.34224403e-01
 -1.96014702e-01 -7.27724284e-03  2.45622396e-01  4.16232258e-01
 -1.25852257e-01 -6.06284514e-02  1.99517533e-01  6.82386607e-02
 -1.03542939e-01 -1.78052276e-01 -1.48861445e-02 -3.50554381e-03
  2.72548467e-01  6.58151507e-02  1.24878153e-01  2.24469185e-01
 -8.10122490e-02  1.02277324e-02 -5.30748248e-01  7.12443516e-03
  1.22136131e-01 -1.22796439e-01  3.00834067e-02  1.51505291e-01
  4.21520948e-01 -1.24284022e-01 -7.62922689e-02 -8.62131268e-02
 -6.50876015e-02  1.50527567e-01  7.54377469e-02 -3.20157230e-01
 -3.20240445e-02 -3.85381490e-01  4.11042303e-01 -1.93105787e-02
  5.23765385e-02 -2.57737100e-01  1.12529889e-01  1.19777836e-01]"
ppc64le: //tensorflow/contrib/lite/kernels:resize_bilinear_test test fails CPU test stat:community support comp:lite,"Please assign this issue to me and add the tag: stat:community support

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ppc64le Ubuntu 16.04.4
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: master from June 27th
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.11.0
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
bazel test -c opt --cache_test_results=no //tensorflow/contrib/lite/kernels:resize_bilinear_test


### Describe the problem
```
FAIL: //tensorflow/contrib/lite/kernels:resize_bilinear_test (see /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/ppc-opt/testlogs/tensorflow/contrib/lite/kernels/resize_bilinear_test/test.log)
INFO: From Testing //tensorflow/contrib/lite/kernels:resize_bilinear_test:
==================== Test output for //tensorflow/contrib/lite/kernels:resize_bilinear_test:
[==========] Running 10 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 10 tests from ResizeBilinearOpTest
[ RUN      ] ResizeBilinearOpTest.HorizontalResize
[       OK ] ResizeBilinearOpTest.HorizontalResize (0 ms)
[ RUN      ] ResizeBilinearOpTest.HorizontalResize8Bit
[       OK ] ResizeBilinearOpTest.HorizontalResize8Bit (0 ms)
[ RUN      ] ResizeBilinearOpTest.VerticalResize
[       OK ] ResizeBilinearOpTest.VerticalResize (0 ms)
[ RUN      ] ResizeBilinearOpTest.VerticalResize8Bit
[       OK ] ResizeBilinearOpTest.VerticalResize8Bit (0 ms)
[ RUN      ] ResizeBilinearOpTest.TwoDimensionalResize
[       OK ] ResizeBilinearOpTest.TwoDimensionalResize (0 ms)
[ RUN      ] ResizeBilinearOpTest.TwoDimensionalResize8Bit
[       OK ] ResizeBilinearOpTest.TwoDimensionalResize8Bit (1 ms)
[ RUN      ] ResizeBilinearOpTest.TwoDimensionalResizeWithTwoBatches
[       OK ] ResizeBilinearOpTest.TwoDimensionalResizeWithTwoBatches (0 ms)
[ RUN      ] ResizeBilinearOpTest.ThreeDimensionalResize
[       OK ] ResizeBilinearOpTest.ThreeDimensionalResize (0 ms)
[ RUN      ] ResizeBilinearOpTest.TwoDimensionalResizeWithTwoBatches8Bit
tensorflow/contrib/lite/kernels/resize_bilinear_test.cc:261: Failure
Value of: m.GetOutput<uint8>()
Expected: has 18 elements where
element #0 is approximately 3 (absolute error <= 9.9999997e-06),
element #1 is approximately 5 (absolute error <= 9.9999997e-06),
element #2 is approximately 6 (absolute error <= 9.9999997e-06),
element #3 is approximately 7 (absolute error <= 9.9999997e-06),
element #4 is approximately 9 (absolute error <= 9.9999997e-06),
element #5 is approximately 10 (absolute error <= 9.9999997e-06),
element #6 is approximately 9 (absolute error <= 9.9999997e-06),
element #7 is approximately 11 (absolute error <= 9.9999997e-06),
element #8 is approximately 12 (absolute error <= 9.9999997e-06),
element #9 is approximately 4 (absolute error <= 9.9999997e-06),
element #10 is approximately 8 (absolute error <= 9.9999997e-06),
element #11 is approximately 10 (absolute error <= 9.9999997e-06),
element #12 is approximately 8 (absolute error <= 9.9999997e-06),
element #13 is approximately 12 (absolute error <= 9.9999997e-06),
element #14 is approximately 14 (absolute error <= 9.9999997e-06),
element #15 is approximately 10 (absolute error <= 9.9999997e-06),
element #16 is approximately 13 (absolute error <= 9.9999997e-06),
element #17 is approximately 16 (absolute error <= 9.9999997e-06)
  Actual: { '\x3' (3), '\x5' (5), '\x6' (6), '\a' (7), '\t' (9), '\n' (10, 0xA), '\t' (9), '\v' (11, 0xB), '\f' (12, 0xC), '\x4' (4), '\b' (8), '\n' (10, 0xA), '\b' (8), '\f' (12, 0xC), '\xE' (14), '\n' (10, 0xA), '\xE' (14), '\x10' (16) }, whose element #16 doesn't match, which is 1 from 13
tensorflow/contrib/lite/kernels/resize_bilinear_test.cc:278: Failure
Value of: const_m.GetOutput<uint8>()
Expected: has 18 elements where
element #0 is approximately 3 (absolute error <= 9.9999997e-06),
element #1 is approximately 5 (absolute error <= 9.9999997e-06),
element #2 is approximately 6 (absolute error <= 9.9999997e-06),
element #3 is approximately 7 (absolute error <= 9.9999997e-06),
element #4 is approximately 9 (absolute error <= 9.9999997e-06),
element #5 is approximately 10 (absolute error <= 9.9999997e-06),
element #6 is approximately 9 (absolute error <= 9.9999997e-06),
element #7 is approximately 11 (absolute error <= 9.9999997e-06),
element #8 is approximately 12 (absolute error <= 9.9999997e-06),
element #9 is approximately 4 (absolute error <= 9.9999997e-06),
element #10 is approximately 8 (absolute error <= 9.9999997e-06),
element #11 is approximately 10 (absolute error <= 9.9999997e-06),
element #12 is approximately 8 (absolute error <= 9.9999997e-06),
element #13 is approximately 12 (absolute error <= 9.9999997e-06),
element #14 is approximately 14 (absolute error <= 9.9999997e-06),
element #15 is approximately 10 (absolute error <= 9.9999997e-06),
element #16 is approximately 13 (absolute error <= 9.9999997e-06),
element #17 is approximately 16 (absolute error <= 9.9999997e-06)
  Actual: { '\x3' (3), '\x5' (5), '\x6' (6), '\a' (7), '\t' (9), '\n' (10, 0xA), '\t' (9), '\v' (11, 0xB), '\f' (12, 0xC), '\x4' (4), '\b' (8), '\n' (10, 0xA), '\b' (8), '\f' (12, 0xC), '\xE' (14), '\n' (10, 0xA), '\xE' (14), '          \x10' (16) }, whose element #16 doesn't match, which is 1 from 13
[  FAILED  ] ResizeBilinearOpTest.TwoDimensionalResizeWithTwoBatches8Bit (0 ms)
[ RUN      ] ResizeBilinearOpTest.ThreeDimensionalResize8Bit
tensorflow/contrib/lite/kernels/resize_bilinear_test.cc:293: Failure
Value of: m.GetOutput<uint8>()
Expected: has 18 elements where
element #0 is approximately 3 (absolute error <= 9.9999997e-06),
element #1 is approximately 4 (absolute error <= 9.9999997e-06),
element #2 is approximately 5 (absolute error <= 9.9999997e-06),
element #3 is approximately 8 (absolute error <= 9.9999997e-06),
element #4 is approximately 6 (absolute error <= 9.9999997e-06),
element #5 is approximately 10 (absolute error <= 9.9999997e-06),
element #6 is approximately 7 (absolute error <= 9.9999997e-06),
element #7 is approximately 8 (absolute error <= 9.9999997e-06),
element #8 is approximately 9 (absolute error <= 9.9999997e-06),
element #9 is approximately 12 (absolute error <= 9.9999997e-06),
element #10 is approximately 10 (absolute error <= 9.9999997e-06),
element #11 is approximately 14 (absolute error <= 9.9999997e-06),
element #12 is approximately 9 (absolute error <= 9.9999997e-06),
element #13 is approximately 10 (absolute error <= 9.9999997e-06),
element #14 is approximately 11 (absolute error <= 9.9999997e-06),
element #15 is approximately 13 (absolute error <= 9.9999997e-06),
element #16 is approximately 12 (absolute error <= 9.9999997e-06),
element #17 is approximately 16 (absolute error <= 9.9999997e-06)
  Actual: { '\x3' (3), '\x4' (4), '\x5' (5), '\b' (8), '\x6' (6), '\n' (10, 0xA), '\a' (7), '\b' (8), '\t' (9), '\f' (12, 0xC), '\n' (10, 0xA), '\xE' (14), '\t' (9), '\n' (10, 0xA), '\v' (11, 0xB), '\xE' (14), '\f' (12, 0xC), '          \x10' (16) }, whose element #15 doesn't match, which is 1 from 13
tensorflow/contrib/lite/kernels/resize_bilinear_test.cc:305: Failure
Value of: const_m.GetOutput<uint8>()
Expected: has 18 elements where
element #0 is approximately 3 (absolute error <= 9.9999997e-06),
element #1 is approximately 4 (absolute error <= 9.9999997e-06),
element #2 is approximately 5 (absolute error <= 9.9999997e-06),
element #3 is approximately 8 (absolute error <= 9.9999997e-06),
element #4 is approximately 6 (absolute error <= 9.9999997e-06),
element #5 is approximately 10 (absolute error <= 9.9999997e-06),
element #6 is approximately 7 (absolute error <= 9.9999997e-06),
element #7 is approximately 8 (absolute error <= 9.9999997e-06),
element #8 is approximately 9 (absolute error <= 9.9999997e-06),
element #9 is approximately 12 (absolute error <= 9.9999997e-06),
element #10 is approximately 10 (absolute error <= 9.9999997e-06),
element #11 is approximately 14 (absolute error <= 9.9999997e-06),
element #12 is approximately 9 (absolute error <= 9.9999997e-06),
element #13 is approximately 10 (absolute error <= 9.9999997e-06),
element #14 is approximately 11 (absolute error <= 9.9999997e-06),
element #15 is approximately 13 (absolute error <= 9.9999997e-06),
element #16 is approximately 12 (absolute error <= 9.9999997e-06),
element #17 is approximately 16 (absolute error <= 9.9999997e-06)
  Actual: { '\x3' (3), '\x4' (4), '\x5' (5), '\b' (8), '\x6' (6), '\n' (10, 0xA), '\a' (7), '\b' (8), '\t' (9), '\f' (12, 0xC), '\n' (10, 0xA), '\xE' (14), '\t' (9), '\n' (10, 0xA), '\v' (11, 0xB), '\xE' (14), '\f' (12, 0xC), '          \x10' (16) }, whose element #15 doesn't match, which is 1 from 13
[  FAILED  ] ResizeBilinearOpTest.ThreeDimensionalResize8Bit (1 ms)
[----------] 10 tests from ResizeBilinearOpTest (2 ms total)

[----------] Global test environment tear-down
[==========] 10 tests from 1 test case ran. (2 ms total)
[  PASSED  ] 8 tests.
[  FAILED  ] 2 tests, listed below:
[  FAILED  ] ResizeBilinearOpTest.TwoDimensionalResizeWithTwoBatches8Bit
[  FAILED  ] ResizeBilinearOpTest.ThreeDimensionalResize8Bit

 2 FAILED TESTS
================================================================================
```

### Source code / logs
see above
",True,"[-2.88643032e-01 -3.63769174e-01 -1.64579839e-01  9.53489915e-03
 -8.85195136e-02 -2.44176418e-01 -1.54168218e-01  1.80173233e-01
 -3.81139517e-01 -3.20920795e-01  9.75491386e-03 -3.96551862e-02
 -2.67975628e-01 -1.18540034e-01 -3.27498138e-01 -4.92637232e-03
  6.95265532e-02 -2.39519298e-01  2.84679353e-01 -2.40499042e-02
 -3.29998970e-01  4.53254245e-02 -3.89998466e-01  2.76336849e-01
  1.32227212e-01  8.34834576e-02 -1.31671503e-01  2.07166940e-01
 -1.18685171e-01  1.07652411e-01  6.70515954e-01  4.35660213e-01
  1.71860889e-01  9.10325199e-02  1.82601824e-01  1.81528687e-01
  8.99016410e-02 -2.53393710e-01 -1.87467232e-01 -1.14565246e-01
  2.14733332e-01  5.36879860e-02 -5.10556027e-02  9.10836458e-02
  2.73266464e-01 -1.69042274e-01 -1.54016286e-01  6.57749921e-03
 -9.67688262e-02 -1.84176356e-01 -4.03367318e-02 -4.23877016e-02
 -1.21720456e-01 -2.53012657e-01  3.21923345e-02 -8.48368853e-02
  1.34825408e-01 -5.85873351e-02  8.34471211e-02  1.56001955e-01
  1.41118690e-01  3.04300766e-02  1.51606888e-01  6.94561005e-02
  1.24581352e-01  1.24521002e-01  3.31857175e-01 -1.28523424e-01
  4.50727642e-01  1.11037903e-01 -1.16696268e-01 -1.05441855e-02
 -3.15911055e-01 -6.34021014e-02  6.35374933e-02  2.32093960e-01
 -4.15934503e-01  8.79105553e-03  4.05736148e-01 -1.23636395e-01
  1.06425539e-01 -1.44547477e-01  2.39857063e-01 -1.60918340e-01
 -8.71813204e-03  2.54995227e-01  1.94047153e-01  7.20761642e-02
  4.65048254e-02 -2.96992123e-01  1.94986969e-01  4.25026357e-01
 -8.77541676e-02  1.26060933e-01  9.50106978e-02 -1.19522311e-01
  7.01969862e-03 -3.42457443e-01 -1.99234068e-01  6.88530728e-02
 -7.21401721e-02 -1.23077780e-01  5.09400666e-02  1.23525083e-01
 -4.01277870e-01 -2.19446123e-01  1.76511675e-01  2.56862819e-01
 -1.41602680e-01 -1.01493336e-01  3.64982605e-01 -4.74254265e-02
 -3.22748423e-02 -6.84512556e-02  3.17070186e-01  1.11095145e-01
  1.70712899e-02  1.13077965e-02  9.88421366e-02  8.21442485e-01
 -2.30847076e-01 -1.06857494e-01 -7.18552694e-02  1.73405156e-01
  4.85420525e-01  5.38452342e-02  3.26707959e-06  1.50429696e-01
  7.33610839e-02  2.19458774e-01  8.12806189e-02  1.35423154e-01
  1.25241466e-02  4.64726165e-02 -9.60992575e-02  1.45725489e-01
 -1.11743227e-01 -1.80857956e-01  8.74793828e-02  1.01879172e-01
  1.15056559e-01  3.01923364e-01  8.63171145e-02 -1.92584395e-01
 -1.02035031e-01  9.89336818e-02 -3.87316585e-01 -2.35634297e-02
 -1.70738876e-01  4.88578640e-02  6.51096255e-02  1.57368556e-03
 -2.08998658e-02  1.84327811e-01  1.05309181e-01  1.19573206e-01
  4.58420277e-01 -9.78165120e-02 -4.31075655e-02 -3.54099572e-01
 -6.37044013e-02  3.28743428e-01 -1.43963486e-01  5.63380495e-02
  2.98682719e-01 -4.63543274e-02 -4.08948660e-01 -1.06355414e-01
  1.08673379e-01  2.18402863e-01  5.75030670e-02 -1.16120353e-01
 -5.35723753e-03  2.14201972e-01  1.41392216e-01 -2.68894434e-02
  3.31356078e-01 -1.62791476e-01 -3.22837472e-01  2.71123320e-01
  1.47452235e-01 -9.33683664e-03  2.44143419e-02  1.34209454e-01
  2.66232193e-02  7.67671168e-02  4.58141156e-02 -1.12853035e-01
 -1.59094274e-01 -1.88665450e-01 -4.85698521e-01 -1.74833819e-01
  7.35235736e-02 -2.59407550e-01 -2.18658656e-01 -3.24212909e-02
  1.48376554e-01  2.10050434e-01  9.86335203e-02  1.79857209e-01
 -1.82521850e-01  1.92333162e-01  3.85382250e-02 -1.06569976e-01
  1.57224178e-01  2.52007544e-02 -4.63869363e-01 -2.20590740e-01
 -1.04772806e-01 -1.09056428e-01 -4.75505255e-02 -2.92051464e-01
 -5.17266989e-03 -2.36200184e-01 -2.27687240e-01  3.47769633e-02
  1.06243454e-01 -2.22923085e-01 -1.06174812e-01  2.73375481e-01
  6.94172904e-02 -1.55450821e-01 -7.79945850e-02 -1.64811999e-01
 -3.70551556e-01 -1.34038463e-01 -2.10824251e-01  2.50363111e-01
 -3.87669355e-02  1.35656565e-01  2.84357294e-02  5.90249225e-02
  2.79008985e-01  1.39287695e-01  1.78506255e-01 -1.38355881e-01
  3.00628040e-02 -1.67483523e-01 -2.34194770e-01  1.11400917e-01
 -4.72875357e-01 -2.31109887e-01  1.73372239e-01  2.02591807e-01
  1.51979238e-01  1.43960327e-01 -3.61647904e-02  2.76839852e-01
 -1.05845518e-02  3.08505222e-02 -1.45228684e-01 -3.98877382e-01
  2.82611728e-01 -4.50329110e-02  2.05355823e-01 -6.86558262e-02
 -1.63303658e-01 -1.08396262e-01  1.10451981e-01 -1.80604249e-01
  2.47085109e-01  5.23003936e-01 -6.24208078e-02  3.83573145e-01
  3.30427378e-01  2.00168341e-01 -2.93147266e-01 -4.26295847e-02
  1.81271791e-01 -1.71477020e-01  4.21704173e-01 -2.90137798e-01
  8.34718570e-02 -1.44551963e-01  1.29636198e-01 -1.74182087e-01
  3.25954854e-01  2.61671811e-01 -1.59994978e-03  2.08669603e-02
  2.81669557e-01  2.10434526e-01 -2.81185091e-01  1.60733983e-01
  2.66010500e-02 -2.13434815e-01  4.23432663e-02 -2.31903329e-01
 -1.56469658e-01 -1.94662184e-01 -3.98510881e-03  1.03424385e-01
  3.34973931e-02  2.79173076e-01 -2.36308292e-01  1.99370146e-01
  5.95615990e-02  3.81406806e-02 -4.51768469e-03 -4.69759777e-02
 -2.13671654e-01 -1.27497911e-01  1.56667568e-02  9.39439982e-02
 -1.78154513e-01 -3.28351371e-02  1.56480506e-01 -1.39687546e-02
  5.02929747e-01 -2.83914566e-01  7.71368891e-02  2.21332014e-01
  1.34360520e-02  3.93657267e-01 -1.37776896e-01 -9.92073677e-03
 -3.19432795e-01  3.37671638e-01 -1.99572295e-01 -4.39765565e-02
  3.69856894e-01  1.90724120e-01 -3.19829255e-01  2.07098618e-01
  4.65679616e-02 -1.11276843e-01 -1.71901152e-01 -2.35293061e-01
 -3.11772943e-01  6.86734021e-02 -4.43635732e-02 -7.87797477e-03
 -1.34818390e-01 -6.78782314e-02 -8.78432319e-02 -7.42002800e-02
 -2.20140353e-01  3.97429675e-01  3.13052982e-02 -1.68125227e-01
  1.61598772e-01 -1.19716808e-01  1.33680686e-01 -2.18147367e-01
 -1.00198030e-01 -2.30283737e-01  3.36830914e-01  3.52513194e-01
 -1.99156046e-01  2.75352616e-02 -1.19258270e-01  4.18607034e-02
 -4.57102656e-01 -4.13811952e-02 -4.13429439e-02  2.76687145e-01
 -3.31972018e-02  2.70728767e-01  1.14723340e-01  6.16366029e-01
 -4.40506011e-01 -1.63114280e-01 -4.90362555e-01 -2.92763978e-01
  1.63327262e-01 -1.82479352e-01 -1.30455181e-01 -2.86049634e-01
  1.13378517e-01  1.33874714e-01 -7.59088472e-02  1.84909075e-01
 -2.06024304e-01  1.15924351e-01  2.76005238e-01 -2.26100698e-01
 -2.23021388e-01  1.76740550e-02 -2.34170243e-01 -2.51599401e-01
  6.72872514e-02  1.05672423e-03  2.75148824e-02  6.02873750e-02]"
//tensorflow/contrib/distributions:matrix_inverse_tril_test fails on ppc64le stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
      Ubuntu 16.04 (ppc64le)
- **TensorFlow installed from (source or binary)**:
      Installed from source
- **TensorFlow version (use command below)**:
      TF master
- **Python version**: 
     Python 2.7.5
- **Bazel version (if compiling from source)**:
     bazel-0.11.1
- **CUDA/cuDNN version**:
     NA
- **GPU model and memory**:
      NA
- **Exact command to reproduce**:
 `bazel test -c opt --jobs 1 -k --cache_test_results=no --test_output=errors  //tensorflow/contrib/distributions:matrix_inverse_tril_test`

### Describe the problem
Getting error `InvalidArgumentError: assertion failed: [Input must be lower triangular.] [Condition x == y did not hold element-wise:]`.
Not sure what exactly causing this issue, need to investigate.

### Source code / logs
```
E.......
======================================================================
ERROR: testBatch (__main__.MatrixInverseTriLBijectorTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 625, in decorated
    f(self, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/kernel_tests/bijectors/matrix_inverse_tril_test.py"", line 110, in testBatch
    y_, x_back_, fldj_, ildj_ = self.evaluate([y, x_back, fldj, ildj])
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 864, in evaluate
    return sess.run(tensors)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 877, in run
    run_metadata_ptr)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1272, in _do_run
    run_metadata)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1291, in _do_call
    raise type(e)(node_def, op, message)
InvalidArgumentError: assertion failed: [Input must be lower triangular.] [Condition x == y did not hold element-wise:] [x (matrix_inverse_tril_1/inverse/MatrixBandPart:0) = ] [[[[0 2.77555756e-17][0]]]...] [y (matrix_inverse_tril_1/inverse/zeros_like:0) = ] [[[[0 0][0]]]...]
         [[Node: matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT, DT_STRING, DT_FLOAT], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_0, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_1, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_2, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch_1, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_4, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch_2)]]

Caused by op u'matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert', defined at:
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/kernel_tests/bijectors/matrix_inverse_tril_test.py"", line 190, in <module>
    test.main()
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 64, in main
    return _googletest.main(argv)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 100, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 344, in benchmarks_main
    true_main()
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 99, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 70, in g_main
    return unittest_main(argv=argv)
  File ""/usr/lib/python2.7/unittest/main.py"", line 95, in __init__
    self.runTests()
  File ""/usr/lib/python2.7/unittest/main.py"", line 232, in runTests
    self.result = testRunner.run(self.test)
  File ""/usr/lib/python2.7/unittest/runner.py"", line 151, in run
    test(result)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/usr/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/usr/lib/python2.7/unittest/case.py"", line 393, in __call__
    return self.run(*args, **kwds)
  File ""/usr/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 625, in decorated
    f(self, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/kernel_tests/bijectors/matrix_inverse_tril_test.py"", line 106, in testBatch
    x_back = inv.inverse(x_inv_)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/distributions/bijector_impl.py"", line 800, in inverse
    return self._call_inverse(y, name)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/distributions/bijector_impl.py"", line 779, in _call_inverse
    mapping = mapping.merge(x=self._inverse(y, **kwargs))
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/ops/bijectors/matrix_inverse_tril.py"", line 80, in _inverse
    return self._forward(y)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/ops/bijectors/matrix_inverse_tril.py"", line 74, in _forward
    with ops.control_dependencies(self._assertions(x)):
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/contrib/distributions/python/ops/bijectors/matrix_inverse_tril.py"", line 138, in _assertions
    message=""Input must be lower triangular."")
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/check_ops.py"", line 382, in assert_equal
    return control_flow_ops.Assert(condition, data, summarize=summarize)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/util/tf_should_use.py"", line 118, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py"", line 151, in Assert
    guarded_assert = cond(condition, no_op, true_assert, name=""AssertGuard"")
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py"", line 2049, in cond
    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py"", line 1890, in BuildCondBranch
    original_result = fn()
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/control_flow_ops.py"", line 149, in true_assert
    condition, data, summarize, name=""Assert"")
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_logging_ops.py"", line 51, in _assert
    name=name)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py"", line 432, in new_func
    return func(*args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 3206, in create_op
    op_def=op_def)
  File ""/root/.cache/bazel/_bazel_root/78afe71156c58ea89dc510bdb03ba2b0/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/contrib/distributions/matrix_inverse_tril_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1701, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): assertion failed: [Input must be lower triangular.] [Condition x == y did not hold element-wise:] [x (matrix_inverse_tril_1/inverse/MatrixBandPart:0) = ] [[[[0 2.77555756e-17][0]]]...] [y (matrix_inverse_tril_1/inverse/zeros_like:0) = ] [[[[0 0][0]]]...]
         [[Node: matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_FLOAT, DT_STRING, DT_FLOAT], summarize=3, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_0, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_1, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_2, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch_1, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/data_4, matrix_inverse_tril_1/inverse/assert_equal_1/Assert/AssertGuard/Assert/Switch_2)]]


----------------------------------------------------------------------
Ran 8 tests in 3.177s
```",True,"[-4.62359875e-01 -8.13243389e-01 -2.51122802e-01 -4.59619910e-02
  1.54240519e-01 -2.75501013e-01  9.97937843e-02 -1.71694070e-01
 -3.77592683e-01 -2.04591125e-01  3.63531113e-01 -6.19190559e-02
 -1.23900816e-01  1.18845120e-01 -2.92761147e-01  5.89880235e-02
 -1.88840464e-01 -4.22321081e-01  2.89784312e-01  4.55792472e-02
 -1.24720447e-01 -1.92640424e-01 -4.67680395e-01  4.36700225e-01
  2.73120075e-01  2.73733437e-01 -1.65527016e-01 -1.28471911e-01
  8.08411986e-02  8.83144587e-02  8.47770512e-01  3.52238357e-01
 -9.36155468e-02 -3.41882296e-02 -8.72758925e-02  2.63334453e-01
 -8.71680900e-02 -9.79436189e-02  5.28370440e-02  9.51369777e-02
  2.20916778e-01  5.87540194e-02 -2.09256522e-02  1.92775607e-01
  5.21431565e-02 -1.71676800e-02  1.18132733e-01  2.30494887e-03
 -9.92155373e-02 -1.54086068e-01  1.18782707e-01  2.51509491e-02
 -2.12380111e-01 -3.05519223e-01  5.56830876e-02 -1.11154407e-01
  3.24266940e-01 -5.71085364e-02 -5.02405614e-02  2.30759561e-01
  2.16904044e-01 -2.64242012e-03 -1.04912058e-01  1.65824071e-01
 -6.00210205e-02  2.33962059e-01  2.96252817e-01 -2.59921253e-01
  6.69478476e-01 -3.26866880e-02  2.97402255e-02 -1.18684266e-02
 -2.21561342e-01 -1.79451585e-01 -7.13753775e-02  2.38396883e-01
 -3.20163481e-02  1.82996504e-03  2.44506910e-01 -9.78381634e-02
  1.60499305e-01  1.29344940e-01  3.90484273e-01 -1.29381329e-01
 -5.30999824e-02 -8.88921767e-02  2.73277164e-01  2.92893738e-01
  2.28071824e-01 -3.26777756e-01  6.00391269e-01  1.64909720e-01
 -1.07884608e-01  3.05116549e-02  1.55867875e-01  2.50358403e-01
 -7.12490082e-02  2.00084865e-01  2.00671218e-02 -1.93765327e-01
 -8.36715251e-02 -3.22046056e-02 -2.57522404e-01  6.00767732e-02
 -2.78901398e-01 -5.93353175e-02  1.03296131e-01 -5.04367910e-02
  9.04160365e-02 -8.14921558e-02 -1.17273621e-01  8.10265318e-02
  1.71262771e-01  8.24835524e-02  2.56314278e-01  7.30677918e-02
 -9.85109061e-02  1.76545933e-01  1.03953108e-01  8.34095001e-01
 -4.23166811e-01 -2.80819893e-01  2.92757720e-01  6.25929162e-02
  4.10477221e-01  1.14557043e-01 -2.88056135e-02 -8.70466977e-02
  1.43201023e-01  1.60309464e-01  2.78301507e-01  7.50036687e-02
 -3.27219844e-01  9.25617293e-02 -2.27486879e-01 -2.52492607e-01
 -1.90449655e-01  7.12032150e-03 -3.43468264e-02  7.15621561e-02
 -1.88194931e-01  3.09241354e-01 -1.92257077e-01 -9.12668109e-02
  4.39089015e-02 -8.23391080e-02 -1.84234843e-01  1.29741758e-01
  5.04124910e-06  2.36710012e-01 -1.98901325e-01  6.77310675e-02
 -8.90467167e-02  3.16787153e-01  1.65884234e-02  4.19681013e-01
  3.04613292e-01 -1.30365759e-01 -1.47913069e-01 -4.64021266e-01
  5.85855991e-02  4.81281906e-01 -2.29438916e-01  3.45887244e-02
  3.16985659e-02  8.10171440e-02 -4.45202440e-01 -2.72509694e-01
  1.45914897e-01  1.80726141e-01  1.94049358e-01 -5.18384995e-03
 -5.93461245e-02  3.53836715e-01 -8.46345536e-03 -3.56417857e-02
  3.47677171e-01 -6.60432398e-01 -1.56126752e-01  2.97674328e-01
 -3.46566401e-02 -1.34636730e-01  3.86060700e-02  3.23892564e-01
 -2.12446041e-03  2.43604146e-02  1.81806073e-01 -2.09373847e-01
 -2.79558331e-01 -8.01338181e-02 -3.16886514e-01 -2.05929458e-01
  9.80685353e-02 -1.30681172e-01 -4.25370514e-01 -1.53289467e-01
  4.23891783e-01  2.83132493e-01 -2.79923715e-02  2.19717935e-01
 -2.57715344e-01 -2.58928776e-01  1.55210555e-01  7.26787895e-02
 -9.01803281e-03 -3.09000462e-01 -2.98205733e-01 -9.25239474e-02
 -5.21301568e-01  2.49677226e-02  4.70294990e-02 -4.12338614e-01
  1.80019483e-01  4.43377420e-02 -3.49216759e-01 -1.93100162e-02
  2.14792371e-01 -2.28204921e-01 -1.42787978e-01  2.00981617e-01
  2.01297954e-01 -3.14592242e-01 -7.25082867e-03 -2.78941125e-01
 -2.28592381e-01 -2.33547792e-01 -1.21267393e-01 -4.66222018e-02
  1.60035968e-01  2.91891903e-01 -1.97432995e-01  3.63018624e-02
  4.57589924e-02  7.95933679e-02  2.46537760e-01 -2.33146653e-01
 -6.06283918e-02 -1.38013974e-01 -1.21903449e-01  1.39050588e-01
 -4.64379132e-01 -2.45196924e-01  1.37878358e-01 -5.66615090e-02
  5.23354635e-02  5.67289330e-02 -6.75638020e-02  5.97922653e-02
 -3.04626673e-01  2.55111873e-01 -1.66830927e-01 -3.47788870e-01
  3.62278759e-01  1.55491263e-01  2.75159895e-01  5.36466278e-02
 -7.97998011e-02  1.07339278e-01  2.26708516e-01 -7.66811147e-02
  3.39845598e-01  3.26121300e-01 -4.73841161e-01  5.65162301e-01
  8.45584124e-02  3.72959912e-01 -5.88520579e-02  3.15892518e-01
 -7.08872601e-02 -1.42408520e-01  2.23987311e-01 -2.20916107e-01
  2.72512168e-01 -1.30542070e-01 -1.49859190e-01 -8.11888725e-02
  3.49120408e-01  1.60156056e-01  1.21235922e-01  5.40793501e-02
  3.06859314e-01  2.21070990e-01 -1.81514844e-01  9.64484736e-03
 -2.52194107e-01 -2.50720114e-01 -1.20687485e-01 -3.76214862e-01
 -2.23923653e-01  1.38174951e-01 -2.92255372e-01  1.33286521e-01
  1.89767241e-01  9.22581553e-02 -1.98806211e-01  1.53485239e-01
  1.14257231e-01  7.51135945e-02  2.14712918e-01  3.23317796e-01
 -2.87537277e-01 -9.09515172e-02  3.85296792e-01 -6.91783428e-02
 -1.39730573e-01 -1.33373260e-01  3.53858054e-01  9.45350751e-02
  3.27215880e-01 -3.33976775e-01  2.48524964e-01 -1.24279819e-01
  1.80245683e-01  1.80800438e-01  7.71951526e-02  1.69235226e-02
 -2.24539027e-01  5.11419594e-01  2.78214991e-01 -1.58767283e-01
  2.18604773e-01 -6.40194714e-02 -4.01269794e-01  4.77351844e-02
  8.37164521e-02  6.66271336e-03 -1.66992754e-01 -3.40879917e-01
 -2.43602216e-01 -3.22742015e-03 -5.99886104e-02 -1.15399934e-01
 -1.25051469e-01 -1.59806721e-02 -4.53349985e-02 -1.22295231e-01
 -2.11304262e-01  3.79513711e-01  1.34613246e-01 -3.73005509e-01
  9.02772546e-02 -5.08029684e-02  8.83940905e-02 -4.33408707e-01
 -6.96721077e-02 -3.55067253e-02  5.35789490e-01  2.23155499e-01
 -3.31543803e-01  3.87643874e-01 -2.04079390e-01  1.53576396e-02
 -5.21636963e-01 -2.19086200e-01  4.26141545e-03  3.00112069e-01
  1.87474862e-01 -5.62856868e-02  2.39043176e-01  5.72821975e-01
 -1.41814485e-01 -2.16827929e-01 -2.77474612e-01 -2.42713749e-01
  2.68417895e-01 -2.52703577e-01 -1.62707821e-01 -2.80374795e-01
  2.78442323e-01  3.25786620e-01 -2.07682073e-01  3.88061166e-01
 -1.72105715e-01  2.07337290e-01  5.27719140e-01 -3.45397741e-01
 -4.06324506e-01  1.17846485e-03  4.97172996e-02 -3.90588850e-01
  3.90403271e-02 -1.22877255e-01  9.91610289e-02 -9.71008092e-02]"
Unable to reuse opaque_kernel variable in CudnnLSTM for FP16 ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary using GPUs
- **TensorFlow version (use command below)**: 1.7.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**:  Cuda 9.0/ CuDNN 7.0.5
- **GPU model and memory**:  Volta(V100), 16 GB
- **Exact command to reproduce**: Please see the code below

### Describe the problem
Unable to reuse opaque_kernel variables with FP16 data type. Works for FP32 data type. Here is the stack trace:

 File ""cudnn_lstm_example.py"", line 37, in <module>
    outputs_2, _ = lstm(inputs_2)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 696, in __call__
    self.build(input_shapes)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py"", line 358, in build
    ""opaque_kernel"", initializer=opaque_params_t, validate_shape=False)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1297, in get_variable
    constraint=constraint)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1093, in get_variable
    constraint=constraint)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 431, in get_variable
    return custom_getter(**custom_getter_kwargs)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py"", line 291, in _update_trainable_weights
    variable = getter(*args, **kwargs)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 408, in _true_getter
    use_resource=use_resource, constraint=constraint)
  File ""/usr/local/anaconda2/envs/tensorflow_1.7/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 758, in _get_single_variable
    found_type_str))
ValueError: Trying to share variable cudnn_rnn/cudnn_bi_lstm/opaque_kernel, but specified dtype float32 and found dtype float16_ref.

### Source code / logs

```
import tensorflow as tf

num_layers = 1
num_units = 40
batch_size = 60
dir_count = 2

inputDType= tf.float16 # Does not work
#inputDType = tf.float32 # Works

inputs_1 = tf.random_uniform([
    num_layers * dir_count, batch_size, num_units], dtype=inputDType)

inputs_2 = tf.random_uniform([
    num_layers * dir_count, batch_size, num_units], dtype=inputDType)

with tf.variable_scope(""cudnn_rnn"", reuse=False):
    lstm = tf.contrib.cudnn_rnn.CudnnLSTM(
        num_layers=num_layers,
        num_units=num_units,
        direction=""bidirectional"",
        dtype=inputDType,
        name=""cudnn_bi_lstm"")
                
    outputs_1, _ = lstm(inputs_1)

with tf.variable_scope(""cudnn_rnn"", reuse=True):
    lstm = tf.contrib.cudnn_rnn.CudnnLSTM(
        num_layers=num_layers,
        num_units=num_units,
        direction=""bidirectional"",
        dtype=inputDType,
        name=""cudnn_bi_lstm"")

    outputs_2, _ = lstm(inputs_2)

loss1 = tf.reduce_sum(outputs_1)
loss2 = tf.reduce_sum(outputs_2)
loss= loss1+loss2
var = lstm.trainable_variables[0]

grad = tf.gradients(loss, var)[0]
print('grad.shape: %s' % grad.shape)
print('var.shape: %s' % var.shape)

opt = tf.train.AdamOptimizer()
train_op = opt.apply_gradients([(grad, lstm.trainable_variables[0])])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(outputs_1))
    print(sess.run(outputs_2))
    sess.run(train_op)
    print(sess.run(outputs_1))
    print(sess.run(outputs_2))
```
",True,"[-3.19512784e-01 -6.57504380e-01 -6.26479745e-01 -1.14441775e-01
  1.33302838e-01 -2.31589139e-01 -6.99460208e-02 -1.99705243e-01
 -2.36966103e-01 -2.51989245e-01  2.63478398e-01 -3.98861952e-02
 -2.31848031e-01 -1.20277286e-01 -2.84567028e-01  1.18053347e-01
  2.17359260e-01 -1.69128805e-01  2.26469785e-01 -2.48501867e-01
 -2.60074019e-01  1.04849830e-01 -1.79850250e-01  7.30541125e-02
  3.26582193e-01  9.04589221e-02  1.16005167e-02 -1.35801598e-01
  1.04296982e-01 -1.45419046e-01  7.36612380e-01  2.19728857e-01
 -1.19922765e-01  2.53603518e-01  2.72437274e-01  3.49854320e-01
 -1.88384056e-01 -1.42653435e-01  2.93863174e-02 -6.03200123e-02
 -1.58583879e-01  3.87011841e-02  5.38935438e-02  1.31992593e-01
  1.49582744e-01 -2.20859557e-01 -1.36026414e-03 -2.68497616e-02
 -2.10485131e-01 -4.64693189e-01  5.45595348e-01 -1.16172761e-01
 -3.77156734e-01 -7.72212669e-02  5.26858047e-02 -1.70373678e-01
  1.45692632e-01 -4.40288857e-02 -1.61741734e-01  9.24935937e-02
  1.30380630e-01 -2.10761949e-01  2.02664174e-02  3.67717668e-02
 -2.87785195e-02  1.71952784e-01  2.59754419e-01 -3.38042647e-01
  6.45291746e-01 -1.69650793e-01  6.61321133e-02  1.00884929e-01
 -9.37493518e-02 -2.74575770e-01  1.81517098e-02  2.12503150e-01
  1.00881472e-01  3.00837487e-01  2.82685697e-01  1.74545795e-02
  2.95927733e-01  1.90317750e-01  3.28852713e-01 -3.42863053e-01
 -2.02790976e-01  1.32598519e-01  4.22842205e-01  5.93483821e-02
  5.93450107e-02 -3.19775939e-01  3.56826454e-01  2.56200790e-01
 -1.33882880e-01  1.77395567e-01  1.48585707e-01  2.02088729e-02
  1.22914404e-01  7.49174580e-02 -4.59033251e-02 -1.16463624e-01
 -3.11246157e-01 -1.92581773e-01  7.09618255e-02  3.48191440e-01
 -3.06542784e-01  7.86800236e-02  9.44931358e-02  9.94670391e-02
 -2.50005901e-01 -2.34623253e-01 -3.32340933e-02 -3.57096940e-02
  1.48095220e-01  2.06448417e-02  3.79433811e-01 -1.90652460e-01
 -5.38692251e-02  1.23883620e-01 -5.25045618e-02  4.20249104e-01
 -1.49397865e-01  4.31924835e-02  2.72218347e-01 -4.04170714e-02
  5.24980605e-01  1.09904096e-01 -6.14820197e-02 -2.89436541e-02
  9.89906117e-03  5.62908053e-02 -3.50476503e-02 -6.02969043e-02
  2.29167327e-01  2.18788460e-01  6.17217086e-02 -1.70382652e-02
 -2.36041054e-01  2.00985312e-01 -6.36296421e-02 -1.90915801e-02
 -1.10799707e-01  1.61842331e-01  4.52442244e-02 -2.36722827e-01
 -1.84202828e-02  2.21539319e-01 -1.82332724e-01  3.84923778e-02
 -1.50629997e-01  4.26917821e-01 -1.90434963e-01 -1.52137980e-01
 -2.17103120e-02  1.04535118e-01 -1.27793938e-01  3.01171899e-01
  4.23141181e-01 -2.02847257e-01  1.13064222e-01 -3.48733902e-01
  2.24668570e-02  3.79849762e-01 -3.44081223e-01 -1.56125203e-01
 -4.86878343e-02  5.48798069e-02 -9.86954272e-02 -2.98955619e-01
  1.88845545e-01  2.42512301e-01 -2.26537377e-01 -1.18594527e-01
 -2.88167112e-02  1.66968271e-01  1.62447199e-01 -1.73524410e-01
  2.89653659e-01 -6.50859892e-01 -1.20191202e-01  4.13774192e-01
  7.89205432e-02 -1.77687615e-01  1.45715892e-01 -5.90182543e-02
  2.90932804e-01  1.93195015e-01  3.02955031e-01  2.11994171e-01
 -4.09092873e-01 -1.31815091e-01 -2.01623246e-01 -2.96383113e-01
  2.35353746e-02 -3.27965707e-01 -2.12002024e-01  7.94640183e-02
  2.67340124e-01  3.34501445e-01 -2.39138916e-01  2.74753571e-01
 -4.56993520e-01 -1.75377980e-01  1.30246535e-01  3.05897743e-02
  1.17454037e-01 -6.19093627e-02 -4.59116846e-01 -1.22697338e-01
 -3.88004661e-01  2.49814987e-01 -9.43991989e-02 -1.19782910e-01
 -1.17701292e-03  1.20684415e-01 -3.03913742e-01 -1.69614330e-02
  2.01797992e-01 -1.22111723e-01 -2.98128515e-01  2.23178506e-01
  3.93422246e-01 -2.51554161e-01  8.41578692e-02 -3.02147537e-01
 -1.30638242e-01 -6.35258406e-02 -3.52324724e-01  1.13067523e-01
 -3.65472361e-02  1.51341528e-01 -8.67798701e-02  1.06772393e-01
 -8.86313170e-02  3.79081592e-02  2.30010092e-01  1.01992264e-01
 -4.80769351e-02 -3.15124393e-01 -1.29465580e-01  2.94465214e-01
 -6.84415817e-01 -1.08599335e-01  1.76698510e-02  1.02812797e-01
  1.30621046e-01  1.89096346e-01 -1.96924657e-01  7.58290291e-02
 -3.01934481e-01  9.97368842e-02 -6.31232485e-02 -1.89059347e-01
  2.94581324e-01  2.14559827e-02  3.12177092e-01 -1.40432473e-02
 -4.98134643e-04  1.94199815e-01 -2.13956907e-02 -2.67803110e-02
  2.04291627e-01  1.76610962e-01 -2.03249097e-01  5.69559515e-01
  2.97369838e-01  4.91893739e-01 -1.89368978e-01  4.98093128e-01
 -7.28060231e-02 -2.93227553e-01  1.23633474e-01 -1.75096422e-01
  3.75432521e-01 -3.75561476e-01  1.07838050e-01 -1.11707792e-01
  4.38858151e-01  9.87671614e-02  6.10546879e-02  2.92585760e-01
  5.60683273e-02 -1.56174630e-01  1.79744944e-01 -9.71175171e-03
  1.26587406e-01 -2.64892608e-01 -2.79446214e-01 -2.49459535e-01
 -1.85123146e-01 -2.73029566e-01 -1.66785777e-01  9.94789004e-02
  1.70735329e-01 -3.86894941e-02 -1.67804748e-01 -3.42935696e-02
  1.11108385e-02  4.14952368e-01  1.98679000e-01  1.46846682e-01
 -1.40814856e-01 -3.20861310e-01 -1.75616499e-02  4.26271074e-02
 -1.21460095e-01 -2.46556662e-02  2.32238039e-01 -4.50069793e-02
  6.08827412e-01 -2.18553543e-01  3.14596295e-01 -1.35688577e-03
 -1.08924337e-01  1.75884306e-01  1.83095299e-02 -8.09753388e-02
 -2.93588817e-01  4.80320811e-01  4.11455572e-01 -1.00354239e-01
  3.09399609e-02 -1.61510974e-01 -5.93060404e-02  5.14297336e-02
  5.33019155e-02 -3.18024755e-01 -6.11021891e-02 -1.33207098e-01
 -1.43822819e-01  1.16120726e-01 -2.36368105e-01 -5.28362095e-02
 -1.78875178e-01 -8.88584405e-02 -9.63185728e-02  4.01743114e-01
 -3.27747703e-01  3.03248465e-01  1.50364816e-01 -1.27556026e-01
 -1.36047542e-01  6.85073435e-02 -1.48454145e-01 -2.41074875e-01
 -1.85636461e-01  2.57304478e-02  5.02407312e-01  6.07908368e-01
 -1.30418003e-01  6.73685372e-02  4.97109443e-02 -9.31479484e-02
 -2.18834102e-01 -4.69343401e-02 -1.36638597e-01  3.77541244e-01
  3.88797373e-02 -1.04948461e-01  1.43585622e-01  4.11521792e-01
 -2.21169025e-01 -1.71683550e-01 -1.65623412e-01 -2.39995822e-01
 -9.39671993e-02 -1.18642434e-01 -2.65657634e-01 -2.03592658e-01
  7.05703162e-03  4.59714949e-01 -1.41271830e-01  1.40983999e-01
 -1.88738927e-01  3.25314552e-01  5.89123011e-01 -1.56510502e-01
 -4.53677714e-01 -5.40633574e-02  6.80606291e-02 -2.11282074e-01
  9.19130538e-03 -5.56122474e-02  2.93830156e-01 -8.61770287e-02]"
tensorflow-1.8.0rc0: tf.compat.as_str returns bytes for python3 since 20180409 type:bug,"The issue appeared first in `tf-nightly==1.8.0.dev20180409` but is now present in `tensorflow==1.8.0rc0`.

Reproduce steps:

```
$ python3 -c ""import tensorflow as tf; print(tf.VERSION, type(tf.compat.as_str('hello')) == str)
```

Is expected to always print ""True"".  But gets:

```
# tensorflow
1.6.0 True
1.7.0 True
1.8.0-rc0 False           <= Broken!

# tf-nightly
1.8.0-dev20180408 True
1.8.0-dev20180409 False   <= Broken!
```",True,"[-3.50532860e-01 -7.31066823e-01 -2.91424632e-01  2.25793689e-01
 -1.31509509e-02 -4.22893822e-01 -2.69631028e-01  5.36448322e-03
 -3.34375381e-01 -1.30748004e-01 -1.50006622e-01 -1.99137241e-01
 -2.58809477e-01  2.92781621e-01 -2.11640164e-01  1.27454549e-01
 -1.53697044e-01 -3.36622655e-01  2.08141096e-02  9.42574441e-03
 -1.93158820e-01 -5.82811935e-03 -2.92687446e-01  3.79176766e-01
  1.47431284e-01 -2.85524093e-02 -8.67456570e-02 -1.88626468e-01
  1.22936964e-01  1.38005435e-01  1.55608431e-01  1.85444187e-02
 -1.96634471e-01  2.11493149e-02 -4.17024195e-02  2.88054019e-01
 -3.40631306e-01 -2.83603162e-01 -1.57113105e-01 -1.46417826e-01
 -4.64045294e-02  5.74140921e-02 -5.42130321e-02  9.42197815e-02
  1.13708396e-02  4.62903604e-02  2.13722467e-01  1.50753647e-01
 -5.16075343e-02 -2.64248103e-02 -1.01156078e-01  1.75974831e-01
 -3.03077579e-01 -4.41907078e-01 -7.75020942e-02  6.66478053e-02
  1.47778124e-01  8.72270912e-02  9.20772851e-02 -1.06760904e-01
 -3.10292095e-02 -3.57464887e-03  1.39936656e-01  1.85804904e-01
  2.01764911e-01  1.29975200e-01  2.39105821e-01 -2.44993806e-01
  3.40118885e-01 -1.82318717e-01  5.14142364e-02  1.34835958e-01
 -3.02775770e-01  1.51957691e-01 -1.87000379e-01  2.29035005e-01
 -4.66919169e-02  3.24333549e-01  1.19259126e-01 -4.82727885e-02
 -6.71513230e-02 -9.40854549e-02  3.26505661e-01 -1.04339883e-01
  2.06218511e-01 -2.64869705e-02  3.07406873e-01  5.50152361e-03
  3.52031887e-01  4.03860286e-02  4.44041491e-01  1.05275065e-01
 -1.96936339e-01  3.88163514e-02  2.22194642e-01 -1.29881734e-03
  1.26163155e-01  1.90708101e-01  6.20395169e-02 -1.38353556e-01
 -1.74576253e-01 -4.39827815e-02 -9.84974429e-02  7.99631178e-02
  2.09781498e-01  2.09773913e-01  1.69538870e-01 -1.03762902e-01
  2.76453018e-01  2.80754883e-02  1.96025565e-01 -1.81215078e-01
  2.88463563e-01 -5.08240312e-02  7.26503581e-02  1.78935766e-01
 -2.46611327e-01  9.41519067e-02  1.41543403e-01  7.24229097e-01
 -2.51620859e-01 -1.85019553e-01  9.48472917e-02 -2.73532942e-02
  1.02607116e-01 -2.87858360e-02 -7.38916025e-02 -3.87510285e-02
  5.88745028e-02 -1.44801751e-01  2.15886295e-01  2.17171386e-02
 -2.97806889e-01  9.16817784e-02  2.04926252e-01 -6.96030110e-02
 -3.18938464e-01 -6.33863449e-01 -2.64048994e-01 -3.86859834e-01
  1.13667920e-04  1.16880871e-01 -3.32070410e-01 -4.14869547e-01
  2.00451910e-01  1.41599655e-01 -2.47914299e-01  4.83338118e-01
 -8.88081640e-02 -1.90336958e-01 -1.02469027e-01  1.48709729e-01
 -2.44941294e-01  3.88355285e-01  1.22225136e-01  3.10690045e-01
  4.27219331e-01 -2.69754767e-01  3.19618993e-02 -5.49922228e-01
 -5.66767268e-02  4.33538556e-01 -1.77112162e-01 -9.52452868e-02
  1.89382777e-01  1.54402107e-01 -3.37438881e-01 -7.87814856e-02
 -5.80523629e-03  1.32200032e-01  9.65558514e-02  2.80922595e-02
 -1.40970320e-01  9.09424126e-02  4.78352427e-01 -8.02936703e-02
  1.44975692e-01 -7.05696464e-01  4.50247228e-02  2.01113746e-01
  9.43961367e-02  1.46174103e-01  1.02797709e-01 -6.68558851e-02
 -4.56762090e-02 -1.48923099e-01  5.11374958e-02  2.56452322e-01
  2.07075104e-02  1.78839475e-01 -1.47390245e-02 -1.20544806e-01
  2.03177035e-01  6.50598202e-03 -3.05150859e-02  2.16983855e-02
  2.70755917e-01  1.17431954e-01  1.02583960e-01  1.13537982e-01
  3.11865099e-02 -6.16557524e-02 -7.41314590e-02  1.36597514e-01
  6.23006746e-02 -5.54952502e-01  2.51574293e-02 -2.13333532e-01
 -1.42123967e-01  4.99897860e-02  2.66867071e-01 -4.49290276e-01
 -2.68768780e-02 -2.27407426e-01 -2.25139230e-01  6.05108999e-02
  2.47270599e-01 -1.17178872e-01 -1.39515191e-01  1.03161484e-01
  5.73332831e-02 -9.85541046e-02  4.07949463e-02 -2.60277361e-01
 -9.83859599e-02 -6.62700459e-02 -2.20395416e-01  1.01227291e-01
 -2.25476563e-01  1.48528799e-01  4.95475009e-02  2.55650908e-01
  3.22902381e-01  3.28040197e-02  2.59462953e-01 -1.16622135e-01
 -1.20160334e-01 -7.97233060e-02  1.17336120e-02  6.29579276e-02
 -1.27853185e-01  4.02059406e-02 -3.61256190e-02 -3.57535958e-01
  7.14140981e-02  7.27088451e-02  7.44215958e-03 -1.36732906e-01
 -2.87862688e-01  3.78135741e-01 -1.43058896e-01 -1.15853578e-01
  1.37631178e-01  9.76379067e-02  4.92375255e-01  9.13749412e-02
 -9.52344313e-02  9.21670869e-02  1.68894842e-01  9.20696259e-02
  3.79914582e-01  2.57342637e-01  1.15400530e-01  5.02019227e-01
  6.38928413e-02  2.05971420e-01 -7.82948732e-02  2.89157927e-01
 -6.14596605e-02  2.31688544e-02 -9.20688435e-02  2.59112455e-02
  5.48375845e-01 -3.73229504e-01  2.18505353e-01 -2.94090867e-01
  3.76406431e-01  1.85438514e-01 -1.83357596e-01  1.36094496e-01
  1.27351433e-01  2.86244690e-01 -3.64297271e-01  2.88589764e-02
 -4.32883874e-02 -9.03337449e-02  9.04562175e-02 -2.74490058e-01
 -9.78346169e-02  1.65922672e-01 -1.02569461e-01  9.37542766e-02
  3.91048670e-01 -6.68562949e-02 -7.23358989e-02  7.27388337e-02
 -5.76215833e-02  9.08680111e-02  8.53964686e-02 -1.57176778e-02
 -3.14958155e-01  1.71431936e-02  2.84827888e-01 -9.81360525e-02
  1.66177541e-01  7.85282031e-02  2.95755982e-01  4.40228060e-02
  2.18739361e-01 -4.48082328e-01  4.98719811e-01 -1.73710540e-01
 -2.90481627e-01  3.45741838e-01 -1.09363526e-01  9.00085121e-02
 -1.99875712e-01  6.53221011e-01  4.16123748e-01  2.83429809e-02
 -1.65789485e-01 -3.83798361e-01 -4.99435484e-01  1.61979161e-02
  1.24970533e-01  8.27183276e-02 -4.40564379e-02 -3.48786801e-01
 -1.98553413e-01 -4.65901084e-02 -3.05001706e-01 -3.31239700e-01
 -1.29008800e-01  8.19891766e-02  1.13136843e-01 -7.83264339e-02
 -5.31704783e-01 -3.53393890e-02 -1.40370708e-02 -4.54559773e-01
  8.99820253e-02 -3.02000672e-01  1.36430664e-02 -3.48130673e-01
 -4.26701568e-02 -1.04766935e-01  3.39845836e-01  6.90168858e-01
 -2.15724334e-01 -1.55191049e-01  1.20642528e-01  2.52873421e-01
 -1.42897204e-01 -3.74766290e-02  1.38525829e-01  1.39289290e-01
 -7.48475790e-02 -6.82846010e-02  1.36552095e-01  3.14412266e-01
 -2.79178381e-01  3.34014976e-03 -2.40451306e-01 -1.04921974e-01
  1.71598911e-01 -2.47854114e-01 -4.11486983e-01  1.80545092e-01
  2.19939053e-01  2.61337399e-01 -2.74175592e-02  3.23564351e-01
 -3.39146495e-01  1.39571130e-01  4.84769166e-01 -5.04648149e-01
 -4.12517250e-01  1.47879303e-01  1.12752974e-01 -2.52031952e-01
 -3.95686924e-02  1.19176470e-01  3.33167017e-01 -9.01115984e-02]"
Segfault on bad input to tf.constant. ,"One can get a segfault providing some wrong data into a `tf.constant`:

```python
>> tf.constant(tf.string, ""[,]"")
Segmentation fault (core dumped)
```

While `tf.constant(tf.string, ""[]"")` and `tf.constant(tf.string, "","")` both give the proper error (`dtype not understood`).

",True,"[-4.08409871e-02 -7.14691043e-01 -2.23403230e-01 -2.38167524e-01
 -2.55709440e-01 -3.57649215e-02  2.66428351e-01  1.89872727e-01
 -3.56658846e-01 -2.33841479e-01 -3.43547463e-01  3.27738941e-01
  2.86201745e-01  2.92539626e-01 -4.87702280e-01  2.83602238e-01
 -2.63104606e-02 -2.43634537e-01  2.65128110e-02 -1.30640730e-01
  5.04945107e-02  2.07232133e-01 -1.00348368e-01  5.12915730e-01
 -4.06864673e-01  7.78006315e-02  2.11277738e-01 -4.53191176e-02
  4.21077237e-02  1.75415531e-01 -1.12640314e-01  2.20369488e-01
 -2.40231186e-01  1.23512983e-01 -2.16306046e-01  1.48814425e-01
 -9.42422375e-02  4.38908190e-02 -1.94802597e-01 -8.20387825e-02
  1.07193522e-01 -7.92926103e-02  1.36928007e-01  1.30719215e-01
 -9.80937034e-02 -8.41770992e-02 -1.84638783e-01  1.31252989e-01
 -3.37149382e-01  2.12538838e-01 -3.33502531e-01  2.43825331e-01
 -1.63996905e-01 -5.34873791e-02  4.92532670e-01  1.72924139e-02
 -1.36153594e-01  3.38258892e-02  1.68410227e-01 -4.55066822e-02
 -5.02089644e-03  3.15104164e-02  3.00839096e-01  6.56542107e-02
  1.54626429e-01  3.22851129e-02  2.80410379e-01  2.42943168e-01
  3.92512560e-01  5.06529398e-02  9.45661440e-02  2.37821102e-01
 -2.38903105e-01  7.98404515e-02 -1.24822401e-01  1.01439774e-01
 -4.14873809e-01  1.99750990e-01  1.17675446e-01 -2.49692634e-01
 -1.76246520e-02  4.03672159e-02  1.34053707e-01 -4.34460104e-01
  2.69110858e-01  2.10947115e-02  3.04700643e-01 -2.58596450e-01
  2.27988198e-01  3.91939521e-01  3.94873053e-01 -2.47193083e-01
 -3.35519165e-01  1.80716112e-01 -2.01064516e-02 -3.23287286e-02
  4.87935804e-02 -3.39573443e-01 -7.97792673e-02 -8.05045217e-02
 -2.68807024e-01 -1.25543430e-01 -1.24447398e-01  3.69035989e-01
  1.56309709e-01 -1.54255703e-01 -1.25660777e-01  1.75076991e-01
 -1.26235768e-01  1.04770809e-01  1.87814191e-01  4.32195403e-02
  1.02110341e-01 -2.62291998e-01 -1.07430905e-01  2.30682582e-01
  1.91687830e-02 -4.07231376e-02  1.11881413e-01  4.11812216e-01
  1.09361693e-01  4.68521826e-02 -1.14708887e-02  2.64826328e-01
  2.57626951e-01  2.40840614e-01 -3.20313215e-01 -1.56843290e-01
  1.96951643e-01 -2.69681662e-01  4.35154326e-02 -2.60440893e-02
 -6.29819274e-01  1.54821038e-01 -9.54327285e-02  2.48109937e-01
 -4.70366716e-01 -4.82802428e-02 -2.80054301e-01  4.96178903e-02
  4.67591191e-04 -1.76076934e-01  5.11493860e-03 -1.99718475e-01
  1.54637009e-01 -1.55219823e-01 -3.42136085e-01  8.74763131e-02
 -1.99963182e-01 -3.49714025e-03 -1.45991698e-01 -1.95606835e-02
 -3.22404429e-02 -5.35626933e-02 -3.10048431e-01  1.86075851e-01
  2.87928700e-01 -2.88114876e-01  1.78291216e-01 -7.32350275e-02
  8.30643158e-03  3.88583094e-01  3.22485507e-01 -1.19212747e-01
  1.24329329e-01  3.31177086e-01 -4.27765921e-02 -5.25245704e-02
 -9.61597934e-02 -8.29897970e-02  1.11970492e-03 -1.81038585e-02
 -5.91636300e-02 -2.43218958e-01 -2.05899533e-02 -2.23306164e-01
 -1.40462354e-01 -4.18374062e-01  2.10093573e-01  1.43292546e-01
  2.56379157e-01  2.08927929e-01 -4.52771664e-01  1.77062556e-01
 -1.02010034e-01 -4.62945312e-01  8.74206424e-02  2.07038164e-01
 -1.73322722e-01 -2.06054673e-01 -3.89825478e-02 -5.33396900e-01
  3.06164641e-02 -3.40053439e-02  1.36167109e-01 -1.89200893e-01
  1.76702514e-01 -4.23692688e-02  4.63934869e-01 -4.51180413e-02
  1.41188487e-01 -1.18172817e-01  1.35405123e-01 -1.07649334e-01
 -4.89852503e-02  1.09921135e-01 -4.12231714e-01 -2.88631529e-01
  2.25073934e-01  1.17973737e-01  3.12217586e-02 -1.34623334e-01
 -1.83059290e-01 -1.55603126e-01 -2.57798284e-01  2.92976592e-02
 -9.66802686e-02 -1.34488702e-01  1.51920959e-01 -3.72746848e-02
  4.52180803e-01  1.38303572e-02  2.26777449e-01 -1.30439326e-01
  1.34723201e-01  3.03981304e-01  2.00842887e-01  4.20678854e-01
 -7.10212737e-02  6.60464466e-02  8.42069611e-02 -1.65848270e-01
  9.60293785e-02 -2.05060944e-01  4.74241748e-02  2.35868976e-01
 -3.03037763e-01  3.88301648e-02 -1.21205680e-01 -2.34389782e-01
 -2.09859863e-01  1.25033274e-01  1.39688537e-01 -3.13769788e-01
  1.90565392e-01 -3.96159917e-01  1.03221752e-01  5.78755215e-02
 -2.32721642e-01  1.18545696e-01 -2.97937036e-01  2.13012114e-01
 -2.64492840e-01  1.06307194e-01  4.41549271e-01  1.09348230e-01
  1.71406850e-01 -8.56729671e-02  8.35050121e-02 -4.50545341e-01
 -2.66923979e-02  3.73091787e-01  2.33109355e-01  3.82661849e-01
 -8.57129321e-02  3.35541040e-01  5.14751188e-02 -8.27634931e-02
  7.11967126e-02 -9.86058488e-02 -9.90109667e-02  1.16116695e-01
  8.57514679e-01 -2.01289684e-01  3.77238184e-01 -2.46694207e-01
  2.32157871e-01  1.49073288e-01 -1.01743728e-01  2.59055734e-01
 -1.03495680e-01  1.67065367e-01 -8.07289109e-02  3.58497947e-01
  2.13757887e-01  3.90667580e-02 -2.36205116e-01 -2.57786930e-01
 -5.88788129e-02  2.80770729e-03  3.18694487e-02 -2.31108427e-01
  3.18074614e-01 -1.77260190e-01  5.16198203e-03  1.28614113e-01
 -2.14548066e-01 -1.52582498e-02 -7.33412728e-02 -2.70662040e-01
  6.25909120e-02  2.23827604e-02  2.12444410e-01  3.12494069e-01
  2.76742756e-01 -9.15150810e-03 -5.25166392e-02  2.16788411e-01
 -2.16005772e-01 -1.97421700e-01  2.37661809e-01  3.63526642e-02
 -7.37566501e-02  1.67756617e-01 -3.66010576e-01  3.24253082e-01
 -7.46751800e-02 -6.33930191e-02 -9.47352648e-02  1.93510637e-01
 -3.41238201e-01 -5.78134298e-01 -4.23891217e-01 -1.41296819e-01
  2.51152873e-01 -1.09253131e-01 -3.17198224e-02 -1.67710647e-01
 -7.41852894e-02 -2.40828112e-01  1.74791977e-01  8.75776038e-02
 -2.97823817e-01 -3.10729682e-01 -1.37695074e-01 -1.03531592e-01
 -3.54323626e-01 -1.42828047e-01  1.36077479e-01  1.00586385e-01
 -9.79009718e-02 -4.21282679e-01 -2.19185501e-01 -7.05574267e-03
 -1.02297276e-01 -8.14529136e-02  4.38683331e-02  4.82016534e-01
 -2.87356228e-02 -6.09451579e-03  1.17516950e-01  2.89895087e-02
  2.23525986e-01 -2.90125348e-02 -3.35746378e-01  2.39745900e-01
  1.96489602e-01 -1.75583980e-03  1.07810840e-01  5.46825349e-01
 -1.45236239e-01 -2.35807672e-01  6.52519688e-02 -1.04783498e-01
 -1.72532603e-01 -9.47893411e-02 -4.09674533e-02  1.88808471e-01
  1.89174369e-01  1.46437541e-01  3.07137787e-01  3.74541253e-01
 -1.53479204e-01  8.18356052e-02  1.52393728e-01 -1.81654364e-01
 -6.61466122e-02  2.15771925e-02  2.88600028e-01 -2.05904558e-01
 -3.26254278e-01 -9.35533941e-02  3.55303317e-01  2.29717255e-01]"
tf.nn.conv3d_transpose operation with data_format='NCDHW' ,"### System information
Irrelevant.

### Describe the problem
In implementation of ""tf.nn.conv3d_transpose"" operation with ""data_format='NCDHW'"",
the shape compatibility of ""output shape"" and ""kernel size"" check is not carried out correctly. (However, the check is fine in tf.nn.conv2d_transpose)


### Source code / logs
`    if isinstance(output_shape, (list, np.ndarray)):
      # output_shape's shape should be == [5] if reached this point.
      if not filter.get_shape()[3].is_compatible_with(output_shape[4]):
        raise ValueError(
            ""output_shape does not match filter's output channels, ""
            ""{} != {}"".format(output_shape[4],
                              filter.get_shape()[3]))`

should be changed to 

`    if isinstance(output_shape, (list, np.ndarray)):
      # output_shape's shape should be == [5] if reached this point.
      if not filter.get_shape()[3].is_compatible_with(output_shape[axis]):
        raise ValueError(
            ""output_shape does not match filter's output channels, ""
            ""{} != {}"".format(output_shape[axis],
                              filter.get_shape()[3]))`

more specifically, output_shape[4] to output_shape[axis].",True,"[-2.82629251e-01 -1.30375192e-01 -7.26351067e-02 -1.80698901e-01
  1.69107188e-02 -1.90271437e-01  1.43594086e-01  1.52832866e-01
 -5.55544317e-01  2.05544941e-03 -2.80357838e-01  1.36040837e-01
 -5.35778254e-02 -4.80956510e-02 -3.72375369e-01  3.09655905e-01
 -2.38514036e-01 -1.48747712e-01  5.51148951e-02 -3.30598682e-01
 -1.30495697e-01 -5.36446385e-02  1.36489784e-02  2.04964578e-01
  4.89274040e-02 -3.07441819e-02  1.67722821e-01 -5.85207529e-02
 -1.65898334e-02 -3.09910551e-02 -2.10108817e-01 -1.29099637e-01
 -6.11230135e-01  1.58438087e-01 -2.53921412e-02 -2.31931619e-02
 -2.84283459e-01  5.36832102e-02 -2.14616567e-01 -1.14800170e-01
  6.78432584e-02 -2.08651111e-01 -2.69645713e-02 -9.91808921e-02
  2.36134138e-03 -5.10292612e-02 -2.87594765e-01  1.24213167e-01
 -4.26572680e-01  1.49677277e-01  2.98909217e-01  4.36510742e-02
 -3.00977468e-01 -1.57456696e-01  4.45732176e-01  4.96478155e-02
  1.21354841e-01 -7.43139163e-03  2.51276940e-01  4.12713327e-02
  1.99479952e-01 -1.35358572e-01 -1.16914429e-01 -1.59061372e-01
  3.39606643e-01 -1.10984996e-01  1.15257144e-01  3.19256842e-01
  6.73503894e-03  9.56784040e-02  1.15648389e-01  4.32149991e-02
 -5.82057387e-02 -4.24157791e-02  9.44537520e-02  4.23079729e-02
 -2.17910334e-01  3.66435230e-01 -5.65080494e-02 -2.66202867e-01
  5.06910961e-03  4.13967788e-01  1.96545690e-01 -1.53721124e-01
  7.10331351e-02  1.71518978e-02  1.21235982e-01 -7.91524053e-02
  4.30579722e-01 -1.86840910e-02  1.90759748e-01  2.77082250e-03
 -2.69845068e-01  1.33300692e-01  1.26404062e-01  1.56559736e-01
 -2.12002639e-02  9.49570760e-02  1.44590080e-01 -1.88319206e-01
 -2.11384803e-01 -9.53925923e-02 -1.63931891e-01  2.42427588e-02
 -7.63986856e-02  4.32192869e-02  9.88987535e-02  2.92526841e-01
  1.03653215e-01 -9.50903371e-02  1.48838162e-01 -7.95039237e-02
  7.53186047e-02  1.84693411e-01  8.86507854e-02  2.28456780e-01
  2.33628109e-01 -5.45662269e-02 -1.03272066e-01 -1.57391265e-01
  6.40583187e-02 -1.29826516e-01 -6.38948567e-03  1.82044625e-01
  2.39880681e-01 -1.85796052e-01 -2.28379473e-01  1.89978033e-02
  7.02388138e-02  9.93696973e-03  3.44106480e-02  1.74633786e-01
  2.44383454e-01  6.50197193e-02 -1.00367188e-01  4.89062443e-02
 -3.68810624e-01  1.53841943e-01 -4.24630344e-01 -1.76721841e-01
 -1.90807283e-01 -8.27126354e-02 -1.25804588e-01 -2.62862504e-01
  2.76422441e-01  4.53925207e-02 -6.71333015e-01  3.34440410e-01
  8.32473114e-02 -2.40234677e-02  1.82797521e-01 -2.70333458e-02
 -2.68755853e-01  1.75912127e-01 -2.86973357e-01  8.81609172e-02
  1.79017872e-01 -2.23519087e-01  1.51506752e-01 -3.35853577e-01
  1.49346903e-01  1.05215117e-01  5.43092564e-02 -1.95855454e-01
  2.35863179e-01  1.25654280e-01 -3.44818145e-01 -7.28260279e-02
  5.60505912e-02 -7.40250051e-02  1.33947492e-01 -2.43467540e-01
 -1.56354904e-01  2.00668007e-01  3.71572286e-01 -1.47027701e-01
 -7.33343884e-02 -3.89390975e-01 -2.63104141e-02  2.75088251e-01
  2.93328643e-01 -1.39805555e-01  1.49991840e-01 -1.65803611e-01
  6.05047829e-02 -2.03774109e-01  3.78231406e-01  3.25914353e-01
  2.41762269e-02  8.52995068e-02 -1.13735735e-01 -3.11654687e-01
 -7.99673721e-02  2.51372904e-02  1.80129856e-01 -1.69436932e-01
  1.95335746e-01 -2.73028284e-01  1.15814686e-01  1.76311821e-01
  2.09163636e-01 -2.33324803e-03  2.22901344e-01 -5.92659526e-02
  1.00521177e-01 -1.49357840e-01 -1.57344371e-01 -5.28746307e-01
 -1.46325320e-01  1.02206379e-01 -1.54893458e-01 -7.45894313e-01
 -8.40120390e-03 -2.89130926e-01 -2.38792747e-01 -5.90417050e-02
 -1.36395380e-01 -1.07264996e-01 -2.03916579e-01  1.32337138e-01
  2.02442974e-01 -9.32776555e-02  2.16066152e-01 -2.62534291e-01
  8.05060714e-02  2.23039418e-01 -1.03039220e-01 -1.24587320e-01
 -9.32526365e-02  1.95756540e-01  2.89639145e-01 -4.50345010e-01
  4.99145091e-02 -6.03861026e-02  2.91603684e-01  4.23696153e-02
 -2.25416526e-01 -1.92142099e-01 -1.28681302e-01  1.17741767e-02
 -4.11929369e-01  1.33018028e-02 -2.54592627e-01 -2.74739385e-01
  3.28100204e-01  1.95991248e-02 -3.44718471e-02  1.96051940e-01
 -1.16311230e-01  2.94312060e-01 -2.27634445e-01  5.50522730e-02
 -4.20973264e-03  8.14873576e-02  3.11278403e-01  5.89654818e-02
  4.52032030e-01 -1.61452219e-04  1.01497285e-01 -6.56232163e-02
  4.05432910e-01  3.84847999e-01  6.38299510e-02  4.88984764e-01
  1.48326308e-01  1.95275560e-01 -2.05082655e-01  1.22468889e-01
 -2.16796353e-01 -3.57492827e-02 -1.21562593e-02 -2.59726524e-01
  4.78675365e-01 -8.19475651e-02  4.12709802e-01 -2.01512456e-01
  5.30276060e-01 -5.91155048e-03  2.34109893e-01  3.66214335e-01
 -4.44881469e-02  3.61847520e-01  9.29022133e-02  3.95424902e-01
  1.34150404e-02  1.12870008e-01 -9.55916345e-02 -4.73797143e-01
  4.67785336e-02 -2.39174649e-01 -1.01821907e-02  7.57550225e-02
  5.74285015e-02 -1.70718879e-01 -1.20691076e-01  2.62924075e-01
  1.11752331e-01  1.13973446e-01  1.73611432e-01 -2.86547899e-01
  4.38898355e-02  1.60673946e-01  6.64762557e-02  1.84102803e-01
 -1.15505055e-01  2.54358612e-02  2.66723722e-01  1.06676668e-01
  1.75697982e-01 -2.30534077e-01  2.01168820e-01 -1.33055001e-01
 -6.13193586e-02  4.64674294e-01 -1.59498468e-01  2.44652987e-01
  7.31784105e-02  2.22373337e-01 -1.57988165e-02  1.21189676e-01
 -1.56983495e-01 -6.16097927e-01 -5.46418801e-02 -1.50959700e-01
 -3.45003121e-02 -1.47193760e-01  5.08420110e-01  5.39559647e-02
 -1.24994643e-01 -2.52379049e-02 -2.32613325e-01 -1.89911753e-01
 -9.91144180e-02  5.12408912e-02 -8.02156925e-02 -2.26124197e-01
 -2.18195021e-02  2.70006597e-01  1.56225478e-02 -1.03967533e-01
 -3.51888835e-01 -2.67709732e-01 -3.93631130e-01  2.79724479e-01
 -9.13561583e-02 -2.84836680e-01  3.03715356e-02  4.02292430e-01
  1.03157982e-01  3.26904133e-02  1.18031234e-01  1.16704218e-01
 -4.93288320e-03  1.62378192e-01 -5.39340042e-02  3.96362662e-01
 -8.90517756e-02  2.42167294e-01 -1.39912650e-01  3.18671376e-01
 -2.16791734e-01  3.24760139e-01 -2.01779366e-01 -1.01845358e-02
  3.49036753e-02 -3.59877273e-02 -1.74012810e-01 -8.74471739e-02
  1.50714189e-01  2.39159986e-02  5.01225479e-02  3.73746514e-01
 -6.77432045e-02 -1.56667233e-02  2.25066930e-01 -3.32366377e-01
  7.00756162e-02  1.51431188e-01  1.41418427e-01 -9.29754153e-02
 -2.42736340e-01  1.61492378e-01  2.75853157e-01 -1.45774126e-01]"
Luong attention fails when used with scale=True and dtype=tf.float16 ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: b'v1.6.0-rc1-1857-g67e2efa' 1.6.0
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: not relevant
- **Exact command to reproduce**:

```
import tensorflow as tf
dtype = tf.float16

with tf.variable_scope(""name"", dtype=dtype):
    cell = tf.nn.rnn_cell.LSTMCell(128)

    encoder_outputs = tf.placeholder(dtype, shape=[64, None, 256])
    input_lengths = tf.placeholder(tf.int32, shape=[64])
    tgt_lengths = tf.placeholder(tf.int32, shape=[64])
    input_vectors = tf.placeholder(dtype, shape=[64, None, 128])

    attention_mechanism = tf.contrib.seq2seq.LuongAttention(
        num_units=128,
        memory=encoder_outputs,
        scale=True,
        memory_sequence_length=input_lengths,
        probability_fn=tf.nn.softmax,
        dtype=dtype,
    )
    attn_cell = tf.contrib.seq2seq.AttentionWrapper(cell, attention_mechanism)

    helper = tf.contrib.seq2seq.TrainingHelper(
        inputs=input_vectors,
        sequence_length=tgt_lengths,
    )

    decoder = tf.contrib.seq2seq.BasicDecoder(
        cell=attn_cell,
        helper=helper,
        initial_state=attn_cell.zero_state(64, dtype),
    )

    tf.contrib.seq2seq.dynamic_decode(decoder=decoder)
```

### Describe the problem
Luong attention fails when using with scale=True and dtype=tf.float16. Changing lines 341-342 of attention_wrapper.py to:
```
g = variable_scope.get_variable(
    ""attention_g"", dtype=dtype, shape=(),
    initializer=init_ops.ones_initializer(),
)
```
seems to solve the problem.

### Source code / logs
Traceback:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-4dec9cd8e3b2> in <module>()
     31     )
     32 
---> 33     tf.contrib.seq2seq.dynamic_decode(decoder=decoder)

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in dynamic_decode(decoder, output_time_major, impute_finished, maximum_iterations, parallel_iterations, swap_memory, scope)
    307         ],
    308         parallel_iterations=parallel_iterations,
--> 309         swap_memory=swap_memory)
    310 
    311     final_outputs_ta = res[1]

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)
   3203     if loop_context.outer_context is None:
   3204       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
-> 3205     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
   3206     if maximum_iterations is not None:
   3207       return result[1]

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)
   2941       with ops.get_default_graph()._lock:  # pylint: disable=protected-access
   2942         original_body_result, exit_vars = self._BuildLoop(
-> 2943             pred, body, original_loop_vars, loop_vars, shape_invariants)
   2944     finally:
   2945       self.Exit()

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
   2878         flat_sequence=vars_for_body_with_tensor_arrays)
   2879     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
-> 2880     body_result = body(*packed_vars_for_body)
   2881     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
   2882     if not nest.is_sequence(body_result):

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/decoder.py in body(time, outputs_ta, state, inputs, finished, sequence_lengths)
    252       """"""
    253       (next_outputs, decoder_state, next_inputs,
--> 254        decoder_finished) = decoder.step(time, inputs, state)
    255       if decoder.tracks_own_finished:
    256         next_finished = decoder_finished

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/basic_decoder.py in step(self, time, inputs, state, name)
    135     """"""
    136     with ops.name_scope(name, ""BasicDecoderStep"", (time, inputs, state)):
--> 137       cell_outputs, cell_state = self._cell(inputs, state)
    138       if self._output_layer is not None:
    139         cell_outputs = self._output_layer(cell_outputs)

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/rnn_cell_impl.py in __call__(self, inputs, state, scope)
    230         setattr(self, scope_attrname, scope)
    231       with scope:
--> 232         return super(RNNCell, self).__call__(inputs, state)
    233 
    234   def _rnn_get_variable(self, getter, *args, **kwargs):

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/layers/base.py in __call__(self, inputs, *args, **kwargs)
    712 
    713         if not in_deferred_mode:
--> 714           outputs = self.call(inputs, *args, **kwargs)
    715           if outputs is None:
    716             raise ValueError('A layer\'s `call` method should return a Tensor '

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in call(self, inputs, state)
   1409       attention, alignments, next_attention_state = _compute_attention(
   1410           attention_mechanism, cell_output, previous_attention_state[i],
-> 1411           self._attention_layers[i] if self._attention_layers else None)
   1412       alignment_history = previous_alignment_history[i].write(
   1413           state.time, alignments) if self._alignment_history else ()

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _compute_attention(attention_mechanism, cell_output, attention_state, attention_layer)
   1046   """"""Computes the attention and alignments for a given attention_mechanism.""""""
   1047   alignments, next_attention_state = attention_mechanism(
-> 1048       cell_output, state=attention_state)
   1049 
   1050   # Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in __call__(self, query, state)
    427     """"""
    428     with variable_scope.variable_scope(None, ""luong_attention"", [query]):
--> 429       score = _luong_score(query, self._keys, self._scale)
    430     alignments = self._probability_fn(score, state)
    431     next_state = alignments

~/Documents/venv/lib/python3.5/site-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py in _luong_score(query, keys, scale)
    340     # Scalar used in weight scaling
    341     g = variable_scope.get_variable(
--> 342         ""attention_g"", dtype=dtype, initializer=1.)
    343     score = g * score
    344   return score

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)
   1315       partitioner=partitioner, validate_shape=validate_shape,
   1316       use_resource=use_resource, custom_getter=custom_getter,
-> 1317       constraint=constraint)
   1318 get_variable_or_local_docstring = (
   1319     """"""%s

~/Documents/venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)
   1064         if init_dtype != dtype:
   1065           raise ValueError(""Initializer type '%s' and explicit dtype '%s' ""
-> 1066                            ""don't match."" % (init_dtype, dtype))
   1067       if initializer is None:
   1068         initializer = self._initializer

ValueError: Initializer type '<dtype: 'float32'>' and explicit dtype '<dtype: 'float16'>' don't match.
```",True,"[-0.35427302 -0.32101685 -0.30222687  0.14541385  0.37931478 -0.48077807
  0.05525095  0.09767854 -0.02139824 -0.23033765  0.05732574 -0.28089178
 -0.23453721 -0.17658535 -0.29812622  0.24577309  0.07506528 -0.10053615
 -0.05531001 -0.05410264 -0.19016337 -0.42580974 -0.1172277   0.28951097
  0.15992415  0.21049747 -0.19478767  0.21961999  0.1693308   0.06305477
  0.6772007   0.12358677  0.03617934  0.11391675 -0.08586463  0.37082222
 -0.17836827 -0.21956412 -0.08764307 -0.2638722   0.01610287  0.06593886
  0.36961997  0.03320092  0.20371076  0.04181023 -0.02206495 -0.09057613
 -0.25171185 -0.34923986 -0.00356631 -0.12653252 -0.39307743 -0.19851013
 -0.19673057 -0.06187122 -0.06890348 -0.20023319  0.12056302 -0.06868578
 -0.0683629  -0.0685666  -0.15386446  0.1738196   0.08941549  0.18112284
  0.27333713 -0.24995667  0.43927953  0.16970652  0.16822478 -0.01321626
 -0.30107892  0.04224489  0.2642448   0.25781283  0.09493796  0.40875986
  0.23619625 -0.3278068   0.20277932 -0.27649933  0.3173179  -0.29737675
 -0.05899141  0.12729862  0.35622045  0.02980503  0.17127132 -0.40605506
  0.57201326  0.06449927 -0.28099436  0.09729805  0.16463068  0.07350235
  0.01926879  0.03625097 -0.32510614 -0.11526698 -0.17511976 -0.05967055
  0.02617443  0.080832    0.10358299 -0.05103619  0.27623063  0.1213955
 -0.08384179  0.19266894  0.06629436  0.02337773 -0.07715705 -0.03422798
 -0.01043833 -0.18575525 -0.23808116  0.08429475 -0.10963785  0.50307095
 -0.2220197  -0.0565501  -0.23096633 -0.20386054  0.30049598  0.13614157
  0.01017466  0.03530858 -0.28159493 -0.03153284 -0.15106809 -0.00153518
 -0.16899933 -0.11589134  0.02153085 -0.01593021 -0.22771029  0.2392596
 -0.19027245  0.07468167 -0.0832988   0.10699253  0.04640635 -0.21347755
  0.09944635 -0.03380852 -0.09573238  0.08863573 -0.04946528 -0.05692341
 -0.26307043 -0.2956816  -0.13227674  0.3754897   0.12722665  0.29209495
  0.1339266   0.03383023 -0.18345101 -0.26925504 -0.0542182   0.2647872
 -0.09091912 -0.09592688 -0.14031014  0.07254901 -0.21552375 -0.080091
  0.01632959  0.42445928 -0.20517987  0.1467282  -0.04095618  0.69023
  0.37637734 -0.33087987  0.48159704 -0.20059654 -0.2472323  -0.03391567
 -0.00801219  0.04229598  0.17021509 -0.08468284  0.04657468  0.04013275
  0.08210894  0.2813051  -0.17762011 -0.29237652 -0.2616676  -0.17148963
  0.26806974 -0.18728691 -0.3498717   0.13755894  0.1581882  -0.1821993
  0.00399334  0.29296896 -0.13656083  0.03891621 -0.08328176 -0.16793466
  0.20181954 -0.16513379 -0.20820102  0.2124965  -0.19093782  0.12244576
  0.09774598 -0.2936201  -0.20733136  0.21515465 -0.00504451 -0.00827897
  0.14744404 -0.30499327 -0.260998    0.3203063   0.18766461 -0.08983929
  0.00885262 -0.39812517 -0.18962991 -0.10282443 -0.11242354  0.2746075
  0.0642507   0.46669936  0.08244725  0.03785471  0.44557434  0.134812
 -0.02635537 -0.0435862  -0.29654232 -0.23541966 -0.13213034  0.02407717
 -0.14865649 -0.3533675   0.22715661  0.19177844  0.02031924  0.24247578
 -0.43253648  0.08302611 -0.16857202  0.01434598 -0.11914882 -0.06342071
  0.00111255  0.30081207  0.16621074 -0.00310834 -0.01605295  0.19151926
  0.14625798 -0.17271133  0.41117954  0.39244986 -0.26281697  0.40345404
  0.2911625   0.27872705 -0.35466903  0.33941346  0.0980933   0.041011
 -0.10041965 -0.16236421  0.34402966 -0.39579165 -0.08066936 -0.14648601
  0.29213092 -0.01309783 -0.22726186  0.03035907  0.42725468  0.04481989
  0.03326995 -0.11912198  0.07526483 -0.23724434 -0.05412336 -0.46502724
 -0.13950881  0.12123886  0.1214413   0.13238817 -0.17919427  0.08149662
 -0.17115173 -0.12176484 -0.11944198  0.00961032 -0.005042    0.25393516
 -0.0148076   0.05113891  0.07050394  0.0951525   0.04889062 -0.0064712
  0.1762781   0.13410652  0.09858856  0.01714428  0.0556382  -0.02850534
 -0.17632824  0.32784587  0.06220922  0.39137173 -0.1363689   0.57974124
  0.23696312  0.00600504 -0.04482902 -0.22124812 -0.07199103  0.13473861
 -0.03937618  0.10055584 -0.23390865 -0.20900252 -0.11385333  0.37423757
 -0.1986098  -0.26831642 -0.24164699  0.17633523 -0.13973492 -0.10794403
 -0.3401786   0.41774747 -0.19030294 -0.11180977  0.10118817 -0.1077298
  0.30084628 -0.57711345 -0.07872197 -0.05095402  0.15422009  0.732415
 -0.17675883  0.05163741  0.08559633  0.06016355 -0.22982785  0.20322117
 -0.18262416  0.13249075 -0.0754145  -0.06231711  0.11668696  0.4237097
  0.06780886 -0.15818     0.03962999  0.06882606  0.08895671  0.06582887
  0.02141864 -0.10515048  0.3412252   0.1793802   0.14022225  0.05635949
 -0.26322964  0.31220296  0.49432963 -0.5276091  -0.3571398   0.09967344
 -0.129741   -0.006013   -0.03668745 -0.13308428 -0.23037043  0.16991657]"
"`tf.keras.estimator._create_ordered_io` casts everything to floatx, which breaks non-floatx inputs type:bug","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 3.16.36
- **TensorFlow installed from (source or binary)**: Installed via pip
- **TensorFlow version (use command below)**: `('v1.6.0-0-gd2e24b6039', '1.6.0')`
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: Requires significant code, let me know if necessary

### Describe the problem

This is kind of a simple issue with using `Keras` models as Tensorflow Estimators. I unfortunately need to do this awkward conversion in order to use SageMaker, which is even more awkwardly behind by two versions of Tensorflow. Which is fun.

Basically, I have a `Keras` model that expects a `tf.string` input `dtype`, which is then passed through to a Lookup layer for some text embeddings. This works fine as a Keras model and works fine if I extract the input layers myself and connect them into an Estimator. However, if I go to create an estimator from the model using `model_to_estimator` I run into this code path: https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/keras/_impl/keras/estimator.py#L80

This conversion then causes the model to break further down the line. I'm not sure why this float cast occurs, but this commit https://github.com/tensorflow/tensorflow/commit/4c86ece040cb96ea689f5c0d084b6959274eab91#diff-69effda952f96b36c8015cc1a3462d65 seems to imply that Keras models are meant to only take floatx input, which doesn't really seem right.

Would not doing this cast break anything? If so, is there a way to use a non-float32 input with Keras models that need to be converted to Estimators?

Thanks!

### Source code / logs

Here's the exact traceback for the issue:

```
/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument 
of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.          
  from ._conv import register_converters as _register_converters                                                                           
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp6Wogzk                                                               
2018-03-29 14:12:41.586292: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow       
binary was not compiled to use: AVX2 FMA                                                                                                   
WARNING:tensorflow:Output ""final_representation"" missing from loss dictionary. We assume this was done on purpose, and we will not be      
expecting any data to be passed to ""final_representation"" during training.                                                                 
WARNING:tensorflow:Output ""oov_code"" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any    
data to be passed to ""oov_code"" during training.                                                                                           
Testing common_estimator_fns.py locally                                                                                                    
Making estimator                                                                                                                           
Model dir: /tmp/tmp6Wogzk                                                                                                                  
Training estimator                                                                                                                         
float64                                                                                                                                    
Tensor(""random_shuffle_queue_DequeueMany:1"", shape=(32, 1), dtype=string, device=/device:CPU:0)                                            
Traceback (most recent call last):                                                                                                         
  File ""common_estimator_fns.py"", line 423, in <module>                                                                                    
    hooks=[tf_debug.LocalCLIDebugHook()])                                                                                                  
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 352, in train 
    loss = self._train_model(input_fn, hooks, saving_listeners)                                                                            
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 812, in       
_train_model                                                                                                                               
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)                                                                            
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 793, in       
_call_model_fn                                                                                                                             
    model_fn_results = self._model_fn(features=features, **kwargs)                                                                         
  File ""common_estimator_fns.py"", line 381, in model_fn                                                                                    
    return keras_model_fn(features, labels, mode)                                                                                          
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/estimator.py"", line 160,  
in model_fn                                                                                                                                
    labels)                                                                                                                                
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/estimator.py"", line 109,  
in _clone_and_build_model                                                                                                                  
    model = models.clone_model(keras_model, input_tensors=input_tensors)                                                                   
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/models.py"", line 1557, in 
clone_model                                                                                                                                
    return _clone_functional_model(model, input_tensors=input_tensors)                                                                     
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/models.py"", line 1451, in 
_clone_functional_model                                                                                                                    
    output_tensors = topology._to_list(layer(computed_tensor, **kwargs))                                                                   
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 
258, in __call__                                                                                                                           
    output = super(Layer, self).__call__(inputs, **kwargs)                                                                                 
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 673, in __call__      
    self._assert_input_compatibility(inputs)                                                                                               
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 1204, in              
_assert_input_compatibility                                                                                                                
    ', found dtype=' + str(x.dtype))                                                                                                       
ValueError: Input 0 of layer lookedup is incompatible with the layer: expected dtype=<dtype: 'string'>, found dtype=<dtype: 'float32'>     
```

I can provide code if absolutely necessary, but it'd take some work to get to a minimal reproduction.",True,"[-1.86122596e-01 -4.96445417e-01 -3.58065665e-01 -1.15579836e-01
 -2.64083855e-02 -4.60293412e-01  2.46868744e-01 -2.31429845e-01
 -2.69582987e-01  3.50436755e-02  6.55636713e-02  1.09801590e-02
 -2.05774188e-01  3.74337472e-03 -2.47558057e-01  1.55723333e-01
 -1.44838661e-01 -2.04035014e-01  1.93528593e-01 -1.21889403e-02
  2.87168771e-01  2.79607791e-02 -3.05192113e-01  3.38077664e-01
  1.32447973e-01  3.82450968e-01 -1.31398365e-01 -5.75478375e-03
  8.73737782e-02 -2.48546630e-01  2.30797783e-01  2.01524004e-01
 -5.03824428e-02 -1.89958438e-01 -4.24551703e-02  3.90137106e-01
 -2.18893409e-01 -1.12798110e-01 -2.15571404e-01 -5.85788637e-02
  1.65584862e-01  1.22213289e-01  6.86324686e-02 -1.46112993e-01
  1.92576349e-01 -8.75884369e-02 -9.57330614e-02 -1.32515550e-01
 -5.70021421e-02 -1.53454348e-01 -1.94114536e-01  6.32720292e-02
 -1.02898255e-01 -6.02834672e-03 -2.20532686e-01 -3.31360161e-01
  5.20921461e-02 -1.45707428e-01 -4.96385582e-02  3.62106189e-02
 -9.78793353e-02  1.99054480e-01  9.48309377e-02  2.91012019e-01
  1.84214830e-01  1.78142920e-01  1.24862224e-01 -2.17180625e-01
  3.28695476e-01 -1.68356881e-01  1.28522456e-01 -7.26069286e-02
 -3.23751926e-01 -1.83637254e-02 -1.78579316e-02  1.40932322e-01
 -8.84460360e-02  1.45021170e-01  2.81224102e-01 -1.58527046e-01
 -9.80846882e-02 -2.48527415e-02  7.05079660e-02 -1.31956339e-02
 -1.02028012e-01  1.39318720e-01  1.74740702e-01  2.65174285e-02
  2.02090532e-01 -8.50796402e-02  5.45902133e-01  4.09624577e-01
 -3.37531120e-02  1.73558563e-01  4.47456479e-01  5.29784039e-02
  7.48862922e-02  1.50245696e-01 -5.39878421e-02 -1.08029656e-02
  1.09922878e-01 -1.38470650e-01 -1.74735174e-01  5.84884435e-02
  1.13015048e-01  6.53192624e-02  2.59309590e-01 -1.16246775e-01
  2.15036437e-01 -4.82624695e-02  4.12555784e-02  1.47127407e-02
  1.01223119e-01  5.69619164e-02 -1.49673790e-01  6.93119466e-02
  4.85342629e-02  4.01836727e-03 -1.54106811e-01  5.97528934e-01
  1.23574836e-02 -2.47428536e-01 -1.03747800e-01  3.99371088e-01
  4.16697025e-01  3.28092575e-01 -2.06907034e-01 -2.83344649e-03
 -1.35577917e-01  4.89155091e-02  2.26171017e-01 -3.76769006e-02
 -1.83146656e-01 -1.23938639e-02  9.53498334e-02  1.30972281e-01
 -4.22055036e-01  1.43376321e-01 -3.49117666e-01  2.27357596e-02
 -3.97345163e-02  2.18912601e-01 -7.37608075e-02 -3.58020723e-01
  2.21791774e-01  1.85873747e-01 -9.38068628e-02  2.73842160e-02
 -1.44105926e-01  2.73261309e-01 -1.98531464e-01 -1.04298830e-01
 -3.23596187e-02  3.88377398e-01 -6.58076406e-02  3.32959369e-02
  1.07078210e-01  5.18301725e-02 -9.76510122e-02 -4.71395314e-01
 -1.95921227e-01  2.22569272e-01 -2.50882089e-01 -3.81648578e-02
  1.31605834e-01  1.76373035e-01 -4.03969944e-01 -1.83140963e-01
 -6.45020604e-02  1.35995701e-01 -1.42606348e-01  6.21995628e-02
 -9.62807387e-02  2.99285442e-01  2.51053929e-01  2.35250860e-01
  4.02639478e-01 -4.21298027e-01 -4.66182344e-02  1.29208192e-01
  2.34010309e-01  7.81019181e-02  1.67764693e-01  2.22699121e-01
 -2.91111581e-02 -1.29806459e-01  1.23777188e-01  1.73567414e-01
 -1.81697816e-01 -3.16818058e-02 -2.53048360e-01  1.29755139e-02
  3.10338169e-01 -2.05752663e-02 -1.55022249e-01  1.33754477e-01
  2.60065347e-01 -2.33532101e-01 -5.70867211e-04  8.68375152e-02
 -3.95545028e-02 -2.16847226e-01 -1.24364726e-01 -2.38221698e-02
 -1.75179258e-01 -3.51190895e-01 -8.95352438e-02 -1.36691451e-01
 -2.18441322e-01  2.23080873e-01  7.49751478e-02 -4.21666265e-01
 -1.11519545e-01  6.36343956e-02 -1.34238094e-01 -1.92585781e-01
  2.06948996e-01 -2.02639878e-01 -3.59831572e-01  2.09130675e-01
  1.54857427e-01 -2.33003154e-01  4.68478538e-02 -3.12311023e-01
 -2.69729227e-01 -2.49072745e-01 -3.01051199e-01  2.07838118e-01
 -2.67718919e-02  1.55382231e-01 -5.60259186e-02  5.28541766e-02
  3.09250295e-01  9.81359705e-02  1.04545847e-01 -1.57488361e-02
 -2.80246548e-02 -2.56106198e-01 -2.73254424e-01  9.60421264e-02
 -3.68415505e-01 -2.76113778e-01  3.56162637e-02 -8.24969560e-02
 -1.48979813e-01  7.76827037e-02 -1.45285472e-01  2.28942186e-01
 -3.40826780e-01  3.08860362e-01 -1.25919253e-01  3.84396613e-02
  2.91292042e-01  1.91176683e-01  3.51582505e-02  7.44303688e-02
  1.35895506e-01  1.31230593e-01  1.88350409e-01 -2.53189467e-02
  1.24873340e-01  3.05880845e-01  4.09549773e-02  5.74756563e-01
  8.93456787e-02  2.20547795e-01 -3.03234786e-01  3.00948381e-01
 -9.90867764e-02 -1.83654308e-01 -1.44099936e-01  1.67208806e-01
  5.33011496e-01 -2.71406680e-01 -8.27615187e-02 -3.88335228e-01
  3.63869190e-01  1.24688502e-02  4.37443480e-02  5.42858876e-02
  1.52781695e-01  2.75538474e-01 -1.10508934e-01  8.10002312e-02
 -2.71451846e-02 -2.41948217e-02 -6.51985183e-02 -4.31836188e-01
 -1.40739501e-01 -1.47610500e-01 -5.32738641e-02 -1.91530753e-02
  2.19856024e-01  8.63606185e-02 -9.24463719e-02  4.48481515e-02
  1.10353231e-01  7.54603893e-02  1.22197039e-01  3.11011910e-01
  5.31114936e-02 -9.79845673e-02  1.85849071e-01  8.67989361e-02
 -2.69792024e-02  5.64062744e-02  3.15755844e-01  2.16970712e-01
  4.23567146e-01 -1.30578786e-01  1.96662501e-01 -1.91950984e-02
 -1.35761738e-01  2.87368119e-01  3.71767916e-02  3.11722793e-02
 -2.70454228e-01  4.72133309e-01  3.63651197e-04 -4.88948375e-02
  2.04734020e-02 -3.28423381e-01 -1.90211132e-01  1.74393058e-01
 -6.01580180e-03 -2.19905764e-01  9.91104171e-03 -9.16558206e-02
 -1.94376916e-01 -9.45271999e-02 -9.77380425e-02 -2.56385535e-01
 -1.83870643e-01  4.31808345e-02 -1.54660836e-01 -2.88126856e-01
 -1.52219117e-01  3.81773949e-01 -1.43416584e-01 -2.90964842e-01
  1.56911463e-02 -2.61858881e-01 -6.92195911e-03 -3.90628278e-01
  5.48046790e-02 -3.75373840e-01  2.80751973e-01  3.14970285e-01
  3.05359028e-02  1.33500263e-01 -3.38236522e-03 -1.10630086e-02
 -1.45259857e-01  1.81361139e-01 -7.92271830e-03  1.15476519e-01
 -1.90096587e-01  3.51724774e-02  2.01515518e-02  4.59170938e-01
 -3.82212996e-01  7.87704960e-02 -2.99690157e-01 -3.17560852e-01
  7.80081600e-02 -1.12299822e-01 -1.52079314e-01  1.41902063e-02
  4.49055612e-01  2.88925320e-01 -7.40303099e-02  3.37239802e-01
 -1.98396981e-01  4.41791534e-01  4.96940792e-01 -2.10363001e-01
 -1.05682299e-01  1.63115114e-01  2.57520527e-01 -1.26565695e-01
  2.96726972e-02 -1.08342007e-01 -1.59823954e-01 -4.70030122e-02]"
tf.gfile doesn't understand NTFS Unicode filenames on Windows ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows Server 2016 Datacenter, 64-bit, installed on a GCE VM
- **TensorFlow installed from (source or binary)**: binary pip package
- **TensorFlow version (use command below)**: 1.6.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: `python -c ""dir = '/tmp/tftest/\u8c37\u6b4c'; import tensorflow as tf; import os; os.makedirs(dir); print(os.path.exists(dir), tf.gfile.Exists(dir))""`

### Describe the problem
The tf.gfile functions that involve file paths appear not to understand Windows with NTFS filenames that contain Unicode characters, such as  (""Google"" in Chinese).  It seems like something about the name isn't translated in a way that actually results in successful lookups, so everything behaves as if those files didn't exist.

Command to repro:
```
python -c ""dir = '/tmp/tftest/\u8c37\u6b4c'; import tensorflow as tf; import os; os.makedirs(dir); print(os.path.exists(dir), tf.gfile.Exists(dir))""
```

This command run in Windows PowerShell prints ""True False"" i.e. `os.path.exists()` returns a different result than `tf.gfile.Exists()`.  This is true even for different ways of formulating that path -
 in addition to `/tmp/tftest/\u8c37\u6b4c` I tested `C:/tmp/tftest/\u8c37\u6b4c` and `C:\\tmp\\tftest\\\u8c37\u6b4c` (double backslashes for the path separators to escape within a python string) and they both produce the same result.  If you remove the unicode characters from the path, it prints ""True True"" as expected.

In contrast, on my gLinux workstation this prints ""True True"" all the time, i.e. the results are the same.

I tested this with Exists(), IsDirectory(), and ListDirectory() but I'm assuming it applies generally to all the tf.gfile functions that take a path argument.  Note however that ListDirectory() will *return* the Unicode name just fine if run in the parent directory, the same as `os.listdir()`.

We've gotten a report about this for TensorBoard: https://github.com/tensorflow/tensorboard/issues/861
",True,"[-0.18632464 -0.60736513 -0.43896994 -0.309836   -0.06667832 -0.1229389
 -0.10274005  0.04684225 -0.07247143 -0.23532352 -0.10543422 -0.30363086
 -0.05241254 -0.22241294 -0.37285194 -0.06330176 -0.12655142 -0.1563162
  0.27827168 -0.1786787  -0.1105928   0.08161259 -0.11747844  0.15999034
  0.26264128  0.21832706  0.05579596 -0.06650452 -0.03168155  0.03235919
  0.25188962  0.06972753 -0.08440666  0.04742464  0.35791904  0.1721488
  0.04102774  0.00424527 -0.36304158 -0.25698793 -0.05020605 -0.0668648
 -0.00273084  0.37437627 -0.202932    0.0955035   0.13466537 -0.01321661
 -0.5053194   0.18306908 -0.05040122  0.08733787 -0.33610362 -0.19961733
  0.23076038 -0.26262242 -0.06424794  0.12141786  0.08768474  0.20509921
 -0.12689169 -0.2010923  -0.00999759  0.18449491  0.13205555  0.3126801
  0.19130707 -0.22586845  0.22559705 -0.3407858  -0.1163983  -0.03240639
 -0.1561861   0.04526749  0.08412081  0.26095796 -0.02045557  0.33001283
 -0.0042259  -0.25830513  0.11307017  0.0550461   0.32284182 -0.07305315
 -0.03730539  0.08287086  0.13532245 -0.26849127  0.17572111  0.04760026
  0.39520952 -0.04068821  0.18417142  0.28436857  0.19013882 -0.04674997
 -0.0245104   0.37245673 -0.10598838 -0.07149484 -0.19869319 -0.08215435
 -0.11623618  0.2069214   0.01731713 -0.29276448  0.04802036  0.1777313
 -0.01940043 -0.19866604  0.04219018  0.08090658 -0.12783872 -0.33201277
 -0.2215663  -0.10861453 -0.02867329  0.15912944  0.04337927  0.29335898
 -0.04930634 -0.36686504 -0.18028417  0.01007223 -0.03614035  0.12515394
 -0.39100355 -0.01352499  0.10019548  0.2497701   0.18261811  0.00877673
  0.1619094  -0.03645808  0.14149833 -0.07483729 -0.5616288   0.10777293
 -0.4191648  -0.02508346 -0.13837749  0.0298628   0.08134723 -0.06231079
 -0.19016883 -0.09157145 -0.13899742  0.31897688  0.1093552   0.51967955
  0.25152433  0.12032908 -0.01282711  0.32823497 -0.01070266 -0.09213126
  0.24094877 -0.18908529 -0.20697036 -0.31714484 -0.02837997  0.13055006
  0.03207009 -0.12625808  0.1574437   0.03348878 -0.22239216  0.12741229
  0.14817029 -0.06654005 -0.00957182  0.27769274  0.00685898  0.3377726
  0.49625492 -0.25596792  0.4717605  -0.27631918 -0.05641317  0.2788936
  0.00197618  0.03323507  0.21743041 -0.08904076 -0.2497494   0.16192387
  0.2882343   0.23574781 -0.16087335 -0.10952118  0.25153708 -0.07469535
  0.13287535 -0.217069   -0.20344576 -0.30485207  0.213061    0.01977251
  0.02218893  0.42261267  0.0455206  -0.24367853  0.22073527  0.00180482
  0.07205196 -0.02256193 -0.40065396  0.03564388 -0.43956435  0.2482527
  0.04514585 -0.03285404 -0.16299537  0.13028422 -0.23663673 -0.16464171
 -0.0708474  -0.4461884  -0.05807487  0.11715546  0.05868898 -0.10331586
 -0.10532007 -0.33214182  0.10433325 -0.1322478  -0.20984785  0.26499242
 -0.13426952  0.07843968 -0.04698804  0.20080522  0.12201537  0.03043876
  0.40603155 -0.07958042  0.07845874 -0.4016583  -0.23171037  0.1958786
 -0.26272133 -0.08939695 -0.18303326 -0.29693907 -0.00510554  0.10738161
 -0.35311675  0.17057142 -0.05387     0.08615912 -0.09489357  0.06010634
  0.09545782  0.30110595  0.2979552   0.246026   -0.11716662 -0.10144714
  0.035738    0.15676531  0.19585076  0.21985139  0.05083394  0.41345719
  0.14344817  0.33934826  0.00512461  0.4226666  -0.32505322 -0.03253908
 -0.14638495 -0.02758575  0.30296054 -0.18530992  0.310408    0.09038421
  0.32820028 -0.1856524  -0.107637    0.19959849 -0.05669885  0.25717127
 -0.1721282   0.3149137   0.09590785 -0.07938286  0.02683032 -0.53879446
 -0.1622863  -0.12065376  0.07135127 -0.15063532  0.07819523  0.17851219
 -0.02801964 -0.30052036 -0.24195771  0.11620484  0.14378181  0.01253842
 -0.09263629  0.05952039  0.06372018  0.2842929  -0.08633268 -0.05566033
  0.30141303  0.1992999   0.578851   -0.16967703  0.27379632  0.24570192
 -0.10210858  0.24295218  0.04298408  0.00078974 -0.3672079   0.60461855
  0.07765166  0.09925398  0.00426167 -0.4291624  -0.46518064  0.00910272
  0.04897594  0.2022857  -0.16320097 -0.1056558  -0.01022324  0.20882732
  0.02777862 -0.3184747  -0.68147916 -0.10012532 -0.15134415  0.01593243
 -0.14769179  0.46182573 -0.10333449 -0.11627612 -0.2010866  -0.09272046
  0.0331838  -0.44634315 -0.14699638 -0.05688526  0.20883524  0.1727562
 -0.13869111  0.01983972 -0.11186378  0.0758409  -0.07887912 -0.21576902
 -0.15873852  0.48233563  0.00764408  0.22410357  0.0554658   0.31273198
  0.02615981 -0.03963981 -0.12859948 -0.0171001  -0.3030826   0.12107649
 -0.49246734  0.04433598  0.30042875  0.14568645  0.09352069  0.39441943
 -0.33485037  0.42026383  0.3243009  -0.1278554  -0.02863774  0.1848507
  0.33969152  0.11827299 -0.04625271 -0.12825803  0.08213525 -0.07593869]"
Keras plot_model() giving error 'Model' object has no attribute '_container_nodes' stat:contribution welcome,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     TensorFlow Keras code
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
     Windows 10
- **TensorFlow installed from (source or binary)**:
     Binary
- **TensorFlow version (use command below)**:
     1.6, Keras version: 2.1.3-tf
- **Python version**: 
     3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
     9/7
- **GPU model and memory**:
       GeForce 860M
- **Exact command to reproduce**:
     plot_model(model1, to_file='model1.png')
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
 I am getting the following error with the  Keras plot_model()  command. 
'Model' object has no attribute '_container_nodes'
This error was first reported in #14542 and fixed in #14553. But it seems to have not carried over.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
input_X1 = Input(shape=(32,))
y1 = Dense(units=64, activation='relu')(input_X1)
y1 = Dense(units=32, activation='relu')(y1)
prediction1 = Dense(units=1, activation='relu',name='prediction_2')(y1) #
model1 = Model(inputs=input_X1,outputs=prediction1,name='model1')
plot_model(model1, to_file='mobilenet1.png')
```",True,"[-1.59203038e-01 -4.94756162e-01 -3.96046638e-01  9.00892764e-02
  2.47602344e-01 -1.32307753e-01 -2.19663218e-01 -2.10550457e-01
 -1.37074322e-01 -5.75091355e-02  1.92881450e-01  1.85688779e-01
 -1.12870529e-01 -1.91745795e-02 -2.14508191e-01  2.37196416e-01
 -1.78401306e-01  2.94853654e-03 -6.62567019e-02  2.58627534e-01
  2.16244966e-01 -7.61201326e-03  2.12004498e-01  5.39064705e-02
  1.48056783e-02 -2.80844569e-02 -9.56430435e-02 -1.72262371e-01
  3.73806320e-02 -1.71288550e-01  2.39510089e-01  2.13503912e-01
 -2.35596418e-01 -7.77608156e-02  1.74580783e-01  6.85003996e-02
 -6.42577112e-02 -2.55147040e-01 -1.11204103e-01  1.16414443e-01
  3.56058896e-01  9.19101238e-02  9.02414918e-02  1.20560691e-01
  2.08798870e-02 -1.05164483e-01 -5.06291389e-02 -3.89783442e-01
 -4.58159074e-02 -6.69022053e-02 -3.25084388e-01 -3.01594555e-01
 -2.66602725e-01 -4.31690276e-01 -3.68993729e-02 -3.00346255e-01
  1.40392572e-01 -2.27743238e-02 -1.84446685e-02 -2.31163166e-02
  3.32202911e-02 -3.72419097e-02  1.25789404e-01  1.08107090e-01
  2.82620192e-01 -3.71245854e-02 -1.01531558e-01 -9.97456163e-02
  3.47017765e-01  1.72244906e-02  1.50985122e-01 -2.70725284e-02
 -2.92755812e-01 -2.19681673e-02  3.70968021e-02 -5.31936325e-02
 -3.35542023e-01  2.53797263e-01  3.91131997e-01  4.15136591e-02
  5.88912331e-03 -8.32791626e-02  6.07502088e-02 -1.10533848e-01
  1.00347817e-01  1.24803610e-01  1.45063221e-01  5.27335033e-02
  1.68216363e-01 -1.97455790e-02  4.27944571e-01  3.35997075e-01
 -3.08570936e-02  4.03694570e-01  4.07123148e-01  2.25585654e-01
 -4.91422638e-02  1.29264176e-01 -2.98648059e-01 -1.83443815e-01
 -2.14268327e-01 -2.39617988e-01 -4.56769615e-02  2.02468365e-01
 -4.58669588e-02  6.21864200e-03  2.82523222e-02 -1.24685355e-02
  9.91886407e-02  5.27692921e-02  2.80662864e-01 -1.09752968e-01
  5.87158091e-02  1.53205633e-01 -9.89442468e-02 -1.32956207e-01
  2.01037616e-01  2.45005429e-01 -3.29110980e-01  3.26970518e-01
  5.48115466e-04 -9.35826004e-02  1.76573068e-01  1.43851027e-01
  1.98272079e-01  1.14867829e-01 -4.03092265e-01  2.56517619e-01
 -8.49049315e-02  1.10309675e-01  2.37751454e-01 -4.05182503e-02
 -8.10848773e-02 -1.75395250e-01 -1.14602737e-01 -2.70162523e-01
 -3.13823342e-01  1.02172522e-02 -2.48040259e-01  8.86014104e-02
 -8.25769268e-03  1.36507735e-01 -9.73947346e-02 -2.22677588e-01
  1.55983001e-01  2.84469157e-01 -9.59554762e-02 -6.47608936e-02
  1.61626160e-01 -1.30864814e-01 -1.78514361e-01 -1.31234005e-02
 -1.53973758e-01  4.25330848e-01  4.17635310e-03  2.05740288e-01
  2.98535451e-02  9.08144843e-03  5.82906418e-03 -2.32930586e-01
 -2.44482215e-02  1.44858211e-01 -2.71746367e-02 -9.51724499e-02
  3.41729462e-01 -1.47408303e-02 -1.85212702e-01 -2.36395836e-01
  2.67384946e-03  1.80781297e-02 -4.13037315e-02  1.40632808e-01
 -1.63901359e-01 -8.01036805e-02  5.32659411e-01  1.19762219e-01
  2.16056168e-01 -4.75096911e-01 -3.01167399e-01  2.30301186e-01
  3.92238736e-01 -4.38973941e-02 -6.26748651e-02  1.36562020e-01
  2.57185996e-02 -1.24472275e-01 -1.01720907e-01 -3.64745349e-01
 -1.48712829e-01 -1.69796556e-01 -2.73285925e-01 -1.35098696e-01
  2.07752258e-01 -2.63139233e-02 -9.21265632e-02 -3.55377346e-02
  2.35148609e-01 -2.33899310e-01 -9.34715793e-02 -4.75104973e-02
 -1.28732681e-01  3.97052094e-02 -1.12555295e-01 -2.98565328e-01
 -1.30395025e-01 -1.82679534e-01  5.18353395e-02 -1.47505850e-02
 -3.90706480e-01  1.27956830e-02 -1.09948650e-01 -3.31622034e-01
 -1.29737586e-01  2.26720646e-02 -2.55888104e-01  1.34305045e-01
  6.51650280e-02  2.07636073e-01 -5.82692325e-02  2.05885023e-01
 -1.80955548e-02 -8.82789940e-02  5.62872961e-02 -2.46204510e-01
 -2.92166471e-01  1.65490657e-01  3.69568095e-02  3.42166007e-01
  1.15643591e-01  1.00494184e-01  1.07676677e-01 -1.79199815e-01
  5.72299540e-01  1.27773061e-01 -3.17680836e-03 -2.42375452e-02
 -2.10553363e-01 -1.32534755e-02  2.19489962e-01 -1.85355276e-01
 -5.18313169e-01 -9.87050384e-02 -3.68195251e-02 -2.55448759e-01
 -5.64720035e-02 -4.75015305e-03 -1.68805107e-01 -1.22418264e-02
 -1.81170627e-01  3.25519502e-01 -1.68473274e-01 -4.26395059e-01
  3.59490275e-01  1.31035909e-01  2.23309949e-01  1.24325819e-01
  2.09034458e-02  8.43326151e-02  2.15615854e-01  1.95959508e-01
  3.40854615e-01 -2.14722738e-01  1.33208990e-01  5.10495424e-01
  3.21282744e-01  2.47750103e-01 -1.68238670e-01  1.05105087e-01
 -9.09812599e-02 -2.75962293e-01  2.87530720e-01 -2.07539108e-02
  3.69726181e-01 -1.09159693e-01 -4.52504829e-02  3.01771164e-02
  2.85884559e-01 -9.75365341e-02 -1.08326711e-01 -9.88973081e-02
  2.34297112e-01  1.28616288e-01 -2.39481866e-01  1.26690358e-01
  1.72622785e-01 -6.86259940e-02  3.79937328e-03 -3.14904809e-01
 -7.66245872e-02  2.54067499e-03 -7.38118365e-02  2.88486630e-02
  3.00904632e-01  1.37171954e-01 -1.59554422e-01 -6.35854006e-02
  3.69692333e-02 -9.83285606e-02  1.47462636e-01  3.38006079e-01
 -1.84707016e-01  2.16819346e-01  3.77497196e-01 -3.06049615e-01
 -2.17539459e-01  7.93959349e-02  1.76891953e-01  1.26733363e-01
  7.03668475e-01 -1.17830388e-01  1.41281247e-01  2.06021816e-01
 -1.27121359e-01  4.37816858e-01 -2.65486725e-03 -1.69509023e-01
 -6.70474023e-02  4.56685334e-01 -1.89551756e-01 -8.80419910e-02
 -3.09231300e-02  4.33116034e-02 -1.31190434e-01  5.96918724e-02
  1.61111742e-01  6.34355396e-02 -2.36500159e-01 -9.52894837e-02
 -9.94663835e-02  2.57265091e-01 -2.39003412e-02  2.50984654e-02
  4.63881344e-03  1.83400512e-01 -8.84630308e-02 -2.97341555e-01
 -8.92061740e-02  5.39461493e-01 -7.70690367e-02 -3.80412966e-01
 -1.32254407e-01 -1.39226854e-01 -1.20403618e-01 -4.20295537e-01
 -2.10900784e-01 -1.22526258e-01  1.60933703e-01  7.12404370e-01
 -2.68467888e-03  2.14836001e-01  1.63188666e-01  1.14228174e-01
 -3.52132827e-01  2.10946202e-01 -2.54347503e-01  2.44292706e-01
  9.33383219e-03 -2.61123925e-02  2.23569926e-02  5.20479202e-01
 -2.50354946e-01  5.15706465e-02 -1.15897432e-01 -9.75857079e-02
  6.24428988e-02  2.87402958e-01 -1.34333760e-01 -2.24729419e-01
  4.19783294e-02  2.64247775e-01 -3.10013145e-01  6.57876162e-03
 -2.42678881e-01  3.30770984e-02  4.59369943e-02 -2.48494409e-02
  1.52461350e-01  1.39634401e-01  1.32952541e-01  3.90369259e-03
 -7.62607008e-02 -2.91928500e-01 -2.66700268e-01 -1.47318184e-01]"
tf1.6 error: tf.contrib.ffmpeg.decode_video ,"@yongtang  I use tensorflow 1.6on ubuntu and decodevideo.

`with tf.Session() as sess:
  movie_bin = tf.read_file('/home/xucl/app/data/bilibili/video/DongFangLieChe.mp4')
  movie = tf.contrib.ffmpeg.decode_video(movie_bin)
  movie_ev = movie.eval()
  print(""****"",len(movie_ev))`

but get an error

`2018-03-08 10:48:23.000491: F tensorflow/contrib/ffmpeg/default/ffmpeg_lib.cc:400] Non-OK-status: ReadInfoFile(stderr_filename, width, height, frames) status: Unknown: Not enough video info returned by FFmpeg [106, 0, 640, 3]Could not read FFmpeg stderr file: /tmp/tmp_file_tensorflow_3_cPNCGW.err
 ()
`
",True,"[-2.09131062e-01 -3.51842999e-01 -1.79193258e-01 -1.58943802e-01
  5.33903003e-01  3.78026292e-02  2.80800700e-01  6.30433783e-02
  9.17143300e-02  1.75445035e-01 -5.32585308e-02 -2.81158805e-01
  7.93749020e-02  3.00129831e-01  9.53791365e-02  9.06246006e-02
 -9.32833552e-02 -7.07396120e-02  8.19727182e-02 -1.61760479e-01
  3.33633460e-02 -3.90469640e-01 -2.10274011e-05  1.19223654e-01
 -7.94971436e-02  2.43053794e-01  7.38670975e-02 -2.70223320e-01
  1.32183731e-01  1.37661725e-01  2.02568606e-01 -3.84246260e-02
  1.51367396e-01 -1.12401387e-02  1.15231544e-01 -7.43506327e-02
  6.69582142e-03 -2.13171571e-01 -4.36008930e-01 -6.10329390e-01
 -4.32561785e-02  1.74552485e-01  2.67847419e-01 -8.30630958e-02
 -8.27839673e-02 -1.15533218e-01  4.11401279e-02 -2.07022317e-02
 -6.17904402e-02  2.42719818e-02 -3.15795511e-01  4.00504291e-01
 -5.87793648e-01 -1.74260318e-01  8.90475214e-02 -2.37516779e-02
 -4.77811173e-02  4.27892923e-01  4.00390446e-01  5.31019159e-02
 -2.50672728e-01  8.26797634e-02 -1.30170435e-01  9.03480500e-02
  1.12636812e-01 -9.31668356e-02  1.43419281e-02  1.44823730e-01
  2.90912390e-01 -4.23029155e-01 -6.79395571e-02  1.11492962e-01
 -8.86286497e-02 -5.79522401e-02 -1.95947886e-01  3.13384719e-02
  1.87976733e-02  5.50760865e-01 -6.07094355e-02 -1.56230733e-01
  2.03408346e-01 -3.39435786e-02  6.30767271e-02  2.67134588e-02
  2.07763584e-03  1.85300380e-01  8.94235075e-02 -6.84580356e-02
  4.17200089e-01  1.28278315e-01  3.01716447e-01 -3.03458691e-01
  2.69933105e-01  2.15679631e-02  1.43083498e-01 -2.51089558e-02
  1.52159691e-01  2.48639166e-01  4.86420691e-02 -1.74327850e-01
 -6.50838837e-02 -4.82996881e-01 -1.14474095e-01  4.65266943e-01
 -5.19092456e-02  2.26515383e-02  1.50123298e-01  2.96541095e-01
  2.47402593e-01  1.81794867e-01 -1.05047300e-01 -1.33456867e-02
  2.47900963e-01 -9.29437876e-02  1.59271941e-01  3.33233178e-01
 -2.27299720e-01 -6.62001893e-02 -7.40017965e-02  2.66230583e-01
  2.00227737e-01 -2.35005602e-01  7.92082697e-02 -1.03812575e-01
  2.48731092e-01  8.08395892e-02 -6.61313534e-02 -1.81794882e-01
  1.15546718e-01  2.42515896e-02  3.47461224e-01  2.59121835e-01
 -2.33064309e-01 -3.31802517e-02 -2.11820185e-01 -3.20610851e-02
 -4.86255705e-01 -3.63813043e-02 -4.55832034e-01 -1.73669070e-01
 -2.60753304e-01  1.44073069e-01 -4.52940613e-02 -1.25033155e-01
 -2.32505187e-01 -1.92042902e-01 -1.62857696e-01  4.85949367e-01
 -3.18563849e-01 -1.47418782e-01 -4.33007628e-02 -4.16944548e-02
 -2.34666571e-01  2.38290384e-01 -1.25597328e-01  3.19840610e-01
  4.77153957e-01  1.03460275e-01 -3.10020030e-01 -3.56956840e-01
  9.13774073e-02  2.78399736e-01  2.47746065e-01  6.30496293e-02
 -7.51411170e-02  2.42424041e-01 -7.40493178e-01  2.12122440e-01
 -1.18381299e-01  1.19098686e-01 -3.59826505e-01  2.11712327e-02
  1.70498312e-01 -4.71928179e-01  3.08946609e-01 -7.69412518e-02
  1.66207273e-02  1.45692289e-01 -2.93895066e-01  2.81345069e-01
  1.44111931e-01 -1.86441734e-01 -8.31078589e-02 -1.27210721e-01
  2.04625100e-01 -1.49290413e-01  5.55291995e-02  3.59001420e-02
 -6.44461205e-03  2.32574672e-01 -2.11351290e-01 -9.37998071e-02
  1.96704626e-01  1.49095535e-01 -1.53121054e-01  1.30810976e-01
  2.77236328e-02  4.24279645e-02 -8.03218484e-02  4.40712482e-01
  2.94241071e-01  2.88533550e-02  2.42383629e-01 -5.75398915e-02
  2.54457891e-01  1.81390435e-01 -5.61519824e-02 -3.32539231e-01
 -2.69665331e-01  4.51453254e-02  6.38656169e-02 -3.03129017e-01
 -2.42452919e-01 -1.92177266e-01 -2.74759948e-01  1.29284849e-02
 -2.01706260e-01 -3.65688264e-01 -8.33139122e-02 -1.20708905e-02
  2.42207378e-01 -1.41836584e-01  1.06297433e-01 -1.14978559e-01
 -1.17942646e-01  9.53173786e-02 -2.10739493e-01 -5.38562834e-02
 -2.88558066e-01  1.26647368e-01 -1.36798341e-03 -1.78823844e-01
  5.71465313e-01 -1.08554326e-01  1.00686513e-01 -2.44948924e-01
 -2.12322831e-01 -1.40069097e-01 -1.19936347e-01 -4.83170077e-02
  1.28304474e-02 -4.02601123e-01 -1.54664233e-01 -3.43753904e-01
 -1.79177940e-01 -3.53601635e-01  4.58427727e-01 -1.26811132e-01
 -3.70392740e-01  4.01800334e-01  2.14233398e-01 -2.49214191e-03
  2.99228370e-01  2.98327923e-01  4.09791142e-01  7.02024698e-02
 -4.50608134e-03  6.47341758e-02  1.27981275e-01 -7.93794096e-02
  3.21227133e-01  5.21444321e-01 -2.46027783e-01  2.33184963e-01
 -2.07095712e-01  2.56801516e-01 -4.43879902e-01 -1.80130333e-01
  1.05104856e-02  1.35355070e-01  3.64364963e-03 -3.11263949e-01
  4.23665106e-01 -1.52014434e-01  3.07390094e-03 -5.34976602e-01
  3.52242112e-01 -1.43913567e-01 -1.93560019e-01  4.10621688e-02
  3.01820159e-01  5.51377237e-01  3.31054702e-02  6.78486079e-02
  3.15156043e-01  2.11645126e-01 -1.68890461e-01 -5.37256777e-01
 -1.63588524e-01 -1.27917349e-01 -3.20305616e-01  2.94990152e-01
  8.78864974e-02 -5.49384430e-02  3.66705433e-02 -2.82959878e-01
 -3.14010531e-02  2.61059970e-01 -1.15369365e-01  3.26764137e-02
 -1.58936262e-01  3.97575557e-01  2.12599397e-01  5.05936503e-01
  2.53780812e-01  2.08749156e-03  6.00878224e-02 -4.44612876e-02
  3.74424636e-01 -7.40870237e-02 -1.80391163e-01 -4.12333943e-02
 -2.74132732e-02  2.67108858e-01 -2.20600784e-01  4.03612196e-01
  1.44534349e-01  2.40887001e-01  2.40147233e-01  4.59025390e-02
  8.46736655e-02 -7.05925107e-01 -4.92083907e-01 -1.61663860e-01
 -9.09747183e-02  2.16856241e-01 -3.34509939e-01 -3.79368007e-01
  7.56308995e-03 -4.04278412e-02 -7.61725157e-02 -1.47519317e-02
 -1.67159051e-01  1.50332317e-01 -4.00024951e-01  1.68270797e-01
 -3.37341130e-02  2.91239060e-02  4.84889746e-02 -9.84853506e-02
  2.52764761e-01 -2.96614826e-01 -3.67385559e-02 -2.46942908e-01
 -2.49280930e-01 -2.76573211e-01  2.14498881e-02 -3.37400138e-02
  1.09679073e-01  3.99120748e-01 -1.01045899e-01  2.56105624e-02
 -1.71999902e-01  2.26631552e-01 -2.46368259e-01  2.75102388e-02
  1.07323594e-01 -1.19070783e-01 -4.78377827e-02  6.01545691e-01
  5.96257001e-02  2.20917732e-01 -1.80277467e-01 -3.70567143e-01
  2.59533703e-01 -6.21645264e-02  1.58312079e-02 -2.89241880e-01
 -2.59213671e-02  9.74164158e-02 -1.71002835e-01  2.73288429e-01
 -5.48024058e-01  2.76841044e-01  4.07997787e-01 -2.28933990e-01
  2.39271984e-01  3.03304076e-01  2.41427451e-01 -3.24647129e-02
 -3.11672628e-01  2.63407618e-01  1.14162385e-01 -2.17730060e-01]"
CI: TensorFlow build is failing on Windows ,"Before merging #16659, we should fix the TF build on Windows first.

Current failure:
```
subprocess.CalledProcessError: Command '['bazel', '--batch', 'version']' returned non-zero exit status 36
```
Culprit is https://github.com/tensorflow/tensorflow/commit/671baf080238025da9698ea980cd9504005f727c
because `f.write('import %s\n' % _TF_BAZELRC)` writes backslash into bazelrc file without escaping.
I'll send a fix.
FYI, @gunan @martinwicke @case540 
",True,"[-2.02151790e-01 -6.16351008e-01 -1.76296338e-01  1.77971259e-01
 -3.09761893e-02 -3.74599278e-01 -1.43360734e-01 -3.21263596e-02
 -1.30132586e-01 -3.02900553e-01 -1.86517492e-01 -2.68609405e-01
 -3.38309526e-01 -1.97654478e-02 -5.66032290e-01  1.07714206e-01
 -2.49096185e-01 -2.60855611e-02  1.55926660e-01 -4.28660959e-02
 -4.55344290e-01 -1.62850589e-01 -4.17176127e-01  3.00389439e-01
  3.79270136e-01  5.32120466e-02  2.38351971e-02 -1.24720678e-01
  1.81668535e-01  4.24003303e-01  2.06052572e-01 -1.74302399e-01
  7.46702589e-03  2.13864464e-02  2.27032334e-01  2.48699486e-01
 -1.26503229e-01 -4.20542024e-02 -3.01588863e-01 -3.76676083e-01
  1.16154790e-01  1.10286988e-01 -9.56618935e-02  1.29149884e-01
 -2.64072549e-02  6.83912411e-02  1.34416148e-01 -1.02090620e-01
 -1.94026321e-01 -6.43805340e-02 -1.80011183e-01  1.62070289e-01
 -3.93832892e-01 -4.72858965e-01 -4.24707048e-02 -2.74339050e-01
  2.07584769e-01  1.80069223e-01 -5.22294827e-03  1.20253526e-01
 -9.43506211e-02 -2.40731448e-01  1.13918461e-01 -4.52641062e-02
  4.62839678e-02  1.49982795e-01  1.68571889e-01 -4.92127985e-03
  2.44826615e-01 -2.86857486e-01  3.35145175e-01  2.77115889e-02
 -1.74258739e-01  1.28956493e-02  2.00919896e-01  3.62751842e-01
 -3.90783399e-02  3.99113297e-01  5.27788997e-01 -2.29183227e-01
  8.29984099e-02 -1.94388703e-01  9.72021669e-02  8.10660124e-02
  1.26716405e-01  3.12859826e-02  1.73867032e-01 -7.07389042e-02
  4.69759166e-01 -2.80781299e-01  5.14382780e-01 -1.28622483e-02
 -9.63243470e-02  3.28175724e-01  1.32428110e-01  2.38077343e-01
 -1.64474308e-01  3.95795226e-01  3.47035639e-02 -2.03792661e-01
 -1.63802385e-01 -1.03519492e-01 -3.74383807e-01  2.83078998e-01
  1.80294931e-01 -3.27563882e-01  5.05415559e-01  9.97064039e-02
  1.94864482e-01 -2.19919577e-01  2.72886068e-01 -2.87813693e-01
  2.48214334e-01 -1.29920304e-01  5.59727885e-02  3.29425931e-01
  2.06753880e-01  2.06114084e-01  2.17044093e-02  2.71892548e-01
 -9.42477584e-02 -8.04548338e-02  4.85821962e-02 -4.14927840e-01
  5.97190410e-02 -1.43382505e-01 -4.27442729e-01  7.85994157e-02
  2.56142765e-01 -1.44592986e-01  2.70925492e-01  3.94174814e-01
  4.96948287e-02 -1.73151359e-01  2.50060201e-01  1.31322145e-01
 -4.29996073e-01 -2.33363971e-01 -5.25435925e-01 -5.93006015e-01
  6.14939863e-03  2.27118954e-01 -1.99377120e-01 -5.29101610e-01
  6.75692782e-02 -3.68379988e-02 -1.76321357e-01  1.54575184e-01
 -1.80789143e-01  1.16448976e-01 -1.99905202e-01  2.96792090e-01
  1.33601218e-01  4.12702173e-01  2.96502113e-02  5.24583161e-02
  1.67087406e-01 -4.05822173e-02 -2.21837908e-01 -5.94002247e-01
 -2.74469052e-02  3.77541572e-01 -1.87450662e-01  6.49654567e-02
  3.86468232e-01  3.09101850e-01 -5.17649829e-01  1.03242667e-02
  3.22043419e-01  2.44235888e-01 -2.43242666e-01 -1.85686484e-01
  1.25335306e-01 -1.94791585e-01  3.38039905e-01 -3.39729115e-02
  3.45327914e-01 -2.37087294e-01 -1.08534388e-01  2.72432506e-01
  4.68277186e-01  8.95809941e-03  9.54133049e-02 -2.06139416e-01
 -1.47548944e-01  1.85995787e-01  2.41842821e-01  3.15793604e-01
  1.03591438e-02  1.33139774e-01 -3.42873067e-01  3.48171949e-01
  4.43683267e-02  1.04494244e-01 -1.26371980e-01  1.62144646e-01
 -3.57638374e-02  7.94506073e-02  1.02122240e-02  9.90149677e-02
 -5.09189144e-02 -8.09882581e-02  2.74886549e-01  5.01319468e-02
  1.69466734e-01 -3.83524269e-01  3.37650366e-02 -4.74258095e-01
 -3.17257345e-02  1.08383588e-01  1.07296750e-01 -3.06094199e-01
 -2.79196799e-01  3.46373804e-02 -1.18467972e-01  2.05223233e-01
 -1.44342348e-01 -8.33334923e-02 -2.19069332e-01  5.10493666e-03
  2.00409904e-01 -9.81501788e-02 -8.02510828e-02 -5.25939047e-01
 -7.75905624e-02 -1.57865122e-01 -1.56443819e-01 -1.27517298e-01
 -1.03551701e-01  2.78337955e-01 -9.32103544e-02 -1.50874063e-01
  7.18551576e-02  4.26188745e-02  5.38323283e-01 -2.23610356e-01
 -4.48247045e-02 -2.49281287e-01 -1.26639336e-01  1.20595574e-01
 -2.86465377e-01 -2.98686445e-01 -7.34311044e-02 -2.13549107e-01
  2.59499699e-02  2.06086174e-01 -6.03384152e-02 -1.28063440e-01
 -1.54831678e-01  4.26130176e-01  1.80474773e-01  7.67314881e-02
  2.68640429e-01  3.19251239e-01  2.72939920e-01  8.93652737e-02
  2.98274666e-01  4.28475410e-01  1.66412041e-01  9.24913436e-02
  3.83518636e-01  5.92299215e-02  1.54367074e-01  1.57081455e-01
 -4.08855528e-02  3.88251513e-01 -9.21524912e-02  4.94724512e-01
 -1.75321341e-01  3.00894026e-02  7.33268708e-02 -1.61885053e-01
  5.83421290e-02 -6.00465536e-01 -7.99543187e-02 -2.58796394e-01
  4.90479976e-01  2.80186713e-01 -3.17913204e-01  2.11872041e-01
 -1.95727870e-03  4.99964148e-01 -1.86649263e-01  3.79287839e-01
  4.84608591e-01 -1.60922110e-01  5.76735772e-02 -4.29676235e-01
 -1.82652771e-01 -5.67894429e-04 -1.52948409e-01  2.26632416e-01
  3.59535605e-01 -1.83095373e-02 -1.78705990e-01  1.95797339e-01
 -1.83040157e-01 -9.10070837e-02 -2.63531860e-02  1.33642465e-01
 -1.00380808e-01  9.86698046e-02 -2.45639943e-02 -3.53392251e-02
 -8.14000219e-02 -2.46994972e-01  1.62717521e-01  1.65263653e-01
  3.07537854e-01 -4.30104971e-01  3.67665678e-01  2.47382104e-01
 -9.59555954e-02  4.51664209e-01 -4.23307329e-01 -1.03278518e-01
 -2.24243045e-01  5.48867762e-01 -2.36887574e-01 -5.04657365e-02
  1.44364908e-02 -2.72335052e-01 -3.20165813e-01  1.94605030e-02
  8.73288065e-02  1.18221305e-01 -3.23908955e-01 -1.79581240e-01
 -9.85444933e-02  1.73009694e-01 -2.29209766e-01 -2.76737809e-01
 -2.26510972e-01 -8.74357522e-02 -6.64327070e-02 -5.12017943e-02
 -2.19957739e-01  1.43558159e-01  4.50033918e-02 -2.26510108e-01
 -7.46334717e-03 -2.72274971e-01  1.35078013e-01 -1.91228688e-01
  9.06749256e-03 -2.35866949e-01  1.04020394e-01  3.11621279e-01
 -7.60001689e-02  2.36485213e-01 -7.88382739e-02  4.33728285e-02
 -5.70347309e-02  1.41717225e-01 -1.86690032e-01  3.10887516e-01
 -1.30365640e-01  2.14503169e-01  4.83691618e-02  2.95359194e-01
  4.15615924e-02 -8.89579058e-02 -2.38031060e-01 -1.27435699e-01
  2.29270607e-01 -6.54336065e-02 -5.48923075e-01  1.00527413e-01
  1.44522831e-01  1.99419439e-01 -1.05363112e-02  5.06361008e-01
 -2.87814021e-01  2.45445698e-01  5.64398527e-01 -4.43619162e-01
 -2.35062033e-01  2.73341507e-01  5.98917343e-02  6.04197010e-02
  2.14634508e-01  1.60385504e-01  9.76352468e-02  7.16424808e-02]"
NotFoundError: Op type not registered 'KafkaDataset' in binary. ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: - 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('unknown', '1.6.0-rc1')
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.8.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**:

~~~python
from tensorflow.contrib.kafka.python.ops import kafka_dataset_ops
from tensorflow.python.data.ops import iterator_ops
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops

topics = array_ops.placeholder(dtypes.string, shape=[None])
num_epochs = array_ops.placeholder(dtypes.int64, shape=[])
batch_size = array_ops.placeholder(dtypes.int64, shape=[])

repeat_dataset = kafka_dataset_ops.KafkaDataset(
    topics, group=""test"", eof=True).repeat(num_epochs)
batch_dataset = repeat_dataset.batch(batch_size)

iterator = iterator_ops.Iterator.from_structure(batch_dataset.output_types)
init_op = iterator.make_initializer(repeat_dataset)
init_batch_op = iterator.make_initializer(batch_dataset)
get_next = iterator.get_next()
~~~

ENV: https://pastebin.com/89aihba7

### Describe the problem
Getting `NotFoundError` even with `TF_NEED_KAFKA=1` and `--define with_kafka_support=true`:
~~~
NotFoundError: Op type not registered 'KafkaDataset' in binary running on 68a9f992375e. Make sure the Op and Kernel are registered in the binary running in this process.
~~~
`KafkaDataset` was merged into master last month. Is there something missing that needs to be done in order to utilize the new op? 


### Source code / logs
https://pastebin.com/5Vy6b6kf",True,"[-0.30305743 -0.8043948  -0.58039397  0.04349881  0.15215148 -0.18920198
 -0.05435749 -0.04475495 -0.3028748  -0.26399088  0.2629993  -0.01221585
 -0.29171377  0.11878182 -0.29063147  0.0487923   0.08986661 -0.30751818
  0.3724751  -0.00748837 -0.10817643 -0.02005428 -0.02769966  0.42732984
  0.09096209  0.34773207 -0.09126887 -0.1010973  -0.04617352 -0.05565896
  0.70961314  0.05188452  0.06817428  0.03504806  0.11142746  0.38335684
 -0.22882889  0.12298682  0.08247128 -0.05778135  0.1318996  -0.00828337
  0.03560887  0.1280188  -0.15561393 -0.04240242 -0.0424641   0.11630084
 -0.0996151  -0.3078705   0.10148986 -0.14334473 -0.46893287 -0.28228962
 -0.07088047 -0.18669307  0.09314558  0.01559827 -0.02411628  0.23220572
  0.14661193  0.04376748 -0.14968044  0.19533764 -0.00353622  0.2211797
  0.35412523 -0.33607402  0.5566067  -0.34466836  0.1218767   0.04369446
 -0.17765555 -0.01611523 -0.14009857  0.16094786 -0.01223724  0.1459583
  0.12037066 -0.03312084  0.18874493 -0.10984822  0.30830407  0.04991015
 -0.16884197  0.1982449   0.2885675   0.10024387  0.3516073  -0.01898214
  0.26482487  0.27639377  0.00177165  0.17197181  0.06408603  0.17327134
  0.1787822   0.32466888  0.01420026 -0.16269177  0.01395397 -0.1072446
 -0.10510734  0.0643677  -0.07216518 -0.20728618  0.31038868 -0.06026922
  0.05041312 -0.09148201 -0.08529895 -0.02993319  0.02455491 -0.15129484
  0.16703475 -0.04500284 -0.10115968  0.09261385 -0.26649216  0.71252596
 -0.11761416  0.03765095  0.29823312  0.266065    0.49168074  0.15352559
  0.01942666  0.01741654  0.22327895  0.12460811  0.12338006  0.0154599
 -0.18797043  0.12916678 -0.05514587 -0.11777426 -0.18870467  0.13683607
 -0.0723217  -0.06149033 -0.12953913  0.24119207 -0.01396327 -0.18171054
  0.01043933 -0.09362575  0.10301118  0.32038605  0.1704165   0.1768524
 -0.24954513  0.1433283   0.01745206  0.43470532 -0.01105447  0.17522007
  0.18241818 -0.06935987  0.07026866 -0.5711907  -0.2737332   0.13091908
 -0.18351486 -0.10404268 -0.08445954 -0.09687201 -0.45297968 -0.34453326
 -0.00511503  0.10776623 -0.22445615 -0.04865684 -0.00361407  0.08957973
  0.05305954  0.07081675  0.38994604 -0.46944314 -0.08022244  0.284532
  0.09713339 -0.0246084   0.05445639  0.05986847 -0.01956969  0.18699814
  0.17149451  0.08519588 -0.07380454  0.02283781 -0.08547437 -0.07023689
  0.04130165 -0.23917365 -0.3172059  -0.13391487  0.21018201  0.20673653
 -0.13329342  0.16127105 -0.25608498 -0.00720773  0.0295401   0.00188618
  0.01181382 -0.13555741 -0.36415744 -0.01980297 -0.58255446 -0.01172467
  0.0517388  -0.20521247 -0.00579016  0.24229975 -0.17453727 -0.10240239
  0.06067308 -0.01180033 -0.16293386  0.09920613  0.13437223 -0.20460966
  0.22337042 -0.2019904  -0.06856411 -0.11179321 -0.18455729 -0.1473425
 -0.07139395  0.23088625 -0.30863583  0.01615882  0.21195516  0.12854773
  0.16385248  0.01666271 -0.016255   -0.15306094 -0.13196863  0.21880591
 -0.49242732  0.0295294   0.15859297 -0.06512471  0.18103096  0.14588906
 -0.13461094  0.07197107 -0.20147079  0.10893791 -0.2501546   0.0108949
  0.35376057  0.24647748  0.29465327  0.3744354  -0.11052294  0.02127465
 -0.00581189 -0.09676196  0.02456829  0.32245424 -0.19104698  0.41600156
  0.20521994  0.46250945 -0.3520818   0.20492306 -0.23308931 -0.11342502
  0.04050658 -0.14840941  0.36550498 -0.24161339 -0.03238875 -0.06551386
  0.22500557  0.01181121  0.09102831  0.05707977  0.12365244 -0.00552712
 -0.11098352 -0.28547174 -0.01707662 -0.42837316 -0.07429031 -0.26186615
 -0.24330181 -0.07435746 -0.08166061  0.09183004  0.1470157   0.22977103
 -0.03716998 -0.17875008  0.04957279  0.2556625   0.23798904  0.30351385
 -0.13421366 -0.17520447  0.01192913  0.11088873  0.07096217  0.02096044
  0.20263883 -0.10885467  0.5789632  -0.2181274   0.18656671  0.0219715
 -0.03131364  0.27030128 -0.05488449  0.19521402 -0.18046772  0.27011672
  0.19571555 -0.00454531  0.19632322 -0.20860103 -0.15409194  0.17204165
 -0.04828756 -0.09381417 -0.2179036  -0.43422118 -0.18851954 -0.07964382
 -0.23791122 -0.09826919 -0.07933697 -0.03117802 -0.07312427  0.00813253
 -0.28075105  0.29525262  0.00721904 -0.1127698  -0.1139403  -0.27378958
 -0.14459175 -0.41713673 -0.14028274 -0.03770766  0.6547424   0.31602073
 -0.31067127  0.3479526  -0.07069656 -0.05835715 -0.11210328 -0.13598391
 -0.40562493  0.31550574  0.1050424  -0.05772966  0.16863458  0.38787258
 -0.43328756 -0.3189819  -0.36725986 -0.27772617  0.01505129  0.01666373
 -0.278089   -0.04301876  0.42354423  0.3873405  -0.1337077   0.18392472
 -0.12003721  0.16683298  0.45918748 -0.12732255 -0.21894349 -0.0840831
  0.23520689 -0.4086876   0.03464842 -0.18640836  0.20433715 -0.23574524]"
tf.einsum not replicating np.einsum; struggles to read input with whitespace ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Not sure
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0
- **Python version**:  3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0.61
- **GPU model and memory**:  NVIDIA Corporation GK210GL [Tesla K80] (rev a1)
- **Exact command to reproduce**: See below.


### Problem
Unlike `np.einsum`, I have found that `tf.einsum` struggles to parse input strings with spaces.
Either it throws an error, or it produces varying results for the same syntax (e.g. `ij,jk->ik` gives different results than `ij,jk-> ik`). 

I believe the problem traces to the regular expressions match in line 164 (carried to line 178) of this file: `tensorflow/tensorflow/python/ops/special_math_ops.py`.


### Source code
```
import tensorflow as tf
import numpy as np

import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

x = tf.range(5,10,  dtype=tf.float64)
y = tf.ones(shape=(2,6), dtype=tf.float64)

# Each invocation of tf.einsum produces a different result
tf.einsum(""i,jk->ijk"", x, y)    # shape = (5,2,6)
tf.einsum(""i,jk-> ijk"", x, y)   # shape = ()
tf.einsum(""i, jk -> ijk"", x, y) # Error

x = np.arange(5,10,  dtype=np.float64)
y = np.ones(shape=(2,6), dtype=np.float64)

# In contrast, np.einsum produces consistent results
np.einsum(""i,jk->ijk"", x, y)    # Array 5x2x6
np.einsum(""i,jk-> ijk"", x, y)   # Array 5x2x6
np.einsum(""i, jk -> ijk"", x, y) # Array 5x2x6
```",True,"[-0.28071845 -0.59229493 -0.41519383 -0.06401331  0.03268365 -0.2537626
  0.07543418 -0.10275313 -0.16156086 -0.06885427  0.15124059  0.01488656
 -0.18564633  0.1034595  -0.30580276  0.11614855 -0.08604231 -0.40150017
  0.23738614 -0.05316118 -0.0354477  -0.09843637 -0.13986145  0.30276945
  0.01950207  0.29858088 -0.21437892 -0.1047148   0.01885166 -0.05484401
  0.26656932  0.04387219 -0.16392112 -0.03445416  0.09174205  0.47390187
 -0.06717667  0.05360629 -0.25854567 -0.03834499  0.0362467  -0.0923398
 -0.02097442 -0.05365107  0.06042067 -0.07160583  0.19853178  0.01903942
 -0.19901623 -0.12423575  0.03922953  0.06110947 -0.38052726 -0.2143108
  0.06988712  0.0444649   0.10307581 -0.01096211  0.08145852  0.138397
  0.06897508  0.08980774 -0.1066787   0.15725805  0.1601984   0.20799336
  0.3135928  -0.22404541  0.292265   -0.118637    0.08272991 -0.07498845
 -0.32731345 -0.06381297  0.11431035  0.30269647 -0.00556547  0.09247877
  0.26134872 -0.19033518  0.11324541 -0.01160191  0.08417656  0.09100229
 -0.06729776  0.06775298  0.26635817  0.08837675  0.33657432 -0.26562572
  0.465049    0.03574152 -0.02538887  0.05921024  0.26766032  0.09987234
  0.0041505   0.38395667  0.14564107 -0.07777119 -0.13750403 -0.13217497
  0.10174444 -0.03155145 -0.06611805 -0.10619375  0.09784859  0.02221864
  0.18850344 -0.07253198 -0.10099186  0.12537785  0.05705266 -0.04278772
 -0.02507953 -0.03672075 -0.05896115 -0.0174139   0.03253627  0.5148649
  0.07700264 -0.29810005  0.09372266  0.24691701  0.40531182  0.10727482
 -0.17386517  0.05434201  0.11436058  0.15367875  0.368229    0.03988722
 -0.25180784  0.13657422  0.01869285 -0.0876613  -0.32052904  0.03435539
 -0.27523735 -0.05590345 -0.21069099  0.15449661 -0.03646596 -0.28037447
  0.03110689  0.07518418 -0.02956565  0.23299706 -0.13373831  0.32936746
 -0.05599398 -0.12459798  0.13900185  0.2074031  -0.04612453  0.18607914
  0.19441622 -0.0851936   0.04630607 -0.4987219  -0.2795689   0.24843356
 -0.09502825 -0.03214273  0.10894923  0.11117449 -0.36589876 -0.43788734
  0.05003032  0.3049132   0.04846197 -0.0081061  -0.01555652  0.25486094
  0.26293588 -0.14414959  0.5111819  -0.4833058  -0.17930306  0.20748521
  0.11305384 -0.06934857  0.18161905 -0.01033474 -0.04983564  0.14606383
  0.24536037  0.34432423 -0.1310466   0.0077242  -0.16680856 -0.0498457
  0.20759213  0.00836371 -0.33329037 -0.09685754  0.34282637 -0.09578495
  0.02338404  0.29216    -0.1429506  -0.22454488  0.04673286 -0.04866338
  0.01492811 -0.16069116 -0.19683647 -0.16627975 -0.3745059  -0.00303672
  0.03868345 -0.20261112  0.00360339  0.06580001 -0.19234872 -0.1520229
  0.00986773 -0.25010735 -0.15028132  0.13401303  0.11146401 -0.18853058
  0.02972855 -0.34880066 -0.10956926 -0.13004518 -0.32839385  0.16476235
 -0.01800173  0.2172705  -0.03963452  0.12466152 -0.08484982  0.07182297
  0.38798678 -0.10454079 -0.00156446 -0.24942742 -0.289115    0.13082045
 -0.47868896 -0.22273013 -0.01231873 -0.02757883 -0.05945904  0.18871213
 -0.10290617  0.19039539 -0.17443249  0.35150307 -0.27130818  0.01530339
  0.18250793  0.11165129  0.21206151  0.26340535  0.01671456  0.03823157
  0.13941881 -0.07440005  0.1083168   0.33506393  0.06051488  0.55458796
  0.3242591   0.41063833 -0.13273531  0.25886    -0.10312014  0.05625008
 -0.03844853 -0.16072977  0.4212867  -0.26141    -0.03758914 -0.23098013
  0.34243113 -0.17830703  0.05784561  0.3201574   0.05046832  0.2957654
 -0.09248953  0.12328912 -0.0412158  -0.22939263  0.05375163 -0.45705384
 -0.23382139 -0.13536912 -0.17683771  0.0319322  -0.00621131 -0.05462166
 -0.17475143 -0.06798898  0.08267264  0.15827408  0.17420611  0.22121412
 -0.05695783 -0.2670703   0.11587173  0.17482802  0.10773405  0.01484111
  0.4040649   0.02677403  0.42695916 -0.25847358  0.22918507 -0.12450179
 -0.07829164  0.3080207  -0.03885637  0.16134238 -0.31799755  0.49345046
  0.2245355  -0.01963527  0.07076924 -0.21695867 -0.12535441  0.03538754
 -0.09580199 -0.13284953  0.01500624 -0.3006122  -0.05965895  0.07319491
 -0.2805528  -0.1503604  -0.2958935  -0.02624259 -0.33032382 -0.08256486
 -0.3482563   0.3040164  -0.08159588 -0.15208781  0.03251686 -0.08137039
 -0.16349392 -0.16265075 -0.05850167 -0.17641404  0.20619512  0.22933507
 -0.14600304  0.08648638  0.05076616  0.14112261 -0.18565235 -0.0553488
 -0.24660838  0.12705165 -0.21098143  0.01938092  0.03711324  0.27789384
 -0.06253481 -0.0125156  -0.40196696 -0.11593112  0.12904221 -0.04280691
 -0.27659565  0.00409376  0.23479116  0.2818697  -0.04661134  0.42824858
 -0.14071305  0.11729769  0.5761782  -0.31894037 -0.20765379  0.06771956
  0.23575678 -0.29422274  0.08936608 -0.16933718  0.09156899 -0.11235132]"
"incorrect logging formatting used in tensorflow / examples / image_retraining / retrain.py, causes error ","In tensorflow -> examples -> image_retraining -> retrain.py, currently lines 347 / 348 look like this:

```
tf.logging.info('Successfully downloaded', filename, statinfo.st_size,
                    'bytes.')
```

This understandably causes an error since this function accepts strings and it is being fed an instance of statinfo.st_size which does not seem to be a string.  On my machine at least (TensorFlow 1.5, Windows 10) this causes the following error in function maybe_download_and_extract:

`TypeError: not all arguments converted during string formatting`

Here is a screenshot if that helps:

![error](https://user-images.githubusercontent.com/5672876/35772391-74f4433c-08f2-11e8-83d2-084605c14844.png)

The line numbers are slightly different in my screenshot because I moved a few lines around, but I can assure you the line above is causing the logging error.

I would suggest changing this line to the following, or similar:

`tf.logging.info('Successfully downloaded ' + str(filename) + ', statinfo.st_size = ' + str(statinfo.st_size) + ' bytes')`
",True,"[-0.32556498 -0.52815676 -0.3470117   0.01424927  0.23353526 -0.06081676
 -0.11030957  0.14136523 -0.0116172  -0.01939464 -0.2008894   0.39469504
 -0.0636638   0.11320741 -0.19186611  0.266234   -0.07859679 -0.10044199
  0.09887929 -0.16981512  0.13391164 -0.11363032  0.1655178   0.06699428
 -0.09999722  0.06891909 -0.24386644 -0.08682612  0.15533805  0.10253332
 -0.19257699  0.24990672 -0.17343251  0.05773936  0.07699829  0.15269186
 -0.22781488 -0.08561143 -0.31991768  0.16899677 -0.10918953 -0.352055
 -0.10652182  0.09069764  0.15278536  0.08515917  0.0152938   0.06309927
 -0.12303891  0.14613585 -0.13006794  0.39167935 -0.27005625 -0.01686563
  0.09147395  0.19150351 -0.00157352  0.26597878  0.05544259  0.18936586
  0.02545544 -0.07247117 -0.1768377   0.03612026 -0.03671243  0.01489719
  0.10319185 -0.11251962  0.54218054 -0.26247182  0.05497589  0.03195571
 -0.15740635  0.25096494  0.04260146  0.16836917 -0.25263253  0.33567703
  0.2661741  -0.36417875 -0.02391249  0.04187813  0.3301118  -0.12504315
  0.22203262  0.03831819  0.45582482 -0.32553315  0.13448383  0.21863389
  0.27698153 -0.27517402 -0.06151311  0.27504984  0.3653441   0.02476995
 -0.09190089  0.2848377  -0.2818448  -0.16501814  0.05163961 -0.03674036
  0.02507109  0.55404675  0.02245406 -0.04536667  0.14152029  0.30225414
  0.04694623 -0.15313719  0.25253224 -0.09543075  0.06428366  0.14995334
  0.08750775  0.19719099 -0.5050231  -0.00478396 -0.15733245  0.5424949
 -0.02458892 -0.23442048 -0.19290935 -0.04108145  0.06885385 -0.20001929
 -0.13555583  0.05138241  0.1664646   0.07166915  0.23855434 -0.10866915
 -0.01790017  0.04551692 -0.12431729 -0.15597571 -0.12482299 -0.13924013
 -0.43042824  0.05840696 -0.01526667  0.07118549  0.14016299 -0.24649522
 -0.05177138  0.42234254 -0.43286353  0.1806294   0.06238876 -0.17841789
 -0.11759989 -0.2546955  -0.2478525   0.10695517 -0.2512583   0.09814155
  0.39231473  0.02747385 -0.09572252 -0.5389941   0.08296762  0.00486092
  0.12027717 -0.28134337  0.5044212   0.0364281  -0.15574981 -0.10016355
 -0.10001668  0.07268928  0.05606434 -0.07413559 -0.28772783  0.02057765
  0.24231827  0.0777784  -0.14369707 -0.50509024 -0.05661027  0.30187178
  0.3920164  -0.07578167 -0.04332678 -0.07244051 -0.01135063 -0.08971006
  0.03273308  0.09000254 -0.03138398  0.1632936  -0.07225583 -0.10522768
  0.02297418 -0.01230749 -0.05386747 -0.11175241  0.23898584  0.16291319
  0.07732157  0.14561325 -0.14729543  0.27114376  0.33019605 -0.18666428
  0.19087559 -0.04437775 -0.36771366 -0.5199356  -0.04890409 -0.10389294
 -0.1332883  -0.22012684 -0.10045112 -0.25540233 -0.33116513 -0.22173704
 -0.15267736 -0.03756134 -0.09113786  0.09803962  0.01012344 -0.0665339
 -0.14811547 -0.32471907 -0.14177036  0.33112127  0.08678316 -0.01329859
 -0.03849772  0.24593873  0.53362966  0.18922862  0.3147179  -0.10048451
  0.12979881 -0.13150617 -0.17749405 -0.15184456 -0.01675266  0.03897611
 -0.41617534  0.08160816 -0.13503017 -0.2643603   0.07650623 -0.3436731
  0.02265578  0.18389507 -0.02829629  0.36173454 -0.19885091  0.26358008
 -0.05561897  0.16774648  0.27866596 -0.01560127 -0.2299355   0.13242885
  0.05371171  0.05421611  0.23093008  0.37538323  0.14307478  0.294626
  0.4582497   0.19537576 -0.10047482  0.15988442 -0.25323758 -0.1227346
 -0.1327518  -0.08648539  0.36349586 -0.08500353  0.28094688 -0.13860682
  0.4686942   0.16973698 -0.13016546  0.01467655 -0.1875189   0.30196604
 -0.50240076  0.16059057  0.087855   -0.10355178 -0.15960142 -0.37308306
  0.00291759 -0.3913083  -0.07181536 -0.1623137   0.19097607 -0.05165023
 -0.12968698  0.00491902 -0.02818143  0.29944173  0.00102846 -0.33331627
 -0.26300055 -0.22408977  0.25559407  0.14975306  0.02292735 -0.26499242
  0.22632909 -0.01415409  0.21640442  0.07452002  0.33737576  0.32827055
 -0.030005    0.20147192  0.04314263  0.21756785 -0.31819558  0.4272834
 -0.06023513 -0.00292667 -0.09343034 -0.38769245  0.06129458  0.20746207
  0.14333874 -0.02847873  0.10312659  0.08668607 -0.19013485 -0.2532478
 -0.01359022 -0.0125931  -0.08618601 -0.2794987  -0.06590284 -0.02250295
 -0.26283276  0.32643834  0.11560237 -0.2832391  -0.05927678 -0.23528427
 -0.30973598  0.00190312 -0.03442281 -0.3741485   0.47231704  0.65519595
 -0.08158553 -0.11584796  0.22639951  0.18905923  0.13292056 -0.00528216
 -0.16839361  0.23857717 -0.00120526  0.02806825 -0.07735276  0.20967597
 -0.2549896   0.00917848 -0.14082992 -0.05828292  0.20113395 -0.15166847
 -0.22465286  0.18011206  0.1414669   0.06480332  0.00696234  0.26588696
 -0.20294201  0.11172754  0.24912974 -0.2910794  -0.16122939  0.1524695
 -0.03760735  0.00090683 -0.4297278   0.01072268  0.34025508 -0.1596028 ]"
Feature deprecated in h5py is used in TF1.5 stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, OS X
- **TensorFlow installed from (source or binary)**: source and binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0 - 7.0
- **GPU model and memory**: GTX1060, GTX 1050Ti 
- **Exact command to reproduce**:
`sudo pip3 install h5py`
run python3, from there, type:
`import tensorflow as tf`


### Describe the problem
A feature of h5py used in TF 1.5 is deprecated, in particular: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.

### Source code / logs
Warning message: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
",True,"[-0.14880742 -0.6166201  -0.33171883 -0.11059929  0.20651618 -0.17200288
  0.08437201  0.02521306 -0.54967046  0.02092742  0.03772402 -0.09721157
 -0.11280078  0.10935388 -0.18574381  0.20819181  0.2980009  -0.24166581
  0.12044283 -0.24455978 -0.0878329   0.04026518 -0.1051829   0.43178117
  0.12281629  0.20377588 -0.29691133 -0.05701278 -0.17697734  0.23314464
  0.4026531   0.29777187 -0.08237015 -0.06323013 -0.12209655  0.34264317
  0.03126565 -0.29896665 -0.18448362 -0.00210861 -0.10630557  0.12119922
 -0.06201515  0.15483327  0.15594041 -0.08633265 -0.01396909 -0.06236691
 -0.28039622 -0.08893146  0.04924621 -0.01270109 -0.3958468  -0.29869032
 -0.131274   -0.15109764  0.10524039  0.180165    0.0127489   0.02559486
 -0.19794916 -0.04452681  0.05231696  0.18061073  0.24592824  0.16456914
  0.2749688  -0.29038584  0.4771224   0.05381712 -0.02262511 -0.04529046
 -0.28956687 -0.21318159 -0.13042802  0.29524466  0.03422642  0.12005116
  0.28121483 -0.38804036  0.247302   -0.09967034  0.32593545 -0.0080811
  0.00602256  0.1873761   0.2965977   0.091702    0.33634904 -0.09241456
  0.3483802   0.08745435 -0.0519904   0.02130868  0.20522386 -0.02762623
 -0.09175205  0.15493572 -0.03578626  0.03554541 -0.1399031  -0.30739012
 -0.06822847 -0.08573632  0.03390472 -0.06075903  0.23386088  0.2634016
  0.22897965  0.08633699  0.08094967  0.03742274  0.19711922 -0.3140791
  0.08640637 -0.1673629  -0.13242674  0.12482528 -0.20503698  0.4489469
  0.21891704 -0.19237974  0.36356294  0.10455377  0.4885761  -0.03987229
 -0.1737664   0.12954542  0.299029   -0.03155224  0.51139677  0.1098541
 -0.27734044  0.32178816  0.02477392  0.02952046 -0.25954628 -0.13730632
 -0.351836   -0.02679747 -0.1286742   0.1455103  -0.16471814 -0.28275555
 -0.19029406  0.26453832 -0.32957572  0.29190084 -0.1006209   0.40472376
 -0.10287032  0.05336269  0.10230266  0.17728704  0.2458765   0.2668826
  0.10415405 -0.05999782 -0.25394043 -0.53674424 -0.17382583  0.18171549
 -0.21859963 -0.26139206 -0.12396117  0.1812071  -0.2747433  -0.16315863
  0.16029035  0.25242895  0.00239596 -0.00707575 -0.08087162  0.3334099
  0.2721768   0.01160203  0.12962669 -0.5066211  -0.02477224  0.22704996
 -0.18869072 -0.07903447 -0.07795327  0.1116586   0.02315039  0.09855021
  0.23093857  0.15936437 -0.12307367 -0.03738127 -0.3185897  -0.14073688
  0.20363627 -0.13288483 -0.15614893 -0.00609964  0.23461255  0.05596131
  0.04416271  0.18370306 -0.17780055  0.03267067  0.11907837 -0.0598742
  0.19926178 -0.27114227 -0.34098712  0.01153775 -0.5417843  -0.09969558
 -0.05460694 -0.26795542 -0.13605297  0.09214339 -0.17152362 -0.15652442
  0.06655053 -0.12815642 -0.10559751 -0.02584347  0.1731756  -0.23458476
  0.02742589 -0.51719075 -0.24041533  0.00227717 -0.22926366  0.20751753
 -0.01913863  0.3242581  -0.05545817  0.05317472  0.2177203   0.21836129
  0.23165177  0.03070233 -0.0420078  -0.25446585 -0.1536833   0.32637846
 -0.54156566 -0.2740041  -0.163414    0.10851178  0.06008182  0.24897434
 -0.17069082  0.09904456 -0.16010955  0.23585403 -0.28001896 -0.10103413
  0.4785974   0.22352594  0.2827378  -0.00808343 -0.12189952  0.17532082
  0.35518223  0.01325546  0.18252891  0.11595643 -0.0601243   0.5439323
  0.24354018  0.17113823 -0.12133147  0.5034406  -0.06020636 -0.1805649
 -0.03631439 -0.18881583  0.32454485 -0.09480903 -0.08919361 -0.13552558
  0.24671228 -0.03243858 -0.03493691 -0.02177471  0.17263015  0.16853757
 -0.26427615  0.06788495  0.03158873 -0.03821044  0.01920872 -0.24039477
 -0.28421384 -0.09219284 -0.0082816   0.2592242   0.04131566 -0.01795431
 -0.01806613 -0.00583217  0.08245667  0.08924839  0.10255304  0.1084815
 -0.1368396  -0.12746611  0.011916    0.15808249 -0.1466502  -0.01275918
  0.4737885   0.19339405  0.5016828  -0.2539401   0.05479762  0.0241059
 -0.15120107  0.48785132 -0.07031261 -0.03265338 -0.30721065  0.6046058
 -0.0042182  -0.10643135  0.06694092 -0.04059857 -0.11916219  0.2622196
  0.00783384  0.01190881 -0.14054736 -0.13989022 -0.03712319  0.14562403
  0.02938364 -0.18513584 -0.28366688 -0.11604616 -0.02905151 -0.27212048
 -0.37668848  0.24664305 -0.16158023 -0.24638642  0.12542783 -0.16275749
 -0.10472522 -0.39060163  0.0015743  -0.1035444   0.40380707  0.42189208
 -0.24607082 -0.09289027  0.22017106  0.07161873 -0.4753942  -0.19186382
 -0.02616376  0.21853805  0.20486173  0.12177327  0.02414236  0.39349544
 -0.3483587  -0.1522997  -0.1258994  -0.08054326  0.0550845  -0.17802666
 -0.16390112 -0.15393409  0.25165164  0.16718708 -0.0822513   0.2737432
 -0.37746084  0.44379956  0.5091473  -0.2647114  -0.21182722  0.1472606
  0.16299853 -0.2336675   0.02365517 -0.08094272  0.19031405 -0.04651804]"
"Cannot opened include file ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"": no such file or directory ","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: current git master branch, should be v1.4.1 or v1.5.0rc1?
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: MSVC2015
- **CUDA/cuDNN version**: CPU build only, gpu function is off
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Trying a minimal build with cmake, with only snappy support and optimize for native arch turned on

### Describe the problem
Build failing due to missing header files ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"".

Everything build succesful except for tpu project. I don't really know how to generate the pb.h file from protoc manually. I trying to fix the problem by chaning .cmake files, but not sure which one is for tpu.

### Source code / logs
133>D:\MSVC-source\tensorflow\tensorflow\contrib\tpu\ops\tpu_embedding_ops.cc(16): fatal error C1083: Cannot open include file: 'tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h': No such file or directory

",True,"[-3.61051500e-01 -8.97848845e-01 -3.24984193e-01  2.18997866e-01
  3.46751690e-01 -3.24246854e-01 -2.01897338e-01 -2.35296831e-01
 -3.26127052e-01 -1.14525221e-01  1.00323826e-01 -6.19257577e-02
 -4.89926726e-01  1.97744578e-01 -8.20336640e-02  3.62496197e-01
  6.00204691e-02 -4.72408056e-01  1.83299929e-01 -6.54842108e-02
  3.76336798e-02 -3.83261628e-02 -1.73991278e-01  2.13366061e-01
  1.47252530e-01  2.25324258e-01 -2.57152200e-01 -2.51809061e-01
  7.14538991e-02  7.18254969e-02  6.65888190e-01 -1.15607768e-01
 -2.32907906e-02  1.01319522e-01  6.21811263e-02  3.21851850e-01
 -8.19677263e-02  6.96089268e-02  4.94488887e-03 -3.20946008e-01
  1.87372178e-01  4.58653942e-02 -6.55016769e-03  1.78636000e-01
 -4.52494211e-02 -1.07250229e-01  1.11028664e-01 -1.00896865e-01
 -1.83257967e-01 -2.14579120e-01 -2.89712325e-02 -1.67352766e-01
 -6.10659599e-01 -2.08099082e-01 -2.15711638e-01 -1.05508596e-01
  1.72873795e-01  1.08136144e-02  4.31535169e-02  1.12090245e-01
 -1.08379297e-01 -4.60629351e-03  2.44195908e-02  1.32062599e-01
  8.01542401e-02  7.83481449e-02  2.24709466e-01 -9.62448586e-03
  6.20040059e-01 -1.08414896e-01  3.22759226e-02 -5.73727675e-03
 -2.91222274e-01  5.58010265e-02  1.82257257e-02  2.12206051e-01
  1.78817026e-02  3.79097104e-01  2.61061907e-01 -1.82456523e-01
  2.06497043e-01  3.08365235e-03  2.47168869e-01 -9.10428166e-02
 -9.94509533e-02  2.28371769e-01  2.64515221e-01  1.69578016e-01
  3.07737529e-01 -2.10362494e-01  4.29532230e-01 -1.88700818e-02
 -5.42540587e-02  7.95024484e-02  3.67801994e-01  3.59890126e-02
  3.75377340e-03  5.07808506e-01 -1.08659282e-01 -2.44865224e-01
 -3.39806825e-01 -3.42116207e-01 -2.93526128e-02 -1.56404525e-02
 -7.12837875e-02  4.03715447e-02  2.44161636e-01  1.34997576e-01
  6.00215681e-02 -3.07478458e-01  2.55545527e-02 -6.34076223e-02
  2.65383303e-01 -2.80925989e-01  1.33851975e-01 -4.60169278e-03
 -2.65134186e-01  2.69737422e-01 -3.24514918e-02  6.78039193e-01
 -1.13679960e-01 -1.83178380e-01  2.14467466e-01  1.14275264e-02
  3.76069397e-01  1.39429197e-02 -2.14659393e-01 -6.17286004e-03
  1.38286054e-01  4.25878987e-02  1.35466993e-01  1.83501765e-01
 -7.17781484e-03  2.42935643e-01  5.98245189e-02 -8.51627588e-02
 -4.48545039e-01  3.37176323e-02 -1.07891217e-01 -1.59038305e-01
 -1.38723671e-01  3.94321531e-01  1.26656806e-02 -3.53991866e-01
  2.41446681e-03 -5.31517044e-02  5.58984801e-02  1.49221271e-01
 -9.85969305e-02  1.58453792e-01 -3.94396782e-01  1.81414522e-02
 -1.10316705e-02  3.12901556e-01  3.32575329e-02  4.36443955e-01
  2.30718791e-01 -1.45557284e-01 -1.82560951e-01 -6.61338925e-01
 -6.63511381e-02  3.81138533e-01 -3.09013665e-01 -1.28695238e-02
 -3.97156551e-03  5.89195080e-02 -4.23292339e-01 -3.86319220e-01
  1.46583170e-01  2.91316032e-01 -1.16732962e-01 -1.21161275e-01
 -1.52668625e-01  2.24397540e-01  2.08695784e-01  2.36065444e-02
  4.15430009e-01 -7.00749815e-01 -2.40764365e-01  2.53268361e-01
  1.82744980e-01 -1.24612659e-01 -3.42126898e-02  6.99189156e-02
  5.16797900e-02 -6.10726234e-03  1.08791754e-01  1.21207222e-01
 -1.88631922e-01  5.51754460e-02 -2.45142967e-01  2.39218269e-02
  2.03440577e-01 -1.32585019e-01 -2.17626929e-01  2.26849675e-01
  3.60575259e-01 -1.96189173e-02 -1.98749065e-01  1.18672945e-01
 -2.46656671e-01 -1.71412498e-01 -8.70515853e-02  1.44781411e-01
  9.14879143e-02 -2.97291547e-01 -2.03767389e-01 -3.41295488e-02
 -5.37931204e-01 -9.67558101e-02  9.00266096e-02 -7.83704072e-02
  5.96609786e-02  2.17222586e-01 -2.88505852e-01 -1.17117725e-03
  1.01665899e-01 -1.67058453e-01 -2.21864879e-01  1.58978581e-01
  3.31949800e-01 -6.84991032e-02  5.94499633e-02 -3.29384297e-01
 -5.68339638e-02 -1.35539353e-01 -3.26662987e-01  9.74539593e-02
 -2.48223487e-02  5.84647894e-01 -1.26655608e-01 -7.12573528e-02
  2.10994333e-01  1.21049665e-01  1.33521318e-01 -4.42321077e-02
  1.01653501e-01 -2.36526489e-01 -1.99911356e-01  2.62247026e-01
 -5.81844568e-01 -1.37566596e-01 -9.29869898e-03  3.67230289e-02
  5.24740107e-03  1.37375221e-01 -1.48602515e-01 -1.34786600e-02
 -2.43321285e-01  3.40954930e-01 -1.35178030e-01 -6.86468184e-03
  3.04240584e-01  4.41242099e-01  2.30146915e-01  2.80641377e-01
 -6.56027496e-02  1.16839811e-01 -8.43436643e-03 -8.05893466e-02
  2.70850658e-01  2.30023146e-01  1.32051855e-02  4.27652895e-01
  2.16991484e-01  4.92394388e-01 -3.08035403e-01  3.71093333e-01
 -1.62072897e-01 -3.94206420e-02 -1.72222659e-01 -1.81563705e-01
  2.07557797e-01 -3.26504469e-01 -2.54485756e-01  3.05879489e-02
  2.75797606e-01 -1.31103605e-01 -9.11749154e-02  9.78979319e-02
  2.57240444e-01  1.13894030e-01 -1.45153150e-01 -1.27506629e-01
  1.87562928e-01 -3.58342603e-02 -1.22474842e-01 -5.07706881e-01
 -1.88033447e-01 -7.77612776e-02 -2.25643422e-02  2.25974470e-01
  9.68188047e-02 -1.02375187e-01 -1.28644764e-01 -1.64169911e-02
  4.82854396e-02  8.69116187e-02  1.16768211e-01  4.13181037e-01
 -1.00939892e-01 -1.65126383e-01  3.99667211e-02  1.03998937e-01
 -2.58802976e-02 -8.48573744e-02  2.70310193e-01 -7.50665516e-02
  5.37279248e-01 -2.51939029e-01  1.37105167e-01  3.26400846e-02
  2.46821754e-02  3.98095131e-01 -2.74321847e-02  3.60771045e-02
 -6.70158863e-02  7.10027933e-01  2.12921575e-01 -1.65228903e-01
  1.14527740e-01 -1.93636417e-01 -1.33505568e-01  8.33626911e-02
 -4.73589450e-02 -3.06842923e-02 -2.04218239e-01 -1.47972092e-01
 -1.56268105e-01  2.78813362e-01 -1.32416502e-01 -1.80254638e-01
 -2.26969913e-01  2.14847684e-01 -3.44094157e-01  7.09225833e-02
 -2.59580106e-01  2.66252756e-01  6.71750605e-02 -3.00657123e-01
  1.19033255e-01 -6.19094186e-02  1.17918868e-02 -2.32217252e-01
 -1.32163297e-02 -3.06627452e-01  3.13633621e-01  2.64622867e-01
 -3.04938734e-01  3.31390023e-01 -1.31113872e-01  8.37806985e-02
 -2.50363201e-01  2.19135676e-02 -1.11709163e-01  3.38624150e-01
  6.34646565e-02  2.59904191e-05  2.52783112e-03  6.01458013e-01
 -1.24522999e-01  6.65419698e-02 -1.44451290e-01 -3.01164985e-01
  1.24660105e-01 -9.42866057e-02 -3.15870941e-01 -2.55045027e-01
  2.03254923e-01  4.39019084e-01  1.40820034e-02  3.58694494e-01
 -3.38789225e-01  2.62476504e-01  6.10389650e-01 -1.88689336e-01
 -5.23412764e-01  3.21512938e-01  1.45596981e-01 -7.68854618e-02
 -5.62401041e-02 -1.05709232e-01 -3.81900668e-02 -1.03079230e-01]"
Dataset from string generator raises Exception with python 2 stat:awaiting response,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS X 10.12.6
- **TensorFlow installed from (source or binary)**: binary cpu version from pypi
- **TensorFlow version (use command below)**: ('v1.5.0-rc0-9-gf9472619f6', '1.5.0-rc1')
- **Python version**: 2.7.14

### Describe the problem
I'm reading text data from file in generator. Encoding: UTF-8.
After some preprocessing i return it in generator manner.
Next, i'm trying to create Dataset from this generator.

Code below produce exception in both Python 2&3 for TensorFlow 1.4.
For TF 1.5.rc1 & Python 3 there is no errors.
For TF 1.5.rc1 & Python 2 error exist.

### Source code / logs
```python
# -*- coding: utf-8 -*-
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf


def generator():
    data = [
        [u'', u'', u''],
        [u'', u'', u'', u'']
    ]

    for seq in data:
        yield seq, [0, 1, 2, 3]


def dataset():
    dataset = tf.data.Dataset.from_generator(
        generator,
        (tf.string, tf.int32),
        (tf.TensorShape([None]), tf.TensorShape([None]))
    )
    dataset = dataset.padded_batch(2, padded_shapes=([None], [None]), padding_values=('', 0))

    return dataset


iterator = dataset().make_one_shot_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
    value = sess.run(next_element)
    print(value)
```",True,"[-1.70890570e-01 -7.31989384e-01 -3.98559928e-01  9.10258368e-02
  2.22255617e-01 -2.10200965e-01 -9.12688375e-02 -5.33074606e-03
 -4.69354212e-01 -9.57959890e-02  1.26997799e-01 -1.23658240e-01
 -1.76722586e-01  5.48328720e-02 -2.07544088e-01  2.82770842e-01
 -9.66654718e-02 -2.96235919e-01  1.92176655e-01 -6.17067851e-02
 -1.72266103e-02 -8.07863474e-02 -1.86697707e-01  1.85633585e-01
  5.42614348e-02  2.47813225e-01 -3.89601409e-01 -6.31212592e-02
  2.73099728e-03  1.34621650e-01  4.96226698e-01  7.93299749e-02
  2.53893360e-02  6.66280687e-02  2.58975420e-02  4.36334044e-01
 -9.05691013e-02 -1.34368539e-01 -2.09782422e-01  6.19843602e-02
  5.75957373e-02 -2.69242302e-02  6.14079200e-02  1.91690046e-02
 -4.19625547e-04 -3.06104813e-02  2.03899443e-02  5.80659546e-02
 -1.00268215e-01 -1.84862673e-01 -8.95244926e-02  7.03076497e-02
 -2.39076883e-01 -3.29064667e-01 -9.85933542e-02 -8.93273652e-02
  1.68391407e-01 -2.89649963e-02 -1.37891352e-01  2.21019030e-01
  3.05052456e-02 -1.16689622e-01  4.73269895e-02 -5.72213680e-02
  5.42507619e-02  1.50886461e-01  3.10735881e-01 -3.08346272e-01
  6.36725962e-01 -1.77655652e-01  1.24595843e-01 -1.07664317e-02
 -3.42124701e-01 -5.11252917e-02 -1.23046771e-01  2.85985559e-01
 -1.48072066e-02  3.61867025e-02  2.25021467e-01 -2.21718237e-01
 -3.63942012e-02 -1.49456292e-01  3.57233971e-01 -1.31609350e-01
  8.14054534e-03  1.66323483e-01  3.42433780e-01  1.28746033e-01
  2.84675747e-01  1.83984756e-01  4.34522212e-01  9.84907001e-02
  3.05136256e-02  9.30371583e-02  3.49932194e-01  2.11169779e-01
  4.18536924e-03  3.50046903e-01 -3.47013585e-04 -9.82072651e-02
 -4.66236006e-03 -2.45710835e-01 -9.00715292e-02  1.52882040e-01
  1.94329262e-01 -1.13905355e-01  1.70405865e-01  5.39324209e-02
  8.70319307e-02 -1.25421301e-01  1.38083428e-01  7.36467615e-02
  1.46592617e-01 -8.45624804e-02  8.34892988e-02 -1.25202954e-01
 -1.93792075e-01  1.56899571e-01 -9.16477740e-02  7.35235691e-01
  2.89087407e-02 -1.58040933e-02  2.84853309e-01  1.93872437e-01
  2.88856268e-01  1.50905460e-01 -1.93295240e-01 -5.83397262e-02
  3.99515256e-02 -1.02446437e-01  3.77510786e-01 -1.26626506e-01
 -1.57142699e-01  1.75820574e-01 -2.66916871e-01 -1.90878510e-01
 -2.13393793e-01  7.21906647e-02 -1.61989927e-01 -2.32770681e-01
 -7.90545046e-02  6.47531003e-02  5.65966815e-02 -3.36970985e-01
  5.66851720e-02  8.20062459e-02  2.99722468e-03  2.20331490e-01
 -5.78816980e-05  5.06125242e-02 -9.08371881e-02  1.76579639e-01
 -2.31130987e-01  4.37774092e-01  1.55057639e-01  3.23928535e-01
  3.12887490e-01 -4.51812670e-02 -4.18362767e-02 -6.95205927e-01
 -1.39427811e-01  2.25721359e-01 -1.57366678e-01 -2.83162922e-01
  1.67598784e-01  8.91065523e-02 -3.24646413e-01 -1.27359137e-01
 -1.02871016e-01  2.22627237e-01 -3.52871977e-02  3.45784873e-02
 -8.03189427e-02  9.94234383e-02  3.18120196e-02 -1.85755379e-02
  1.85528040e-01 -5.95685959e-01 -3.86798233e-02  2.18314588e-01
  2.00012922e-01 -1.07531816e-01  3.35782915e-02 -3.14627662e-02
 -3.63769084e-02  4.63408977e-02  6.91631287e-02  1.43660575e-01
 -1.67586774e-01 -4.83923592e-02 -2.42517233e-01 -5.51440418e-02
  2.67314672e-01 -1.08633041e-01 -1.47625655e-01 -1.73821822e-01
  1.54376388e-01  2.19061404e-01  4.18008119e-02  2.39396945e-01
 -2.50617266e-01 -1.18043885e-01  4.02075499e-02  5.25358245e-02
  1.17434591e-01 -2.92502880e-01 -4.37674522e-01 -1.23047061e-01
 -4.03167695e-01  1.90617442e-01 -1.18809201e-01 -2.11916268e-01
  1.26450062e-01  1.98310480e-01 -3.06183726e-01 -2.19979495e-01
  1.03708766e-01 -1.51274562e-01 -2.46926188e-01  9.08072218e-02
  4.77298200e-02 -1.92589372e-01 -8.50333199e-02 -2.42916167e-01
 -7.56115839e-03  8.21523145e-02 -1.54073879e-01 -5.34001254e-02
  1.93065599e-01  1.87860399e-01 -7.07774535e-02  1.85534686e-01
  1.41446069e-01  8.10071453e-02  1.36767492e-01 -1.09953314e-01
  8.60655382e-02 -2.49869809e-01 -1.83427840e-01  4.11938369e-01
 -5.84311008e-01 -1.12698324e-01 -7.77467266e-02  6.78618019e-03
  2.12145336e-02  1.39191598e-01 -1.32630244e-01  8.83245692e-02
 -1.82532191e-01 -2.09311172e-02 -1.41598433e-01 -8.83667842e-02
  3.25149775e-01  1.02371827e-01  2.81931639e-01  2.95923710e-01
 -9.29338783e-02 -1.51438322e-02  1.66497797e-01 -2.23893046e-01
  1.70373678e-01 -2.23284364e-02 -1.59491953e-02  4.47445512e-01
  1.73509493e-01  2.64153630e-01 -2.00164497e-01  4.23225105e-01
 -7.87738152e-03 -7.96524137e-02  3.19039524e-02  3.26336399e-02
  2.88029015e-01 -2.60173231e-01 -1.15378067e-01  2.60784850e-02
  4.50130939e-01  7.62997493e-02 -5.22595569e-02 -2.20830575e-01
  1.45614684e-01  8.66039470e-02 -3.70889992e-01  5.86446375e-03
 -2.00028479e-01 -2.63878822e-01  3.86269018e-02 -2.33829081e-01
 -1.85386389e-01  1.82761267e-01 -7.34727159e-02  1.13291539e-01
  2.11889029e-01 -3.29702310e-02  2.37368606e-02 -4.57371213e-03
 -1.94517933e-02  7.08635710e-03 -1.11149810e-01  2.87927032e-01
 -2.89961606e-01  1.68963186e-02  2.48629123e-01  7.54500479e-02
  6.21590503e-02 -1.38440341e-01  5.49159586e-01  5.04290462e-02
  4.35641170e-01 -2.63333470e-01  3.32283616e-01  3.82836834e-02
 -2.36782864e-01  3.72526586e-01  4.25131992e-04 -1.48023248e-01
 -3.78771216e-01  7.01853693e-01  3.59309435e-01 -7.66018033e-03
 -1.28301561e-01 -1.49080858e-01 -2.26145923e-01  1.13181040e-01
  2.33127475e-01  1.73911154e-01 -2.73341358e-01 -3.24428022e-01
 -1.20693699e-01  1.21248826e-01 -8.03889632e-02 -1.40652403e-01
 -2.38565534e-01  1.24779135e-01  4.71161073e-03 -1.69392154e-01
 -2.84978330e-01  2.95252234e-01  8.95891711e-02 -1.31432429e-01
  7.23409504e-02 -2.74953127e-01 -1.42839719e-02 -4.79868233e-01
 -1.74960926e-01 -2.74446487e-01  3.50913942e-01  4.27581310e-01
 -2.61212766e-01  9.05059278e-02  1.19847670e-01 -8.76857415e-02
 -1.83452129e-01 -1.30664229e-01 -1.00366287e-01  3.12012196e-01
 -1.36206210e-01 -8.29673335e-02  7.76490867e-02  3.35674822e-01
 -2.86183417e-01 -1.23028174e-01 -2.24062979e-01 -2.02683926e-01
  8.12636223e-03 -1.98783427e-01 -4.23115373e-01 -1.96185429e-02
  4.69210953e-01  2.33517617e-01 -1.79832637e-01  2.27882296e-01
 -1.03047490e-01  4.94513631e-01  5.72988510e-01 -7.94196129e-02
 -3.15790892e-01  1.04052052e-01  1.56785429e-01 -1.03421971e-01
  4.81582657e-02 -1.04770735e-01  7.98720345e-02 -2.12283626e-01]"
Exception when not providing optional parameter frequency_skip in TimeFreqLSTMCell stat:contribution welcome type:bug,"### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution: 
- TensorFlow installed from: `pip3 install --user tensorflow-gpu`
- TensorFlow version: 1.4.1
- Python version: 3.5.2
- CUDA: 8.0
- GPU: NVidia Titan X

### Describe the problem

Using a `TimeFreqLSTMCell` in a `dynamic_rnn` or `static_rcnn` without providing the optional parameter `frequency_skip` results in an exception:

```
TypeError: unsupported operand type(s) for /: 'int' and 'NoneType'
```

The line which throws this exception is https://github.com/tensorflow/tensorflow/blob/8b78c23c161c9d0bec462d5f4c73f0fca413bc8b/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L474-L475
`frequency_skip` has it's default value `None` here.

Maybe the default should be changed to `1`?

### Source code / logs

Sadly I am not allowed to share my full source code. However, this is how I create the RNN layers:

```
lstmcell = tf.contrib.rnn.TimeFreqLSTMCell(lstm_input.shape.as_list()[2], forget_bias = self.lstm_forget_bias, feature_size = lstm_input_rev.shape.as_list()[2])
                
stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstmcell] * self.layers_lstm)
                
lstm_output, lstm_state = tf.nn.dynamic_rnn(stacked_lstm, lstm_input_rev, dtype=""float32"", time_major=True)
```",True,"[-7.69616850e-03 -5.27545571e-01 -1.69924840e-01  1.53714493e-02
  1.86415359e-01 -1.69562325e-01 -6.63048849e-02 -9.36253518e-02
 -2.01385513e-01 -2.00447127e-01 -9.14095640e-02 -9.96741280e-02
 -2.21001551e-01  5.80136552e-02 -1.00716524e-01  9.15762782e-02
 -2.90999860e-02 -1.28694326e-01  1.87240452e-01 -9.06641930e-02
 -7.80389607e-02 -1.35660306e-01 -2.53454506e-01  2.42184684e-01
 -2.90320236e-02 -7.05001317e-03 -2.71713346e-01  1.12237878e-01
  1.52156338e-01  1.87210411e-01  2.29715571e-01  3.03645581e-01
 -4.52079736e-02 -1.08771801e-01 -8.10043588e-02  1.42088473e-01
 -2.89624751e-01 -1.52902603e-01 -2.51227856e-01  1.01401888e-01
  1.32836491e-01 -4.36557345e-02 -2.53426135e-02  7.89708737e-03
 -4.41891067e-02 -1.94517747e-02  1.74693346e-01 -5.29085472e-02
 -1.49782360e-01 -5.74217439e-02 -1.17500275e-01  8.54979381e-02
 -2.54128516e-01 -2.58219838e-01 -1.14166334e-01 -1.28087774e-01
  1.68303341e-01  6.96972758e-02 -5.46430312e-02  1.81200087e-01
  3.44194099e-02 -1.21784359e-01 -1.83440149e-01 -7.46354312e-02
 -1.58470899e-01  9.90575626e-02  2.87224621e-01 -7.22698346e-02
  3.62357855e-01 -7.04847127e-02 -3.92115721e-03  2.45186333e-02
 -3.26701164e-01 -1.43940836e-01  1.24603733e-02  2.53424108e-01
 -7.09152073e-02  2.23798096e-01  2.30168775e-01 -2.66194046e-01
  6.32991791e-02  1.00347891e-01  1.47803634e-01 -5.28017543e-02
  3.06506222e-03 -8.05013403e-02  2.18120992e-01  2.84590364e-01
  1.60095036e-01 -2.16064572e-01  4.65629160e-01  2.11156040e-01
 -6.74551576e-02  8.94478559e-02  5.21887302e-01  1.30808473e-01
  3.29887122e-02  2.50450194e-01 -7.39852265e-02 -6.93872720e-02
 -5.69330677e-02 -6.58382475e-02 -3.17877352e-01  3.81805301e-01
  7.86524862e-02 -1.00068431e-02  1.58241466e-01  1.22900724e-01
  1.22292349e-02 -1.21517748e-01  1.50195062e-01  1.17802233e-01
  1.75816715e-01 -9.65837315e-02  7.34290481e-02  1.03724733e-01
 -1.59711704e-01 -1.82070658e-02  1.38591886e-01  5.73287845e-01
 -1.88135028e-01 -1.68685317e-01  1.53394744e-01 -2.84669865e-02
  1.43144488e-01  6.23187311e-02  8.86636823e-02 -4.04097885e-03
  3.30398083e-02 -1.04629129e-01  2.52008557e-01 -2.99510062e-02
 -2.76863985e-02  1.16553515e-01 -4.82961163e-02 -1.55556217e-01
 -3.15896235e-04 -4.01059985e-02 -6.85142577e-02 -1.70193523e-01
 -1.33744195e-01  6.65976331e-02 -1.23009361e-01 -2.33567446e-01
  9.53377038e-02  2.19167233e-01 -8.73828456e-02  1.17415302e-01
  1.12941012e-01 -1.92651585e-01 -4.94993925e-02  3.54525521e-02
 -2.53247730e-02  5.55468023e-01  1.48188978e-01  1.92561448e-01
  2.19285727e-01 -3.12788263e-02  1.35813087e-01 -4.70484287e-01
 -1.58248246e-01  1.88243985e-01 -2.64201969e-01 -1.28288284e-01
  1.25929862e-01  1.68756247e-01 -2.92647332e-01 -1.71433508e-01
  4.08152007e-02  1.65172130e-01 -2.33212605e-01 -7.71926641e-02
 -1.03955373e-01  6.74775317e-02  1.29024476e-01 -1.93251476e-01
  2.34557465e-01 -4.17674452e-01 -6.84125125e-02  1.44930720e-01
  1.00300595e-01  3.51815224e-02  1.05621248e-01  7.43987039e-02
 -1.40511185e-01  8.52579549e-02  3.44249979e-02  6.88602924e-02
  1.02070078e-01  2.95308735e-02 -3.54388714e-01 -1.94161519e-01
  3.01567733e-01  2.03742944e-02 -1.13888606e-01 -6.53238744e-02
  3.30869615e-01 -6.82855546e-02  1.38933480e-01  1.20838374e-01
 -1.81695879e-01  6.99653476e-02  1.24700088e-03  8.87755156e-02
  1.65038213e-01 -2.47984409e-01 -1.46598769e-02 -3.01349849e-01
 -2.44013190e-01 -1.44953057e-02  6.22091256e-03 -1.57599181e-01
  8.58856216e-02 -1.09551266e-01 -2.05061436e-01  6.25881925e-02
 -2.90157080e-01 -1.48490462e-02 -2.85761237e-01  1.25979543e-01
 -6.54883534e-02  4.57121395e-02 -6.77446499e-02 -1.39686227e-01
 -8.39301497e-02 -2.38941293e-02 -1.68459207e-01 -8.01235810e-02
  1.14549786e-01  2.14703903e-01  2.89932191e-02  4.35261577e-02
  3.85275781e-01  9.18025672e-02  1.31323248e-01 -1.78845033e-01
 -4.60726209e-02 -1.41979590e-01 -2.32821330e-01  9.64325070e-02
 -3.41001213e-01 -2.04465121e-01 -3.58891673e-02  5.02709858e-02
  9.67406258e-02  4.15052287e-03 -1.84467241e-01 -5.21150008e-02
 -2.32309848e-01  3.88459712e-02 -1.07500508e-01 -2.43097991e-01
  1.40184075e-01  2.54467595e-04  2.39253134e-01  1.74275696e-01
 -3.47376652e-02  2.25727275e-01  2.46064708e-01  6.68034777e-02
  8.45149383e-02  2.00090289e-01 -2.58220285e-02  6.23736799e-01
  3.68010432e-01  2.98419297e-01 -8.36140811e-02  4.31152899e-03
 -7.29241669e-02  8.52217823e-02 -1.38245702e-01 -1.89298809e-01
  1.59533128e-01 -2.34415621e-01  3.40116732e-02 -1.07100576e-01
  4.01880622e-01  1.83756053e-01  7.04006702e-02  1.15199015e-03
  1.86510026e-01  2.20244944e-01 -2.62964547e-01  4.27338481e-02
  7.03044608e-03 -2.37065941e-01 -4.12644595e-02 -2.32263148e-01
  7.26880282e-02  1.55367374e-01 -9.66103971e-02  2.88631991e-02
  2.53327847e-01  4.12315642e-03 -1.15909413e-01  1.94996282e-01
 -1.28967598e-01 -2.96254512e-02  6.51823170e-03  1.01786003e-01
 -7.63389170e-02  5.50301298e-02  2.05087155e-01 -2.28730232e-01
 -9.41999629e-02 -6.43175691e-02  3.91031563e-01  1.89359620e-01
  2.14550272e-01 -2.85550952e-01  3.64566684e-01  1.89717457e-01
 -1.69618428e-01  3.99198264e-01  1.97342411e-02 -1.08082853e-02
 -2.62093037e-01  4.33635801e-01  7.50568062e-02 -6.13674521e-04
 -4.84869778e-02 -2.10710481e-01 -2.02779502e-01  1.54751465e-02
  1.26415163e-01  2.62368042e-02 -2.45702922e-01 -1.84177995e-01
 -2.68096328e-01  1.19334742e-01  4.06527966e-02 -1.63787618e-01
  5.24501652e-02  1.56776235e-01 -2.18946487e-04 -1.56151354e-01
 -2.68086493e-01  2.80007750e-01  8.84831101e-02 -2.87616491e-01
 -4.94912602e-02 -1.16329640e-01  1.30508125e-01 -3.75530064e-01
 -9.56677943e-02 -2.93949723e-01  2.09889174e-01  4.53220606e-01
  9.46218669e-02  1.88264307e-02  3.76151465e-02  3.61791365e-02
 -2.82797962e-01 -1.98961198e-01 -2.24285811e-01  2.45567426e-01
 -1.06797023e-02 -1.53340966e-01  6.47097528e-02  1.24145590e-01
 -2.09470004e-01 -4.75710109e-02 -1.62306264e-01 -2.17625171e-01
  2.09827542e-01 -1.53368980e-01 -2.74750292e-01 -4.56039682e-02
  1.69721782e-01  2.25835934e-01 -1.12650856e-01  2.98827589e-01
 -1.27228573e-01  2.34320924e-01  3.95728976e-01 -1.40177995e-01
 -1.99604183e-01  9.50649306e-02  1.65731177e-01 -2.46477485e-01
 -2.83120930e-01  5.47315255e-02 -1.80978253e-02 -1.34445041e-01]"
Feature Request: clarify supported environments for official binaries. ,"As it stands now, binary release of TensorFlow 1.5 is set to drop compatibility with Ubuntu 14.04 ( https://github.com/tensorflow/tensorflow/issues/15777), and compatibility with Debian Linux distros, such as Amazon Linux AMI (`ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found`).

To avoid surprise, TensorFlow should either:
1. Follow other open-source projects like Ray/PyTorch and provide official binaries for these systems
or
2. Document that support is dropped, to encourage other players (ie, AWS) to take over the job of providing these binaries

@martinwicke",True,"[-1.93670928e-01 -6.96169138e-01 -2.92772323e-01 -1.57914311e-01
  3.21340293e-01 -1.77979529e-01 -2.64139295e-01 -2.99961299e-01
 -3.47180337e-01 -2.78122842e-01  1.81852818e-01  1.61291957e-01
 -2.03017503e-01  1.47387952e-01  5.67911305e-02  8.75732396e-03
  1.05724849e-01 -3.28768730e-01  1.78310663e-01  1.11596040e-01
 -2.38404736e-01  3.71111929e-02 -1.71920061e-01  2.09721804e-01
  2.26436839e-01 -3.51512469e-02  6.28980063e-03 -5.49995638e-02
 -8.05197433e-02 -2.06458271e-01  6.64672971e-01  2.47195303e-01
 -1.95670664e-01 -1.33297145e-01 -1.46970689e-01  3.78741026e-01
  1.03831023e-01 -3.99483681e-01 -2.00360194e-02  1.46957114e-01
 -6.26760721e-02  2.29761153e-01  5.87354451e-02 -3.67059782e-02
  1.13130786e-01 -7.34429061e-03  8.97387490e-02 -4.08828892e-02
 -9.13051516e-02 -2.44741082e-01  5.46770319e-02 -1.35084614e-01
 -9.28544849e-02 -2.20586479e-01  6.22957870e-02  1.70894265e-02
  4.01078463e-02  2.37040192e-01 -1.33949444e-01  1.16547212e-01
  8.44852626e-02  1.23184741e-01 -1.36073574e-01  1.05113357e-01
  7.67182559e-02  2.10981205e-01  4.68422979e-01 -5.09111047e-01
  6.99840605e-01 -7.70900548e-02 -6.38637468e-02 -1.59656703e-01
 -2.59833783e-01 -2.72466749e-01 -1.64761171e-01  4.43158984e-01
  1.78711221e-01  2.62910128e-01  1.81999624e-01 -1.04497164e-01
 -1.59166157e-01 -8.82805064e-02  2.83529788e-01 -1.57237932e-01
 -1.19276024e-01  8.71573668e-03  1.80183113e-01  1.69166654e-01
  5.03517576e-02 -1.73549458e-01  3.37900341e-01  2.64051706e-01
  4.71739098e-03 -9.48250368e-02  1.83675185e-01  1.68299943e-01
 -6.55562207e-02  2.46678054e-01  2.21982703e-01 -3.63461152e-02
 -2.08382428e-01 -3.73967707e-01 -2.07583845e-01  1.36096060e-01
 -1.31646305e-01 -7.70441592e-02  4.78055894e-01 -1.15529252e-02
  2.62330472e-01 -9.03875679e-02  1.54969603e-01  1.64975226e-02
  2.80501366e-01  2.25214027e-02  1.68276921e-01  1.42039746e-01
 -1.20450065e-01  1.85243458e-01 -6.80392608e-02  6.48335934e-01
  1.84678100e-03 -6.90276325e-02  5.47909677e-01  8.23614746e-02
  3.42369258e-01  1.77935332e-01 -2.60582507e-01 -1.43445343e-01
 -5.90885393e-02 -2.07163826e-01  1.69248328e-01 -4.55387458e-02
 -1.57611296e-01  2.23014474e-01  1.28032584e-02  1.23263318e-02
 -1.28636450e-01 -1.63142830e-01 -1.47831798e-01 -6.72830343e-02
 -2.76985139e-01  2.55907327e-01 -5.64580485e-02 -5.76777160e-02
 -6.07150719e-02  2.53842533e-01  1.00104019e-01  4.84924056e-02
  2.53129266e-02 -1.19110428e-01 -3.69482875e-01  2.34385077e-02
  2.38874257e-01  3.34673136e-01  3.58191013e-01  4.52795506e-01
  2.51636446e-01 -2.63647288e-01 -3.59637797e-01 -3.41441572e-01
  1.23514012e-02  4.00507271e-01 -4.56836164e-01 -4.28996794e-02
 -2.40391091e-01  2.60714352e-01 -2.62684166e-01 -2.35593483e-01
  1.74977109e-01  2.66103327e-01 -3.15977454e-01 -2.21656524e-02
  2.29965955e-01  3.92940082e-02  3.35637778e-01 -1.03074394e-01
  4.52850223e-01 -6.28869414e-01 -5.99156171e-02  2.09496796e-01
 -1.85576100e-02  2.22975351e-02  6.06055483e-02  1.48652911e-01
 -8.95614624e-02 -6.28307611e-02 -5.88115901e-02 -4.55255508e-02
 -9.00584906e-02  1.24964535e-01 -2.29378864e-01 -1.36710227e-01
  1.72711074e-01 -1.25583321e-01 -2.27164868e-02  4.09013703e-02
  1.99156359e-01  1.28077611e-01 -3.11008602e-01  1.27664149e-01
 -1.07949838e-01 -1.15596414e-01 -8.64767581e-02 -4.28943299e-02
 -8.43044221e-02 -3.70369405e-01  6.26348108e-02  5.62919267e-02
 -7.17087984e-01 -1.83210075e-01 -5.25261573e-02 -1.30466580e-01
 -1.89689815e-01  2.19856381e-01 -3.56047928e-01 -4.06613462e-02
  1.00853831e-01 -9.87839848e-02 -8.42608660e-02  2.29570359e-01
 -3.81614361e-03 -2.81604171e-01  8.20847303e-02 -2.50449777e-01
 -1.15220301e-01 -4.00684088e-01 -2.10665822e-01  2.57695079e-01
  7.42938295e-02  3.79754096e-01 -1.22731917e-01  1.53309330e-01
  5.07766366e-01  1.67669892e-01 -1.55944703e-02 -1.36150597e-02
 -8.44608024e-02 -1.17135063e-01 -2.23817348e-01  1.38438851e-01
 -2.88841069e-01 -1.47563994e-01  3.07600379e-01  1.25015885e-01
  6.77869916e-02  4.86371517e-02 -1.19247399e-01 -2.53768861e-01
 -1.61776617e-02  2.02190831e-01  9.03666578e-03 -3.07863772e-01
  2.47584790e-01  1.96788281e-01  1.70892805e-01  1.66201681e-01
 -1.68954223e-01  3.30665186e-02  2.97253609e-01  3.60622257e-03
 -1.17815509e-01  2.36801878e-01 -2.71159619e-01  4.78045940e-01
  1.44686982e-01  4.76512462e-01 -3.76217782e-01  4.39868867e-01
 -4.35866462e-03 -8.56810734e-02  8.92614797e-02  1.05023719e-02
  3.08259845e-01 -4.07095432e-01 -1.37309656e-02  1.64241940e-01
  2.37517580e-01 -6.65491223e-02 -9.07835439e-02 -5.07329367e-02
 -3.06888297e-03  1.18861347e-01 -3.36493641e-01  3.55261005e-02
 -7.24899843e-02 -1.89070940e-01  7.21812248e-05 -4.25395548e-01
 -2.78106213e-01  1.54246157e-02 -1.98391184e-01  3.70813012e-01
  3.36910695e-01  2.20150799e-01  2.00314522e-01 -7.77077898e-02
  1.40538335e-01  1.52469277e-01  1.61649868e-01  3.67818654e-01
 -2.17641015e-02 -6.81913942e-02  8.40599835e-02 -1.46200344e-01
 -5.17720655e-02  1.03619471e-01 -1.29703134e-02  1.27867028e-01
  5.63286424e-01 -1.21412627e-01  1.79027021e-01  1.04661077e-01
 -6.25971779e-02  3.12939078e-01 -4.93046381e-02  6.22774176e-02
 -2.73781210e-01  5.68902254e-01 -1.11028170e-02 -9.91218239e-02
  2.31857121e-01  5.24967238e-02 -3.29844654e-01  4.43303213e-02
 -1.73460450e-02  8.28478336e-02 -3.12470764e-01 -1.91774249e-01
 -2.59791911e-01  1.96418613e-01  4.65031713e-02 -2.27152914e-01
 -2.16561630e-01  2.16780245e-01 -1.13056406e-01  1.00756988e-01
 -3.87158722e-01  3.30052406e-01 -8.52231681e-02 -2.38090694e-01
 -2.58917008e-02 -8.79909694e-02 -1.54833734e-01 -4.94113386e-01
 -1.76743090e-01 -1.01762578e-01  3.54118556e-01  2.44558826e-01
 -2.99112707e-01  4.28227447e-02 -1.34388894e-01  1.21409237e-01
 -4.57654923e-01 -6.89997897e-02  1.90280601e-01  2.40595415e-01
 -2.44581744e-01  1.60796106e-01  2.54364312e-01  4.66112643e-01
 -2.69900292e-01 -1.20900676e-01 -2.14692980e-01 -2.53505111e-01
  6.13484047e-02 -2.74072498e-01  2.97872834e-02 -1.51529282e-01
  1.03926897e-01  4.90706503e-01  8.21796209e-02  1.58463940e-01
 -3.92382830e-01  3.20990533e-01  5.45309663e-01 -1.30104423e-01
 -4.50979829e-01 -5.15117124e-02 -1.94165856e-02 -1.86541215e-01
 -1.82048142e-01 -1.09204218e-01  2.47632176e-01 -2.25298345e-01]"
Imperfect implementation of tf.losses.mean_pairwise_squared_error stat:awaiting response,"### System information
- **TensorFlow version**: 1.4.0, 1.4.1, and 1.5.0-rc0 (checked)
- **Have I written custom code**: N/A
- **OS Platform and Distribution**: N/A
- **TensorFlow installed from**: N/A
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
The implementation of `tf.losses.mean_pairwise_squared_error` looks imperfect.
For example, as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error)
> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are three pairs of differences are summed to compute the loss: loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3

let me put the following data as `labels` and `predictions`:
```
labels = tf.constant([[0., 0.5, 1.]])
predictions = tf.constant([[1., 1., 1.]])
tf.losses.mean_pairwise_squared_error(labels, predictions)
```
In this case, the result should be `[(0-0.5)^2+(0-1)^2+(0.5-1)^2]/3=0.5`, but tensorflow returns different value 0.3333333134651184.

### Suggestion to fix the source code
[tensorflow/python/ops/losses/losses_impl.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py)

If the loss function `mean_pairwise_squared_error` measures the differences between pairs of corresponding elements of `predictions` and `labels` as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error), here is a simple patch:
> (lines 520-521 need to be changed as)
> `term1 = 2.0 * _safe_div(sum_squares_diff_per_batch, num_present_per_batch-1)`
and
> (lines 525-526 need to be changed as)
> `term2 = 2.0 * _safe_div(math_ops.square(sum_diff), math_ops.multiply(num_present_per_batch, num_present_per_batch-1))`
  ",True,"[-0.61391425 -0.72146434 -0.28868735  0.10669812  0.06777246 -0.12351248
  0.14764251  0.07322094 -0.1557959  -0.11450107  0.28305638 -0.32870615
 -0.16094078  0.07525099 -0.4743792   0.15749215 -0.22637384 -0.30588314
  0.08686662  0.22406647 -0.24128495 -0.08529864 -0.38306504  0.5572078
  0.20581865  0.14226633 -0.27301738  0.15216546  0.13782312  0.08777064
  0.37246758  0.14083397 -0.14989838  0.04931635  0.00194225  0.14904201
 -0.1619862  -0.07519101 -0.27488917  0.02620901  0.16986759  0.0145124
  0.00488486 -0.06257942  0.0954339  -0.03654059 -0.05758465 -0.07269934
 -0.17601395 -0.08746542  0.0611307   0.19936875 -0.38394043 -0.22322622
  0.02943262 -0.11201835  0.10031102 -0.02239315  0.11913069  0.19891009
  0.3112309  -0.07527804 -0.2280224   0.01565677  0.0801741   0.24800336
  0.27995777 -0.3059437   0.34840047 -0.09257253  0.10468571 -0.12425652
  0.00085718 -0.07000399 -0.13756272  0.05771759  0.01465309  0.24658738
  0.1004164  -0.19954833 -0.01851818 -0.06427544  0.05967924 -0.07207366
 -0.05820105 -0.03088506  0.32193375 -0.0080306   0.498625   -0.32657668
  0.6090395   0.2413031   0.0847967   0.13419531  0.408022    0.13416983
 -0.11530606  0.21503675  0.14068113  0.25212568 -0.09747189 -0.14561336
 -0.11040499  0.03122514 -0.1589281  -0.15312056 -0.01617768 -0.10374592
  0.14799958  0.06630825  0.08158006  0.02180812  0.41339484  0.22552145
  0.09309553  0.10678487 -0.12486827  0.15623893  0.18672295  0.4882462
 -0.02072    -0.12538062  0.26842016  0.10306503  0.40773284  0.13179389
 -0.10597109  0.05987802  0.20858717  0.0521998   0.24465963  0.0744139
 -0.3418057   0.1347371  -0.29749012 -0.12655827 -0.22068997  0.22026974
 -0.17824948  0.11881948 -0.26421827  0.29110694 -0.02661395 -0.2933562
  0.25215942 -0.10136017 -0.07276604  0.4411943   0.03793941  0.26016194
 -0.00734592  0.13068525  0.08206394  0.24134165  0.09172729  0.16045369
  0.484285   -0.00992706  0.0226623  -0.72563314  0.11279973  0.05360392
  0.02217945 -0.12638669  0.15461233  0.25027326 -0.44451135 -0.2969366
  0.20263563  0.26248318 -0.12678277 -0.08279654 -0.13868138  0.37078947
  0.1383861  -0.16696018  0.2408582  -0.5922916  -0.17773356  0.07992877
 -0.13102008  0.02241426  0.08836149 -0.07083099  0.04401516  0.12858582
  0.06887742  0.28174496 -0.18363973 -0.25101864 -0.28032678 -0.24846625
  0.26773655  0.15647464 -0.1375774  -0.01092676  0.22258247 -0.37720355
  0.05093605  0.27671748 -0.30725825 -0.15228787  0.07267614 -0.40813053
  0.1881257  -0.15087375 -0.15893045 -0.35568497 -0.3935663   0.24799468
 -0.04887764 -0.6244277  -0.10717919 -0.0437227  -0.33038586 -0.19021854
  0.06544723 -0.2778458  -0.18528159  0.32782283  0.33941954 -0.17823738
  0.02011472 -0.45850036 -0.5389978   0.06509887 -0.14833084  0.14248255
 -0.07028274  0.05262046  0.03795138  0.17221819  0.12543912 -0.05852003
  0.3418095  -0.1730317  -0.06705813 -0.2742864   0.04049518  0.1091617
 -0.6050631  -0.3158736  -0.04638744 -0.2449361   0.21053372  0.09958188
 -0.14357375  0.01596315 -0.22437197  0.61352795 -0.215712   -0.01699233
  0.3214038   0.23062794  0.31296715  0.12168882  0.06328963 -0.00674747
  0.31649852  0.00582057  0.357215    0.29374832 -0.1531164   0.5489908
  0.31012583  0.3352631  -0.07175152  0.15506402 -0.17144822  0.07336597
  0.0175761  -0.3062358   0.49958378 -0.2426621  -0.06352772 -0.14598502
  0.5570818   0.16867922 -0.17981184  0.1562969   0.3723293   0.3509126
 -0.16200359  0.10079074 -0.1751123  -0.1301419  -0.25918156 -0.26735026
 -0.1398508   0.06881934 -0.15577197 -0.16705686  0.12954515 -0.10643734
 -0.232567    0.15805757  0.01401721 -0.02483798 -0.01579405  0.10678021
 -0.20158485  0.05041774  0.38955313  0.00989553 -0.12029997  0.00774951
  0.62470216  0.28650105  0.39802027 -0.15441369  0.36973622 -0.2998813
 -0.0024215   0.44810903 -0.11066407  0.03154503 -0.11388578  0.43051487
  0.34159338 -0.1376907   0.27062684 -0.3789799  -0.28662193 -0.07782378
  0.11314786 -0.28661853 -0.04209815 -0.39367703 -0.04598048  0.15138678
 -0.14526488 -0.17533907 -0.2714191   0.11761764 -0.2101873  -0.11762448
 -0.38010275  0.35876587  0.01445845 -0.40957022 -0.06977503 -0.20172675
 -0.02905293 -0.55200434 -0.2703177  -0.29260895  0.4008527   0.5240082
 -0.29076064  0.38417393  0.2101571   0.05251366 -0.20957652 -0.00975936
 -0.15779603  0.32265002  0.1163184  -0.04600265  0.2614317   0.6375068
 -0.20822711  0.0147703  -0.21476081 -0.08334106  0.1419616  -0.3258916
 -0.4039411  -0.14486134  0.23850688  0.12535024 -0.09654578  0.49424878
 -0.29377425  0.04926211  0.63476515 -0.33800492 -0.3083299   0.04725716
  0.10445107 -0.34726638  0.02234466 -0.30832392 -0.10141913 -0.00340627]"
Switching branch and run ./configure does not regenerate spec.json type:build/install,"When building from source with TensorFlow and switch to another branch, error returned even if I rerun `./configure`:

```
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
ubuntu@ubuntu:~/tensorflow$ git checkout -b test
ubuntu@ubuntu:~/tensorflow$ ./configure
..........
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded).
INFO: Found 1 target...
ERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:1671:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 1)
Traceback (most recent call last):
  File ""tensorflow/tools/git/gen_git_source.py"", line 284, in <module>
    generate(args.generate)
  File ""tensorflow/tools/git/gen_git_source.py"", line 229, in generate
    (old_branch, new_branch))
RuntimeError: Run ./configure again, branch was 'refs/heads/master' but is now 'refs/heads/test'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.025s, Critical Path: 0.30s
FAILED: Build did NOT complete successfully
```


I think the issue is that `spec.json` is not updated when running `./configure`


```
ubuntu@ubuntu:~/tensorflow$ cat /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_git/gen/spec.json
{
  ""path"": ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/org_tensorflow/"", 
  ""git"": true, 
  ""branch"": ""refs/heads/master""
}
ubuntu@ubuntu:~/tensorflow$ 
```

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) 
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```sh
git checkout -b test
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

  ",True,"[-4.60256815e-01 -7.33633518e-01 -2.23291665e-01  2.50212133e-01
  2.68199086e-01 -1.68116882e-01 -3.71568501e-01 -5.32888062e-02
 -3.66469443e-01  7.03889579e-02  1.17312260e-01 -2.91430384e-01
 -4.13194835e-01  2.87039787e-01  5.50393425e-02  5.10514736e-01
 -2.75721043e-01 -2.28656679e-01  1.88277259e-01  1.20697320e-01
 -3.08948040e-01 -1.23306692e-01 -9.88816917e-02  1.60816461e-01
  2.40579322e-01  2.57661611e-01 -8.51043910e-02 -1.65029451e-01
  2.67883360e-01  1.14771370e-02  5.54776073e-01 -1.45879477e-01
 -3.27109605e-01  3.31023522e-03  2.00720996e-01  2.99952924e-01
  1.93392724e-01 -1.96869045e-01  1.14938408e-01 -9.98629928e-02
  1.52494133e-01  2.26176754e-01  2.10850835e-02 -1.55527130e-01
 -9.10496712e-02 -1.23019964e-01  1.28088608e-01 -3.99442725e-02
 -2.97838926e-01 -2.04010218e-01 -1.89130038e-01  1.95419818e-01
 -5.98816812e-01 -3.78594369e-01  3.37572396e-02  1.76722273e-01
  1.99956119e-01  1.75493017e-01  1.73293024e-01  1.26457378e-01
  1.13487236e-01  1.36947379e-01  1.46209106e-01  1.90796494e-01
 -3.70128453e-01  6.41755313e-02  7.89557099e-02 -2.83407271e-01
  4.52932596e-01 -4.89285082e-01  2.92452812e-01 -1.14095420e-01
 -1.83755815e-01 -1.00323513e-01  9.35842916e-02  6.09815493e-02
 -1.40223816e-01  5.66631973e-01  5.09398803e-02 -1.32058501e-01
 -2.28727281e-01 -1.87933035e-02  8.35754424e-02 -9.26161464e-03
  1.38905168e-01  1.27085537e-01  6.99772686e-02 -1.56616956e-01
  2.04014897e-01 -8.39875713e-02  2.47475088e-01  8.03002864e-02
  1.34403482e-01  1.27428606e-01  1.33832067e-01  2.09951699e-01
  4.50655678e-03  2.46436805e-01 -9.17964205e-02 -3.30560744e-01
  1.75601561e-02 -2.63994068e-01  8.14683437e-02  1.27399802e-01
 -1.22578055e-01  2.36043781e-01  1.47865027e-01  4.64984536e-01
  8.07388872e-02 -3.43023181e-01  4.54472572e-01  1.42619878e-01
  9.54601318e-02 -4.10817228e-02  1.50930639e-02  4.93348539e-01
 -5.96902490e-01 -6.51141535e-03 -3.81104238e-02  1.08534837e+00
 -2.78985083e-01 -9.68557596e-02  1.73511535e-01 -5.80799580e-02
  5.30077994e-01  1.61657669e-02  1.08945169e-01 -5.56599386e-02
  1.52898818e-01  1.15328081e-01  1.72062546e-01  3.00381511e-01
 -2.45808378e-01  1.42625779e-01  2.73949087e-01 -3.55172679e-02
 -4.86616462e-01 -3.19966108e-01  1.64789148e-04  2.00149119e-02
 -3.10710460e-01  6.16977811e-01 -4.42974083e-02 -2.70203471e-01
 -9.44034159e-02  1.26703605e-01 -1.11343905e-01  7.81471133e-02
 -1.92130953e-01  1.98513269e-01 -2.19846904e-01 -1.52672842e-01
  1.72463246e-02  2.38764346e-01  3.69410932e-01  3.36135864e-01
  1.16837971e-01 -2.67459117e-02  5.09592704e-03 -4.24792320e-01
 -7.31527060e-02  4.20646250e-01 -3.48838747e-01 -3.38203430e-01
 -1.34987935e-01 -9.28123295e-02 -6.33173525e-01 -2.82354385e-01
  5.57072461e-02  2.69373327e-01 -3.36835027e-01 -1.44609064e-03
  1.30381748e-01  2.16489770e-02  1.24684535e-01  2.42736917e-02
  7.02008307e-01 -4.37996715e-01 -3.31428587e-01  5.46343029e-01
  4.59497094e-01  1.71191156e-01 -2.17845123e-02 -1.90588623e-01
  2.01665193e-01  1.67791963e-01  3.43447864e-01 -4.83384356e-04
 -5.17690033e-02  1.03988677e-01 -3.04922044e-01  4.03414190e-01
  1.79065853e-01 -3.10979337e-01 -3.40973675e-01  9.42647159e-02
  2.74321914e-01  2.34123915e-01  1.28119364e-01  3.09790764e-03
 -2.06751600e-01 -2.61081830e-02 -1.23355679e-01  1.76098168e-01
 -1.97288826e-01 -5.96556515e-02 -1.28514975e-01  3.94519567e-02
 -2.86736757e-01 -1.84854325e-02  2.20070127e-02 -2.03438312e-01
  7.66559392e-02 -1.39173880e-01 -1.99405357e-01 -1.01948865e-01
  2.31943727e-01 -1.42126918e-01 -2.60135345e-02  8.40222836e-02
  1.48225665e-01 -1.73515260e-01  2.76142269e-01 -3.84390503e-01
 -8.82836878e-02 -1.43522352e-01 -9.04595703e-02  1.53230846e-01
 -3.11939493e-02  1.58637285e-01 -1.67269424e-01  1.30386353e-01
  2.76572734e-01  2.87957132e-01  3.91372561e-01 -2.74691939e-01
  1.06363222e-01 -9.26755667e-02 -1.25744238e-01  2.88039267e-01
 -1.13476694e-01 -2.65762880e-02  4.49331701e-01  4.42874730e-02
  5.85667826e-02  3.00026536e-01 -2.81257220e-02 -2.05722824e-02
 -3.70207429e-01  1.46458730e-01 -1.63917124e-01 -3.64611208e-01
  4.11181487e-02  1.31189942e-01  3.04403365e-01  2.92537928e-01
 -3.59713197e-01 -5.99374846e-02  2.14707971e-01 -6.83169216e-02
 -1.90977037e-01  3.51761103e-01 -1.15892105e-03  3.02883238e-01
  1.16397858e-01  2.26924539e-01 -1.84270024e-01  1.64914966e-01
 -2.79281884e-01 -2.08536372e-01  2.59483099e-01 -8.52189735e-02
 -5.44348806e-02 -1.84130102e-01  3.03350389e-05  1.47873402e-01
  4.17166770e-01 -2.79205859e-01 -1.58352256e-01  1.03418037e-01
  2.76614547e-01  5.86379409e-01 -5.57997763e-01  1.91160873e-01
  1.36464685e-01 -4.52585578e-01  1.26621783e-01 -4.36581880e-01
 -3.21886897e-01 -1.54995918e-03 -3.61891687e-01  1.78556487e-01
 -9.32794958e-02 -1.67911917e-01 -3.52622867e-01 -2.05241844e-01
  3.15939814e-01  4.14328724e-02  1.03561476e-01  1.39569193e-01
 -1.27426699e-01 -1.07559808e-01  4.56000388e-01 -1.21317394e-01
  1.45316765e-01 -3.36301088e-01  1.57019526e-01  8.08854029e-02
  5.98949909e-01 -4.05831695e-01  1.01001970e-02 -6.20489195e-02
  5.96466521e-03  9.64969471e-02  3.00986040e-02  1.12171978e-01
 -4.40135039e-02  2.22704723e-01  2.04127342e-01 -1.65285721e-01
  1.85660526e-01  9.26452503e-02 -3.51586193e-01  3.29235137e-01
 -1.49647042e-01 -1.11856878e-01 -2.80087650e-01  2.74256915e-02
 -2.97401071e-01  4.39203709e-01 -8.39367732e-02  1.04874358e-01
 -2.33873613e-02  2.05492020e-01 -3.91991198e-01 -1.36375606e-01
 -4.13559705e-01  3.17730457e-01  6.25590235e-02 -2.04112723e-01
  2.65278757e-01 -2.49002039e-01  3.61031771e-01 -3.71829391e-01
 -4.03882682e-01 -4.03842002e-01  5.55144191e-01  7.48429447e-02
 -2.02880859e-01  3.28362465e-01 -1.18067548e-01  9.87304375e-02
 -5.47675669e-01  1.44228876e-01  2.01000422e-01  4.93735746e-02
 -5.47760725e-02  4.34468538e-02  2.50285208e-01  5.75552106e-01
 -1.91538557e-01  9.80225354e-02 -3.71975809e-01 -4.60915476e-01
  1.21962667e-01 -1.60909355e-01 -2.16551244e-01  7.21847825e-03
 -7.42477328e-02  3.73995304e-01 -2.12932765e-01  1.98156565e-01
 -4.58409339e-01 -2.31478930e-01  5.45852542e-01 -3.04626584e-01
 -4.53398854e-01  1.72973692e-01 -1.52704492e-02 -1.79772362e-01
  8.68753344e-02  1.22324489e-01  1.61800742e-01 -1.77785784e-01]"
Sample Distorted Bounding Box Bug awaiting review stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Stock Example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
16.04.3 LTS
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
Python 3.5.2
- **Bazel version (if compiling from source)**:
0.7.0
- **GCC/Compiler version (if compiling from source)**:
5.4.0
- **CUDA/cuDNN version**:
cuda_9.0.176_384.81 / cudnn 7
- **GPU model and memory**:
GTX 755M 2gb Memory x2
- **Exact command to reproduce**:
begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        bounding_boxes=bounding_boxes)


### Describe the problem
When using the tf.image.sample_distorted_bounding_box() function the parameter min_object_covered seems to default to value None which causes an error (ValueError: None values not supported.)  If you give an argument for min_object_covered it seems to work fine.

There seems to be two versions of this function in the source [v2 which takes min_object_covered](https://github.com/tensorflow/tensorflow/blob/fc49f43817e363e50df3ff2fd7a4870ace13ea13/tensorflow/core/ops/image_ops.cc#L930) as a argument and a [v1](https://github.com/tensorflow/tensorflow/blob/fc49f43817e363e50df3ff2fd7a4870ace13ea13/tensorflow/core/ops/image_ops.cc#L844) which has the default value of 0.1 as an attribute.  It appears v2 is the one [being used](https://github.com/tensorflow/tensorflow/blob/73658420db2498ad7f07363bfa72cba6e2d9fdd2/tensorflow/python/ops/image_ops_impl.py#L1536).  Not sure what approach is best to take for fixing this bug but believe the root of the issue is coming from tensorflow/core/ops/image_ops.cc

### Source code / logs
Attached
[boundingbox.txt](https://github.com/tensorflow/tensorflow/files/1576836/boundingbox.txt)

Examples of code being implemented [here](https://github.com/wagonhelm/image_augment/blob/master/DataAug.ipynb).

",True,"[-0.16134246 -0.77005285 -0.2899863  -0.05113722  0.2991904  -0.21406552
  0.11404517 -0.06960415 -0.3569331  -0.25876507  0.02183968 -0.02074261
 -0.16999005  0.12468853 -0.39409053  0.18889008  0.0016082  -0.16068612
  0.09062524  0.07372082 -0.09921919 -0.19886741 -0.25300065  0.3732252
  0.07378504  0.17732137 -0.19650552 -0.14123869  0.11023654 -0.17101315
  0.37181276  0.24951848  0.07493684  0.05639569 -0.14869297  0.29831293
 -0.23796192 -0.00641038 -0.08096865  0.1493295   0.10459785  0.06317786
  0.22757977  0.08475071  0.11417464  0.13996336  0.05373377  0.13594337
 -0.08972533 -0.09472385 -0.03201141  0.13212669 -0.2759318  -0.40844923
 -0.2246294  -0.30003738  0.23620452 -0.29711992 -0.07950004  0.33426088
  0.1359112  -0.0282862  -0.07546733  0.03785573  0.1095458   0.2962479
  0.3515951  -0.24129513  0.45600092  0.14077136  0.21961609 -0.11195977
 -0.41229135 -0.15154228 -0.0671494   0.14494321  0.05623794  0.07017839
  0.39687878 -0.34404022  0.06021183 -0.1216049   0.27297515 -0.20025018
 -0.03830811 -0.06744358  0.5032713   0.22989535  0.22049695 -0.2354025
  0.74569964  0.19353375  0.04519308  0.23230812  0.383824    0.27769184
  0.01778139  0.15435635  0.10615572 -0.10713972  0.02254862  0.00419561
  0.07639989  0.15480936  0.06676755 -0.23006985  0.17683282 -0.07964128
  0.18055242  0.09734343  0.02037846  0.01638708  0.30267245  0.03156167
 -0.06006256 -0.15907848 -0.00521526  0.2403456  -0.2293082   0.703725
  0.05654061 -0.01132249  0.21576273  0.23225711  0.40738383  0.15962705
 -0.10848968 -0.05872592  0.20755216 -0.04290276  0.2334731  -0.01612891
 -0.3256392   0.11464814 -0.27672398 -0.15546934 -0.23613681  0.20057182
 -0.11732344 -0.04453591 -0.2207517   0.39962262 -0.12802735 -0.1967536
 -0.06670109  0.04662218 -0.22170973  0.21112306 -0.23014054  0.08461858
 -0.13351186 -0.10758011 -0.07032307  0.27778903  0.05290326  0.21184878
  0.2747292  -0.01807037  0.09716843 -0.65310574 -0.18049186  0.27776963
 -0.24312612 -0.12241057  0.1036946   0.17552868 -0.32728624 -0.17879379
  0.04603286  0.30305478 -0.00890039 -0.0490898  -0.02229809  0.30879742
 -0.00399278 -0.03526779  0.12736775 -0.5883287  -0.12538168 -0.04285194
  0.1308487  -0.02061988  0.16997312  0.09812231 -0.138414   -0.03466644
  0.19158831  0.20946206 -0.22190827 -0.16174936 -0.30935758 -0.23692879
  0.21629772  0.01873444 -0.18724509 -0.05878225  0.18833463 -0.14272809
  0.02067436  0.23838687 -0.22242206 -0.15538433 -0.0538822  -0.16430475
  0.27647132 -0.20174971 -0.4327338  -0.2889024  -0.40578604  0.10249191
  0.02159157 -0.32362086 -0.20037325  0.1043959  -0.27786544 -0.0114507
  0.0966249  -0.17348224 -0.39533168  0.19764006  0.04536989 -0.27354562
  0.15624675 -0.29979163 -0.30997834 -0.14467123 -0.12327851  0.0387444
 -0.02728992  0.20248008  0.1227839   0.13008004  0.07168178  0.12755278
  0.09204394 -0.20696525  0.03948682 -0.20327264 -0.05116296  0.4051773
 -0.31864735  0.0424068  -0.03292349  0.13419415 -0.05838607  0.14372763
 -0.10277695  0.25448605 -0.15062003  0.14877146 -0.358184   -0.15888524
  0.37684032  0.1118767   0.23349008  0.0869379  -0.02794316  0.10948777
  0.2485535  -0.20812872  0.23042864  0.25414044 -0.23301497  0.55273706
  0.2187641   0.34473848 -0.24223614  0.16683923  0.34094453 -0.24281693
 -0.01419994 -0.20401932  0.17452514 -0.22672236 -0.14285989 -0.12637776
  0.43237835  0.1998708  -0.26436555  0.04648274  0.13947785  0.11841697
 -0.04561276  0.1022042  -0.25227076 -0.17490813 -0.01941257 -0.35802805
 -0.09117222 -0.07413678 -0.17178297  0.15748376  0.2781521   0.13817546
 -0.09234555  0.08717538  0.09717297  0.16111001  0.06198876  0.38376397
 -0.11293918 -0.08891497  0.13803147  0.02270332  0.05404285  0.03979933
  0.48667243  0.1432086   0.2210584  -0.18251595  0.34149778 -0.163504
 -0.06411397  0.18357387 -0.13284382 -0.15366343 -0.11786199  0.5915534
  0.22821158  0.0335554   0.09113489 -0.28360125  0.03854683  0.10675389
  0.01523149 -0.14626613 -0.11791623 -0.32003263 -0.281577    0.00765206
 -0.04055266 -0.26653573 -0.3364569  -0.10170218  0.0447226  -0.16274989
 -0.36366743  0.5600642  -0.01643193 -0.1701653  -0.03449563 -0.16413082
  0.15528235 -0.50140566  0.03062764 -0.22126439  0.45802987  0.23290485
 -0.18303406 -0.00713733  0.09371024  0.10373029 -0.18022923 -0.18044673
 -0.23989157  0.23928013  0.03204628 -0.12680167  0.10412661  0.40115407
 -0.37283635 -0.35409895 -0.19871047 -0.14125659  0.20776692 -0.293008
 -0.24751629 -0.2334046   0.3542892   0.14734492  0.02473222  0.3676306
 -0.08900611  0.43435618  0.5460067  -0.3459391  -0.44028926  0.1880961
  0.12594856 -0.2803201  -0.26355875 -0.02727394  0.22102226 -0.05514027]"
XLA/AOT Windows support stat:awaiting response stat:community support stale,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.8.0
- **GCC/Compiler version (if compiling from source)**: VS 2017 15.5
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

Similar to #8310, but specifically about running `tfCompile` on Windows rather than Linux to produce `x86_64-windows-msvc` binaries.

XLA/AOT depends on LLVM which has excellent Windows support via CMake, but Bazel cannot interop with CMake. [llvm.BUILD](https://github.com/tensorflow/tensorflow/blob/master/third_party/llvm/llvm.BUILD) is auto-generated and the script to generate it is not open-sourced, this make it difficult for external contributor to make improvement. [tensorflow/compiler](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/compiler) might not need too much changes as #9908 already addressed some of them.

One possible path is to let user to run CMake in host machine when invoking `configure.py`, then feed CMake generated files into custom script to generate `LLVM.BUILD`.

Note:

[Rumour has it](https://chromium-review.googlesource.com/c/chromium/src/+/753588) that there is a Google-internal tool called `tfNative` to generate `.h/.cpp` files instead of `.lib` binaries, though I suspect that even if the tool is open-sourced, it might not be immediately available for Windows developers.",True,"[-0.30092114 -0.23680589 -0.16559486 -0.11116007  0.04927265 -0.10473078
 -0.21651196 -0.05053553 -0.17819534 -0.18695202 -0.10022524 -0.11441921
 -0.06405371 -0.28053802 -0.37806582  0.12136458 -0.0515994  -0.11857754
  0.17736596  0.15418395 -0.12311129 -0.37133038 -0.25869697  0.15781927
  0.37685093  0.22809571 -0.1631912   0.08929851  0.02066548  0.28391004
  0.5979196   0.03804962  0.18967424  0.18823236 -0.09289773  0.14935371
  0.01031209 -0.12183584 -0.4051726   0.15535192  0.11354581  0.03809378
  0.02662117  0.03601249  0.17963237  0.00700193  0.0193671  -0.33297843
 -0.16053411  0.08313721 -0.19442442  0.07584084 -0.1803251  -0.42080736
 -0.31653488 -0.11039999  0.00439895 -0.11560969 -0.16104442  0.23555622
  0.1452539  -0.10056332  0.00508486  0.07920013 -0.08287442  0.43506843
  0.25142917 -0.05947046  0.41209042 -0.2632433   0.06335695  0.029704
 -0.21521816 -0.06855869 -0.02617806  0.44953835  0.03323895 -0.16717052
  0.4094643  -0.2804538   0.11833021 -0.22464296  0.21712944  0.23982528
  0.2347853   0.2404322   0.15006663  0.16810232  0.17576039 -0.12957776
  0.4513211   0.27120423  0.1560671   0.19821665  0.48807496  0.34409037
  0.11143778  0.18149784 -0.2197156  -0.0830752  -0.09125841  0.104005
 -0.11286595 -0.12321426  0.10872412 -0.40564314  0.2529777   0.03781462
 -0.2151182  -0.10107604  0.2359377  -0.20480591  0.04756308 -0.16069102
  0.08104007 -0.06218107 -0.02058842  0.23654526 -0.07569063  0.57082117
  0.0364531  -0.1206015   0.16236138  0.17196295  0.3768841  -0.03364488
 -0.18004848  0.12663917 -0.13379484  0.12577695  0.408797    0.09934335
  0.00106219  0.02071292 -0.06478237 -0.13133073 -0.21187656  0.06299222
 -0.25483978 -0.02321132 -0.07456633  0.194691    0.02139376 -0.2799968
 -0.19138923  0.057424   -0.13431361  0.05946665  0.08796144 -0.07512315
 -0.15928541  0.16559687 -0.1427754   0.19341682  0.12736365  0.10975955
  0.07406048  0.09454502 -0.11177294 -0.6518743  -0.03806618  0.08539496
 -0.01250332 -0.08691195  0.01648319  0.10817561 -0.40661746 -0.04680094
  0.15450156  0.27058172 -0.02314143 -0.13573211 -0.01695836  0.32112283
  0.0854068  -0.10525334  0.16244765 -0.30582184 -0.08464798  0.15910974
  0.21059136 -0.085527    0.03756887 -0.04862981 -0.26194853  0.24785343
  0.10472063  0.2518622  -0.17052758 -0.09589346 -0.17996034  0.08907425
 -0.05976122 -0.05389983 -0.29216075 -0.31574118 -0.04799858  0.15824202
 -0.00348942  0.28651556 -0.2588443   0.08156093  0.103833   -0.15697043
  0.29895478 -0.13167477 -0.47868696 -0.32250053 -0.30254248 -0.09135562
 -0.0332203  -0.2641316   0.06669645  0.2434725   0.01794398  0.15499133
  0.00542989 -0.08727783 -0.2754849  -0.1157837  -0.03875104 -0.02618276
 -0.01709357 -0.42546278 -0.18437707  0.00587505 -0.06396994 -0.1072647
  0.13130186  0.41056535  0.20839977  0.0634094   0.25007302  0.10073109
  0.42791775 -0.24964324  0.08646576 -0.19317162 -0.11477712  0.13669254
 -0.39612353 -0.39362746 -0.02866095  0.15586364  0.11238113  0.05478851
 -0.31303826  0.07468414  0.01437462  0.22024298 -0.21475106 -0.10209525
  0.5164231   0.13376302 -0.02186012  0.26512742 -0.06142222 -0.00313649
  0.17140421 -0.09949421  0.37112498  0.13497621 -0.19961861  0.11011579
  0.17902175 -0.00795319 -0.38291633  0.27651083  0.27395785  0.08666214
  0.4485156  -0.37508795  0.11440401 -0.35374132 -0.14391667  0.27266294
  0.35880053  0.37159157  0.01665344  0.1572099   0.07319771  0.10786767
 -0.4476012  -0.08773437 -0.07614676 -0.4066258   0.04850613 -0.34605026
 -0.21256964 -0.01521252  0.07862536 -0.13048334 -0.09173577  0.19988027
 -0.19945243 -0.09738    -0.04090903 -0.07140455  0.01084426  0.20077634
 -0.28315848 -0.16552867  0.04872674  0.12899897 -0.23549423 -0.2539596
  0.41096163  0.17440401  0.45661768 -0.24210218  0.44967264 -0.03463642
 -0.12047082  0.35577923 -0.18156627 -0.12189899 -0.5428517   0.56868225
 -0.01031349 -0.05313297  0.1743875  -0.14160384 -0.0534668   0.21493715
  0.02472338  0.20490272 -0.23194474 -0.33110476 -0.04717752  0.07987996
  0.15478885 -0.07602005 -0.35224366 -0.11335707 -0.06491461 -0.1313616
 -0.24528667  0.47551793  0.04352251 -0.14042403 -0.02481451 -0.07372339
  0.28264493 -0.43617597  0.05523702  0.08094041  0.35886973  0.15347278
 -0.05050483  0.44061956  0.06333661 -0.02248853 -0.17091568 -0.21350113
 -0.344487    0.18115833  0.35170162  0.05630937  0.14543943  0.29342413
  0.06524784 -0.13808031 -0.3632205  -0.12818274  0.27984446 -0.09296287
 -0.09955432 -0.38860607  0.24961267 -0.0067179   0.05267924  0.1212218
 -0.22736618  0.3661233   0.35805514 -0.04814065 -0.33667028  0.04811816
  0.01335405  0.05839572  0.07097548 -0.18928769 -0.01896833  0.01264421]"
CMake: If/else statement in CMAKE_CACHE_ARGS breaks CMake build on Ubuntu 17.10 stat:contribution welcome type:build/install,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10.
- **TensorFlow installed from (source or binary)**: Source (CMake)
- **TensorFlow version (use command below)**: 2cfb088cf72b52c74a742d780cc5c4f93a74640e (tip of master at time of writing)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**: cmake ../../tensorflow/contrib/cmake && make

### Source code / logs
    [  1%] Performing build step for 'zlib'
    [ 40%] Built target zlibstatic
    [ 42%] Linking C shared library libz.so
    /usr/bin/ld: CMakeFiles/zlib.dir/deflate.o: relocation R_X86_64_PC32 against symbol `deflate' can not be used when making a shared object; recompile with -fPIC
    /usr/bin/ld: final link failed: Bad value
    collect2: error: ld returned 1 exit status

### Cause
In v1.4.0, you will find the argument `-DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON` given to  CMAKE_CACHE_ARGS for several external projects (png, zlip, sqlite etc). In master, this has been changed to 

	CMAKE_CACHE_ARGS
		if(tensorflow_ENABLE_POSITION_INDEPENDENT_CODE)
			-DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=ON
		else()
			-DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=OFF
		endif()

which yields the following init cache entries on my machine: 

	set(CMAKE_POSITION_INDEPENDENT_CODE ""ON;if;(;tensorflow_ENABLE_POSITION_INDEPENDENT_CODE;);else;(;)"" CACHE BOOL ""Initial cache"" FORCE)
	set(CMAKE_POSITION_INDEPENDENT_CODE ""OFF;endif;(;)"" CACHE BOOL ""Initial cache"" FORCE)

and the build fails because `CMAKE_POSITION_INDEPENDENT_CODE` ends up not being set to `ON`. I could imagine this breaks a lot of builds. Perhaps CMake behaves differently on Windows and therefore this has not been caught? Is there a reason for this change I am not aware of? 

### Solution
A possible solution is to not inline the if/else statement and instead use the argument

    -DCMAKE_POSITION_INDEPENDENT_CODE:BOOL=${tensorflow_ENABLE_POSITION_INDEPENDENT_CODE}

Are you interested in a PR?
",True,"[-1.41622871e-01 -5.29485583e-01 -3.69334161e-01 -1.83477640e-01
  2.37849876e-01 -3.70822757e-01  3.93257700e-02 -4.58588861e-02
 -5.29686451e-01 -3.37114990e-01 -4.16737124e-02 -1.04549721e-01
 -1.33429617e-01 -3.55746329e-01 -1.93894446e-01  1.74314231e-01
  1.29838794e-01 -3.49277556e-01  1.48395419e-01 -1.73473701e-01
 -2.49104500e-01  1.95659846e-01 -2.96872556e-01  3.94739002e-01
  6.19837642e-02  2.25075394e-01 -1.50509924e-01 -4.76977825e-02
  8.36942196e-02 -2.66564846e-01  8.27731729e-01 -9.58421640e-03
 -1.71917856e-01  6.29679412e-02  1.87683418e-01  3.78594220e-01
 -1.61033928e-01 -1.81110874e-02 -3.81790400e-02 -4.30355631e-02
  4.23602387e-03  1.33259833e-01  6.06159344e-02  6.86324090e-02
  1.11998215e-01 -6.15973100e-02 -2.90220737e-01  4.07129750e-02
 -9.21071768e-02 -1.94171146e-01 -7.06042051e-02  6.80460036e-02
 -1.26384735e-01 -1.34417638e-02 -8.43813121e-02  6.53499141e-02
 -2.00743094e-01 -1.77973405e-01  1.22478738e-01  2.71548390e-01
  2.82758534e-01 -4.10972349e-02 -5.11634722e-02  1.40819877e-01
  2.98596740e-01  1.92719966e-01  5.43314099e-01 -2.65654743e-01
  4.86811876e-01  1.06299616e-01  2.49582715e-02 -2.01236069e-01
 -5.17834842e-01 -4.93049026e-01  8.25830251e-02  4.17270660e-01
 -1.02553770e-01  1.35157466e-01  2.15363353e-01 -4.45085406e-01
 -1.31612182e-01 -2.29699522e-01  3.95125210e-01 -1.60523295e-01
 -9.04619619e-02  1.19022824e-01  2.32133061e-01 -2.11311460e-01
  2.96663940e-01 -2.47475117e-01  2.68795550e-01  3.95503730e-01
  9.01723206e-02  5.53812623e-01  1.97530076e-01  2.00089663e-01
  1.48416460e-01  2.11494178e-01 -2.08930895e-02  8.96581486e-02
 -1.36191994e-01  8.58452544e-02 -4.79512028e-02  4.66284230e-02
 -1.50946617e-01 -3.63044947e-01  3.73333156e-01  1.85362905e-01
  1.27474532e-01 -2.28055656e-01  3.63281295e-02  1.34572655e-01
  1.66903913e-01  6.25877902e-02  3.31792980e-01 -6.86982367e-03
  1.52894199e-01  2.31190398e-01 -2.10222095e-01  5.37036419e-01
 -1.24373056e-01  3.51051301e-01 -1.84530497e-01  2.03061044e-01
  3.05171013e-01  2.23550275e-01 -1.15918107e-01  1.16564766e-01
  1.73200354e-01 -4.73553389e-02 -7.82701522e-02  2.21137896e-01
 -1.32686004e-01  3.93001497e-01 -1.44228056e-01  5.60413375e-02
  5.61521715e-03  3.06361139e-01  2.85465956e-01 -2.38455653e-01
 -3.74776304e-01  2.98416317e-01  3.11725974e-01 -3.08058381e-01
 -8.53713155e-02 -1.22867450e-01 -1.84432045e-01  3.46875191e-02
 -3.35759372e-01  5.16492426e-02 -3.18696439e-01  2.95136243e-01
  1.54333383e-01  5.80610394e-01  4.60824937e-01  2.52214193e-01
  2.17936620e-01 -4.40171622e-02  2.20321834e-01 -4.77654755e-01
 -2.62063265e-01  5.06795049e-01 -4.10874307e-01 -1.17133252e-01
 -4.37932834e-03  8.72568786e-02 -2.70945668e-01 -1.14479028e-01
 -1.11328885e-01  5.91847561e-02 -1.99855149e-01 -2.07697302e-01
 -1.64048910e-01  1.30810402e-03  2.30519101e-01 -2.13575568e-02
  5.26379228e-01 -5.40425777e-01  7.24663436e-02  7.31055140e-02
  1.74877681e-02 -1.04052052e-01  2.62321141e-02  9.15327668e-02
 -1.54184297e-01  1.67592719e-01  1.78325206e-01  1.27683148e-01
 -2.30048701e-01 -3.91149282e-01 -3.02247584e-01 -5.23588806e-02
  1.60247475e-01  1.97988868e-01 -2.52578855e-02 -2.10153282e-01
  2.99681455e-01 -7.71467835e-02  4.30377573e-03  1.99237391e-01
 -5.02110273e-02 -1.70093961e-02  9.72941369e-02 -1.36724696e-01
  2.01075375e-02 -3.25642288e-01 -1.49419397e-01  6.75623268e-02
 -2.69878805e-01 -1.17689908e-01 -1.15445480e-01 -1.31899521e-01
 -3.84598486e-02 -1.54999405e-01 -3.15246463e-01 -2.05657393e-01
  2.17987910e-01 -8.23192000e-02 -2.23915279e-01  9.64144766e-02
  8.22407454e-02 -1.22364700e-01  2.89584458e-01 -3.50312591e-01
  1.65228713e-02  8.70505162e-03 -1.75220877e-01  2.18417972e-01
  2.88256090e-02 -6.60857465e-03 -1.51481479e-01  2.21318156e-01
  1.06934115e-01 -2.37642020e-01  1.53320953e-01  1.40317306e-01
  6.47760630e-02 -2.32586600e-02 -4.28127915e-01  3.41072947e-01
 -1.91755354e-01 -3.24498951e-01 -3.38588189e-03  5.80103556e-03
  3.20979416e-01  2.01710790e-01 -3.54936719e-01  1.89311728e-01
  5.14966324e-02  1.51612803e-01 -8.93244743e-02 -1.28521323e-01
  4.77437437e-01 -4.36522067e-03  4.38923538e-01  3.72575700e-01
  2.15913123e-03  4.95292187e-01  1.34457260e-01 -4.84592840e-02
  1.53786615e-01  4.55796063e-01 -1.47102758e-01  3.49103838e-01
  2.97077373e-02  2.12208450e-01 -3.05227250e-01  1.13936432e-01
  4.23621871e-02 -9.01448503e-02  3.84442270e-01 -1.56559587e-01
  3.27914283e-02  5.21749333e-02 -9.17307138e-02 -3.59109603e-02
  2.60403007e-01  2.14087293e-02 -6.45221472e-02  1.10903084e-01
  2.53051102e-01 -1.18383452e-01  6.30187094e-02 -1.68110356e-01
 -6.11227266e-02  1.56132609e-01  1.51548579e-01 -2.78589904e-01
 -5.04712224e-01  2.13806137e-01 -2.39672571e-01  1.16473481e-01
  2.30584562e-01  4.24692810e-01  9.00119543e-02 -7.91648775e-02
 -3.02267894e-02  1.08189598e-01  2.27492645e-01  3.31266910e-01
 -1.56736299e-02 -1.75623953e-01  8.80250037e-02 -6.34062737e-02
 -1.42555624e-01  1.32975966e-01  2.88638115e-01  3.64162847e-02
  6.14310622e-01 -2.66792953e-01  6.32769614e-02 -1.81570537e-02
 -3.04600745e-01  2.67545879e-01 -2.10070714e-01 -1.76307231e-01
 -3.17483932e-01  5.68482399e-01  2.90373772e-01 -3.34685333e-02
  2.53105879e-01 -1.64493591e-01 -2.84547359e-01 -1.38638347e-01
  1.67823821e-01 -4.00811434e-02 -2.40636557e-01 -2.41069525e-01
 -3.31861824e-01 -3.11200675e-02 -1.24061424e-02 -1.38618499e-01
 -3.31727505e-01  1.53107688e-01 -4.88299131e-01 -1.44566327e-01
 -2.14506909e-01  3.95291328e-01 -2.77201861e-01 -3.11522543e-01
 -3.35409641e-01 -2.84682453e-01  3.11066359e-01 -3.88173252e-01
 -2.55293638e-01 -2.79626660e-02  1.12872034e-01  2.14965284e-01
 -2.82705456e-01  5.32329269e-02  7.77606666e-02  9.41171795e-02
 -1.40570089e-01 -1.52796924e-01  1.08360231e-01  1.66584358e-01
  4.23543248e-03  5.44028357e-04  4.00975347e-01  3.06876838e-01
 -4.42678094e-01 -1.64147273e-01 -2.20063418e-01 -1.86982960e-01
 -4.78727371e-02 -1.62705570e-01 -5.21775126e-01 -1.10327266e-01
  2.76778400e-01  2.75875956e-01 -5.36498427e-02  7.45413378e-02
 -9.34814587e-02  2.67573357e-01  3.15266430e-01 -3.31781060e-03
 -2.27145910e-01  2.36111313e-01 -1.05815940e-03 -3.91154081e-01
 -2.50226319e-01 -1.33203924e-01 -2.96437126e-02 -2.67888397e-01]"
"tf.nn.softmax(input, dim=-1, name=None), argument `dim` cannot take negative index other than -1? stat:contribution welcome type:bug","I expect that in the function `tf.nn.softmax(input, dim=-1, name=None)`, the argument `dim` can take dim either positive or negative. However, it seems the only negative index can be taken is -1?

```
import tensorflow as tf

tf.__version__
> '1.4.0'

xx = tf.constant(1, shape=[10, 28, 28, 3], dtype=tf.float32)

tf.nn.softmax(xx, dim=-1)
> <tf.Tensor 'Reshape_1:0' shape=(10, 28, 28, 3) dtype=float32>

tf.nn.softmax(xx, dim=3)
> <tf.Tensor 'Reshape_3:0' shape=(10, 28, 28, 3) dtype=float32>

tf.nn.softmax(xx, dim=-2)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-8-c0dec6c1fa17> in <module>()
----> 1 tf.nn.softmax(xx, dim=-2)

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in softmax(logits, dim, name)
   1665       dimension of `logits`.
   1666   """"""
-> 1667   return _softmax(logits, gen_nn_ops._softmax, dim, name)
   1668 
   1669 

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in _softmax(logits, compute_op, dim, name)
   1624   # Swap logits' dimension of dim and its last dimension.
   1625   input_rank = array_ops.rank(logits)
-> 1626   logits = _swap_axis(logits, dim, math_ops.subtract(input_rank, 1))
   1627   shape_after_swap = array_ops.shape(logits)
   1628 

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.pyc in _swap_axis(logits, dim_index, last_index, name)
   1596     return array_ops.transpose(logits,
   1597                                array_ops.concat([
-> 1598                                    math_ops.range(dim_index), [last_index],
   1599                                    math_ops.range(dim_index + 1, last_index),
   1600                                    [dim_index]

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc in range(start, limit, delta, dtype, name)
   1232       delta = cast(delta, inferred_dtype)
   1233 
-> 1234     return gen_math_ops._range(start, limit, delta, name=name)
   1235 
   1236 

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc in _range(start, limit, delta, name)
   3257   if _ctx.in_graph_mode():
   3258     _, _, _op = _op_def_lib._apply_op_helper(
-> 3259         ""Range"", start=start, limit=limit, delta=delta, name=name)
   3260     _result = _op.outputs[:]
   3261     _inputs_flat = _op.inputs

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc in _apply_op_helper(self, op_type_name, name, **keywords)
    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,
    786                          input_types=input_types, attrs=attr_protos,
--> 787                          op_def=op_def)
    788       return output_structure, op_def.is_stateful, op
    789 

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)
   2956         op_def=op_def)
   2957     if compute_shapes:
-> 2958       set_shapes_for_outputs(ret)
   2959     self._add_op(ret)
   2960     self._record_op_seen_by_control_dependencies(ret)

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in set_shapes_for_outputs(op)
   2207       shape_func = _call_cpp_shape_fn_and_require_op
   2208 
-> 2209   shapes = shape_func(op)
   2210   if shapes is None:
   2211     raise RuntimeError(

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc in call_with_requiring(op)
   2157 
   2158   def call_with_requiring(op):
-> 2159     return call_cpp_shape_fn(op, require_shape_fn=True)
   2160 
   2161   _call_cpp_shape_fn_and_require_op = call_with_requiring

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc in call_cpp_shape_fn(op, require_shape_fn)
    625     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,
    626                                   input_tensors_as_shapes_needed,
--> 627                                   require_shape_fn)
    628     if not isinstance(res, dict):
    629       # Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).

/home/jetadmin/anaconda2/envs/ygtf/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc in _call_cpp_shape_fn_impl(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)
    689       missing_shape_fn = True
    690     else:
--> 691       raise ValueError(err.message)
    692 
    693   if missing_shape_fn:

ValueError: Requires start <= limit when delta > 0: 0/-2 for 'range' (op: 'Range') with input shapes: [], [], [] and with computed input tensors: input[0] = <0>, input[1] = <-2>, input[2] = <1>.
```",True,"[-2.71586835e-01 -1.99916631e-01 -4.71927412e-03  1.32543921e-01
  2.63193280e-01 -5.83254024e-02  1.34663641e-01  2.11056158e-01
 -9.47118700e-02 -1.63765609e-01  9.05883498e-03 -3.55943926e-02
 -2.88977344e-02  1.20728932e-01  3.31039801e-02  1.23815030e-01
  6.76255599e-02  4.52550240e-02  8.40531364e-02  1.05454460e-01
 -9.05668139e-02 -5.79856001e-02 -2.27090046e-01  4.72983718e-02
 -1.05536357e-01 -2.01804340e-01 -2.90660501e-01 -6.61663432e-03
  3.33949596e-01  1.46330789e-01  9.16017890e-02  1.57681316e-01
 -1.52754530e-01  1.15114242e-01 -2.45006867e-02  1.14534289e-01
 -2.95148492e-01 -1.60922948e-02 -2.49121860e-01 -1.65071022e-02
  2.06629299e-02  9.55397561e-02 -7.54511282e-02 -3.92598398e-02
  7.87147433e-02  6.05400391e-02 -3.61094810e-02 -1.06214419e-01
 -9.55239758e-02  5.55505678e-02 -1.74495950e-01  4.89032716e-01
 -2.54438400e-01 -1.98630631e-01  1.67035609e-01 -2.25709043e-02
 -2.48787433e-01 -8.29506516e-02 -1.06196322e-01 -3.46905552e-04
  1.15825251e-01  8.17203149e-02 -4.54833433e-02 -7.10523203e-02
  3.41548920e-01  1.28080457e-01  1.11877508e-01 -1.31544799e-01
  6.11418933e-02 -3.05490822e-01  1.76299244e-01 -6.59523606e-02
 -2.91914433e-01 -2.70354357e-02 -1.00837588e-01  3.07754517e-01
 -2.98820317e-01  2.90019840e-01  1.15669727e-01 -1.39345616e-01
  3.66642103e-02  1.04207113e-01 -1.01549201e-01 -9.24605876e-02
  7.12168366e-02 -5.98545698e-03  3.02374899e-01 -7.43487403e-02
  2.21167445e-01  1.40115693e-01  3.34347278e-01  1.29347444e-01
 -2.11375609e-01  1.09272912e-01  3.88064295e-01 -1.34281702e-02
  9.29476917e-02 -1.23789802e-01 -2.56154239e-01  5.31088747e-02
 -7.17796311e-02 -1.96831077e-01 -3.13531011e-01  9.00370106e-02
 -9.37918946e-02  3.45344767e-02  5.78354374e-02  5.30566946e-02
  1.82993904e-01  2.36580670e-01  2.94988424e-01 -1.08564489e-01
  3.30079317e-01  1.79957628e-01 -3.86926830e-01  3.40280905e-02
  2.24034935e-01 -7.75760114e-02  1.94780737e-01  2.36494362e-01
  1.76289231e-01  1.66436918e-02 -1.09241530e-02  1.02906436e-01
  2.00346053e-01  2.73898542e-01 -2.45619282e-01 -7.52827004e-02
  7.15524703e-03  6.97492361e-02  1.80389643e-01 -1.95648164e-01
 -7.45992959e-02 -4.96678613e-02 -3.66919935e-02  7.37155676e-02
 -3.29874009e-01 -6.26114383e-03 -3.65805387e-01  8.73363167e-02
 -4.93819937e-02 -1.93178698e-01 -2.47410327e-01 -2.93602586e-01
  2.38762051e-02 -4.34927009e-02 -1.37902617e-01  1.06954515e-01
 -7.36350119e-02 -2.62763143e-01 -1.40564635e-01  2.79530644e-01
 -1.82182848e-01  8.33344609e-02 -3.29224885e-01 -1.23422220e-01
  4.95084167e-01 -1.73852667e-01 -2.94623710e-02 -3.06933016e-01
  2.57160902e-01  1.31591484e-01  7.60342479e-02 -1.35336658e-02
 -3.42629105e-03  3.18914115e-01 -1.59233883e-01 -1.21149063e-01
 -3.56780618e-01  2.93313920e-01  4.07789201e-02 -6.65487647e-02
 -2.77247250e-01  9.51515585e-02  3.96370411e-01  1.62247494e-02
 -9.65919644e-02 -2.27592140e-01 -2.83067882e-01  9.19745043e-02
  4.25790623e-02  1.06192447e-01 -6.10811170e-04 -2.10608076e-03
 -2.48976946e-01  3.67313921e-02 -1.46558434e-01  2.16490582e-01
 -1.52493834e-01  2.37107739e-01 -2.84573376e-01 -2.58367598e-01
  1.94338903e-01  1.70894682e-01 -1.58123463e-01 -1.24818616e-01
  2.54388690e-01 -3.92650127e-01  1.91609383e-01  2.23162413e-01
 -1.91733968e-02 -1.27725363e-01  1.85193688e-01 -1.09984756e-01
  2.90284753e-01 -2.98258383e-02  3.54534760e-02 -6.37676001e-01
 -1.86879158e-01  6.29625395e-02 -2.39144918e-02 -2.71277875e-01
 -2.78428108e-01 -1.02162674e-01 -1.83634460e-01  4.46858965e-02
  4.34606113e-02 -5.43826461e-01 -6.08722679e-02  3.42522621e-01
 -7.10735321e-02 -2.00201407e-01 -1.07812434e-01 -2.36415178e-01
 -1.90102875e-01  6.30449317e-03 -2.95528203e-01  2.37457901e-01
 -5.85236847e-02  1.80845976e-01  9.09155682e-02 -2.80443251e-01
  4.28792447e-01  1.44012153e-01  3.32246006e-01  9.98334140e-02
 -2.41268218e-01 -8.08466226e-02  1.44942999e-02 -2.48126179e-01
 -1.86386645e-01  8.23578089e-02 -1.24733850e-01  5.06355427e-02
 -6.00852743e-02  3.34731996e-01 -1.24464542e-01 -7.57545680e-02
 -1.43038049e-01  3.19973022e-01 -2.18173385e-01  1.74105734e-01
 -6.67040348e-02  8.40051472e-02  4.61902499e-01  2.03247175e-01
  8.11227784e-02 -6.97226003e-02  1.62141457e-01 -1.47618741e-01
 -5.20852879e-02  1.58394724e-01  6.03910647e-02  5.35080314e-01
 -1.59543119e-02  4.09413464e-02  4.58234129e-03  2.77751565e-01
 -2.99195647e-02 -1.12195030e-01 -1.91192687e-01 -2.76933700e-01
  6.53566599e-01 -4.06965613e-01 -6.02829196e-02 -2.53998429e-01
  3.39637876e-01  2.88226873e-01 -2.88103253e-01 -4.97397892e-02
  2.35112414e-01  3.87308449e-01 -2.14547873e-01  1.39266074e-01
  1.95445672e-01 -6.59860075e-02 -1.89712346e-01 -2.69905210e-01
 -8.38316977e-02 -3.34821284e-01 -2.26550922e-02  1.15785897e-01
  3.71701807e-01 -1.03552192e-01 -1.38313919e-01 -7.48767424e-03
  2.92096883e-01 -8.30339789e-02 -2.64554560e-01  8.60733837e-02
  2.46469051e-01  3.17522347e-01  4.80000019e-01 -1.74268991e-01
  2.65352800e-02  3.36266100e-01  2.44693086e-01 -1.54580250e-01
  2.08169803e-01 -1.10440999e-01  2.73685008e-01  2.19925568e-01
 -1.91004455e-01  2.68832505e-01 -3.51241261e-01  1.01985589e-01
 -1.19353004e-01  3.45490932e-01  1.36892825e-01  1.64682537e-01
  4.71542291e-02 -3.89477849e-01 -4.49873269e-01  1.77376673e-01
  2.76582818e-02 -1.83519304e-01 -3.31874907e-01 -2.62967765e-01
 -5.59171885e-02  1.31077543e-01  9.74434465e-02 -8.42873976e-02
 -2.46848002e-01 -2.25193173e-01  1.24314576e-01 -2.07300991e-01
 -3.44877720e-01  2.64127046e-01 -6.84982687e-02 -1.60260245e-01
 -2.66319543e-01 -3.17839265e-01 -1.73503533e-01 -2.62192488e-01
  4.32322025e-02 -2.14988172e-01  7.00814947e-02 -1.12164080e-01
 -6.90564290e-02  6.15647957e-02  1.99018359e-01  3.77736747e-01
  7.53866211e-02  8.40015635e-02  1.71250314e-01  2.69614398e-01
 -3.60900275e-02  1.36881083e-01 -7.68163055e-02  3.79872739e-01
 -1.90962225e-01  1.87157810e-01 -2.90650249e-01 -1.74202979e-01
 -2.38331612e-02 -1.22772351e-01  3.96941900e-02 -1.44133836e-01
  5.35367012e-01  2.72384316e-01  1.66689694e-01  3.33225310e-01
 -2.44063824e-01  9.44743901e-02  7.19119832e-02 -1.92077994e-01
  2.51774751e-02  4.14228514e-02  1.32116884e-01 -2.30691925e-01
  1.38989389e-01  2.74537671e-02  1.37492880e-01  1.31336659e-01]"
cmake error on MacOS stat:community support,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.12.6
- **TensorFlow installed from (source or binary)**: source
- **Python version**: python 3.6.2
- **TensorFlow version**: master
- **Bazel version**: 0.7.0-homebrew. Build timestamp: 1510456291
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.38)
- **Exact command to reproduce**:
```
# in tensorflow directory
cd tensorflow/contrib/cmake
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release -DPYTHON_EXECUTABLE=/usr/local/bin/python3
make tf_tutorials_example_trainer
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I'm following the instructions [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cmake/README.md) to build using cmake on Mac. However during make, the following error is thrown.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
# ...
Scanning dependencies of target tf_tutorials_example_trainer
[100%] Building CXX object CMakeFiles/tf_tutorials_example_trainer.dir/Users/kevenwang/VirtualBoxShared/another_tf/tensorflow/cc/tutorials/example_trainer.cc.o
[100%] Linking CXX executable tf_tutorials_example_trainer
Undefined symbols for architecture x86_64:
  ""_ares_cancel"", referenced from:
      on_readable_cb(grpc_exec_ctx*, void*, grpc_error*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)
      on_writable_cb(grpc_exec_ctx*, void*, grpc_error*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)
  ""_ares_destroy"", referenced from:
      grpc_ares_ev_driver_unref(grpc_ares_ev_driver*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)
  ""_ares_free_data"", referenced from:
      on_srv_query_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
      on_txt_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
  ""_ares_gethostbyname"", referenced from:
      grpc_dns_lookup_ares_impl(grpc_exec_ctx*, char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
      on_srv_query_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
  ""_ares_getsock"", referenced from:
      grpc_ares_notify_on_event_locked(grpc_exec_ctx*, grpc_ares_ev_driver*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)
  ""_ares_inet_ntop"", referenced from:
      on_hostbyname_done_cb(void*, int, int, hostent*) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
  ""_ares_init"", referenced from:
      _grpc_ares_ev_driver_create in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)
     (maybe you meant: _grpc_ares_init, _grpc_resolver_dns_ares_init )
  ""_ares_library_cleanup"", referenced from:
      _grpc_ares_cleanup in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
  ""_ares_library_init"", referenced from:
      _grpc_ares_init in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
  ""_ares_parse_srv_reply"", referenced from:
      on_srv_query_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
  ""_ares_parse_txt_reply_ext"", referenced from:
      on_txt_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
  ""_ares_process_fd"", referenced from:
      on_readable_cb(grpc_exec_ctx*, void*, grpc_error*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)
      on_writable_cb(grpc_exec_ctx*, void*, grpc_error*) in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)
  ""_ares_query"", referenced from:
      grpc_dns_lookup_ares_impl(grpc_exec_ctx*, char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
  ""_ares_search"", referenced from:
      grpc_dns_lookup_ares_impl(grpc_exec_ctx*, char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
  ""_ares_set_servers_ports"", referenced from:
      grpc_dns_lookup_ares_impl(grpc_exec_ctx*, char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
  ""_ares_strerror"", referenced from:
      grpc_dns_lookup_ares_impl(grpc_exec_ctx*, char const*, char const*, char const*, grpc_pollset_set*, grpc_closure*, grpc_lb_addresses**, bool, char**) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
      _grpc_ares_init in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
      on_hostbyname_done_cb(void*, int, int, hostent*) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
      on_srv_query_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
      on_txt_done_cb(void*, int, int, unsigned char*, int) in libgrpc_unsecure.a(grpc_ares_wrapper.cc.o)
      _grpc_ares_ev_driver_create in libgrpc_unsecure.a(grpc_ares_ev_driver_posix.cc.o)
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[3]: *** [tf_tutorials_example_trainer] Error 1
make[2]: *** [CMakeFiles/tf_tutorials_example_trainer.dir/all] Error 2
make[1]: *** [CMakeFiles/tf_tutorials_example_trainer.dir/rule] Error 2
```",True,"[-1.73974216e-01 -7.36467719e-01 -4.39521313e-01 -6.28542304e-02
  2.96340317e-01 -1.81650072e-01 -1.33924216e-01  4.16034088e-03
 -2.86771059e-01 -2.38500938e-01  1.44844636e-01 -3.08304101e-01
 -1.34253696e-01 -7.40979314e-02 -2.19677180e-01  2.84249425e-01
  2.16613896e-02 -2.83759117e-01  4.35279906e-02  7.90797472e-02
 -1.71022251e-01 -5.60583770e-02 -1.35802805e-01  3.06026042e-01
  1.73589960e-01  3.61024380e-01 -2.94006884e-01 -2.94713527e-02
 -7.41650760e-02  5.43873459e-02  8.17984104e-01  4.31795493e-02
  2.48526484e-01  1.19349651e-01  2.36629114e-01  3.17030191e-01
  2.32574493e-02 -1.45206839e-01 -4.09759760e-01 -2.42264830e-02
  1.81925092e-02  1.57453358e-01  2.07793087e-01  1.30749017e-01
  5.26168942e-02  1.61563605e-03  1.29218791e-02 -6.88349083e-02
 -7.02508539e-02 -2.62519032e-01 -9.90848243e-02 -7.73470253e-02
 -2.94572771e-01 -3.25135589e-01 -1.04421355e-01  1.71664596e-01
 -2.11426973e-01 -1.17374092e-01  1.30341679e-01  2.79985547e-01
  2.50730813e-01 -1.59660559e-02 -2.07096577e-01  1.46223918e-01
  1.17967278e-01  3.86598200e-01  4.28004503e-01 -3.16752195e-01
  5.35711884e-01 -1.80054992e-01 -2.03138124e-02 -6.97598010e-02
 -2.48432040e-01 -1.12204880e-01  2.40051262e-02  4.09898341e-01
  6.65739179e-04  3.31566818e-02  6.26715794e-02 -3.86002123e-01
 -6.13982640e-02 -2.07938761e-01  2.80533701e-01 -7.08400384e-02
 -6.85079396e-02  1.66717708e-01  3.63158971e-01  1.38611784e-02
  1.97859168e-01 -6.48752302e-02  4.16550845e-01  2.38468140e-01
  1.45916224e-01  8.48073959e-02  3.25236201e-01  1.36840478e-01
  8.03943500e-02  2.04944372e-01  5.24859503e-02 -2.36513130e-02
 -6.76244423e-02 -2.89404225e-02 -3.70489135e-02  1.98062733e-01
 -1.55218542e-02 -3.74601305e-01  3.47226322e-01  8.34171399e-02
 -7.47587234e-02  2.51162797e-04  8.87718722e-02  2.63936222e-01
  2.03029782e-01 -1.31799161e-01  3.86179239e-01  1.31085236e-03
 -1.94143802e-01  2.65550852e-01 -1.68667343e-02  8.84752512e-01
  5.38417362e-02  1.72765572e-02 -2.69446280e-02  9.07345340e-02
  4.88478899e-01  2.45742619e-01 -1.75611913e-01  4.00202051e-02
  3.92920114e-02 -4.39367369e-02  1.71029359e-01 -5.96870445e-02
 -1.35788530e-01  3.32414925e-01 -3.06449234e-01 -1.23798117e-01
  9.74655151e-02  2.83273816e-01  5.10483310e-02 -2.11880594e-01
 -2.41142541e-01  2.39714801e-01  1.30560547e-01 -3.30479681e-01
 -2.08672002e-01 -1.03003822e-01 -4.80862409e-02  1.70714885e-01
 -1.02137342e-01 -5.35396859e-02 -3.40264887e-01  8.97074789e-02
 -2.49492005e-01  3.48304629e-01  4.48239356e-01  2.68102527e-01
  4.52770948e-01  3.38509567e-02 -1.28758818e-01 -5.61310410e-01
 -1.48032308e-01  2.76714027e-01 -2.23861247e-01 -1.87656097e-02
 -9.48396623e-02 -1.98639520e-02 -3.56387496e-01 -7.71111771e-02
 -1.61994368e-01  1.79254979e-01 -2.04600409e-01 -1.04403690e-01
 -1.58449560e-01  1.92963377e-01 -1.39436185e-01 -1.79366581e-02
  4.45337296e-01 -5.51528394e-01 -1.05530225e-01  5.71345314e-02
  6.92972988e-02 -1.12736553e-01  3.24446224e-02 -2.68473942e-03
 -1.61146581e-01  1.39783755e-01 -7.25971758e-02  1.54215410e-01
 -3.20531726e-01 -1.60475522e-01 -3.65817904e-01 -1.69320479e-02
  4.40561295e-01  1.65284008e-01 -8.37704390e-02 -1.95543304e-01
  2.21073762e-01 -3.03594023e-02 -2.20646620e-01  3.92086506e-01
 -1.99286252e-01  5.37506044e-02  5.09278327e-02 -1.41116410e-01
  1.56635150e-01 -1.67869091e-01 -3.50883305e-01  9.42779928e-02
 -6.36788130e-01  4.91718724e-02 -5.19118086e-02 -1.57212049e-01
  1.05505466e-01  9.77488160e-02 -3.98488700e-01  1.61777020e-01
  3.14460456e-01 -6.29790276e-02 -2.23310471e-01  8.11367780e-02
  6.28664494e-02 -1.21510379e-01 -2.03215495e-01 -4.43172991e-01
 -1.07702471e-01 -8.07943102e-03 -2.85353422e-01  1.67941749e-01
 -1.02007359e-01  2.16655031e-01 -2.95023292e-01  1.88550472e-01
  2.99299479e-01 -2.10864723e-01  7.68224001e-02  3.23589332e-02
  1.43490463e-01 -3.57955337e-01 -1.89865604e-01  2.90527463e-01
 -5.89659572e-01 -2.49751210e-01  7.35379457e-02  9.42506045e-02
  1.21874899e-01 -1.31968409e-04 -2.98362195e-01  1.82731926e-01
  2.76240706e-03  1.59036905e-01 -9.42652375e-02  7.78073668e-02
  3.40856671e-01  1.37997359e-01  3.54562640e-01  3.73632312e-01
 -1.04196638e-01 -1.60306841e-01  4.92449328e-02 -1.04677081e-01
  2.66499579e-01  3.07630956e-01 -7.75579736e-02  5.56268692e-01
  3.90002877e-02  2.38149807e-01 -3.03461313e-01  1.23376325e-01
  1.85011938e-01  2.59865560e-02  3.16020072e-01 -3.08152080e-01
  2.91801900e-01 -1.74539790e-01 -2.66657412e-01  9.54242498e-02
  2.17853516e-01 -6.47875443e-02 -9.28831547e-02  1.21543169e-01
  1.48490503e-01  7.79579431e-02 -8.29550251e-02 -2.38306671e-01
 -1.75889894e-01 -7.65565485e-02  1.98393576e-02 -4.45448041e-01
 -3.55224460e-01  1.99296787e-01 -4.24764268e-02  1.01459533e-01
  1.27084792e-01  4.09754634e-01 -1.33086190e-01 -3.03007849e-02
  8.45857263e-02  3.00350152e-02  1.02509066e-01  3.93689871e-01
 -9.65703055e-02 -8.76555741e-02  4.41997796e-02  6.24803752e-02
  4.06284928e-02  2.31603444e-01  4.19970155e-01  2.53283717e-02
  7.03095078e-01 -8.35026577e-02  2.30207875e-01 -5.05993441e-02
 -2.08053574e-01  2.98953652e-01 -4.01000902e-02 -9.08199698e-02
 -4.25979733e-01  6.46466970e-01  3.64712179e-01 -1.26629904e-01
  2.22430557e-01 -1.69861242e-01 -3.53802443e-01  1.52023003e-01
  2.88750798e-01  1.22003108e-01 -2.34534144e-01 -4.24978971e-01
 -6.68268055e-02  4.60459143e-02  1.22710019e-01 -9.30369943e-02
 -5.24250090e-01  1.22281425e-01 -2.91136563e-01 -4.13047858e-02
 -2.13860199e-01  3.63187134e-01 -1.13573052e-01 -3.17208976e-01
  3.19854878e-02 -2.27653742e-01  1.73850745e-01 -5.11517525e-01
 -1.61836386e-01 -1.23385750e-01  3.41217399e-01  3.94124776e-01
 -3.39108169e-01  2.49839559e-01  8.13295543e-02  3.96419019e-02
 -2.66365558e-01 -2.94312716e-01 -3.51833850e-01  2.37712011e-01
 -6.49534166e-03 -5.80977276e-03  4.25751716e-01  4.29609895e-01
 -3.76794457e-01 -2.38725275e-01 -3.37898403e-01 -2.03553975e-01
 -6.49196282e-03 -3.03151086e-02 -5.45831323e-01 -5.39809577e-02
  2.86695719e-01  1.58967271e-01 -3.11868954e-02  1.76083729e-01
 -8.04260299e-02  3.54940057e-01  5.68130016e-01  1.53542519e-01
 -2.44366229e-01  1.03060305e-01  4.61244807e-02 -2.74005651e-01
 -2.50787199e-01 -1.69393346e-01 -7.68852979e-02 -2.21661463e-01]"
TF 1.4 build_all_android.sh fails with nsync.a stat:awaiting response type:build/install stale,"Running

    NDK_ROOT=""$HOME/Library/Android/sdk/ndk-bundle"" bash $MAKEFILE_DIR/build_all_android.sh

results in this error

```
/Users/era/Library/Android/sdk/ndk-bundle/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: tensorflow/contrib/makefile/downloads/nsync/builds/armeabi-v7a.android.c++11/nsync.a: malformed archive header name at 8
/Users/era/Library/Android/sdk/ndk-bundle/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: tensorflow/contrib/makefile/downloads/nsync/builds/armeabi-v7a.android.c++11/nsync.a: malformed archive header name at 8
/Users/era/Library/Android/sdk/ndk-bundle/toolchains/arm-linux-androideabi-4.9/prebuilt/darwin-x86_64/bin/../lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: fatal error: tensorflow/contrib/makefile/downloads/nsync/builds/armeabi-v7a.android.c++11/nsync.a: attempt to map 60 bytes at offset 67800 exceeds size of file; the file may be corrupt
collect2: error: ld returned 1 exit status
make: *** [.../tensorflow/tensorflow/contrib/makefile/gen/bin/benchmark] Error 1
```

NDK version 15c, Android Studio 3.0, macOS High Sierra 10.13.1",True,"[-2.92193502e-01 -1.49734750e-01 -2.49038100e-01 -1.52920365e-01
  7.67662525e-02 -3.45987618e-01 -2.74622202e-01  1.51715443e-01
 -4.05364156e-01  1.49122924e-01  8.67787451e-02 -3.47765207e-01
  1.87122822e-01 -1.56557094e-02 -1.04574062e-01  2.25955695e-01
 -3.07539314e-01 -5.77161551e-01  2.51495093e-01  2.20064804e-01
 -2.27118939e-01  6.42033890e-02  2.97516286e-01  9.30761322e-02
  2.06740826e-01  2.43871629e-01 -1.81502342e-01 -1.52798280e-01
  1.12517849e-01  2.46751115e-01  3.75648588e-01  4.87517156e-02
 -5.69698364e-02  1.41053692e-01  2.88785726e-01  8.62532407e-02
 -4.16220129e-01 -4.27569635e-03 -2.50834525e-01 -1.93613857e-01
 -6.50241747e-02  1.41554028e-01  1.02402896e-01  5.00144809e-02
 -4.05971944e-01  3.93041298e-02 -1.30751297e-01  1.18996903e-01
 -2.70327389e-01 -1.18326001e-01 -1.03701651e-01  3.34487826e-01
 -1.81901783e-01 -8.26914191e-01  2.15981156e-02  8.66946131e-02
 -2.30287388e-01  4.29525346e-01  3.56331587e-01  4.71148938e-01
  1.94784999e-01  5.27291223e-02  8.74735415e-03  1.15952089e-01
  1.51951209e-01  1.70084208e-01  1.05440784e-02 -2.35440254e-01
  5.03560528e-02 -1.22093901e-01  8.08910578e-02 -4.39654216e-02
  2.11551145e-01 -1.18266158e-01  7.81519935e-02  1.19426265e-01
 -2.24231303e-01  4.77850616e-01 -3.90378177e-01 -4.03827578e-01
 -4.59183604e-01  2.92033628e-02 -1.92429736e-01  1.55856490e-01
  1.34568438e-02  1.04118392e-01  1.22528441e-01 -1.14891566e-01
  4.51150417e-01  2.50786304e-01  2.23376065e-01  3.68195891e-01
  2.06462502e-01  1.73142940e-01 -2.02874929e-01  2.70041466e-01
 -2.68398017e-01  3.89122665e-01 -3.92310381e-01 -9.02583450e-02
 -1.53480023e-01 -2.80719340e-01 -2.18422383e-01  3.26483361e-02
 -1.38414979e-01 -1.74667686e-01 -1.07455522e-01 -5.75327836e-02
 -1.89007059e-01  3.13201725e-01  9.33562964e-02 -5.64644337e-02
 -1.84037685e-02  5.69619425e-02  2.18219072e-01  1.96790114e-01
 -1.03901610e-01 -2.38669664e-01 -1.78730413e-01  3.95509511e-01
 -1.07229009e-01 -2.95709640e-01 -1.44314215e-01 -2.09896892e-01
  9.97168124e-02 -8.10677260e-02 -4.10011187e-02 -8.07127729e-03
  3.77625793e-01  2.17923567e-01 -4.07692716e-02  1.53225929e-01
 -2.64844120e-01 -1.82084143e-01  8.13834276e-03  1.68104112e-01
 -5.36444336e-02 -2.10759282e-01  4.96561006e-02  1.55124798e-01
 -1.56639233e-01  1.55781865e-01  4.59664851e-01 -7.57080764e-02
 -7.96564892e-02 -1.67428777e-01 -1.28488854e-01  2.38158435e-01
 -1.72639266e-01 -1.19511656e-01 -2.91013181e-01 -4.18955654e-01
 -1.50482640e-01  3.79074931e-01  3.87095273e-01 -1.16179585e-02
  1.30961925e-01  5.94390035e-02 -2.27326944e-01 -3.26375902e-01
  8.66582617e-03  2.27308214e-01  1.43739820e-01  2.49162644e-01
 -1.26948416e-01  2.37292767e-01 -3.99886191e-01  1.52552113e-01
 -1.93250909e-01 -2.25887835e-01 -3.64985354e-02  5.54411933e-02
  2.28159815e-01  9.75137502e-02  2.80929387e-01 -3.16281140e-01
  6.56687282e-03  1.92241333e-02 -7.53506646e-02  3.10220927e-01
  5.41558921e-01  1.61369666e-01  3.49432789e-02 -6.24861717e-02
 -1.57939404e-01 -7.28485361e-02 -3.98807526e-02  1.92705944e-01
  3.23610008e-02  1.06885999e-01 -9.61669311e-02  2.73561925e-01
 -3.01636755e-01  2.81524360e-02 -1.22670755e-01 -7.17198551e-02
  2.11623490e-01  4.92860153e-02  5.36954641e-01 -5.44566289e-02
  2.58730650e-01  2.05467418e-01 -1.12922192e-01 -1.70848429e-01
  2.18009204e-01  1.38628572e-01 -3.09526503e-01 -9.82348472e-02
  1.32316902e-01  1.88225601e-03 -3.22276838e-02 -3.55841041e-01
  1.49166510e-01 -2.45899931e-01 -1.59042716e-01  1.01117678e-01
 -2.08718956e-01  1.22079656e-01 -7.10605457e-02 -1.14625782e-01
  2.40868792e-01 -3.02057981e-01 -4.49080728e-02 -1.43335700e-01
  1.52535528e-01  9.34650898e-02  1.45940810e-01 -2.81865209e-01
  6.53013885e-02  4.13072407e-02 -4.37935531e-01 -1.39766946e-01
  8.10311288e-02  7.46509507e-02 -7.69022778e-02 -3.08234036e-01
  4.07769203e-01 -4.35055912e-01 -5.02384976e-02 -1.65872321e-01
 -8.98612961e-02 -1.22326232e-01  1.16826542e-01 -4.82269883e-01
  1.58346623e-01 -2.39871815e-01  2.57775426e-01  3.49352300e-01
  6.66504912e-03 -4.88506481e-02 -4.69309539e-02  2.33547032e-01
  5.10489047e-02  1.46949306e-01  2.05202073e-01  2.36312039e-02
 -2.57523656e-02  1.31618708e-01  2.21725509e-01 -1.81708604e-01
 -1.83925405e-01  4.36466157e-01  4.62140888e-04  6.22304231e-02
 -1.21046817e-02 -3.53263378e-01 -3.04569900e-01 -1.93923891e-01
 -1.49754032e-01 -1.80312172e-01  2.87035286e-01 -1.13511786e-01
  4.67325926e-01 -8.80176499e-02  3.24220121e-01  3.04093480e-01
  3.74544948e-01 -2.28396188e-02 -1.61997810e-01  3.16071928e-01
  1.33799002e-01  4.13061291e-01 -1.90783769e-01 -8.94344449e-02
  3.61091793e-01 -2.28747755e-01 -1.05086945e-01 -1.59793854e-01
 -2.58064419e-01 -2.08321661e-01  1.76340267e-01 -1.52094364e-02
 -1.84268892e-01  1.11339197e-01 -2.20679879e-01 -1.10317186e-01
 -1.96226954e-01  1.64772630e-01  3.21654305e-02  1.31125748e-01
  1.54227272e-01  1.36898421e-02  2.60567307e-01  1.78218096e-01
 -2.54156709e-01  9.24605727e-02 -1.21477574e-01  2.83559054e-01
  4.68196869e-01 -3.61593366e-01  4.27736193e-01 -1.62789866e-01
 -2.85276175e-01  4.10830021e-01 -7.97109008e-02  4.76341248e-01
 -2.00954482e-01  4.77484912e-01 -5.20397387e-02 -5.02004474e-03
  4.12931204e-01 -2.67957509e-01 -2.53527999e-01  2.59186774e-02
  3.75110984e-01 -6.60864562e-02 -1.85631678e-01 -1.67506099e-01
  5.16427681e-02  3.33115309e-02  2.30898798e-01  1.25993803e-01
 -1.35117978e-01  2.74289727e-01 -1.68733999e-01  4.06213067e-02
 -1.24261625e-01  2.35010490e-01 -1.41765922e-01 -2.46309087e-01
  1.00705586e-03 -4.48648423e-01 -4.04597111e-02 -2.62335062e-01
 -2.19162121e-01 -2.58290768e-03  2.80583799e-01 -9.62557718e-02
 -8.94812793e-02  5.70956945e-01  3.34608182e-02 -1.35606170e-01
 -6.37061447e-02 -4.23484147e-02  7.79480860e-02 -8.86861905e-02
  2.72745900e-02  6.53316155e-02 -6.17525075e-04  6.67533636e-01
 -2.73860633e-01  3.88884723e-01 -5.55226579e-02 -1.81767628e-01
 -1.02618285e-01 -5.13248742e-01 -2.16110498e-01  1.16876259e-01
 -5.21147735e-02  6.06848970e-02 -1.80962384e-01  3.55557740e-01
 -5.73403418e-01 -3.27149928e-01  2.84949929e-01 -1.07203230e-01
 -2.31827218e-02 -9.48970914e-02  1.51429370e-01  5.48555776e-02
  3.49180326e-02 -6.71328232e-03  2.24339038e-01  2.25691050e-01]"
Missing MPI collectives op symbols in TF build ,"Running into a build issue when trying to use MPI collectives. We are able to build a recent version of TF using NVIDIA's changes for CUDA 9/cuDNN 7. However, it appears that the build strips the MPI collectives library ops:

```
% python -c 'import tensorflow.contrib.mpi_collectives as mpi'
Traceback (most recent call last):
 File ""<string>"", line 1, in <module>
 File ""/mnt/home/lib/python3.6/site-packages/tensorflow/contrib/mpi_collectives/__init__.py"", line 126, in <module>
   from tensorflow.contrib.mpi_collectives.mpi_ops import size
 File ""/mnt/home/lib/python3.6/site-packages/tensorflow/contrib/mpi_collectives/mpi_ops.py"", line 59, in <module>
   'MPIAllreduce'])
 File ""/mnt/home/lib/python3.6/site-packages/tensorflow/contrib/mpi_collectives/mpi_ops.py"", line 51, in _load_library
   (expected_op, name))
NameError: Could not find operator MPISize in dynamic library mpi_collectives.so
```

It seems like the issue might be caused by commit 5c7f9e3, which changes linking behavior, but we're unable to bisect due to commit order dependencies.


Anyone have an idea how to fix the issue? We're willing to update the MPI collectives code and submit a PR fix.



### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: We've applied NVIDIA's CUDA 9/cuDNN 7 patches for mixed-precision per this page: http://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html#training_tensorflow
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 14.04
- **TensorFlow version (use command below)**: ea94bbe9fa9f9b3d01fb057c02ef7873d76bf09c
- **Python version**: 3.6
- **Bazel version**: 0.5.4
- **CUDA/cuDNN version**: CUDA 9.0.103_rc/cuDNN 7.0-rc
- **Exact command to reproduce**: `python -c 'import tensorflow.contrib.mpi_collectives as mpi'`




",True,"[-0.17644988 -0.36034277 -0.22549286  0.02192051  0.11603835 -0.21284017
 -0.0881318  -0.07163478 -0.24897775  0.04900496 -0.09216496 -0.18140325
 -0.05552325  0.02304192 -0.09527891 -0.16049111 -0.21960835 -0.16535369
  0.01474254  0.23149303 -0.2771028   0.00947098 -0.10831402  0.26618007
  0.21828893  0.03517253 -0.14100648 -0.0158944   0.2037648   0.17107348
  0.22627863 -0.03245959  0.10216329  0.09372063  0.21833524  0.35012853
 -0.05861208 -0.17235085 -0.177374   -0.17737678 -0.06326988 -0.03216722
 -0.0287304   0.15651819 -0.20991658 -0.01595472  0.11621055  0.05547054
 -0.21176225  0.12252492 -0.07553107  0.15675539 -0.34638816 -0.2996298
  0.135393    0.01253647  0.09242757  0.46850455  0.22372401  0.00201934
  0.08855684 -0.133021   -0.08086774 -0.01959594  0.0801836   0.11637981
  0.03450413  0.14044993  0.10587236 -0.26000828  0.10800713  0.03374758
 -0.23342827 -0.17352217 -0.07850726  0.4921478  -0.15405798  0.14808288
 -0.02583662 -0.19786666  0.18128633  0.1691677   0.06036811  0.2477833
  0.18473513  0.12399846  0.22558151 -0.13051067  0.339495   -0.06783582
  0.20804043  0.00795733  0.15537615  0.15451926  0.1753686   0.15804182
  0.20481327  0.34805593 -0.11507322 -0.02485752 -0.23772687 -0.17764598
 -0.28279552  0.24823782 -0.31462646 -0.1185284  -0.01523775  0.20947513
 -0.00689172 -0.05238604  0.26543608 -0.01490119 -0.0153275  -0.14796105
  0.03610376 -0.06937616 -0.18301809 -0.11624424  0.21535364  0.03748627
 -0.11270166 -0.1946251  -0.11275079 -0.12461054  0.16342431  0.03456726
 -0.22397363 -0.01403087  0.05024648 -0.08718727  0.13254222  0.12814009
  0.01445913 -0.24146092  0.3591792  -0.11108605 -0.53481126 -0.2783621
 -0.37657148 -0.02652482 -0.07159303  0.09999664 -0.18040362 -0.22200724
  0.12512514  0.10652328 -0.2484743   0.27146432 -0.05594949  0.2502673
 -0.08482    -0.05584025  0.07541706  0.2608691  -0.01770715  0.10575151
  0.18031996 -0.11203091 -0.101069   -0.26323718 -0.14153345  0.15357442
 -0.04163744  0.11379116 -0.05975352  0.11520778 -0.43245912 -0.11769335
  0.02685464 -0.10291646 -0.27912265 -0.19307752  0.02606478 -0.04208132
  0.52116865 -0.24448477 -0.2545903  -0.16002671  0.07679772  0.26528633
  0.13685358  0.04422586  0.09273048 -0.30752555 -0.05709681  0.16104603
  0.33235997  0.03002602 -0.02436225  0.08902226 -0.09998015  0.23675537
  0.14937568  0.15183495 -0.13263354 -0.1020449   0.2672475   0.20454742
  0.23618294  0.2798596  -0.2694353   0.14205432  0.17623898  0.1378214
 -0.01734536  0.11340763 -0.18309359 -0.3427351  -0.27560258  0.2245093
 -0.11828099 -0.09773497  0.16561314 -0.2649492   0.04761048 -0.05250711
 -0.2565737  -0.30489397 -0.09951176 -0.17667419  0.15051205 -0.05380181
 -0.01726146 -0.18571033  0.25065458 -0.08583416  0.05711282 -0.14874586
 -0.24405916  0.2687232  -0.1787436  -0.16236062  0.478114    0.10829406
  0.19732495  0.11186451  0.05111416 -0.21848068 -0.04239805 -0.08269978
 -0.05951924 -0.28390774 -0.13258971 -0.277678    0.14083064 -0.05050689
  0.23014422  0.12372278 -0.14256075  0.1491303  -0.11154777 -0.27313602
  0.2805367   0.10469832  0.19429815 -0.06946933 -0.2177976   0.06628924
  0.02320747 -0.11431579  0.02373813  0.39186168 -0.10950105  0.23222747
  0.02149791  0.09776505  0.08987427  0.3960634  -0.4536518  -0.14720443
 -0.00942241  0.05959886  0.23860133 -0.3058372   0.2761879  -0.11342792
  0.3995847  -0.17405231 -0.07563204  0.19751921  0.4675577   0.35560793
 -0.26780003  0.27299166  0.12066506 -0.20432791 -0.4396605  -0.08400349
 -0.2066367  -0.02089997 -0.22709328 -0.18636502 -0.03102449 -0.0325058
 -0.17481606 -0.04440381 -0.07947124  0.03159501 -0.16388816 -0.40835
  0.11189507  0.27888137  0.14794442  0.2212306   0.06091482 -0.09606545
  0.18888251  0.0061447   0.38276905 -0.26562047  0.48098773  0.18345693
 -0.13320695  0.43979245 -0.02960467  0.22594057  0.14279954  0.31468707
  0.04220702  0.07092126 -0.02408341 -0.28188246 -0.43867707  0.05655115
 -0.06345007 -0.06969381 -0.34725726 -0.03866261 -0.0169416   0.28948426
  0.16041733 -0.36814743 -0.12606648  0.07980439  0.0332961  -0.01093883
 -0.00312119 -0.02520369 -0.07045623 -0.26110774 -0.03584568 -0.1919469
  0.0580274  -0.02116497 -0.22990397 -0.04803798 -0.0158396  -0.01468597
 -0.00070874  0.23254627  0.13386205  0.21622434 -0.27764362 -0.03130784
 -0.20156744  0.3040231   0.32044527  0.06952907 -0.08879442 -0.01679612
  0.02521268 -0.02717257 -0.05032601 -0.25451884  0.0317276  -0.10381427
 -0.3338404  -0.06976391  0.03422466  0.02537346  0.03175985  0.35961378
 -0.24870583  0.18227638  0.17463239 -0.12488545 -0.01627371  0.21632868
  0.21893123  0.0583284   0.09144797  0.09998024  0.21057469 -0.12931871]"
Name/variable scopes of tensorflow.python.layers.base.Layer stat:awaiting tensorflower,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

This is a tiny code that creates layers and connects them in series.

```python
import tensorflow as tf
from tensorflow.python.layers.base import Layer


class A(Layer):
    def build(self, input_shape):
        self.v = self.add_variable('v', (), tf.float32)
        self.built = True
    
    def call(self, inputs):
        return self.v * inputs
    

# Case 1
with tf.Graph().as_default() as graph:
    x = tf.placeholder(tf.float32, (), 'x')
   
    out = x
    out = A()(out)
    out = A()(out)
    out = A()(out)
    
    tf.summary.FileWriter('/tmp/tensorboard/1', graph=graph).close()

# Case 2
with tf.Graph().as_default() as graph:
    x = tf.placeholder(tf.float32, (), 'x')
   
    out = x
    out = A(name='a')(out)
    out = A(name='a_1')(out)
    out = A(name='a_2')(out)
    
    tf.summary.FileWriter('/tmp/tensorboard/2', graph=graph).close()

# Case 3
with tf.Graph().as_default() as graph:
    x = tf.placeholder(tf.float32, (), 'x')
   
    out = x
    out = A(name='a_1')(out)
    out = A(name='a_2')(out)
    out = A(name='a_3')(out)
    
    tf.summary.FileWriter('/tmp/tensorboard/3', graph=graph).close()
```

#### Results

Other than case 3, an unexpected graph is generated.
Is this a bug?

- Case 1

<img alt=""result"" height=""450"" src=""https://user-images.githubusercontent.com/7009040/31054076-b290a772-a6e5-11e7-8e6a-2d97420b5e0e.png"">

- Case 2

<img alt=""result"" height=""450"" src=""https://user-images.githubusercontent.com/7009040/31054078-b2a7ca7e-a6e5-11e7-9a17-805fd56e6ea7.png"">

- Case 3

<img alt=""result"" height=""450"" src=""https://user-images.githubusercontent.com/7009040/31054077-b291b7c0-a6e5-11e7-9256-be7c986094ec.png"">",True,"[-0.20944022 -0.958339   -0.52856493  0.04419753  0.03777668 -0.20031264
 -0.07446734 -0.11101153 -0.32668617 -0.07214893  0.11740167 -0.16232446
 -0.18868004  0.2941116  -0.22085187  0.36041814 -0.16334936 -0.20528975
  0.13262239 -0.05741601  0.04956979 -0.00994972 -0.17746827  0.25767937
  0.44359687  0.21129075 -0.17384878 -0.12365848  0.06536297 -0.03927914
  0.5207689   0.12946069 -0.15566397  0.20302033 -0.20666277  0.4975785
 -0.27206933 -0.02257706 -0.11373428  0.1780084   0.07628848  0.10972128
  0.08952063  0.0761518   0.08125468 -0.15371063  0.08737886 -0.08335188
 -0.12092023 -0.24699442  0.04361931 -0.04025143 -0.5593598  -0.28160575
 -0.04373993 -0.04809936  0.30354756 -0.11804649 -0.07890145 -0.06084564
 -0.12794961 -0.05395794 -0.00955062  0.11700155 -0.18450269  0.24077877
  0.25692096 -0.28255495  0.6255461  -0.22582036 -0.06113036  0.04417801
 -0.3356339  -0.20339221 -0.16688249  0.38670135  0.27575928  0.173773
  0.24592476 -0.3131352   0.04309567  0.04518376  0.29260883 -0.01504577
 -0.0561064   0.3102473   0.45460275  0.10585489 -0.01475766 -0.06887323
  0.63434637 -0.00881508 -0.13155325 -0.0367495   0.36037236  0.14124057
  0.08258346  0.31730092  0.23224902 -0.18777522 -0.0352303  -0.25749883
 -0.08480093 -0.07891627  0.1779843   0.02995135  0.08678217 -0.14222252
 -0.00941879 -0.17088723  0.05859677  0.14019406  0.16039723 -0.1884079
  0.15853193 -0.20979677 -0.22234713  0.3890602  -0.03101008  0.9882609
  0.01365238  0.07704829  0.35343158  0.25608096  0.41353026  0.13765329
  0.06567532 -0.10700744  0.08233957  0.1166259   0.48796108  0.10423551
 -0.372961    0.34141234 -0.24982014  0.06673975 -0.29463276  0.07303814
 -0.28547484 -0.08537999 -0.25667024  0.07709728 -0.18248264 -0.3587149
  0.00488514  0.1186012  -0.1375018   0.21662304 -0.11725688  0.22416644
 -0.26608664  0.1389226  -0.09987678  0.14708883  0.22701491  0.37310433
 -0.01733662 -0.0914476   0.00203045 -0.6081742  -0.03009296  0.29468796
 -0.27496344 -0.13878916 -0.03811641  0.11436202 -0.31841457 -0.35006356
  0.01951781  0.2137111  -0.03646044  0.1099617  -0.11735126  0.5184638
  0.04640906  0.04313777  0.30172288 -0.7811414   0.01190648 -0.03029574
  0.12291469  0.03071925 -0.10933657  0.03661994 -0.2129702  -0.12613437
  0.10676331  0.00703447 -0.14647314  0.02797278 -0.3392877  -0.15058121
  0.29523453 -0.18867677 -0.10353047 -0.05148248  0.22842038  0.13439062
 -0.2010151   0.3118509  -0.30082804 -0.15769768  0.07164285  0.22024706
  0.11357117 -0.31748587 -0.24535143  0.04713298 -0.6627097   0.24133305
  0.08202852 -0.4792305  -0.00522874  0.1030459  -0.3128385  -0.12537315
  0.17699187 -0.0851938  -0.3779622   0.06869355  0.14588806 -0.04063275
  0.10698904 -0.29323834 -0.15767543 -0.04813971 -0.28658646  0.05816225
  0.180358    0.31691444 -0.12787215  0.28132188  0.01853541  0.2399197
  0.19241914 -0.16116825 -0.04493488 -0.16513717 -0.07152484  0.38491794
 -0.6545218  -0.03268633 -0.0398097   0.15205613 -0.16973549  0.11275381
 -0.13738185 -0.12111993 -0.18155783  0.1420597  -0.13957691 -0.29576433
  0.43550888  0.30880782  0.277309    0.14979896 -0.1585251   0.17101909
  0.06530537  0.00281258  0.30785474 -0.06707309 -0.13014688  0.5231831
  0.05698225  0.29335934 -0.12435221  0.49253976  0.15581168 -0.24701783
 -0.0792836  -0.10587622  0.10134868 -0.29189008 -0.39935645 -0.00473372
  0.19016209  0.06891671 -0.01139903  0.12033893  0.06152008  0.08647811
 -0.22824927 -0.06418006 -0.0890651  -0.38567758  0.07445121 -0.13732953
 -0.26322484  0.08860441 -0.10722068  0.14333919  0.12490471 -0.05690914
 -0.1257548   0.03870432  0.13522989  0.03252967  0.01200678  0.47084785
  0.01993866 -0.12219507  0.04140179  0.15213926  0.098901   -0.07413371
  0.4502706   0.13752191  0.49732423 -0.09361169  0.38928208 -0.14078465
 -0.06836991  0.26643893 -0.01616087 -0.26536405 -0.2509939   0.6994475
  0.3632851  -0.12013713 -0.12927827 -0.11388092 -0.19773184  0.12358632
 -0.02496978 -0.02967362  0.00404833 -0.12120815 -0.27057865  0.13691215
 -0.18368146 -0.19526437 -0.24212903  0.07797041 -0.00325191  0.06480937
 -0.48174044  0.40613183 -0.02249403 -0.00284484  0.28064644 -0.06277708
 -0.07766527 -0.45526624 -0.10288452 -0.3239903   0.40359056  0.48451596
 -0.3220718   0.03350314  0.19921327  0.08351725 -0.5336807  -0.1391435
 -0.07337919  0.35338277 -0.00641545 -0.09882817  0.1833508   0.7276846
 -0.34441417 -0.2057146  -0.05782609 -0.24238129  0.3248139  -0.07360321
 -0.08675916 -0.18482292  0.18129018  0.3202062  -0.03889992  0.3370217
 -0.21808848  0.5247122   0.5799229  -0.02505338 -0.506931    0.21730769
  0.11811516 -0.43762034 -0.16756961 -0.04358526 -0.00261829 -0.30687368]"
"make ""build_all_ios.sh""  occur error type:build/install","I try to build tensorflow support at Android and iOS by makefile [tutorial](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile
) in current master branch 04c318b69c5b565436cfeeaab1cb7fd5419dde27

When running the build_all_ios.sh script, the below error message show
```
Undefined symbols for architecture x86_64:
  ""nsync::nsync_mu_init(nsync::nsync_mu_s_*)"", referenced from:
      tensorflow::mutex::mutex() in env.o
      tensorflow::mutex::mutex() in random.o
  ""nsync::nsync_mu_lock(nsync::nsync_mu_s_*)"", referenced from:
      tensorflow::mutex::lock() in env.o
      tensorflow::mutex::lock() in random.o
      tensorflow::mutex::lock() in histogram.o
  ""nsync::nsync_mu_unlock(nsync::nsync_mu_s_*)"", referenced from:
      tensorflow::mutex::unlock() in env.o
      tensorflow::mutex::unlock() in random.o
      tensorflow::mutex::unlock() in histogram.o
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [/Users/CSL.Peter/tensorflow/tensorflow/tensorflow/contrib/makefile/gen/host_bin/proto_text] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'armv7 compilation failed.'
armv7 compilation failed.
+ exit 1
```
The `download_dependencies.sh` and `compile_ios_protobuf.sh` run successfully but `compile_ios_tensorflow.sh` failed. I find same issues #3191 and #4252 and seem to be fixed at #4287, but this problem still happen.",True,"[-4.11990955e-02 -2.17174113e-01 -3.21346581e-01  7.60697499e-02
  2.02078104e-01 -4.18021709e-01 -1.28273249e-01 -2.02024162e-01
 -5.92958093e-01 -1.33714139e-01 -7.20150024e-02 -3.52022707e-01
 -5.08229852e-01  1.38440341e-01 -1.02105394e-01  4.28462595e-01
 -1.70776576e-01 -4.01825845e-01  1.37208447e-01  1.09478876e-01
  2.34965943e-02  1.07426047e-02  2.48222262e-01  5.97831681e-02
  3.25151443e-01  2.41903424e-01 -1.10685974e-01 -1.42189234e-01
  2.30304152e-01  1.78944111e-01  6.35220408e-01 -4.21680845e-02
  3.50094199e-01  1.35476843e-01  2.68849075e-01  4.42363143e-01
 -1.09822296e-01 -1.84506208e-01 -1.43321708e-01 -9.88559425e-02
  1.44873932e-02  2.45438084e-01  2.01193973e-01 -8.17170516e-02
  1.95603937e-01  3.76033008e-01 -6.92542046e-02 -5.26441187e-02
  1.08897574e-01 -1.92210674e-01  5.04408330e-02 -3.60055149e-01
 -3.33017170e-01 -5.87795258e-01 -1.56356394e-01  7.96690434e-02
 -6.79021701e-02  1.68887198e-01  2.54880100e-01  2.72868991e-01
 -9.05565917e-03 -1.46208912e-01  5.06431758e-02  4.24309075e-02
  1.00540683e-01  9.19303298e-02 -6.56027123e-02 -1.81010842e-01
  2.08876044e-01 -7.87463039e-02  3.70074451e-01 -9.58993137e-02
  2.28505451e-02 -3.61462831e-02  2.10285634e-01  3.32034469e-01
 -2.31939971e-01  3.14510822e-01 -3.88465635e-02 -2.20216498e-01
 -2.68519431e-01 -3.05104591e-02  1.22268148e-01  9.60094556e-02
  3.01691324e-01  6.98038638e-02  1.69637352e-01 -3.55225205e-02
  3.05656731e-01  4.38230298e-03  2.94232488e-01  9.68313292e-02
 -1.63591862e-01  1.88615501e-01  3.65418136e-01 -1.25192488e-02
  1.09260589e-01  2.27470413e-01 -1.34836346e-01 -2.92844355e-01
 -1.11941949e-01 -2.77032912e-01  1.28974050e-01  1.93338633e-01
 -2.56335050e-01  3.43305245e-02 -5.29644638e-02 -9.41713341e-03
  1.20564930e-01 -4.35717441e-02  2.01181203e-01 -2.03481600e-01
  7.17032105e-02  9.24816802e-02  1.37466341e-01 -2.29075141e-02
 -6.78566992e-02 -2.68242300e-01  1.70386285e-02  7.18486309e-01
 -4.25670028e-01 -3.23351920e-02  1.54006988e-01  3.32573685e-03
  3.37595083e-02 -1.54556990e-01 -4.28531051e-01 -4.90199029e-02
 -7.27427527e-02  1.76582292e-01 -1.53318614e-01  2.24850714e-01
 -1.37150399e-02 -1.55335546e-01  2.28682458e-01 -4.61718552e-02
 -2.28082299e-01 -4.57934618e-01 -3.93814966e-02 -4.95703757e-01
  1.26675263e-01  4.60569471e-01 -3.59842852e-02 -3.50300282e-01
  2.40004092e-01 -3.18124413e-01 -8.53835046e-02  2.33880609e-01
 -1.81094140e-01 -3.33284408e-01 -2.57684320e-01 -1.61593616e-01
 -3.39651145e-02  3.63775492e-01  4.77665633e-01  3.31140876e-01
  2.46874601e-01 -1.24689929e-01 -1.92149282e-01 -5.64482927e-01
  9.11952555e-02  1.94584459e-01 -4.88103181e-02  1.84193447e-01
  3.05187721e-02 -1.09895542e-02 -3.91660273e-01 -2.72594988e-01
 -3.04961205e-01  1.05758898e-01 -8.23453367e-02 -1.75439104e-01
  8.99357498e-02  3.61111879e-01  2.43090913e-01 -2.52871275e-01
  4.94805187e-01 -3.63904655e-01 -2.30140880e-01  2.21536949e-01
  5.29721856e-01 -9.38121006e-02  2.87775844e-02 -4.70972285e-02
 -8.54663104e-02  2.60206401e-01 -6.09117821e-02  1.40056983e-01
 -4.28768694e-02  4.50233281e-01 -4.17441934e-01  4.28062886e-01
 -3.86662841e-01 -1.69485867e-01  1.47639373e-02  1.33101359e-01
  2.01113150e-01  1.71822309e-03  6.02453277e-02  5.32466844e-02
 -5.05748577e-02  1.10241085e-01 -2.57216245e-01 -3.13210078e-02
 -3.30370441e-02 -2.25225002e-01 -2.08273172e-01  4.69934056e-03
 -1.12387300e-01  2.95581967e-01  2.00652331e-01 -3.38198602e-01
  1.90596163e-01 -2.01740623e-01 -2.20955744e-01 -4.41016778e-02
  2.68398114e-02  3.32858443e-01 -3.25190455e-01 -2.00332515e-03
  1.24341384e-01 -3.63410115e-01 -2.26449937e-01 -2.92006463e-01
  2.24118531e-01 -2.98443228e-01 -9.82810110e-02 -3.20544690e-01
 -1.01600371e-01  3.01217079e-01 -1.52426258e-01 -1.60395563e-01
  4.65000421e-01 -1.82140231e-01  4.34043050e-01 -6.08547330e-02
  1.47524506e-01 -3.60357881e-01 -2.59827852e-01 -6.44216165e-02
 -5.61717808e-01 -8.01935345e-02  4.38854843e-01 -6.35681003e-02
  3.21975559e-01 -1.87021285e-01  2.41016224e-01 -9.73362010e-03
 -1.31649494e-01  2.38900781e-01  2.58025154e-02  1.28256798e-01
  3.30971777e-01 -6.15427271e-05  7.79369920e-02  2.56579459e-01
 -1.17687710e-01  2.52816796e-01  9.51585025e-02 -1.64108351e-01
  8.41088519e-02  2.73995221e-01  1.11112811e-01  2.19618678e-01
  7.72769004e-02  3.29395473e-01 -3.65266740e-01  1.16847306e-01
  3.87499630e-02 -2.24591605e-03  2.43193552e-01  9.30947065e-02
  2.06512600e-01 -5.43039680e-01  9.00715142e-02  2.22847268e-01
  3.69577050e-01  1.94640104e-02 -6.63617030e-02 -2.47657672e-03
  1.59771055e-01  2.15065092e-01 -2.86955774e-01 -1.28406975e-02
  1.19452558e-01 -4.10744727e-01  3.26735914e-01 -2.60187685e-01
 -4.17255580e-01 -1.00092620e-01 -1.56129345e-01  5.32878526e-02
  1.81633428e-01  2.62932539e-01 -3.14217567e-01  3.56567167e-02
 -3.66228372e-02  2.56564140e-01 -1.96981683e-01  3.78709078e-01
  2.36189172e-01 -3.96771990e-02  1.52127296e-01 -1.23336866e-01
 -8.28183815e-02  4.03318275e-03  1.70899421e-01  2.07381189e-01
  8.72791588e-01 -6.35642886e-01  3.20561111e-01  3.42028216e-04
 -3.32290113e-01  5.25871634e-01  1.38585493e-01  2.78159916e-01
 -1.34693325e-01  4.46049690e-01 -1.42297940e-02 -2.10558325e-01
  7.90133849e-02 -2.08126605e-01 -2.30627224e-01 -2.72814363e-01
 -2.59066597e-02 -1.88293949e-01 -4.26595151e-01 -4.43231821e-01
 -1.99687108e-01 -1.48989454e-01 -1.92104340e-01 -4.74390015e-02
 -2.35742241e-01  4.17513520e-01 -2.39908606e-01 -8.88162404e-02
 -3.45835268e-01  3.33054304e-01 -1.32005382e-03 -3.61358076e-01
  1.46895200e-02 -1.56188130e-01  2.20027380e-02 -1.70952618e-01
 -9.55263376e-02 -2.12943017e-01  2.60895878e-01  2.53831923e-01
 -2.58178085e-01  2.76385844e-01  1.55123889e-01  2.73768306e-01
 -8.37256610e-02 -4.95436788e-02  2.19474852e-01  7.07919300e-02
 -3.86546887e-02  2.56143976e-04  1.22253060e-01  7.28205204e-01
 -1.74195752e-01  1.06838934e-01 -1.32489055e-01 -3.32051218e-01
  2.71467745e-01 -2.94291973e-01 -1.79857537e-01 -9.15805325e-02
 -8.82555768e-02  4.08267438e-01 -7.64710605e-02  3.81947458e-01
 -3.27074081e-01  1.24050304e-02  4.43181396e-01 -4.51484293e-01
 -3.04317534e-01  2.39796657e-02  2.11930960e-01  8.72787088e-02
  2.52475560e-01  8.15403610e-02  2.52800435e-01  5.45481965e-02]"
tf.pow edge case failure type:docs-bug,"The tf.pow() function has an edge case which causes it to hang with no error message.

If you try to evaluate tf.pow(x,y), when x is an integer (and thus the output tensor is also an integer), while y is a negative value, tensorflow hangs trying to cast the fraction as an integer.

Examples;

sess.run(tf.pow([5,2],[-2,3]))
sess.run(tf.pow([5],[-2]))
sess.run(tf.pow(5, -2))
sess.run(tf.pow(tf.constant(5), tf.constant(-2)))",True,"[-1.90870553e-01 -3.05277050e-01 -1.78327203e-01 -4.29199897e-02
  6.45901263e-02 -9.84728038e-02  3.76464069e-01  4.02491271e-01
 -1.66947916e-01  8.93242657e-02 -8.05565938e-02 -1.80753786e-02
 -1.17542706e-01  3.13429028e-01 -1.61175087e-01  1.49588108e-01
 -4.45685804e-01 -3.18380564e-01  1.76593482e-01  7.89487064e-02
 -9.91898924e-02  9.40608010e-02 -1.74871907e-01  2.20166266e-01
 -1.37602359e-01  1.33003980e-01  1.73152864e-01 -1.01542778e-01
  6.95335492e-02  6.38390705e-02  1.34832799e-01  1.18940704e-01
  3.05698477e-02  5.15306033e-02 -5.59898559e-03  1.71309590e-01
 -3.90620008e-02  9.42350179e-02 -1.63112640e-01 -6.79820478e-02
  4.01112735e-02 -6.14125244e-02  1.19191602e-01  8.57938528e-02
  3.10088813e-01  1.45509243e-02  1.79451331e-01  2.81112865e-02
 -3.50937210e-02  1.11464277e-01 -6.87461719e-02  2.27708608e-01
  1.92549638e-02  5.71130589e-02 -6.85495436e-02 -7.08080009e-02
  3.74554172e-02  6.60923868e-02  2.41485778e-02  1.68781653e-01
  7.20423609e-02  1.15593806e-01  1.70540109e-01  7.12656677e-02
  3.42547536e-01  3.35564911e-01  1.84940651e-01 -2.07469836e-01
  1.93550944e-01 -1.59940347e-01  2.87188113e-01  1.16057634e-01
 -4.77085829e-01  7.11696893e-02 -1.32865816e-01  2.70120889e-01
 -1.48609266e-01 -1.18460357e-02  1.77423388e-01  5.73561043e-02
 -3.17533344e-01 -8.76072720e-02  6.18598275e-02 -1.14764675e-01
 -2.40944430e-01  1.38586774e-01  2.93292463e-01 -1.80838704e-01
  5.38578510e-01  2.14450210e-02  3.77429724e-01  1.26906112e-01
 -3.05250674e-01 -1.48641327e-02  3.94363254e-01 -3.00151929e-02
  1.50961373e-02  4.41342443e-02 -8.75776038e-02 -3.34125310e-02
 -9.54102948e-02 -2.20755026e-01 -2.48681635e-01  6.76746443e-02
  1.71202362e-01  7.58118406e-02  1.02426574e-01 -2.20717713e-01
  4.57955152e-02 -1.47424206e-01  2.42124230e-01  4.92665172e-02
  4.03228879e-01  1.37606785e-01 -9.18303505e-02  5.03609926e-02
  6.04492053e-02 -2.50431746e-02 -1.52952103e-02  3.08788449e-01
  1.24245726e-01 -1.98909603e-02 -1.25333562e-01  1.08011022e-01
  6.14187233e-02  1.32997066e-01 -9.24192145e-02 -4.56690043e-02
 -1.62435710e-01 -1.81341231e-01  2.60235429e-01 -2.67756134e-02
 -1.63407743e-01 -1.12229072e-01 -1.96381897e-01 -7.15180933e-02
 -1.84348091e-01 -3.78026143e-02 -3.36753249e-01 -1.45127803e-01
  1.56389177e-01 -2.97922432e-01 -6.88794553e-02 -1.40291139e-01
  9.27506089e-02 -9.44575816e-02 -3.53355974e-01  1.87635079e-01
 -1.58356875e-01 -5.54536507e-02 -8.22396856e-03  1.04315937e-01
 -4.47277427e-02  1.60293534e-01 -4.07477207e-02  4.69780266e-02
  3.93355906e-01 -1.35594979e-01 -3.61961395e-01 -3.69259655e-01
  8.24979320e-03  3.03659171e-01  8.52888003e-02  9.37073492e-03
  3.06531906e-01  4.29425240e-01 -1.30344987e-01 -1.09119207e-01
 -2.80996740e-01  2.88428932e-01  2.85765320e-01  1.89779162e-01
 -1.21174373e-01 -9.65705365e-02  1.23831689e-01  2.89905518e-02
  1.17996082e-01 -1.28516778e-01 -1.17122784e-01 -3.53884138e-03
  2.93363512e-01  7.30330944e-02  1.91361219e-01  1.58708379e-01
 -3.57541144e-02 -1.39558688e-04 -2.89169215e-02  8.57343227e-02
 -7.94058740e-02 -2.99968254e-02 -2.51650900e-01 -2.81325012e-01
 -2.91544944e-02  1.63880110e-01 -1.76329136e-01 -2.31863558e-01
  8.52361470e-02 -1.30099416e-01  2.09177464e-01  3.31940472e-01
  1.96426243e-01 -1.96768373e-01  5.40335961e-02 -7.25219622e-02
  2.12912112e-01 -1.47337899e-01 -1.16727732e-01 -5.05563259e-01
  1.58544868e-01 -5.85786551e-02 -3.84740494e-02 -2.50939369e-01
 -4.38962579e-02 -2.85623550e-01 -2.79219240e-01 -3.52812022e-01
 -8.18998292e-02 -2.33864218e-01 -1.29724249e-01  1.51909858e-01
  4.30907011e-02  6.77587837e-03  4.76062521e-02 -2.94987679e-01
 -2.67336547e-01 -1.96916774e-01 -1.35880768e-01 -4.77217063e-02
  1.89298950e-02 -3.21485400e-02  2.04823345e-01  8.57752115e-02
  4.49452735e-02 -2.30647713e-01  1.15655020e-01 -2.48295009e-01
 -6.03429198e-01  1.37196630e-01 -1.42183468e-01 -5.45834340e-02
 -1.83098823e-01  1.44696593e-01 -1.33041982e-02 -1.82697088e-01
  1.28261954e-01  4.64738160e-03 -1.96813159e-02  7.62793869e-02
  1.57656878e-01  1.83913052e-01 -4.73065555e-01 -1.22957695e-02
  1.13681056e-01  6.04003295e-02  1.40103459e-01 -1.14814982e-01
  1.71261668e-01  2.38366961e-01  9.40402299e-02 -9.93786380e-03
  1.93015814e-01  2.73501992e-01  1.34444356e-01  1.44281358e-01
 -5.48043996e-02  2.22406343e-01  1.61816403e-01  4.85527255e-02
 -9.88308340e-02  2.69341350e-01 -9.35395211e-02  1.92875445e-01
  5.51274836e-01 -6.90738112e-02 -9.75346267e-02 -3.98662150e-01
  1.62595779e-01  2.38723516e-01 -1.67162418e-01  1.78046972e-02
  5.14620729e-02  2.77235150e-01 -5.13123631e-01  3.30538750e-02
  1.54598951e-01 -1.48227289e-01  2.55377918e-01 -4.65615273e-01
 -1.29110496e-02 -2.93435037e-01 -1.65208936e-01  1.03561766e-01
  2.59898841e-01 -8.66731256e-02  2.22217776e-02  1.37557879e-01
 -1.10482778e-02 -9.53123122e-02  1.51997060e-01  1.85105011e-01
 -7.01264367e-02 -2.41445825e-01  4.59103100e-03  1.61095217e-01
  3.79386216e-01  2.62852944e-02  3.08692396e-01  3.89316201e-01
 -9.85774994e-02  2.49021024e-01  1.61857903e-01  1.31353810e-01
 -1.32223563e-02  3.83797348e-01 -3.12756449e-01  1.12253934e-01
 -1.86723739e-01  4.24291551e-01  1.86877102e-01  4.97128107e-02
 -1.57093495e-01 -6.88827753e-01 -4.59349245e-01 -1.45307392e-01
 -9.53527018e-02  1.29755408e-01 -3.45983990e-02 -3.75054508e-01
 -1.92990005e-01 -1.28399521e-01 -1.02256544e-01 -1.23692371e-01
 -4.89742428e-01  6.98900968e-02 -2.91678667e-01 -2.67245829e-01
 -2.46682703e-01  8.92842114e-02  1.01690456e-01 -9.94836986e-02
 -3.38171124e-02 -3.22083652e-01 -2.45374903e-01 -3.01669866e-01
  6.87483475e-02 -1.76760122e-01  1.28962159e-01  4.34696227e-02
 -3.11128665e-02  7.97948986e-02  6.17832728e-02  2.16820955e-01
  1.63655579e-01 -5.55794239e-02 -1.28756166e-01  1.98770434e-01
 -1.68702349e-01  1.49919987e-01  1.78018078e-01  2.92142659e-01
 -3.21776390e-01  3.06888968e-02 -2.22508907e-01 -2.17017040e-01
  3.12613845e-01 -1.69305086e-01 -2.99960189e-02  1.40265882e-01
  4.89139646e-01  1.76904723e-01 -9.33769718e-02  3.60534303e-02
 -1.78508580e-01  3.20782751e-01  3.68888676e-01 -3.00638378e-01
 -2.99056694e-02  8.42937082e-02  1.16768464e-01 -3.03257823e-01
 -2.47964546e-01  1.01626582e-01  3.29029024e-01  9.96228382e-02]"
tf_cnn_benchmarks.py stuck when running with multiple GPUs and ImageNet data with protocol grpc+verbs stat:contribution welcome type:bug,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, running tf_cnn_benchmarks.py from benchmarks repo
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**: Unmodified source with RDMA Verbs enabled
- **TensorFlow version (use command below)**: 1.3.0-rc0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.5.1
- **CUDA/cuDNN version**: 8.0/6
- **GPU model and memory**: NVIDIA Tesla P100 PCIe 16GB (8 per node)
- **Exact command to reproduce**: 

PS: CUDA_VISIBLE_DEVICES='' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=ps --task_index=0 --server_protocol grpc+verbs

Worker0: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=0 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs

Worker1: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=1 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs

- **RDMA driver version**: MLNX_OFED_LINUX-4.1-1.0.2.0

### Describe the problem
When running the above commands (Inception V3 synchronized data parallelism training with 2 workers and 1 external ps), the tf_cnn_benchmarks application hangs forever after some iterations (usually in warm up).

It happens only when real data is involved (ImageNet), and with >4 GPUs. (More GPUs, less iterations before it hangs). Doesn't happen with grpc protocol, or when running with ""synthetic"" data.

The master_service in the workers is stuck [here](
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc#L608), which I guess means some operations in the computation have not been completed.

The RDMA protocol looks valid and clean, all messages corresponds to the protocol (see below logs). 
There some tensors requested by the workers which they don't receive, but they are passed by the RDMA Verbs transport to the BaseRendezvoudMgr with RecvLocalAsync in a valid way, and for some reason the higher level worker service doesn't trigger the Send kernel on those tensors.

Any help is much appreciated!
If there are some debug mechanisms I can use to understand which tensors/operations have not been completed it can greatly help. I was mostly debugging this from the RDMA Verbs layer till now, without much success, and I feel I don't have enough information there to understand what's missing.
Also I feel we don't have enough knowledge on how the step_id acts (diving into this in the code now, but there's some higher level documentation it can greatly help).

My initial guess was an occurrence of a racy condition when loading the data, since it creates a gap in execution time (worker0 starts the first training step 30-60 seconds after worker1, since it does the preprocessing of the data twice for a reason I couldn't understand yet), but after the first iteration (which usually passes successfully) the time is synchronized between workers.

### Source code / logs
Those are the logs of the runtime after moving the logging in [rdma.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc) to VLOG(0) (also adding Tensor name and step id for all cases, in some cases the step_id doesn't mean anything like BUFFER_REQUEST/RESPONSE for example), and also some VLOG in [master_session.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc)

[worker0](https://gist.github.com/shamoya/15a42f421e088473b8f02bf00c16d0fc)
[worker1](https://gist.github.com/shamoya/dd3126c02c73990a6e28b534d9a9ddf6)
[ps](https://gist.github.com/shamoya/0c856365802ae4d42b38baf988149574)

Unfortunately they are fairly large, but it's better then to cut the log files IMO.
Example for analysis I did in the verbs layer, comparing the Sent Tensor requests to the actual received tensors writes in both workers:

worker 0:
 -  /job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:0/gpu:0;edge_116943_group_deps_2/NoOp_1;0:0 80661058974090965
-  /job:worker/replica:0/task:1/cpu:0;1a50d5c51cd9c5d1;/job:worker/replica:0/task:0/gpu:0;edge_116947_group_deps_3/NoOp_1;0:0 80661058974090965
- /job:worker/replica:0/task:1/gpu:2;7f00fadabfe781f5;/job:worker/replica:0/task:0/gpu:0;edge_111078_group_deps_1/NoOp_2;0:0 80661058974090965
- /job:worker/replica:0/task:1/gpu:4;b07185dd19f62088;/job:worker/replica:0/task:0/gpu:0;edge_111080_group_deps_1/NoOp_4;0:0 80661058974090965

worker 1:
- /job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:1/cpu:0;edge_155113_AssignAdd;0:0 80661058974090965
- /job:worker/replica:0/task:0/gpu:0;f3df8abf03739fe8;/job:worker/replica:0/task:1/cpu:0;edge_116948_group_deps_3;0:0 80661058974090965

The tensors requests received well by the other side and passed to RecvLocalAsync, but are not called later.

Thanks a lot.",True,"[-1.69623882e-01 -6.62934959e-01 -4.69189882e-01 -2.33104289e-01
  1.70316815e-01 -1.41867250e-01 -9.53965709e-02  9.70542580e-02
 -4.55989957e-01 -8.20422843e-02  6.03692839e-04  7.38543719e-02
 -6.61099628e-02  7.06617832e-02 -1.48636594e-01  3.08307946e-01
 -6.94875270e-02 -3.29523414e-01  3.47772002e-01  1.59999151e-02
  2.13886052e-02 -1.36199951e-01 -2.64137238e-01  3.78706306e-01
  2.18404084e-03  1.38066232e-01 -2.05518872e-01 -1.80553019e-01
  1.83401972e-01  2.60750473e-01  4.47048068e-01  1.73275545e-01
 -2.63778448e-01  2.87260879e-02  2.07542121e-01  2.53258973e-01
 -2.32434452e-01 -2.20881253e-01 -1.42731788e-02  1.83749441e-02
  2.14049965e-01 -1.96870804e-01 -2.74964161e-02  8.76914710e-05
  6.94227517e-02 -3.61898914e-02  5.41087613e-02  1.05112739e-01
 -1.83732301e-01 -6.27756119e-02 -6.44516721e-02  8.82152691e-02
 -3.83874655e-01 -1.98268503e-01  1.90499872e-01 -5.02130985e-02
 -3.95006202e-02  3.06998372e-01  1.38473034e-01  6.45474494e-02
  9.16477293e-02  5.51788621e-02 -1.39460191e-02  1.35871381e-01
  1.91789716e-01  6.17044345e-02  7.53862858e-02 -2.14014903e-01
  4.44455415e-01 -2.47354172e-02 -1.48617610e-01 -4.28674594e-02
 -3.13359201e-01 -1.41240172e-02  1.97660878e-01  2.75032938e-01
 -2.66195834e-02  4.77200523e-02  1.05738109e-02 -2.91988462e-01
  2.41039380e-01 -7.04145581e-02  3.03902447e-01 -4.70730215e-02
  9.01033431e-02  4.51588258e-02  1.88560903e-01  1.24445997e-01
  1.15053326e-01 -2.09355593e-01  4.52393562e-01  8.50830674e-02
  6.65273964e-02  2.35064059e-01  1.78914279e-01 -8.65156502e-02
 -2.17181854e-02  9.64709595e-02 -3.52985442e-01  3.23894024e-02
 -9.16172005e-03 -4.34425682e-01 -1.36595756e-01  2.90721297e-01
 -4.49183285e-02 -4.23628539e-02  2.24268973e-01  1.74357504e-01
 -5.00179455e-02 -3.75738442e-02 -1.19741056e-02  2.48471081e-01
  9.25202668e-02 -1.24995142e-01  1.78307876e-01 -1.88269377e-01
 -4.03920859e-01  4.02298570e-03 -6.92319721e-02  3.95383477e-01
 -2.00887084e-01  2.69750282e-02  8.41875374e-02  1.62476152e-01
  1.96595281e-01  1.79492757e-01  8.74744952e-02 -4.33915891e-02
  2.19828501e-01  4.59667742e-02  1.62526712e-01 -3.11486106e-02
 -1.58454068e-02  1.72630578e-01 -1.00769863e-01 -1.33801714e-01
 -2.64921308e-01  4.41936590e-03 -1.96701363e-02 -1.72180057e-01
 -5.43061644e-02  1.46944880e-01  4.80973125e-02 -6.79267123e-02
  3.91692743e-02  3.16635668e-01 -1.44340977e-01  2.99035519e-01
 -8.34649149e-03  1.81245804e-01 -7.22021386e-02  1.74905047e-01
 -1.25883281e-01  2.96115994e-01 -4.94468845e-02  3.95749003e-01
  2.21143186e-01  2.25934163e-02 -1.29480079e-01 -4.07203019e-01
  1.16702117e-01  8.82335901e-02 -7.00502396e-02 -3.22966397e-01
  7.60988593e-02 -1.01950154e-01 -3.00623983e-01 -2.11589694e-01
  5.66394720e-03  2.64580637e-01 -2.15197429e-01  3.59249581e-03
 -2.65014112e-01  2.19440371e-01  1.28409594e-01  1.51046608e-02
  1.93274096e-02 -5.03025353e-01 -1.41910627e-01  9.06803161e-02
 -2.25869432e-01 -1.03743412e-01 -8.82919207e-02 -1.28929645e-01
  1.79703478e-02 -9.31095257e-02  2.49949709e-01 -3.58951166e-02
 -7.78404251e-02 -6.89631701e-02 -2.73705959e-01 -2.92640448e-01
  2.27220282e-01 -4.41775471e-02 -1.54698268e-01  8.54996368e-02
  5.14261842e-01  2.73837805e-01  1.11644901e-01  3.28352362e-01
 -3.68790403e-02 -2.01561689e-01  1.23553686e-01  1.57142177e-01
  1.66385427e-01 -2.21521080e-01 -4.49678659e-01  1.33453310e-02
 -2.88109750e-01  5.63431792e-02 -7.55456537e-02 -1.82946384e-01
 -4.10209633e-02 -1.71508379e-02 -1.76025871e-02 -2.38793850e-01
 -2.24398524e-02 -8.29078928e-02 -1.39980763e-01  1.46464169e-01
  1.65635616e-01 -6.21829256e-02 -1.36394486e-01 -3.88425559e-01
 -1.43838942e-01 -7.02793300e-02 -1.71037063e-01  7.11196363e-02
  8.27984512e-02  4.94702086e-02  2.50214282e-02  2.46183239e-02
  3.25154543e-01  8.79116356e-02  3.02442193e-01 -9.71374586e-02
 -1.17650740e-01 -1.69782981e-01 -8.63129646e-02  1.89744413e-01
 -5.06939948e-01 -1.28402889e-01 -6.38486817e-04  4.62061651e-02
  1.65136188e-01  2.16321461e-02 -3.05171371e-01  3.11922908e-01
 -2.13234901e-01  3.56322169e-01 -8.18425715e-02 -2.37913474e-01
 -5.49960136e-03  1.83626071e-01  1.97007716e-01  4.60415892e-03
 -2.35731959e-01  1.16579328e-02  2.13196874e-01  1.14446923e-01
  1.20109469e-01  2.14731276e-01 -1.47064447e-01  4.14319038e-01
  3.78691763e-01  4.01826531e-01 -2.38601193e-01  4.32556182e-01
 -4.71320786e-02 -1.59147203e-01 -1.17392793e-01 -1.81526452e-01
  1.25256211e-01 -1.79962754e-01  1.13924295e-02 -9.86176729e-02
  4.57188427e-01 -9.31990594e-02 -2.18658656e-01  2.21554860e-02
  2.46460974e-01  4.43157345e-01 -1.36069402e-01 -5.49708754e-02
  5.80055863e-02 -2.28396058e-01 -3.88782859e-01 -3.37241799e-01
 -1.43882811e-01 -1.25747606e-01 -7.47986371e-03  3.59280817e-02
  2.95165688e-01  9.52294022e-02 -1.61387101e-01  7.71886855e-02
 -8.21066722e-02  2.12909192e-01 -9.32780430e-02  1.29515707e-01
 -5.09016752e-01  1.49844056e-02  2.72344649e-01  3.68985832e-01
 -1.67645514e-01  2.30538063e-02  1.42426640e-01 -6.96573481e-02
  4.82118100e-01 -1.03227280e-01  2.37040043e-01  1.46576852e-01
 -1.22943379e-01  6.53417557e-02  9.04817656e-02  1.85213953e-01
 -5.90518415e-02  4.94966388e-01  3.12137693e-01  1.40008852e-02
  1.30113721e-01 -1.89598054e-01 -4.05782998e-01  2.39523381e-01
 -2.61675380e-03 -1.69716403e-01 -2.36338332e-01 -2.53164805e-02
  2.22822502e-02  1.89031988e-01  1.40923262e-02 -1.19796008e-01
 -3.81380141e-01  6.17642030e-02  3.04271244e-02 -7.12901354e-02
 -2.87044406e-01  2.69409418e-01  7.17962012e-02 -2.76972950e-01
 -5.43128289e-02 -2.81670988e-01 -4.86701727e-02 -4.42564368e-01
 -1.95706904e-01 -1.77041143e-01  5.54131866e-01  4.98307109e-01
 -1.30295530e-01  2.94976607e-02  1.80699512e-01 -1.02383658e-01
 -2.66404271e-01 -1.34015530e-01 -2.64852121e-02  2.91560054e-01
 -5.41108996e-02 -2.04201117e-02  1.93484157e-01  4.40371215e-01
 -2.75331497e-01 -1.51483297e-01 -7.30392933e-02 -2.50621408e-01
 -9.82279330e-02 -1.71938166e-01 -1.31300054e-02  7.61333555e-02
  4.19372559e-01  7.07277656e-02 -6.79586828e-02  1.63569361e-01
 -1.90513164e-01  1.81873605e-01  5.28950930e-01 -3.57347727e-01
 -7.42510483e-02  1.06287472e-01 -2.38096453e-02 -3.84096384e-01
 -1.86019987e-01  1.77400112e-01  1.57972336e-01 -2.53057331e-01]"
"softmax_cross_entropy_with_logits aborts the process, if a tensor with zero first dimension is passed as an argument stat:awaiting tensorflower type:bug","### Environment info

Operating System: Ubuntu 16.04
Installed version of CUDA and cuDNN: CUDA-8.0, CUDNN 5.1.5 

Tensorflow version: 0.12.1 installed from
https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl

Reproduced also using tf-0.11.0, CUDA-7.5, CUDNN-5.1.3

### Minimal reproducible example 

```
import tensorflow as tf
y = tf.placeholder(""int64"", [None], ""y"")
one_hot_y=tf.one_hot(y,10)
ce = tf.nn.softmax_cross_entropy_with_logits(one_hot_y, one_hot_y)
sess = tf.Session()
sess.run(ce, {y: []})
```
Result on GPU: 
```
E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
F tensorflow/core/common_runtime/gpu/gpu_device.cc:104] EigenAllocator for GPU ran out of memory when allocating 0. See error logs for more detailed info.
Aborted (core dumped)
```
Result on CPU: 
```
array([], dtype=float32)
```",True,"[-0.49014378 -0.56424683 -0.34041697  0.13430221  0.1704599  -0.36145815
 -0.06870659 -0.03438603 -0.26274642 -0.2222528  -0.02053713 -0.04498294
 -0.16910733 -0.07828804 -0.45133328  0.18855353 -0.1561036   0.02348997
  0.14468831  0.03686763 -0.1350093   0.11423002 -0.31457546 -0.00584
  0.32286882  0.05966446 -0.09597777 -0.33268034  0.26037443 -0.16632932
  0.41926646 -0.06104712 -0.02553518  0.07328068 -0.00527139  0.1663417
 -0.43030113 -0.28358275 -0.06204724 -0.01514849  0.02609118 -0.0073137
  0.15128872 -0.0087645   0.09149926  0.08884084 -0.07888766  0.02831095
  0.03748374 -0.20658259  0.14225881  0.07410438  0.02659711 -0.11178666
 -0.11417049 -0.18029508  0.11492366  0.11839756 -0.17416158  0.14646025
  0.18294057 -0.0030851   0.08413086 -0.01686589  0.2160303   0.15534852
  0.17445156 -0.32774973  0.48049515  0.18333617  0.08037806  0.03888409
 -0.27923387 -0.180989    0.06860147  0.21414301  0.14639297  0.54776096
  0.17273891 -0.14077505  0.00995028  0.22974862  0.19050486 -0.3419005
  0.00185264  0.00293321  0.3930624   0.07736492  0.29512048 -0.29024065
  0.3764673   0.36728677 -0.13035591  0.19951606  0.3226204   0.1663288
 -0.03914395  0.3486145  -0.09470285 -0.14892538 -0.20113645  0.1235235
 -0.14288792  0.30684063  0.04085765  0.02085119  0.22596839 -0.11617076
  0.0252992   0.05218656  0.3980325  -0.10420582  0.30340356  0.44928873
  0.1750986   0.04351043 -0.06285445  0.05173948 -0.01478058  0.45403397
 -0.39993936  0.09289774  0.08791001 -0.03643225  0.13338773  0.0892058
  0.08229825 -0.12522078 -0.06127222 -0.12055342  0.03974891 -0.1652863
 -0.03862203  0.08566744 -0.22195461  0.02614156 -0.2102307   0.11647101
 -0.38922188  0.05327952 -0.08243316  0.18757889 -0.1165486  -0.29022273
 -0.05896264  0.18622911 -0.12694123 -0.10005122 -0.09901734  0.09537054
 -0.11228025 -0.04059477 -0.25938648  0.19109014  0.26323873  0.18304944
  0.23027235 -0.20744094 -0.2705117  -0.5166384   0.28567982  0.37908497
 -0.11906699  0.25439146  0.20988566  0.35518867 -0.32963878 -0.04775423
  0.04653442  0.34315014 -0.13842967 -0.19267291 -0.12795968  0.00073784
  0.4179909  -0.24010107 -0.06655294 -0.4934244  -0.2050164   0.07422105
 -0.08814131  0.09840967  0.1906068  -0.02256868  0.13781978  0.12297776
  0.03521691  0.16714446 -0.33207136 -0.13203469 -0.1671134  -0.26599437
  0.01795735 -0.07349464 -0.0410292   0.1622447   0.12554719 -0.05379969
 -0.08720499  0.11692709 -0.19311951 -0.17897153  0.0663074   0.1905276
 -0.08442004 -0.31103355 -0.26514357 -0.2291131  -0.27725035 -0.04928918
 -0.03725845 -0.301063   -0.21804476 -0.09483927 -0.28722498 -0.0444612
 -0.07787563 -0.19001001 -0.3755856   0.37533817  0.503279   -0.04682676
  0.10222028 -0.15301435 -0.18890123 -0.25620773  0.15969041 -0.02249943
  0.02904254  0.16912758 -0.00594687  0.12664768  0.00608248  0.13302138
  0.33059895  0.06654409 -0.1241336  -0.05054229  0.02827457  0.20991558
 -0.19381946 -0.13664155 -0.04054834  0.12730862  0.34002706  0.23045698
 -0.22208932  0.05617503 -0.3463851   0.16127712  0.2564125  -0.3322363
  0.17877091 -0.14280106  0.1882271  -0.00768246 -0.00980742  0.19595002
  0.07428686 -0.04313487  0.27230555  0.23630098 -0.2952959   0.2706657
  0.3656425   0.45813733 -0.4190103   0.33662242 -0.02431666 -0.18943077
 -0.17891338  0.04755097  0.15153408 -0.30571657  0.07072229 -0.16255865
  0.6249459   0.11846499 -0.05772288 -0.0985725   0.22932285 -0.0088735
 -0.17357223  0.10042619 -0.0459142  -0.16186273 -0.2644265  -0.21267894
 -0.23737581 -0.13075288 -0.2483564   0.10553186  0.35040826  0.03407394
 -0.16481613  0.13416357  0.07286116  0.22857226 -0.00608468 -0.00525607
 -0.18882714 -0.2298896   0.18059734 -0.44376433 -0.34757766 -0.08237568
  0.27922928  0.30160955  0.36713725 -0.09667351  0.19903514 -0.23132803
 -0.16761352  0.35035336 -0.0917881   0.01969858 -0.26025796  0.5217699
  0.30986184  0.02792517 -0.00869504 -0.5672269  -0.2019685   0.06539592
  0.2527703  -0.11500574 -0.34054536  0.08379784 -0.42881882  0.309982
  0.01597795 -0.20210302  0.02168893  0.11674695 -0.12963112  0.15008436
 -0.08659232  0.14186442  0.14489159 -0.40611616 -0.3239118  -0.20304799
 -0.192343   -0.41297457  0.01928081 -0.20830055  0.34896806  0.29136473
 -0.23933151 -0.1711543  -0.07032203  0.33794105 -0.03886339  0.01626395
  0.18005773  0.29930702  0.24567856  0.02328048  0.1423065   0.21378818
 -0.12584418  0.25448138  0.01558833 -0.22494742  0.10195711 -0.25393528
 -0.18531251 -0.05641218  0.30656767  0.24424666 -0.05303454  0.20453778
  0.15539593  0.23456664  0.47168875 -0.3829733  -0.38536823  0.10880902
 -0.22770405  0.13070785 -0.10938245 -0.10708171  0.16758773 -0.04337617]"
tensorflow-0.7.1-cp35-none-linux_x86_64.whl ,"sudo pip3 install --upgrade http://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp35-none-linux_x86_64.whl
tensorflow-0.7.1-cp35-none-linux_x86_64.whl is not a supported wheel on this platform.
Storing debug log for failure in /home/xxx/.pip/pip.log
",True,"[-2.86086559e-01 -3.35340261e-01 -3.74030858e-01  2.33476847e-01
  9.39725712e-03 -1.83935374e-01 -8.07548687e-02 -1.46058396e-01
 -1.20939225e-01 -1.78506538e-01  6.49409965e-02  8.77776444e-02
 -1.11904338e-01  3.30255866e-01 -1.68275908e-01  3.19505960e-01
 -1.94068044e-01 -3.30280602e-01  2.14915365e-01 -1.85645539e-02
 -2.48277828e-01 -3.11796106e-02  2.10256800e-02  2.23639905e-01
  2.65793363e-03  2.18981147e-01 -3.68188828e-01 -1.18020482e-01
  1.04760855e-01  2.95577407e-01  5.01161695e-01 -1.85035482e-01
 -1.64473027e-01 -5.51872104e-02  3.89039397e-01  1.10109687e-01
 -2.65857458e-01 -2.02527553e-01 -1.16634540e-01 -1.55391097e-01
  2.02875793e-01  6.33292645e-02 -1.41593575e-01  6.51492923e-02
 -5.22068068e-02 -1.90614060e-01  1.71520747e-04 -1.03317738e-01
 -1.23258010e-01 -3.67923677e-02 -2.05553830e-01  1.99339613e-02
 -1.93553165e-01 -4.63156551e-02 -4.47297543e-02 -6.80242479e-02
  1.93339407e-01  4.96006519e-01  4.80079800e-02  1.89473033e-01
  2.53715813e-02  1.87324554e-01 -4.63285521e-02  9.65624601e-02
 -1.73895627e-01  2.22179875e-01  2.85879880e-01 -6.31397724e-01
  2.75657326e-01 -3.65015030e-01  3.12304348e-02 -1.49245426e-01
 -3.50573093e-01 -1.03634603e-01 -5.35047539e-02  1.00268938e-01
  5.59387030e-03  4.49882418e-01 -7.02303052e-02 -1.62860289e-01
 -1.97102308e-01 -5.50902933e-02  1.46824479e-01  6.42456710e-02
  3.88579816e-02  8.86152033e-03 -3.96345295e-02 -1.07660685e-02
  7.68606290e-02 -6.04901575e-02  9.37005281e-02  9.32211801e-02
  1.01014361e-01  2.05880478e-02  1.50276959e-01  1.64788395e-01
  1.02667227e-01  4.80409563e-01  3.48947942e-02 -2.29293138e-01
 -1.89326793e-01 -1.92244887e-01 -2.12489605e-01 -1.54249221e-02
 -3.06366265e-01  1.32026285e-01  1.69129580e-01  2.07274586e-01
  1.31963253e-01 -1.12230971e-01  1.23742700e-01  7.69823939e-02
  3.70323181e-01  1.32320672e-01  5.73265068e-02  2.20229030e-02
 -5.69785953e-01  9.52482820e-02 -4.90728803e-02  5.83935738e-01
 -2.11074144e-01 -8.66850689e-02  3.13535810e-01  4.72337902e-02
  2.48859137e-01 -2.36898795e-01 -2.38772780e-02  5.98642826e-02
  4.38606262e-01  8.28113779e-02  1.08938634e-01  2.98887223e-01
 -1.81247115e-01  5.24194613e-02  2.67819706e-02  5.38641959e-02
 -1.38138056e-01 -3.51609111e-01 -1.44955024e-01  5.77637069e-02
 -1.84255168e-01 -8.02900195e-02  6.50640503e-02 -2.62482584e-01
 -2.24481434e-01  1.16521530e-01  1.00776711e-02  2.61730552e-01
  1.16994251e-02  1.19180694e-01 -1.41778409e-01 -1.02300882e-01
 -8.56746808e-02  3.68746042e-01  2.29701221e-01  2.05170304e-01
  3.29576313e-01  1.89276785e-01 -3.19512784e-01 -2.95725584e-01
 -1.13077365e-01  1.12402938e-01 -3.09527099e-01  2.24116027e-01
 -4.12846088e-01  2.29674459e-01 -4.30578589e-01 -6.47661760e-02
  1.68958101e-02  6.75217956e-02 -5.57956658e-02 -8.50196481e-02
 -1.60870031e-02  5.69532253e-02  1.03135400e-01 -1.57691449e-01
  3.54029566e-01 -1.84339881e-01 -6.70581609e-02  2.94327974e-01
 -2.33068377e-01  1.10876910e-01  1.09777391e-01 -1.57248497e-01
  1.03799753e-01  3.54241550e-01  3.73458900e-02  5.22027314e-02
 -2.79092081e-02  1.01072893e-01 -2.64238328e-01 -9.35189202e-02
  2.51636744e-01  9.03210491e-02 -2.83725441e-01  1.01884730e-01
  2.79215425e-01 -1.52220353e-02  1.42074734e-01 -1.02464654e-01
  3.94718051e-02  1.75494492e-01  1.03189766e-01 -1.56199671e-02
  8.66808444e-02 -2.87648384e-02  6.59864917e-02 -2.05043882e-01
 -2.92870879e-01 -2.16005132e-01  1.52842730e-01 -5.04466631e-02
 -1.26842290e-01 -2.47647554e-01 -1.45799279e-01  5.41076399e-02
 -2.23141059e-01 -2.77091488e-02 -5.62771000e-02  2.35402763e-01
  6.38009384e-02 -1.28382370e-01 -1.28962278e-01 -1.90740824e-01
 -1.28804952e-01 -2.33418524e-01 -2.19177648e-01 -9.52242687e-03
 -1.53156787e-01  2.83545703e-01 -1.72513142e-01 -2.51163810e-01
  8.33020210e-02 -2.92571057e-02  4.67676282e-01 -2.24336445e-01
  7.61964843e-02 -7.65387435e-03 -2.76204683e-02  2.16488764e-01
 -3.12226564e-01 -2.45074868e-01 -4.89433631e-02  1.06657296e-01
  2.30854414e-02  1.94979802e-01 -3.36466372e-01  7.82469362e-02
 -2.36901373e-01  3.00495148e-01  1.21260270e-01  1.22646298e-02
  3.40534776e-01  2.69152641e-01  4.14148122e-01  1.58032298e-01
 -1.55858144e-01  1.54348820e-01  2.69522578e-01 -1.32038832e-01
 -3.13853547e-02  5.25549233e-01 -6.20586127e-02  1.41878337e-01
  3.59218448e-01  9.21819955e-02 -2.64608800e-01  4.08847094e-01
  8.64278376e-02  1.42680094e-01  7.52946362e-02 -5.00213742e-01
  1.37804508e-01  1.49930883e-02  1.87585667e-01  7.90402740e-02
  2.87434757e-01 -2.97330879e-03 -8.64443183e-02  2.23062605e-01
  2.46431213e-02  3.19422662e-01 -2.70328343e-01 -1.04730666e-01
  6.22611307e-02 -6.20208755e-02  1.79433376e-01 -3.64966691e-01
 -4.07906771e-01  3.06220707e-02  1.24018509e-02 -2.35644370e-01
 -1.10499442e-01 -7.30240941e-02 -1.17185995e-01 -2.33372986e-01
  4.03872848e-01  1.43829718e-01  4.55843121e-01  3.56981695e-01
 -1.89726263e-01  4.08986621e-02  2.21481442e-01 -2.48795629e-01
 -1.89815788e-03 -2.01576762e-02  1.43562347e-01  1.17763765e-01
  6.11858368e-01 -4.73182470e-01  6.08779415e-02 -9.96653140e-02
 -6.29535168e-02  2.84378082e-01  1.75727442e-01  6.07166290e-02
 -2.93836534e-01  2.93414652e-01  3.05603057e-01 -2.82953560e-01
  3.01230371e-01 -3.57436463e-02 -5.36257088e-01  9.62476432e-02
 -2.96535082e-02  7.84883425e-02 -2.71841250e-02 -4.01240349e-01
 -1.94278359e-01  1.73586965e-01  8.86383206e-02  1.18469838e-02
  3.21683511e-02  2.82146871e-01 -2.59101272e-01  9.18384045e-02
 -6.07494354e-01 -1.34483039e-01  3.72483283e-02 -5.15188932e-01
  8.37050974e-02 -2.85581470e-01  1.89205468e-01 -1.75391480e-01
 -1.03441879e-01 -2.90914357e-01  3.66005242e-01  1.63704216e-01
 -1.08678609e-01 -7.28368759e-02 -1.19798526e-01  2.14255452e-01
 -3.92466784e-01  6.77073188e-03  1.82803184e-01 -9.94735956e-03
  1.22214712e-01  1.95125550e-01  2.45631412e-01  4.95609283e-01
 -1.57577351e-01  1.54354259e-01 -2.51511991e-01 -1.90408498e-01
  4.24090147e-01 -1.53381735e-01 -2.60884196e-01 -2.12078840e-02
 -1.79708272e-01  4.87600207e-01 -1.00120500e-01  3.43861997e-01
 -2.34001905e-01 -4.34663892e-02  2.75005132e-01 -7.43827224e-02
 -1.84814155e-01  3.73796895e-02 -2.08580792e-01 -4.45099831e-01
 -2.03686953e-02  4.61275503e-02  2.84709930e-01 -1.45521447e-01]"
Can't recognize GPU in tensorflow. type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11.1

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

nvidia GTX 1060 6GB

### Current behavior?

I installed tensorflow and cuda/cudnn but cannot see GPU in tensorflow. 
print('GPU', tf.config.list_physical_devices('GPU'))
The output is:
GPU []
The output of 'print(tf.config.list_physical_devices())' is
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.config.list_physical_devices())
print('GPU', tf.config.list_physical_devices('GPU'))
```


### Relevant log output

_No response_",True,"[-4.21326578e-01 -3.84122550e-01 -2.07055271e-01  6.58285767e-02
  1.24234289e-01 -2.39096344e-01 -3.71079564e-01  7.07089808e-03
 -3.57944965e-02 -3.65208477e-01  4.44655977e-02 -2.08412223e-02
 -2.23993748e-01  1.34737074e-01 -9.72090065e-02  2.05757141e-01
 -1.84415609e-01  1.39487043e-01  1.43751323e-01  2.94500172e-01
 -1.10182740e-01 -2.39794195e-01 -3.21242809e-01  6.66741095e-03
  3.00315797e-01  2.51999021e-01  1.84122548e-02 -2.21006840e-01
 -2.48646438e-02  2.57383704e-01  7.09106982e-01  7.64865726e-02
  3.08716238e-01  3.05719733e-01  1.83164507e-01  3.63710895e-03
 -1.33349538e-01 -5.55388868e-01 -2.99419373e-01  8.80145133e-02
  2.10286736e-01  1.66301101e-01  2.31850266e-01  9.63191688e-02
 -4.46675979e-02 -4.25039291e-01 -1.95227325e-01 -2.90752858e-01
 -7.44046792e-02 -2.35885650e-01  9.53876972e-02 -1.26050487e-01
 -6.24414444e-01 -3.18245411e-01 -3.86785477e-01  5.21207005e-02
  2.59426832e-01 -9.11460370e-02 -2.51007006e-02  1.32751539e-01
  1.45657346e-01 -8.67274590e-03  1.71818540e-01 -2.11906046e-01
 -8.15043896e-02  1.35966793e-01  7.11192787e-02 -1.29033297e-01
  7.68789053e-01 -4.45874989e-01  3.75092998e-02 -9.65522081e-02
 -3.19639772e-01 -2.22231477e-01 -1.99899785e-02  1.31161183e-01
  1.41769826e-01  2.41529778e-01  6.59412369e-02 -1.01963490e-01
  2.45989636e-01 -1.81855157e-01 -2.61818655e-02 -1.38550311e-01
  2.61657894e-01  1.28490478e-01  2.91676491e-01  1.74335212e-01
  5.51272452e-01 -2.65466928e-01  2.98341274e-01  2.19059527e-01
 -5.72792068e-02 -1.96830988e-01  4.60011542e-01  2.28541549e-02
  3.18082392e-01 -1.34084165e-01 -6.96456879e-02 -3.16130221e-01
 -2.29894109e-02 -1.04047239e-01  1.64402038e-01  2.59144098e-01
 -2.09091321e-01 -7.00815693e-02  1.01986200e-01  1.77734308e-02
 -5.00327051e-02 -3.10795724e-01  1.12386085e-01  1.28751025e-01
  3.42501223e-01 -3.59030813e-02 -7.26362988e-02  1.86604057e-02
 -4.08315599e-01  2.63682604e-01 -8.86757821e-02  7.94052899e-01
 -1.26200557e-01 -4.32145447e-02  7.09922761e-02 -8.74881893e-02
  3.42925280e-01  1.91130653e-01 -2.90655732e-01 -4.69982512e-02
  6.23837933e-02 -1.85343772e-01  1.68219760e-01  5.11802547e-02
  2.55527139e-01  2.36421585e-01  7.52578601e-02  2.07057476e-01
 -1.39455989e-01 -2.45969921e-01 -2.06071138e-01 -1.32527679e-01
 -1.16404019e-01  2.58422256e-01 -2.32265353e-01 -5.91911197e-01
  8.46520290e-02  2.81914026e-01 -3.32163811e-01  4.14658248e-01
 -8.68290812e-02  1.51776791e-01 -1.26833439e-01  1.57810271e-01
 -1.25262320e-01  3.43442559e-01 -5.47963381e-02  8.83644894e-02
  3.70465308e-01 -1.62184462e-01 -7.28689134e-02 -4.95349258e-01
 -3.56820561e-02  3.98230970e-01 -3.15653756e-02 -5.75728938e-02
  1.67670846e-01  8.62401053e-02 -2.71562934e-01 -3.33618581e-01
  1.29998431e-01  3.41831207e-01 -9.41452160e-02 -1.50884807e-01
  1.12623852e-02  1.32746086e-01 -4.37102132e-02 -9.81251225e-02
  2.49800503e-01 -7.51930475e-01 -3.10984612e-01  2.59252101e-01
 -2.43254989e-01  1.98282748e-01  1.99062228e-01 -4.55786958e-02
  1.59698501e-01  2.01426923e-01  5.24626523e-02  6.20631203e-02
 -2.93628275e-01 -4.94014323e-02 -1.72586083e-01 -2.09932208e-01
  3.22058320e-01 -4.08197969e-01 -3.22537303e-01  2.03523487e-01
  2.65735030e-01  2.08160222e-01 -1.00968748e-01  2.23615393e-01
 -3.41734707e-01 -2.00623170e-01  1.48905203e-01  2.22743839e-01
  2.29876749e-02 -2.36500412e-01 -2.89095700e-01 -2.36144856e-01
 -6.20703399e-01  9.93737727e-02  9.05478597e-02 -5.46053886e-01
 -5.50358109e-02  1.98100775e-01 -3.00243616e-01  1.20158665e-01
  2.26610497e-01  6.32560998e-02 -3.81032914e-01  2.17467993e-01
  2.18764439e-01 -7.25455135e-02 -1.84780210e-01 -2.74332315e-01
 -3.12740862e-01  8.68000463e-03 -1.83279455e-01 -6.53131008e-02
 -1.39850616e-01  2.04174697e-01  5.44535704e-02 -2.51092017e-02
  4.09399033e-01  2.10877091e-01  4.99509364e-01  3.46405208e-01
 -1.09562173e-01 -1.96711749e-01 -1.93789210e-02  9.53875184e-02
 -8.07745904e-02 -2.21044779e-01  2.85250787e-02  1.01542130e-01
  2.37641424e-01  3.36438835e-01 -1.95753008e-01 -2.16914669e-01
 -3.14131379e-01  1.30190253e-01 -1.53852791e-01 -7.63050020e-02
  3.27841043e-01  2.03692198e-01  6.17830753e-01  1.95245326e-01
  5.61340004e-02  2.26437241e-01  1.00415744e-01  8.42879564e-02
  3.78254563e-01  8.61706287e-02 -3.11470956e-01  4.35141593e-01
  2.18600169e-01  2.48299286e-01 -2.94131279e-01  5.45375109e-01
  3.24368507e-01 -2.00727493e-01  1.61915347e-01 -3.31891477e-01
  6.36344731e-01 -3.77263367e-01  4.16160300e-02 -4.03108187e-02
  3.51326048e-01  1.11047246e-01 -2.44949684e-02 -7.94312060e-02
  2.81361997e-01  4.22698200e-01 -3.55864286e-01 -3.82197827e-01
  2.47885182e-01 -1.89596534e-01 -2.02430770e-01 -6.47377372e-01
 -3.58621836e-01  2.15209872e-02 -2.22410306e-01  1.72261357e-01
  1.72486186e-01  7.98024330e-03 -9.66988951e-02 -4.78007123e-02
  1.75162405e-01  1.51643544e-01  2.16501296e-01  2.28907749e-01
 -2.58553624e-01 -4.69675623e-02  1.96119979e-01 -6.67009771e-01
 -2.45718241e-01 -8.44840184e-02  2.06999734e-01  2.10796505e-01
  4.35165226e-01 -3.91229719e-01  2.83985958e-02 -1.63578108e-01
  6.19511912e-03  2.61340469e-01 -1.74563229e-01  1.57515541e-01
 -5.07220089e-01  4.85439718e-01  2.35302940e-01  2.27993131e-02
  1.52246669e-01 -2.71218061e-01 -4.35882330e-01  2.33573988e-01
  1.71932906e-01 -5.40427685e-01 -1.95244148e-01 -3.48441601e-01
 -2.18473505e-02  2.54163921e-01 -1.70130298e-01  4.23753401e-03
  1.45023122e-01 -5.75734787e-02  1.06678933e-01  1.35797769e-01
 -3.10761422e-01  2.06178665e-01  1.30314767e-01 -3.92277420e-01
 -3.57313633e-01 -1.27957970e-01  9.08665061e-02 -3.78255069e-01
 -6.86564483e-04 -1.33566767e-01  5.65873504e-01  5.50462842e-01
 -7.12425485e-02  2.35493869e-01  9.04670954e-02  2.23369956e-01
 -4.53984380e-01 -1.21307880e-01 -1.24736503e-02  5.92304587e-01
  4.09567475e-01 -2.42987290e-01  8.03434014e-01  1.04707450e-01
 -2.08091140e-01  3.13551351e-02 -3.65093023e-01  5.80039136e-02
 -6.17281497e-02 -1.77271992e-01 -2.23493744e-02 -1.99207157e-01
  1.49673492e-01  6.20973945e-01 -3.18540394e-01  1.43581748e-01
 -4.29675758e-01  1.56944171e-01  4.92949516e-01 -3.20588231e-01
 -4.28684115e-01 -2.54722297e-01  9.45445746e-02 -3.11089039e-01
  1.34507073e-02  2.88490988e-02  2.72685766e-01 -8.89711753e-02]"
Callbacks - tf.keras.callbacks.ModelCheckpoint and tf.keras.callbacks.EarlyStopping -> set_params not working  type:bug comp:keras TF2.14,"```
Issue type                                           : Bug
Have you reproduced the bug with TensorFlow Nightly? : No
Source                                               : source
TensorFlow version                                   : v2.14.0-rc1-21-g4dacf3f368e 2.14.0
Custom code                                          : Yes
OS platform and distribution                         : Google colab 
Python version                                       : Python 3.10.12
```

`tf.keras.callbacks.ModelCheckpoint` and `tf.keras.callbacks.EarlyStopping`  -> set_params not working as assumed. 

```python 
import tensorflow as tf                                                         # load tensorflow 
ckpt=tf.keras.callbacks.ModelCheckpoint('/content/')                            # make check point 
ckpt_params=ckpt.__dict__                                                       # get all parameters 
print('Parameters:',ckpt_params)                                                # see parameters dictionary
ckpt_params['save_best_only']=True                                              # update parameter ""save_best_only"" to True (default False)
ckpt_params['save_weights_only']=True                                           # update parameter ""save_weights_only"" to True (default False)
ckpt.set_params(ckpt_params)                                                    # try to make update parameters
print('Is same as updated?',ckpt_params==ckpt.__dict__)                         # make check both are same or not 
# both look same but if you look carefully `ckpt_params` and `ckpt.__dict__` will contain copy of themself in new key `'params'`
print('Parameters:',ckpt_params)                                                # see parameters dictionary
```
```shell
Parameters: {'validation_data': None, 'model': None, '_chief_worker_only': False, '_supports_tf_logs': True, 'monitor': 'val_loss', 'verbose': 0, 'filepath': '/content/', 'save_best_only': False, 'save_weights_only': False, 'save_freq': 'epoch', 'epochs_since_last_save': 0, '_batches_seen_since_last_saving': 0, '_last_batch_seen': 0, 'best': inf, '_options': <tensorflow.python.saved_model.save_options.SaveOptions object at 0x7a5f5942f5e0>, 'load_weights_on_restart': False, 'period': 1, 'monitor_op': <ufunc 'less'>}
Is same as updated? True
Parameters: {'validation_data': None, 'model': None, '_chief_worker_only': False, '_supports_tf_logs': True, 'monitor': 'val_loss', 'verbose': 0, 'filepath': '/content/', 'save_best_only': True, 'save_weights_only': True, 'save_freq': 'epoch', 'epochs_since_last_save': 0, '_batches_seen_since_last_saving': 0, '_last_batch_seen': 0, 'best': inf, '_options': <tensorflow.python.saved_model.save_options.SaveOptions object at 0x7a5f5942f5e0>, 'load_weights_on_restart': False, 'period': 1, 'monitor_op': <ufunc 'less'>, 'params': {...}}
```

This change can be seen by making a copy of `ckpt_params`. 

```python 
import tensorflow as tf                                                         # load tensorflow 
ckpt=tf.keras.callbacks.ModelCheckpoint('/content/')                            # make check point 
ckpt_params=ckpt.__dict__.copy()                                                # get all parameters 
print('Parameters:',ckpt_params)                                                # see parameters dictionary
ckpt_params['save_best_only']=True                                              # update parameter ""save_best_only"" to True (default False)
ckpt_params['save_weights_only']=True                                           # update parameter ""save_weights_only"" to True (default False)
#ckpt_params_deep_copy={key:value for key,value in ckpt_params.items()}          # make deep copy of `ckpt_params`
ckpt.set_params(ckpt_params)                                                    # try to make update parameters
print('Is same as updated?',ckpt_params==ckpt.__dict__)                         # make check both are same or not 
# both look same but if you look carefully `ckpt_params` and `ckpt.__dict__` will contain copy of themself in new key `'params'`
print('Uncomman parameters:',[key for key in ckpt.__dict__ if key not in ckpt_params])# see parameters dictionary
print('Uncomman parameters:',ckpt.__dict__['params'])                           # which is copy of itself only 
```
```shell
Parameters: {'validation_data': None, 'model': None, '_chief_worker_only': False, '_supports_tf_logs': True, 'monitor': 'val_loss', 'verbose': 0, 'filepath': '/content/', 'save_best_only': False, 'save_weights_only': False, 'save_freq': 'epoch', 'epochs_since_last_save': 0, '_batches_seen_since_last_saving': 0, '_last_batch_seen': 0, 'best': inf, '_options': <tensorflow.python.saved_model.save_options.SaveOptions object at 0x7a5f5942e3e0>, 'load_weights_on_restart': False, 'period': 1, 'monitor_op': <ufunc 'less'>}
Is same as updated? False
Uncomman parameters: ['params']
Uncomman parameters: {'validation_data': None, 'model': None, '_chief_worker_only': False, '_supports_tf_logs': True, 'monitor': 'val_loss', 'verbose': 0, 'filepath': '/content/', 'save_best_only': True, 'save_weights_only': True, 'save_freq': 'epoch', 'epochs_since_last_save': 0, '_batches_seen_since_last_saving': 0, '_last_batch_seen': 0, 'best': inf, '_options': <tensorflow.python.saved_model.save_options.SaveOptions object at 0x7a5f5942e3e0>, 'load_weights_on_restart': False, 'period': 1, 'monitor_op': <ufunc 'less'>}
```
Worse is with `tf.keras.callbacks.EarlyStopping`. It not only make a copy but also don't update parameters. 
```python
import tensorflow as tf                                                         # load tensorflow 
lstp=tf.keras.callbacks.EarlyStopping()                                         # make check point 
lstp_params=lstp.__dict__.copy()                                                # get all parameters 
print('Parameters        :',lstp_params)                                        # see parameters dictionary
lstp_params['patience']=10                                                      # update parameter ""patience"" to 10 (default 0)
lstp_params['verbose']=1                                                        # update parameter ""verbose"" to 1 (default 0)
print('Updated parameters:',lstp_params)                                        # see updated parameters 
lstp.set_params(lstp_params.copy())                                             # try to make update parameters
print('Is same as updated?',lstp_params==lstp.__dict__)                         # make check both are same or not 
# Even values are not updated from `lstp_params` and `lstp.__dict__` will contain copy of themself in new key `'params'`
print('Object parameters :',lstp.__dict__)                                              # see parameters dictionary
```
```shell
Parameters        : {'validation_data': None, 'model': None, '_chief_worker_only': None, '_supports_tf_logs': False, 'monitor': 'val_loss', 'patience': 0, 'verbose': 0, 'baseline': None, 'min_delta': 0, 'wait': 0, 'stopped_epoch': 0, 'restore_best_weights': False, 'best_weights': None, 'start_from_epoch': 0, 'monitor_op': <ufunc 'less'>}
Updated parameters: {'validation_data': None, 'model': None, '_chief_worker_only': None, '_supports_tf_logs': False, 'monitor': 'val_loss', 'patience': 10, 'verbose': 1, 'baseline': None, 'min_delta': 0, 'wait': 0, 'stopped_epoch': 0, 'restore_best_weights': False, 'best_weights': None, 'start_from_epoch': 0, 'monitor_op': <ufunc 'less'>}
Is same as updated? False
Object parameters : {'validation_data': None, 'model': None, '_chief_worker_only': None, '_supports_tf_logs': False, 'monitor': 'val_loss', 'patience': 0, 'verbose': 0, 'baseline': None, 'min_delta': 0, 'wait': 0, 'stopped_epoch': 0, 'restore_best_weights': False, 'best_weights': None, 'start_from_epoch': 0, 'monitor_op': <ufunc 'less'>, 'params': {'validation_data': None, 'model': None, '_chief_worker_only': None, '_supports_tf_logs': False, 'monitor': 'val_loss', 'patience': 10, 'verbose': 1, 'baseline': None, 'min_delta': 0, 'wait': 0, 'stopped_epoch': 0, 'restore_best_weights': False, 'best_weights': None, 'start_from_epoch': 0, 'monitor_op': <ufunc 'less'>}}
```




",True,"[-4.56014812e-01 -4.88542676e-01 -3.23187232e-01 -1.02496669e-01
  4.61623929e-02 -2.30051130e-01 -1.81081563e-01 -1.32588260e-02
 -2.16463566e-01 -3.58548090e-02  6.30111322e-02  7.47540072e-02
 -2.06130534e-01  1.94912314e-01  9.35513601e-02  3.56213868e-01
 -7.52537996e-02 -1.46078169e-01  3.46855879e-01  9.22137871e-02
  5.27848937e-02 -7.03864172e-02 -1.91248238e-01  2.13037074e-01
 -9.79746059e-02 -4.49718386e-02 -2.09176764e-01 -2.79361129e-01
 -3.03318724e-03  1.08571179e-01  1.35752112e-01  2.00824603e-01
 -2.12571099e-01 -1.37954563e-01  1.18312679e-01  2.28418782e-01
 -3.77484649e-01 -7.40609169e-02 -1.52337208e-01 -2.00999752e-02
  6.26771748e-02 -2.30255704e-02 -1.10005647e-01 -1.66031539e-01
 -1.07471764e-01 -2.46867642e-01  1.52205527e-01  4.85809296e-02
 -6.86982647e-02 -8.38846266e-02  8.56506526e-02  3.36662866e-03
 -3.66593361e-01 -4.62062955e-01 -3.14789154e-02 -4.62291352e-02
  1.96647067e-02  2.98510909e-01  9.45510566e-02 -3.80843803e-02
 -2.97369491e-02  7.81996325e-02 -6.28823936e-02  3.94934341e-02
 -2.78069023e-02  1.07741214e-01  5.44731617e-02 -4.47814167e-02
  3.92292082e-01 -1.55237019e-01 -7.63142407e-02 -1.64884880e-01
 -2.87464976e-01 -6.03557378e-02  5.07382378e-02  2.43719220e-01
 -2.28257440e-02  1.35428742e-01  1.62390679e-01 -1.91986382e-01
 -4.87630740e-02 -2.61333495e-01  1.24708056e-01 -9.49894860e-02
  1.86070323e-01 -5.44773936e-02  3.36568296e-01 -1.77846745e-01
  3.48242998e-01  1.10063970e-01  5.58793485e-01  5.38267270e-02
  7.06362873e-02  1.79886401e-01 -1.91441923e-02 -4.38781939e-02
  1.30138963e-01  3.81927975e-02 -2.42426351e-01 -6.90290630e-02
 -5.80651909e-02 -1.19256452e-01  9.91541222e-02  2.75950432e-01
  1.50645554e-01 -1.15147665e-01  1.60649464e-01 -2.74679214e-01
  2.34977767e-01  8.38303287e-03  1.93312526e-01 -1.28827035e-01
  1.85439840e-01 -1.42546147e-01  7.07076192e-02 -6.68322369e-02
 -1.69116929e-01  5.54973222e-02  1.80998623e-01  3.91401350e-01
  5.99617660e-02 -2.37903371e-01  1.54753834e-01  1.42833471e-01
  1.01569101e-01  2.60916293e-01 -8.19327831e-02  7.40216672e-02
  4.63181287e-02 -1.38651758e-01  9.18471962e-02  1.12355202e-01
 -8.90762135e-02 -1.16869554e-01  1.09761998e-01 -6.11880198e-02
 -3.38671535e-01 -3.05844724e-01 -2.23505348e-01 -4.35123563e-01
 -7.15562478e-02 -9.42660645e-02  7.87379891e-02 -2.44253874e-01
  1.00608543e-03 -4.36902978e-03 -1.72964975e-01  3.60535920e-01
 -1.02355272e-01 -9.72868353e-02 -2.85463165e-02 -7.26251025e-03
  2.18494348e-02  3.09472144e-01  7.56741986e-02  1.86008483e-01
  2.64310718e-01 -4.07081060e-02  2.61542141e-01 -3.61164391e-01
  4.77522872e-02  3.60795200e-01 -1.69214889e-01 -1.88296720e-01
  2.73950875e-01 -1.02077127e-01 -2.40439162e-01 -2.67859638e-01
  1.36134997e-02  2.07818329e-01 -3.07643786e-04  1.06034517e-01
 -8.01277757e-02 -4.32174876e-02  3.96772325e-01  1.02996700e-01
  1.28999054e-01 -6.50328517e-01 -6.18599355e-02  2.91349828e-01
  1.27025902e-01  2.57234126e-01 -4.47313935e-02 -1.48546070e-01
 -9.40342620e-03 -1.13373324e-01  1.49927050e-01 -5.39431423e-02
  1.28734291e-01  3.35951149e-03 -1.26065910e-01 -2.66215444e-01
  2.66318321e-01 -1.50455758e-01 -3.94751951e-02  5.87833598e-02
  2.45015129e-01 -1.39190465e-01  1.75172091e-01  1.89430378e-02
  3.49980295e-02  1.08935058e-01  2.65938025e-02 -5.33869043e-02
  1.21758051e-01 -2.16270790e-01 -1.20930284e-01 -2.04496384e-01
  5.13240434e-02 -1.35538667e-01  7.93757141e-02 -3.19495261e-01
 -1.21526942e-01  5.36900759e-03 -6.82374015e-02  7.41028786e-02
  2.42197841e-01  8.11396912e-03 -3.05341482e-01  1.83812901e-01
  1.36268944e-01  9.00084972e-02 -2.31434926e-02 -2.54364729e-01
 -1.22829832e-01 -9.37429368e-02 -1.30686402e-01  3.15742493e-02
 -8.95595253e-02  1.61117703e-01 -1.66838244e-02 -3.17878723e-02
  2.65200615e-01 -5.70836402e-02  6.52721003e-02 -8.95888954e-02
 -1.40838355e-01  5.87115064e-04  9.97800827e-02  5.14910594e-02
 -2.97882080e-01  1.70467049e-02 -7.05413967e-02 -3.54687691e-01
 -9.59052145e-03 -1.05272219e-01 -2.33586222e-01 -3.16837728e-02
 -2.10978553e-01  2.05977947e-01 -1.83238104e-01 -1.26137704e-01
  3.69727135e-01  1.52493551e-01  3.01076889e-01  1.49815768e-01
 -1.28950506e-01 -8.10060278e-03  2.00140029e-01  1.37001261e-01
  2.62645841e-01 -3.02572474e-02  1.48144484e-01  2.81589925e-01
  3.11052203e-01  3.11347455e-01 -7.65943453e-02  8.43019038e-02
 -1.29805487e-02 -5.91287985e-02 -1.04761846e-01 -1.69937909e-01
  3.25750768e-01 -2.87662774e-01  2.87889540e-01 -1.41059741e-01
  3.95571291e-01  2.29181468e-01 -1.04844503e-01 -1.54234841e-02
  2.16249645e-01  3.40426683e-01 -2.66643822e-01  6.17119744e-02
  2.83599701e-02 -2.91939497e-01 -4.34746407e-02 -4.69018072e-01
  2.23631784e-02  2.47451365e-02  3.48945148e-03  7.96646904e-03
  4.97971147e-01 -2.14910597e-01 -3.01994920e-01  3.02928276e-02
 -8.51556957e-02 -2.44850144e-02 -1.94605850e-02  1.10484801e-01
 -3.78589928e-01 -5.60580790e-02  3.37164819e-01 -1.24142081e-01
  8.09771046e-02 -6.32353574e-02  3.49779069e-01  1.21604368e-01
  1.31763086e-01 -2.78057307e-01  3.63105148e-01  2.65487134e-01
 -4.84053791e-02  2.54454255e-01 -5.47814416e-03  9.83855799e-02
 -5.78911155e-02  6.27578795e-01  1.48112297e-01 -6.93270192e-02
 -6.59201518e-02 -1.78573519e-01 -1.60625920e-01  7.85889179e-02
  1.21080883e-01  5.77024482e-02 -5.67189455e-02 -2.58714914e-01
  1.41717987e-02 -2.10290626e-01 -2.48568952e-01 -6.94618672e-02
 -1.76450551e-01  1.74820453e-01  1.43147349e-01 -6.97414204e-02
 -4.04992074e-01  2.33179003e-01  4.09782678e-02 -3.94762993e-01
 -1.45815998e-01  9.92781520e-02  1.10206947e-01 -4.10358578e-01
 -2.61967212e-01 -1.25608593e-01  2.62985170e-01  6.37717366e-01
 -9.53217968e-04 -2.70671938e-02  1.07871622e-01  1.19578481e-01
 -1.97783634e-01 -4.47351933e-02 -7.95988292e-02  3.23753774e-01
  6.85324334e-03 -1.37761489e-01  7.26179034e-02  4.60361630e-01
 -1.70031071e-01 -2.18797326e-01 -2.43958265e-01 -1.44630492e-01
  1.36534452e-01 -4.12941724e-02 -2.27042943e-01 -6.69155791e-02
  1.68290347e-01  3.56369823e-01 -8.87357146e-02  1.98094919e-01
 -9.06486064e-04  7.07223788e-02  2.41038680e-01 -2.67970383e-01
 -6.62466735e-02  1.47062391e-01  2.77803242e-01 -9.97942239e-02
  4.39567342e-02  6.09304495e-02  9.32213143e-02  1.38580054e-02]"
TensorFlow doesn't detect cuda drivers type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.1

### Custom code

No

### OS platform and distribution

Li

### Mobile device

Gentoo Linux 6.1.57-gentoo-x86_64

### Python version

3.11.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8/8.7.0.84

### GPU model and memory

NVIDIA GeForce GTX 1650 Mobile / Max-Q

### Current behavior?

Current behaviour: list of cuda-capable devices is empty

```sh
> python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
2023-11-06 01:01:39.145881: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-11-06 01:01:39.200934: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-11-06 01:01:39.201518: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-06 01:01:40.245608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-11-06 01:01:41.021740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-11-06 01:01:41.022476: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
```

Expected behaviour: list of coda-capable devices contains one item.

I would be happy to reproduce the bug in `tf-nightly`, but I can't even install it due to broken dependencies with tensorrt:

```sh
> python3 -m pip install 'tf-nightly[and-cuda]'
Collecting tf-nightly[and-cuda]
  Using cached tf_nightly-2.16.0.dev20231103-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)
Collecting absl-py>=1.0.0 (from tf-nightly[and-cuda])
  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)
Collecting astunparse>=1.6.0 (from tf-nightly[and-cuda])
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting flatbuffers>=23.5.26 (from tf-nightly[and-cuda])
  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)
Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tf-nightly[and-cuda])
  Using cached gast-0.5.4-py3-none-any.whl (19 kB)
Collecting google-pasta>=0.1.1 (from tf-nightly[and-cuda])
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Collecting h5py>=3.10.0 (from tf-nightly[and-cuda])
  Using cached h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)
Collecting libclang>=13.0.0 (from tf-nightly[and-cuda])
  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)
Collecting ml-dtypes~=0.3.1 (from tf-nightly[and-cuda])
  Using cached ml_dtypes-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)
Collecting opt-einsum>=2.3.2 (from tf-nightly[and-cuda])
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Collecting packaging (from tf-nightly[and-cuda])
  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)
Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tf-nightly[and-cuda])
  Using cached protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)
Requirement already satisfied: setuptools in ./miniconda3/envs/tf-test/lib/python3.11/site-packages (from tf-nightly[and-cuda]) (68.0.0)
Collecting six>=1.12.0 (from tf-nightly[and-cuda])
  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Collecting termcolor>=1.1.0 (from tf-nightly[and-cuda])
  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)
Collecting typing-extensions>=3.6.6 (from tf-nightly[and-cuda])
  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)
Collecting wrapt<1.15,>=1.11.0 (from tf-nightly[and-cuda])
  Using cached wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Collecting grpcio<2.0,>=1.24.3 (from tf-nightly[and-cuda])
  Using cached grpcio-1.59.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Collecting tb-nightly~=2.16.0.a (from tf-nightly[and-cuda])
  Using cached tb_nightly-2.16.0a20231105-py3-none-any.whl.metadata (1.7 kB)
Collecting tf-estimator-nightly~=2.14.0.dev (from tf-nightly[and-cuda])
  Using cached tf_estimator_nightly-2.14.0.dev2023080308-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting keras-nightly~=3.0.0.dev (from tf-nightly[and-cuda])
  Using cached keras_nightly-3.0.0.dev2023110403-py3-none-any.whl.metadata (5.3 kB)
Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tf-nightly[and-cuda])
  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)
Collecting numpy<2.0.0,>=1.23.5 (from tf-nightly[and-cuda])
  Using cached numpy-1.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
Collecting nvidia-cublas-cu12==12.2.5.6 (from tf-nightly[and-cuda])
  Using cached nvidia_cublas_cu12-12.2.5.6-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-cupti-cu12==12.2.142 (from tf-nightly[and-cuda])
  Using cached nvidia_cuda_cupti_cu12-12.2.142-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cuda-nvcc-cu12==12.2.140 (from tf-nightly[and-cuda])
  Using cached nvidia_cuda_nvcc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.2.140 (from tf-nightly[and-cuda])
  Using cached nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-runtime-cu12==12.2.140 (from tf-nightly[and-cuda])
  Using cached nvidia_cuda_runtime_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cudnn-cu12==8.9.4.25 (from tf-nightly[and-cuda])
  Using cached nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cufft-cu12==11.0.8.103 (from tf-nightly[and-cuda])
  Using cached nvidia_cufft_cu12-11.0.8.103-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-curand-cu12==10.3.3.141 (from tf-nightly[and-cuda])
  Using cached nvidia_curand_cu12-10.3.3.141-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cusolver-cu12==11.5.2.141 (from tf-nightly[and-cuda])
  Using cached nvidia_cusolver_cu12-11.5.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cusparse-cu12==12.1.2.141 (from tf-nightly[and-cuda])
  Using cached nvidia_cusparse_cu12-12.1.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-nccl-cu12==2.18.3 (from tf-nightly[and-cuda])
  Using cached nvidia_nccl_cu12-2.18.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvjitlink-cu12==12.2.140 (from tf-nightly[and-cuda])
  Using cached nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting tensorrt==8.6.1.post1 (from tf-nightly[and-cuda])
  Using cached tensorrt-8.6.1.post1.tar.gz (18 kB)
  Preparing metadata (setup.py) ... done
Collecting tensorrt-bindings==8.6.1 (from tf-nightly[and-cuda])
  Using cached tensorrt_bindings-8.6.1-cp311-none-manylinux_2_17_x86_64.whl (980 kB)
INFO: pip is looking at multiple versions of tf-nightly[and-cuda] to determine which version is compatible with other requirements. This could take a while.
Collecting tf-nightly[and-cuda]
  Using cached tf_nightly-2.16.0.dev20231102-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)
  Using cached tf_nightly-2.16.0.dev20231101-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)
  Using cached tf_nightly-2.16.0.dev20231031-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)
  Using cached tf_nightly-2.16.0.dev20231026-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)
Collecting tb-nightly~=2.15.0.a (from tf-nightly[and-cuda])
  Using cached tb_nightly-2.15.0a20231023-py3-none-any.whl.metadata (1.7 kB)
Collecting tf-nightly[and-cuda]
  Using cached tf_nightly-2.16.0.dev20231025-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)
Collecting nvidia-nccl-cu12==2.16.5 (from tf-nightly[and-cuda])
  Using cached nvidia_nccl_cu12-2.16.5-py3-none-manylinux1_x86_64.whl (188.7 MB)
Collecting tf-nightly[and-cuda]
  Using cached tf_nightly-2.16.0.dev20231024-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)
  Using cached tf_nightly-2.16.0.dev20231022-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)
INFO: pip is still looking at multiple versions of tf-nightly[and-cuda] to determine which version is compatible with other requirements. This could take a while.
  Using cached tf_nightly-2.16.0.dev20231021-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)
  Using cached tf_nightly-2.16.0.dev20231020-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)
  Using cached tf_nightly-2.16.0.dev20231013-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
Collecting ml-dtypes~=0.2.0 (from tf-nightly[and-cuda])
  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)
Collecting keras-nightly~=2.15.0.dev (from tf-nightly[and-cuda])
  Using cached keras_nightly-2.15.0.dev2023092207-py3-none-any.whl.metadata (2.5 kB)
Collecting tf-nightly[and-cuda]
  Using cached tf_nightly-2.15.0.dev20231012-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20231011-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
  Using cached tf_nightly-2.15.0.dev20231010-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20231009-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20231006-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20231005-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20231004-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20231003-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20231002-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20231001-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230930-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230929-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230928-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230927-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230926-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230925-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230924-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230923-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230922-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230921-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230920-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230919-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)
  Using cached tf_nightly-2.15.0.dev20230918-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting nvidia-cuda-runtime-cu11==11.8.89 (from tf-nightly[and-cuda])
  Using cached nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)
Collecting nvidia-cublas-cu11==11.11.3.6 (from tf-nightly[and-cuda])
  Using cached nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)
Collecting nvidia-cufft-cu11==10.9.0.58 (from tf-nightly[and-cuda])
  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
Collecting nvidia-cudnn-cu11==8.7.0.84 (from tf-nightly[and-cuda])
  Using cached nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)
Collecting nvidia-curand-cu11==10.3.0.86 (from tf-nightly[and-cuda])
  Using cached nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)
Collecting nvidia-cusolver-cu11==11.4.1.48 (from tf-nightly[and-cuda])
  Using cached nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)
Collecting nvidia-cusparse-cu11==11.7.5.86 (from tf-nightly[and-cuda])
  Using cached nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)
Collecting nvidia-nccl-cu11==2.16.5 (from tf-nightly[and-cuda])
  Using cached nvidia_nccl_cu11-2.16.5-py3-none-manylinux1_x86_64.whl (210.3 MB)
Collecting nvidia-cuda-cupti-cu11==11.8.87 (from tf-nightly[and-cuda])
  Using cached nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)
Collecting nvidia-cuda-nvcc-cu11==11.8.89 (from tf-nightly[and-cuda])
  Using cached nvidia_cuda_nvcc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (19.5 MB)
Collecting tf-nightly[and-cuda]
  Using cached tf_nightly-2.15.0.dev20230917-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
  Using cached tf_nightly-2.15.0.dev20230916-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
  Using cached tf_nightly-2.15.0.dev20230915-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
  Using cached tf_nightly-2.15.0.dev20230914-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
  Using cached tf_nightly-2.15.0.dev20230913-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
  Using cached tf_nightly-2.15.0.dev20230911-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
  Using cached tf_nightly-2.15.0.dev20230910-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
  Using cached tf_nightly-2.15.0.dev20230909-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
  Using cached tf_nightly-2.15.0.dev20230908-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230907-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230906-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230904-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230903-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230902-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230901-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230831-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230830-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230829-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230828-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230827-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230826-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230825-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230824-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230817-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230816-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230815-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230814-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230813-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230812-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230811-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230810-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
Collecting tb-nightly~=2.14.0.a (from tf-nightly[and-cuda])
  Using cached tb_nightly-2.14.0a20230808-py3-none-any.whl.metadata (1.8 kB)
Collecting keras-nightly~=2.14.0.dev (from tf-nightly[and-cuda])
  Using cached keras_nightly-2.14.0.dev2023080207-py3-none-any.whl.metadata (2.5 kB)
Collecting tf-nightly[and-cuda]
  Using cached tf_nightly-2.15.0.dev20230809-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230808-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
  Using cached tf_nightly-2.15.0.dev20230807-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)
ERROR: Cannot install tf-nightly[and-cuda]==2.15.0.dev20230807, tf-nightly[and-cuda]==2.15.0.dev20230808, tf-nightly[and-cuda]==2.15.0.dev20230809, tf-nightly[and-cuda]==2.15.0.dev20230810, tf-nightly[and-cuda]==2.15.0.dev20230811, tf-nightly[and-cuda]==2.15.0.dev20230812, tf-nightly[and-cuda]==2.15.0.dev20230813, tf-nightly[and-cuda]==2.15.0.dev20230814, tf-nightly[and-cuda]==2.15.0.dev20230815, tf-nightly[and-cuda]==2.15.0.dev20230816, tf-nightly[and-cuda]==2.15.0.dev20230817, tf-nightly[and-cuda]==2.15.0.dev20230824, tf-nightly[and-cuda]==2.15.0.dev20230825, tf-nightly[and-cuda]==2.15.0.dev20230826, tf-nightly[and-cuda]==2.15.0.dev20230827, tf-nightly[and-cuda]==2.15.0.dev20230828, tf-nightly[and-cuda]==2.15.0.dev20230829, tf-nightly[and-cuda]==2.15.0.dev20230830, tf-nightly[and-cuda]==2.15.0.dev20230831, tf-nightly[and-cuda]==2.15.0.dev20230901, tf-nightly[and-cuda]==2.15.0.dev20230902, tf-nightly[and-cuda]==2.15.0.dev20230903, tf-nightly[and-cuda]==2.15.0.dev20230904, tf-nightly[and-cuda]==2.15.0.dev20230906, tf-nightly[and-cuda]==2.15.0.dev20230907, tf-nightly[and-cuda]==2.15.0.dev20230908, tf-nightly[and-cuda]==2.15.0.dev20230909, tf-nightly[and-cuda]==2.15.0.dev20230910, tf-nightly[and-cuda]==2.15.0.dev20230911, tf-nightly[and-cuda]==2.15.0.dev20230913, tf-nightly[and-cuda]==2.15.0.dev20230914, tf-nightly[and-cuda]==2.15.0.dev20230915, tf-nightly[and-cuda]==2.15.0.dev20230916, tf-nightly[and-cuda]==2.15.0.dev20230917, tf-nightly[and-cuda]==2.15.0.dev20230918, tf-nightly[and-cuda]==2.15.0.dev20230919, tf-nightly[and-cuda]==2.15.0.dev20230920, tf-nightly[and-cuda]==2.15.0.dev20230921, tf-nightly[and-cuda]==2.15.0.dev20230922, tf-nightly[and-cuda]==2.15.0.dev20230923, tf-nightly[and-cuda]==2.15.0.dev20230924, tf-nightly[and-cuda]==2.15.0.dev20230925, tf-nightly[and-cuda]==2.15.0.dev20230926, tf-nightly[and-cuda]==2.15.0.dev20230927, tf-nightly[and-cuda]==2.15.0.dev20230928, tf-nightly[and-cuda]==2.15.0.dev20230929, tf-nightly[and-cuda]==2.15.0.dev20230930, tf-nightly[and-cuda]==2.15.0.dev20231001, tf-nightly[and-cuda]==2.15.0.dev20231002, tf-nightly[and-cuda]==2.15.0.dev20231003, tf-nightly[and-cuda]==2.15.0.dev20231004, tf-nightly[and-cuda]==2.15.0.dev20231005, tf-nightly[and-cuda]==2.15.0.dev20231006, tf-nightly[and-cuda]==2.15.0.dev20231009, tf-nightly[and-cuda]==2.15.0.dev20231010, tf-nightly[and-cuda]==2.15.0.dev20231011, tf-nightly[and-cuda]==2.15.0.dev20231012, tf-nightly[and-cuda]==2.16.0.dev20231013, tf-nightly[and-cuda]==2.16.0.dev20231020, tf-nightly[and-cuda]==2.16.0.dev20231021, tf-nightly[and-cuda]==2.16.0.dev20231022, tf-nightly[and-cuda]==2.16.0.dev20231024, tf-nightly[and-cuda]==2.16.0.dev20231025, tf-nightly[and-cuda]==2.16.0.dev20231026, tf-nightly[and-cuda]==2.16.0.dev20231031, tf-nightly[and-cuda]==2.16.0.dev20231101, tf-nightly[and-cuda]==2.16.0.dev20231102 and tf-nightly[and-cuda]==2.16.0.dev20231103 because these package versions have conflicting dependencies.

The conflict is caused by:
    tf-nightly[and-cuda] 2.16.0.dev20231103 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.16.0.dev20231102 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.16.0.dev20231101 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.16.0.dev20231031 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.16.0.dev20231026 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.16.0.dev20231025 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.16.0.dev20231024 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.16.0.dev20231022 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.16.0.dev20231021 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.16.0.dev20231020 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.16.0.dev20231013 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20231012 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20231011 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20231010 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20231009 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20231006 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20231005 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20231004 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20231003 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20231002 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20231001 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230930 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230929 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230928 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230927 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230926 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230925 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230924 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230923 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230922 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230921 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230920 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230919 depends on tensorrt-libs==8.6.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230918 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230917 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230916 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230915 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230914 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230913 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230911 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230910 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230909 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230908 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230907 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230906 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230904 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230903 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230902 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230901 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230831 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230830 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230829 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230828 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230827 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230826 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230825 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230824 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230817 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230816 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230815 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230814 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230813 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230812 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230811 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230810 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230809 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230808 depends on tensorrt==8.5.3.1; extra == ""and-cuda""
    tf-nightly[and-cuda] 2.15.0.dev20230807 depends on tensorrt==8.5.3.1; extra == ""and-cuda""

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts
```

### Standalone code to reproduce the issue

I've just copied commands from the official website:

```shell
python3 -m pip install 'tensorflow[and-cuda]'
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output
Driver:

```sh
> nvidia-smi
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.113.01             Driver Version: 535.113.01   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce GTX 1650        Off | 00000000:01:00.0  On |                  N/A |
| N/A   43C    P8               3W /  50W |    119MiB /  4096MiB |      7%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     15023    C+G   ...95206080,4704969098354582108,262144       36MiB |
|    0   N/A  N/A     26218      G   /usr/bin/X                                   81MiB |
+---------------------------------------------------------------------------------------+
```

Log of the `tensorflow[and-cuda]` installation:

```shell
Collecting tensorflow[and-cuda]
  Downloading tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting absl-py>=1.0.0 (from tensorflow[and-cuda])
  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)
Collecting astunparse>=1.6.0 (from tensorflow[and-cuda])
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting flatbuffers>=23.5.26 (from tensorflow[and-cuda])
  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)
Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow[and-cuda])
  Using cached gast-0.5.4-py3-none-any.whl (19 kB)
Collecting google-pasta>=0.1.1 (from tensorflow[and-cuda])
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Collecting h5py>=2.9.0 (from tensorflow[and-cuda])
  Using cached h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)
Collecting libclang>=13.0.0 (from tensorflow[and-cuda])
  Using cached libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)
Collecting ml-dtypes==0.2.0 (from tensorflow[and-cuda])
  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)
Collecting numpy>=1.23.5 (from tensorflow[and-cuda])
  Using cached numpy-1.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
Collecting opt-einsum>=2.3.2 (from tensorflow[and-cuda])
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Collecting packaging (from tensorflow[and-cuda])
  Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)
Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow[and-cuda])
  Using cached protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)
Requirement already satisfied: setuptools in ./miniconda3/envs/tf-test/lib/python3.11/site-packages (from tensorflow[and-cuda]) (68.0.0)
Collecting six>=1.12.0 (from tensorflow[and-cuda])
  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)
Collecting termcolor>=1.1.0 (from tensorflow[and-cuda])
  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)
Collecting typing-extensions>=3.6.6 (from tensorflow[and-cuda])
  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)
Collecting wrapt<1.15,>=1.11.0 (from tensorflow[and-cuda])
  Using cached wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)
Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow[and-cuda])
  Using cached tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)
Collecting grpcio<2.0,>=1.24.3 (from tensorflow[and-cuda])
  Using cached grpcio-1.59.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Collecting tensorboard<2.15,>=2.14 (from tensorflow[and-cuda])
  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)
Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow[and-cuda])
  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting keras<2.15,>=2.14.0 (from tensorflow[and-cuda])
  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)
Collecting nvidia-cuda-runtime-cu11==11.8.89 (from tensorflow[and-cuda])
  Using cached nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)
Collecting nvidia-cublas-cu11==11.11.3.6 (from tensorflow[and-cuda])
  Using cached nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)
Collecting nvidia-cufft-cu11==10.9.0.58 (from tensorflow[and-cuda])
  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)
Collecting nvidia-cudnn-cu11==8.7.0.84 (from tensorflow[and-cuda])
  Using cached nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)
Collecting nvidia-curand-cu11==10.3.0.86 (from tensorflow[and-cuda])
  Using cached nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)
Collecting nvidia-cusolver-cu11==11.4.1.48 (from tensorflow[and-cuda])
  Using cached nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)
Collecting nvidia-cusparse-cu11==11.7.5.86 (from tensorflow[and-cuda])
  Using cached nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)
Collecting nvidia-nccl-cu11==2.16.5 (from tensorflow[and-cuda])
  Using cached nvidia_nccl_cu11-2.16.5-py3-none-manylinux1_x86_64.whl (210.3 MB)
Collecting nvidia-cuda-cupti-cu11==11.8.87 (from tensorflow[and-cuda])
  Using cached nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)
Collecting nvidia-cuda-nvcc-cu11==11.8.89 (from tensorflow[and-cuda])
  Using cached nvidia_cuda_nvcc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (19.5 MB)
INFO: pip is looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.
Collecting tensorflow[and-cuda]
  Downloading tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)
WARNING: tensorflow 2.13.1 does not provide the extra 'and-cuda'
Collecting gast<=0.4.0,>=0.2.1 (from tensorflow[and-cuda])
  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)
Collecting keras<2.14,>=2.13.1 (from tensorflow[and-cuda])
  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)
Collecting numpy<=1.24.3,>=1.22 (from tensorflow[and-cuda])
  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)
      17.3/17.3 MB 10.9 MB/s eta 0:00:00
Collecting tensorboard<2.14,>=2.13 (from tensorflow[and-cuda])
  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)
      5.6/5.6 MB 11.1 MB/s eta 0:00:00
Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow[and-cuda])
  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow[and-cuda])
  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)
Collecting wrapt>=1.11.0 (from tensorflow[and-cuda])
  Downloading wrapt-1.15.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)
      78.9/78.9 kB 7.1 MB/s eta 0:00:00
Requirement already satisfied: wheel<1.0,>=0.23.0 in ./miniconda3/envs/tf-test/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.41.2)
Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading google_auth-2.23.4-py2.py3-none-any.whl.metadata (4.7 kB)
Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)
Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading Markdown-3.5.1-py3-none-any.whl.metadata (7.1 kB)
Collecting requests<3,>=2.21.0 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)
Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)
Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)
Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)
Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)
      181.3/181.3 kB 6.6 MB/s eta 0:00:00
Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading rsa-4.9-py3-none-any.whl (34 kB)
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)
Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)
Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading idna-3.4-py3-none-any.whl (61 kB)
      61.5/61.5 kB 6.4 MB/s eta 0:00:00
Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading urllib3-2.0.7-py3-none-any.whl.metadata (6.6 kB)
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)
Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)
Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)
      83.9/83.9 kB 7.6 MB/s eta 0:00:00
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow[and-cuda])
  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)
      151.7/151.7 kB 9.0 MB/s eta 0:00:00
Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)
    130.2/130.2 kB 5.7 MB/s eta 0:00:00
Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)
Downloading grpcio-1.59.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)
    5.3/5.3 MB 10.8 MB/s eta 0:00:00
Downloading h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)
    4.8/4.8 MB 10.9 MB/s eta 0:00:00
Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)
    1.7/1.7 MB 10.2 MB/s eta 0:00:00
Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)
    22.9/22.9 MB 10.9 MB/s eta 0:00:00
Downloading protobuf-4.25.0-cp37-abi3-manylinux2014_x86_64.whl (294 kB)
    294.4/294.4 kB 10.0 MB/s eta 0:00:00
Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)
    440.8/440.8 kB 8.7 MB/s eta 0:00:00
Downloading tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)
    2.4/2.4 MB 11.2 MB/s eta 0:00:00
Downloading packaging-23.2-py3-none-any.whl (53 kB)
    53.0/53.0 kB 6.1 MB/s eta 0:00:00
Downloading tensorflow-2.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.7 MB)
    479.7/479.7 MB 6.0 MB/s eta 0:00:00
Downloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)
    183.3/183.3 kB 8.9 MB/s eta 0:00:00
Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)
    102.2/102.2 kB 8.1 MB/s eta 0:00:00
Downloading requests-2.31.0-py3-none-any.whl (62 kB)
    62.6/62.6 kB 6.2 MB/s eta 0:00:00
Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)
    6.6/6.6 MB 11.1 MB/s eta 0:00:00
Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)
    226.7/226.7 kB 10.6 MB/s eta 0:00:00
Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)
Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)
    158.3/158.3 kB 10.5 MB/s eta 0:00:00
Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)
    140.3/140.3 kB 8.9 MB/s eta 0:00:00
Downloading MarkupSafe-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)
Downloading urllib3-2.0.7-py3-none-any.whl (124 kB)
    124.2/124.2 kB 9.0 MB/s eta 0:00:00
Installing collected packages: libclang, flatbuffers, wrapt, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, pyasn1, protobuf, packaging, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, rsa, requests, pyasn1-modules, opt-einsum, h5py, google-pasta, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow
Successfully installed MarkupSafe-2.1.3 absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.2 certifi-2023.7.22 charset-normalizer-3.3.2 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.23.4 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.59.2 h5py-3.10.0 idna-3.4 keras-2.13.1 libclang-16.0.6 markdown-3.5.1 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.2 protobuf-4.25.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 six-1.16.0 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.1 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0 typing-extensions-4.5.0 urllib3-2.0.7 werkzeug-3.0.1 wrapt-1.15.0
```
",True,"[-0.6035999  -0.56856114 -0.31844038  0.045815    0.30523866 -0.53064597
 -0.13204208  0.04852163 -0.293206   -0.40057632  0.14318715 -0.10328902
 -0.16410393  0.02021115 -0.26372147  0.13116437 -0.11143768  0.0911083
  0.10518055  0.12656897 -0.30354938 -0.12132894 -0.2766534  -0.14338131
  0.28631175  0.20438834 -0.02871986 -0.21522817  0.09540318 -0.04698649
  0.5556431   0.33057943  0.02082043  0.09441723 -0.02525435  0.08651946
 -0.35627723 -0.29607284 -0.14962775 -0.17685255 -0.20988032 -0.18732825
  0.29578823 -0.04244306 -0.04446584 -0.2736372   0.08708324  0.03064886
 -0.09825107 -0.3702618   0.25788504 -0.18591738 -0.29735404 -0.31280667
 -0.16454947  0.02305701 -0.06126207 -0.07700916 -0.01614825  0.3626431
  0.14832336 -0.0189069  -0.01648928 -0.17001987  0.05150088  0.30244076
  0.24783225 -0.34512377  0.63068044 -0.22877799  0.08749373  0.15631634
 -0.20993131 -0.13380116 -0.04095284  0.19875367  0.2751301   0.3749023
  0.01185292  0.04640758  0.27594864 -0.02041183  0.16760191 -0.4968099
  0.22180131 -0.03290387  0.34820443  0.12665936  0.36170977 -0.23856156
  0.3057915   0.43643293  0.07550368  0.02126869  0.06643449  0.10414659
  0.20001084  0.03916346 -0.110635   -0.33604297 -0.30603725 -0.03175873
  0.07293773  0.47552422 -0.19825065 -0.19856006  0.11870625  0.00229621
 -0.09762394 -0.3424024   0.17061552 -0.11243132  0.25969744 -0.07656159
  0.03754075 -0.06427917 -0.44620144 -0.29761586  0.15096821  0.5530018
 -0.29103202 -0.17579374  0.05645068  0.00394086  0.5773728   0.20782147
 -0.09393868 -0.03636706 -0.20626566 -0.05959297  0.02890504  0.08815166
  0.46758622  0.27221006 -0.3418728   0.26252437 -0.25448126 -0.22635484
 -0.23856267 -0.15407601 -0.22057864  0.2885837   0.11335292 -0.3691554
  0.09810939  0.07348169 -0.2015928   0.22791994  0.0044108   0.28531486
 -0.03578455  0.10296736 -0.2441779   0.49657035  0.13790786  0.18425725
  0.4266972  -0.07536185  0.0186825  -0.7495725   0.32978922  0.51225334
 -0.17872816  0.02921952 -0.152864    0.21180944 -0.23855409 -0.15796126
  0.002811    0.57226884 -0.411453   -0.16662341  0.22135663 -0.10458103
 -0.00563068 -0.22736424  0.10101035 -0.5866657  -0.04231128  0.6277944
 -0.22005816  0.18801308  0.5365826   0.10238784  0.2695262   0.33605027
  0.12784469  0.23388532 -0.30615053 -0.02613406 -0.40135843 -0.3417346
  0.18544687 -0.15204456  0.04676443  0.1717102   0.14404337  0.21856077
 -0.03329648 -0.16516891 -0.25925213 -0.16159165 -0.06239754  0.30045602
 -0.0346783  -0.29125935 -0.13383543 -0.21941072 -0.22765253  0.09280542
 -0.01681824 -0.48654655  0.00300066 -0.04853828 -0.44786903  0.31178582
  0.15078643 -0.051236   -0.2937322   0.5460129   0.4119998  -0.24628216
  0.23287845 -0.2789279  -0.10231562 -0.01415849 -0.04756527 -0.14610909
 -0.14083081  0.07456545  0.10635471  0.07064152  0.17472865  0.14961283
  0.29628268  0.04258312 -0.26499024 -0.1861594  -0.01396881  0.10067653
 -0.58663845  0.01057333 -0.01787167  0.09067942  0.2316894   0.55005175
 -0.11524878  0.04239596 -0.5590912  -0.11684189 -0.09103127  0.13598225
  0.2427073   0.0549977   0.30460203 -0.05006611  0.23173444  0.22641698
  0.17526703  0.06324761  0.49402148  0.46832836 -0.15424496  0.3638134
  0.24027033  0.45654958 -0.5106392   0.55378634  0.03944475  0.00938649
 -0.14522658 -0.24962805  0.57332045 -0.4909898   0.15728483 -0.02845047
  0.44266203 -0.08702254  0.01298695  0.1136141   0.33381096 -0.0093839
  0.0635113   0.09382789  0.10749601 -0.4374075  -0.24119458 -0.67724335
 -0.18390098 -0.15803978 -0.30618972  0.398156    0.02914352  0.00623495
 -0.32448173  0.03485653  0.09741679  0.05095334  0.1854054   0.06811161
 -0.3188516  -0.06720784  0.29305938 -0.865036   -0.177289   -0.13883024
  0.32555655  0.3424322   0.27478075 -0.32447553  0.08613788 -0.11760524
 -0.03616681  0.41329378 -0.04832901  0.17364168 -0.28126264  0.66851735
  0.28096685  0.03009841 -0.04074682 -0.47335434 -0.30927926 -0.09387547
  0.34479624 -0.08840963 -0.21569872 -0.16517045  0.02198751  0.4300293
 -0.02335857  0.13793676  0.05075876  0.24276984 -0.19911063  0.23213884
 -0.34545588 -0.05530327  0.140084   -0.3118582  -0.31074268  0.02431475
 -0.17593795 -0.15313047  0.15282132 -0.33152926  0.42297286  0.5652609
  0.04978979  0.1137908  -0.28570518  0.24215831 -0.34157294 -0.2579834
 -0.01016489  0.54546815  0.38220823 -0.35422832  0.5163355  -0.06343618
 -0.03136525  0.06638407 -0.3139928   0.037579    0.03347655 -0.34781224
 -0.04678402 -0.26766253  0.14025638  0.34300393 -0.43039644 -0.02434049
 -0.24628419  0.28541332  0.5742979  -0.5905293  -0.486578   -0.0702673
  0.23551825  0.15532634  0.02693784 -0.08101851  0.41980708 -0.12997748]"
"How to solve ""DataType error: DataType 0 is not recognized in Java."" when using tensorflowlite in android stat:awaiting response type:bug comp:lite TF 2.7","I am trying to perform on device training in kotlin with tensorflowlite in Android Studio. I followed the tutorial from [text](https://tensorflow.google.cn/lite/examples/on_device_training/overview) and the github page on it, but I am trying to train a MLP instead of CNN, and the data is generated by hand. I learnt from the example code from github codes in that web and as long as the training process began, the error occured and the logcat is as follow:
```
FATAL EXCEPTION: pool-2-thread-1
Process: com.example.deeplearningforinfer, PID: 32094
java.lang.IllegalArgumentException: DataType error: DataType 0 is not recognized in Java.
at org.tensorflow.lite.DataTypeUtils.fromC(DataTypeUtils.java:69)
at org.tensorflow.lite.TensorImpl.<init>(TensorImpl.java:479)
at org.tensorflow.lite.TensorImpl.fromSignatureInput(TensorImpl.java:49)
at org.tensorflow.lite.NativeSignatureRunnerWrapper.getInputTensor(NativeSignatureRunnerWrapper.java:49)
at org.tensorflow.lite.NativeInterpreterWrapper.getInputTensor(NativeInterpreterWrapper.java:418)
at org.tensorflow.lite.NativeInterpreterWrapper.runSignature(NativeInterpreterWrapper.java:193)
at org.tensorflow.lite.Interpreter.runSignature(Interpreter.java:261)
at com.example.deeplearningforinfer.TransferLearningHelper.train(TransferLearningHelper.kt:64)
at com.example.deeplearningforinfer.TransferLearningHelper.startTraining$lambda$5(TransferLearningHelper.kt:146)
at com.example.deeplearningforinfer.TransferLearningHelper.$r8$lambda$7My04QIePTcHhSGX3SRRzpn0w1Y(Unknown Source:0)
```
The android device is Pixel 6(API level 30), and I also test it on my vivo neo5(API 33). The version of tensorflow for converting the model is 2.7.0 and on Ubuntu 20.04.

The full description is [stackoverflow](https://stackoverflow.com/questions/77332595/how-to-solve-datatype-error-datatype-0-is-not-recognized-in-java-when-using)
",True,"[-0.3007279  -0.46369904 -0.25543573 -0.10727851  0.2542751  -0.19657364
 -0.08800012  0.1955674  -0.40195727 -0.16133384  0.1827136  -0.08598466
 -0.05331785  0.09717771 -0.1612817   0.1313252  -0.17496037  0.13141085
 -0.0694647  -0.05324391 -0.13555022 -0.11002634  0.27126038  0.20653123
  0.30131245  0.18510509 -0.25072825 -0.27814493  0.25761408  0.09154959
  0.11222416  0.11060634 -0.10264055  0.06066516 -0.25212556  0.04008866
 -0.24530075  0.08307028 -0.199824    0.0435436   0.18344083  0.00362652
  0.13135555  0.16320297 -0.06193289  0.16972674 -0.1519435   0.10675744
 -0.20996478 -0.03713397 -0.10166456 -0.160876   -0.40902868 -0.15910576
 -0.06593027  0.1056165  -0.16090956  0.0834972   0.10010403  0.11277133
  0.09326105 -0.08606499  0.09439329 -0.1055999  -0.14913073  0.22779745
  0.16108945 -0.21959704  0.6140016  -0.43779352  0.20446002  0.08472759
 -0.42927518  0.3659115  -0.06118319  0.13172705 -0.13287485  0.29933807
  0.22077343 -0.18135202  0.10134437 -0.06483054  0.17003214  0.21950679
  0.3659228  -0.01140971  0.25229287  0.15827276  0.174393    0.3305078
  0.15532672  0.25197816 -0.15510716  0.12084472  0.16842832  0.26735723
 -0.07451113  0.22089761 -0.15874223 -0.12223838 -0.11651635 -0.32764724
 -0.21420854  0.26428503  0.20689192 -0.31941664 -0.05554274 -0.02529533
  0.29501957 -0.0673403   0.29366937 -0.10868581 -0.03747762  0.0569449
  0.09593755 -0.05947142 -0.18392189  0.28382936 -0.21370065  0.5385639
 -0.2539288  -0.18474539 -0.03171625 -0.12965728  0.29354066  0.03242802
 -0.34463882 -0.0432953  -0.12460895  0.08679777  0.3126411   0.08140597
 -0.21810524 -0.26751977 -0.1122556  -0.13006213 -0.0787185  -0.11905986
 -0.19288133 -0.07531746 -0.17971106  0.21911034 -0.0441568   0.00801661
  0.1501921   0.05677013 -0.11678854  0.14350751  0.0723266  -0.20382556
 -0.12201127 -0.00455106 -0.11576679  0.25195384  0.20801386  0.15550834
  0.3433764   0.03648465 -0.03444523 -0.5461917  -0.05518034  0.19206083
 -0.06946956 -0.1561206   0.29259515 -0.02344056 -0.2858507   0.01653122
 -0.13517211  0.08233636  0.05685459  0.04245669  0.01191688  0.1733414
  0.24238464 -0.02157285  0.25702608 -0.45765454  0.00176584 -0.02922706
  0.4439023   0.02032239  0.1417764   0.19802669  0.02136536  0.13141257
 -0.10070522  0.09262459 -0.32468605  0.12551913 -0.40825254  0.1686455
  0.11723845 -0.04226286 -0.25525242 -0.29592413  0.14751878 -0.09976161
  0.02597186 -0.00232008 -0.13881348  0.10872854  0.04635879 -0.06393965
  0.03917135 -0.27761957 -0.49447963 -0.0940724  -0.22409876  0.20628427
  0.08925413 -0.32048863 -0.09467795 -0.22221103 -0.1834828   0.01198242
  0.0994952   0.13188961 -0.08447108  0.32965755 -0.1366266   0.03796539
 -0.18096022 -0.20730102 -0.49496344 -0.03602393  0.22140658  0.01649876
  0.0481328   0.46882358 -0.06606826  0.09105863  0.32230297  0.01867917
  0.1008746  -0.3860461  -0.2296029  -0.33171326 -0.26712185 -0.28162888
 -0.38124365  0.09168203 -0.11440521 -0.13042736  0.07108415  0.09287896
 -0.04990927  0.14928564 -0.0740266   0.23708531 -0.19108514 -0.0455689
  0.0878519   0.09343069  0.35868078  0.06256329  0.07462053 -0.34901178
  0.14545919 -0.0550455   0.40797803  0.05507369 -0.03584633  0.48941523
  0.0822686   0.00903889 -0.33271396  0.02690457 -0.22459552 -0.24515581
  0.1917163  -0.18808734  0.2519945  -0.07345607  0.10554466  0.08421772
  0.42174953  0.10808156 -0.06018755 -0.11582558  0.29505247  0.12818962
 -0.27334404 -0.19160247 -0.06289561 -0.48067915 -0.01056613 -0.21867707
 -0.14914796  0.0395546  -0.03664025  0.2590856   0.0329948   0.239458
 -0.12369986  0.06119197  0.26932952 -0.05690621  0.2500366   0.13601923
 -0.37404978  0.19028834  0.20208275 -0.17379054  0.18544906 -0.09227218
  0.207243   -0.08623265  0.47115028  0.02164264  0.09383944  0.05940557
 -0.06195029  0.46510202 -0.08740595  0.02949513 -0.21851258  0.38850757
  0.05891959  0.05123894  0.12459457 -0.09495555 -0.10905384 -0.13006052
  0.08277765  0.37793538 -0.2214348  -0.02995804 -0.23336422  0.20838398
 -0.27018157  0.088108    0.05372355  0.17829116  0.03383138 -0.29060054
 -0.14491753  0.27831474  0.1161325  -0.04886511  0.11841267 -0.07179976
 -0.20195645 -0.18332621  0.01974415 -0.2507841   0.39071375  0.67102385
 -0.07908407  0.18429928 -0.08177204  0.01212347 -0.23812005 -0.04067992
 -0.23128472  0.31616348  0.04068017 -0.1885786   0.24184161  0.49504447
 -0.25421607  0.02177349 -0.27360767 -0.17139477  0.23752965 -0.06588106
  0.05423713 -0.0689518   0.20253687  0.49643344 -0.18540177  0.14091218
 -0.39546245 -0.19823174  0.25285363 -0.15802458 -0.13315925 -0.0167041
 -0.15471667 -0.00081464 -0.3419926  -0.10185851  0.12976062 -0.20656568]"
@org_tensorflow//tensorflow/lite/schema components visibility stat:awaiting tensorflower type:bug comp:lite TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13+

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi,

I am updating https://github.com/google-coral/pycoral to support Tensorflow 2.13+. This repo used to be able to reference and use the following bazel libs:

@org_tensorflow//tensorflow/lite/schema:schema_conversion_utils
@org_tensorflow//tensorflow/lite/schema:schema_utils

But the visibility of these components was changed from v2.2.0 and is now "":utils_friends""

Could these be reverted to public so we can continue to use them or else suggest how I might use them as they currently stand?

Many thanks

### Standalone code to reproduce the issue

```shell
.
```


### Relevant log output

```shell
ERROR: /workspace/coral/learn/BUILD:21:11: in cc_library rule //coral/learn:utils: target '@org_tensorflow//tensorflow/lite/schema:schema_conversion_utils' is not visible from target '//coral/learn:utils'. Check the visibility declaration of the former target if you think the dependency is legitimate
ERROR: /workspace/coral/learn/BUILD:21:11: in cc_library rule //coral/learn:utils: target '@org_tensorflow//tensorflow/lite/schema:schema_utils' is not visible from target '//coral/learn:utils'. Check the visibility declaration of the former target if you think the dependency is legitimate
ERROR: /workspace/coral/learn/BUILD:21:11: Analysis of target '//coral/learn:utils' failed
```
",True,"[-5.07151604e-01 -4.37344134e-01 -3.79115902e-02  1.41002819e-01
  3.96763712e-01 -4.30639744e-01 -4.30693328e-02 -6.15372173e-02
 -2.91916221e-01 -3.61410916e-01  7.34258890e-02 -1.86460018e-01
 -1.80069745e-01 -3.49149741e-02 -1.95476815e-01  2.69159108e-01
 -1.52297765e-01 -6.65908232e-02  8.56213123e-02  2.14297641e-02
 -1.50748745e-01 -1.99745417e-01 -2.86031723e-01  2.18142092e-01
  1.50795430e-01  2.15576410e-01 -3.87100279e-01 -5.91548644e-02
 -6.14119843e-02  1.06972709e-01  4.46414530e-01 -5.86375035e-03
 -2.49858826e-01  1.58962190e-01 -1.79763995e-02  2.49782264e-01
 -2.68391311e-01 -1.41623288e-01 -2.61471301e-01  8.17042291e-02
  8.05139095e-02 -4.37768698e-02  1.62207872e-01 -1.84050143e-01
  6.01916835e-02 -1.95376158e-01 -2.41259728e-02 -1.85063437e-01
 -1.44666672e-01 -1.70196265e-01 -1.68292701e-01 -1.47741646e-01
 -6.12416625e-01 -2.28670284e-01 -1.62004471e-01 -3.21992412e-02
  2.03648895e-01 -1.41803235e-01 -9.12961550e-03  1.21221945e-01
  1.05491482e-01  7.06975013e-02  1.01490468e-01 -5.16552068e-02
  7.23916441e-02  1.69472113e-01  3.06538224e-01 -7.48066530e-02
  5.51213026e-01 -1.59818813e-01  3.20833087e-01 -3.14897224e-02
 -4.16012615e-01  2.69271545e-02 -5.87836020e-02  1.79616183e-01
 -2.32809056e-02  2.21382126e-01  3.02848935e-01 -2.76157200e-01
 -1.44211054e-02 -2.06443071e-01  6.15552925e-02 -2.49343753e-01
  2.65404195e-01 -7.63627365e-02  4.19945776e-01  2.85172015e-01
  4.45062280e-01 -2.61791825e-01  4.10921901e-01  2.60741711e-01
  6.48148172e-03  9.69383121e-02  4.32996690e-01  9.35993269e-02
  1.06050730e-01  1.95142716e-01  3.12844291e-02 -1.57915518e-01
 -2.52776235e-01 -1.86550319e-01  5.49895987e-02  7.20325932e-02
 -4.40482497e-02 -4.92131934e-02  1.31139278e-01 -4.15338166e-02
  8.30399320e-02 -5.30624390e-02  1.93319097e-01 -1.04255751e-01
  2.81772196e-01 -1.86278462e-01 -8.87747854e-02 -2.40805358e-01
 -1.51657909e-01 -2.12066639e-02 -1.61971617e-02  8.18660021e-01
  8.91361684e-02  1.50554463e-01  1.06026568e-01  1.54888153e-01
  4.96483445e-01  1.30802512e-01 -5.93706258e-02  7.52129927e-02
  2.30915640e-02  1.03548393e-02 -3.91521230e-02  7.49355331e-02
  2.66765114e-02  3.03196967e-01 -2.97695193e-02  1.56189948e-01
 -2.27178782e-01 -2.05817878e-01 -9.04511362e-02 -7.56337196e-02
 -3.84269476e-01  1.66089669e-01 -1.35399699e-01 -5.24677634e-01
  3.13466668e-01  1.79869860e-01 -1.66317403e-01  2.05251396e-01
 -1.78628534e-01 -2.23636199e-02 -6.27931505e-02  1.20807372e-01
 -6.22772798e-02  3.79375279e-01  5.08907922e-02  2.76082158e-01
  2.18607470e-01 -3.32562625e-02  5.86858690e-02 -5.25691450e-01
  1.34457320e-01  4.14230615e-01 -7.56940171e-02 -1.70488447e-01
  1.22141227e-01  9.41869766e-02 -4.42954302e-01 -2.84314036e-01
  9.91530567e-02  4.47065145e-01  6.28903806e-02 -1.59542218e-01
 -1.55810844e-02  2.66029835e-01  8.77145007e-02 -1.39638871e-01
  4.03135717e-01 -6.98085546e-01 -1.19700737e-01  3.45621437e-01
  9.96548235e-02  7.09975287e-02  1.60814583e-01  2.24173024e-01
  4.19693142e-02  3.40911821e-02  1.39206961e-01  1.02607861e-01
 -2.43662268e-01  1.02179144e-02 -3.66198599e-01 -1.82021007e-01
  4.89261866e-01 -1.96483523e-01 -7.26022050e-02  1.50697470e-01
  2.82147229e-01 -3.28152895e-01  4.50468622e-04 -1.05561819e-02
 -1.69754386e-01 -1.14504948e-01 -2.53020704e-01 -1.93410050e-02
  9.03572291e-02 -3.37274849e-01 -1.24663532e-01 -3.11287820e-01
 -4.59055722e-01  3.52531485e-03  1.98493630e-01 -5.58363199e-01
  1.61331594e-02 -1.21197775e-01 -2.86698520e-01  4.23368961e-01
  3.73420864e-02  3.60777900e-02 -2.13930994e-01  3.17804068e-01
  1.86836705e-01 -1.77771017e-01 -1.08354740e-01 -3.92792642e-01
 -3.17800462e-01  1.00183025e-01 -2.40035117e-01  1.23977020e-01
  1.03632316e-01  4.05527443e-01  4.17425111e-02  7.23837689e-02
  4.81403232e-01  3.41040492e-01  3.73988628e-01 -6.15578890e-02
 -1.95349783e-01 -3.68518800e-01 -2.74413049e-01 -8.59142393e-02
 -2.13595152e-01 -1.05878606e-01 -2.38372423e-02 -1.29947457e-02
  2.93919563e-01  3.61714363e-01 -2.36579955e-01 -1.78778350e-01
 -3.89417797e-01  3.69122922e-01 -2.90413320e-01  1.41602516e-01
  2.24279612e-01  6.62825406e-02  6.15045369e-01  1.64504290e-01
  1.55644074e-01  1.77487656e-01  3.43047380e-01 -1.88890561e-01
  4.83896106e-01  5.55587783e-02 -3.93385217e-02  4.13204730e-01
  2.87084520e-01  3.37311327e-01 -3.30784559e-01  4.68099207e-01
 -2.56639626e-02 -1.62210524e-01  9.14515182e-02 -4.01338995e-01
  5.82158387e-01 -3.08072627e-01 -6.46876991e-02 -1.19579196e-01
  4.40251708e-01 -1.29029714e-02 -1.17340833e-01  1.09052166e-01
  8.62468630e-02  2.99352795e-01 -2.99869478e-01 -6.01483323e-02
  1.04927301e-01 -2.14878932e-01 -7.66133070e-02 -5.37423730e-01
 -2.56244332e-01  5.44475652e-02 -3.73568028e-01  1.55447692e-01
 -1.02828957e-01 -4.36656363e-02 -1.19292423e-01 -2.07303017e-02
  6.62601888e-02  2.29034796e-02  1.92665637e-01  1.58617079e-01
 -9.07417294e-03  8.62887725e-02  5.59159100e-01 -5.16770661e-01
 -1.18076742e-01 -1.20636582e-01  4.32788908e-01  2.13667378e-01
  4.73280013e-01 -5.40758073e-01  1.28245816e-01 -2.98926890e-01
 -2.01519113e-04  4.37969059e-01 -3.15727927e-02  8.52393061e-02
 -3.12173069e-01  7.08676457e-01  2.88447261e-01 -2.17637777e-01
  2.97287107e-01 -8.59732181e-02 -3.22058827e-01 -2.76016388e-02
  2.37696260e-01 -1.94377482e-01 -5.30001260e-02 -4.77201760e-01
 -1.08774379e-01  2.13253319e-01 -1.71701267e-01 -1.30252942e-01
  5.79482019e-02 -1.31363690e-01 -1.08864799e-01 -1.85016036e-01
 -5.02226293e-01  1.46521196e-01 -7.68803209e-02 -3.20176005e-01
 -1.64007246e-01 -6.76993355e-02  5.55033684e-02 -2.76887566e-01
  3.02581936e-02 -2.28116825e-01  3.53571475e-01  5.50338745e-01
 -1.87059134e-01  2.07914442e-01  1.21303732e-02  2.88763762e-01
 -3.77336800e-01  1.13186399e-02  1.65925547e-02  3.65192711e-01
 -4.63270582e-03 -6.45665228e-02  4.10027385e-01  3.63041818e-01
 -1.38437122e-01  4.33732867e-02 -1.94160730e-01  1.46653548e-01
  2.73754358e-01 -1.52997598e-01 -5.40420301e-02 -3.08716357e-01
  1.74507290e-01  4.87460703e-01 -8.57113376e-02  1.31303310e-01
 -3.23646277e-01  1.43167377e-01  3.99342030e-01 -5.63707590e-01
 -2.15153366e-01  1.16705641e-01  1.84833273e-01 -2.86587000e-01
  1.09929614e-01 -2.15377495e-01  1.47904664e-01  1.70001909e-02]"
Different Behavior of tf.raw_ops.Cos+tf.raw_ops.Erfc with jit_compile=True stat:awaiting tensorflower type:bug comp:ops TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

When the **tf.raw_ops.Cos+tf.raw_ops.Erfc** operation is invoked within a tf.function with JIT compilation enabled (**jit_compile=True**), it produces different results compared to the same operation called without JIT compilation. This inconsistency is observed when the code is executed on a **CPU** device.
The problem occurs when input Tensors pass through **tf.raw_ops.Cos+tf.raw_ops.Erfc** and raw_ops.Sin. With individual Ops there is no issue.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import traceback

class Network(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
      
      x = tf.raw_ops.Cos(x=x, )        
      x = tf.raw_ops.Erfc(x=x, )        
      return x

m = Network()
inp = {
    ""x"": tf.random.normal([10, 9, 8], dtype=tf.bfloat16),
    }

with tf.device('/CPU:0'):
    tf.config.run_functions_eagerly(True)
    no_op_res = m(**inp)
    tf.config.run_functions_eagerly(False)
    with tf.device('/CPU:0'):
        op_res = m(**inp)

    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
```


### Relevant log output

```shell
File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 27, in <module>
    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=(10, 9, 8) dtype=float64) = '
0.17578125, 1.1484375, 0.267578125, ...
b'y (shape=(10, 9, 8) dtype=float64) = '
0.1767578125, 1.1484375, 0.267578125, ...
```
",True,"[-5.57124794e-01 -6.09963536e-01 -8.85344148e-02  2.89732870e-02
  3.10321271e-01 -5.65026522e-01 -1.49771459e-02  1.18448302e-01
 -3.63798141e-01 -4.54994678e-01  1.44967481e-01  9.86509919e-02
 -1.00365534e-01  2.55728494e-02 -2.99067348e-01  2.35272765e-01
 -2.43297145e-02 -1.46109015e-01  9.42909569e-02  1.11166812e-01
 -3.09825242e-01 -3.53789210e-01 -2.74049968e-01  7.12101310e-02
  1.56124949e-01  3.65968347e-01 -3.80772144e-01  2.65713096e-01
 -4.37034108e-03  2.98443764e-01  4.94472504e-01  1.72505736e-01
  9.25513729e-03  1.65835261e-01 -1.10708505e-01  2.02620074e-01
 -2.58277446e-01 -1.36381447e-01 -1.18809536e-01  2.90139467e-02
 -1.26202673e-01 -3.71407606e-02  2.51918286e-01 -1.47059098e-01
 -7.70294070e-02 -2.27110693e-03  4.56909016e-02 -8.41078982e-02
 -4.91463467e-02 -2.79451907e-01  2.68225335e-02  2.32863531e-01
 -4.65067565e-01 -3.59558374e-01  4.62085828e-02  2.25860745e-01
  5.52240238e-02 -2.62674354e-02 -7.81648457e-02  1.81395158e-01
  1.93228036e-01 -6.57218844e-02 -3.28964889e-02  1.43218040e-01
  2.26408318e-01  1.02667123e-01  3.72516781e-01  3.69392596e-02
  4.31804419e-01 -3.17837179e-01  2.16189444e-01  4.81875576e-02
 -3.33254844e-01  1.80404425e-01  2.88742315e-02  2.20127165e-01
 -1.00689799e-01  1.59417167e-01  3.42259556e-01 -2.59745955e-01
  1.28104702e-01 -1.44430146e-01  4.66918088e-02 -1.99318767e-01
  1.75878435e-01 -1.68849558e-01  3.79841268e-01  4.32905555e-02
  4.07710135e-01 -2.60390818e-01  6.59756064e-01  3.47634137e-01
 -1.09461397e-01 -2.15593092e-02  4.96201813e-01  2.25308597e-01
  1.69516101e-01  2.14186147e-01 -1.24946952e-01 -3.18408571e-02
 -1.63157895e-01 -2.08518252e-01 -2.27413654e-01  2.41200000e-01
 -1.14453711e-01 -1.10316195e-01  5.05284071e-02 -1.00859612e-01
  8.70356634e-02  4.40429337e-03  2.87804067e-01  1.70784146e-01
  4.07010168e-02 -2.03145370e-01 -1.13297097e-01 -1.59627303e-01
 -1.40045613e-01 -3.07387300e-02  1.13613211e-01  6.02418005e-01
 -7.76450802e-03 -1.17162384e-01 -2.58921027e-01  1.71182245e-01
  5.16957998e-01  1.27887547e-01 -2.68862862e-02 -1.31541230e-02
 -9.69673507e-04  7.15805031e-03  4.71593440e-02 -8.33086595e-02
 -1.23373196e-01  1.85259789e-01 -9.96554568e-02  1.09415993e-01
  7.64380582e-03  1.31158113e-01 -2.66509295e-01  5.90443015e-02
 -2.87038803e-01  2.30761468e-01 -1.10231444e-01 -4.91186202e-01
  2.28242218e-01  1.10477291e-01 -2.90196121e-01  2.69559801e-01
 -7.78626353e-02 -1.13358751e-01 -8.92800931e-03  9.09914915e-03
 -2.09495306e-01  5.47228396e-01 -1.07111931e-01  1.26345932e-01
  4.72832739e-01  5.65645285e-04  1.33214802e-01 -4.60193545e-01
  9.02913287e-02  5.44793785e-01 -2.09493592e-01 -6.62142560e-02
  2.40201145e-01  3.53678539e-02 -3.76315236e-01 -8.36066306e-02
 -2.38797009e-01  4.12192345e-01 -2.48876348e-01 -1.40287042e-01
 -8.79096612e-02 -1.55289825e-02  1.12856999e-01 -9.59735364e-02
  2.30766147e-01 -3.56741428e-01 -9.11205858e-02  1.46635324e-01
  3.02170128e-01  2.24754233e-02  1.94550142e-01  3.35819781e-01
  9.67936963e-02 -5.69574013e-02  2.13699728e-01  2.52722561e-01
 -5.25247037e-01 -2.38295496e-01 -7.58012235e-01 -1.14347421e-01
  4.24762368e-01  2.31297284e-01  7.57638365e-02 -3.80738191e-02
  2.22975850e-01  1.35542497e-01  2.04703975e-02  8.15499425e-02
 -5.45818284e-02 -6.57083541e-02 -1.37530863e-01  4.68634628e-02
  1.44352466e-01 -1.67607248e-01 -2.39976734e-01 -5.68853498e-01
 -4.36132550e-01  6.91265613e-02 -6.16867021e-02 -3.49013746e-01
  1.41936481e-01  1.21933527e-01 -4.17871743e-01  1.66483939e-01
 -7.90709704e-02  7.16125816e-02 -6.50291517e-02  2.27906793e-01
  2.38938332e-01 -1.44967526e-01  1.59444809e-01 -2.91232139e-01
 -3.80545735e-01  1.83622241e-01 -5.80902815e-01 -1.42408516e-02
  9.10993200e-03  1.41728938e-01 -5.04959114e-02 -6.77141696e-02
  3.59420896e-01  2.49933243e-01  2.02611953e-01 -1.75005361e-01
 -3.63353252e-01 -2.00766906e-01 -1.01083234e-01  4.79637273e-03
 -3.36898208e-01 -1.49326194e-02  8.74451734e-03 -1.27774253e-01
  3.93594563e-01  2.95633405e-01  4.75188009e-02 -1.42627293e-02
 -3.17148805e-01  2.21498489e-01 -3.28096271e-01 -1.02238640e-01
  1.03734404e-01  8.88573080e-02  3.84541839e-01  2.15116933e-01
  2.53059596e-01  1.36878476e-01  2.03920498e-01 -2.12608770e-01
  1.78367257e-01  2.76435345e-01  1.53103590e-01  3.60326946e-01
  7.28234500e-02  3.03436160e-01 -2.63265163e-01  3.24239880e-01
 -1.23245157e-01 -9.95485112e-02 -7.74649158e-03 -2.00700760e-01
  7.82548308e-01 -3.31871927e-01  1.45734072e-01 -3.39324206e-01
  2.33466759e-01 -2.18632549e-01 -2.69361317e-01  1.11574009e-02
  1.89832836e-01  1.49391934e-01 -3.93598318e-01  3.76067460e-02
  9.75665003e-02 -3.16371232e-01 -1.70208216e-01 -4.77776080e-01
 -2.65793711e-01  1.92286558e-02 -3.38951766e-01  2.45558619e-02
 -5.68576679e-02  1.14303425e-01 -2.17130005e-01  5.77607565e-02
  1.56825989e-01 -2.51936853e-01  5.56014515e-02  4.05595228e-02
 -2.28635401e-01 -2.03296542e-04  4.61050034e-01 -2.40677267e-01
 -1.28806561e-01  1.34756088e-01  3.93368959e-01  9.19899344e-02
  2.04678312e-01 -5.47508717e-01  1.21719934e-01 -3.20834629e-02
 -1.23261914e-01  2.81051636e-01 -3.98664996e-02  2.35467777e-01
 -1.58852950e-01  6.86308503e-01  1.87833816e-01 -1.55466646e-02
  1.61954939e-01 -2.43021190e-01 -3.80217463e-01 -7.61204213e-03
  1.78724647e-01 -8.99447426e-02 -1.59534320e-01 -2.00086668e-01
  9.08287168e-02  1.02972135e-01 -5.39575666e-02 -8.39319266e-03
 -1.93600044e-01 -6.56903982e-02 -7.79215023e-02 -1.34302899e-01
 -2.44613588e-01  1.27640620e-01 -1.22777149e-01 -2.58876711e-01
 -1.21946968e-01 -1.46371692e-01 -1.66816100e-01 -1.41798377e-01
 -4.02066335e-02 -2.17358112e-01  4.19053167e-01  3.92380834e-01
 -6.00780137e-02  3.43764871e-01  3.47305201e-02  1.51328310e-01
 -1.71608105e-01  1.27575845e-01 -9.93078575e-04  2.87483752e-01
  6.17062785e-02 -1.18303224e-01  2.89376944e-01  1.02419123e-01
 -4.31731045e-01  2.09129602e-01 -2.92532355e-01  9.04899985e-02
  2.75902092e-01 -9.98127684e-02 -2.37234205e-01 -9.19016004e-02
  3.05874020e-01  2.34185100e-01 -6.77848235e-02  2.22134069e-02
 -3.13681155e-01  1.62416935e-01  5.05473852e-01 -4.92277384e-01
 -2.69823790e-01  1.82432055e-01  3.90180349e-01 -2.37948880e-01
 -1.29831702e-01 -1.48277834e-01  2.18955696e-01  5.41564040e-02]"
Different Behavior of tf.raw_ops.SqrtGrad with jit_compile=True type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

When the **tf.raw_ops.SqrtGrad** operation is invoked within a tf.function with JIT compilation enabled (**jit_compile=True**), it produces different results compared to the same operation called without JIT compilation. This inconsistency is observed when the code is executed on a **CPU** device.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import traceback

class Network(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
      real_part = tf.random.normal([], dtype=tf.float64)
      imag_part = tf.random.normal([], dtype=tf.float64)
      tensor = tf.complex(real_part, imag_part)
      tensor = tf.cast(tensor,dtype=tf.complex128)
      x = tf.raw_ops.SqrtGrad(dy=x, y=tensor)        
      return x

m = Network()
real_part = tf.random.normal([], dtype=tf.float64)
imag_part = tf.random.normal([], dtype=tf.float64)
tensor = tf.complex(real_part, imag_part)
tensor = tf.cast(tensor,dtype=tf.complex128)
inp = {
    ""x"": tensor,
}

with tf.device('/CPU:0'):
    tf.config.run_functions_eagerly(True)
    no_op_res = m(**inp)
    tf.config.run_functions_eagerly(False)
    with tf.device('/CPU:0'):
        op_res = m(**inp)

    tf.debugging.assert_near(no_op_res, op_res, atol=0.001, rtol=0.001)
```


### Relevant log output

```shell
File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 33, in <module>
    tf.debugging.assert_near(no_op_res, op_res, atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=() dtype=complex128) = '
(-0.20583126869122956-0.1606528338452279j)
b'y (shape=() dtype=complex128) = '
(0.2721269549260611+0.24474350338228776j)
```
",True,"[-6.78912044e-01 -5.07815659e-01 -2.18402725e-02 -5.15635870e-02
  3.79824430e-01 -5.69829226e-01 -1.15187258e-01  4.03404608e-02
 -3.81285548e-01 -4.24406528e-01  8.28333199e-02 -1.00674927e-02
 -2.21875235e-01  1.26888260e-01 -8.77137631e-02  3.80164027e-01
 -2.64665246e-01 -2.12046549e-01  2.00441480e-01  1.37762845e-01
 -4.16886091e-01 -3.60402524e-01 -2.29769588e-01  1.32623434e-01
  2.01950669e-01  3.46481860e-01 -2.80197293e-01  2.08297849e-01
  6.50946200e-02  2.18326867e-01  3.60325307e-01  3.07108998e-01
 -8.15967023e-02  1.53950021e-01 -2.14048643e-02  1.38318524e-01
 -2.62851059e-01 -2.82187045e-01 -3.41843575e-01  6.42950088e-02
 -1.63908854e-01  1.73321903e-01  1.59274295e-01 -3.21401834e-01
  8.16001371e-02 -1.87995791e-01  3.82550396e-02 -1.20684154e-01
 -1.59773499e-01 -4.45612311e-01  3.23934183e-02  1.22398436e-01
 -4.84267175e-01 -2.57436156e-01  8.15021098e-02  6.10247999e-02
  9.82763469e-02 -1.39959037e-01  5.07280193e-02  3.93278122e-01
  5.75432666e-02  4.37811092e-02  1.10959671e-02 -5.39714098e-03
  1.07369900e-01 -2.15582419e-02  4.40462083e-01 -1.07104637e-01
  4.88913000e-01 -4.62886393e-01  1.97167844e-02 -1.25981227e-01
 -2.77414203e-01  2.68954873e-01  6.86308518e-02 -2.26445943e-01
  1.87204890e-02  2.29931325e-01  3.50345820e-01 -1.64078251e-01
  1.73594952e-01 -3.70181590e-01 -2.17505097e-01 -1.75190315e-01
  1.04726866e-01 -2.32287675e-01  1.88868105e-01  3.26778218e-02
  4.31099892e-01 -2.54360110e-01  4.29251462e-01  6.11480832e-01
 -9.56433266e-02  2.66657889e-01  5.40083706e-01  1.66960105e-01
  1.84163734e-01  1.73329055e-01  3.13196108e-02  3.08207944e-02
 -1.14866123e-01 -3.88769388e-01 -1.07300252e-01  4.80299518e-02
 -1.32034302e-01 -5.53224050e-02  2.17570335e-01  1.11389518e-01
  2.50508636e-01 -3.39992493e-02  1.68986663e-01  3.20192337e-01
  3.08680385e-02 -4.69833761e-01 -1.72459304e-01 -1.48127601e-01
 -2.49668241e-01 -1.46249533e-01  1.16885111e-01  5.49750984e-01
  7.65498579e-02 -1.45379439e-01 -2.11093143e-01  2.24795118e-01
  7.33476579e-01  1.41409248e-01 -2.66349167e-01  6.95629716e-02
  1.28043920e-01 -3.82725820e-02 -2.17291247e-02  5.26537932e-03
 -6.78763986e-02  1.62353233e-01  1.30122960e-01  1.35898158e-01
 -2.67623544e-01  1.17836118e-01 -1.97001293e-01 -3.14762592e-01
 -6.17258310e-01  1.49341971e-01  1.38614744e-01 -6.18108451e-01
  3.77262458e-02  9.22899693e-02 -2.53616601e-01  3.96322370e-01
 -2.08587423e-01 -3.20210494e-02 -1.35120183e-01  9.86804515e-02
  8.01827759e-04  5.35764337e-01  1.48665577e-01  1.29678091e-02
  4.98932987e-01 -2.51861252e-02  1.41290992e-01 -4.41570789e-01
 -1.64804216e-02  3.97672474e-01 -1.13851696e-01 -2.75119722e-01
  1.72241896e-01  1.37956515e-01 -5.54338634e-01 -7.21476823e-02
 -1.04318634e-01  4.81956065e-01 -2.15066239e-01 -2.43205577e-01
  2.12470099e-01 -3.61977331e-02  1.81670010e-01  1.88444909e-02
  4.21339631e-01 -3.61551702e-01 -1.35537222e-01  3.99758041e-01
  2.09998816e-01  2.04485297e-01  1.62612647e-01  2.22973198e-01
  2.39924699e-01  3.45161036e-02  1.91589445e-01  3.79513919e-01
 -5.29644847e-01 -2.41459146e-01 -9.05866027e-01 -6.99436292e-03
  4.78316784e-01 -1.73660535e-02  7.90786892e-02  1.30411774e-01
  1.62426814e-01 -4.88021038e-02 -7.20505938e-02  6.74300045e-02
 -3.33623976e-01 -9.19789001e-02 -2.21270341e-02 -5.83627075e-02
  5.44303544e-02 -4.53926586e-02  4.49062418e-03 -3.92833531e-01
 -4.15461719e-01  1.78850830e-01  3.90191227e-02 -4.00673926e-01
  1.39230236e-01 -1.26926884e-01 -4.39553708e-01  3.11882287e-01
  2.00140983e-01  2.14709248e-03 -8.89894217e-02  2.42953122e-01
  3.20492595e-01 -1.93388700e-01  1.75320834e-01 -4.24831003e-01
 -3.14555883e-01  3.79858136e-01 -5.34020305e-01  3.19398940e-01
 -1.58396155e-01  2.78709531e-02 -8.88312310e-02 -5.17788865e-02
  5.55758417e-01  1.49964929e-01  3.80863667e-01 -6.12402987e-03
 -2.12819844e-01 -2.14644074e-01 -2.31680229e-01  1.02317497e-01
 -4.79039848e-01 -3.37578177e-01 -2.68258136e-02 -1.79778039e-01
  4.84217882e-01  3.70316088e-01 -1.08226240e-01 -2.79097855e-01
 -2.94799685e-01  3.32939208e-01 -3.19958866e-01  2.56639957e-01
  3.06697965e-01  2.26665974e-01  6.35782480e-01  1.27740502e-01
  1.92943007e-01  6.71906173e-02  2.63028920e-01 -2.34767988e-01
  1.28145874e-01  2.68629998e-01  1.56500906e-01  4.86997128e-01
  1.04546502e-01  1.97577000e-01 -3.58703971e-01  2.96285152e-01
 -2.34442860e-01 -2.93266922e-02  1.01271279e-01 -2.72564679e-01
  9.48392332e-01 -2.41102248e-01  1.14920408e-01  1.88457221e-02
  4.54644501e-01 -2.96143144e-01 -7.07876831e-02 -3.19441333e-02
  1.59742400e-01  2.84391999e-01 -3.55569750e-01  4.35637385e-02
  2.51805246e-01 -1.25820249e-01  4.00115922e-02 -7.49024153e-01
 -3.25818360e-01  7.93753415e-02 -4.05458659e-01  1.33694425e-01
 -5.50775267e-02 -1.27420351e-02 -1.69377029e-01 -2.40796059e-02
  2.90789008e-02 -3.38868022e-01  9.34795588e-02  3.47480923e-02
 -1.95267066e-01  1.60198092e-01  5.73184848e-01 -2.37628505e-01
 -7.86500499e-02  1.26611829e-01  4.58783925e-01  1.87182367e-01
  3.21594983e-01 -3.83075416e-01  3.20200212e-02 -1.27494231e-01
 -4.68954369e-02  3.15435261e-01 -1.02656893e-01  2.57119536e-01
 -2.89323747e-01  5.51466048e-01  7.24646822e-02 -1.08790472e-01
  1.61288798e-01 -1.02565430e-01 -5.28221786e-01 -6.91395700e-02
  3.36382926e-01 -1.11400492e-01 -1.59233123e-01 -2.92715579e-01
  2.27336943e-01  2.70723373e-01 -7.64201134e-02  2.25563526e-01
 -2.41111904e-01 -8.11066292e-03 -1.33741170e-01 -2.03953564e-01
 -3.09871078e-01  1.18918963e-01 -1.27822459e-01 -3.07073653e-01
 -2.68242031e-01 -9.53492895e-02 -2.03003109e-01 -6.15563802e-02
 -1.38093159e-01 -2.71911144e-01  3.03721368e-01  5.81375837e-01
  5.87269999e-02  3.14949274e-01 -1.02691144e-01  1.47099078e-01
 -4.93792295e-01  9.41030085e-02  7.27309287e-03  3.43883574e-01
  1.62416101e-02 -1.64459497e-01  4.83922124e-01  2.19179332e-01
 -2.65021175e-01  1.23182952e-01 -2.55464226e-01  9.00925323e-02
  1.10505104e-01 -2.18327880e-01 -2.60605752e-01 -2.44732708e-01
  2.31676012e-01  2.45800823e-01  4.84985784e-02  1.69132397e-01
 -6.19916439e-01  2.51699120e-01  5.75553298e-01 -4.35529053e-01
 -1.28659382e-01  8.69818255e-02  2.76150435e-01 -2.09855035e-01
 -2.06568256e-01 -1.65937930e-01  3.21554035e-01  1.88493401e-01]"
Different Behavior of tf.raw_ops.RightShift with jit_compile=True stat:awaiting response type:bug stale comp:ops TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GPU 0: NVIDIA GeForce RTX 2070 GPU 1: NVIDIA GeForce RTX 2070 GPU 2: NVIDIA GeForce RTX 2070 GPU 3: NVIDIA GeForce RTX 2070

### Current behavior?

When the tf.raw_ops.RightShift operation is invoked within a tf.function with JIT compilation enabled (jit_compile=True), it produces different results compared to the same operation called without JIT compilation. This inconsistency is observed when the code is executed on a GPU device.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import traceback

class Network(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(jit_compile=True)
    def __call__(self, x):
      random_tensor = tf.random.uniform([],minval=0,maxval=255,dtype=tf.int32)
      int8_tensor = tf.dtypes.cast(random_tensor, tf.int8)
      x = tf.raw_ops.RightShift(y=x, x=int8_tensor)        
      return x


m = Network()
random_tensor = tf.random.uniform([4,1],minval=0,maxval=255,dtype=tf.int32)
int8_tensor = tf.dtypes.cast(random_tensor, tf.int8)
inp = {
    ""x"": int8_tensor,
}

with tf.device('/GPU:0'):
    tf.config.run_functions_eagerly(True)
    no_op_res = m(**inp)
    tf.config.run_functions_eagerly(False)
    with tf.device('/GPU:0'):
        op_res = m(**inp)

    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
```


### Relevant log output

```shell
File ""/home/guihuan/LLM/results/tf-2/2023-10-22-20-21/test.py"", line 30, in <module>
    tf.debugging.assert_near(tf.cast(no_op_res, tf.float64), tf.cast(op_res, tf.float64), atol=0.001, rtol=0.001)
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/guihuan/.conda/envs/night/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_assert.py"", line 102, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b''
b'x and y not equal to tolerance rtol = tf.Tensor(0.001, shape=(), dtype=float64), atol = tf.Tensor(0.001, shape=(), dtype=float64)'
b'x (shape=(4, 1) dtype=float64) = '
-24.0, -1.0, -1.0, ...
b'y (shape=(4, 1) dtype=float64) = '
9.0, 0.0, 0.0, ...
```
",True,"[-7.12279916e-01 -3.98743838e-01 -1.39311850e-01 -5.69524653e-02
  3.32714140e-01 -4.60111052e-01 -6.17518350e-02  2.04741374e-01
 -2.42365420e-01 -4.31068420e-01  2.99438424e-02  9.30894166e-02
 -2.91249864e-02 -9.57716182e-02 -1.86709851e-01  2.66758621e-01
 -9.87591967e-02 -2.68644929e-01  9.77142006e-02  1.60812721e-01
 -3.79473567e-01 -2.81448305e-01 -3.22487533e-01  7.89552331e-02
  1.24072388e-01  3.99810791e-01 -3.08144718e-01  4.22505319e-01
  2.09259037e-02  3.16830039e-01  2.87981749e-01  2.39149228e-01
 -1.17130950e-01  2.66946852e-01  2.85835704e-03  2.03288257e-01
 -3.31598550e-01 -2.31465071e-01 -2.69560963e-01 -1.78060122e-02
 -6.75184876e-02  1.05467699e-01  1.99002877e-01 -2.00219214e-01
 -8.13245550e-02  8.16543400e-02 -5.87266870e-02 -8.50070640e-02
 -1.01773530e-01 -3.58051777e-01 -1.69714354e-02  5.17926931e-01
 -3.43941599e-01 -1.95555121e-01  1.26615256e-01  9.08657759e-02
  1.10927559e-01 -8.09578374e-02 -1.30811617e-01  2.75191844e-01
  1.17724285e-01  6.52852505e-02  1.55958012e-02  4.18244824e-02
  2.42059842e-01  6.85714185e-02  3.68607700e-01 -2.44204979e-02
  3.79031688e-01 -3.15049827e-01  1.38156265e-01 -3.80893499e-02
 -4.33669955e-01  2.03123406e-01  2.52995118e-02 -2.94135753e-02
 -2.32462347e-01  5.39762229e-02  3.42063308e-01 -2.91720927e-01
  1.67637572e-01 -3.52247119e-01 -9.11281258e-02 -8.01894367e-02
  9.36522335e-02 -1.83309853e-01  2.45452374e-01 -2.19368264e-02
  5.00500619e-01 -2.22302705e-01  5.00266492e-01  5.02745152e-01
 -1.75569355e-02  1.63907364e-01  5.91218054e-01  1.61490321e-01
  1.06839173e-01  2.16254890e-01 -2.87132710e-01  5.06546013e-02
 -7.94365704e-02 -1.77709132e-01 -2.65472114e-01  3.90035398e-02
 -1.39010608e-01 -1.41504079e-01  1.15654975e-01  2.82022692e-02
  2.14949269e-02  1.06322527e-01  1.96765900e-01  2.60722458e-01
 -1.70457996e-02 -2.07613319e-01 -1.23712651e-01 -7.11425543e-02
 -1.87709808e-01 -6.65668994e-02  3.90853509e-02  3.71635616e-01
  1.41908020e-01 -8.95148739e-02 -3.95236701e-01  2.11764634e-01
  5.60333133e-01  1.00187235e-01 -2.62147244e-02  1.13043055e-01
  8.87748078e-02 -1.23332433e-01  6.15584478e-02 -5.54784127e-02
 -7.70346969e-02  1.11407936e-01  4.21464816e-02  5.60973622e-02
 -5.58416620e-02  2.11816579e-01 -1.83053523e-01 -7.45388120e-02
 -3.73661071e-01  1.59481078e-01  1.39029428e-01 -4.48113322e-01
  7.73544684e-02  1.32889852e-01 -8.50208327e-02  3.74136180e-01
 -8.13872889e-02 -8.22327137e-02 -3.62902880e-02  8.00651982e-02
 -2.60528445e-01  4.98621881e-01  1.68801546e-02  8.83562565e-02
  4.20836747e-01  5.95496744e-02  1.00981884e-01 -4.02129799e-01
  4.10600901e-02  3.79883558e-01 -3.31977904e-02 -2.35374540e-01
  2.62088060e-01  5.26251346e-02 -4.16870177e-01 -1.18054911e-01
 -1.48970529e-01  4.26205635e-01 -3.40718865e-01 -3.12640131e-01
 -6.79986775e-02 -1.49818540e-01  1.56694904e-01 -9.73685235e-02
  2.05218658e-01 -1.12821802e-01 -1.57878667e-01  2.42048353e-01
  3.22011173e-01  1.86952069e-01  4.95423302e-02  3.13943684e-01
  1.52063817e-01 -6.98610395e-02  1.00662105e-01  3.47903311e-01
 -6.14467621e-01 -3.14477742e-01 -8.58375371e-01 -4.79638055e-02
  5.19751370e-01  1.24067008e-01  5.61020598e-02 -1.07107118e-01
  2.53923625e-01  9.17374045e-02  2.06649415e-02  7.45740533e-02
 -1.62623167e-01  2.29823813e-02 -6.21224083e-02 -1.57169819e-01
  1.67074353e-01  5.05191833e-02 -1.79436520e-01 -4.48625147e-01
 -1.69499159e-01  2.34860647e-02 -8.88063908e-02 -2.15979397e-01
  1.52507991e-01 -1.21459901e-01 -3.47631782e-01  2.63014466e-01
 -2.16567203e-01  9.29400884e-03  5.27136400e-03  1.74737543e-01
  3.19547206e-01 -2.10625038e-01  3.12222298e-02 -3.69721323e-01
 -3.69579434e-01  2.80360729e-01 -5.72404802e-01  2.88765013e-01
  4.75658625e-02  8.96630250e-03  5.05000316e-02  2.93858238e-02
  4.35674012e-01  3.01098347e-01  3.35685968e-01 -6.77928180e-02
 -1.84537679e-01 -2.47028545e-01 -1.05483569e-02  3.29062305e-02
 -4.23457086e-01 -2.49730930e-01  1.91587061e-02 -1.20612003e-01
  5.35582066e-01  3.68909061e-01 -2.86463611e-02 -5.44977263e-02
 -1.29843771e-01  2.08403766e-01 -2.85283178e-01  5.43039627e-02
  1.61069751e-01  2.17049979e-02  5.28196931e-01  7.51048028e-02
  1.58178285e-01  9.82081443e-02  1.89143330e-01 -3.80284786e-01
  1.75577432e-01  2.95632124e-01  2.60738730e-01  2.98704207e-01
  1.49783015e-01  2.76932895e-01 -2.38216385e-01  3.63070667e-01
 -1.31661028e-01 -4.13643420e-02  2.68820710e-02 -2.39050373e-01
  7.64099717e-01 -1.99954748e-01  1.53410956e-01 -1.53428555e-01
  3.36512506e-01 -2.52057076e-01 -2.00684562e-01  8.18111151e-02
  1.09311685e-01  2.37077445e-01 -3.73765945e-01  1.66934490e-01
  3.97935510e-01 -2.56103873e-01 -1.57493263e-01 -4.33915079e-01
 -3.04276526e-01 -6.25403523e-02 -3.28651160e-01 -2.42611393e-03
 -5.07488251e-02  6.26354888e-02 -2.57888198e-01 -4.22515832e-02
  1.34642437e-01 -2.87658989e-01  1.21007152e-01  4.29278985e-02
 -2.98139751e-01  9.61264074e-02  5.11144876e-01 -1.05479576e-01
 -2.60286629e-01  8.20621401e-02  3.10786843e-01  8.03704709e-02
  2.95688748e-01 -3.97395194e-01  7.87623525e-02  2.54789256e-02
 -4.93879057e-02  2.40083799e-01 -1.41890764e-01  1.56496108e-01
 -3.77818525e-01  6.78010583e-01  7.10298494e-02 -2.41956785e-02
  2.32062906e-01 -1.82956785e-01 -4.09593642e-01  8.95177275e-02
  2.00980589e-01 -6.27595335e-02 -7.18402192e-02 -2.04637930e-01
  1.62253529e-01  2.24065453e-01  8.35854039e-02  6.49058074e-02
 -2.95818001e-01  2.12827697e-04 -9.09130126e-02 -1.83933541e-01
 -1.86473340e-01  1.08973816e-01 -1.27068996e-01 -3.52312028e-01
 -1.01944238e-01 -3.27799857e-01 -1.76174968e-01 -1.02791846e-01
 -1.49334759e-01 -1.61943793e-01  2.99827933e-01  3.21811199e-01
  5.57572879e-02  3.55308205e-01  1.67212798e-03  4.49067280e-02
 -1.81864768e-01  1.28202677e-01 -1.40005141e-01  2.73878157e-01
 -7.54531845e-03 -1.59964055e-01  3.58708560e-01  7.46701658e-02
 -3.52462262e-01  2.54738301e-01 -3.63094389e-01  1.47509187e-01
 -1.68293342e-02 -1.10238895e-01 -8.60827491e-02 -1.98383272e-01
  1.19938731e-01  9.49745253e-02 -5.99827394e-02  9.57600772e-02
 -4.17425513e-01  4.16357592e-02  5.12189150e-01 -4.56020534e-01
 -1.20261773e-01  8.01881030e-02  1.63544670e-01 -1.87746778e-01
 -1.41195446e-01 -9.70790833e-02  8.83198678e-02  1.66904360e-01]"
TensorflowLite C API not linking with TensorflowLite_Flex delegate (Automatically or manually) stat:awaiting response type:bug stale comp:lite TF2.14,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution**: Windows 11
-   **Mobile device**: Additionally happens on Android 14 Pixel 7a
-   **TensorFlowLite installed from (source or binary)**: C API build with CMake and Tensorflowlite_flex built with bazel
-   **TensorFlow version (use command below)**: 2.14
-   **Bazel version (if compiling from source)**: Bazelisk version: v1.18.0
-   **GCC/Compiler version (if compiling from source)**: Visual Studio 17.7.4
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**: N/A

Using Cmake To build and link the TensorflowLite C API Works perfectly with any tflite model signature not using Flex operators. However, after linking the C API with tensorflowlite_flex, using the following as part of a CMakeLists.txt:
```cmake
if(${CMAKE_SYSTEM_NAME} STREQUAL ""Windows"")
  set_target_properties(tensorflowlite_c PROPERTIES 
    IMPORTED_LOCATION_DEBUG ""${TFLITE_ROOT}/lib/windows/Debug/tensorflowlite_c.dll""
    IMPORTED_IMPLIB_DEBUG ""${TFLITE_ROOT}/lib/windows/Debug/tensorflowlite_c.lib""

    IMPORTED_LOCATION_RELEASE ""${TFLITE_ROOT}/lib/windows/Release/tensorflowlite_c.dll""
    IMPORTED_IMPLIB_RELEASE ""${TFLITE_ROOT}/lib/windows/Release/tensorflowlite_c.lib""
    )

  set_target_properties(tensorflowlite_flex PROPERTIES 
    IMPORTED_LOCATION ""${TFLITE_ROOT}/lib/windows/tensorflowlite_flex.dll""
    IMPORTED_IMPLIB ""${TFLITE_ROOT}/lib/windows/tensorflowlite_flex.dll.if.lib""
    )
  target_link_libraries(tensorflowlite_c INTERFACE tensorflowlite_flex)
endif()
```  
, the running process outputs: ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.

My final approach was to use (what I guessed) was the auto linking code that exists in the C++ code (namely:
```cpp
auto hdll = LoadLibrary(""tensorflowlite_flex.dll"");

auto flex_fp = reinterpret_cast<TfLiteDelegatePtr(*)()>(
    GetProcAddress(hdll, ""TF_AcquireFlexDelegate""));
if (flex_fp == NULL) {
  throw std::runtime_error(""flex_fp couldn't be run"");
}

flex_ = flex_fp(); // flex_ is a member (not shown here) // This line gets the message: INFO: Created TensorFlow Lite delegate for select TF ops.

TfLiteInterpreterOptionsAddDelegate(options_.get(), flex_.get()); // *THIS* is the important line

```
). If the final line is commented out we get the nice INFO message that the delegate has been created but end up with the same warning message. Alternatively, if the last line is included we get an invalid memory access exception. 

I am going to switch to the C++ API for tensorflow lite and see if that works as I am under a tight deadline but any help would be much appreciated as the stable ABI for the C API is far better for my PhD.


Additionally, I built ""tensorflow/lite/delegates/flex:delegate"" with bazelisk (bazel v1.18.0) but (after waiting an extremely long time for it to compile) I cannot track down what it built and did not change anything.


(Note, some small changes to CMakeFiles were required to build tensorflowlite_c.dll and the exact binary I have used can be obtained from [here](https://github.com/SagaraBattousai/tflite-clib-builder/releases/download/v1.0.1/tensorflowlite_c-winx64-Android.zip) )



",True,"[-0.21008557 -0.5428432  -0.4013495  -0.0141195   0.24905509 -0.15450482
  0.00629719 -0.03493787 -0.3795148  -0.12496872  0.08680216 -0.2392632
 -0.11212572  0.2900839   0.12705311  0.37387556 -0.26173592 -0.15016617
  0.12014513  0.00487131 -0.02368772 -0.13023135 -0.19191204  0.30726635
  0.174167    0.34302908 -0.25616372 -0.24300651  0.30483025  0.12694246
  0.4323545  -0.10315751 -0.27460554 -0.02752794 -0.15419711  0.37754494
 -0.42899016 -0.04891822 -0.23996519 -0.13905218  0.03297859  0.26686734
  0.06163636  0.12359575 -0.11793105  0.16213696  0.14188704  0.25965554
 -0.15829998 -0.07943601 -0.07016541 -0.11904417 -0.45122635 -0.26922998
 -0.11321523  0.21056959  0.04962826 -0.15519738  0.10032082  0.17452973
  0.06782435 -0.07450266 -0.12281021  0.06472321 -0.16936485  0.20888916
  0.38932955 -0.2175178   0.3735243  -0.16505043  0.1514349  -0.16102007
 -0.16289905  0.04144826  0.19448769  0.18240264 -0.1112808   0.32484436
  0.37838817 -0.06309809 -0.06481114 -0.01235496  0.19980171  0.1280482
  0.38370517 -0.15313339  0.30259773  0.15411603  0.27383178  0.02019245
  0.42419612  0.3648315   0.00610604  0.00219348  0.3667913   0.27171272
  0.00603083  0.5643885   0.01790501 -0.19713064 -0.19612262 -0.11587939
 -0.12219055  0.18722469  0.01143158 -0.1715819   0.01586798 -0.01709015
  0.16529836 -0.09294702  0.10546477 -0.11943401  0.08954565 -0.01602692
 -0.00738736  0.10958659 -0.12471591  0.12973878 -0.02779624  0.6432407
 -0.06665777 -0.10024794  0.0767962   0.22376037  0.34801847 -0.0106823
 -0.28627488 -0.10291719  0.24689901  0.10182394  0.16456425  0.33917654
 -0.2739011  -0.09042866 -0.17737883 -0.08920744 -0.40929842 -0.18601689
 -0.191827   -0.26252687 -0.03125678  0.24808739 -0.02150503 -0.33434486
  0.2519316   0.00213156 -0.02911792  0.17447159 -0.16405888 -0.03602551
 -0.10658386  0.18640834 -0.10437456  0.3398229   0.3505996   0.3244661
  0.25265497  0.03493398  0.03333379 -0.7071005  -0.00133493  0.31740195
 -0.0177752  -0.12047434  0.01615419 -0.02579501 -0.45742342 -0.19108495
 -0.04010244  0.23966016 -0.24278425  0.00728624 -0.12616271  0.14472765
  0.2983749   0.06065668  0.3984676  -0.6005368  -0.02486538  0.05980647
  0.08673123  0.12513271 -0.01015645 -0.1045612  -0.35252815  0.09770818
 -0.04400573 -0.09639867 -0.12314512  0.0275516  -0.47411978  0.11453868
  0.18857391  0.0122884  -0.11347909 -0.11493439  0.32489794 -0.19111171
  0.02872631  0.2818376   0.0785504   0.0029745  -0.17011684  0.04781866
  0.21378842 -0.25612944 -0.24063072 -0.15993798 -0.3241392   0.03580831
  0.3162127  -0.44530767 -0.01882698 -0.17734689 -0.26632613 -0.10118142
 -0.19555089 -0.09594664 -0.34065354  0.15927425  0.0826266  -0.06362934
 -0.10877542 -0.34434626 -0.31687778 -0.19483407  0.07260322  0.03284269
  0.05578054  0.42535537  0.02333645 -0.0756993   0.3077765   0.02043226
  0.19128755 -0.37142932 -0.05311459 -0.20813766 -0.0372206  -0.06057583
 -0.43188184 -0.22701344 -0.05758387 -0.10014966  0.17171514  0.12702852
  0.07855431  0.02347618 -0.10572907  0.36265984 -0.22758262  0.07359383
  0.5432712   0.27008405  0.2707632   0.13528629 -0.10773315 -0.15797058
  0.15367314 -0.05666953  0.09899597  0.22808427  0.02412199  0.5657342
  0.22121638  0.19239104 -0.2947622   0.24873605  0.11548055 -0.27928513
  0.0013096  -0.21456844  0.08841853 -0.3440648  -0.01072463  0.05966782
  0.46360677  0.12960447 -0.11831083  0.18779251  0.19683024  0.2847619
 -0.15650661  0.07295926  0.03902748 -0.44494247  0.00968896 -0.26859325
 -0.06583757  0.24621384 -0.10228372 -0.09283434 -0.06640427  0.1521392
 -0.21522847  0.10149972  0.02225504 -0.06836401  0.13281798  0.3485605
  0.04877415  0.14649436  0.2818315   0.00121356 -0.0200368  -0.11700787
  0.5295241  -0.08053721  0.5183065  -0.38514468  0.16933852  0.09372453
 -0.03741339  0.5433215  -0.28538972 -0.09585606 -0.17927717  0.5583048
  0.28534272 -0.03078594  0.14053781 -0.14017317 -0.13570105  0.0144316
  0.14687696  0.23134883 -0.14381734 -0.2820392  -0.25217754  0.12694514
 -0.06941986 -0.31210327 -0.03080986  0.13457559 -0.2305515  -0.3114558
 -0.44174048  0.36784646 -0.16502437 -0.24644028 -0.05709144  0.04160661
  0.01724993 -0.18974584 -0.08509874 -0.41194284  0.32624793  0.62419146
 -0.10442232  0.16757077 -0.2665499   0.06536512 -0.25448066 -0.30790302
 -0.49200386  0.28756315  0.00289745 -0.31697056  0.07184158  0.7172222
 -0.14881684  0.00448156 -0.27715638 -0.275738    0.37861824 -0.26454282
 -0.1220599  -0.25194132  0.14279872  0.48162052 -0.099587    0.37862158
 -0.3993971   0.266036    0.25022888 -0.33739194 -0.16040438  0.06622962
 -0.0414191  -0.13084784 -0.23523062 -0.08384481 -0.04042388 -0.11431132]"
tensorflow new(sz=18446744073709551615) got std::bad_alloc stat:awaiting response type:bug stale TF 1.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 1.14.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.2

### Bazel version

unknown

### GCC/compiler version

gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609

### CUDA/cuDNN version

no

### GPU model and memory

_No response_

### Current behavior?

throw std::bad_malloc

### Standalone code to reproduce the issue

```shell
no std::bad_malloc, no new(sz=18446744073709551615)
```


### Relevant log output

```shell
#0  __lll_lock_wait () at ../sysdeps/unix/sysv/linux/x86_64/lowlevellock.S:135
#1  0x00007f7f6df7be42 in __GI___pthread_mutex_lock (mutex=0x7f7f6eb43970 <_rtld_global+2352>) at ../nptl/pthread_mutex_lock.c:115
#2  0x00007f7f6dceb0df in __GI___dl_iterate_phdr (callback=0x7f7f45dc4df0, data=0x7f7e28ff92a0) at dl-iteratephdr.c:41
#3  0x00007f7f45dc616e in _Unwind_Find_FDE () from /lib/x86_64-linux-gnu/libgcc_s.so.1
#4  0x00007f7f45dc2b63 in ?? () from /lib/x86_64-linux-gnu/libgcc_s.so.1
#5  0x00007f7f45dc3d80 in ?? () from /lib/x86_64-linux-gnu/libgcc_s.so.1
#6  0x00007f7f45dc422e in _Unwind_RaiseException () from /lib/x86_64-linux-gnu/libgcc_s.so.1
#7  0x00007f7f4605833c in __cxxabiv1::__cxa_throw (obj=0x7f7e258670a0, tinfo=0x7f7f4633c770 <typeinfo for std::bad_alloc>, dest=0x7f7f46056550 <std::bad_alloc::~bad_alloc()>)
    at ../../../../libstdc++-v3/libsupc++/eh_throw.cc:82
#8  0x00007f7f4605886c in operator new (sz=18446744073709551615) at ../../../../libstdc++-v3/libsupc++/new_op.cc:54
#9  0x00007f7f5170f2b9 in Eigen::internal::aligned_malloc(unsigned long) () from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007f7f526fa18e in void* Eigen::internal::TensorContractionBlockMemAllocator<float, float>::allocateSlices<Eigen::ThreadPoolDevice const>(Eigen::ThreadPoolDevice const&, long, long, long, long, long, long, std::vector<float*, std::allocator<float*> >*, std::vector<float*, std::allocator<float*> >*) () from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007f7f52911ec5 in void* Eigen::internal::TensorContractionKernel<float, float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer> >::allocateSlices<Eigen::ThreadPoolDevice const>(Eigen::ThreadPoolDevice const&, int, int, int, std::vector<Eigen::internal::ColMajorBlock<float, long>, std::allocator<Eigen::internal::ColMajorBlock<float, long> > >*, std::vector<Eigen::internal::ColMajorBlock<float, long>, std::allocator<Eigen::internal::ColMajorBlock<float, long> > >*) () from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007f7f5293cf30 in void Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::evalProduct<0>(float*) const ()
   from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007f7f5293f1cb in Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const> const, Eigen::ThreadPoolDevice, true, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<float const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const> const&, Eigen::ThreadPoolDevice const&) () from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007f7f529a2c7e in tensorflow::MatMulOp<Eigen::ThreadPoolDevice, float, false>::Compute(tensorflow::OpKernelContext*) ()
   from /usr/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007f7f4dee0146 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long) ()
   from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1
#16 0x00007f7f4ded05a5 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) ()
   from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1
#17 0x00007f7f4df76cfe in Eigen::ThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) () from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1
#18 0x00007f7f4df73b98 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /usr/lib/python3.7/site-packages/tensorflow/python/../libtensorflow_framework.so.1
#19 0x00007f7f460828b3 in std::execute_native_thread_routine_compat (__p=<optimized out>) at ../../../../../libstdc++-v3/src/c++11/thread.cc:110
#20 0x00007f7f6df796ba in start_thread (arg=0x7f7e28ffb700) at pthread_create.c:333
#21 0x00007f7f6dcaf4dd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
```
",True,"[-0.5469682  -0.30474302 -0.11067703  0.137957    0.25293332 -0.4169862
 -0.01603694  0.07050748 -0.22360897 -0.498233    0.07294489 -0.03620243
 -0.20176992  0.07553846 -0.2636943   0.2960329  -0.23282376 -0.10999466
  0.06974384  0.10759287 -0.1398792  -0.20873967 -0.2789516   0.26144856
  0.19059302  0.21925597 -0.21356712 -0.18510064 -0.04847076  0.25666225
  0.438048    0.19836247  0.06877671  0.05506319 -0.19825098  0.17390734
 -0.3266454  -0.11577004 -0.34152788  0.05877391  0.07236449  0.13275039
  0.24829274 -0.15660366  0.06786691 -0.3247222  -0.0278226  -0.09380449
  0.02584863 -0.11638331 -0.00621594  0.01907298 -0.4703915  -0.36673048
 -0.15268603 -0.13908374  0.06820504 -0.24156952 -0.07068104  0.11410203
  0.06589951  0.11177445 -0.01209206 -0.07761869  0.33197686  0.06320532
  0.26565832 -0.03674305  0.5829432  -0.06337314  0.07637445 -0.04402277
 -0.34096867  0.20550877 -0.01149142  0.2539846  -0.04005045  0.04031903
  0.33394554 -0.04576926  0.11710402 -0.39741427  0.03685173 -0.3242899
  0.3297834  -0.05676331  0.38662553  0.11940216  0.5475714  -0.33406135
  0.57455075  0.5137377  -0.11510271  0.19062135  0.6460458  -0.00149821
  0.01690506  0.14362487 -0.09107058 -0.13965924 -0.13449174 -0.20145123
 -0.08515836  0.03648654 -0.07479192 -0.10243891  0.04940181 -0.24788718
  0.18848513 -0.01165585  0.1626159  -0.06115101  0.34115222 -0.00263852
 -0.09437627 -0.09204209 -0.11094242  0.14110592 -0.07598202  0.8453867
  0.07427672 -0.09436274 -0.08999276  0.1366115   0.2847555   0.21021777
 -0.12697528  0.01625308  0.15404081 -0.15838462  0.10203671 -0.01176178
 -0.13267745  0.3886596   0.00418391  0.01722544 -0.25256652 -0.21299946
 -0.24454589 -0.07849029 -0.24517497  0.20993453 -0.1331029  -0.56471395
  0.07793614 -0.02378556 -0.11952657  0.23144479 -0.14872198  0.21854907
 -0.09721619 -0.15749505 -0.12717736  0.42793941  0.11629922  0.18567881
  0.2486954  -0.0721521  -0.05327468 -0.64118254  0.04560426  0.5242518
  0.02542463 -0.23835061  0.2418942   0.17861149 -0.34788537 -0.34121254
  0.16763371  0.46569192 -0.11728168 -0.1165489  -0.03017655  0.09708396
 -0.16827047 -0.1278046   0.22658306 -0.70840347 -0.08076309  0.23970586
 -0.15243979  0.14039223 -0.02100324  0.22992328  0.11330666  0.11499381
  0.02457108  0.09244278 -0.22973385 -0.04065903 -0.13859494 -0.22619589
  0.5204577  -0.09296762 -0.02410059  0.12183365  0.13546538 -0.29744846
  0.0931167   0.04161483 -0.21883687 -0.12021933  0.01281161 -0.1344125
  0.2543406  -0.39591166 -0.05692169 -0.32876927 -0.13692     0.01001935
  0.15662819 -0.38706654 -0.01333636 -0.08323389 -0.41575652  0.2704965
  0.11425501  0.20766944 -0.15775953  0.14827444  0.30765766 -0.18425909
 -0.01014237 -0.33229923 -0.26453418  0.17824866 -0.21489602  0.24929385
  0.02273843  0.26922843  0.27174842  0.28904912  0.4042951   0.37772143
  0.4454468  -0.201186   -0.22277042 -0.11564259  0.09397635 -0.06948739
 -0.38867122 -0.10724236  0.00165737 -0.16787477  0.31028193  0.30626872
 -0.16778742 -0.27510735 -0.32761866  0.19814509 -0.42169422  0.20106578
  0.29589397  0.174506    0.54578507  0.08126765  0.21981487  0.26625568
  0.26040295 -0.16708471  0.5790509   0.06888783  0.0941827   0.32948858
  0.18933946  0.20659862 -0.29162723  0.53636265  0.22957276 -0.19072932
 -0.015447   -0.33418053  0.7451925  -0.412237    0.07362175 -0.1495713
  0.31936333  0.12888694 -0.07611392  0.19260988  0.07285801  0.30432642
 -0.2786853  -0.03095075  0.0658086  -0.0943985  -0.15518907 -0.5725279
 -0.17785776  0.03095844 -0.27937788  0.14089437 -0.1486571  -0.04318857
 -0.2175081   0.24874145  0.02325163 -0.05414743  0.19893253  0.33907962
 -0.06973657  0.0146217   0.32140207 -0.45799434 -0.31651017 -0.12898108
  0.306266    0.24780054  0.27264428 -0.47443685  0.26944017 -0.23789895
  0.03855489  0.52507234  0.02091909  0.07252897 -0.35078484  0.7017441
  0.25441545 -0.06277338  0.07431478 -0.16007161 -0.38840872  0.13097166
  0.32477403 -0.10850972  0.04937362 -0.46265528  0.14533664  0.22776866
 -0.06577017 -0.08666115 -0.21796107 -0.09884517 -0.11758743 -0.1963765
 -0.5028813  -0.00844011  0.02482999 -0.44417354 -0.22536579 -0.12328286
 -0.10100812 -0.32107937  0.0882763  -0.44560182  0.3713919   0.6800933
 -0.05872702  0.09243751  0.03228806  0.20613754 -0.30561766 -0.03599786
 -0.32169265  0.4521494   0.11860071 -0.32430798  0.39969555  0.28038692
 -0.16459006  0.07379597 -0.18874395  0.28741834  0.11520249 -0.3431442
 -0.15802813 -0.27832925 -0.04638813  0.35573643 -0.0480462   0.25410217
 -0.39794672  0.11571857  0.5178523  -0.6867202  -0.44253224  0.16937214
  0.04915486 -0.21836029 -0.10124525 -0.25552005  0.07702517  0.07776417]"
Missing `'tensorflow.python.training.tracking'` in version 2.14.0; cannot load pickled model type:bug type:support TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current behavior?

We used to load some pickled TF models using:

```
import pickle
pickle.load(open(""/content/cosmopower/cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN.pkl"", 'rb'))
```

but as of version 2.14.0 we get:

```
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
[<ipython-input-3-776b938106f2>](https://localhost:8080/#) in <cell line: 1>()
----> 1 pickle.load(open(""/content/cosmopower/cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN.pkl"", 'rb'))

ModuleNotFoundError: No module named 'tensorflow.python.training.tracking'
```

We checked that there are no errors with TF `2.13.0`, and everything works as expected. The `tensorflow.python.training.tracking` module seems to have been removed since `2.14.0`, but we are unable to find it in the release notes. This error is observed on Colab as well as on other platforms and OSs.

### Standalone code to reproduce the issue

```shell
! git clone https://github.com/alessiospuriomancini/cosmopower.git # download folder with TF model

# this is intended for colab; change path if necessary
import pickle
pickle.load(open(""/content/cosmopower/cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN.pkl"", 'rb'))
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-3-776b938106f2> in <cell line: 1>()
----> 1 pickle.load(open(""/content/cosmopower/cosmopower/trained_models/CP_paper/CMB/cmb_TT_NN.pkl"", 'rb'))

ModuleNotFoundError: No module named 'tensorflow.python.training.tracking'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
""Open Examples"" button below.
---------------------------------------------------------------------------
```
",True,"[-4.03044760e-01 -5.86063266e-01 -1.58545271e-01  1.57211125e-01
  4.22413200e-01 -3.60767961e-01 -7.74037316e-02 -1.49048448e-01
 -2.80997753e-01 -3.07352006e-01  2.07373768e-01 -9.06112194e-02
 -3.63790691e-01  1.96260363e-01 -8.05540830e-02  1.18329652e-01
 -5.36149293e-02 -4.74873856e-02  1.61845252e-01  6.90061450e-02
 -1.55558288e-01 -2.78667420e-01 -1.58342302e-01  2.22266197e-01
  9.62580442e-02  1.54711664e-01 -1.67075872e-01 -2.92494625e-01
  2.84695905e-02  1.87764689e-01  2.20874161e-01  5.12169227e-02
 -1.34790450e-01 -8.93091485e-02 -1.69371907e-02  2.99521774e-01
 -3.31363678e-01 -2.48602837e-01 -2.33187124e-01 -2.68697180e-02
 -4.47174013e-02  1.02276802e-02  5.97572550e-02 -7.70195425e-02
 -7.74319395e-02 -2.31336325e-01 -3.38734873e-02 -1.41328990e-01
 -1.80478990e-01 -1.95191294e-01  1.76207572e-02 -1.20414853e-01
 -4.49067056e-01 -4.78640437e-01 -1.91623539e-01  5.67222647e-02
  2.30823055e-01  1.64952446e-02  6.69192225e-02  1.12847552e-01
 -1.05499715e-01  1.16020113e-01 -1.53383017e-02 -1.31186247e-01
  1.21857017e-01  1.68558404e-01  3.02690715e-02 -1.19738474e-01
  6.66311741e-01 -2.77400076e-01  1.55427665e-01 -1.49526045e-01
 -3.58091772e-01  1.25015050e-01 -1.17185146e-01  4.99569215e-02
  2.60671884e-01  2.12225735e-01  2.02431932e-01 -7.92265832e-02
 -1.35269105e-01 -3.71677518e-01 -2.41552759e-02 -1.71013728e-01
  1.82768896e-01 -5.68830743e-02  3.34495217e-01  2.46566102e-01
  5.21937191e-01 -1.70196116e-01  6.70616269e-01  1.68474421e-01
 -4.72598858e-02  1.41045257e-01  4.03352380e-01  1.45307779e-01
 -3.83820035e-03  2.96442717e-01 -1.52338818e-01 -2.98469037e-01
 -5.61557040e-02 -2.97084391e-01  1.68671489e-01  2.81369090e-01
 -8.62318054e-02 -5.97268343e-02  1.59137666e-01 -2.51624346e-01
  2.38239855e-01 -1.74630851e-01  3.97165567e-02 -5.62441610e-02
  3.78359556e-01 -8.79661664e-02  6.65123463e-02 -5.78272194e-02
 -2.64788121e-01  4.66293655e-02 -7.69878775e-02  7.91568398e-01
  8.73687565e-02 -1.71107411e-01  2.42307156e-01  8.76396243e-03
  3.42408150e-01  1.42535165e-01 -1.38050839e-01  2.86715999e-02
  2.90411208e-02 -1.53100550e-01  1.69258282e-01  8.85798335e-02
 -1.10925987e-01  1.56842455e-01  8.87484290e-03  6.22449405e-02
 -1.88030019e-01 -2.92648047e-01 -6.63846284e-02 -1.94901407e-01
 -3.50383461e-01  1.29088596e-01 -1.38388336e-01 -5.61768055e-01
  1.58507288e-01  1.36217803e-01 -1.71796560e-01  3.65602076e-01
 -1.50495470e-01  2.29630675e-02 -1.32614240e-01  9.70556512e-02
  5.17626759e-04  4.36702192e-01  1.93001732e-01  2.84768671e-01
  3.74628633e-01 -6.25279844e-02 -2.18845196e-02 -6.83808386e-01
 -7.51200691e-02  5.03986716e-01 -1.11509427e-01 -2.70403743e-01
  2.05906987e-01  1.56711131e-01 -3.42857778e-01 -2.96486914e-01
  1.34647608e-01  4.07461435e-01 -1.42840415e-01 -1.23105526e-01
 -5.46552613e-03  3.54631320e-02  1.34414002e-01 -4.17456105e-02
  2.08626747e-01 -8.55087519e-01  5.73714934e-02  3.10723305e-01
  1.06778033e-01  8.08052570e-02  1.34153426e-01  2.12894127e-01
  1.78695202e-01  6.80542067e-02  9.53713208e-02 -7.48067051e-02
 -4.88856956e-02  2.31207639e-01 -1.09838411e-01 -2.69757628e-01
  5.22462964e-01 -1.13442056e-01 -7.53701776e-02  2.19460607e-01
  3.16982478e-01  3.92317623e-02 -8.19498673e-02 -2.46911235e-02
 -1.39412299e-01 -8.91485289e-02  1.11984894e-01  1.09004555e-02
  8.74246284e-02 -4.09821689e-01 -5.35762161e-02 -4.89782155e-01
 -2.88113266e-01  1.17136136e-01  1.66890860e-01 -5.87165475e-01
  2.05184579e-01 -1.13608725e-01 -3.34555209e-01  6.23613223e-02
  1.58278376e-01 -9.69803147e-03 -7.75015354e-02  1.91913158e-01
 -1.40484929e-01 -4.07847762e-02 -1.52949393e-01 -2.88675666e-01
 -2.60870457e-01  1.03423279e-03 -1.57116890e-01 -5.53552341e-03
  2.45580450e-02  1.87863231e-01  2.68612429e-03  5.71443401e-02
  4.15083170e-01  3.07867765e-01  4.14036810e-01 -2.03984365e-01
 -3.33575368e-01 -1.20078996e-01  6.71518594e-02  2.03099862e-01
 -3.48491490e-01 -1.20693818e-01 -1.05690822e-01 -1.33196145e-01
  2.66914874e-01  3.04650515e-01 -1.94188356e-01 -1.36556536e-01
 -4.79747772e-01  1.44516915e-01 -3.41328025e-01  1.85856342e-01
  3.94667745e-01  8.28400552e-02  4.88689125e-01  9.02410075e-02
  1.99614048e-01  2.51044512e-01  2.56694853e-01 -1.45177200e-01
  3.88303638e-01  3.90565276e-01  1.21384412e-02  3.85538936e-01
  1.53609917e-01  1.98490798e-01 -3.22051704e-01  7.09135413e-01
 -8.82223994e-03 -1.28524110e-01  1.88765809e-01 -4.68687624e-01
  4.49755251e-01 -3.38563621e-01  1.34312913e-01 -6.48202151e-02
  3.70754838e-01  1.40084773e-01 -1.92349911e-01  1.00821145e-01
  1.83361322e-01  2.54233181e-01 -2.72578299e-01 -1.01018324e-03
 -6.32288828e-02 -9.74503085e-02 -2.03887925e-01 -6.94428623e-01
 -1.95579350e-01 -3.64618413e-02 -1.46492615e-01  3.17649543e-01
  1.22217938e-01 -2.21324220e-01 -3.64617467e-01  1.76324785e-01
  1.84757203e-01 -3.93558703e-02  1.28622949e-01  2.83694804e-01
 -1.87449172e-01 -3.38441618e-02  3.57066840e-01 -5.51312208e-01
 -4.23314869e-02 -2.19037265e-01  3.62148345e-01  9.02003497e-02
  2.49830499e-01 -4.68003154e-01  2.34796107e-03  3.97930853e-02
 -1.29995495e-01  2.94678301e-01  1.02156699e-01  1.02867395e-01
 -2.20943928e-01  8.02585900e-01  2.58859217e-01 -2.67090686e-02
  8.98138508e-02 -1.46344334e-01 -3.46481711e-01  3.79313566e-02
  2.70584643e-01 -1.45722568e-01  1.10031199e-02 -2.55482733e-01
 -9.05320719e-02  1.42801329e-01 -1.93095639e-01 -6.19796365e-02
 -3.15137617e-02  1.36089772e-01 -9.63156372e-02 -5.38816154e-02
 -4.46842939e-01  3.86308730e-01  6.59454763e-02 -3.62017155e-01
 -8.45189542e-02 -1.18041098e-01 -2.39750370e-02 -2.78793395e-01
 -8.68716165e-02 -2.57035673e-01  3.70035946e-01  5.99631727e-01
 -2.62485817e-03  1.97872341e-01  4.99149263e-02  2.96140552e-01
 -4.19987381e-01 -1.43512264e-01 -7.99348354e-02  5.48744023e-01
  6.60214648e-02 -3.04422438e-01  4.29963976e-01  3.82703543e-01
 -5.72553091e-02  1.01467527e-01 -2.60850012e-01  5.86129650e-02
  1.86638549e-01 -3.46987605e-01 -1.98853910e-01 -4.63340700e-01
  8.16382095e-03  3.31430733e-01 -1.61859363e-01  1.20826527e-01
 -3.12281251e-01  2.73178339e-01  6.13119960e-01 -2.08824307e-01
 -2.42760226e-01  6.54899925e-02  1.67686701e-01 -1.31409153e-01
  8.16712230e-02 -2.41916448e-01  1.63163096e-01 -7.19213635e-02]"
Deserialization issues with non-default dtypes stat:awaiting response type:bug comp:apis TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf v2.14.0-rc1-21-g4dacf3f368e

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following model cannot be deserialized:

```python
x = keras.Input(batch_shape=(4, 16), dtype=tf.float16)
output = 1.0 + x
keras_model = keras.Model(x, output)
```
It appears as if Keras fails to recognize the `dtype` of constants and always saves them as default type (`float32`/`int32`).
Setting `dtype` explicitly does not help:

```python
output = tf.convert_to_tensor(1, dtype=tf.float16) + x
```
The problem persists across various non-default data types (`float16`, `float64`, `int16`, `int64`), operations (`add`, `sub`, `mul`, `pow`) and model formats (`.h5`, `SavedModel`). Oddly enough, it goes away when the arguments are swapped:

```python
output = x + 1.0  # Works just fine!
```
In `v2.16-nightly`, which resolves to Keras 3, everything's alright.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow import keras


x = keras.Input(batch_shape=(4, 16), dtype=tf.float16)
output = tf.convert_to_tensor(1, dtype=tf.float16) + x
keras_model = keras.Model(x, output)

model_path = 'model.h5'
keras_model.save(model_path)
print('Model saved')

keras_model_restored = keras.models.load_model(model_path)
print('Model loaded')
```


### Relevant log output

```shell
TypeError: Exception encountered when calling layer ""tf.__operators__.add"" (type TFOpLambda).

Input 'y' of 'AddV2' Op has type float16 that does not match type float32 of argument 'x'.

Call arguments received by layer ""tf.__operators__.add"" (type TFOpLambda):
   x=tf.Tensor(shape=(), dtype=float32)
   y=tf.Tensor(shape=(4, 16), dtype=float16)
   name=None
```
",True,"[-0.53856814 -0.16514374 -0.1633586  -0.13452336  0.13757329 -0.14615238
 -0.1767707   0.03246943 -0.4442937  -0.35989282  0.25175875 -0.34129125
 -0.11056618 -0.16798782 -0.02933947  0.33019644  0.05710755  0.02226399
  0.1501928   0.05799806 -0.15756738 -0.04978823 -0.45545864  0.1922848
  0.2016707   0.11308828 -0.37466243  0.10414899  0.10435729  0.32522443
  0.3571353   0.12437625  0.11602362  0.28253424  0.18637705  0.16602002
 -0.3159349   0.03094231 -0.09429222  0.21223283  0.05734235 -0.01830057
  0.05414834 -0.15425718  0.15995532 -0.2538338   0.03216221  0.02698461
 -0.05634618 -0.2997017   0.18822159 -0.005644   -0.650311   -0.50141495
 -0.20617521 -0.02988526 -0.01727369  0.13865009 -0.08378157  0.05799137
  0.24013582 -0.12658261 -0.01856238 -0.09380546  0.22045326  0.28688413
  0.2904936   0.13814566  0.6383693  -0.08319662  0.29900438 -0.0660717
 -0.29378134  0.1251158   0.18804242  0.02587065 -0.04865166  0.22122785
  0.2720888  -0.51287365  0.03151986 -0.4910166  -0.27697024 -0.35185346
  0.11352887 -0.3825995   0.5554453   0.06730868  0.48252794 -0.25754702
  0.47004738  0.37726477 -0.16550905  0.30633745  0.07591331  0.06732978
  0.15481333 -0.00657512  0.10044739 -0.06812543 -0.29824448 -0.41978732
 -0.11456032  0.13930956 -0.21009904 -0.41658214  0.18346244 -0.04629257
  0.20504332  0.11951369  0.0383247   0.04706226  0.0069654  -0.15010875
 -0.0651031  -0.00404597  0.11525346  0.01738405 -0.08379681  0.31026846
  0.24195752 -0.11309961  0.17589067  0.25755915  0.48354012 -0.03187802
 -0.08467177 -0.07805087  0.05141312  0.00606268  0.13514988 -0.01694983
  0.07411336  0.32006666  0.06355587  0.06141341 -0.12182006 -0.1956494
 -0.17305577 -0.18678996 -0.44716763  0.04626524 -0.22953202 -0.46211982
  0.20886171 -0.06309883 -0.26405177  0.03411832 -0.23008794  0.07274275
 -0.01609604  0.07800583 -0.5162681   0.34373438  0.23966809  0.06973505
  0.39254767 -0.08047991  0.08365057 -0.42222166  0.12351459  0.4331863
 -0.08105959 -0.1949086   0.01349577  0.17995167 -0.15030381 -0.09943484
  0.02999602  0.591316   -0.13096306 -0.21345623 -0.09842762 -0.10514921
  0.0616478  -0.33233055  0.14561895 -0.45000285 -0.09301852  0.31856394
  0.0830889   0.27475825  0.17353323  0.19733533 -0.01137511  0.08308653
  0.06335124  0.06700685 -0.06789324 -0.17849706 -0.47320276 -0.12673429
  0.22015001  0.03554353 -0.02939682  0.0835617   0.34306252 -0.19375294
  0.2096541   0.10616058 -0.11515431 -0.06022181 -0.0374031  -0.05368183
  0.34399733 -0.1721221  -0.04561818 -0.25994664 -0.319378    0.2945331
 -0.10418119 -0.33221048  0.01737181 -0.15019299 -0.176307    0.4268294
  0.13029224  0.1413092  -0.10378388  0.2336444  -0.03463944 -0.25573468
  0.11141571 -0.43582663 -0.14620334  0.15322654 -0.4035498   0.05616952
 -0.11857267  0.0234819   0.03095275  0.29995513  0.43023324  0.35688072
  0.35176435 -0.20024626 -0.32957283 -0.22086534 -0.35867795 -0.18446839
 -0.42488688 -0.00958279  0.04378414 -0.16351444  0.39180917  0.55644214
 -0.2521582  -0.01035165 -0.4830135   0.13129762 -0.30399185  0.42160225
  0.29264888 -0.04322106  0.37644196  0.27425337  0.23768474  0.04410993
  0.13581401 -0.13952616  0.59380984  0.21309057  0.02473467  0.2542221
  0.18854028  0.3472402  -0.3272311   0.5976461   0.33820057 -0.1276021
  0.4248413  -0.44709313  0.6832372  -0.296214    0.2596786  -0.14797117
  0.04384809 -0.09805518 -0.23381801  0.1575273  -0.07766733  0.38358203
 -0.2172744   0.18029597  0.13867188 -0.34854317 -0.20537421 -0.698722
 -0.41066962  0.07397854 -0.2099751   0.24379802  0.04908467  0.03306537
 -0.3807261   0.04743247  0.03687089 -0.20228958  0.20054774  0.19230436
 -0.30137515  0.08051562  0.4057147  -0.48523843 -0.31513488 -0.12014005
  0.25147992  0.24276474  0.63026243 -0.6357909  -0.00347082 -0.10286263
 -0.21075213  0.53614986 -0.00918185  0.08751416 -0.3443007   0.9057752
  0.19502172 -0.1371565   0.21824303 -0.0622872  -0.20917109  0.12142907
  0.31320545  0.19690637 -0.07254346 -0.49265987  0.41803837 -0.05289554
 -0.28096142  0.12842691 -0.31052053 -0.07386797 -0.29043314 -0.19804113
 -0.48299176  0.2588425   0.03039031 -0.2599051  -0.24889307  0.07037774
 -0.25474697 -0.21966985  0.00187167 -0.30202958  0.23471071  0.5015458
  0.03562226  0.0836557   0.11031467  0.07247038 -0.16218516 -0.03507244
 -0.02910624  0.23177326 -0.07594737 -0.02983241  0.40613177  0.35382354
 -0.09258621  0.02235958 -0.350586    0.07477002  0.2513348  -0.16923043
 -0.09359604 -0.30656832  0.15870203  0.28501794 -0.13002041  0.08354696
 -0.3032527  -0.06153974  0.35125265 -0.40713456  0.04986407  0.23402399
  0.3622352   0.05819763  0.08398981 -0.2575876   0.22729276  0.0231569 ]"
//tensorflow/python/eager:forwardprop_test test fails on AARCH64 type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

17.0.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Since Eigen was updated, the unit test //tensorflow/python/eager:forwardprop_test now fails.
Eigen was updated by https://github.com/tensorflow/tensorflow/commit/57e6377cf9879e33f3612f1ffd3619b6513e5296
It looks like this commit in Eigen is the problem.
https://gitlab.com/libeigen/eigen/-/commit/81b48065ea673cd352d11ef9b6a3d86778ac962d

This seems to only affect AARCH64 and not x86.

### Standalone code to reproduce the issue

```shell
bazel test --cache_test_results=no --build_tests_only --config=mkl_aarch64_threadpool --test_env=TF_ENABLE_ONEDNN_OPTS=1 --copt=-flax-vector-conversions --test_env=TF2_BEHAVIOR=1 --test_env=PORTSERVER_ADDRESS=@unittest-portserver --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --test_tag_filters=--oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 --jobs=75 -- //tensorflow/python/eager:forwardprop_test
```


### Relevant log output

```shell
======================================================================
FAIL: testNumericHigherOrder (__main__.ForwardpropTest)
ForwardpropTest.testNumericHigherOrder
Warms up, gets object counts, runs the test, checks for new objects.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 713, in decorator
    f(self, *args, **kwargs)
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/forwardprop_test.py"", line 455, in testNumericHigherOrder
    _test_gradients(
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/forwardprop_test.py"", line 215, in _test_gradients
    _test_gradients(
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/forwardprop_test.py"", line 215, in _test_gradients
    _test_gradients(
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/forwardprop_test.py"", line 231, in _test_gradients
    testcase.assertAllClose(sym_jac_back, sym_jac_fwd, rtol=srtol, atol=satol)
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1657, in decorated
    return f(*args, **kwds)
           ^^^^^^^^^^^^^^^^
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3293, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3229, in _assertAllCloseRecursive
    self._assertArrayLikeAllClose(
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 3186, in _assertArrayLikeAllClose
    np.testing.assert_allclose(
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/pypi_numpy/site-packages/numpy/testing/_private/utils.py"", line 1527, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/python/eager/forwardprop_test_cpu.runfiles/pypi_numpy/site-packages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=1e-06, atol=1e-06
Mismatched value: a is different from b. 
not close where = (array([0]), array([0]), array([2]))
not close lhs = [-155.46957]
not close rhs = [-155.46982]
not close dif = [0.00024414]
not close tol = [0.00015647]
dtype = float32, shape = (1, 4, 4)
Mismatched elements: 1 / 16 (6.25%)
Max absolute difference: 0.00024414
Max relative difference: 1.5703409e-06
 x: array([[[-4755.0654  ,  -139.43396 ,  -155.46957 ,   158.52019 ],
        [ -139.43398 ,  -119.946   ,   -26.157635,    20.20992 ],
        [ -155.4697  ,   -26.15764 ,   428.02136 ,  -365.0485  ],...
 y: array([[[-4755.0654  ,  -139.43398 ,  -155.46982 ,   158.52022 ],
        [ -139.43399 ,  -119.946014,   -26.157648,    20.209923],
        [ -155.46967 ,   -26.157635,   428.02112 ,  -365.04846 ],...

----------------------------------------------------------------------
Ran 13 tests in 95.972s

FAILED (failures=1)
================================================================================
```
",True,"[-5.11403501e-01 -3.02938640e-01 -2.92819202e-01  1.36596069e-01
  2.25599453e-01 -4.71671045e-01 -1.31166667e-01 -6.90652579e-02
 -3.62976044e-01 -2.33420700e-01  2.10095923e-02 -1.25395149e-01
 -1.70527846e-01  6.77807257e-02  1.63915195e-03  1.76015660e-01
  1.32342592e-01 -2.80143581e-02  2.46278614e-01  2.29259562e-02
 -1.45407304e-01 -2.45820671e-01 -1.22232527e-01  6.07679188e-02
  7.19711557e-02  1.38335943e-01 -2.73589611e-01 -2.18687817e-01
 -7.35018589e-03  2.39142954e-01  5.96917212e-01  6.51256181e-04
 -1.03797868e-01  4.07595076e-02 -4.18640263e-02  4.08615500e-01
 -1.39919221e-01 -4.58007455e-01 -3.02691102e-01  1.13724150e-01
 -1.64258108e-02  1.51915580e-01  1.11576095e-01  2.85103209e-02
  3.24851368e-03 -1.43189326e-01 -4.23491783e-02  6.90816343e-02
 -4.86485213e-02 -3.48934740e-01  5.02157584e-02 -2.35020205e-01
 -2.97778547e-01 -3.15932631e-01 -2.66728908e-01  2.72259265e-02
  1.17735296e-01 -5.07261753e-02 -2.08222494e-02  6.95401505e-02
  9.54647958e-02 -7.21107721e-02 -3.69526893e-02 -1.91916019e-01
  1.70011684e-01  1.65856436e-01  2.87341416e-01 -7.54090399e-03
  6.47246063e-01 -9.86403674e-02 -5.47157489e-02 -1.07762497e-03
 -3.98865163e-01 -2.17870325e-02 -5.08420691e-02 -7.93943405e-02
  1.87966317e-01  2.99627692e-01  2.87108690e-01 -1.57337502e-01
  1.47794738e-01 -1.74743667e-01 -3.20632104e-03 -1.90734282e-01
  1.78899050e-01 -8.97552148e-02  3.09086621e-01  4.64117944e-01
  3.43644321e-01 -2.90831868e-02  4.19329196e-01  1.44017801e-01
 -2.39408642e-01  1.60474151e-01  3.41246843e-01  1.36152297e-01
  1.70730680e-01  9.76370275e-02 -1.70395389e-01 -1.74550146e-01
 -2.46246047e-02 -2.75414556e-01  1.35168731e-01 -1.01825505e-01
 -1.80580556e-01 -1.16401494e-01  2.44857445e-01  7.35938922e-02
 -7.64932036e-02 -2.83665121e-01  1.16208866e-02 -4.03319076e-02
  1.15372822e-01 -2.63827562e-01  4.35295999e-02 -1.47220284e-01
 -4.64878440e-01 -1.36016339e-01  1.92181021e-01  4.68037069e-01
  6.78213760e-02 -1.23419806e-01  1.66993141e-01  1.47364169e-01
  5.00296712e-01  2.70741731e-02  1.64036602e-02  4.74571660e-02
  1.10884920e-01 -2.27128625e-01  2.33824216e-02  7.43135363e-02
  9.27530080e-02  3.77085328e-01  2.37953961e-01  1.54052824e-01
 -2.70094037e-01 -2.41739243e-01  2.20359564e-02 -3.10057819e-01
 -2.93686271e-01  2.11045578e-01 -1.00495800e-01 -6.76410854e-01
  1.08096497e-02 -1.62169896e-02 -3.10351789e-01  4.18063819e-01
 -3.73713039e-02 -1.81844473e-01 -1.23540558e-01 -2.54334491e-02
 -1.84159100e-01  5.13237119e-01  7.35474080e-02  1.76725745e-01
  3.62639070e-01 -7.86075220e-02  2.34348908e-01 -3.72034907e-01
 -1.07955404e-01  5.55002689e-01 -5.42467162e-02 -7.29754642e-02
 -1.21406153e-01  1.42346531e-01 -2.48237237e-01 -2.21886456e-01
 -1.56493515e-01  3.98319483e-01 -1.29715279e-01 -1.10705689e-01
  1.94277376e-01  1.98794961e-01  3.66591699e-02 -3.23867761e-02
  2.45799989e-01 -6.53401017e-01  1.64371133e-01  5.06411910e-01
 -1.89804137e-01  2.25417748e-01  1.80662677e-01  3.37507248e-01
  5.78983314e-02  3.68413210e-01  7.67741352e-02  2.09468976e-01
 -2.11051747e-01  6.51311129e-02 -3.13997269e-01 -2.36015290e-01
  2.48193622e-01 -2.69679934e-01 -9.61175784e-02  1.77698314e-01
  3.05539846e-01  3.34171623e-01 -1.18974365e-01  4.43761200e-02
 -2.55695313e-01  1.72488615e-02 -1.96112454e-01  1.53467372e-01
  1.74309611e-01 -1.96201690e-02 -3.04357521e-02 -5.02876043e-01
 -1.83877498e-01  1.17308035e-01 -1.51607648e-01 -4.02933717e-01
  2.68285185e-01  1.48595229e-01 -3.10643703e-01 -8.24635103e-03
  2.73309678e-01  7.95643777e-02 -1.50386631e-01  1.74078494e-01
  5.32469004e-02 -2.90226072e-01 -8.35988149e-02 -4.10550594e-01
  1.81519359e-01  7.39551634e-02 -4.59352553e-01  5.49565926e-02
 -1.37071103e-01  1.28335342e-01  6.66455552e-02  1.04712784e-01
  3.49337697e-01  2.95036167e-01  4.55966324e-01 -6.86234832e-02
 -1.16752662e-01 -9.02158618e-02 -1.88093990e-01  1.57647163e-01
 -3.68228465e-01 -3.66799027e-01  6.66093007e-02 -9.72516835e-02
  2.00523078e-01  5.32089591e-01 -8.41524675e-02 -9.53191817e-02
 -4.28859830e-01  8.17822441e-02 -1.17257871e-01  2.07345173e-01
  2.09351450e-01 -8.68753344e-02  5.03111959e-01  2.57282972e-01
  1.43034235e-01  1.19428225e-01  1.13759086e-01 -2.00975284e-01
  2.36161277e-01  2.33690694e-01  1.64653450e-01  4.73683774e-01
  3.62036973e-01  4.16689128e-01 -2.44274080e-01  6.32534206e-01
  5.13003990e-02 -3.28392256e-03  2.14587122e-01 -3.26951593e-01
  5.16501486e-01  4.08515241e-03  2.41801083e-01 -1.53996289e-01
  4.60927188e-01 -1.83190063e-01 -4.12254781e-02  9.75043699e-02
  3.08712292e-03  1.70296535e-01 -1.45766139e-01  5.94157018e-02
  5.96357472e-02 -2.59185880e-01  1.05680868e-01 -7.60196924e-01
 -2.43774414e-01 -1.32023215e-01 -1.95266068e-01  8.89204256e-03
  2.21587479e-01 -2.09174752e-02 -1.88388377e-01 -4.25827429e-02
  1.39266580e-01 -1.53512090e-01  4.26535130e-01  2.56596118e-01
 -3.39959353e-01 -8.90396535e-02  1.98264420e-01 -6.55281126e-01
 -9.79315192e-02 -1.07226931e-01  4.66932982e-01  1.69415370e-01
  2.89846331e-01 -5.36582947e-01  2.21317992e-01 -1.31186277e-01
 -3.12275201e-01  1.97837055e-01  1.49604697e-02  2.14532778e-01
 -2.48727441e-01  6.77603960e-01  2.04496369e-01 -1.45507932e-01
  4.68892008e-02 -1.46876037e-01 -4.92561728e-01 -9.57238674e-02
  1.88557535e-01 -4.47113439e-02 -1.20675497e-01 -3.56657803e-01
 -1.38167799e-01  5.05703650e-02 -1.47869468e-01 -1.38571441e-01
 -1.74878165e-01  6.52234480e-02 -2.11209077e-02  7.96138868e-02
 -5.52544534e-01  1.31490111e-01 -1.15435742e-01 -3.95029873e-01
 -2.25378141e-01  1.03393167e-01 -6.94954470e-02  9.31047648e-03
  1.43018020e-02  2.62070224e-02  4.62829411e-01  4.18753624e-01
 -1.11151762e-01 -2.01276317e-01 -3.22707230e-03  2.40731835e-01
 -3.81347716e-01 -7.59439021e-02  1.41438544e-02  6.48217678e-01
  5.78756519e-02 -1.90660179e-01  4.13731486e-01  1.10370755e-01
  3.02756205e-04  9.95065197e-02 -1.10404626e-01  4.91397157e-02
 -6.89913332e-02 -2.81222254e-01 -2.08218351e-01 -2.68013835e-01
  5.36111481e-02  2.93614686e-01 -2.52355397e-01  9.60082933e-02
 -5.16107261e-01  4.16505098e-01  6.93267763e-01 -4.11695868e-01
 -3.85049850e-01 -3.84077951e-02  1.29962921e-01 -6.19682446e-02
 -6.88090920e-02 -1.11499622e-01  2.97007620e-01  9.31237191e-02]"
"TensorShape is (None, None, None) on images type:bug","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

An I/O pipeline for images using Dataset reads images into tensor but the shape is (None, None, None), preventing some applications that need the tensor shape further down in preprocessing (for example, resizing the image by a factor).

In the example below I can get by by recovering the image dimensions with a trick but it feels very *very* hacky. Is there a reason why the shape should be None?

### Standalone code to reproduce the issue

```shell
def preprocess(file_name):
    x = tf.io.read_file(file_name)
    x = tf.io.decode_jpeg(x)
    
    nrows, ncols,_ = x.shape
    
    x = tf.image.resize(x, (nrows//2, ncols//2))
    
    x = tf.image.random_crop(x, (32,32,3))
    
    return x, tf.zeros_like(x)

def preprocess_workaround(file_name):
    x = tf.io.read_file(file_name)
    x = tf.io.decode_jpeg(x)
    
    nrows = tf.math.reduce_sum(tf.ones_like(x[:,0], dtype=tf.int32))
    ncols = tf.math.reduce_sum(tf.ones_like(x[0,:], dtype=tf.int32))
    
    x = tf.image.resize(x, (nrows//2, ncols//2))
    
    x = tf.image.random_crop(x, (32,32,3))
    
    return x, tf.zeros_like(x)

model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(32,32,3)))
model.add(tf.keras.layers.Conv2D(3, (3,3), padding='same'))
model.summary()

files = glob.glob(""/mnt/ng/ncl/acquisition/stitches/20231019/row1/??/1/crop.jpg"")
print(len(files))

## crashes with the output below
ds = tf.data.Dataset.from_tensor_slices(files).map(preprocess).batch(1)


## the following works:
## ds = tf.data.Dataset.from_tensor_slices(files).map(preprocess_workaround).batch(1)

model.fit(ds)
```
```


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_583425/761276435.py in <module>
      1 files = glob.glob(""/mnt/ng/ncl/acquisition/stitches/20231019/row1/??/1/crop.jpg"")
      2 print(len(files))
----> 3 ds = tf.data.Dataset.from_tensor_slices(files).map(preprocess).batch(1)
      4 model.fit(ds)

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic, name)
   2292         warnings.warn(""The `deterministic` argument has no effect unless the ""
   2293                       ""`num_parallel_calls` argument is specified."")
-> 2294       return MapDataset(self, map_func, preserve_cardinality=True, name=name)
   2295     else:
   2296       return ParallelMapDataset(

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)
   5497     self._use_inter_op_parallelism = use_inter_op_parallelism
   5498     self._preserve_cardinality = preserve_cardinality
-> 5499     self._map_func = structured_function.StructuredFunctionWrapper(
   5500         map_func,
   5501         self._transformation_name(),

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
    261         fn_factory = trace_tf_function(defun_kwargs)
    262 
--> 263     self._function = fn_factory()
    264     # There is no graph to add in eager mode.
    265     add_to_graph &= not context.executing_eagerly()

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py in get_concrete_function(self, *args, **kwargs)
    224         `tf.Tensor` or `tf.TensorSpec`.
    225     """"""
--> 226     concrete_function = self._get_concrete_function_garbage_collected(
    227         *args, **kwargs)
    228     concrete_function._garbage_collector.release()  # pylint: disable=protected-access

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
    190 
    191     with self._lock:
--> 192       concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
    193       seen_names = set()
    194       captured = object_identity.ObjectIdentitySet(

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py in _maybe_define_concrete_function(self, args, kwargs)
    155       kwargs = {}
    156 
--> 157     return self._maybe_define_function(args, kwargs)
    158 
    159   def _get_concrete_function_internal_garbage_collected(self, *args, **kwargs):

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py in _maybe_define_function(self, args, kwargs)
    358             args, kwargs = generalized_func_key._placeholder_value()  # pylint: disable=protected-access
    359 
--> 360           concrete_function = self._create_concrete_function(args, kwargs)
    361 
    362           graph_capture_container = concrete_function.graph._capture_func_lib  # pylint: disable=protected-access

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py in _create_concrete_function(self, args, kwargs)
    282     arg_names = base_arg_names + missing_arg_names
    283     concrete_function = monomorphic_function.ConcreteFunction(
--> 284         func_graph_module.func_graph_from_py_func(
    285             self._name,
    286             self._python_function,

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)
   1281         _, original_func = tf_decorator.unwrap(python_func)
   1282 
-> 1283       func_outputs = python_func(*func_args, **func_kwargs)
   1284 
   1285       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py in wrapped_fn(*args)
    238           attributes=defun_kwargs)
    239       def wrapped_fn(*args):  # pylint: disable=missing-docstring
--> 240         ret = wrapper_helper(*args)
    241         ret = structure.to_tensor_list(self._output_structure, ret)
    242         return [ops.convert_to_tensor(t) for t in ret]

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py in wrapper_helper(*args)
    169       if not _should_unpack(nested_args):
    170         nested_args = (nested_args,)
--> 171       ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
    172       ret = variable_utils.convert_variables_to_tensors(ret)
    173       if _should_pack(ret):

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    690       except Exception as e:  # pylint:disable=broad-except
    691         if hasattr(e, 'ag_error_metadata'):
--> 692           raise e.ag_error_metadata.to_exception(e)
    693         else:
    694           raise

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    687       try:
    688         with conversion_ctx:
--> 689           return converted_call(f, args, kwargs, options=options)
    690       except Exception as e:  # pylint:disable=broad-except
    691         if hasattr(e, 'ag_error_metadata'):

~/anaconda3/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)
    437     try:
    438       if kwargs is not None:
--> 439         result = converted_f(*effective_args, **kwargs)
    440       else:
    441         result = converted_f(*effective_args)

/tmp/__autograph_generated_filecu6im70c.py in tf__preprocess(file_name)
     11                 x = ag__.converted_call(ag__.ld(tf).io.decode_jpeg, (ag__.ld(x),), None, fscope)
     12                 (nrows, ncols, _) = ag__.ld(x).shape
---> 13                 x = ag__.converted_call(ag__.ld(tf).image.resize, (ag__.ld(x), (ag__.ld(nrows) // 2, ag__.ld(ncols) // 2)), None, fscope)
     14                 x = ag__.converted_call(ag__.ld(tf).image.random_crop, (ag__.ld(x), (32, 32)), None, fscope)
     15                 try:

TypeError: in user code:

    File ""/tmp/ipykernel_583425/4207213119.py"", line 7, in preprocess  *
        x = tf.image.resize(x, (nrows//2, ncols//2))

    TypeError: unsupported operand type(s) for //: 'NoneType' and 'int'
```
",True,"[-0.335535   -0.4509246  -0.08335231  0.23331824  0.30230045 -0.62974834
 -0.08792038 -0.12749386 -0.21675989 -0.26011145  0.31025183 -0.12246701
 -0.13727427  0.1169084  -0.1952937   0.32919672 -0.19945481 -0.14332376
  0.14511502  0.44985884 -0.06493405 -0.05035466 -0.24633466  0.19821908
  0.19266602  0.12108476 -0.34362996 -0.29202306 -0.00241525  0.00213217
  0.6935206   0.02688943 -0.25205833  0.15446073  0.13660482  0.32733083
 -0.341816   -0.23091745 -0.28884023 -0.03762102 -0.02680735  0.13950911
  0.09434836 -0.30627173  0.30014342 -0.11015414  0.02951917 -0.42025745
  0.19942538 -0.22833517 -0.07198475 -0.16095507 -0.6078354  -0.17478794
 -0.18496314 -0.05452131 -0.02795669 -0.12642644  0.0527176   0.25726336
  0.11984667  0.02023907  0.05069326 -0.09214996  0.1850498   0.07196683
  0.3864421  -0.23672614  0.55698514 -0.4029835   0.07719942 -0.12250304
 -0.24809569  0.214773    0.04097379  0.2426501   0.02285247  0.2327666
  0.22578135 -0.15677439  0.04443353 -0.30159253  0.09144478 -0.3311843
  0.17134786  0.00100206  0.3934851   0.12967925  0.36317462 -0.29499593
  0.38268617  0.36556494 -0.13617024 -0.00814428  0.42596275 -0.21239308
  0.13706207  0.16724062  0.010375   -0.18803474 -0.24671666 -0.09796374
 -0.04295028 -0.02096156 -0.25349218  0.10501465  0.17816076 -0.10924681
  0.0340683  -0.03083257  0.06438084 -0.11405616  0.22212239 -0.27644226
  0.01638255 -0.1313321  -0.24004993 -0.08180878 -0.13684718  0.726133
  0.02961251 -0.09668452  0.24054193  0.25706434  0.42250657  0.1538312
 -0.06800783 -0.01084242  0.05846236 -0.15774204  0.0543166  -0.01106726
  0.37573892  0.2513981  -0.08902711  0.05287174 -0.05291716 -0.3758718
 -0.13890895 -0.29460698 -0.30860633  0.32989728 -0.25224    -0.54853797
  0.28731504 -0.11902471 -0.18558522  0.16835043 -0.19513968  0.26035437
 -0.11596178  0.19493687 -0.01741221  0.41512197  0.1315367   0.31677747
  0.33981758 -0.22774798 -0.12915677 -0.57148075  0.18027188  0.35738784
 -0.09165403 -0.11867967 -0.07532109  0.12075412 -0.38775074 -0.25194314
 -0.1457851   0.47582066 -0.14872816 -0.14911608 -0.00290825  0.14365436
  0.06214856  0.21792766  0.28433123 -0.6393132  -0.13317302  0.44799882
 -0.00807842  0.29497954  0.04999544  0.16352919  0.01112948 -0.0402722
 -0.12121816  0.09154864 -0.20162967  0.05640021 -0.3382143  -0.2098059
  0.33325118 -0.09689346 -0.0305693   0.4209633   0.25578302 -0.16706677
 -0.11364947 -0.03946014 -0.1048722  -0.31399912  0.02744832 -0.05059153
 -0.05840019 -0.42019385 -0.12841156 -0.35324967 -0.57251704  0.0739703
  0.17547423 -0.58572894 -0.07674076 -0.04661546 -0.29791707  0.1312045
  0.21009049  0.11827835 -0.22498894  0.2739035   0.2152113  -0.39889133
  0.01608079 -0.42446312  0.06899022  0.04016269 -0.29637086  0.26670936
 -0.04284607  0.33422744  0.07720622  0.192069    0.33451152  0.06408218
  0.54284775 -0.02343482 -0.42997375 -0.1535092  -0.15934843 -0.02808608
 -0.36026365 -0.34873137 -0.05366247 -0.01082496  0.17146705  0.4242736
 -0.13826293 -0.1842265  -0.28380334  0.2936684  -0.11707188  0.331684
  0.45408577  0.27495     0.63509023  0.15130273  0.31458905  0.1421616
  0.23251587 -0.12650609  0.59400964  0.17595541 -0.09952715  0.58735716
  0.18379572  0.26664722 -0.47368166  0.58146167  0.01999541 -0.15520203
  0.09192763 -0.38931805  0.63922274 -0.43000576  0.04762639 -0.0709306
  0.3073595   0.10692183 -0.04341922 -0.00326982  0.31190512  0.33639032
 -0.3954188  -0.01184303 -0.17393719 -0.22454387 -0.10706575 -0.6088003
 -0.22049384  0.03950747 -0.09511098  0.19531843  0.26219213  0.02489035
 -0.20737645 -0.01635522  0.1453818  -0.09098966  0.20454459  0.31884646
 -0.14278117  0.00281395  0.4136718  -0.7852108  -0.00487999 -0.25723708
  0.42495963  0.42730397  0.41763243 -0.40994442  0.17826806 -0.40849024
 -0.01158493  0.4348784  -0.01273283  0.03322403 -0.27287042  0.8006913
  0.24483016 -0.08790904  0.23122784 -0.00635202 -0.36781    -0.07387857
  0.30193716 -0.17538741 -0.00810603 -0.46811107 -0.09681788  0.0418289
 -0.03556667  0.12039653 -0.16917089 -0.08912509 -0.14421749 -0.12014861
 -0.4923672   0.3284636   0.00165467 -0.48276657 -0.28121638 -0.06112642
 -0.10893737 -0.29831555  0.2129202  -0.37376663  0.45536706  0.5606245
 -0.39563808  0.18251905  0.14372931  0.30966634 -0.5331893   0.02512537
 -0.0347642   0.3272566   0.08829966 -0.2439736   0.44511694  0.29645061
 -0.199283    0.26535326 -0.1864307   0.02694739  0.13913153 -0.22812113
  0.08875029 -0.30354273  0.15529469  0.6237048  -0.16212837  0.34621608
 -0.43291506  0.32350445  0.35445994 -0.61869836 -0.3011455   0.11578064
  0.1561053  -0.10435291  0.15175119 -0.19469285  0.31407592 -0.05027746]"
Trying to process RBG images and use RandomContrast: LookupError: gradient registry has no entry for: AdjustContrastv2 stat:awaiting response type:bug stale comp:ops TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

google collaboration

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

T4

### Current behavior?

I see an error when I add RandomContrast layer into the model. Otherwise, it works fine.


### Standalone code to reproduce the issue

```shell
# Clean session
clear_session()

model_6 = Sequential()

model_6.add(Conv2D(filters = 16, kernel_size = (3, 3)
                   , padding = ""same"", input_shape = (img_height, img_wdith, 3)
                   , activation='relu'))


model_6.add(RandomRotation(factor=0.3))
model_6.add(layers.RandomFlip(""horizontal_and_vertical""))
model_6.add(MaxPooling2D(pool_size = (2, 2)))
model_6.add(Conv2D(filters = 16, kernel_size = (3, 3) , padding = ""same"" , activation='relu'))
model_6.add(MaxPooling2D(pool_size = (2, 2)))

# Add batch normalization
batch_normalization = BatchNormalization()
batch_normalization.trainable = True
model_6.add(batch_normalization)
model_6.add(layers.RandomContrast(factor=0.2, seed=11))

model_6.add(Flatten())

# Adding a fully connected dense layer with 256 neurons
model_6.add(Dense(256))

model_6.add(LeakyReLU(0.1))

model_6.add(Dense(num_classes, activation = 'softmax'))

# Printing the model summary
model_6.summary()
```


### Relevant log output

```shell
Epoch 1/20
---------------------------------------------------------------------------
StagingError                              Traceback (most recent call last)
[<ipython-input-72-d1c53732b3af>](https://39j3jgoo0eu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20231016-060118_RC00_573771261#) in <cell line: 7>()
      5 
      6 # Fit training dataset to model
----> 7 history_6 = model_6.fit(n_train_ds, validation_data=n_val_ds, epochs=num_epochs, verbose = 1 )

1 frames
[/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py](https://39j3jgoo0eu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20231016-060118_RC00_573771261#) in autograph_handler(*args, **kwargs)
     50     except Exception as e:  # pylint:disable=broad-except
     51       if hasattr(e, ""ag_error_metadata""):
---> 52         raise e.ag_error_metadata.to_exception(e)
     53       else:
     54         raise

StagingError: in user code:

    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1084, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
        grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients
        grads = tape.gradient(loss, var_list)

    LookupError: gradient registry has no entry for: AdjustContrastv2
```
",True,"[-5.87129176e-01 -2.89101630e-01 -3.97793651e-02 -6.74305204e-03
  3.92332792e-01 -3.31411272e-01 -1.95075929e-01  6.96053132e-02
 -2.23950163e-01 -3.35057795e-01  5.19803055e-02 -1.12726074e-02
 -9.38583985e-02  1.64404195e-02 -2.19025850e-01  2.89253056e-01
  7.39060342e-03  1.05856128e-01  2.09254533e-01  5.70165217e-02
 -2.49429226e-01 -2.08258599e-01 -9.44932327e-02 -3.22505161e-02
  7.14507475e-02  8.15603137e-02 -3.01583052e-01 -8.73475969e-02
 -5.97435236e-03  1.96861774e-01  4.41032588e-01  7.93543458e-03
  1.74525380e-02  1.94999561e-01 -7.79257268e-02  1.43625066e-01
 -2.46486142e-01 -3.17703217e-01 -2.62121975e-01  1.48469247e-02
  1.06651150e-03  9.77258459e-02  6.25102520e-02 -1.00154042e-01
 -1.15682177e-01 -1.27935693e-01 -1.28231030e-02  4.68399376e-03
 -1.15883693e-01 -1.57527521e-01 -1.25115708e-01 -4.42516245e-02
 -6.33928657e-01 -5.34309864e-01 -1.42273933e-01  4.59123552e-02
  1.15601152e-01 -9.12324637e-02  4.14562598e-02  1.04387134e-01
  2.27244437e-01 -5.21189626e-03  1.40482664e-01 -1.53781846e-03
  1.14963487e-01  1.13260165e-01  2.13162303e-01 -1.32562518e-01
  6.01156116e-01 -3.29315186e-01  2.24915668e-01 -3.36664869e-03
 -4.42181587e-01  2.16785967e-01 -3.45733613e-02  1.77673876e-01
 -1.87713057e-02 -1.41567625e-02  2.44134218e-01 -3.69945079e-01
  5.38503081e-02 -2.83454508e-01  5.80734499e-02 -1.86174482e-01
  1.92377955e-01 -3.35756838e-02  2.64418751e-01  1.52955368e-01
  3.08989406e-01 -2.04531159e-02  5.31865180e-01  8.12944993e-02
  2.38557123e-02  3.01792398e-02  3.28781903e-01  1.92421108e-01
  7.09136352e-02  8.72976184e-02  6.20197505e-04 -1.43772677e-01
  5.51134720e-02 -3.03710699e-01  2.74270047e-02  1.34937197e-01
 -1.56277746e-01 -1.50083020e-01  7.47759864e-02  5.54795340e-02
  6.10899851e-02  1.02277398e-01  1.52989745e-01 -3.05426866e-03
  2.31530041e-01 -1.23359554e-01 -5.96297383e-02  2.08727065e-02
 -2.15436563e-01  4.37610596e-02 -6.23319671e-02  6.83681250e-01
  2.32168719e-01 -9.24312398e-02  6.28277734e-02  1.30751729e-01
  5.44102490e-01  1.34816974e-01 -2.05996469e-01  1.23617083e-01
  5.96856326e-02 -2.30780505e-02  6.12513125e-02  5.70605360e-02
  2.95757540e-02  1.48159295e-01 -5.24132997e-02  1.33075178e-01
 -3.05553705e-01  4.67758290e-02 -7.28468895e-02 -1.48311228e-01
 -2.27692619e-01  1.63655192e-01 -2.40996078e-01 -5.49283922e-01
  1.31828085e-01  9.82150510e-02 -1.19678214e-01  2.72430211e-01
 -2.45578423e-01 -1.85750186e-01  1.85644291e-02  2.09043592e-01
 -3.12777698e-01  5.00220418e-01  9.72898155e-02  1.90426648e-01
  4.31547225e-01  6.23144284e-02 -4.81729731e-02 -5.30062377e-01
  4.13437597e-02  2.86767781e-01 -5.35200723e-03 -1.91549614e-01
  2.73096263e-01 -3.74779552e-02 -2.90599734e-01 -2.14335501e-01
  7.42114261e-02  3.18682581e-01 -2.89078474e-01 -6.17414750e-02
  8.26616064e-02  8.19913670e-03  2.23229855e-01 -5.60852885e-02
  1.24521203e-01 -4.91885573e-01 -8.92085768e-03  2.63979733e-01
  1.77910015e-01  6.56207949e-02 -8.09953082e-03  3.26872826e-01
  2.24669695e-01 -1.26689812e-03  1.65283442e-01  1.46024197e-01
 -2.28497952e-01  1.08590901e-01 -3.33012730e-01 -1.88875467e-01
  4.85922515e-01 -1.19403087e-01 -1.01540059e-01  3.58559750e-03
  2.14768812e-01  2.19226643e-01  9.21291411e-02  2.42156476e-01
  5.22387736e-02 -1.94741815e-01 -1.02247164e-01  7.28721768e-02
 -2.28822380e-02 -8.48899111e-02 -1.82986304e-01 -4.49605405e-01
 -3.80281150e-01  1.91989645e-01  1.29797086e-02 -7.13821650e-01
  2.43941806e-02 -4.54325713e-02 -2.85489738e-01  3.08868200e-01
 -5.23221679e-03  6.47833273e-02 -5.34421280e-02  2.37868920e-01
  9.34162885e-02 -5.50384782e-02  6.28700256e-02 -3.29933763e-01
 -3.93418342e-01  2.97530089e-02 -2.27496117e-01  3.77665125e-02
 -3.22626531e-03  1.42156199e-01  1.80952959e-02  6.19572848e-02
  4.53803301e-01  1.95816517e-01  3.03879321e-01 -2.64697080e-03
 -7.97443688e-02 -2.05404788e-01  1.15621731e-01 -3.98136768e-03
 -2.58081853e-01 -7.23520666e-02 -3.80396619e-02 -6.77992404e-02
  2.44070113e-01  3.42042744e-01 -2.15159491e-01  2.60788016e-03
 -2.22801149e-01  6.22244924e-02 -2.88547575e-01 -1.23803969e-02
  3.65004957e-01  1.03330888e-01  2.62852311e-01  1.46008670e-01
 -2.01465432e-02 -3.76338605e-03  2.44884551e-01 -2.13206023e-01
  2.56767154e-01  2.00657457e-01  1.15289293e-01  4.10824716e-01
  1.62625283e-01  1.56360686e-01 -3.73188287e-01  4.61494863e-01
  2.43961215e-01 -1.72537893e-01  1.60424605e-01 -3.47809613e-01
  5.97519934e-01 -2.62757182e-01  6.87086135e-02  1.12014763e-01
  2.87998021e-01 -5.66481650e-02 -7.59205222e-02  2.33374059e-01
 -2.17673779e-02  3.18626702e-01 -1.54801443e-01  6.74811751e-02
 -1.66066349e-01 -2.38930434e-01 -2.50545472e-01 -5.54761291e-01
 -1.44783765e-01 -1.99892204e-02 -2.15093315e-01  1.19632147e-02
 -4.63843085e-02  2.35161111e-02 -1.48616284e-01  2.06456840e-01
  1.19844496e-01 -2.28604034e-01  1.41377687e-01  4.00167555e-02
 -1.34532154e-03  2.20626771e-01  2.95436442e-01 -3.40961158e-01
 -1.46918476e-01 -6.79737926e-02  4.62635815e-01  1.87199004e-02
  4.26026285e-01 -3.93394649e-01 -1.66254282e-01  2.44801268e-01
 -1.26975954e-01  3.62893403e-01 -1.51732385e-01 -7.10361451e-03
 -1.23041123e-01  4.37054336e-01  1.26703650e-01  2.36408412e-02
  1.31629467e-01 -3.49302381e-01 -1.13796636e-01  1.67870283e-01
  4.37902063e-01 -5.95977977e-02 -1.37130141e-01 -1.87118977e-01
  4.33083512e-02  1.83122367e-01 -1.67607069e-02  6.15261262e-03
 -2.08777368e-01  1.07981719e-01 -3.73382419e-02 -6.56117797e-02
 -3.73046637e-01  2.84279525e-01 -9.55903605e-02 -1.90192461e-01
 -3.23274076e-01 -2.84687787e-01  6.39397427e-02 -3.45750272e-01
 -1.94041938e-01 -3.20450068e-01  2.12375835e-01  2.77821124e-01
 -5.30650318e-02  1.10096917e-01 -1.46069527e-01  9.47261900e-02
 -4.54028785e-01 -2.86454987e-02 -5.70481643e-02  3.35921824e-01
  7.60510266e-02 -6.06515184e-02  3.07373405e-01  7.91777074e-02
 -1.08503401e-01  8.12149271e-02 -2.99837172e-01  1.32142633e-01
 -3.63875367e-02 -1.45780772e-01  2.34045479e-02 -3.74869943e-01
 -2.82922871e-02  2.00930297e-01 -1.59695357e-01 -5.91833629e-02
 -3.80195200e-01  3.11809748e-01  5.58828294e-01 -3.73277903e-01
 -5.28165745e-03 -5.02417870e-02  9.03541148e-02  1.91634715e-01
 -4.87204120e-02 -6.91100284e-02  3.81636918e-02 -1.27969123e-03]"
Invalid classifier used for CUDA 12.2 type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Uploads to PyPi fail with invalid classifier.

Introduced by https://github.com/tensorflow/tensorflow/commit/c3b98ea5a8387eec21e808caa6e999417494e09a

### Standalone code to reproduce the issue

```shell
python3 -m twine upload --verbose /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/whl/* -u ""__token__"" -p ***
```


### Relevant log output

```shell
INFO     Response from https://upload.pypi.org/legacy/:                         
         400 Invalid value for classifiers. Error: Classifier 'Environment ::   
         GPU :: NVIDIA CUDA :: 12.2' is not a valid classifier.                 
INFO     <html>                                                                 
          <head>                                                                
           <title>400 Invalid value for classifiers. Error: Classifier          
         'Environment :: GPU :: NVIDIA CUDA :: 12.2' is not a valid             
         classifier.</title>                                                    
          </head>                                                               
          <body>                                                                
           <h1>400 Invalid value for classifiers. Error: Classifier 'Environment
         :: GPU :: NVIDIA CUDA :: 12.2' is not a valid classifier.</h1>         
           The server could not comply with the request since it is either      
         malformed or otherwise incorrect.<br/><br/>                            
         Invalid value for classifiers. Error: Classifier &#x27;Environment ::  
         GPU :: NVIDIA CUDA :: 12.2&#x27; is not a valid classifier.            
                                                                                
                                                                                
          </body>                                                               
         </html>                                                                
ERROR    HTTPError: 400 Bad Request from https://upload.pypi.org/legacy/        
         Invalid value for classifiers. Error: Classifier 'Environment :: GPU ::
         NVIDIA CUDA :: 12.2' is not a valid classifier.
```
",True,"[-0.4920479  -0.52923465 -0.37554646  0.05837401  0.13454056 -0.40151095
  0.00173499  0.10800464 -0.44986662 -0.30023313  0.18415293 -0.20593613
 -0.1329417   0.06479379 -0.26485473  0.24213877 -0.05837511  0.13351782
  0.16752598  0.0696708  -0.1699722   0.1969674  -0.35176915 -0.01414723
  0.32576495  0.161692   -0.29660785 -0.28217137  0.21410692 -0.01351338
  0.3861958   0.09740663  0.09832053  0.24521436 -0.01802422  0.21723208
 -0.31616074 -0.4554999  -0.06520461  0.00688267 -0.26148707 -0.00767559
  0.21325892 -0.15226391 -0.01869623 -0.33580953 -0.04952913 -0.07710153
 -0.05625837 -0.4220025   0.32137674 -0.12320152 -0.40690392 -0.15815122
 -0.01881351  0.09096545  0.09123063 -0.10074518 -0.12166364  0.10977325
  0.09815085 -0.10977173  0.00991394 -0.19158837 -0.07066231  0.23780635
  0.16371827 -0.03275726  0.59544903 -0.24055602  0.04387347  0.12973268
 -0.15784556  0.13213798  0.17259204  0.01902608  0.16244087  0.41906518
  0.15891705  0.02424654  0.19310938  0.0535818   0.11276659 -0.40312368
  0.22674434 -0.15984839  0.27313438  0.11282425  0.3956843  -0.09059054
  0.3340525   0.41884997  0.09235019  0.12446988  0.30401003 -0.09251211
 -0.00240736  0.09252139 -0.04634055 -0.15395075 -0.38037533 -0.22222036
  0.01103677  0.37594253 -0.17720379 -0.19386089  0.21410075 -0.14191717
 -0.2009156  -0.28337836  0.12585601 -0.06301078  0.09491847 -0.1383712
 -0.00961435 -0.16321842 -0.2718193  -0.38015187  0.06223391  0.4690574
 -0.00141798 -0.11252467  0.1054645   0.33020562  0.3935219   0.12146859
  0.07530885 -0.13532762 -0.2018195  -0.08006918  0.13697056  0.02249999
  0.45586458  0.34399015 -0.1038392   0.19482848 -0.48283726 -0.19181636
 -0.14630087 -0.2605042  -0.22588879  0.47155392  0.12565878 -0.4543534
  0.15669538  0.12256814 -0.26564908  0.35679525  0.01840414  0.4172889
 -0.04961783  0.11295126 -0.25363597  0.51688826  0.04479894  0.15943684
  0.46244138 -0.21788943  0.24090208 -0.8038416   0.2733637   0.3891182
 -0.01005679 -0.11648873  0.16775881  0.24950892 -0.3933591  -0.31769022
  0.10397151  0.5206219  -0.39036214 -0.0816937   0.37934816 -0.07199256
  0.16056105 -0.28455848  0.06141478 -0.62193584 -0.02542474  0.5255999
 -0.16660099  0.26690695  0.22928292  0.03431116  0.17709878  0.45072258
  0.17923722  0.19197708 -0.24170163 -0.05819437 -0.5573493  -0.17371893
 -0.00466001 -0.34187183  0.05768875  0.08458694  0.25806195  0.09065612
 -0.11829517 -0.03239483 -0.53212035 -0.01560095  0.02384812  0.22087774
  0.10137356 -0.2621757  -0.16843247 -0.34725606 -0.34453267  0.3728737
 -0.11193119 -0.36013293 -0.10529695 -0.05771596 -0.34562695  0.1950192
  0.2016399   0.27277946 -0.21693829  0.31580928  0.2677261  -0.25654668
  0.31988195 -0.32633582  0.18050322  0.20143601 -0.31857044 -0.03619476
 -0.10983127  0.02154284  0.00704609  0.02609986  0.00893424  0.19937442
  0.3693146   0.05701518 -0.07364704 -0.0526199   0.08167961  0.22846714
 -0.63653135 -0.04118556 -0.1913343  -0.00553529  0.30047828  0.6431844
  0.02647696 -0.04549505 -0.587983   -0.01681736 -0.26708063  0.15892553
  0.39974242 -0.05961293  0.31127927  0.06868301  0.3262314   0.19785242
 -0.10437429 -0.13280508  0.23082855  0.1601172  -0.02025323  0.5218145
  0.24608904  0.3756608  -0.37861365  0.4727604  -0.17068869 -0.14020059
  0.29897764 -0.14579883  0.7183888  -0.4758548   0.07601286 -0.12140253
  0.31040406 -0.26098818  0.09115896 -0.09413249  0.29169565 -0.01667192
 -0.18365648  0.13848124  0.16970512 -0.3062836  -0.21084596 -0.4962804
 -0.25983742 -0.2651951  -0.3634217   0.46280885 -0.02473372 -0.08976826
 -0.22283083 -0.04965351  0.17434718 -0.03484103  0.05924142 -0.05166852
 -0.38430783 -0.01688066  0.21104029 -0.72863036 -0.35182437 -0.26395535
  0.4655315   0.30246994  0.3832191  -0.4799512   0.36047524 -0.2493847
 -0.33869016  0.64030176 -0.05614332 -0.07765908 -0.26404524  0.54118
  0.22818115 -0.01711125 -0.15681985 -0.37943187 -0.40217322  0.02683616
  0.219618   -0.09378016 -0.17289117 -0.08591628 -0.01090381  0.4981095
 -0.12804364  0.00148315 -0.11985148  0.10478757 -0.19286206  0.2302241
 -0.27144885  0.21443698  0.1854355  -0.15222505 -0.28704533  0.16004032
 -0.31625172 -0.12840605 -0.01412075 -0.3123303   0.4916901   0.61991245
  0.02168618  0.09965768  0.03622898  0.23525715 -0.23089129 -0.22221798
 -0.0522095   0.6401732   0.2699561  -0.36730897  0.37717146 -0.0039749
  0.02297945  0.12601957 -0.28944677 -0.05990382 -0.05347433 -0.14351112
 -0.15576832 -0.34231687  0.18602632  0.2979595  -0.3642645  -0.05905939
 -0.3107109   0.43305665  0.70218027 -0.32887506 -0.292897   -0.09802896
  0.23336814  0.07236621 -0.0766235   0.03182529  0.43022984 -0.06041652]"
Internal Error for truncatediv when denominator is an integer zero. stat:awaiting response type:bug stale comp:ops TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.truncatediv raises internal error when denominator is an integer zero. If the denominator is a float zero, this function works normally.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant(33, dtype=""int32"")
y = tf.constant(0, dtype=""int32"")
print(x, y)
print(tf.truncatediv(x,y))
```


### Relevant log output

_No response_",True,"[-0.49400875 -0.05729748 -0.06941143  0.08189838  0.23378488 -0.30065182
 -0.07058569  0.23730159 -0.3216027  -0.34312195  0.1900638  -0.10366986
 -0.1776632  -0.06112454 -0.08783955  0.18602112 -0.25980592 -0.10962596
  0.20906809  0.18929659 -0.11795117  0.12743936 -0.49306896  0.19613293
  0.30881763  0.1966174  -0.30392826  0.14398944 -0.10245338  0.30317652
  0.3243658   0.14274748  0.15661652  0.24643376  0.17909026  0.1900056
 -0.27520514 -0.18513386 -0.48812583  0.09824643 -0.11050272  0.18975927
  0.01242639 -0.1172845   0.07454261 -0.02347474 -0.06247582 -0.22384536
  0.04561616 -0.18961537  0.01439389  0.29086226 -0.402589   -0.27594942
 -0.0112985  -0.14918229  0.02077938  0.00952841 -0.09172651  0.17128721
  0.20778793 -0.1990048  -0.07166393 -0.23201473  0.1997256   0.07467981
  0.41048348 -0.20924124  0.4387579  -0.35961455  0.20808999 -0.20900059
 -0.34241015  0.05628154 -0.1105285   0.0976945  -0.33839655  0.07593174
  0.23197615 -0.23671862  0.02912579 -0.25495705 -0.32579446 -0.20311213
  0.14959925 -0.20484608  0.3045676   0.10962525  0.433058   -0.1851781
  0.49783182  0.38236344 -0.11416864  0.16388716  0.37355986  0.06242891
  0.11072603  0.08109401 -0.05578977 -0.03637973 -0.07985745 -0.15264007
 -0.33952746  0.09178156 -0.35491422 -0.16684613 -0.04128907 -0.04345258
  0.0823666   0.05407805  0.20061812 -0.00681502  0.2683636  -0.00108303
  0.01025321 -0.00202713  0.14801434  0.03077871 -0.01105165  0.5015069
  0.27965528  0.06611259 -0.14379549  0.500265    0.53753257  0.0482807
 -0.11894147  0.07763758  0.34977975 -0.06027236  0.23020463 -0.08505602
 -0.18891709  0.22408518 -0.21631952  0.06238199  0.08991457 -0.10135417
 -0.26992458 -0.11850813 -0.11801114 -0.02308907 -0.27808404 -0.74433935
  0.11201517  0.13349324 -0.1716165   0.24505565 -0.20396955  0.09793442
 -0.07796086  0.18902488 -0.30978262  0.40245265 -0.02523153 -0.0256173
  0.5043601  -0.03196175  0.06607348 -0.5566423   0.06273717  0.36558264
 -0.05194768 -0.21523541  0.20453176  0.11359303 -0.5451801  -0.30931726
  0.07950222  0.44040275 -0.23874599 -0.25448382 -0.04889321 -0.14346103
  0.08070023  0.02720048  0.25381356 -0.33419612 -0.20507531  0.40891635
  0.1267981   0.09767416 -0.00391345  0.19865891  0.05232572  0.00435136
 -0.05465512  0.17424259 -0.36126578 -0.08967547 -0.58429456 -0.20487455
  0.3605532   0.03250439  0.02938675 -0.01608218  0.1938139  -0.13390723
  0.1522181   0.06463636 -0.09745842  0.14522785 -0.05631965 -0.06102595
  0.20133172 -0.16363989 -0.06894943 -0.5575117  -0.33222455  0.19817966
  0.03292359 -0.5221214   0.21446173 -0.17366691 -0.36411643  0.36233884
 -0.00146456  0.02745306 -0.14569692  0.06970934  0.10884998 -0.2206662
  0.08341033 -0.3882653  -0.2204936   0.13055502 -0.62757325  0.04951537
  0.0070248   0.08375008  0.18338124  0.24295926  0.39814425  0.26105562
  0.39084795 -0.07417172 -0.14278826 -0.04912422  0.01605778  0.06600253
 -0.5367569  -0.23602146  0.03229456  0.02035045  0.4408101   0.43637213
 -0.07157908 -0.13246952 -0.31342527  0.2062077  -0.16801752  0.33314624
  0.29947254 -0.06175682  0.47553965  0.09187119  0.19961572  0.25723237
  0.14374712 -0.2873187   0.23856819  0.06865537  0.14496037  0.44757336
  0.10311069  0.28748038 -0.24768193  0.5570489   0.22332506 -0.24063376
  0.298619   -0.34653485  0.73395085 -0.26112694  0.07537867 -0.13313422
  0.38862163  0.01336916 -0.01828109  0.06948313 -0.05181594  0.29708192
 -0.35158485  0.12441915  0.1384065  -0.21367496 -0.10625153 -0.63852245
 -0.3067668   0.01739336 -0.1736676   0.02447649 -0.02872047  0.06918566
 -0.39937913  0.21556517  0.11267655 -0.2575839   0.17667618  0.22011146
 -0.10583015  0.20294118  0.3508657  -0.5031662  -0.24471542 -0.12725106
  0.5729053   0.25171104  0.64998406 -0.4717025  -0.12122883  0.05245819
 -0.01022538  0.5463855  -0.16609238 -0.07218576 -0.26490378  0.6419717
  0.14746258 -0.00730968  0.26795346 -0.11765482 -0.29425836  0.16115895
  0.4506811  -0.05687047 -0.04708156 -0.442451    0.19584668  0.02590095
 -0.03394631  0.10600415 -0.23719251 -0.14585188 -0.16803446 -0.29107562
 -0.26405218  0.29587832 -0.07731096 -0.33737054 -0.32334134 -0.1346016
 -0.2597258  -0.26889807 -0.03311879 -0.33267194  0.29288697  0.5196506
 -0.12897052  0.19204827  0.09392942  0.09265266 -0.12247676 -0.04233021
 -0.17648163  0.392977    0.264538    0.02285062  0.362366    0.25094944
 -0.4221943   0.26953876 -0.5193661   0.05561856  0.0790199  -0.24751908
 -0.25071254 -0.29052302 -0.03064726  0.18478851 -0.20852108  0.27142152
 -0.37076226  0.14397782  0.4041413  -0.39985627 -0.02588234  0.07327305
  0.1393359  -0.05639189  0.0281947   0.01086317  0.11482903  0.03691243]"
Errors when building v2.14 from source - ubuntu 20.04 / arm64 / GPU + CUDA (capabilities 8.7) / clang 16 stat:awaiting response type:bug type:build/install subtype: ubuntu/linux TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.14.0

### Custom code

No

### OS platform and distribution

Ubuntu 20.04.6

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

6.1.0

### GCC/compiler version

clang 16.0.6

### CUDA/cuDNN version

11.4 / 8.6.0

### GPU model and memory

jetson orin agx  -> nvidia ampere

### Current behavior?

Attempting to build TF from source
- Jetson Orin w Ampere GPU, arm64
- TF 2.14.0
- bazel 6.1.0
- python 3.10

`./configure`
- no ROCm
- yes CUDA (capabilities = 8.7)
- no TensorRT
- using clang 16 as cuda compiler

`bazel build //tensorflow/tools/pip_package:build_pip_package`

Resultng behavior: build fails, with the following error:
```
.../tensorflow/core/kernels/BUILD:4996:18: Compiling tensorflow/core/kernels/sparse_reorder_op_gpu.cu.cc failed: (Exit 1): clang failed: error executing command (from target
 //tensorflow/core/kernels:sparse_reorder_op_gpu) /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/aarch64-opt/bin/tensorflow/core/kernels/_objs/sparse_reorder_op_gpu/sparse_reorder_op_gpu.cu.pic.d ... (remaining 192 a
rguments skipped)
In file included from tensorflow/core/kernels/sparse_reorder_op_gpu.cu.cc:21:
In file included from ./tensorflow/core/kernels/gpu_prim.h:24:
In file included from bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/device_radix_sort.cuh:40:
In file included from bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/dispatch/dispatch_radix_sort.cuh:40:
In file included from bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/dispatch/../../agent/agent_radix_sort_histogram.cuh:38:
bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/dispatch/../../agent/../block/radix_rank_sort_operations.cuh:124:20: error: explicit qualification required to use member 'ProcessFl
oatMinusZero' from dependent base class
        return BFE(ProcessFloatMinusZero(key), bit_start, num_bits);
                   ^
 ...
 
 bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/device_radix_sort.cuh:168:65: note: in instantiation of member function 'cub::DispatchRadixSort<false, long, long, int>::Dispatch' r
equested here
        return DispatchRadixSort<false, KeyT, ValueT, OffsetT>::Dispatch(
                                                                ^
./tensorflow/core/kernels/gpu_prim_helpers.h:117:37: note: in instantiation of function template specialization 'cub::DeviceRadixSort::SortPairs<long, long>' requested here
    err = gpuprim::DeviceRadixSort::SortPairs(
                                    ^
./tensorflow/core/kernels/gpu_prim_helpers.h:159:18: note: in instantiation of function template specialization 'tensorflow::detail::GpuRadixSortImpl<false, long, long>' requested here
  return detail::GpuRadixSortImpl</*Descending=*/false>(
                 ^
tensorflow/core/kernels/sparse_reorder_op_gpu.cu.cc:104:12: note: in instantiation of function template specialization 'tensorflow::GpuRadixSort<long, long>' requested here
        c, GpuRadixSort(c, num_elems, /*keys_in=*/flat_indices,
           ^
bazel-out/aarch64-opt/bin/external/local_config_cuda/cuda/cuda/include/cub/device/dispatch/../../agent/../block/radix_rank_sort_operations.cuh:98:52: note: member is declared here
    static __device__ __forceinline__ UnsignedBits ProcessFloatMinusZero(UnsignedBits key)
                                                   ^
2 errors generated when compiling for sm_87.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9646.040s, Critical Path: 487.26s
INFO: 17934 processes: 6479 internal, 11455 local.
FAILED: Build did NOT complete successfully
```

### Standalone code to reproduce the issue

```shell
see below
```


### Relevant log output

```shell
<someuser>:tensorflow   git status
HEAD detached at v2.14.0
nothing to commit, working tree clean
<someuser>:tensorflow   ./configure
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /home/someuser/.pyenv/versions/3.10.13/bin/python3]:


Found possible Python library paths:
  /home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages
Please input the desired Python library path to use.  Default is [/home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]:
No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.4 in:
    /usr/local/cuda-11.4/targets/aarch64-linux/lib
    /usr/local/cuda-11.4/targets/aarch64-linux/include
Found cuDNN 8 in:
    /usr/lib/aarch64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.7


Do you want to use clang as CUDA compiler? [Y/n]:
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-16/bin/clang]:


You have Clang 16.0.6 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
<someuser>:tensorflow   bazel build //tensorflow/tools/pip_package:build_pip_package
WARNING: while reading option defaults file '/home/someuser/projects/other/tensorflow/.bazelrc':
  invalid command name 'startup:windows'.
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=214
INFO: Reading rc options for 'build' from /home/someuser/projects/other/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/someuser/projects/other/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/someuser/projects/other/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/someuser/.pyenv/versions/3.10.13/bin/python3 --action_env PYTHON_LIB_PATH=/home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages --python_path=/home/someuser/.pyenv/versions/3.10.13/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.4 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.7 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-16/bin/clang --copt=-Wno-gnu-offsetof-extensions --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /home/someuser/projects/other/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/someuser/projects/other/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /home/someuser/projects/other/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/someuser/projects/other/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda_clang in file /home/someuser/projects/other/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/someuser/projects/other/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/someuser/projects/other/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/someuser/projects/other/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/668e33c6401abe7844691fb7d47a3cf2d2012dbc.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/tensorflow/runtime/archive/769f5cc9b8732933140b09e8808d13614182b496.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: while reading option defaults file '/home/someuser/projects/other/tensorflow/.bazelrc':
  invalid command name 'startup:windows'.
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Build options --copt and --host_copt have changed, discarding analysis cache.
WARNING: Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/stablehlo/archive/9ae6c373a6e2941ff84a8831bb3724728cb2b49a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/pytorch/cpuinfo/archive/87d8234510367db49a65535021af5e1838a65ac2.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/XNNPACK/archive/b9d4073a6913891ce9cbd8965c8d506075d2a45a.zip failed: class java.io.FileNotFoundException GET returned 404 Not Found
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/openxla/triton/archive/cl546794996.tar.gz failed: class java.io.FileNotFoundException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (1 packages loaded, 47122 targets configured).
INFO: Found 1 target...
[1,765 / 6,027] 8 actions running
[1,802 / 6,027] 8 actions running
    Compiling mlir/lib/Dialect/Arith/IR/ArithOps.cpp [for tool]; 41s local
    Compiling mlir/lib/Dialect/Arith/IR/ArithDialect.cpp [for tool]; 40s local
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: build interrupted
INFO: Elapsed time: 195.807s, Critical Path: 101.10s
INFO: 301 processes: 15 internal, 286 local.
FAILED: Build did NOT complete successfully

<someuser>:tensorflow 
<someuser>:tensorflow 

<someuser>:tensorflow 
<someuser>:tensorflow   reset

<someuser>:tensorflow   bazel clean
WARNING: while reading option defaults file '/home/someuser/projects/other/tensorflow/.bazelrc':
  invalid command name 'startup:windows'.
WARNING: The following configs were expanded more than once: [cuda_clang, cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=214
INFO: Reading rc options for 'clean' from /home/someuser/projects/other/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'clean' from /home/someuser/projects/other/tensorflow/.bazelrc:
  Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'clean' from /home/someuser/projects/other/tensorflow/.tf_configure.bazelrc:
  Inherited 'build' options: --action_env PYTHON_BIN_PATH=/home/someuser/.pyenv/versions/3.10.13/bin/python3 --action_env PYTHON_LIB_PATH=/home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages --python_path=/home/someuser/.pyenv/versions/3.10.13/bin/python3 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.4 --action_env TF_CUDA_COMPUTE_CAPABILITIES=8.7 --config=cuda_clang --action_env CLANG_CUDA_COMPILER_PATH=/usr/lib/llvm-16/bin/clang --copt=-Wno-gnu-offsetof-extensions --config=cuda_clang
INFO: Found applicable config definition build:short_logs in file /home/someuser/projects/other/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/someuser/projects/other/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda_clang in file /home/someuser/projects/other/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/someuser/projects/other/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda_clang in file /home/someuser/projects/other/tensorflow/.bazelrc: --config=cuda --repo_env TF_CUDA_CLANG=1 --@local_config_cuda//:cuda_compiler=clang
INFO: Found applicable config definition build:cuda in file /home/someuser/projects/other/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/someuser/projects/other/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/someuser/projects/other/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.
<someuser>:tensorflow   reset
'





















<someuser>:tensorflow   git status
HEAD detached at v2.14.0
nothing to commit, working tree clean
<someuser>:tensorflow   bazel --version
bazel 6.1.0
<someuser>:tensorflow   clang-16 --version
Ubuntu clang version 16.0.6 (++20230710042046+7cbf1a259152-1~exp1~20230710162136.105)
Target: aarch64-unknown-linux-gnu
Thread model: posix
InstalledDir: /usr/bin
<someuser>:tensorflow   ./configure
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /home/someuser/.pyenv/versions/3.10.13/bin/python3]:


Found possible Python library paths:
  /home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages
Please input the desired Python library path to use.  Default is [/home/someuser/.pyenv/versions/3.10.13/lib/python3.10/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]:
No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.4 in:
    /usr/local/cuda-11.4/targets/aarch64-linux/lib
    /usr/local/cuda-11.4/targets/aarch64-linux/include
Found cuDNN 8 in:
    /usr/lib/aarch64-linux-gnu
    /usr/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 8.7


Do you want to use clang as CUDA compiler? [Y/n]:
Clang will be used as CUDA compiler.

Please specify clang path that to be used as host compiler. [Default is /usr/lib/llvm-16/bin/clang]:


You have Clang 16.0.6 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl            # Build with MKL support.                                                                                                                                                    [12/7774]
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
<someuser>:tensorflow   bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package



/////////////////////////
```
",True,"[-4.51587647e-01 -4.16162193e-01 -1.52878910e-01 -4.82141040e-04
  1.97120279e-01 -5.50940752e-01 -2.34404519e-01  1.00726224e-01
 -2.84558237e-01 -2.97895402e-01  1.30389825e-01  4.24510092e-02
 -8.03485960e-02  2.96230614e-03 -2.31679991e-01  4.38427508e-01
 -1.67469889e-01 -5.63057028e-02  2.40116745e-01  2.62353659e-01
 -3.76935601e-01 -9.24757570e-02 -3.59467328e-01  1.02747623e-02
  2.72849500e-01  2.00681448e-01 -3.08484077e-01  8.68310854e-02
  2.42963791e-01  1.10219210e-01  5.91055512e-01  1.93705291e-01
 -5.14741167e-02  1.42384976e-01  2.20959723e-01  1.96665704e-01
 -2.66229182e-01 -2.92856097e-01 -1.09919310e-01 -6.03753552e-02
 -9.35517699e-02  4.16412428e-02  7.34369010e-02 -2.36598611e-01
 -1.20542496e-02 -1.98182017e-01  1.27700821e-01  4.68385965e-03
  4.51988168e-02 -5.56198478e-01  1.15669042e-01  7.76172336e-03
 -2.75211126e-01 -3.59078407e-01 -1.16058558e-01  1.48109928e-01
 -6.86886087e-02  5.10210767e-02  4.02938090e-02  2.46341601e-01
  5.23212016e-01 -5.60157113e-02  1.79498456e-02 -1.71415985e-01
  6.34387136e-02  1.64318815e-01  2.25853458e-01 -1.01282105e-01
  4.59836572e-01 -2.67449766e-01  7.04461336e-02 -3.01675908e-02
 -5.98271117e-02 -3.11022475e-02  1.29522741e-01  1.84477121e-01
  2.15879500e-01  1.59873754e-01  1.51289567e-01 -8.99948701e-02
  2.05640327e-02  2.07381081e-02  4.81063128e-02 -2.33352363e-01
  2.50824273e-01 -6.70666695e-02  2.56476611e-01  2.75265753e-01
  3.65944743e-01 -3.77092034e-01  3.91054869e-01  5.04744291e-01
 -2.42223293e-02  1.22512385e-01  2.88603932e-01  4.28401381e-02
  8.82161111e-02  1.04317516e-01 -2.17849135e-01 -1.08085185e-01
 -2.11391866e-01 -9.58292708e-02  1.54949069e-01  3.50017190e-01
 -4.65406239e-01 -1.22148991e-01  1.21252924e-01  2.02326458e-02
 -7.59825334e-02 -6.87283427e-02  1.63515300e-01  4.71188352e-02
  7.29474053e-03  7.77754188e-02 -7.43455663e-02  1.14601165e-01
 -4.07509148e-01 -2.80911386e-01 -9.44740325e-02  4.93537724e-01
 -2.46199429e-01 -3.04517210e-01 -1.84573084e-02  2.99607273e-02
  4.83862579e-01  6.82962239e-02 -1.04996569e-01 -2.59154826e-01
  7.25500360e-02  3.13685536e-01 -8.87147412e-02  1.04996964e-01
  2.29552433e-01  2.14722872e-01 -8.72754231e-02  7.90575445e-02
 -1.17427751e-01  2.50902120e-03 -1.35001630e-01 -1.76296860e-01
 -1.16345689e-01  3.35328579e-01  1.50493264e-01 -4.21937466e-01
  1.06114462e-01 -6.73098788e-02 -1.50400788e-01  1.09169975e-01
  4.41336520e-02  1.50222793e-01 -2.16954425e-02  1.61893055e-01
 -1.95264131e-01  5.05990565e-01  6.48187250e-02  1.56163886e-01
  4.15425837e-01 -1.54184133e-01 -1.19578332e-01 -6.90896630e-01
  2.28487268e-01  4.56880093e-01  7.88106117e-03 -2.09179461e-01
 -3.99127603e-02  5.68001159e-02 -2.59892106e-01 -3.75588655e-01
  2.19198182e-01  5.41141152e-01 -1.76029891e-01 -1.28238350e-01
  3.35332125e-01  2.86640882e-01  1.51449919e-01 -2.83562005e-01
  2.95578212e-01 -2.94401020e-01 -1.66525602e-01  6.19949818e-01
  1.42737955e-01 -5.22821546e-02  2.25926667e-01  2.09491402e-01
  2.61299133e-01  1.41347766e-01  1.07655548e-01  1.11870147e-01
 -4.59385395e-01 -5.17862067e-02 -3.71535838e-01 -2.45619975e-02
  6.16391003e-02 -1.31152660e-01 -2.68141359e-01  2.44077310e-01
  2.94420481e-01  1.03199249e-02  1.14676774e-01 -3.86447757e-02
 -2.15494484e-01  4.54344042e-03 -1.45728951e-02  1.28465518e-01
 -1.30110420e-02 -3.32584262e-01 -9.33061689e-02 -2.87383020e-01
 -4.52840626e-01 -5.00756465e-02 -7.50823170e-02 -3.46273273e-01
  2.84454912e-01 -1.20006755e-01 -2.23687559e-01  4.73421589e-02
  2.14048252e-01  4.78397356e-03 -2.33054191e-01  3.82327318e-01
  2.33854115e-01 -2.99696505e-01  1.02802843e-01 -2.70772159e-01
  5.56577593e-02 -5.09992354e-02 -2.41673157e-01 -1.52162820e-01
  7.91313499e-02 -1.94428582e-03 -6.52354062e-02  7.91324303e-02
  2.96248823e-01  1.36022702e-01  3.25066149e-01 -5.85003644e-02
 -1.33942693e-01 -1.14456519e-01 -2.54103929e-01  1.86153620e-01
 -4.47317898e-01 -1.29281372e-01  5.92215955e-02  7.84046054e-02
  3.52228343e-01  2.92899549e-01  3.32938954e-02  3.33288983e-02
 -3.88136715e-01  1.49174973e-01  2.96909865e-02 -1.94816798e-01
  2.20378503e-01  1.27030909e-01  2.43304461e-01  2.38150254e-01
  1.82615966e-01  2.31505543e-01  2.15103477e-01 -8.98639485e-02
  1.93414673e-01  3.07209581e-01 -2.70924509e-01  9.33693945e-02
  2.85792768e-01  4.38864321e-01 -3.23410988e-01  4.11781371e-01
 -1.59675747e-01 -1.43243819e-01  3.75741690e-01 -1.85960695e-01
  5.12988567e-01 -4.81368423e-01  2.71826565e-01 -9.06257555e-02
  4.64028835e-01 -8.13177899e-02  1.16737664e-01 -1.09502487e-01
  3.31470191e-01  1.53022915e-01 -2.33116373e-01  2.64588445e-02
  1.11314461e-01 -3.48778456e-01 -4.24266279e-01 -7.45736301e-01
 -3.97138536e-01 -1.27836205e-02 -2.36849785e-01  1.10788509e-01
 -1.09757200e-01 -4.04662490e-02 -3.97731543e-01 -2.26060022e-02
  1.41935259e-01  7.92606995e-02 -4.54640761e-02  5.29114977e-02
 -4.13262248e-01 -2.26312682e-01  4.58362937e-01 -6.34929657e-01
 -1.35404542e-01 -1.54285982e-01  9.41239446e-02  2.37045571e-01
  4.45850790e-01 -4.45437491e-01 -1.82412155e-02  5.59643395e-02
 -3.63811757e-03  3.34653080e-01  1.49609476e-01  8.25785846e-02
 -2.34266251e-01  6.74007058e-01  2.05424592e-01 -2.15333208e-01
  2.24787325e-01 -1.41039252e-01 -2.83276290e-01 -9.38792303e-02
  1.32913589e-01 -1.59765214e-01 -1.56147957e-01 -1.50596589e-01
  1.42237842e-01  3.77452314e-01 -5.86430728e-02  3.92956436e-02
  7.84354359e-02  1.40655011e-01 -3.03965986e-01 -3.42383645e-02
 -2.54228741e-01  1.23346284e-01  1.99560542e-02 -4.11895841e-01
 -1.88319176e-01 -7.51482844e-02 -1.09200507e-01 -2.48576745e-01
 -1.20675966e-01 -2.74191946e-01  3.31693709e-01  2.37197205e-01
 -1.67042449e-01  2.64138341e-01 -1.89429633e-02  2.38892883e-01
 -1.93640769e-01 -2.35295836e-02 -1.43514425e-01  1.33451045e-01
  1.02298513e-01  6.38897866e-02  4.92329061e-01  1.46421254e-01
 -5.28188460e-02  2.87506819e-01 -2.23226383e-01 -3.71986888e-02
  3.84772383e-03 -2.01689392e-01 -3.57038260e-01 -1.39727533e-01
  7.32964426e-02  1.70931995e-01 -6.15805238e-02  1.76422879e-01
 -2.77307391e-01 -2.86880918e-02  5.56720853e-01 -3.16228360e-01
 -4.84153986e-01  1.59375221e-01  1.93690941e-01 -2.08253115e-01
  2.57667471e-02 -8.63034725e-02  2.10237682e-01 -7.73987770e-02]"
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position XX: invalid continuation byte stat:awaiting response type:bug stale comp:apis TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Windows 11 Home 22H2

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I've created a model in google colab: [here](https://colab.research.google.com/drive/1776CZxkKcetQsPkJfMACWnNel3Hj-hbA?usp=sharing). Then downloaded it and loaded few times on my computer. However after these times it stopped to load by the function `tf.keras.saving.load_model(model_name)` and started to drop this error: 

```
---------------------------------------------------------------------------
UnicodeDecodeError                        Traceback (most recent call last)
Cell In[3], line 9
      5 import time
      6 from random import sample
----> 9 model = tf.keras.saving.load_model(' ')

File C:\Program Files\Python39\lib\site-packages\keras\src\saving\saving_api.py:262, in load_model(filepath, custom_objects, compile, safe_mode, **kwargs)
    254     return saving_lib.load_model(
    255         filepath,
    256         custom_objects=custom_objects,
    257         compile=compile,
    258         safe_mode=safe_mode,
    259     )
    261 # Legacy case.
--> 262 return legacy_sm_saving_lib.load_model(
    263     filepath, custom_objects=custom_objects, compile=compile, **kwargs
    264 )

File C:\Program Files\Python39\lib\site-packages\keras\src\utils\traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File C:\Program Files\Python39\lib\site-packages\h5py\_hl\files.py:562, in File.__init__(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)
    553     fapl = make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,
    554                      locking, page_buf_size, min_meta_keep, min_raw_keep,
    555                      alignment_threshold=alignment_threshold,
    556                      alignment_interval=alignment_interval,
    557                      meta_block_size=meta_block_size,
    558                      **kwds)
    559     fcpl = make_fcpl(track_order=track_order, fs_strategy=fs_strategy,
    560                      fs_persist=fs_persist, fs_threshold=fs_threshold,
    561                      fs_page_size=fs_page_size)
--> 562     fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)
    564 if isinstance(libver, tuple):
    565     self._libver = libver

File C:\Program Files\Python39\lib\site-packages\h5py\_hl\files.py:235, in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)
    233     if swmr and swmr_support:
    234         flags |= h5f.ACC_SWMR_READ
--> 235     fid = h5f.open(name, flags, fapl=fapl)
    236 elif mode == 'r+':
    237     fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)

File h5py\_objects.pyx:54, in h5py._objects.with_phil.wrapper()

File h5py\_objects.pyx:55, in h5py._objects.with_phil.wrapper()

File h5py\h5f.pyx:102, in h5py.h5f.open()

File h5py\h5fd.pyx:155, in h5py.h5fd.H5FD_fileobj_get_eof()

File h5py\h5fd.pyx:155, in h5py.h5fd.H5FD_fileobj_get_eof()

File h5py\h5fd.pyx:155, in h5py.h5fd.H5FD_fileobj_get_eof()

UnicodeDecodeError: 'utf-8' codec can't decode byte 0xce in position 77: invalid continuation byte
```

I haven't changed anything in the code, it just suddenly stopped to open. I've tried to recreate the model in colab and download it again, changed tf version on computer to tf 2.13.0 (colab has it). Also I've tried to open another models on my pc and all of them threw this error

[Model in google drive](https://drive.google.com/drive/folders/1QIYJklw1tW5WCBanlfkIFyKFLwMGzez3?usp=sharing)

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1776CZxkKcetQsPkJfMACWnNel3Hj-hbA?usp=sharing - colab code for the nn

https://gist.github.com/DemO-O-On/65d5f1114d8163ae234a57c7789d7039 - script for loading model
```


### Relevant log output

Output from jupyter when trying to load model:

`2023-10-13 16:33:40.479946: E tensorflow/tsl/platform/windows/windows_file_system.cc:363] ERROR: GetSymbolicLinkTarget cannot open file for \\?\C:\Users\termi\Desktop\stlsegm\_\__  \  GetLastError: 5`",True,"[-0.39134324 -0.33741832 -0.26637584 -0.04102394  0.05637466 -0.3933956
 -0.03880865  0.04193985 -0.43867958 -0.15334445  0.06232268 -0.17278491
 -0.2811814  -0.00951168 -0.13756628  0.20324337 -0.2028108  -0.04607258
  0.14936884 -0.02384468 -0.2069735   0.1211501  -0.33845106  0.1243858
  0.24184626  0.26616603 -0.1486418   0.10758273  0.14530706  0.1466499
  0.2891127   0.12048766 -0.20483983 -0.00843154  0.16659878  0.32920516
 -0.07979444 -0.43874282 -0.19406147 -0.14284417 -0.24057136  0.2501716
  0.2552532   0.07464271  0.08149324  0.08642805  0.03894992 -0.19260295
 -0.14085656 -0.11110608  0.0882085   0.18884213 -0.33271077 -0.08462745
 -0.09335269 -0.07695197 -0.02672681 -0.00561859  0.16282645  0.00781158
  0.23041569 -0.03983042 -0.01146424  0.01405003  0.00079362  0.13401785
  0.27968237 -0.12543717  0.37413162 -0.22946516  0.17653826 -0.07154132
 -0.51554084  0.1822902   0.10514174  0.06864785  0.04831322  0.23117507
  0.23117496 -0.24577305 -0.08004843 -0.3100544   0.04733012 -0.2505917
  0.28168187 -0.20322871  0.22343421 -0.02901089  0.30572826 -0.03499113
  0.4938066   0.24043906  0.26204115  0.24340074  0.3286588  -0.08078139
  0.17118093  0.4924077  -0.02150283 -0.05362149 -0.14658776 -0.20320474
 -0.318165    0.08333871 -0.01726501 -0.2401702   0.14424412 -0.01025915
  0.23596823 -0.17759895  0.12639906 -0.07075846  0.06846029  0.10135286
  0.14810994 -0.06551831 -0.1362537   0.00921298  0.10475834  0.6183243
  0.16436271 -0.05981087 -0.3675276   0.2544472   0.3125655   0.04430741
 -0.13692428 -0.12166543  0.1077095  -0.13214073  0.20404488  0.04349819
 -0.08023237  0.0179905  -0.12897435  0.07155244 -0.40969765 -0.19524807
 -0.23921105 -0.21235964 -0.28516775  0.11309339 -0.10944158 -0.5115963
  0.18763377 -0.04216102 -0.19231671  0.5200983  -0.1962394   0.06549145
  0.15043007 -0.04248714 -0.19391513  0.3169337   0.16127306  0.0557754
  0.3464626  -0.10950983 -0.04103725 -0.6679758   0.15994532  0.6089113
 -0.21612076 -0.12463951  0.24078341  0.10371865 -0.55709237  0.02180757
  0.05035717  0.3287673  -0.21355107  0.18377614  0.20680171 -0.02383681
  0.35238802 -0.19618091  0.25953007 -0.42598662 -0.13229023  0.48440498
 -0.09009396  0.15572906  0.13352123  0.23994279  0.08090274 -0.07262381
 -0.04737446  0.1969936  -0.2285469  -0.11823524 -0.25671092 -0.24189289
  0.2685063  -0.30122375  0.00563323 -0.31229603  0.02986265 -0.10983081
 -0.10801999  0.104356    0.35017356 -0.12815903 -0.0941074   0.01653675
  0.12462341 -0.21371949 -0.09660514 -0.3783915  -0.19548026  0.15564868
  0.07127291 -0.31880456  0.10976334 -0.16374296 -0.17282629 -0.13514574
  0.25863594  0.10468183 -0.12647533  0.08463987  0.09488322 -0.24901141
  0.12390453 -0.31287503 -0.08588274  0.01847941 -0.45402178  0.14362493
 -0.28533402  0.08118814  0.28327167  0.19136217  0.33179954  0.14612292
  0.36666015 -0.08932282  0.05177766 -0.10566624 -0.36831653  0.06238374
 -0.23688236  0.00896713  0.07215521 -0.17457336  0.13027048  0.270421
 -0.05620325 -0.10886581 -0.35618147  0.24815795 -0.40742725  0.19023699
  0.2658864  -0.06579179  0.333358   -0.03217138  0.25371036 -0.13654613
 -0.03312395 -0.14463525  0.2086274   0.12959838  0.14560261  0.4646752
  0.12762173  0.34522006 -0.14685564  0.4601993   0.08228977  0.00392161
 -0.12903672 -0.10587786  0.4265647  -0.2720264   0.32148993 -0.21075943
  0.32308608 -0.05530781  0.02165671  0.13267678 -0.1159772   0.38563007
 -0.44941702  0.20980875  0.12139083 -0.13277626  0.17811847 -0.5766461
 -0.0520384   0.10096006 -0.23099683  0.22475454  0.17532107  0.10019969
 -0.26314774 -0.03762029 -0.07064453 -0.03709212  0.20361394  0.1752217
 -0.17286737  0.04749408  0.4094504  -0.10471502 -0.37995118 -0.04888557
  0.32947323  0.10156602  0.5441023  -0.48057932  0.11226693  0.11406006
 -0.25227982  0.39083993  0.14439926  0.24162768 -0.441106    0.45783645
  0.4379573   0.01697288 -0.08191436 -0.35130286 -0.62054044 -0.12983602
  0.31623107 -0.13723162  0.25912556 -0.325911    0.01376474  0.05358317
 -0.11153109 -0.03086474 -0.3424092   0.03227188 -0.44688183 -0.12710363
 -0.18532585  0.17822887 -0.01379574 -0.1913023  -0.19399786 -0.04303862
 -0.24778226 -0.2750414  -0.09665029 -0.2828148   0.38781062  0.5389861
 -0.14938138 -0.09403215 -0.15183488  0.06979797 -0.34699452 -0.14093927
 -0.1894296   0.38795197  0.08765742 -0.07902342  0.32756498  0.18656516
 -0.08123218  0.07191153 -0.37520516  0.11818003  0.19602    -0.10549705
 -0.17649922 -0.00950074  0.19456634  0.10530243  0.12516843  0.11002414
 -0.34989452  0.39943963  0.5223936  -0.5203209  -0.22572505  0.25800747
 -0.02964206  0.2563877  -0.21583205  0.06803921  0.38225293 -0.06533334]"
Error when convert dynamic axes ONNX to dynamic axes TFLite  stat:awaiting response type:bug comp:lite TFLiteConverter TF 2.11,"### 1. System information

- Ubuntu20.04
- Tensorflow: 2.11
- Python: 3.10.8
- Pytorch: 1.12.0

### 2. Code
#### The first convert Pytorch model to Onnx with dynamic input:
    network = 'rSfM120k-tl-resnet50-gem-w'
    state = load_url(PRETRAINED[network], model_dir=os.path.join(get_data_root(), 'networks'))
    net_params = {}
    net_params['architecture'] = state['meta']['architecture']
    net_params['pooling'] = state['meta']['pooling']
    net_params['local_whitening'] = state['meta'].get('local_whitening', False)
    net_params['regional'] = state['meta'].get('regional', False)
    net_params['whitening'] = state['meta'].get('whitening', True)
    net_params['mean'] = state['meta']['mean']
    net_params['std'] = state['meta']['std']
    net_params['pretrained'] = False
    
    net = init_network(net_params)
    net.load_state_dict(state['state_dict'])
    
    if useRmac:
        net.pool = RMAC(3)
    net.cuda()
    net.eval()
    dummy_input = torch.randn(1, 3, 400, 900)
    input_names = [ ""actual_input"" ]
    output_names = [ ""output"" ]
    dynamic_axes_dict = { 'actual_input': { 0: 'bs',  2: 'img_x',3: 'img_y'},'Output': { 0: 'bs'}} 
    torch.onnx.export(net,
                     dummy_input,
                     ""resnet50.onnx"",
                     verbose=False,
                     input_names=input_names,
                     output_names=output_names,
                    dynamic_axes=dynamic_axes_dict,
                     opset_version=12, 
                    )
#### The second convert Onnx model to Tf:
    onnx_path = 'resnet50.onnx'
    onnx_model = onnx.load( onnx_path)
    onnx.checker.check_model(onnx_model)
    tf_path = 'cirtorch_tf_0210_resnet50_withoutNor'
    tf_rep = prepare(onnx_model)  #Prepare TF representation
    tf_rep.export_graph(tf_path)  #Export the model
#### The third convert Tf model to TfLite:
    tf_lite_path = '0210_cirtorch_fl16_resnet50_withoutNor.tflite'
    converter = tf.lite.TFLiteConverter.from_saved_model(tf_path)
    tflite_model  = converter.convert()
    with open(tf_lite_path, 'wb') as f:
        f.write(tflite_model)
#### Test model TfLite with dynamic input:
    tflite_model_path = tf_lite_path
    interpreter = tf.lite.Interpreter(model_path=tflite_model_path,experimental_delegates=[])
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.resize_tensor_input(interpreter.get_input_details()[0]['index'],[1,3,200,200])
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    interpreter.allocate_tensors()
    x1 = torch.ones(1,3,200, 200)
    input_data = np.array(x1, dtype=np.float32)
    print(input_data.shape)
    interpreter.set_tensor(input_details[0]['index'], input_data)
    interpreter.invoke()
    output_data = interpreter.get_tensor(output_details[0]['index'])
    print(output_data.shape)
### But I got error when invoke model TFLite:
Traceback (most recent call last):
  File ""convertCirtorchFreesizeTFlite.py"", line 143, in <module>
    interpreter.invoke()
  File ""/home/anlab/anaconda3/envs/bopw/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py"", line 917, in invoke
    self._interpreter.Invoke()
RuntimeError: tensorflow/lite/kernels/squeeze.cc:63 current >= 0 && current < input_num_dims && input_dims->data[current] == 1 was not true.Node number 0 (SQUEEZE) failed to prepare.Node number 144 (IF) failed to prepare.
#### Or
  File ""/home/anlab/anaconda3/envs/bopw/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py"", line 917, in invoke
    self._interpreter.Invoke()
RuntimeError: tensorflow/lite/kernels/reshape.cc:85 num_input_elements != num_output_elements (2048 != 4194304)Node number 2 (RESHAPE) failed to invoke.Node number 144 (IF) failed to invoke.
",True,"[-2.73293346e-01 -1.18315279e-01 -3.48201931e-01  7.74583444e-02
  1.56509027e-01 -1.77237436e-01  2.05499843e-01  1.32509008e-01
 -2.03387618e-01  1.22537747e-01 -6.23817183e-02  1.28593490e-01
  2.89840922e-02  2.44224653e-01 -4.18070167e-01  2.20031261e-01
 -3.44210863e-01 -2.25209266e-01 -4.39251140e-02  1.19780779e-01
 -1.55526310e-01 -7.82343149e-02  6.58585131e-03  7.67760426e-02
 -8.61946959e-03  1.32779330e-01 -1.52819902e-01  1.58637986e-01
  3.55536863e-02  3.68346751e-01 -2.84216274e-02  1.80693105e-01
 -5.20384789e-01  5.64254075e-02 -2.66846389e-01 -8.20148289e-02
 -5.80223382e-01  1.61669999e-01 -3.93626928e-01 -4.57163304e-02
  2.52566725e-01  1.25867754e-01  1.09753504e-01  8.57069194e-02
 -1.77100703e-01  9.32936594e-02  3.15914750e-01  1.61122397e-01
 -1.52491376e-01 -1.38335396e-02 -8.55403841e-02 -7.98446313e-03
 -1.84526131e-01 -2.94377327e-01  2.02669024e-01  3.63806844e-01
 -7.11133033e-02  3.54846865e-02 -4.15925533e-02  9.71411169e-02
 -5.74613735e-02 -9.08596069e-02 -1.69058368e-02 -1.74943835e-01
  2.28604734e-01  2.44811922e-01 -7.13188499e-02 -3.22599709e-02
 -3.99775058e-02 -1.96188599e-01  8.05426762e-03 -3.15076798e-01
  9.43005979e-02 -1.53236836e-01  2.26219192e-01  2.88802981e-02
 -3.07582647e-01  2.54420996e-01  5.24163619e-02 -1.61702812e-01
 -1.76519185e-01  6.50749877e-02 -1.35464028e-01  2.66881771e-02
  2.58219033e-01 -2.04837080e-02  7.29942620e-02 -1.23522721e-01
  3.04154992e-01  1.69650689e-02  8.23722005e-01  3.65576714e-01
 -1.51671320e-01  1.57314807e-01  4.22319353e-01  4.39272344e-01
 -6.14990061e-03  1.26792327e-01  1.08519047e-01  7.51382411e-02
 -2.98210569e-02 -3.04863751e-01 -2.27229357e-01  3.41672897e-01
 -3.64512689e-02 -2.05360800e-01 -4.49438319e-02  5.73815890e-02
 -2.15983108e-01  1.34573951e-01  2.39901185e-01  2.15717897e-01
  1.10334769e-01 -5.57606714e-03  7.80453607e-02 -1.85165256e-01
  1.90007642e-01  2.96975821e-02  1.22665085e-01  5.64046055e-02
  3.69097888e-02 -2.05497622e-01  9.46181081e-03  7.61556178e-02
  3.56368244e-01 -6.62607849e-02 -3.22073758e-01 -7.54311383e-02
  8.94443914e-02  3.06918621e-01  2.13855416e-01  1.16672218e-01
 -3.37099075e-01 -7.56316166e-03 -1.85504466e-01 -1.11463502e-01
  2.03898370e-01  2.42407084e-01 -5.15568078e-01  3.47056925e-01
 -1.07261121e-01 -1.14301279e-01  2.68746614e-02 -3.48927826e-01
 -7.04375505e-02 -1.45506933e-02 -1.43204436e-01  1.89973176e-01
  7.72265047e-02  3.82061541e-01 -3.76457721e-02 -1.19064473e-01
 -2.53507972e-01  6.22270525e-01  3.06756441e-02 -6.94203423e-03
 -1.25761237e-02 -7.47177601e-02  2.52218693e-01 -3.06431592e-01
 -1.02135882e-01  4.02127169e-02 -2.96790153e-02 -2.50745237e-01
  2.90562928e-01  5.58171235e-02 -1.78611606e-01 -1.59313247e-01
 -2.34934092e-01  2.95838751e-02 -4.02309597e-02 -7.43571594e-02
 -1.35396153e-01  3.04484159e-01  1.05542324e-01 -7.99788833e-02
  2.79491067e-01 -5.62875986e-01  1.21720366e-01 -1.19795106e-01
  2.92001784e-01  2.24081844e-01  1.23146474e-01 -1.54675832e-02
 -2.42327735e-01  5.87990843e-02  3.46109211e-01  7.73505643e-02
 -3.82344216e-01 -5.17926872e-01 -4.37237322e-01 -1.40389696e-01
  3.68636250e-01 -6.14497215e-02 -3.15476656e-02 -1.78311035e-01
  5.63150086e-03 -3.66562605e-01  3.36833566e-01  1.26962155e-01
  1.68839842e-01  1.36673912e-01  3.82638991e-01 -7.77772367e-02
  1.15606286e-01 -3.45365331e-02 -2.51516968e-01 -3.87478650e-01
 -3.75552773e-01  8.98349360e-02 -1.03448860e-01 -4.43248332e-01
  2.88546741e-01 -1.25643849e-01 -2.17879683e-01  1.06799431e-01
 -1.73566937e-02 -6.98758755e-04 -3.51263136e-01 -6.01281188e-02
  2.40985841e-01  3.46949175e-02  8.77003670e-02 -2.29674891e-01
 -3.07605594e-01  1.58730507e-01 -5.51727891e-01  4.28302698e-02
  3.52441445e-02 -1.54660977e-02  1.11779077e-02  1.11316711e-01
  3.78840826e-02 -9.52040106e-02  3.43100429e-01 -1.44028198e-02
 -1.71995923e-01 -8.59879702e-02 -6.15650564e-02  5.85117340e-02
 -4.39003587e-01  2.40971521e-03 -5.91883883e-02 -3.11792910e-01
  1.82316065e-01  1.24055348e-01 -1.65163875e-01 -3.33943516e-02
 -1.39627531e-01  1.85219273e-01 -1.05142474e-01  3.91415730e-02
  1.87190652e-01  2.26009697e-01  1.38180286e-01  2.09895462e-01
  4.03060794e-01  1.15462616e-01  3.49717662e-02  7.04832375e-02
 -6.08995482e-02 -7.87430555e-02  1.71950310e-01  5.51764846e-01
  2.35625267e-01  9.50518996e-04 -2.14269400e-01  1.99176058e-01
 -2.49604493e-01 -6.05500266e-02 -1.56533480e-01  3.10817882e-02
  4.85171050e-01 -1.60695374e-01  2.11188465e-01  6.79678321e-02
  4.84075934e-01 -3.04850250e-01 -1.70871839e-02  1.50397509e-01
  1.74222797e-01  2.21598968e-02 -3.46785299e-02  3.24494332e-01
  1.75700545e-01 -1.63335577e-01 -2.10002244e-01 -1.65994912e-01
 -1.78750694e-01  7.21831322e-02 -2.79857635e-01 -1.20878071e-01
  7.03338161e-02 -3.02947536e-02 -3.60729039e-01  1.25038058e-01
  2.49296874e-01 -3.10131073e-01  1.16149768e-01 -5.82547188e-02
  3.25717270e-01  3.29099476e-01  2.59233862e-01 -4.12419170e-01
 -8.20214599e-02 -2.52358466e-02  3.87545437e-01  6.21865280e-02
  1.07879810e-01 -4.40247744e-01  1.38134867e-01  2.37474237e-02
  8.16760212e-02  3.05870831e-01 -1.15655906e-01 -1.07822999e-01
  1.47446871e-01  3.83699477e-01  6.26994595e-02 -8.04747269e-03
 -5.76004535e-02  2.31401846e-02 -9.71791893e-02 -8.52211714e-02
  1.62392125e-01  3.02786410e-01  1.34477198e-01 -9.71251503e-02
 -1.07318893e-01 -1.76790342e-01  3.88877243e-02 -1.61350608e-01
 -1.18017055e-01  3.94616127e-01  1.84541211e-01  3.96819226e-02
  4.16917801e-02  1.22902781e-01  1.73322372e-02 -4.09746885e-01
 -1.84995860e-01 -2.21888393e-01 -7.61308745e-02 -1.41127389e-02
 -1.56662583e-01 -2.11468786e-01 -2.63231456e-01  3.06494057e-01
  2.21772939e-01  3.80599171e-01 -1.29815340e-01 -7.08878413e-02
  2.90038347e-01  2.67764926e-01 -5.47592282e-01  3.18729877e-01
 -4.04890597e-01 -2.16844007e-01  9.05504078e-02  3.45551401e-01
 -3.72904122e-01  2.82265186e-01  2.33579259e-02 -1.75889820e-01
  4.03976813e-02  6.66098390e-03 -2.96982795e-01 -2.06066258e-02
  1.55329108e-01  8.94146115e-02 -1.98583975e-01  4.32044744e-01
 -6.50427211e-03 -3.69868189e-01  4.75373641e-02 -1.61101863e-01
  2.31403429e-02  2.67692834e-01  1.47322327e-01 -2.93763936e-01
 -1.80554748e-01  1.99608117e-01 -3.84494662e-01 -4.13382575e-02]"
"Unable to change validation dataset for ""model.fit()"" function after it raises an exception type:bug comp:apis comp:keras TF 2.13","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using model.fit() in TensorFlow, the validation data provided as an input parameter is initially used for model evaluation. However, if the initial evaluation fails, subsequent runs of model.fit() still use the previously provided validation data, even if new data is provided.

### Standalone code to reproduce the issue

```shell
The error is quite easy to reproduce. Check the ""compile and train section"" in the below provided colab.

https://colab.research.google.com/drive/1Es8mQd0FOoWea4SrJ2TDz4NRSx0ZxwEt#scrollTo=08WJXiheNxK7

I modified a tensorflow tutorial colab on CNNs to reproduce this error. In the section i first run the model with correct parameters, then with incorrect and then again with correct parameters but this time it fails.
```


### Relevant log output

_No response_",True,"[-0.3166358  -0.37979388 -0.22740144 -0.01076449  0.27820423 -0.26304895
 -0.03791031 -0.07589477 -0.43541217 -0.29930806  0.27675343 -0.21038078
 -0.15576077  0.03218223 -0.25261384  0.28098816 -0.25436243 -0.06072498
  0.11690024  0.07412928 -0.14350958 -0.14176145 -0.16688459  0.26642114
 -0.1167531   0.12501794 -0.1407446  -0.01894289  0.03588363  0.03119937
  0.1254132   0.10130373 -0.07811805  0.09221251  0.10393606  0.23523434
 -0.14146946 -0.2903501  -0.3068763   0.05631315  0.14760888 -0.03205593
  0.09189079 -0.3622086   0.1689451  -0.22415403  0.00149865 -0.12493169
 -0.19564588 -0.02819302  0.04080817 -0.22962353 -0.37625757 -0.6430122
 -0.19870688  0.06815708  0.21604271 -0.08501571 -0.1763204   0.06096148
  0.12076941  0.14719737 -0.02872344 -0.02877393  0.2215398   0.14106664
  0.14025861 -0.09050054  0.5518417  -0.03257771  0.2500685  -0.067553
 -0.3495698   0.16388997  0.10651787  0.06538858  0.15390909  0.13159993
  0.27873728 -0.11070154  0.04568209 -0.2082013  -0.1661616  -0.26036793
  0.08411194 -0.18026409  0.3868381   0.03252469  0.61464214 -0.22504333
  0.42289355  0.22889712 -0.01198398  0.15460545  0.39735425  0.2268509
 -0.03963924  0.10894538  0.10300091 -0.06807196 -0.09282888 -0.24352518
 -0.01866902  0.06351798 -0.11573484 -0.2258685   0.16646132 -0.13947193
  0.2624813  -0.04376673  0.18024683 -0.02008093  0.21711999  0.02700178
 -0.05320499 -0.04678046 -0.00590694  0.12930617 -0.20094645  0.56451714
  0.0100492  -0.17666152  0.25492084  0.02081363  0.43767428  0.20955521
 -0.28298622  0.02625516  0.00748922 -0.08856606  0.23561293  0.09040804
  0.23036972  0.03343821 -0.10711478 -0.08411788 -0.28314418 -0.01566256
 -0.071416   -0.18729103 -0.28545672  0.07346243 -0.11139715 -0.27761444
  0.15531206  0.12949991 -0.16613835  0.27961105 -0.20713747 -0.13602054
  0.03038994  0.11051723  0.01583415  0.2566486   0.09897818  0.22310707
  0.40040872 -0.16615477  0.0609105  -0.40697727 -0.11803804  0.22917041
 -0.07409668 -0.41476053  0.36176133  0.22545695 -0.2793092  -0.17539957
  0.37157768  0.41936678 -0.04047789 -0.13410865  0.17452866  0.08827128
  0.19586626 -0.1408059   0.28486568 -0.60290915 -0.20069851  0.32992753
  0.301284    0.01399246 -0.02899392  0.25835496  0.10343362  0.02660469
  0.18237662 -0.01467652 -0.08974072 -0.0560714  -0.33060533 -0.04176288
  0.31412312 -0.16575664 -0.1067396  -0.11558977  0.26900592 -0.19490732
  0.00151378  0.01991396 -0.33262497 -0.14975631 -0.2788645  -0.3066431
  0.08700281 -0.24617332 -0.04716636 -0.38938385 -0.20967206  0.24020696
  0.04531259 -0.57960796  0.11967927 -0.16736674 -0.31927836  0.13888893
  0.1253426   0.08234944 -0.04588128  0.41728136 -0.03032156 -0.172617
 -0.04769163 -0.34884438 -0.31294572  0.01925124 -0.23530799  0.1495593
  0.11019158  0.06574625  0.24729024  0.12818556  0.39033753  0.18823794
  0.47600183 -0.27558088 -0.10013038 -0.2656027  -0.19312763  0.01430045
 -0.538641   -0.09708644  0.06352554 -0.10577278  0.36830074  0.43907726
 -0.16987312  0.03200103 -0.40959877  0.02791207 -0.46213314 -0.00874959
  0.41175961 -0.00231528  0.2556628   0.2655729   0.17115282  0.17690639
  0.28078172 -0.2432643   0.43075508  0.13604122  0.05828267  0.53402865
  0.22526635  0.2704157  -0.36773917  0.33400887  0.06806485 -0.22174117
  0.46171868 -0.5096891   0.6456257  -0.2812671   0.23674732 -0.0491776
  0.39858335  0.08037682 -0.04402945 -0.04225149  0.15638903  0.125624
 -0.52535665 -0.00513874 -0.12559196 -0.1293467  -0.02351168 -0.54359293
 -0.1533058  -0.02631284 -0.18525414  0.26365072  0.10995298 -0.13475403
 -0.14080659  0.02170841 -0.00913665  0.00775678  0.03310877 -0.01300422
 -0.32826427 -0.08267783  0.47078592 -0.39250088 -0.10770075 -0.27909154
  0.45466053  0.23963378  0.4701719  -0.36980063  0.08496647 -0.03985926
 -0.1255332   0.59279    -0.07804288 -0.02073797 -0.3262387   0.50583255
  0.04791028  0.09007969  0.12134095 -0.31151763 -0.28288952  0.11857804
  0.13153397 -0.13762066  0.07221085 -0.40678737  0.00706629  0.02116337
 -0.25525936  0.10080437 -0.08720798  0.09220891 -0.11258226 -0.19018686
 -0.5292895   0.5640019   0.13436913 -0.17288378 -0.11884242 -0.05785026
 -0.18396842 -0.20534718  0.10668993 -0.3769353   0.21875879  0.5317217
 -0.02874432  0.1986658   0.08867905  0.11076389 -0.40277728  0.00414049
  0.01130591  0.28184575  0.01571465 -0.13843367  0.35711768  0.34693116
 -0.12902325  0.07554153 -0.26337627 -0.00819751  0.07994296 -0.20086879
 -0.13273661 -0.29641736  0.26372313  0.27037048 -0.18469238  0.18011533
 -0.26548603  0.21846592  0.41407418 -0.40507564 -0.12646449  0.15383464
  0.39282185 -0.09244055  0.21041022 -0.10820203  0.25052166  0.00804474]"
Internal definition breaks tensorflow-probability type:bug,"Tensorflow-probability's [test](https://github.com/tensorflow/probability/issues/1753#issuecomment-1753993699) is broken, and it appears to be due to a [bad definition in tensorflow](https://github.com/tensorflow/probability/issues/1753#issuecomment-1756359940).

This is preventing the tensorflow-probability team from updating `typing_extensions`, which then breaks upgrading to Python 3.12.
",True,"[-3.20055425e-01 -8.78966093e-01 -1.00370087e-01 -8.49008858e-02
  2.09940404e-01 -3.47749323e-01  1.12439319e-01 -3.00740264e-02
 -1.37743622e-01 -3.45169812e-01 -1.29662948e-02  2.23561078e-02
 -2.32228324e-01 -1.60277262e-03 -1.55346394e-01  1.16013452e-01
 -3.06118906e-01 -1.18342035e-01 -1.07393853e-01 -7.87266046e-02
 -1.10707112e-01  3.94119844e-02 -2.15452313e-01  3.71785700e-01
  1.72012001e-01  2.38593206e-01  1.44473314e-01 -2.06327647e-01
 -1.48445144e-02  5.65672070e-02  5.01689792e-01  2.13824719e-01
  5.94993979e-02  4.15673926e-02 -7.59281963e-02  2.69172847e-01
 -6.29547238e-01 -2.34569430e-01  5.19109033e-02  1.80359691e-01
  1.40720636e-01  2.18867183e-01  1.19967349e-01  7.09447218e-03
  3.18218842e-02  3.57856452e-02 -1.56827852e-01  1.76492762e-02
  2.99865425e-01 -1.10587731e-01 -1.14559926e-01  1.76135033e-01
 -5.29437065e-01 -1.22649550e-01 -9.06805159e-04 -2.76063919e-01
  2.54806697e-01  2.54044356e-03  3.01336218e-02  1.14967465e-01
  1.28746033e-01  2.16839761e-01 -4.80635092e-03  2.44257301e-01
 -4.86377850e-02  1.84938401e-01  2.75482982e-01 -6.65852726e-02
  4.69661057e-01  7.68088251e-02  2.97647417e-01 -9.06193703e-02
 -6.56819224e-01  3.03448617e-01 -2.10536197e-01  2.66929269e-01
 -1.92889586e-01  1.79908231e-01  3.71465504e-01 -1.97299659e-01
 -7.83096403e-02 -5.71466088e-02  3.69091123e-01 -1.05368324e-01
  3.87409143e-02  2.19532400e-01  2.57845342e-01 -9.47490111e-02
  4.21767175e-01 -4.11469527e-02  3.47734839e-01 -3.74180451e-03
 -7.56876767e-02  1.05934963e-01  4.09944803e-01  3.03505585e-02
 -7.89658651e-02 -6.11292273e-02 -2.26978555e-01 -1.42954111e-01
 -2.78377354e-01  5.96773624e-02 -4.41363454e-01  7.86273777e-02
 -3.00131496e-02 -4.65681776e-03  2.78581381e-01 -1.27003491e-01
  1.75818950e-01 -9.70347375e-02 -3.95049062e-03 -2.34394059e-01
  4.05828387e-01  1.01750232e-01 -1.00461274e-01  4.70594853e-01
 -2.41648201e-02  4.12558824e-01 -2.15015471e-01  1.07133579e+00
 -8.14714134e-02  7.59052038e-02  1.05233796e-01  1.89021766e-01
  2.47045308e-01 -1.01777680e-01 -1.49664789e-01  1.54207021e-01
  1.77717675e-03 -3.08365226e-01  3.50052238e-01 -2.90998351e-02
 -3.20038885e-01  2.71129966e-01  5.03199995e-02 -6.77603185e-02
 -7.04521462e-02 -2.04525948e-01 -1.76385790e-01 -3.74206863e-02
 -3.13047707e-01  1.49292409e-01 -4.48182411e-03 -4.47034627e-01
  3.52419317e-02  8.33285525e-02 -3.61793935e-01  1.06606804e-01
 -4.61127721e-02  7.53441453e-02 -2.31321797e-01  3.02767083e-02
 -2.31778234e-01  4.82078254e-01  2.03910142e-01  4.43423837e-01
  3.29758823e-01 -5.33602759e-02 -4.09525037e-01 -5.25210142e-01
 -1.15156606e-01  4.50988173e-01 -1.26714945e-01  1.04054511e-02
  3.46332133e-01  9.95628536e-04 -6.29627943e-01 -2.74790496e-01
 -4.11767513e-02  2.63063423e-02 -1.46595150e-01  5.21035194e-02
 -2.76730508e-01  1.62172794e-01  1.97354108e-01 -6.55412525e-02
  3.70645165e-01 -6.59395337e-01 -1.04096130e-01 -1.19392741e-02
  4.69062269e-01 -7.97786415e-02 -8.45232308e-02  6.99400753e-02
  3.35712582e-02  5.45195444e-03 -2.55043060e-01 -1.59605652e-01
  3.44641926e-03  3.47510949e-02 -1.24786526e-01  6.61000162e-02
  1.86910629e-01 -8.04275200e-02  6.40771538e-02 -3.35823558e-02
  4.62493122e-01 -1.98836982e-01 -3.83521356e-02  1.32513776e-01
 -2.44196907e-01 -1.50111049e-01 -2.07136035e-01 -7.13660344e-02
 -7.15827942e-02 -3.38644505e-01  2.96207331e-02 -9.62279141e-02
 -3.60229790e-01 -1.59576267e-01  1.80747330e-01 -2.11415306e-01
 -3.23022336e-01 -2.63635963e-02 -4.56148028e-01 -1.10243693e-01
  1.27332155e-02  1.40409609e-02 -1.93350285e-01  9.32021588e-02
 -2.51099840e-02 -4.46287617e-02 -1.01178072e-01 -4.24232304e-01
 -4.77418333e-01 -3.03253233e-01 -2.74245322e-01  1.42119050e-01
 -9.24383774e-02  4.12458181e-01  4.58052196e-02  3.38816285e-01
  4.24377501e-01  1.66338250e-01  1.02895156e-01  1.63795501e-01
 -2.14124411e-01 -5.64633217e-03 -8.64058509e-02 -1.74809098e-01
 -8.97230431e-02  2.51515247e-02 -5.93661405e-02  1.73968971e-02
  8.29116702e-02 -7.69530982e-02 -1.02665074e-01 -2.27175236e-01
  2.93849502e-02  3.85257334e-01 -3.40446457e-02 -1.25005051e-01
  6.27745509e-01  1.31805569e-01  4.11102355e-01 -6.44845515e-02
  1.09039396e-01  2.22058624e-01  2.79834807e-01 -5.80496863e-02
  3.45162541e-01  3.43184620e-02 -1.15221351e-01  1.69649363e-01
  2.01031625e-01  2.92398304e-01 -3.29035640e-01  2.51172304e-01
  2.94636816e-01 -2.83533633e-01 -8.04689080e-02  9.34912562e-02
  3.92895222e-01 -3.07110667e-01 -1.09022871e-01 -1.46408767e-01
  5.69991708e-01  4.30971801e-01 -2.56351173e-01 -7.36373216e-02
 -8.30221921e-02  2.60815859e-01 -1.83774114e-01  3.50145042e-01
  2.95827724e-02 -1.10028267e-01  2.21479237e-01 -5.52933663e-02
 -1.62888288e-01  5.98919690e-02 -1.97969034e-01 -2.02827062e-02
  3.51254880e-01  1.18305340e-01  9.57293957e-02  3.21187139e-01
 -3.03167086e-02 -6.80861026e-02  7.22429156e-02  4.94066894e-01
  1.14939839e-01 -6.68935329e-02  2.15692908e-01 -2.71840453e-01
  1.48549257e-02 -3.02298605e-01  1.51579097e-01  2.48487800e-01
  2.49097735e-01 -4.25236136e-01  2.14350551e-01 -2.68093944e-02
 -2.84947813e-01  5.74845135e-01 -1.38711110e-01 -2.89097041e-01
 -2.35766619e-01  7.33049035e-01 -9.55211893e-02 -2.14196846e-01
 -1.73085913e-01 -3.35182369e-01 -1.86071128e-01  1.54909208e-01
  2.32618034e-01 -1.16090149e-01 -3.23708534e-01 -1.70744523e-01
 -3.50356638e-01 -8.83517712e-02 -2.85002440e-02 -2.33152479e-01
 -3.32163453e-01  1.64981857e-01 -5.19028157e-02 -7.32525885e-02
 -4.27756995e-01  4.82493162e-01  8.08738023e-02 -8.47136304e-02
  1.65245607e-01  1.62348263e-02 -7.84065276e-02 -4.34761941e-01
 -9.17259604e-02 -3.51990908e-01  4.53414798e-01  3.72562289e-01
 -1.16791083e-02  1.15902238e-02  2.05041301e-02  2.84308493e-01
 -2.61699334e-02 -4.15148539e-03 -2.11652800e-01  5.26273966e-01
  6.63581565e-02  2.39905700e-01  7.86890835e-02  4.64012682e-01
 -2.76532352e-01 -1.15850538e-01 -4.51428652e-01  7.29257986e-02
  3.98874015e-01 -9.70493704e-02 -1.39190238e-02 -1.11561231e-01
  1.12866014e-01  2.36687362e-01  1.21330060e-01  4.20133740e-01
 -3.77733707e-01  1.64212972e-01  3.33310097e-01 -3.72705221e-01
 -4.31720793e-01  3.80942881e-01 -2.92404175e-01 -2.10507244e-01
 -2.54408240e-01  2.31103487e-02  8.39797109e-02  1.01194330e-01]"
NNAPI delegate crashes on kTfLiteBuiltinCos in TF Lite 2.14 stat:contribution welcome stat:awaiting tensorflower type:bug comp:lite TFLiteConverter TFLiteNNAPIDelegate TF2.14,"### Issue type

Bug (segfault!)

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

No

### OS platform and distribution

Android 13

### Mobile device

Pixel 4

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In TF Lite 2.14, the NNAPI delegate was changed to support the cos operator. Unfortunately, it does not work as expected and causes a segfault.

The originating change is https://github.com/tensorflow/tensorflow/commit/4aac8c95b7d7827eedca82a76cb71db1525dafc9.

The repro is easy. Use the below network and run with NNAPI.

The error is in `TransformCosIntoSupportedOps`, which makes the assumption that `theta` is non-null. This causes a null-pointer dereference.

But the problem is bigger than this - the NNAPI delegate should _not_ be modifying static data. This is dangerous and will lead to problems if the node is not accepted by the delegate, or if the execution graph is modified again to use a different delegate.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

class MyModel(tf.Module):
    @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])
    def my_operation(self, x):
        return tf.cos(x)

model = MyModel()

concrete_func = model.my_operation.get_concrete_function()
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
tflite_model = converter.convert()

with open(""nnapi_cos_bug.tflite"", ""wb"") as f:
    f.write(tflite_model)
```

Convert to tflite and load using NNAPI. The application will segfault.
```


### Relevant log output

_No response_",True,"[-3.53238434e-01 -3.29002142e-01 -1.45341337e-01  7.94249997e-02
  1.57335371e-01 -1.48858815e-01  5.74129783e-02  9.08028409e-02
 -3.51833284e-01 -2.01222166e-01 -1.20530305e-02 -1.96277007e-01
 -1.25777334e-01  2.68222272e-01 -1.06626935e-01  3.42737794e-01
 -3.57821226e-01 -2.20353544e-01  3.08757424e-01 -2.38741767e-02
 -5.86376190e-02 -3.24397832e-02 -2.61321247e-01  3.34422827e-01
  8.60063210e-02  1.81393325e-01 -2.22133085e-01 -1.00589827e-01
  1.86131578e-02  1.73778564e-01 -7.59502798e-02  1.22879073e-01
 -3.50093633e-01 -8.59366953e-02 -2.22149584e-02  4.82654981e-02
 -3.57941866e-01 -7.71185681e-02 -3.69144291e-01 -3.33812423e-02
 -8.73152241e-02  1.81591570e-01  6.82239085e-02 -7.19033182e-02
 -1.86129719e-01 -7.37861693e-02 -2.51403693e-02  1.41901314e-01
 -1.11388303e-01 -5.31560779e-02  1.29323661e-01  1.41102439e-02
 -4.24821019e-01 -2.19437659e-01  1.05240420e-01  1.70838758e-02
  3.87618244e-02  8.22803378e-02  2.25248501e-01  2.44664356e-01
  1.71276361e-01  6.15197718e-02 -2.37129122e-01 -6.33498654e-02
 -1.43345580e-01  2.01429874e-01  1.89036816e-01 -7.93441534e-02
  3.09199989e-01 -2.66592205e-01  8.01794007e-02 -1.12662554e-01
 -1.85828298e-01  1.65345728e-01  1.50806695e-01  2.73779035e-02
  4.43590060e-02  1.56648874e-01  2.61747301e-01 -8.01752955e-02
  7.25341141e-02 -2.25544870e-01 -1.60392463e-01 -6.83607459e-02
  3.84419113e-01 -9.60326195e-02  1.87276185e-01  5.69353625e-02
  5.23776054e-01  5.41525222e-02  4.09176171e-01  5.68471611e-01
  7.59210661e-02  9.55080092e-02  3.48655224e-01  1.12656370e-01
  4.94732484e-02  8.14715773e-02  7.03862309e-02  1.30029514e-01
  5.75536489e-02 -3.91726464e-01 -2.50680238e-01  1.35366425e-01
  1.16464868e-02 -2.36556977e-01  2.73197889e-01 -1.53906107e-01
  1.54300869e-01  2.45280817e-01  3.10980320e-01  7.09672198e-02
  2.58689284e-01 -1.01812154e-01 -2.96330079e-04  5.66662326e-02
  5.02724722e-02  5.54133728e-02  9.18185860e-02  5.48018575e-01
  1.54065445e-01 -1.16603255e-01 -3.15533355e-02  2.04776406e-01
  4.38616455e-01  2.30765164e-01 -6.54330850e-02  8.44693780e-02
  1.04552589e-01 -1.28645450e-01  4.86461371e-02  2.83303797e-01
  3.34325619e-02  6.07340299e-02 -1.42855808e-01  1.25215918e-01
 -2.66628265e-01 -2.62000531e-01 -3.60795557e-01 -1.34642452e-01
 -4.74222571e-01  9.59967226e-02  4.68435735e-02 -4.82325077e-01
  8.56279805e-02  7.39328936e-02 -1.41342044e-01  3.88403177e-01
 -1.56256020e-01  7.56608248e-02  8.17801058e-03  1.47000656e-01
 -1.31879881e-01  5.39509237e-01  8.42085481e-02 -1.56415235e-02
  2.96904027e-01  2.93107573e-02  5.78707978e-02 -3.94864559e-01
 -9.39137638e-02  1.33441389e-01  8.81967843e-02 -1.27432302e-01
  1.22310489e-01  3.39602008e-02 -5.84080219e-01 -1.63863942e-01
  6.93440903e-03  3.21414709e-01 -3.18904400e-01 -1.29360661e-01
  6.72673211e-02 -7.26527870e-02  1.81340039e-01 -1.38006851e-01
  3.43348920e-01 -5.04996479e-01 -5.53648137e-02  1.40540913e-01
  1.16597846e-01  2.97856808e-01  2.22990289e-01  1.06543168e-01
 -1.25335306e-01 -8.71140510e-02  8.46951157e-02  1.65851891e-01
 -1.69698611e-01 -4.27970886e-02 -4.36280489e-01 -1.11163281e-01
  4.39790249e-01 -1.07703909e-01 -1.15364134e-01 -7.75230676e-02
  2.57956330e-02 -4.10818100e-01  2.28422105e-01 -2.50544846e-02
  2.08955154e-01 -6.58864379e-02 -1.19722307e-01 -1.01667389e-01
  7.48727247e-02  1.10456124e-02 -1.42528743e-01 -5.27004778e-01
 -1.58131331e-01 -1.86700642e-01  1.99201226e-01 -5.32948017e-01
 -8.30594674e-02 -2.12430432e-01 -2.44222596e-01  2.11101007e-02
 -6.85296208e-02 -6.90094754e-02 -2.31594443e-01  1.73545077e-01
  1.36437759e-01 -6.59695864e-02 -1.84575081e-01 -3.66372496e-01
 -3.63724709e-01 -6.66026697e-02 -2.45429590e-01  4.14393097e-02
 -4.35948111e-02  1.43400937e-01 -1.71376467e-01  4.01725769e-02
  4.83831823e-01  1.11796394e-01  3.60073388e-01 -1.32898420e-01
 -1.93464309e-01 -2.15167359e-01 -3.16257179e-01 -1.05365418e-01
 -1.95640117e-01  1.30433217e-02 -1.04258910e-01 -1.73378259e-01
  2.34737247e-01  2.58275717e-01 -6.30371943e-02 -3.74495313e-02
 -2.88700104e-01  4.84642029e-01 -5.49056865e-02  9.30122882e-02
  2.24007875e-01  5.24566099e-02  4.58871454e-01  9.80063379e-02
  1.96357250e-01 -1.99920565e-01  2.56109834e-01 -2.22553328e-01
  3.18785012e-01  1.28728807e-01  2.81153098e-02  4.98737693e-01
  2.73783088e-01  3.89541268e-01 -8.61658081e-02  3.90142202e-01
 -1.36999518e-01 -7.68220276e-02  3.88411656e-02 -4.38677162e-01
  6.65879846e-01 -3.40282917e-01  8.79303440e-02 -2.91841686e-01
  4.03875828e-01 -1.44626036e-01 -1.62521183e-01  1.47875220e-01
  1.99192286e-01  5.32671034e-01 -2.21361905e-01  1.40861765e-01
 -4.44527976e-02 -3.35565358e-01 -6.84278980e-02 -3.78872931e-01
 -1.09834835e-01  4.35252301e-03 -3.48341078e-01  1.32244095e-01
  1.10695764e-01 -2.35209502e-02 -3.06746542e-01  6.97578639e-02
  1.78225171e-02 -1.73634559e-01  1.67656347e-01 -3.06752194e-02
 -8.08193684e-02  3.29596817e-01  3.59745324e-01 -2.58296132e-01
  4.42194231e-02 -1.52443856e-01  2.57043868e-01  1.99704140e-01
  4.48008716e-01 -5.91549754e-01  2.64931023e-01 -5.25365099e-02
  2.97592245e-02  6.54403925e-01 -1.20769933e-01  8.96053687e-02
 -3.75983775e-01  3.25819939e-01  2.25646317e-01 -1.18160024e-01
  3.89459193e-01 -4.26791236e-02 -4.16301072e-01  4.86953780e-02
  2.24094540e-01  1.12733036e-01  4.41535935e-03 -2.46025741e-01
  1.92440636e-02 -5.61307743e-03 -1.34284377e-01  3.97251174e-02
 -9.64501798e-02 -6.19452074e-02 -4.09773402e-02 -1.42679036e-01
 -3.67677331e-01  2.34628201e-01 -1.70245096e-01 -2.36412004e-01
 -1.73067421e-01  3.05880234e-03 -1.52592868e-01 -1.80232435e-01
 -1.05212815e-03 -2.32112825e-01  3.01496804e-01  1.85725212e-01
  2.24034861e-02  2.34578475e-01 -1.89381510e-01  1.23937100e-01
 -3.14956069e-01  7.39509705e-03 -3.85643363e-01  4.28435862e-01
 -2.30473936e-01 -8.39257985e-02  4.46672499e-01  4.63729173e-01
 -1.99831188e-01  2.38411546e-01 -4.56280053e-01 -3.41406390e-02
  3.70093174e-02 -1.57036573e-01 -3.31231475e-01 -7.54102021e-02
  1.59021690e-01  4.59130347e-01 -9.17061865e-02  2.67201543e-01
 -2.91575134e-01  1.19604640e-01  3.09987485e-01 -3.02809834e-01
  1.39308304e-01  2.55996399e-02  4.34061252e-02 -2.31908426e-01
  3.58259082e-02 -8.74613672e-02  9.89546850e-02  7.88808465e-02]"
Shape inference for tf.gather is inconsistent with/without --tf_mlir_enable_mlir_bridge stat:awaiting response type:bug stale comp:ops TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Please refer to the ""Relevant log output"" section for detailed description of current behavior.

### Standalone code to reproduce the issue

```shell
# Run the code snippet with the following flags to reproduce the issue:
# TF_XLA_FLAGS=""--tf_mlir_enable_mlir_bridge --tf_xla_enable_xla_devices --tf_xla_auto_jit=2 --tf_xla_cpu_global_jit --tf_xla_min_cluster_size=1""

import tensorflow as tf

params = tf.constant([
    [0, 0, 1, 0, 2],
    [3, 0, 0, 0, 4],
    [0, 5, 0, 6, 0]])
indices = tf.constant([
    [2, 4],
    [0, 4]])

print(tf.gather(params, indices, axis=1, batch_dims=1))
```


### Relevant log output

```shell
When mlir bridge is disabled, the code snippet works without any error. The return value of tf.gather is [[1 2][3 4]]


when mlir bridge is enabled, the code snippet returned with error:

2023-10-11 11:43:24.846292: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:841 : INVALID_ARGUMENT: Shape used to set computation result layout (s32[3,2]{1,0}) is not compatible with result shape (s32[2,2])
Traceback (most recent call last):
  File ""/tf/playground/test_gather.py"", line 37, in <module>
    print(tf.gather(params, indices, validate_indices=True, axis=1, batch_dims=1).numpy())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py"", line 5888, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shape used to set computation result layout (s32[3,2]{1,0}) is not compatible with result shape (s32[2,2])
         [[{{node cluster_1_1/xla_compile}}]] [Op:GatherV2] name:
```
",True,"[-5.27252197e-01 -4.09552783e-01 -1.07148230e-01 -8.19456764e-03
  2.46057659e-01 -2.95343488e-01 -5.09154163e-02  2.62788571e-02
 -3.80649686e-01 -3.90683919e-01  5.41977882e-02 -3.19060147e-01
 -7.97149166e-02  1.09840862e-01 -1.12309657e-01  2.84138262e-01
 -3.79149735e-01 -2.58647017e-02  1.17709085e-01  6.95226118e-02
 -2.77414203e-01  7.33342208e-03 -3.98361683e-02  3.31203043e-01
  6.51471093e-02  2.82687604e-01 -2.19624564e-01  9.76682901e-02
 -6.23084754e-02  2.88533151e-01  1.36369944e-01  1.43989459e-01
 -2.30645001e-01  4.98279370e-02 -1.43593580e-01  2.43460909e-01
 -3.36468339e-01  3.01121548e-02 -3.56120110e-01  6.64251437e-03
 -1.35426670e-01  7.60139525e-02  1.85485840e-01 -1.94575697e-01
  2.05068320e-01  5.72458617e-02 -1.14147022e-01  2.22295783e-02
 -2.22249031e-01  3.73734124e-02  1.11118406e-01 -4.80547510e-02
 -5.60965896e-01 -3.58138591e-01 -5.97777292e-02  3.25937808e-01
  9.20928866e-02 -6.71685860e-02 -6.56859996e-03  1.86900795e-01
  9.86253768e-02  1.14914469e-01  1.68641910e-01 -8.20424780e-02
  7.37412870e-02  3.21632087e-01  3.25016469e-01  1.03436328e-01
  4.52475309e-01 -1.62409559e-01  2.65175104e-01 -6.02731295e-02
 -3.50185812e-01  5.39071858e-03  6.15791902e-02  1.30566001e-01
 -2.04844818e-01  1.47443026e-01  2.14265570e-01 -2.96350360e-01
 -1.23961218e-01 -1.12085447e-01 -1.59092471e-01 -1.74772553e-03
  2.86264837e-01 -2.59693891e-01  4.06683892e-01  1.45162553e-01
  5.57844281e-01 -1.24432504e-01  4.36778665e-01  3.94958824e-01
 -1.28486827e-01  2.21838951e-01  3.99114579e-01  3.29925120e-01
 -1.48524776e-01  5.81969470e-02  2.13984072e-01  6.05797209e-03
 -3.26436698e-01 -1.25260741e-01 -1.55879170e-01  1.53987199e-01
 -2.86631346e-01 -2.23128408e-01  2.70729437e-02 -1.34819616e-02
  2.48619810e-01  2.35716552e-01  1.86749354e-01  2.51593068e-04
  1.05985492e-01  7.33129233e-02 -2.12784737e-01 -1.68299675e-01
  1.57153606e-02 -1.92123175e-01  6.99239373e-02  4.06743318e-01
  1.85630232e-01 -2.53269196e-01 -6.42468315e-03  3.10578555e-01
  4.54114318e-01 -1.18042387e-01 -3.95825267e-01  5.42706698e-02
  2.65096664e-01 -2.63922401e-02  2.16692656e-01  8.31248015e-02
  7.01972619e-02 -6.25965465e-03 -2.83605039e-01 -3.60896811e-02
 -7.52043054e-02 -2.27807350e-02 -4.14267421e-01 -1.10546023e-01
 -1.82145119e-01  5.31226248e-02 -1.45128325e-01 -4.61066246e-01
  3.23085934e-01  1.52731687e-01 -4.16930616e-01  2.12741703e-01
 -2.96323806e-01  5.09912707e-02  3.66959646e-02 -6.32326975e-02
 -1.19049110e-01  3.54656518e-01  7.95454532e-02  7.68772364e-02
  3.10329258e-01 -5.78345358e-03  2.47370172e-03 -7.28858531e-01
 -2.84140669e-02  2.76755035e-01  1.50232777e-01 -3.07869017e-01
  2.75288552e-01  1.72131792e-01 -4.45854962e-01 -1.82094082e-01
  2.86470860e-01  2.88104594e-01  8.48277211e-02 -2.55830377e-01
 -4.39073108e-02 -9.49988049e-03  2.78175890e-01 -1.64779395e-01
 -1.05580948e-01 -3.97403657e-01  7.02542216e-02  2.33139187e-01
  6.61830828e-02  8.87629837e-02  2.44274378e-01  1.81169793e-01
 -1.21801719e-01  4.76424061e-02  1.47703677e-01  4.01439250e-01
 -2.33041763e-01 -5.87575734e-02 -3.78473818e-01 -1.25370562e-01
  2.45460093e-01  1.58733055e-01 -1.21560231e-01  8.16956609e-02
  3.62829864e-01 -2.08726346e-01  2.49208525e-01  1.30229384e-01
 -1.92406178e-01 -4.67089750e-02 -6.38595521e-02 -2.71168083e-01
  2.80017734e-01 -1.25287652e-01  1.32382259e-01 -5.08224726e-01
 -2.51796991e-01  2.07568884e-01  1.67351961e-02 -7.83025801e-01
  4.47150990e-02 -1.49211705e-01 -2.00088114e-01  1.88477412e-02
 -1.27798215e-01  1.31928757e-01 -2.51365066e-01  2.00074986e-01
  3.19854319e-02 -1.89478517e-01 -8.62682760e-02 -3.31142277e-01
 -3.60842168e-01  2.25465447e-01 -1.59699023e-01  8.63635167e-03
 -3.96410152e-02  1.23289876e-01  1.66830719e-01  1.97175816e-01
  3.46244752e-01  1.95429683e-01  3.41018319e-01 -7.46494681e-02
 -2.66006082e-01 -2.67242670e-01 -4.02237028e-02 -1.90018997e-01
 -3.53341848e-01 -2.91359752e-01 -2.39918083e-01 -1.38043046e-01
  3.68323267e-01  3.75145912e-01  4.79393750e-02 -2.02412028e-02
 -2.75461584e-01  2.62024432e-01 -4.53236222e-01  2.79331971e-02
  1.83495656e-01  1.37597233e-01  4.73852694e-01  2.76887883e-02
  2.63446033e-01  8.74291882e-02  1.70541942e-01 -2.41800740e-01
  3.59789789e-01  2.83841878e-01  2.22005159e-01  4.35951829e-01
  2.56878465e-01  2.29752600e-01 -2.68369019e-01  4.23596919e-01
 -2.75918186e-01 -2.07205400e-01  1.81614086e-01 -2.13189691e-01
  7.11466670e-01 -3.63395929e-01  2.90609002e-01 -6.16215020e-02
  2.92800367e-01 -4.71575111e-02 -7.15318024e-02  2.80498028e-01
  1.57292277e-01  2.97433794e-01 -2.74232239e-01  2.48830587e-01
  3.23618576e-02 -2.16623917e-01 -2.21391246e-01 -6.55857801e-01
  1.14583988e-02  1.96173806e-02 -2.72085428e-01 -7.04674870e-02
 -2.36037105e-01 -1.08753191e-02 -1.56370908e-01  2.40886390e-01
  8.63593668e-02 -7.50429630e-02  1.51397437e-01 -4.18595225e-03
 -1.35327563e-01  1.29187986e-01  4.52201545e-01 -3.44891757e-01
 -1.48720562e-01 -4.06261832e-02  4.96743679e-01  2.91078389e-01
  3.25534523e-01 -4.23297703e-01  3.49749774e-02 -1.32707849e-01
  1.02610938e-01  6.66993499e-01 -1.06369719e-01  1.66565645e-02
 -2.56012738e-01  6.95385754e-01 -2.67191567e-02  5.52340448e-02
  1.75752819e-01 -2.40050197e-01 -2.63453960e-01 -8.71406868e-04
  1.36144519e-01 -6.20943271e-02  1.45243764e-01 -3.70409608e-01
  5.61045147e-02 -1.57416947e-02 -2.16369182e-01 -1.43489167e-01
 -1.25782967e-01 -7.47204646e-02 -1.37127101e-01 -3.09226751e-01
 -2.67453581e-01  1.28051072e-01 -1.35950834e-01 -3.75925779e-01
 -3.13466668e-01 -3.45096886e-01 -2.14177042e-01 -9.18100551e-02
  1.04128286e-01 -3.90029371e-01  1.71252578e-01  5.59815884e-01
 -2.12353449e-02  2.03109026e-01  1.72667265e-01  1.59268320e-01
 -2.97822803e-01 -4.26314771e-02 -7.75125995e-02  3.02100301e-01
  3.15170810e-02 -8.58347937e-02  3.24906588e-01  3.24057996e-01
 -3.20270360e-01  1.59590423e-01 -1.52852297e-01  1.24061354e-01
  1.12551570e-01 -1.84120491e-01 -1.67521149e-01 -3.92800212e-01
  6.75196648e-02  1.89598128e-01 -1.22802243e-01  2.94206232e-01
 -3.93640131e-01  1.93838298e-01  4.40225244e-01 -3.94056261e-01
  2.38870680e-01  2.67135762e-02  2.83627212e-01 -9.82714519e-02
  4.65735942e-02 -1.44666553e-01  8.22802782e-02 -5.09989336e-02]"
"TFRecordWriter stuck (or very slow) while serializing data, depending on feature transformation type type:bug","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

ubuntu 22.4

### Mobile device

_No response_

### Python version

3.8.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?


**tf.io.TFRecordWriter** freeze when preprocessing features with **scikit-learn** (**Dask-ML**) **QuantileTranformer**, whereas its working (writing out few k samples within seconds) when using **StandardScaler**.

How might QuantileTransformer produce output data shaped in a way it breaks TFRecords serialization? Might dtype precision influence serialization performance in such criticality?

### Standalone code to reproduce the issue

```shell
(reduced pseudo code)

# fit scalers
#### NOTE Option 1: using this scaler breaks TF records writer!
feat_standardizer = dask_QuantileTransformer(output_distribution=standardizer_distribution, 
                                                n_quantiles=n_feat_standardizer_quantils, 
                                                subsample=n_fit_samples, copy=False)


# NOTE Option 2: Using this one works for TF records writer
feat_standardizer = dask_StandardScaler(copy=False)


## from here proceed the same way until TF data serialization as TFRecord files

x_train_future = dask_client.scatter(x_train_arr) 
feat_standardizer = dask_client.submit(feat_standardizer.fit, x_train_future).result()
x_train_preproc_future = dask_client.submit(feat_standardizer.transform, x_train_future)
x_train_dask_arr = dask_client.gather(x_train_preproc_future)

....

# Transform training data (in chuncks of subsets of the total training dataset)
X_train_future = dask_client.scatter(x_train_arr_transform_batch)
X_train_future = dask_client.submit(feat_standardizer.transform, X_train_future)
X_train_future = dask_client.submit(feat_normalizer.transform, X_train_future)
x_train_arr_transform_batch  = dask_client.gather(X_train_future)
    
# reshape features: (sample * time, feat) -> (sample, time, feat)
X_train = x_train_arr_transform_batch.reshape((n_train_samples, n_steps, n_feat))

# cast numpy default precision float64 -> TF float32                               
X_train = X_train.astype(np.float32)
y_train = y_train.astype(np.int64)

def array_to_tfrecords(X, y):
        feature_dict = {
            'X': tf.train.Feature(float_list=tf.train.FloatList(value=X.flatten())),
            'y': tf.train.Feature(int64_list=tf.train.Int64List(value=y.flatten()))
        }
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict)) 
        return example.SerializeToString()

tf.io.TFRecordWriter(tfrecords_file_path, options=tf.io.TFRecordOptions(compression_type='ZLIB', compression_level=7)) as writer:
        for x, y in tqdm(zip(X_train, y_train)):         # <---- no progress visible here when using QuantileTransformer!
            serialized = array_to_tfrecords(x, y)
            writer.write(serialized)
```


### Relevant log output

```shell
No output visible. The script just never continues and freeze while pretending to serialize data
```
",True,"[-4.09404367e-01 -3.09228390e-01 -9.97418985e-02 -2.10144311e-01
  1.97979718e-01 -5.21982849e-01 -2.75218010e-01 -5.36653399e-03
 -3.74517083e-01 -1.57583758e-01  1.55548662e-01 -8.27472657e-04
 -2.84142256e-01  2.40735531e-01 -3.35673153e-01  3.57697129e-01
 -1.81222409e-01  7.25160539e-02  1.92846850e-01  9.60791036e-02
 -3.15254420e-01 -1.86952185e-02 -2.28668183e-01  2.47280449e-01
  1.12093329e-01  2.11247146e-01 -2.81862766e-01  3.82743217e-02
  4.80626859e-02  3.24240446e-01  1.01187125e-01  1.81184918e-01
  1.20704830e-01  6.73734695e-02  4.57255244e-02  2.60766476e-01
 -4.17045504e-01 -1.43503934e-01 -3.00964445e-01 -1.86856538e-01
 -9.89802033e-02  3.98686640e-02 -8.86910260e-02 -1.75479650e-01
 -2.00976521e-01 -1.60827152e-02  4.05054837e-02  1.95784152e-01
 -3.06454211e-01 -1.67804241e-01  4.07993942e-02  6.19731378e-03
 -6.40845537e-01 -3.60425651e-01 -1.10118993e-01 -4.75707203e-02
 -6.07281961e-02  1.02762923e-01  1.98364943e-01  2.21305788e-01
  2.65539408e-01  5.27480803e-03 -8.83346424e-02 -7.62983263e-02
  9.83766392e-02  2.58988678e-01  3.92710865e-01 -6.44380078e-02
  2.54011124e-01 -3.09130371e-01  1.30182996e-01 -1.11719869e-01
 -2.93980956e-01  3.42071652e-01  9.56735238e-02 -8.23595077e-02
  1.13259532e-01  1.84903949e-01  1.74779952e-01 -1.05395555e-01
  1.90533213e-02 -1.07073016e-01 -3.02224308e-01 -2.14731261e-01
  1.38700128e-01 -2.71837592e-01  3.45404238e-01  7.81590268e-02
  4.27037060e-01 -2.89441139e-01  4.16359156e-01  4.38869238e-01
  4.17861715e-02  1.99546278e-01  3.56604099e-01  2.69340813e-01
  5.73400594e-02  1.89712510e-01  1.36720419e-01 -3.03137340e-02
  1.78454705e-02 -2.17138439e-01 -1.87732965e-01  9.44568068e-02
  7.32224993e-03 -2.11713269e-01  2.83386916e-01 -5.19352332e-02
  1.28523618e-01  1.17710501e-01 -5.53696938e-02  2.52025276e-01
  4.98627461e-02 -2.16167748e-01 -8.94084126e-02  1.78679377e-01
 -2.00069606e-01  1.20999999e-02  1.29937634e-01  6.20777965e-01
  1.33848429e-01 -3.65501285e-01 -1.40519425e-01  1.33243045e-02
  7.39262342e-01  1.19413234e-01 -9.54450369e-02  4.40981612e-02
  1.59177512e-01 -2.00119138e-01  4.25215252e-03  2.56239343e-02
 -2.04045847e-02  1.29614308e-01 -4.21661101e-02  1.47345513e-01
 -3.26412112e-01 -5.87316751e-02 -2.75065541e-01 -3.88813615e-01
 -3.84421349e-01  4.99112874e-01 -8.28731954e-02 -6.05577707e-01
  1.88099802e-01  1.23294435e-01 -3.52324724e-01  3.80607963e-01
 -2.14043081e-01  2.10708350e-01  1.23975471e-01 -1.70382142e-01
  3.11916098e-02  3.16616863e-01 -1.09323887e-02 -2.21625656e-01
  5.28030157e-01 -1.29617145e-03 -2.88634710e-02 -5.07803679e-01
 -2.50631738e-02  1.04846805e-01 -5.80224283e-02 -3.21149111e-01
  8.85021761e-02  2.77719170e-01 -5.46909988e-01 -2.89226502e-01
  7.61708468e-02  5.80165029e-01 -2.85124958e-01 -2.57162035e-01
  2.71619141e-01  1.15281537e-01  3.08868468e-01  4.80501279e-02
  4.13138896e-01 -3.61115366e-01  8.19006115e-02  2.87565887e-01
  5.97603954e-02  8.20673555e-02  1.46263570e-01  2.12890357e-02
  5.86845055e-02 -1.17745586e-02  2.85313964e-01  2.43507713e-01
 -2.51092106e-01 -3.83481905e-02 -3.96015346e-01 -1.61788926e-01
  5.64286232e-01 -8.05093572e-02 -1.50531292e-01  3.58626246e-02
  2.49371216e-01 -1.87870175e-01  5.16108200e-02  5.61482199e-02
 -2.38839597e-01 -3.78033295e-02 -8.82689655e-02  8.50444585e-02
  1.52368188e-01 -1.85089394e-01 -9.48248059e-02 -5.03741324e-01
 -3.14658791e-01  2.81616449e-01  5.56739829e-02 -5.26981890e-01
  5.66883758e-02 -9.09247696e-02 -2.16565371e-01  3.19759846e-02
  1.96615681e-02 -7.51466118e-03 -1.01531431e-01  2.44625956e-01
  7.15329200e-02 -3.01425219e-01  2.19392270e-01 -5.41377127e-01
 -1.04567967e-01  1.42085433e-01 -3.24711591e-01  7.87691921e-02
 -1.85403034e-01  1.16837565e-02 -1.61999077e-01  1.21972583e-01
  2.21893042e-01  1.51498690e-01  3.04614127e-01 -2.53432691e-01
 -5.31822219e-02 -3.86320591e-01 -2.63905257e-01  8.85231793e-02
 -3.00820649e-01 -2.46073335e-01  6.61340579e-02 -2.88917869e-01
  3.84856373e-01  4.70991731e-01 -1.49784684e-01 -2.33576037e-02
 -2.90957868e-01  2.45500535e-01 -2.68889546e-01  4.43155318e-01
  2.98393220e-01  2.28452623e-01  3.67047757e-01  3.41660559e-01
  3.01903665e-01  1.45642415e-01  3.60628664e-01 -2.00064883e-01
  2.05049455e-01  3.66527081e-01  1.96820855e-01  6.13612413e-01
  3.80064547e-01  4.97132659e-01 -2.60524690e-01  3.88252437e-01
 -2.02333137e-01 -3.27091813e-01  6.06037863e-03 -1.70743793e-01
  7.23347008e-01 -4.15456831e-01  4.30243671e-01 -2.30808705e-02
  5.18938303e-01 -1.34757042e-01  1.83427468e-01  2.09534720e-01
  6.51748627e-02  1.50092691e-01 -1.96286172e-01  8.78369212e-02
 -2.26054817e-01 -3.41123492e-02 -1.68049186e-02 -8.18560719e-01
 -9.85051319e-02  1.59873292e-01 -2.89824694e-01  1.64960653e-01
  1.63399279e-02 -1.08878955e-01 -1.66536316e-01  9.56692770e-02
 -6.08305186e-02 -2.82238841e-01  2.25475296e-01 -1.12805456e-01
 -1.25076294e-01  3.75194773e-02  1.73560724e-01 -1.57934025e-01
 -4.67581749e-02 -3.23201492e-02  3.27615470e-01  3.12761128e-01
  2.29902074e-01 -4.60521877e-01  8.09994936e-02 -7.00689480e-02
 -1.41753197e-01  5.30043483e-01 -5.15286364e-02  2.61630177e-01
 -2.11745247e-01  4.69987512e-01 -7.65465014e-03 -1.21737406e-01
  1.02846548e-01 -2.28999928e-01 -2.25204289e-01  1.41547859e-01
  3.62498105e-01  5.90672018e-04 -1.45647768e-02 -1.69265866e-01
  1.49274588e-01  2.01290190e-01 -1.04832582e-01 -8.33353847e-02
 -2.59080619e-01 -2.32964158e-02 -3.56284648e-01 -2.75377691e-01
 -2.69552827e-01  3.09699774e-02  4.25340645e-02 -2.78808266e-01
 -1.11982137e-01 -1.62488729e-01 -2.36618906e-01 -2.53217518e-01
  1.43549219e-01 -3.08107466e-01  3.55265051e-01  5.86479425e-01
 -2.82297432e-02  9.37497020e-02 -8.18993151e-02  5.41764125e-02
 -2.79405594e-01 -7.99297541e-03 -7.67986476e-02  2.60370433e-01
 -2.06275553e-01 -1.25641733e-01  1.68994233e-01  4.68146354e-02
 -5.79145998e-02  3.10118437e-01 -4.29304421e-01  1.23585947e-03
  9.17267576e-02 -3.63700867e-01 -1.82990223e-01 -3.11311111e-02
  2.80567586e-01  2.68283188e-01 -8.39098021e-02  4.74598527e-01
 -2.50424564e-01  2.58306861e-01  6.23543203e-01 -5.08213043e-01
 -8.12036693e-02  7.94399157e-02  2.59878904e-01 -4.55315784e-02
 -8.59151781e-03 -1.41086161e-01  1.56553745e-01  5.71253896e-03]"
ERROR: No matching distribution found for tensorflow-gpu==2.11 stat:awaiting response type:bug type:build/install stale TF 2.11,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11 / 8

### GPU model and memory

3060

### Current behavior?

WARNING: Ignoring invalid distribution -pencv-python-headless (c:\users\rites\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -pencv-python-headless (c:\users\rites\appdata\roaming\python\python39\site-packages)
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.11 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.12.0)

### Standalone code to reproduce the issue

```shell
WARNING: Ignoring invalid distribution -pencv-python-headless (c:\users\rites\appdata\roaming\python\python39\site-packages)
WARNING: Ignoring invalid distribution -pencv-python-headless (c:\users\rites\appdata\roaming\python\python39\site-packages)
ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==2.11 (from versions: 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.12.0)
```


### Relevant log output

_No response_",True,"[-6.67020798e-01 -3.89985263e-01 -1.06524304e-01  1.08374022e-01
  3.70234966e-01 -2.76759624e-01  2.11259983e-02 -4.13860455e-02
 -2.88230956e-01 -5.04055142e-01  1.56125531e-01 -1.54682368e-01
 -1.35762751e-01  6.54636770e-02 -3.05162996e-01  2.47744620e-01
 -2.27695391e-01 -1.51309401e-01  1.97676659e-01  1.16004229e-01
 -2.37134814e-01 -1.57416731e-01 -3.66202354e-01  2.57933527e-01
  2.12881982e-01  3.07364047e-01 -2.06338644e-01  3.50543782e-02
  1.16583809e-01  2.01131165e-01  5.06327689e-01  1.53332561e-01
 -1.23549744e-01  1.92436561e-01  1.63716048e-01  1.19087338e-01
 -3.79787832e-01 -2.48208344e-01 -2.66577154e-01 -9.34701711e-02
  1.39835313e-01  5.97728323e-03  2.38341272e-01 -6.56706691e-02
 -1.10351771e-01 -2.28258371e-01  1.31318450e-01 -1.75106496e-01
  3.05274259e-02 -1.99141830e-01  5.49718142e-02  2.01688949e-02
 -6.14243746e-01 -1.95807725e-01 -7.13843703e-02  4.45298217e-02
  2.70438969e-01 -5.70795313e-02  6.67979792e-02  1.93234339e-01
  1.93437845e-01  1.43878177e-01 -3.47611941e-02 -1.27872854e-01
 -2.12643202e-02  1.63844563e-02  1.82158083e-01 -1.60545766e-01
  5.23218274e-01 -1.67008549e-01  1.57586455e-01 -5.25810085e-02
 -2.60359526e-01 -5.78399375e-02  7.54451752e-02  1.77875876e-01
  1.55926317e-01  2.20319822e-01  1.78061485e-01 -1.34601116e-01
 -1.36260033e-01 -2.84374595e-01 -1.17637649e-01 -2.39932597e-01
  1.39321178e-01 -3.25561821e-01  4.12273079e-01  2.34581545e-01
  5.88006079e-01 -3.85744333e-01  5.11513233e-01  4.36298698e-01
  1.02980673e-01 -2.48368345e-02  4.19951469e-01  1.24845028e-01
  3.89294177e-02  1.74090669e-01  3.51003334e-02  2.37204377e-02
 -1.35519773e-01 -2.33961329e-01  2.14027837e-01  1.95401803e-01
 -1.09883562e-01 -4.26005982e-02  3.56302075e-02 -4.96614464e-02
  8.72893631e-02 -5.90898618e-02  2.85410993e-02  6.60961345e-02
  2.75769114e-01  1.68586671e-01 -1.24291331e-01 -1.44234210e-01
 -1.43776163e-02  1.03698902e-01 -7.94862807e-02  6.15350544e-01
  2.53350902e-02 -1.20722599e-01  1.06265895e-01  5.01268208e-02
  3.98906410e-01 -7.05345534e-04 -1.83702886e-01 -1.33447781e-01
  2.17821732e-01 -9.11782309e-02  3.09792086e-02  1.59695953e-01
  1.68034639e-02  2.85072744e-01 -7.53468201e-02  9.01392400e-02
 -3.69739294e-01 -1.12587146e-01 -2.37214044e-01 -3.25989984e-02
 -1.82298571e-01  2.32123107e-01 -1.13199405e-01 -4.69058573e-01
  2.61476636e-01  1.53771296e-01 -1.57242462e-01  4.33929890e-01
 -2.53844857e-01  3.66998464e-02 -7.27590173e-02 -8.84916335e-02
 -1.64069653e-01  2.59941876e-01  3.93511765e-02 -3.51252407e-03
  4.22935724e-01 -2.06960768e-01  3.02365758e-02 -3.27920854e-01
  8.09517652e-02  4.40787643e-01 -2.76207924e-01 -1.00578815e-01
  2.31031049e-02  2.50079632e-01 -4.95224357e-01 -2.94292390e-01
  1.61097974e-01  4.28034455e-01 -5.26457913e-02 -2.51341730e-01
  1.36764318e-01  1.61480904e-01  2.31025457e-01 -2.89456964e-01
  5.54485917e-01 -5.97913027e-01 -2.75958389e-01  2.59061277e-01
 -8.47377628e-02  3.15066129e-02  1.53557574e-02 -1.55604715e-02
  1.69003710e-01  1.76758431e-02  1.93995729e-01  1.17741987e-01
 -3.77007663e-01 -1.12080425e-01 -3.85493338e-01 -1.30885780e-01
  4.64808941e-01 -9.26755145e-02 -9.01124850e-02  7.60119259e-02
  2.24121466e-01 -1.32342502e-01  8.89251977e-02  1.80210039e-01
 -2.69046009e-01 -2.04523459e-01  4.40541580e-02 -7.19578117e-02
  8.32405537e-02 -2.92714417e-01 -3.14652361e-02 -4.65428531e-01
 -4.75699961e-01  1.26627401e-01  2.00256377e-01 -5.58809638e-01
  3.84897552e-03 -5.17701060e-02 -3.87874275e-01  3.07537079e-01
  5.96611127e-02  8.60634446e-03 -2.18025610e-01  2.75809288e-01
  2.47284308e-01 -2.62636364e-01  6.93046451e-02 -3.68592203e-01
 -3.63834381e-01  9.52668041e-02 -2.88690299e-01  1.09274797e-01
  4.68381234e-02  2.41465911e-01  8.83077532e-02  4.12363112e-02
  4.37074542e-01  3.04056138e-01  3.96861613e-01 -2.27524549e-01
 -1.09589487e-01 -3.10217947e-01 -1.75888881e-01  8.46347865e-03
 -2.27007836e-01 -1.50831848e-01  3.46272029e-02 -1.00288197e-01
  3.97200733e-01  6.07146025e-01 -2.17051849e-01 -1.26593232e-01
 -4.73124266e-01  2.41270274e-01 -2.38644928e-01  2.06486911e-01
  3.35360020e-01  2.71360099e-01  5.42438209e-01  2.57989943e-01
  1.93801761e-01  2.36562684e-01  2.41805449e-01 -1.74044222e-01
  3.62383455e-01  3.27093840e-01 -1.07328430e-01  5.17414391e-01
  2.34782889e-01  3.97323906e-01 -2.96290815e-01  5.33721864e-01
 -2.28075944e-02 -1.69961169e-01  8.66772085e-02 -4.53467280e-01
  6.75317526e-01 -4.83351856e-01 -8.87597501e-02  1.65316500e-02
  2.69018680e-01  3.78071293e-02 -7.18028191e-03  2.13054210e-01
  5.09665459e-02  5.32130122e-01 -4.55166847e-01 -1.36408523e-01
  5.18946499e-02 -1.13759533e-01 -6.49821013e-02 -6.40564799e-01
 -2.63850749e-01  2.43491858e-01 -2.54345894e-01  9.80739444e-02
 -2.80077346e-02  5.19136898e-04 -9.90527049e-02 -1.33890897e-01
  5.94147630e-02 -6.66181892e-02  3.33749175e-01  3.41165125e-01
  4.74708900e-03  1.01455070e-01  4.87517238e-01 -3.86013746e-01
 -1.52819008e-01 -1.37381166e-01  2.18773842e-01  3.22706342e-01
  4.47426915e-01 -4.87388611e-01  1.95023060e-01 -2.43649632e-01
  1.48170412e-01  4.90898252e-01  4.39378321e-02  2.06759602e-01
 -3.56091589e-01  4.88365918e-01  3.77516091e-01 -1.44196004e-01
  3.14105809e-01 -2.81059682e-01 -4.83925700e-01 -2.37732120e-02
  2.22237796e-01 -3.47313255e-01  2.42105067e-01 -5.70476472e-01
 -8.24225470e-02  2.95687020e-01 -2.57514358e-01 -8.20872188e-02
  3.60364616e-02 -4.07272950e-02 -2.21445933e-01 -1.92529764e-02
 -4.78118211e-01  1.30560443e-01  3.69193032e-02 -3.04088920e-01
 -4.44991231e-01 -1.19820505e-01 -7.04092085e-02 -1.99030936e-01
  3.88855394e-03 -2.32355788e-01  3.09447348e-01  3.44506860e-01
 -1.90235570e-01  7.12853596e-02  1.61306128e-01  2.83639655e-02
 -4.31134433e-01 -7.93186575e-02  3.25977802e-02  1.58988148e-01
  1.53202474e-01 -2.33409867e-01  5.64050376e-01  2.19570488e-01
 -1.25989482e-01 -2.10544840e-02 -1.35056347e-01 -2.23399270e-02
  1.60781175e-01 -2.41589218e-01 -1.85747385e-01 -2.09185660e-01
  8.89892131e-02  3.21651816e-01  4.28802893e-02  1.95472926e-01
 -3.43203306e-01  1.30596489e-01  5.03474832e-01 -6.59217298e-01
 -3.64794791e-01  1.23218961e-01  2.79904544e-01 -4.65662688e-01
 -6.02653474e-02 -1.34493470e-01  2.88813412e-01 -2.27928422e-02]"
tf.math.is_non_decreasing outputs incorrect result when input is an uint tensor awaiting review type:bug comp:apis TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When giving tf.math.is_non_decreasing a decreasing tensor with dtype=uint32. This API incorrectly output True instead of False.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([10,9], dtype='uint32')
print(x)
out = tf.math.is_non_decreasing(x)
print(out)
```


### Relevant log output

_No response_",True,"[-3.99884909e-01 -4.68681663e-01 -9.90146175e-02  1.77759439e-01
  5.55738397e-02 -4.17936921e-01  2.07090713e-02 -4.80414480e-02
 -2.68551290e-01 -1.34206116e-01  3.34362388e-02 -1.09181702e-01
 -3.23645115e-01  3.70190322e-01 -4.41797793e-01  4.58504766e-01
 -2.39082992e-01 -2.81694680e-02  3.15110147e-01 -3.48211341e-02
 -1.54725105e-01 -1.62574708e-01 -3.29605192e-01  4.39040691e-01
  5.20317368e-02  1.44885808e-01 -1.70754313e-01 -1.33623928e-01
  1.18711531e-01  2.63627172e-01  9.43877101e-02  1.41252041e-01
 -1.18922181e-01 -9.62931812e-02 -1.18932098e-01  4.38510060e-01
 -2.02493623e-01 -1.27261102e-01 -2.94120193e-01 -2.96420325e-03
 -1.04098424e-01  8.50590542e-02  2.82882974e-02 -1.31433651e-01
  2.22573563e-01  7.92915653e-03  1.97373495e-01 -1.73551098e-01
 -2.41315350e-01 -1.23635784e-01 -1.27637591e-02  1.14721887e-01
 -4.55288827e-01 -2.79038489e-01 -6.99635148e-02 -2.01015279e-01
  1.10460430e-01 -2.63597488e-01  2.74304867e-01  1.18965350e-01
  2.46903487e-02  7.30050057e-02 -9.90355089e-02 -6.76837265e-02
  2.87024468e-01  1.69865236e-01  5.01669645e-01 -1.44057438e-01
  1.54376119e-01 -2.05856472e-01  3.71881068e-01 -1.65997207e-01
 -2.76131064e-01  1.05260998e-01  1.70385223e-02 -5.12506533e-03
  1.11387543e-01  3.18625718e-01  4.44411635e-01 -7.89532512e-02
  1.43657953e-01 -8.05003345e-02 -8.44531059e-02 -3.39795083e-01
  1.53303683e-01 -1.53314278e-01  4.44174945e-01  8.53572488e-02
  4.81704950e-01 -5.53310633e-01  4.31024134e-01  2.51544058e-01
 -2.01032370e-01  2.10972607e-01  5.32700181e-01  5.56463338e-02
  8.39191079e-02  1.34069715e-02  6.44631237e-02  2.32199840e-02
 -1.52108908e-01 -2.10384667e-01 -2.54147410e-01 -1.41910762e-01
 -1.03713617e-01 -1.14541128e-02  2.32607082e-01 -5.18748350e-02
  3.18436623e-01  5.43241091e-02 -2.73303445e-02 -1.43064912e-02
  3.15401286e-01  1.07275009e-01 -5.16815074e-02 -5.16931750e-02
  1.10299475e-01 -5.22344336e-02  2.77419537e-01  7.93546021e-01
  1.88443422e-01 -4.07209322e-02 -1.87620483e-02  2.69807220e-01
  5.13363123e-01  1.67048186e-01 -1.53094366e-01  5.07477522e-02
  1.85150534e-01 -1.96244165e-01  3.33665431e-01 -3.55237350e-02
 -1.86030105e-01  2.85691559e-01 -2.05305338e-01  9.30497274e-02
 -9.49218422e-02 -1.44459456e-01 -4.03744578e-01 -2.28975862e-01
 -3.76434535e-01  2.72017926e-01 -1.71830863e-01 -6.29389584e-01
  1.56071037e-01 -1.32697951e-02 -1.16371334e-01  4.40058023e-01
 -3.35658729e-01  3.23469043e-02 -4.13954780e-02  2.00757638e-01
  1.53524697e-01  2.55814135e-01  2.88562719e-02  2.85103917e-02
  3.91396552e-01 -1.91050053e-01 -1.22451670e-01 -5.08480847e-01
  1.88514397e-01  2.28671402e-01 -4.08834666e-01 -2.44028032e-01
  1.29884556e-01  4.05664176e-01 -3.51896286e-01 -4.46224421e-01
  7.10205510e-02  5.12537897e-01 -2.28321880e-01 -1.34978920e-01
  6.43955842e-02  2.03877628e-01  1.51625887e-01 -2.99119592e-01
  2.64941007e-01 -6.12139165e-01 -2.58564830e-01  4.01125312e-01
 -5.48814163e-02  6.33917898e-02  2.16386557e-01  3.78866866e-02
  1.14143025e-02 -5.69967330e-02  1.17625676e-01  4.37885702e-01
 -2.06728563e-01  4.52231616e-05 -5.27169943e-01 -2.94569790e-01
  3.86728764e-01  1.57303996e-02  1.01131141e-01 -7.81362429e-02
  2.23742098e-01 -6.01852238e-01 -3.29410806e-02  1.49395585e-01
 -8.16008970e-02 -2.13116139e-01 -1.82653785e-01 -1.85996681e-01
  9.55156088e-02 -2.79367059e-01  4.09748591e-02 -5.37843406e-01
 -4.01887238e-01 -8.78032744e-02  2.28958040e-01 -6.30172253e-01
  1.79816872e-01 -1.53055072e-01 -2.78257251e-01  9.20095518e-02
  8.48368257e-02 -3.16116922e-02 -2.54155606e-01  3.48498106e-01
  6.48461431e-02 -3.73942964e-03  2.26132900e-01 -4.05360192e-01
 -4.11497593e-01 -5.42954803e-02 -4.64827240e-01  2.94055670e-01
 -3.45723003e-01  2.75924914e-02  2.56062150e-01  2.53053494e-02
  3.46612215e-01  1.13645077e-01  5.16425371e-01 -2.95275331e-01
 -4.37573314e-01 -8.62596631e-02 -1.91815555e-01  9.35817510e-02
 -4.98027563e-01 -1.51772827e-01  1.22974008e-01 -1.29445046e-01
  2.97837049e-01  6.82608426e-01 -2.62802064e-01 -2.11226344e-01
 -4.59307700e-01  5.13868570e-01 -1.60829440e-01  2.98571736e-01
  2.87476957e-01  9.49554965e-02  4.35378522e-01  5.34993149e-02
  3.83326054e-01  1.93928510e-01  4.47565228e-01 -2.10599110e-01
  4.63047147e-01  2.15335935e-01  1.51711732e-01  6.65505171e-01
  5.55306196e-01  5.08916438e-01 -1.72185779e-01  3.66330981e-01
 -4.56570573e-02 -1.21279329e-01 -1.86279818e-01 -3.59982789e-01
  7.94195950e-01 -4.45257336e-01  1.51873231e-01 -2.59631217e-01
  4.55211133e-01  1.94196448e-01 -1.54619262e-01  1.25204295e-01
  4.93953228e-02  3.63494009e-01 -3.20970476e-01  2.04608217e-01
 -2.68884569e-01 -1.98615059e-01  5.31940684e-02 -7.30478048e-01
  7.63668492e-03  1.70995295e-02 -3.32525253e-01  2.19159305e-01
  4.51283485e-01  2.33944990e-02 -1.62350938e-01  2.44248420e-01
 -2.17083581e-02 -1.15565360e-01  1.19601086e-01  1.42108679e-01
  7.05957860e-02 -2.65692659e-02  5.01690149e-01 -5.49085557e-01
 -1.83725998e-01  1.11057283e-02  5.86493552e-01  3.00888121e-01
  2.73903996e-01 -4.78422493e-01  1.13255382e-01 -2.47256681e-01
  5.41275293e-02  5.28612852e-01 -2.20577847e-02  1.17403321e-01
 -3.86862248e-01  4.02508825e-01  2.72441030e-01 -4.55647707e-04
  1.73296094e-01 -1.70262694e-01 -3.92675489e-01 -6.28783107e-02
  8.97198915e-02 -2.46210277e-01  2.09296435e-01 -4.79205340e-01
  8.83717984e-02 -6.81158081e-02 -2.48011485e-01  6.13586139e-03
 -1.54808313e-01 -1.64401770e-01 -2.96400189e-01 -1.65488362e-01
 -7.66935885e-01  1.31107554e-01  5.57332598e-02 -4.00634050e-01
 -1.62420988e-01 -6.19859509e-02 -2.52354324e-01 -9.19398367e-02
 -6.65960610e-02 -4.93469924e-01  3.97377491e-01  5.75416923e-01
 -1.97773278e-01 -4.75505590e-02  1.56198442e-02  2.65464544e-01
 -3.93910080e-01  8.14716890e-02  6.63019866e-02  3.09684575e-01
 -8.10020566e-02 -2.17467755e-01  3.09392273e-01  2.91007459e-01
 -2.60580212e-01  1.91727597e-02 -2.00199693e-01  4.37535234e-02
  3.22085083e-01 -3.04830641e-01 -1.86781377e-01 -2.01750636e-01
  3.84035468e-01  1.67577893e-01 -9.54859182e-02  5.49127400e-01
 -2.52910376e-01  1.56869054e-01  5.88066101e-01 -6.76696420e-01
 -1.82672590e-01  2.07823023e-01  1.71140909e-01 -3.32633317e-01
  7.17873871e-02 -1.42003015e-01  1.29191741e-01  1.14199862e-01]"
"Calculating Gradients for a graph containing tf.image.extract_patches, using TF 2.9.0 type:bug comp:ops TF 2.9","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.9.0-rc2-42-g8a20d54a3c1 2.9.0

### Custom code

Yes

### OS platform and distribution

Windows 10-64bit

### Mobile device

_No response_

### Python version

Python 3.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda11.2

### GPU model and memory

_No response_

### Current behavior?

My codes extract patches from a batch of  images as an input and produces a batch of  image patches as an output.

The input (batch_size, rows, cols, 1) is split into patches (patches_num, batch_size, patch_row, patch_col) before being fed to next layer of the network.  This function was called by a custom layer (keras.layers.Layer), and it works well during extracts the patches, but when calculating gradients for a graph with `grads = tape.gradient(loss, model.variables)`, an error came:
```
UnimplementedError: Graph execution error:

Detected at node 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul'

Node: 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul'
2 root error(s) found.
  (0) UNIMPLEMENTED:  A deterministic GPU implementation of SparseTensorDenseMatmulOp is not currently available.
	 [[{{node gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]]
	 [[gradients/fan_weight_8/StatefulPartitionedCall_grad/PartitionedCall/gradients/ExtractImagePatches_grad/ExtractImagePatches/_63]]
  (1) UNIMPLEMENTED:  A deterministic GPU implementation of SparseTensorDenseMatmulOp is not currently available.
	 [[{{node gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference___backward_call_20302_28692]
```
I found an issue about tf.extract_image_patches() opend on Feb 10, 2017 (https://github.com/tensorflow/tensorflow/issues/7414)
and added `tf.cast(input_batch, dtype=tf.float32, name=""castData"") ` in the function, but it wont work.

Here is my code of the function.

```
def spilt_patches(input_batch, 
                   blocks_num_cur, 
                   block_shape_cur):
   '''
    input_batch = np.tile(np.random.randint(0,2,(4,3,3)), (1,2,2))
    blocks_num_cur = 4,
    block_shape_cur = [6,6]
   '''
    shape = input_batch.shape
    input_batch = tf.reshape(input_batch,(shape[0],shape[1], shape[2], 1)) # [batch_size, rows, cols, 1]
    input_batch = tf.cast(input_batch, dtype=tf.float32, name=""castData"")  
    blocks_cur = tf.image.extract_patches(images = input_batch,
                                       sizes=[1, block_shape_cur[0], block_shape_cur[1], 1],
                                       strides=[1, block_shape_cur[0], block_shape_cur[1], 1],
                                       rates=[1, 1, 1, 1],
                                       padding='VALID') 
    blocks_cur = tf.cast(tf.reshape(blocks_cur,[shape[0], blocks_num_cur, block_shape_cur[0], block_shape_cur[1]]), tf.float32) # [batch_size, patch_num, patch_row, patch_col]
    blocks_cur = tf.transpose(blocks_cur,[1, 0, 2, 3]) 
    
    return blocks_cur
```

### Standalone code to reproduce the issue

```shell
https://gist.github.com/RaymondMarzas/7bec1a8099aded089a29d775caef880e
```


### Relevant log output

```shell
Traceback (most recent call last):

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\py3compat.py"", line 356, in compat_exec
    exec(code, globals, locals)

  File ""d:\codes\pycoode\ocnn2nd\fanoutmodel_train_ocnn.py"", line 112, in <module>
    grads = tape.gradient(loss, model.variables)

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\backprop.py"", line 1106, in gradient
    unconnected_gradients=unconnected_gradients)

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\imperative_grad.py"", line 73, in imperative_grad
    compat.as_str(unconnected_gradients.value))

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\function.py"", line 1206, in _backward_function_wrapper
    processed_args, remapped_captures)

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\function.py"", line 1861, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\function.py"", line 502, in call
    ctx=ctx)

  File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tensorflow\python\eager\execute.py"", line 55, in quick_execute
    inputs, attrs, num_outputs)

UnimplementedError: Graph execution error:

Detected at node 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul' defined at (most recent call last):
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\runpy.py"", line 193, in _run_module_as_main
      ""__main__"", mod_spec)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\runpy.py"", line 85, in _run_code
      exec(code, run_globals)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\console\__main__.py"", line 24, in <module>
      start.main()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\console\start.py"", line 340, in main
      kernel.start()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelapp.py"", line 712, in start
      self.io_loop.start()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tornado\platform\asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\base_events.py"", line 541, in run_forever
      self._run_once()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\base_events.py"", line 1786, in _run_once
      handle._run()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\events.py"", line 88, in _run
      self._context.run(self._callback, *self._args)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 730, in execute_request
      reply_content = await reply_content
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\ipkernel.py"", line 387, in do_execute
      cell_id=cell_id,
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\zmqshell.py"", line 528, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 2975, in run_cell
      raw_cell, store_history, silent, shell_futures, cell_id
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3029, in _run_cell
      return runner(coro)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3257, in run_cell_async
      interactivity=interactivity, compiler=compiler, result=result)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3472, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3552, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""C:\Users\xxx\AppData\Local\Temp\ipykernel_29532\1338903705.py"", line 1, in <module>
      runfile('D:/CODES/pycoode/OCNN2nd/fanoutmodel_train_ocnn.py', wdir='D:/CODES/pycoode/OCNN2nd')
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 526, in runfile
      post_mortem, current_namespace, stack_depth=1)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 613, in _exec_file
      capture_last_expression=False)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 469, in exec_code
      exec_fun(compile(ast_code, filename, 'exec'), ns_globals, ns_locals)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\py3compat.py"", line 356, in compat_exec
      exec(code, globals, locals)
    File ""d:\codes\pycoode\ocnn2nd\fanoutmodel_train_ocnn.py"", line 97, in <module>
      trn_pred = model(X_trn) # input[BATCH_SIZE,TARGET_ROWS*TARGET_COLS]], output[BATCH_SIZE,1]
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\engine\training.py"", line 490, in __call__
      return super().__call__(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\engine\base_layer.py"", line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
Node: 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul'
Detected at node 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul' defined at (most recent call last):
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\runpy.py"", line 193, in _run_module_as_main
      ""__main__"", mod_spec)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\runpy.py"", line 85, in _run_code
      exec(code, run_globals)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\console\__main__.py"", line 24, in <module>
      start.main()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\console\start.py"", line 340, in main
      kernel.start()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelapp.py"", line 712, in start
      self.io_loop.start()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\tornado\platform\asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\base_events.py"", line 541, in run_forever
      self._run_once()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\base_events.py"", line 1786, in _run_once
      handle._run()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\asyncio\events.py"", line 88, in _run
      self._context.run(self._callback, *self._args)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\kernelbase.py"", line 730, in execute_request
      reply_content = await reply_content
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\ipkernel.py"", line 387, in do_execute
      cell_id=cell_id,
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\ipykernel\zmqshell.py"", line 528, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 2975, in run_cell
      raw_cell, store_history, silent, shell_futures, cell_id
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3029, in _run_cell
      return runner(coro)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3257, in run_cell_async
      interactivity=interactivity, compiler=compiler, result=result)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3472, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\IPython\core\interactiveshell.py"", line 3552, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""C:\Users\xxx\AppData\Local\Temp\ipykernel_29532\1338903705.py"", line 1, in <module>
      runfile('D:/CODES/pycoode/OCNN2nd/fanoutmodel_train_ocnn.py', wdir='D:/CODES/pycoode/OCNN2nd')
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 526, in runfile
      post_mortem, current_namespace, stack_depth=1)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 613, in _exec_file
      capture_last_expression=False)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 469, in exec_code
      exec_fun(compile(ast_code, filename, 'exec'), ns_globals, ns_locals)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\spyder_kernels\py3compat.py"", line 356, in compat_exec
      exec(code, globals, locals)
    File ""d:\codes\pycoode\ocnn2nd\fanoutmodel_train_ocnn.py"", line 97, in <module>
      trn_pred = model(X_trn) # input[BATCH_SIZE,TARGET_ROWS*TARGET_COLS]], output[BATCH_SIZE,1]
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\engine\training.py"", line 490, in __call__
      return super().__call__(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 64, in error_handler
      return fn(*args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\engine\base_layer.py"", line 1014, in __call__
      outputs = call_fn(inputs, *args, **kwargs)
    File ""C:\ProgramData\Anaconda3\envs\TF2.9.0\lib\site-packages\keras\utils\traceback_utils.py"", line 92, in error_handler
      return fn(*args, **kwargs)
Node: 'gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul'
2 root error(s) found.
  (0) UNIMPLEMENTED:  A deterministic GPU implementation of SparseTensorDenseMatmulOp is not currently available.
	 [[{{node gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]]
	 [[gradients/fan_weight_8/StatefulPartitionedCall_grad/PartitionedCall/gradients/ExtractImagePatches_grad/ExtractImagePatches/_63]]
  (1) UNIMPLEMENTED:  A deterministic GPU implementation of SparseTensorDenseMatmulOp is not currently available.
	 [[{{node gradients/ExtractImagePatches_1_grad/SparseTensorDenseMatMul/SparseTensorDenseMatMul}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference___backward_call_20302_28692]
```
",True,"[-6.72154188e-01 -4.33412194e-01 -3.15987952e-02  3.35770380e-03
  2.29637444e-01 -3.89180928e-02  7.99202472e-02  2.26885825e-02
 -5.45779586e-01 -1.45000771e-01 -1.10901129e-02 -8.94969925e-02
 -3.75615433e-03  2.06751153e-01 -2.77833700e-01  5.33650577e-01
  5.01391292e-02  3.36960852e-02 -5.11233974e-03 -1.93489313e-01
 -1.12746239e-01 -2.30360791e-01 -2.07404464e-01  8.14085156e-02
  3.26471150e-01 -3.98296192e-02 -2.65359253e-01 -1.85820386e-01
  6.55458719e-02  1.83164716e-01  5.91791831e-02  1.53957829e-01
 -1.24002144e-01  1.13439985e-01 -1.68422103e-01  2.23957509e-01
 -2.19230741e-01 -1.53588668e-01 -1.67514324e-01  1.00063041e-01
 -2.27445230e-01  4.70715575e-03 -5.20774722e-03  8.09039772e-02
 -3.01627815e-03 -2.33318776e-01  1.49123639e-01 -1.51307946e-02
 -2.20072925e-01  3.86874676e-02  1.03582397e-01 -1.76318631e-01
 -5.42089939e-01 -4.27883685e-01 -7.14506879e-02  1.73731118e-01
  2.81731457e-01 -4.60000187e-02  2.03464925e-01 -5.03393263e-02
  8.26355368e-02  6.28894866e-02  1.95024252e-01  5.52487262e-02
  2.96897233e-01  1.52606905e-01  6.58746585e-02 -9.09940973e-02
  6.07535481e-01  1.88864190e-02 -3.77903581e-02 -2.23711535e-01
 -3.66441935e-01 -1.00328095e-01  9.30560194e-03  5.50607592e-02
  1.06841691e-01  1.78765655e-01  2.01779932e-01 -3.00230950e-01
  2.36354411e-01 -3.15417886e-01  5.53696696e-03  6.61846250e-02
  2.16444299e-01 -1.31135374e-01  3.45382154e-01  1.94465756e-01
  6.31579161e-01 -5.80745935e-02  6.25298023e-01  1.70336276e-01
 -1.91045150e-01  3.29534858e-02  2.77891397e-01  2.29372352e-01
 -1.28830105e-01  1.65301353e-01  1.77434713e-01  9.51114595e-02
  1.14528071e-02 -4.33341354e-01 -1.44851103e-01  1.39886141e-01
 -1.31489784e-02 -4.11998257e-02  1.43662080e-01 -8.75235200e-02
  1.55458704e-01  2.48563558e-01  1.15225188e-01 -4.21036482e-02
  2.55383909e-01 -1.11961551e-03 -1.56409889e-02 -3.37503880e-01
  9.96149629e-02  1.07658714e-01  1.93668336e-01  5.87201655e-01
  2.68647045e-01 -3.35931405e-02  1.91345841e-01  2.43714929e-01
  4.34496492e-01  1.28227562e-01 -4.28095981e-02  4.91910279e-02
  2.50352204e-01  9.13715214e-02  1.33615911e-01  9.81698856e-02
  1.09788217e-01  1.63115144e-01 -1.27277836e-01 -3.14302705e-02
 -9.21542570e-02  8.18891637e-03 -4.23370361e-01 -1.43170863e-01
 -4.53309000e-01  2.86447287e-01 -6.16730005e-02 -3.65231752e-01
  2.03695357e-01  2.92955339e-03 -2.36392140e-01  4.59493876e-01
 -2.57800519e-01 -2.01895177e-01 -1.30992532e-01  9.36136395e-02
 -3.30734134e-01  2.28197172e-01  1.40943006e-01  2.70150661e-01
  2.62600511e-01 -1.10512719e-01 -1.15640089e-03 -6.41545415e-01
  9.00607184e-02  2.18701869e-01 -1.61274984e-01 -1.58508480e-01
  1.76523387e-01  2.97306001e-01 -1.26501322e-01 -2.06109464e-01
  6.77628741e-02  3.19870114e-01  5.37073202e-02 -1.65460348e-01
  4.10255417e-02  2.35426322e-01  3.64452928e-01 -1.29123002e-01
 -3.08263302e-02 -6.07869864e-01 -6.21940307e-02  2.51724124e-01
  1.10971555e-01 -9.85288620e-02  1.04681648e-01  1.67778835e-01
  1.18761167e-01  9.81887896e-03  2.98781812e-01  2.89129257e-01
 -5.04857659e-01  2.51204856e-02 -2.84999013e-01 -3.03815931e-01
  3.50537628e-01 -2.56961547e-02 -1.33313388e-01 -1.85829885e-02
  2.11657137e-01 -9.78584737e-02 -5.72774038e-02  4.48879242e-01
  4.73029912e-02 -2.34383315e-01 -9.16532502e-02 -1.03284776e-01
  2.83788592e-02 -1.68518215e-01 -4.16588336e-02 -4.91687000e-01
 -5.91994762e-01  6.62820041e-02  3.05757634e-02 -6.66402698e-01
  9.64817852e-02 -6.70744330e-02 -9.97339860e-02  2.21164882e-01
  1.88522011e-01  2.20802408e-02 -2.66152024e-01  1.06154732e-01
  1.70618281e-01 -3.16702425e-02 -2.72865966e-03 -3.15211475e-01
 -5.05666614e-01  1.63145512e-01 -3.46308261e-01 -6.00279644e-02
 -7.85075575e-02 -1.05235539e-02 -2.51293257e-02  1.23484716e-01
  3.22249413e-01  3.12180400e-01  4.35118616e-01 -1.07995868e-01
 -2.43654191e-01 -2.07434207e-01 -1.30843684e-01  8.49091262e-02
 -4.03045893e-01  1.73491091e-01 -1.89076155e-01 -1.53601706e-01
  1.62412897e-01  4.45827037e-01 -1.85481340e-01  1.18662879e-01
 -5.42172380e-02  2.54036158e-01 -2.43263528e-01 -7.75268525e-02
  2.38181606e-01  3.47039729e-01  1.45062447e-01  2.84600835e-02
  1.37377173e-01  8.02836567e-02  1.74711049e-01 -9.40907374e-02
  2.85936236e-01  2.75814552e-02  1.25082172e-02  4.98372525e-01
  3.20599884e-01  2.30851978e-01 -1.27223954e-01  6.91025078e-01
 -5.80211133e-02 -7.81439319e-02  1.00998111e-01 -2.16090351e-01
  4.56195235e-01 -3.95999670e-01  5.44775724e-02  1.90586507e-01
  3.30095142e-01  5.63198552e-02 -2.40741834e-01  1.46341667e-01
 -1.76385254e-01  3.17563862e-01 -4.11388755e-01  1.25659138e-01
 -3.92358422e-01 -3.82187590e-02 -1.66602492e-01 -5.91178894e-01
 -1.27010494e-01 -9.97584015e-02 -2.76959091e-01 -1.11904465e-01
  1.81521997e-02 -8.96541178e-02 -2.97386423e-02  1.74703211e-01
  1.26276821e-01 -2.19567895e-01  2.09537625e-01 -1.16100550e-01
  1.97527915e-01  4.41361487e-01  2.21362278e-01 -3.97383809e-01
 -1.81372523e-01 -7.22514242e-02  6.72004223e-01  2.28787046e-02
  3.88233334e-01 -3.90264899e-01 -7.78040811e-02  1.34618014e-01
  1.47973746e-01  4.92785633e-01  6.00807779e-02  5.35047427e-03
 -4.03373122e-01  4.60532308e-01  3.22811067e-01  4.32730019e-02
  5.34352511e-02 -3.09881121e-01 -3.39585632e-01 -2.94175506e-01
  2.82135427e-01  1.44188643e-01  1.21854551e-01 -3.64337444e-01
  7.63736963e-02 -2.74891406e-03  4.59024087e-02 -1.51997805e-03
 -2.52211839e-01  2.36454234e-01 -2.81863958e-02 -5.83892465e-02
 -3.26968312e-01  2.29779243e-01 -1.06159955e-01 -1.70837373e-01
 -4.43738103e-01 -2.90812582e-01 -3.73387709e-04 -3.10504735e-01
 -1.36796013e-01 -2.75542229e-01 -4.52537388e-02  1.44895628e-01
 -5.20297959e-02 -4.66705263e-02 -3.66705749e-03  6.51462153e-02
 -4.01850581e-01  3.92264239e-02  8.03978518e-02  5.80395997e-01
 -8.32378026e-03 -2.52292573e-01  1.31942451e-01  2.83195376e-01
 -4.98273879e-01 -1.25352014e-02  9.91840474e-03 -3.64699550e-02
  7.69150779e-02 -1.78352818e-01 -1.44325197e-01 -4.59332585e-01
 -2.81026270e-02  1.03572845e-01 -1.34814113e-01  6.80716783e-02
 -3.25174272e-01  4.15325224e-01  5.37066221e-01 -5.87184072e-01
  2.08364055e-02  6.91864938e-02  2.18187630e-01 -1.58734053e-01
 -9.12473649e-02 -2.42190305e-02  3.32755782e-02 -1.24541968e-01]"
"in C++No OpKernel was registered to support Op 'PyFunc' used by {{node PyFunc}}with these attrs: [Tin=[], Tout=[DT_FLOAT], token=""pyfunc_0""] Registered devices: [CPU, XLA_CPU] Registered kernels:   <no registered kernels>           [[PyFunc]] stat:awaiting response type:bug stale TF 1.15","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 1.15.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

15001379103

### Python version

_No response_

### Bazel version

4.2.1

### GCC/compiler version

4.8.5

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

method: C API
version: 1.15.0
package link:https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-linux-x86_64-1.15.0.tar.gz





### Standalone code to reproduce the issue

```shell
#include ""app/federal/model.hpp""
#include <iostream>
#include <fstream>

namespace xcxxx {
    /**
     * Print Tensorflow version
    */
  void Model::PrintTfVersion() {
    std::cout << ""Hello from TensorFlow C library version \n"" <<  TF_Version() << std::endl;
  }

  void NoOpDeallocator(void* data, size_t a, void* b) {
  }

  void DeallocateBuffer(void* data, size_t) {
    std::free(data);
  }

  TF_Buffer* ReadBufferFromFile(std::string file) {
    std::ifstream f(file, std::ios::binary);
    if (f.fail() || !f.is_open()) {
      return nullptr;
    }

    f.seekg(0, std::ios::end);
    const auto fsize = f.tellg();
    f.seekg(0, std::ios::beg);

    if (fsize < 1) {
      f.close();
      return nullptr;
    }

    char* data = static_cast<char*>(std::malloc(fsize));
    f.read(data, fsize);
    f.close();

    TF_Buffer* buf = TF_NewBuffer();
    buf->data = data;
    buf->length = fsize;
    buf->data_deallocator = DeallocateBuffer;
    return buf;
  }
    /**
     * Init Model Instance
    */
   bool Model::Init(std::string model_file) {
    try {

        // create new graph
        TF_Buffer* buffer = ReadBufferFromFile(model_file);
        if (buffer == nullptr) {
          std::cout << ""code:10000"" <<  "", msg: "" <<  ""Error creating the session from  the given model path!""<< std::endl;
          return false;
        }
        TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();
        TF_Status* status = TF_NewStatus();

        m_graph_ = TF_NewGraph();
        TF_GraphImportGraphDef(m_graph_, buffer, opts, status);
        TF_DeleteImportGraphDefOptions(opts);
        TF_DeleteBuffer(buffer);
        if (TF_GetCode(status) != TF_OK) {
            std::cout << ""code:10001"" <<  "", msg: "" << TF_Message(status) << std::endl;
            TF_DeleteGraph(m_graph_);
            m_graph_ = nullptr;
            return false;
        }
        TF_DeleteStatus(status);

        // create new session
        status = TF_NewStatus();
        TF_SessionOptions* options = TF_NewSessionOptions();
        m_session_ = TF_NewSession(m_graph_, options, status);
        TF_DeleteSessionOptions(options);
        if (TF_GetCode(status) != TF_OK) {
            std::cout << ""code:10002"" <<  "", msg: "" << TF_Message(status) << std::endl;
            return false;
        }
        TF_DeleteStatus(status);
        std::cout << ""code:0"" <<  "", msg: load model suc!^_^"" << std::endl;
    
    } catch (std::exception& e) {
        std::cout << ""code:10003"" <<  "", msg: model init.Tf load fail!"" << std::endl;
        return false;
    }

    return true;
   }


  /**
     * Init Model Instance
    */
   bool Model::InitV2(std::string model_file) {
    try {
      TF_Buffer* buffer = ReadBufferFromFile(model_file);
      if (buffer == nullptr) {
        std::cout << ""code:10000"" <<  "", msg: model init.Tf load fail!"" << std::endl;
      }
      TF_Status* status = TF_NewStatus();
      TF_ImportGraphDefOptions* opts = TF_NewImportGraphDefOptions();

      m_graph_ = TF_NewGraph();
      TF_GraphImportGraphDef(m_graph_, buffer, opts, status);
      TF_DeleteImportGraphDefOptions(opts);
      TF_DeleteBuffer(buffer);
      if (TF_GetCode(status) != TF_OK) {
        TF_DeleteGraph(m_graph_);
        m_graph_ = nullptr;
      }
      TF_DeleteStatus(status);

      // create session from graph
      status = TF_NewStatus();
      TF_SessionOptions* options = TF_NewSessionOptions();
      m_session_ = TF_NewSession(m_graph_, options, status);
      TF_DeleteSessionOptions(options);
    } catch (std::exception& e) {
        std::cout << ""code:10003"" <<  "", msg: model init.Tf load fail!"" << std::endl;
        return false;
    }

    return true;
   }

   /**
    *  infer
   */
  std::vector<float> Model::Infer(std::vector<float> input_data) {
    std::vector<float> ret;

    //****** 1Define graph inputs/outputs
    // std::string train_input_name = ""top_model/inputs:0"";
    std::string train_input_name = ""top_model/inputs"";
    // std::string train_output_name = ""top_model/outputs:0"";
    std::string train_output_name = ""top_model/outputs"";

    // Define graph inputs
    uint16_t gr_input_nums = 1;
    TF_Output* inputs = (TF_Output*)malloc(sizeof(TF_Output) * gr_input_nums);
    TF_Output input0 = {TF_GraphOperationByName(m_graph_, train_input_name.c_str()), 0};


    size_t pos = 0;
    uint16_t i = 0;
    TF_Operation* oper;
    
    while ((oper = TF_GraphNextOperation(m_graph_, &pos)) != nullptr) {
      std::cout << ""index:"" << i << "", graphname:"" << TF_OperationName(oper) << std::endl;
      i++;
    }

    if (nullptr == input0.oper) {
      std::cout << ""code:20000"" <<  "", msg: Define graph inputs failure"" << std::endl;
      return ret;
    }
    inputs[0] = input0;

    // Define graph outpus
    uint16_t gr_output_nums = 1;
    TF_Output* outputs = (TF_Output*)malloc(sizeof(TF_Output) * gr_output_nums);
    TF_Output output0 = {TF_GraphOperationByName(m_graph_, train_output_name.c_str()), 0};
    if (nullptr == output0.oper) {
      std::cout << ""code:20001"" <<  "", msg: Define graph outputs failure"" << std::endl;
      return ret;
    }
    outputs[0] = output0;


    //****** 2Create input tensor(s) and populate with feature
    TF_Tensor** input_values = (TF_Tensor**)malloc(sizeof(TF_Tensor*) * gr_input_nums);
    int16_t num_dims = 1;
    int64_t dims[] = {1};
    float_t* data = &input_data[0];
    TF_Tensor* float_tensor = TF_NewTensor(TF_FLOAT, dims, num_dims, data, sizeof(TF_FLOAT), &NoOpDeallocator, 0);
    if (nullptr == float_tensor) {
      std::cout << ""code:20002"" <<  "", msg: allocate failure"" << std::endl;
      return ret;
    }
    input_values[0] = float_tensor;

    TF_Tensor** output_values = (TF_Tensor**)malloc(sizeof(TF_Tensor*) * gr_output_nums);

    //****** 3Run the session
    TF_Status* status = TF_NewStatus();
    TF_SessionRun(
      m_session_, 
      nullptr, 
      // Input tensors
      inputs, input_values, gr_input_nums,
      // Output tensors
      outputs, output_values, gr_output_nums,
      // Target operations
      nullptr, 0,
      // RunMedata
      nullptr,
      // Output status
      status
    );

    if (TF_OK != TF_GetCode(status)) {
      std::cout << ""code:20003"" <<  "", msg: infer failure!\nstatus:"" << TF_Message(status) << std::endl;
      return ret;
    }
    std::cout << ""code:0"" <<  "", msg: infer suc!^_^"" << std::endl;

    //****** 4Output the result
    void* buff = TF_TensorData(output_values[0]);
    float* offsets = (float*)buff;
    std::cout << ""infer result:"" << offsets[0] << std::endl;


    TF_DeleteStatus(status);
    return ret;
  }

}
```


### Relevant log output

```shell
No OpKernel was registered to support Op 'PyFunc' used by {{node PyFunc}}with these attrs: [Tin=[], Tout=[DT_FLOAT], token=""pyfunc_0""]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  <no registered kernels>

         [[PyFunc]]
```
",True,"[-6.93347216e-01 -2.19201446e-01 -1.30011424e-01  8.92213359e-02
  3.11688408e-02 -3.09879363e-01 -4.57209051e-02  3.26635659e-01
 -2.55216509e-01 -1.83452949e-01  1.14373103e-01  3.36525999e-02
  9.49175507e-02 -6.04851432e-02 -1.11834221e-02  2.06219375e-01
  7.68285394e-02 -3.21905494e-01  3.51451635e-01 -1.24637514e-01
 -3.94463353e-02  3.41317877e-02  2.95918100e-02  2.12069787e-02
 -1.72186028e-02 -1.73850954e-01 -1.51529178e-01  4.68217470e-02
  3.57304178e-02  1.83400914e-01  4.53494459e-01  2.36351281e-01
 -1.11895807e-01  5.74818812e-03  1.74937099e-02  1.04877591e-01
 -1.73936591e-01 -1.22567214e-01 -2.10278481e-02  4.43472005e-02
 -1.34326011e-01  9.77217779e-02  3.88203077e-02 -8.24962649e-03
  9.63644832e-02 -1.52663037e-01 -8.03546906e-02  6.78946823e-02
 -6.15278855e-02 -1.62905082e-04  3.57204795e-01 -2.26369090e-02
 -2.24312752e-01 -3.47346187e-01  1.11627832e-01 -1.84596598e-01
 -2.44627923e-01  1.55698538e-01  4.92265150e-02  3.30344200e-01
  3.28735948e-01 -1.10566646e-01 -2.22170472e-01  1.96896419e-02
 -2.32210755e-02  2.11291432e-01  2.94188619e-01 -2.80064225e-01
  3.64844650e-01 -7.17699826e-02 -4.94180545e-02 -1.06181074e-02
 -1.35302633e-01  7.14163203e-03  1.73922330e-01  1.43840015e-01
 -2.32800961e-01 -2.02187896e-02 -5.36688939e-02 -2.20515192e-01
  1.37191102e-01 -2.13850692e-01  2.40588039e-01 -5.90382107e-02
  2.21370906e-01  8.96801353e-02  1.46305054e-01  2.46782750e-01
  2.50462860e-01 -1.23830214e-01  4.00843441e-01  3.33111137e-01
 -4.03194726e-01  1.15642525e-01  1.44735396e-01  1.07673183e-01
  1.90107703e-01  6.85752779e-02 -2.44061708e-01 -7.36614093e-02
 -4.78033721e-02 -2.20839560e-01 -1.88986629e-01  1.45484686e-01
  8.45483169e-02  9.08498317e-02  1.45661980e-01 -2.09306516e-02
 -4.88398038e-02 -6.62374962e-03  1.35180861e-01 -1.70420967e-02
 -4.06240672e-02 -1.34638652e-01 -8.31952244e-02 -1.92294300e-01
 -7.58294910e-02  1.09364986e-01  3.41817886e-02  3.13514829e-01
  1.52010128e-01  5.32988124e-02 -4.85338122e-02  1.71478644e-01
  3.59978706e-01 -5.66842183e-02  9.75500047e-02  7.93828964e-02
 -1.12813264e-01  2.54121691e-01 -3.58528048e-02  1.14415497e-01
 -6.81542084e-02  1.25254035e-01  1.06613219e-01 -1.16600931e-01
 -1.81986779e-01  5.91556989e-02 -1.97926283e-01  6.96243048e-02
 -1.99547902e-01  1.36748943e-02  7.89441913e-02 -3.30931842e-01
 -1.97105967e-02 -1.04428008e-01 -8.99382979e-02  3.32324684e-01
  1.79581881e-01  4.77425195e-03 -1.75255872e-02  7.63902664e-02
 -9.16198641e-02  8.05704176e-01 -1.91469789e-01  1.83103651e-01
  1.92802846e-01  7.35932216e-02  2.06583127e-01 -4.44612384e-01
 -1.00464979e-02  1.69377774e-01  1.27899379e-01 -6.66847676e-02
  2.20803618e-02 -1.21894151e-01 -3.48685831e-01  9.08401161e-02
 -1.75900340e-01 -1.11118192e-03 -2.45829493e-01 -1.24727776e-02
  7.82296993e-05  1.81758046e-01  2.30407808e-02  2.66305823e-03
  3.80990237e-01 -4.73689288e-01 -2.00219095e-01  1.67099208e-01
  6.92605972e-05  2.81443354e-02  1.79978296e-01  5.52313104e-02
 -8.33868533e-02  5.72468527e-02  1.30073577e-01  3.41665186e-02
 -4.72333506e-02 -6.51123524e-02 -3.46012980e-01 -3.47059101e-01
 -2.54720412e-02 -8.56327266e-02  1.61578879e-02 -2.80521482e-01
 -2.80819181e-03  1.52388245e-01  2.20980704e-01  1.80236787e-01
 -1.74083963e-01  4.27679233e-02  1.19993180e-01  3.64101119e-02
  9.26192701e-02  1.51284486e-01 -4.41102058e-01 -1.75417930e-01
 -1.11436807e-01 -2.81348646e-01 -2.08430484e-01 -2.81430244e-01
  4.91590425e-02 -1.74481124e-02 -3.00304770e-01  1.64346233e-01
 -1.00598283e-01  3.44347060e-02 -1.21241063e-01  3.48739140e-02
 -2.24403031e-02 -8.97524506e-02 -1.29212767e-01 -2.45923996e-01
 -6.34969547e-02  7.98909068e-02  1.35000035e-01 -7.41696954e-02
 -1.50410727e-01  1.96985081e-01  1.15560830e-01 -3.43555450e-01
  4.00790095e-01  1.55230582e-01  1.55791700e-01  1.12338766e-01
 -2.56387770e-01 -1.02870669e-02  4.62800823e-02  1.86147451e-01
 -4.57745135e-01 -1.01625472e-01 -1.28992096e-01 -1.76638871e-01
  6.12563128e-03 -4.64901477e-02 -2.15183407e-01  1.05680950e-01
 -5.25225103e-02  5.51252544e-01 -3.08285475e-01 -1.24082007e-01
  1.52629048e-01  1.09108754e-01  4.88218665e-02  3.20570648e-01
  2.61185706e-01  1.35639176e-01  1.91441089e-01  1.30332217e-01
  2.31211588e-01  2.98345387e-01 -1.32749900e-01  5.79931885e-02
  2.49087855e-01  1.97536543e-01 -2.87518948e-01  4.89012688e-01
 -1.30821355e-02 -1.05039328e-01  1.50509015e-01 -4.35541987e-01
  3.38675767e-01 -2.10227966e-01  3.20505679e-01 -7.73375481e-02
  3.33107054e-01 -2.65155315e-01 -1.61322594e-01  2.97585845e-01
 -3.61571647e-03 -4.29734141e-02  2.61076808e-01 -1.76750064e-01
  7.85367340e-02 -3.60002130e-01 -7.48713538e-02 -1.30606428e-01
 -3.33323061e-01 -1.12314180e-01 -3.61175835e-02 -2.43615415e-02
  2.77170897e-01  1.31045401e-01 -3.77007052e-02  3.16266537e-01
  8.29143375e-02 -3.09552729e-01  5.87073602e-02  2.34437853e-01
 -3.46766151e-02 -8.39703977e-02 -2.17983928e-02 -9.08935070e-02
  8.99918005e-02  7.85663649e-02  1.51377469e-01  1.42787382e-01
  3.92213196e-01 -3.74110758e-01  3.06099981e-01  1.70513451e-01
 -6.09003007e-02  1.59286916e-01 -7.00643808e-02  2.87847131e-01
 -3.23674083e-01  5.10361791e-01  2.15158492e-01  1.35837123e-02
  1.46473125e-01 -3.10555659e-02 -1.86846003e-01 -6.46887794e-02
  1.70255139e-01 -4.45823371e-02 -7.31182769e-02 -2.26688504e-01
 -4.21586484e-02  1.32024229e-01 -1.59886479e-01  6.95397183e-02
 -1.26472637e-01 -2.49851838e-01  2.23554149e-02  1.85397863e-02
 -3.30553263e-01  3.22710901e-01 -8.52749273e-02 -1.00836083e-01
 -1.56590819e-01 -8.18752199e-02 -1.05497472e-01 -5.17826557e-01
 -2.33238846e-01 -1.16179913e-01  3.15824658e-01  5.64663708e-01
 -8.44406113e-02  1.23599850e-01  6.68679252e-02 -2.46964484e-01
 -2.64956295e-01 -1.80853698e-02 -2.50424445e-01  5.53857014e-02
  1.25283450e-02 -1.17903732e-01  2.22144663e-01  1.52143478e-01
 -4.07547623e-01  5.14519736e-02 -2.49596566e-01 -5.13336435e-02
 -1.03536204e-01 -6.19242415e-02  3.72612104e-02 -1.95736215e-02
  9.13065672e-02  3.52685392e-01 -1.35241389e-01  8.36484432e-02
 -5.20897329e-01  2.08511442e-01  1.35330468e-01 -2.57399142e-01
 -8.65857229e-02  8.06907788e-02  1.54128551e-01 -2.33217686e-01
 -2.97987759e-01 -1.12837285e-01  2.86988854e-01 -1.97112232e-01]"
model_main_tf2.py issues with training object detection model stat:awaiting response type:bug stale comp:model TF 2.8,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.8

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The tf_slim module class tfexample_decode.py at line 453 calls control_flow_ops.case 
There is no ""case"" attribute to control_flow_ops

### Standalone code to reproduce the issue

```shell
Google Colab, SSD Moblenet v2 graph, tfrecords
```


### Relevant log output

```shell
I1007 00:12:20.542787 134709293735936 dataset_builder.py:162] Reading unweighted datasets: ['.../train.record']
INFO:tensorflow:Reading record datasets for input file: ['.../train/train.record']
I1007 00:12:20.543272 134709293735936 dataset_builder.py:79] Reading record datasets for input file: ['.../train.record']
INFO:tensorflow:Number of filenames to read: 1
I1007 00:12:20.543381 134709293735936 dataset_builder.py:80] Number of filenames to read: 1
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W1007 00:12:20.543440 134709293735936 dataset_builder.py:86] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /content/models/research/./object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.
W1007 00:12:20.550345 134709293735936 deprecation.py:50] From /content/models/research/./object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.
WARNING:tensorflow:From /content/models/research/./object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W1007 00:12:20.572327 134709293735936 deprecation.py:50] From /content/models/research/./object_detection/builders/dataset_builder.py:235: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
Traceback (most recent call last):
  File ""/content/models/research/object_detection/model_main_tf2.py"", line 116, in <module>
    tf.compat.v1.app.run()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/platform/app.py"", line 36, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.10/dist-packages/absl/app.py"", line 308, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.10/dist-packages/absl/app.py"", line 254, in _run_main
    sys.exit(main(argv))
  File ""/content/models/research/object_detection/model_main_tf2.py"", line 107, in main
    model_lib_v2.train_loop(
  File ""/content/models/research/./object_detection/model_lib_v2.py"", line 563, in train_loop
    train_input = strategy.experimental_distribute_datasets_from_function(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py"", line 383, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 1563, in experimental_distribute_datasets_from_function
    return self.distribute_datasets_from_function(dataset_fn, options)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py"", line 1554, in distribute_datasets_from_function
    return self._extended._distribute_datasets_from_function(  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 613, in _distribute_datasets_from_function
    return input_util.get_distributed_datasets_from_function(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_util.py"", line 144, in get_distributed_datasets_from_function
    return input_lib.DistributedDatasetsFromFunction(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1143, in __init__
    self.build()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1165, in build
    _create_datasets_from_function_with_input_context(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1680, in _create_datasets_from_function_with_input_context
    dataset = dataset_fn(ctx)
  File ""/content/models/research/./object_detection/model_lib_v2.py"", line 554, in train_dataset_fn
    train_input = inputs.train_input(
  File ""/content/models/research/./object_detection/inputs.py"", line 908, in train_input
    dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
  File ""/content/models/research/./object_detection/builders/dataset_builder.py"", line 250, in build
    dataset = dataset_map_fn(dataset, decoder.decode, batch_size,
  File ""/content/models/research/./object_detection/builders/dataset_builder.py"", line 235, in dataset_map_fn
    dataset = dataset.map_with_legacy_function(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py"", line 383, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py"", line 4128, in map_with_legacy_function
    return map_op._map_v1_with_legacy_function(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py"", line 85, in _map_v1_with_legacy_function
    _ParallelMapDataset(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py"", line 148, in __init__
    self._map_func = structured_function.StructuredFunctionWrapper(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py"", line 272, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/function.py"", line 579, in add_to_graph
    self._create_definition_if_needed()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/function.py"", line 412, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/function.py"", line 430, in _create_definition_if_needed_impl
    temp_graph = func_graph_from_py_func(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/function.py"", line 1007, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py"", line 178, in wrapped_fn
    ret = wrapper_helper(*args)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py"", line 161, in wrapper_helper
    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 693, in wrapper
    raise e.ag_error_metadata.to_exception(e)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 690, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 439, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/__autograph_generated_fileqkyio7wq.py"", line 74, in tf__decode
    tensors = ag__.converted_call(ag__.ld(decoder).decode, (ag__.ld(serialized_example),), dict(items=ag__.ld(keys)), fscope)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 439, in converted_call
    result = converted_f(*effective_args, **kwargs)
  File ""/tmp/__autograph_generated_file1opxbxnu.py"", line 81, in tf__decode
    ag__.for_stmt(ag__.ld(items), None, loop_body_1, get_state_3, set_state_3, (), {'iterate_names': 'item'})
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 449, in for_stmt
    for_fn(iter_, extra_test, body, get_state, set_state, symbol_names, opts)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 500, in _py_for_stmt
    body(target)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 466, in protected_body
    original_body(protected_iter)
  File ""/tmp/__autograph_generated_file1opxbxnu.py"", line 77, in loop_body_1
    ag__.converted_call(ag__.ld(outputs).append, (ag__.converted_call(ag__.ld(handler).tensors_to_item, (ag__.ld(keys_to_tensors),), None, fscope),), None, fscope)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 441, in converted_call
    result = converted_f(*effective_args)
  File ""/tmp/__autograph_generated_fileok5ldq7m.py"", line 39, in tf__tensors_to_item
    ag__.if_stmt(ag__.ld(self)._repeated, if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1217, in if_stmt
    _py_if_stmt(cond, body, orelse)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/operators/control_flow.py"", line 1270, in _py_if_stmt
    return body() if cond else orelse()
  File ""/tmp/__autograph_generated_fileok5ldq7m.py"", line 35, in else_body
    retval_ = ag__.converted_call(ag__.ld(self)._decode, (ag__.ld(image_buffer), ag__.ld(image_format)), None, fscope)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 441, in converted_call
    result = converted_f(*effective_args)
  File ""/tmp/__autograph_generated_fileubbeey9m.py"", line 80, in tf___decode
    image = ag__.converted_call(ag__.ld(control_flow_ops).case, (ag__.ld(pred_fn_pairs),), dict(default=ag__.ld(check_jpeg), exclusive=True), fscope)
AttributeError: in user code:

    File ""/content/models/research/./object_detection/data_decoders/tf_example_decoder.py"", line 556, in decode  *
        tensors = decoder.decode(serialized_example, items=keys)
    File ""/usr/local/lib/python3.10/dist-packages/tf_slim/data/tfexample_decoder.py"", line 723, in decode  *
        outputs.append(handler.tensors_to_item(keys_to_tensors))
    File ""/usr/local/lib/python3.10/dist-packages/tf_slim/data/tfexample_decoder.py"", line 406, in tensors_to_item  *
        return self._decode(image_buffer, image_format)
    File ""/usr/local/lib/python3.10/dist-packages/tf_slim/data/tfexample_decoder.py"", line 454, in _decode  *
        image = control_flow_ops.case(

    AttributeError: module 'tensorflow.python.ops.control_flow_ops' has no attribute 'case'
```
",True,"[-0.46484244 -0.41313082 -0.27096552 -0.12019929  0.37383187 -0.05439372
 -0.11594192  0.21031184 -0.3138166  -0.20380178  0.2506416  -0.10114098
 -0.10992436  0.22258106  0.03573102  0.15194374 -0.01369787  0.02113153
  0.02516636  0.08219451 -0.18059021 -0.06556447 -0.18099928  0.257055
 -0.07674351  0.1220191  -0.31669715 -0.05020724 -0.07561839  0.38550952
  0.05735764  0.11206886 -0.03126168  0.00409678  0.05420684  0.27976447
 -0.05504229 -0.22301179 -0.3228114   0.03742075 -0.19979548 -0.22203714
  0.06234179 -0.22719336  0.23093857 -0.19267008 -0.11256021 -0.1446067
 -0.15965554 -0.19188836 -0.03437101  0.03896668 -0.4615388  -0.44120535
 -0.18887815  0.14239985  0.29966986  0.11643253 -0.12218538 -0.03019097
 -0.16746716  0.14968924  0.06599057 -0.17260152  0.16317332  0.28414994
  0.17590904 -0.0308288   0.5917934  -0.32629493  0.14252765 -0.11602368
 -0.34529623  0.09855435 -0.05501838  0.0111086   0.05440307  0.1885663
  0.4086774  -0.14611556 -0.05184536 -0.5074729  -0.2526425  -0.3013031
  0.16409847 -0.08174431  0.42413396  0.06851376  0.74048895 -0.15984245
  0.4824745   0.17821908 -0.03507761  0.20461825  0.4514327   0.21777664
 -0.06786925  0.20308974 -0.12960169  0.00447856 -0.24565735 -0.31142008
 -0.11330242  0.09322523 -0.06245741 -0.16533288  0.16928911 -0.19198744
  0.26294968 -0.04584784  0.11108766 -0.07244279  0.12034732  0.05541849
 -0.0330178   0.21727458  0.06147814  0.14158767 -0.02891746  0.43622068
  0.21688032 -0.3286652   0.14076763  0.01113139  0.41505098 -0.02915226
 -0.24185804  0.1246502  -0.00501008 -0.10197313  0.38321784  0.07111532
 -0.08249405  0.10180967 -0.169119    0.02102264 -0.0436081  -0.01093847
 -0.22761093 -0.09092781 -0.31445932  0.09849069 -0.01235839 -0.44963717
  0.04332865  0.19914344 -0.18170087  0.41372877 -0.05588019  0.13613662
  0.10949989  0.15137638 -0.09734245  0.5255328   0.09522259  0.14399903
  0.3860682   0.00486648 -0.0386347  -0.5527712   0.05203933  0.32169396
  0.07710652 -0.26643628  0.42123854  0.25905675 -0.2504325  -0.25961357
  0.2650026   0.41619128 -0.03550983 -0.10522416 -0.0620877  -0.02023954
  0.25259793 -0.23063546 -0.02657163 -0.53171635 -0.15847182  0.2318446
  0.07813513  0.00803009  0.08632186  0.2508433   0.01969314  0.11951713
  0.11887743 -0.09824119 -0.11884524  0.09458026 -0.2131742  -0.11972646
  0.3631066  -0.08955251 -0.30432612 -0.08853097  0.4959991  -0.05211669
  0.08375376  0.03984801 -0.21482769 -0.07303855  0.07532333 -0.17751658
  0.38720614 -0.19661854 -0.08730859 -0.5898411  -0.16411704  0.11223867
  0.05518101 -0.5226952   0.04240069 -0.09090348 -0.36186928  0.17687921
 -0.00908175 -0.11804523 -0.09355589  0.18158507 -0.00344068 -0.03493177
 -0.04392913 -0.46208858 -0.21095422  0.22077084 -0.25583544  0.00755113
  0.1128478  -0.03283698  0.2979951   0.14396472  0.4147364   0.27301466
  0.52607083 -0.24216515 -0.33375007 -0.10728144  0.03182285 -0.19319819
 -0.38449517 -0.2689046  -0.04644368 -0.00295033  0.41138947  0.3461042
 -0.2666011  -0.0564492  -0.2955602   0.33231676 -0.3841179   0.09140273
  0.30263013  0.04965249  0.43061322  0.03974691  0.1808977   0.22526424
  0.23188904 -0.21372414  0.43030024  0.19530998  0.2509858   0.32580894
  0.27101973  0.23763585 -0.46113452  0.64157534 -0.23650612 -0.2230306
  0.3397503  -0.5607096   0.6247611  -0.37690538  0.25875863 -0.16266653
  0.37935093 -0.07523763 -0.24734665  0.16750678  0.21040735  0.16709103
 -0.35510555  0.18941689  0.05169425  0.0314261  -0.30536574 -0.61625904
 -0.19179168 -0.17521724 -0.32993293  0.37974918 -0.04013216 -0.22716144
 -0.21658677  0.30079842  0.09110832  0.01528621  0.01529841 -0.0944307
 -0.45376787 -0.02259904  0.5583409  -0.43436515 -0.22656666 -0.25841773
  0.4261133   0.11766534  0.30807942 -0.31695437  0.14183539  0.15720046
 -0.06657395  0.35603786 -0.00468735  0.09520037 -0.24831283  0.8198453
 -0.07497682 -0.02825687  0.08439347 -0.09771404 -0.2400311   0.19135113
  0.3389269  -0.02036609 -0.17785701 -0.32111767  0.15146081  0.07045501
 -0.16179144 -0.07078086 -0.09373567 -0.15662879 -0.10791202 -0.02506585
 -0.46729255  0.28905767  0.03828413 -0.36441696 -0.05191166 -0.31336313
 -0.2286112  -0.26610786 -0.03503087 -0.49011227  0.10517835  0.7459614
 -0.01244906  0.24487907  0.10136683  0.17416161 -0.27971494  0.01529211
  0.01605465  0.57558554  0.06820396 -0.17404088  0.21564579  0.31837425
 -0.12786752  0.05876074 -0.39171886  0.10094006  0.05549253 -0.17555729
 -0.07231408 -0.6549914   0.15883927  0.2026443  -0.17415315  0.14015844
 -0.43518862  0.07914299  0.73995423 -0.28735042  0.0159048   0.07371697
  0.13703835  0.10148534  0.04518872 -0.29819235  0.09313624  0.04024168]"
Tensorflow 2.14 macOS wheel won't install for Python 3.11 type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14

### Custom code

No

### OS platform and distribution

arm64 macOS

### Mobile device

n/a

### Python version

3.11

### Bazel version

n/a

### GCC/compiler version

n/a

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

When installing tensorflow 2.14 for Python 3.11 I see:

```
ERROR: Could not find a version that satisfies the requirement wrapt<1.15,>=1.11.0 (from tensorflow) (from versions: 1.15.0rc1, 1.15.0)
```

Looking at the metadata of the [2.14 whl for py3.11](https://files.pythonhosted.org/packages/d3/4b/ae9037ea22ba94eb2cf267e991384c3444f3e6142fa49923352b4ab73e14/tensorflow_macos-2.14.0-cp311-cp311-macosx_12_0_arm64.whl) I can see: 

```
Requires-Dist: wrapt (<1.15,>=1.11.0)
```

but wrapt has no packages for Python 3.11 for that version range.

Looking at the metadata for the [2.13.1 whl for py3.11](https://files.pythonhosted.org/packages/c0/d1/d309dea6e67e1b8037f607872486eb67a1ff64fb91a96149086dbdc46ca4/tensorflow_macos-2.13.1-cp311-cp311-macosx_12_0_arm64.whl) I can see:

```
Requires-Dist: wrapt (>=1.11.0)
```

which can be satisfied with wrapt 1.15.0

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_",True,"[-3.60200495e-01 -6.27709329e-01 -1.11718729e-01 -2.90299319e-02
  6.95267990e-02 -4.79055196e-01 -1.69068217e-01 -1.41848758e-01
 -3.40620637e-01 -2.27293611e-01  1.34623259e-01  1.86934292e-01
 -2.87563831e-01  3.42431784e-01 -1.18173257e-01  3.41843426e-01
 -3.49383682e-01 -2.49935225e-01  1.03197083e-01  3.14073354e-01
 -2.22357567e-02 -2.98543215e-01 -1.12161823e-01  1.45275220e-02
  2.32907720e-02  3.39875430e-01 -2.36092024e-02 -3.61646749e-02
 -1.20985523e-01  2.13010222e-01  5.23173869e-01  2.52377689e-01
 -1.14235945e-01  5.90313822e-02  3.05268347e-01  2.95179069e-01
 -2.94040918e-01 -3.90431881e-01 -2.35304326e-01 -1.32003993e-01
 -1.14153214e-01  2.75276415e-02  3.35840553e-01 -1.45837963e-01
 -9.39236730e-02 -3.75786513e-01 -8.27553645e-02 -1.57864928e-01
  9.91218984e-02 -2.90681064e-01  1.19405910e-01 -1.56288058e-01
 -2.03935295e-01 -3.85841608e-01 -1.00160778e-01  1.13313906e-02
  5.30653521e-02  6.35775924e-02  2.19480842e-01  3.05977464e-01
  2.74412334e-01  1.29327644e-02 -2.24177577e-02 -7.34831244e-02
  1.39549881e-01  1.60930857e-01  8.52675438e-02 -2.76976287e-01
  3.50274265e-01 -2.89654374e-01  1.26314044e-01 -1.82453200e-01
 -3.27667415e-01 -2.43997313e-02 -4.95872125e-02  3.16546448e-02
  2.21086651e-01 -7.39901699e-03 -6.67391419e-02 -4.65214625e-02
 -1.53708696e-01 -3.38892117e-02  1.65804803e-01 -6.12062253e-02
  1.41533494e-01 -1.92841113e-01  1.25319406e-01  5.03376797e-02
  3.03038806e-01 -9.12528932e-02  3.38046670e-01  2.21071646e-01
  1.21577211e-01 -1.72126681e-01  4.86876726e-01  6.01729043e-02
  2.17471451e-01  1.08693130e-01 -8.67101103e-02 -1.42777830e-01
  1.54765040e-01 -9.93710607e-02  1.03601888e-01  3.57115194e-02
 -5.82170933e-02  5.21295033e-02  2.70730764e-01 -2.59696454e-01
  1.33322040e-02 -1.19644433e-01  5.57135269e-02  7.79100284e-02
  2.78611898e-01 -1.16301030e-01 -8.91519338e-02  6.20103348e-03
 -6.45060539e-01  6.82038814e-02  1.85490608e-01  9.21869874e-01
 -3.77266482e-02 -1.84914827e-01  1.13211945e-02  1.89113438e-01
  4.96262670e-01  5.33121936e-02 -1.46894321e-01 -3.93503904e-03
 -2.66543210e-01 -1.50981575e-01  3.07515189e-02  1.96168914e-01
  1.70564353e-01  3.11936438e-01  1.78757124e-02  2.69127190e-01
 -1.15901493e-01 -3.92949820e-01 -1.64257124e-01 -1.35003567e-01
 -3.54641438e-01  1.11441933e-01 -5.78612555e-04 -7.43022561e-01
  1.44140169e-01 -1.07988492e-01 -5.09872437e-01  4.29911166e-01
 -8.16237032e-02 -1.41654909e-01 -7.82001466e-02  1.03684932e-01
 -6.37159199e-02  4.26305890e-01  2.29240730e-01  3.16756248e-01
  6.41310692e-01  4.11802828e-02 -2.13567004e-01 -5.50883591e-01
 -6.33426905e-02  4.20825541e-01 -2.86887109e-01 -1.33673940e-02
 -1.64405227e-01 -8.66655037e-02 -4.47078675e-01 -2.24020496e-01
 -2.79773325e-01  2.33236820e-01 -1.98978111e-02 -1.65524483e-01
  6.99229166e-02  2.09089458e-01 -7.19957501e-02 -3.71429026e-02
  2.31282502e-01 -7.32912540e-01  1.29885793e-01  3.42915267e-01
 -8.48415308e-03  2.41632387e-01  1.37166157e-01  1.68643355e-01
  1.29268914e-01  1.89943761e-01  9.54815969e-02  1.92647964e-01
 -1.15332320e-01  1.70267195e-01 -1.70538738e-01 -1.29776418e-01
  3.01427156e-01 -1.04946584e-01 -2.50530187e-02  2.01571181e-01
  1.93775922e-01  3.22299689e-01 -2.60521352e-01  1.14848554e-01
 -1.97172105e-01  5.10151200e-02 -9.59082767e-02 -2.53880247e-02
  8.89849383e-03 -3.79507601e-01 -2.46521831e-01 -2.26135701e-01
 -5.57096124e-01  4.53968942e-02  9.93743166e-02 -4.36371088e-01
  1.61699489e-01 -3.24842930e-02 -4.16494370e-01  4.62158024e-02
  2.91416079e-01  2.06224978e-01 -1.49562716e-01  1.45562455e-01
 -8.67424682e-02 -3.08821619e-01 -1.67628407e-01 -2.40607426e-01
 -9.37772989e-02  9.40141827e-02 -2.55252093e-01 -2.04029083e-01
 -9.11630243e-02  9.88892168e-02 -1.52425468e-01  1.25404354e-03
  2.95586139e-01  7.99666196e-02  3.48525465e-01  3.84878442e-02
  7.94380531e-02 -1.51217222e-01 -6.64497986e-02  1.11446470e-01
 -2.13077366e-01 -3.06046735e-02  2.01651126e-01  3.59083936e-02
  3.34549248e-01  2.95210779e-01 -1.45300731e-01 -2.03203529e-01
 -4.78454024e-01  3.85245234e-02 -2.51367211e-01  2.29683846e-01
  3.50222766e-01  2.53451705e-01  4.72675443e-01  1.17717788e-01
  1.42925546e-01  1.85699940e-01  2.57118970e-01 -1.01592824e-01
  3.20827842e-01  5.66841811e-02 -7.59498626e-02  3.87553543e-01
  2.00317517e-01  2.17740849e-01 -3.43312502e-01  6.12605691e-01
  2.46217147e-01 -3.37014869e-02  1.77092671e-01 -1.81421891e-01
  8.34956765e-01 -2.55364716e-01  1.12671465e-01  9.18439627e-02
  2.58785456e-01 -1.41908482e-01  2.69871950e-02 -1.18400663e-01
  8.80869254e-02  1.62312806e-01 -3.68362337e-01 -2.44580153e-02
 -5.02498634e-02 -2.84995198e-01  1.41719937e-01 -7.57648468e-01
 -3.10445994e-01  1.64770465e-02 -4.14355427e-01  1.71015888e-01
 -3.24140415e-02  1.09231628e-01 -1.95366442e-01  1.81848079e-01
  1.04626141e-01  3.51943076e-04  2.51511008e-01  3.53723109e-01
 -7.55689219e-02 -2.86143199e-02  2.24000826e-01 -7.13820815e-01
  2.99124252e-02 -8.87466818e-02  3.24671537e-01  1.60746247e-01
  3.49196464e-01 -3.56963873e-01  2.78621256e-01 -2.42626995e-01
 -2.33728975e-01  4.29863453e-01  2.06898943e-01  1.94102347e-01
 -4.02360678e-01  7.31849909e-01  2.73277730e-01 -6.22486994e-02
 -2.68704742e-02 -1.38482153e-01 -5.06912112e-01 -6.88608736e-02
  2.57653832e-01 -2.61975646e-01 -6.13527447e-02 -3.99724364e-01
 -4.66406122e-02  1.39668792e-01 -1.78124383e-02  1.13000855e-01
 -4.76145744e-03  3.24900955e-01 -2.41094351e-01  2.78579473e-01
 -3.47379148e-01  1.83538616e-01 -1.42795056e-01 -2.33338624e-01
 -1.89308245e-02 -7.70361796e-02 -1.58778392e-03 -2.16246784e-01
 -2.55347881e-02 -2.61543602e-01  4.67117846e-01  6.30017042e-01
 -2.15421870e-01 -5.04509620e-02 -5.60048446e-02  2.41331100e-01
 -6.41624689e-01 -3.83226052e-02 -4.50121202e-02  4.90823328e-01
 -2.13358402e-02 -2.35300660e-01  6.00643337e-01  2.52655931e-02
 -2.35547304e-01  1.90766871e-01 -5.18566012e-01  1.92705184e-01
  3.47233824e-02 -1.11632094e-01 -3.05710852e-01 -2.97767706e-02
 -1.58005983e-01  3.31713974e-01 -3.23529810e-01  1.26011789e-01
 -1.53850019e-01  3.81187916e-01  7.68704057e-01 -2.45655373e-01
 -4.00611758e-01  1.71043843e-01  9.49800313e-02 -1.81698725e-01
  4.16477174e-02  5.04921302e-02  2.21959516e-01 -9.68118608e-02]"
Using Lambda layers to take different slices of a prevous layer's output causes earlier Lambda layers to be overwritten stat:awaiting tensorflower type:bug comp:apis TF 2.10,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.10.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I used two Lambda layers to extract slices from the same input vector. The output of the first Lambda is somehow overwritten by the output of the second Lambda.

Note: I have confirmed this happens whether the Lambda layers take the model's input directly or the output of another layer. I have also confirmed that the problem is present whether or not the Lambda layers are the direct outputs of the model. But for the sample code below I've removed the extra layers.

### Standalone code to reproduce the issue

```shell
import sys

import tensorflow as tf

print(f""{tf.version.VERSION=} {tf.version.GIT_VERSION=} {tf.version.COMPILER_VERSION=}"")
print(f""{sys.version=}"")

dividers = [0, 2, 5]

assert all(divider >= 0 for divider in dividers)
sizes = [end - start for start, end in zip(dividers[:-1], dividers[1:])]
assert all(size > 0 for size in sizes)
channels = dividers[-1]

i = tf.keras.layers.Input((channels,), name='i')
o = [
    tf.keras.layers.Lambda(lambda x: x[..., start:end],
                           name=f'slice_{start}_{end}')(i)
    for start, end in zip(dividers[:-1], dividers[1:])
]
m = tf.keras.Model(i, o, name='m')
m.build((channels,))
m.summary()
print(f""{m.input_shape=}"")
print(f""{m.output_shape=}"")
print(f""{m.compute_output_shape(m.input_shape)=}"")
x = tf.zeros((1, channels))
print(f""{[y.shape for y in m(x)]=}"")
print(f""{[y.shape for y in m.predict(x)]=}"")
assert m.output_shape == m.compute_output_shape(m.input_shape)
```


### Relevant log output

```shell
tf.version.VERSION='2.10.0' tf.version.GIT_VERSION='v2.10.0-rc3-6-g359c3cdfc5f' tf.version.COMPILER_VERSION='9.3.1 20200408'
sys.version='3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]'
Model: ""m""
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 i (InputLayer)                 [(None, 5)]          0           []                               
                                                                                                  
 slice_0_2 (Lambda)             (None, 2)            0           ['i[0][0]']                      
                                                                                                  
 slice_2_5 (Lambda)             (None, 3)            0           ['i[0][0]']                      
                                                                                                  
==================================================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
__________________________________________________________________________________________________
m.input_shape=(None, 5)
m.output_shape=[(None, 2), (None, 3)]
m.compute_output_shape(m.input_shape)=[TensorShape([None, 3]), TensorShape([None, 3])]
[y.shape for y in m(x)]=[TensorShape([1, 3]), TensorShape([1, 3])]
1/1 [==============================] - 0s 286ms/step
[y.shape for y in m.predict(x)]=[(1, 3), (1, 3)]
Traceback (most recent call last):
  File ""/home/hosford42/PycharmProjects/LSLAM/tf_bug.py"", line 30, in <module>
    assert m.output_shape == m.compute_output_shape(m.input_shape)
AssertionError
```
",True,"[-4.00832474e-01 -4.45568979e-01 -6.26538768e-02  6.50496259e-02
  9.32328999e-02 -3.50764424e-01 -3.04687828e-01  2.00612973e-02
 -3.64316285e-01 -5.56062609e-02  1.01191260e-01 -8.58694166e-02
 -3.46161425e-02  2.81854346e-02 -4.83005047e-02  5.71795166e-01
 -2.89415687e-01 -1.12042919e-01  7.66032040e-02  4.73564602e-02
 -4.24331278e-02 -6.52444512e-02 -3.14039260e-01  1.83236688e-01
  3.90399635e-01 -8.04135203e-02 -4.29873675e-01 -3.20527971e-01
  1.51739091e-01 -5.05109206e-02  2.13927627e-01 -1.64348394e-01
 -1.16437361e-01  1.11744165e-01  3.16087306e-02  3.25253367e-01
 -2.01120794e-01 -1.68341070e-01 -1.33946881e-01  1.35329947e-01
  1.06851645e-02 -4.38270345e-03  4.69504148e-02 -1.66641146e-01
  1.07575424e-01 -2.00094655e-02  6.67658895e-02  4.57333587e-03
 -2.89729387e-02 -2.02002525e-02  1.83930285e-02  4.40063551e-02
 -3.76054376e-01 -3.71087343e-01 -2.92577241e-02 -6.21841103e-03
  4.71962206e-02  1.02750342e-02 -1.43121378e-02  1.00017339e-01
  3.55868936e-02  4.84864786e-02  5.86476251e-02  5.52347489e-02
  9.77010056e-02  1.25015378e-01  4.44819689e-01  8.74346718e-02
  5.01486659e-01 -9.80878249e-02  2.60391712e-01 -1.81399956e-02
 -5.77220142e-01 -7.70227164e-02  1.82288200e-01  1.58534825e-01
 -1.31066069e-01  1.82819054e-01  1.89991176e-01 -2.71948010e-01
  1.39202118e-01  6.00171015e-02  1.91088229e-01 -1.48409814e-01
  2.28917912e-01 -1.30159169e-01  4.13147032e-01 -6.46904409e-02
  2.53484249e-01 -1.29949659e-01  6.79422379e-01 -1.26700103e-02
 -1.70220003e-01  2.22099721e-01  4.01421666e-01 -2.34356895e-03
  8.41198564e-02  3.01346004e-01  9.84858051e-02 -8.16798210e-02
 -2.25661881e-02 -3.42624694e-01 -3.24971914e-01  7.64931142e-02
  8.42607990e-02 -7.79647902e-02  2.18445137e-01 -1.10107690e-01
 -7.26094842e-02 -1.18413530e-01  3.88912141e-01 -1.78262383e-01
 -1.48868840e-02 -3.02222252e-01  2.41220016e-02 -2.72709966e-01
  2.10656822e-02  7.35233873e-02 -1.99830849e-02  6.54564977e-01
  1.41678661e-01  7.71112144e-02 -5.33889234e-03  4.73893762e-01
  4.10995662e-01  1.09369978e-01  1.37247205e-01 -2.10426189e-02
  2.29899481e-01 -3.18124332e-02  3.62173975e-01  1.63819641e-01
  3.71368416e-02  2.14993358e-01 -1.52813364e-02  6.43569753e-02
 -2.21700251e-01 -3.01984131e-01 -2.02474177e-01 -2.27776796e-01
 -3.19330722e-01  3.15274447e-01 -2.40637749e-01 -4.84148324e-01
  2.07733870e-01  9.13840085e-02 -2.21157819e-02  2.06210628e-01
  9.51328129e-03 -1.27784252e-01  8.17691684e-02 -3.14101502e-02
 -3.02886933e-01  3.45628321e-01  2.02170014e-01  2.60290623e-01
  7.14847744e-02 -3.49491835e-02  1.25483409e-01 -7.25378513e-01
  2.85714507e-01  5.09464383e-01 -1.81681529e-01  8.14814568e-02
  3.99824977e-01  1.96415484e-01 -2.26008922e-01 -2.61250794e-01
 -8.07369053e-02  5.59506416e-01 -1.72542870e-01 -3.62491496e-02
  1.58232395e-02  5.66430539e-02  3.48866880e-01 -1.06546029e-01
 -5.66495508e-02 -6.49391413e-01  1.83418933e-02  3.27228725e-01
  3.03441267e-02  2.97231436e-01  6.81104362e-02  3.07985336e-01
 -6.04253588e-03 -1.17651880e-01  2.11567301e-02 -7.43931085e-02
 -1.48200899e-01  1.21058837e-01 -5.14104545e-01 -1.25893772e-01
  1.71394587e-01  7.81743042e-03  3.61158103e-02  6.65740892e-02
  2.96727300e-01 -3.04207087e-01 -1.27599999e-01  5.53722940e-02
 -7.53673762e-02 -3.08053553e-01 -2.07124114e-01  2.02603161e-01
 -1.87133998e-01 -3.77618223e-01 -4.40795422e-02 -3.41502845e-01
 -2.75577664e-01 -6.30018562e-02 -1.66810215e-01 -6.24285400e-01
  1.39166445e-01 -1.75746411e-01 -3.02564740e-01  2.22277999e-01
  8.73863995e-02  1.40474215e-02 -1.30818859e-01  2.11747333e-01
  3.02524745e-01 -9.97569412e-02 -2.31998526e-02 -3.38866889e-01
 -3.92273515e-02 -1.36664361e-01 -1.75811201e-01  8.64101052e-02
  2.31813882e-02  1.98104680e-01  7.63425678e-02  1.14779204e-01
  1.21199176e-01  3.84438455e-01  2.90618479e-01 -7.58153945e-02
 -6.48184866e-02 -2.98672207e-02 -2.99712401e-02  8.19128528e-02
 -5.30666769e-01  1.63162738e-01 -1.37026105e-02  6.25829101e-02
 -9.88472253e-03  4.02099252e-01 -5.16732633e-02 -2.87069492e-02
 -2.66643107e-01  4.39887792e-01 -4.53768998e-01 -9.82614905e-02
  5.17161667e-01 -1.20887809e-01  1.48268566e-01  9.49212462e-02
  9.05080959e-02 -8.60805362e-02  1.91702664e-01  6.59949407e-02
  2.16748327e-01  2.01935947e-01  2.15474159e-01  2.67702878e-01
  1.49444312e-01  4.06351388e-01 -1.40380740e-01  3.81635845e-01
  1.21217355e-01 -7.34792277e-03 -7.98867643e-03 -2.72630036e-01
  3.17497700e-01 -3.25315982e-01 -6.81986287e-02 -7.73620158e-02
  2.78592199e-01 -2.99849361e-02 -3.91096324e-02 -1.11412823e-01
 -2.34528527e-01  4.20588642e-01 -2.86492020e-01  2.27796003e-01
 -1.56801760e-01 -3.03874135e-01 -3.22161615e-02 -5.20312309e-01
 -9.41427276e-02 -6.34215698e-02 -2.07889006e-01  2.96227001e-02
  1.84415966e-01 -1.27937138e-01 -2.25940213e-01  2.81101108e-01
 -1.21077612e-01 -3.16245496e-01  3.47521380e-02  1.49065509e-01
 -1.73712030e-01  7.82929510e-02  3.42689782e-01 -1.77514255e-01
 -3.93844008e-01 -1.23515896e-01  4.56176400e-01  2.76161551e-01
  4.51724529e-01 -3.82373035e-01  4.06015992e-01 -1.08332209e-01
 -2.37409189e-01  5.60021639e-01 -1.23300403e-01 -1.92887485e-02
 -2.22260952e-01  6.15543902e-01  3.33398491e-01 -1.17089063e-01
  1.50052071e-01 -1.70286760e-01 -3.06407481e-01 -4.59969752e-02
  1.67681932e-01 -1.24790385e-01 -1.21682264e-01 -7.37446547e-02
 -2.29858756e-02  1.93381086e-01 -1.57990471e-01  9.62503850e-02
 -9.53270644e-02 -4.56445776e-02 -9.75353047e-02 -1.44122705e-01
 -3.45025599e-01  9.10435766e-02 -2.56281178e-02 -4.62413967e-01
 -2.35862136e-01  1.45497069e-01 -1.03248492e-01 -2.53064811e-01
 -4.79343459e-02 -3.20425153e-01  3.65914762e-01  2.28836283e-01
 -2.83112884e-01  5.08101918e-02 -8.79744142e-02  2.00009137e-01
 -5.69357216e-01 -4.13330197e-02 -6.70876130e-02  3.38976830e-01
  3.84579264e-02 -7.40322620e-02  2.47332156e-01  3.36141288e-01
 -4.31410074e-01  9.75510478e-02 -1.55118495e-01  2.67929677e-02
  4.25463080e-01 -2.52713144e-01 -6.26897346e-03 -1.28989056e-01
 -5.91984689e-02  4.20968533e-01 -1.09797388e-01  1.18965730e-01
 -3.56586486e-01  3.53283763e-01  3.65426540e-01 -3.91815871e-01
 -2.77648658e-01  2.42013291e-01  9.32178795e-02 -2.03902833e-04
 -1.14843592e-01 -1.96679980e-01  8.28554705e-02 -6.63544908e-02]"
ValueError in tensorflow-probability type:bug TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow 2.15.0

### Custom code

Yes

### OS platform and distribution

Windows Subsystem for Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

v1.18.0

### GCC/compiler version

11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to run a program which uses tensorflow agents & tensorflow probability at the back end. When I try to run the train.py using .yaml input file, I am getting the following error:

### Standalone code to reproduce the issue

```shell
lib/python3.10/site-packages/tensorflow_probability/python/internal/prefer_static.py"", line 84, in _copy_docstring raise ValueError(

ValueError: Arg specs do not match: original=FullArgSpec(args=['input', 'dtype', 'name', 'layout'], varargs=None, varkw=None, defaults=(None, None, None), kwonlyargs=[], kwonlydefaults=None, annotations={}), new=FullArgSpec(args=['input', 'dtype', 'name'], varargs=None, varkw=None, defaults=(None, None), kwonlyargs=[], kwonlydefaults=None, annotations={}), fn=<function ones_like_v2 at 0x7f8287141480> Please help me understand the issue & any suggestions to resolve the error is greatly appreciated.
```


### Relevant log output

_No response_",True,"[-3.12506288e-01 -5.71247935e-01 -3.07016224e-02 -1.23336136e-01
  1.13219380e-01 -4.12716299e-01 -2.09472582e-01 -3.99680585e-02
 -1.52741700e-01 -4.43500429e-01 -1.22442385e-02 -6.36907071e-02
  6.14850596e-03  1.22050367e-01 -9.15944278e-02  4.53982741e-01
 -1.04492545e-01 -1.72478229e-01  8.25501308e-02  3.61003950e-02
 -2.00985640e-01 -2.06421226e-01 -2.71851808e-01  9.39689502e-02
  8.96745250e-02  6.41177148e-02 -5.17964289e-02 -1.24732599e-01
 -6.10747486e-02  2.24632055e-01  4.24362481e-01  1.30315229e-01
 -1.03910625e-01  7.95481801e-02  2.36277170e-02  1.82213932e-01
 -4.70983088e-01 -1.28334552e-01 -2.49724984e-01  8.59789550e-02
 -3.40226032e-02 -8.04421604e-02  1.48626417e-01 -1.43070668e-01
 -1.16515011e-01 -3.10896754e-01 -6.71974570e-02 -1.14278480e-01
 -6.12943955e-02 -3.98052856e-02  9.87585708e-02  8.75654370e-02
 -4.99417424e-01 -2.96428144e-01 -2.75716633e-01 -3.49857002e-01
  1.17644623e-01 -2.78590083e-01  3.23139355e-02  2.58066773e-01
  7.02580512e-02  1.04149766e-01  8.88523832e-02 -8.27609599e-02
 -1.05415508e-02  1.10104263e-01  2.93951273e-01  6.44736886e-02
  5.55521488e-01 -2.79150635e-01  1.97890043e-01 -9.74108130e-02
 -2.80055791e-01  6.39894828e-02  4.03838679e-02  3.80735807e-02
  2.77998615e-02  1.26456380e-01  2.78005600e-01 -2.68001795e-01
  3.77676301e-02 -1.86539203e-01  1.37210963e-02 -2.85358667e-01
  2.55340375e-02 -7.96166509e-02  4.71447647e-01  2.25822717e-01
  5.66401780e-01 -1.84669375e-01  3.97076070e-01  1.32868826e-01
 -3.70675698e-03  8.83165300e-02  2.83232570e-01  2.01752067e-01
  7.31292218e-02 -1.41530111e-02  1.26191154e-01 -1.88575655e-01
 -1.69705495e-01 -2.21788079e-01 -2.78144449e-01  1.45362973e-01
  1.17852442e-01 -1.97999582e-01  2.01610088e-01 -1.50582761e-01
  4.84810919e-02 -1.67449743e-01  6.69190288e-03 -9.35879722e-02
  4.75254714e-01 -2.22692475e-01 -5.95442057e-02 -7.65761174e-03
  1.17306553e-01  9.01695862e-02  6.50967807e-02  6.06257379e-01
  1.81522086e-01 -5.30615356e-03  1.45312026e-01  4.40744534e-02
  4.54498291e-01  4.19916883e-02 -2.26769447e-02  5.28208688e-02
  6.07864484e-02 -2.87645638e-01  1.50717109e-01  8.01652595e-02
  2.03860611e-01  2.71482825e-01  8.43337625e-02  2.48338327e-01
 -1.53587729e-01 -1.64391473e-01 -2.18554944e-01 -4.33316201e-01
 -3.12989712e-01  6.17566667e-02 -7.24222139e-02 -5.74224532e-01
  9.82050784e-03 -3.31406258e-02 -3.18642437e-01  4.45972979e-01
 -1.79431647e-01 -1.33982271e-01 -7.53356814e-02  2.78061807e-01
 -1.22859411e-01  2.84193337e-01  1.63179770e-01  2.79255033e-01
  4.79511499e-01 -1.01477802e-01  7.79555142e-02 -4.26553607e-01
 -1.85473301e-02  3.95721138e-01 -1.27360031e-01 -6.58901036e-02
  2.37196386e-01  2.63195723e-01 -5.02636850e-01 -7.40512013e-02
  3.67122665e-02  3.61577153e-01 -1.67165235e-01 -2.53751695e-01
 -1.41236037e-02 -1.83683820e-04  1.02045976e-01 -2.88487077e-02
  1.12362012e-01 -7.17609882e-01 -8.84426236e-02  4.09455717e-01
  8.14567134e-02  1.22632749e-01  2.13457793e-01  1.43235207e-01
  7.68329948e-02  2.64877137e-02  2.94616163e-01  3.56006026e-01
 -1.85540617e-01 -8.86798054e-02 -2.43058026e-01 -1.90277666e-01
  3.29114914e-01 -2.41940558e-01 -6.60552382e-02  1.35234203e-02
  1.47537798e-01 -1.55224800e-01 -4.75930944e-02  1.60377219e-01
 -1.28036201e-01 -2.49345288e-01 -1.36782005e-01  1.28329799e-01
  1.74499765e-01 -2.38597453e-01  4.67875600e-02 -4.82634515e-01
 -4.06211734e-01  1.41291797e-01  1.46453023e-01 -5.30482650e-01
  9.04125050e-02  6.32527331e-03 -3.45217109e-01  1.69140384e-01
 -7.05582555e-03  1.23651534e-01 -3.27992558e-01  1.57152489e-01
  9.41184312e-02 -1.70966357e-01  1.53133506e-02 -3.59318256e-01
 -3.56656879e-01  2.44979382e-01 -6.34997606e-01  5.04445992e-02
  1.10052571e-01  2.90721893e-01  2.94311624e-02  1.69689789e-01
  1.62658244e-01  2.50927508e-01  3.32648456e-01 -6.47719949e-02
 -1.43528894e-01 -1.24461025e-01 -1.67740822e-01  4.80020605e-02
 -2.16796547e-01 -1.48015991e-02 -1.13302410e-01 -7.57979453e-02
  2.82701313e-01  5.33450902e-01 -1.71171203e-01 -1.27848834e-01
 -3.44938606e-01  5.46533391e-02 -1.79727674e-01  2.49699652e-01
  5.19579113e-01  1.28583714e-01  6.95438504e-01  1.83456585e-01
  3.18039060e-01  3.13736707e-01  2.32245475e-01 -1.65595248e-01
  6.61454260e-01  3.88153791e-02  1.01507246e-01  3.62402886e-01
  1.70115441e-01  1.60995573e-01 -4.11587328e-01  3.78753930e-01
  1.54980242e-01 -1.60735473e-01  5.85748330e-02 -2.94381201e-01
  6.97419047e-01 -3.83190662e-01 -4.77940515e-02 -1.92502320e-01
  2.55438358e-01  3.42281945e-02 -1.50526494e-01  8.40006322e-02
 -5.59600703e-02  3.51993680e-01 -1.43999293e-01  3.67053412e-03
 -5.15214950e-02 -1.70866787e-01  3.08246017e-02 -4.26707983e-01
 -3.15753073e-01  1.87247302e-02 -3.33359361e-01  1.94715410e-01
 -5.20109851e-03  5.69147021e-02 -2.59377778e-01  1.08317286e-01
  3.69290709e-02 -5.21941148e-02  1.98182553e-01  1.71364874e-01
  1.20907918e-01  2.60844864e-02  3.74371886e-01 -4.45555627e-01
 -2.56126881e-01 -9.95894521e-03  4.18135226e-01  2.52200335e-01
  2.92714298e-01 -6.45421326e-01  1.67286128e-01 -2.41080150e-01
 -1.27258271e-01  5.00719666e-01 -1.78747043e-01  1.07725896e-01
 -4.09419835e-01  5.87210059e-01  2.53467083e-01 -5.59174344e-02
  1.21003218e-01 -2.16353759e-01 -3.08128953e-01  5.89066036e-02
  1.79076344e-01  4.42859530e-02  8.48804638e-02 -4.21861708e-01
  1.57098562e-01  1.81571037e-01 -8.50008875e-02 -3.55462097e-02
 -1.74655542e-01  9.31872129e-02 -1.03213497e-01  9.63023752e-02
 -2.99084038e-01  7.51020163e-02 -5.31572104e-02 -1.29750282e-01
 -5.00297248e-02  3.06336116e-02  8.82124677e-02 -1.25035971e-01
  2.28100792e-02 -1.73483938e-01  9.67459306e-02  4.72458690e-01
 -9.03131515e-02  2.97811050e-02 -1.53320879e-01  1.40803695e-01
 -3.73141825e-01 -2.00099088e-02 -2.89744567e-02  5.01627386e-01
 -3.82356867e-02 -2.00975344e-01  3.93066943e-01  2.14946464e-01
 -3.25988941e-02  4.40819636e-02 -3.22242707e-01  1.11313596e-01
  2.53320009e-01 -2.31772631e-01 -3.32599491e-01 -4.32932496e-01
  1.80481791e-01  2.84410715e-01 -1.94860429e-01  1.56215042e-01
 -1.84453696e-01  2.25126117e-01  5.93267441e-01 -3.46772462e-01
 -3.09307188e-01  1.11255966e-01  1.34637699e-01 -2.45867565e-01
 -4.01747338e-02 -7.94830918e-03  1.83849245e-01  2.62406282e-03]"
avx512fp16 causing invalid static_cast with cuda12.2 python 3.12 stat:awaiting tensorflower type:bug type:build/install subtype: ubuntu/linux TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux SuSE 15 SP4

### Mobile device

_No response_

### Python version

3.12.0

### Bazel version

3.6.0

### GCC/compiler version

gcc 12.3.0

### CUDA/cuDNN version

cuda 12.2.2 cudnn 8.9.5

### GPU model and memory

H100

### Current behavior?

I am aware this is an unsupported, non-working configuration - reporting early in case it's a real bug requiring fixing.
CPU is Sapphire Rapids; Compiling with --copt=-mavx512fp16
results in the following build error:

tensorflow/core/kernels/linalg/matrix_inverse_op.cc:113:31:   required from here
external/eigen_archive/Eigen/src/Core/MathFunctions.h:429:12: error: invalid static_cast from type const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1> to type __vector(16) float
  429 |     return static_cast<NewType>(x);
  
 Removing --copt=-mavx512fp16 allows the compile to finish. The following options resulted in a successfully build of a tensorflow whl file (although unusable until python 3.12 compatibility is available).
 --config=opt -c opt --copt=-mfpmath=sse --copt=-msse4.2 --copt=-mavx --copt=-mavx2
--copt=-mfma --copt=-mavx512f --copt=-mavx512vnni --copt=-mavx512f --copt=-mavx512bf16 --copt=-mavx512vl  


### Standalone code to reproduce the issue

```shell
TF_PYTHON_VERSION=3.11 CFLAGS=""-O3 -march=native -fPIC"" CXXFLAGS=$CFLAGS LIBRARY_PATH=$LD_RUN_PATH LD_LIBRARY_PATH=$LD_RUN_PATH \
LDFLAGS=""-fPIC  -Wl,--disable-new-dtags -Wl,--rpath -Wl,${LD_RUN_PATH}"" bazel build -j 24 --config=opt -c opt --copt=-mavx --copt=-mavx2 \
--copt=-mfma --copt=-mfpmath=sse --copt=-msse4.2 \
--copt=-mavx512f --copt=-mavx512vnni --copt=-mavx512f --copt=-mavx512fp16 --copt=-mavx512bf16 --copt=-mavx512vl \
--config=cuda --config=mkl --config=tensorrt  //tensorflow/tools/pip_package:build_pip_package --repo_env=TF_PYTHON_VERSION=3.11
```


### Relevant log output

```shell
external/eigen_archive/Eigen/src/Core/Matrix.h:227:24:   required from Eigen::Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>& Eigen::Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>::operator=(const Eigen::DenseBase<OtherDerived>&) [with OtherDerived = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Scalar_ = float; int Rows_ = -1; int Cols_ = -1; int Options_ = 1; int MaxRows_ = -1; int MaxCols_ = -1]
external/eigen_archive/Eigen/src/LU/PartialPivLU.h:135:12:   required from Eigen::PartialPivLU<MatrixType, PermutationIndex>& Eigen::PartialPivLU<MatrixType, PermutationIndex>::compute(const Eigen::EigenBase<OtherDerived>&) [with InputType = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; MatrixType_ = Eigen::Matrix<float, -1, -1, 1, -1, -1>; PermutationIndex_ = int]
tensorflow/core/kernels/linalg/matrix_inverse_op.cc:113:31:   required from here
external/eigen_archive/Eigen/src/Core/MathFunctions.h:429:12: error: invalid static_cast from type const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1> to type __vector(16) float
  429 |     return static_cast<NewType>(x);
      |            ^~~~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```
",True,"[-0.43570888 -0.65491045 -0.23452269  0.2064426   0.08348657 -0.5650972
 -0.13369587  0.07907289 -0.41228226 -0.33139014  0.0592413   0.02910794
 -0.31325966 -0.09011413 -0.35478252  0.22227487 -0.14036785 -0.11205334
  0.36825985  0.13526481 -0.19995672 -0.02273315 -0.42835248  0.03212597
  0.20841241  0.18178296 -0.30815423 -0.04446482  0.14152896  0.00676982
  0.42393446  0.07784462 -0.0491613   0.06496468  0.03795585  0.20217445
 -0.38745642 -0.43282488 -0.18490396  0.0324824  -0.05381838  0.00274896
  0.09929228 -0.3070614  -0.04232854 -0.17775373  0.04653733 -0.04663868
  0.03643586 -0.41391957  0.05528878 -0.03140715 -0.21578322 -0.38848394
 -0.03177034 -0.0901524  -0.00351794  0.03046361  0.02823184  0.04037668
  0.243946    0.04404315  0.08998469 -0.05691861  0.09354936  0.18524542
  0.21123007 -0.10293424  0.630325   -0.20447314  0.16783944  0.11941992
 -0.32670742  0.11363418 -0.00555629  0.29003853  0.3798593   0.1202274
  0.27712187 -0.31332147 -0.06021627 -0.05103682  0.03192624 -0.26630062
  0.2776553  -0.12249472  0.377823    0.18701914  0.2849686  -0.17273304
  0.5759981   0.56402695  0.01404884  0.13801737  0.24588987  0.05747315
  0.17425546  0.21512075 -0.20212086 -0.19725221 -0.15222797 -0.17525947
  0.05234347  0.34545702 -0.19094336 -0.26887214  0.3233794  -0.25583884
 -0.02364947 -0.22899935  0.17019051 -0.10187561  0.2057916   0.01158705
 -0.10515945 -0.07954098 -0.37655574 -0.08481285  0.01403878  0.53242844
 -0.18179604 -0.18541954 -0.11881908  0.18704003  0.3720322   0.17833385
 -0.04552451 -0.15463431  0.02243966  0.2386275   0.0793874  -0.06034465
  0.13381997  0.18131728 -0.08224081  0.33222857 -0.34948808 -0.18391155
 -0.13659844 -0.33955705  0.00990589  0.20083958 -0.08290432 -0.5597029
  0.2648177   0.10359091 -0.08660197  0.20658377 -0.23095658  0.27126217
 -0.0334807   0.05186495 -0.49491993  0.42174187  0.01307765  0.07908728
  0.3540733  -0.00791225 -0.1101042  -0.54424834  0.06867655  0.50763655
 -0.0967979  -0.23690304  0.15357488  0.09323037 -0.40122753 -0.25645852
  0.1405312   0.517582   -0.22256827 -0.04670399  0.32097775  0.12887388
  0.21170156 -0.10911036  0.11826529 -0.37277007  0.05798202  0.53440356
  0.17361537  0.15570262  0.31907967  0.47148472  0.2787981   0.17312306
  0.2525475   0.05485674 -0.29920697  0.04393646 -0.35596454 -0.06380729
  0.22341141 -0.05470273 -0.13452512  0.18509285  0.12663227  0.06299536
  0.02040625 -0.04256807 -0.16104454 -0.06375822 -0.046025    0.18428181
  0.00828046 -0.38623846 -0.09844929 -0.3236148  -0.36919564  0.10161845
 -0.03729381 -0.43996018  0.03728928 -0.1283412  -0.30752248  0.20446274
  0.30779442  0.16073719 -0.10843478  0.27672943  0.17125331 -0.28887963
  0.16483405 -0.27521366 -0.00134932 -0.06148363 -0.32413775 -0.16573238
 -0.04968269  0.09917071 -0.06885649 -0.11813396  0.32185912  0.3439222
  0.373142   -0.16297235 -0.15341985 -0.22288519 -0.16740349  0.09227055
 -0.5380372  -0.00855701 -0.11400428 -0.02474076  0.30580235  0.4871945
  0.14954999  0.08624792 -0.50536966  0.14748664 -0.05019287  0.22370929
  0.08866966 -0.08645506  0.34313777  0.11530679  0.1465089   0.2060388
  0.06037781 -0.16452254  0.43100104  0.14199671 -0.10219818  0.23650998
  0.06897769  0.39958954 -0.35832095  0.55542636  0.09219165 -0.1521369
  0.13674612 -0.11541963  0.69735813 -0.31013107 -0.08144763 -0.28517008
  0.33340824 -0.01858247  0.02073004 -0.3244128   0.20655905  0.15340696
 -0.17729774  0.1490646   0.17955714 -0.28264153 -0.22477421 -0.5573512
 -0.30836034 -0.07073432 -0.37463993  0.10461829 -0.00416918  0.04942658
 -0.33602828  0.01489832  0.06679513  0.00538998  0.04935309  0.18947932
 -0.30443764 -0.1022395   0.28889975 -0.45447117 -0.23778415 -0.25760198
  0.18087775  0.21275896  0.35072666 -0.34295356  0.26855168 -0.06230962
 -0.24271128  0.4454909   0.06050751  0.02171708 -0.27178812  0.6069257
  0.3194201  -0.16598326  0.0208391  -0.31352857 -0.41399825  0.00962585
  0.32330695 -0.1847488  -0.23408218 -0.2294146   0.03691217  0.26953098
 -0.19304489 -0.077727    0.03711645  0.12352408 -0.16603297 -0.04648277
 -0.20114498  0.02870527 -0.06450582 -0.22826214 -0.08385815 -0.02934553
 -0.08582906 -0.1916658  -0.06931422 -0.35271117  0.7138552   0.34672883
 -0.27547523  0.0497793  -0.09798793  0.23419939 -0.3303722   0.10871428
  0.02549868  0.2848509   0.14447027  0.09231216  0.2953826   0.21390413
 -0.1301353   0.2627463  -0.39163902 -0.03800224  0.20986171 -0.07962505
 -0.2782546   0.06439967  0.27957633  0.28081995 -0.09205142  0.12108132
 -0.18054613  0.28642875  0.6083644  -0.34969068 -0.31575632  0.20214874
  0.1521939   0.0439798   0.1462809  -0.12673317  0.31970316 -0.05927831]"
Illegal memory access when running tf.sparse_to_dense stat:awaiting response type:bug type:support stale TF 2.11,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.11

### Custom code

No

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

[This](https://github.com/tensorflow/tensorflow/issues/59126) issue seems to suggest that it was fixed in tf-nightly release. But I am somehow not able to find the commit where this was fixed. Can you please point me to the commit so that I can verify it has been fixed on my side

### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

_No response_",True,"[-3.05141628e-01 -1.49365485e-01 -5.51931739e-01 -1.50012463e-01
  2.78789759e-01 -3.50063354e-01 -5.13425991e-02  2.19357133e-01
 -2.43300200e-01 -1.62845552e-01  1.82390153e-01 -1.68440282e-01
 -2.41815194e-01  3.92220281e-02 -2.05560729e-01  1.28633440e-01
 -1.76524185e-03  1.26557022e-01  1.54332191e-01  2.71487474e-01
 -1.94840044e-01  2.30066925e-02 -2.04423606e-01  2.95586318e-01
  4.62835170e-02  1.66559994e-01 -2.14187056e-01 -1.30218655e-01
 -1.64550290e-01  7.45374858e-02  2.65860349e-01  1.10450909e-01
 -1.51286339e-02  7.52862450e-03  1.36355728e-01  1.04250751e-01
 -2.57704288e-01 -2.49191031e-01 -3.16338956e-01 -3.91040929e-03
 -1.42304480e-01  2.96479724e-02 -2.51640081e-02 -1.71450805e-03
  6.99102804e-02  1.22959889e-01 -4.70842719e-02  1.69770151e-01
 -3.79458964e-02 -3.72666478e-01  2.24653631e-02  2.22060025e-01
 -3.99027705e-01 -3.33057940e-01 -1.03609793e-01 -2.02674821e-01
 -9.03829336e-02 -9.71322358e-02 -1.53404772e-01  4.13081020e-01
  2.01606125e-01  2.76539996e-02  1.96915731e-01 -1.21422872e-01
  4.53488454e-02  1.81741491e-01  3.81067246e-01  3.97908539e-02
  4.97225732e-01 -3.16414177e-01  1.00348920e-01  6.06547222e-02
 -6.70084357e-01  1.97650522e-01  2.86057472e-01 -1.46711934e-02
 -5.77714629e-02  7.72314146e-02  2.30792284e-01 -3.98285389e-01
 -8.03588033e-02 -3.38882059e-01 -2.00270981e-01 -5.35331070e-01
 -2.92629749e-03 -1.75146878e-01  3.78998607e-01  3.20132375e-01
  3.40810657e-01 -2.09739387e-01  3.89931798e-01  2.36880392e-01
 -1.67833552e-01  5.61624654e-02  3.62192333e-01  7.12751597e-03
  2.12895013e-02  3.28079462e-01 -1.25038236e-01  7.09252432e-03
 -5.15311994e-02 -6.89882487e-02 -8.21589082e-02  2.91351020e-01
  2.62748897e-02 -1.52227551e-01  2.93866515e-01  2.00909048e-01
  3.30879763e-02 -8.04170966e-02  2.14015275e-01  2.55172491e-01
  2.59735793e-01 -5.25214970e-02 -7.82114863e-02 -1.97437890e-02
 -3.90080065e-02 -9.64792743e-02  5.40101305e-02  5.75184047e-01
  1.06277041e-01 -1.04195304e-01 -1.15162462e-01  1.25282764e-01
  3.88546288e-01 -1.46335661e-02 -3.45237553e-01  1.01063319e-01
  1.04774266e-01 -7.27293864e-02  1.21262960e-01 -1.34505242e-01
  4.31145132e-02  7.71119222e-02 -2.67202139e-01  1.27416681e-02
 -1.75052911e-01 -3.12386602e-02 -2.34608635e-01 -4.56224948e-01
 -1.47368520e-01  1.68518364e-01  7.92603791e-02 -6.79064393e-01
 -9.55638364e-02  2.55076110e-01  7.60289729e-02  2.80272722e-01
 -1.64756566e-01  3.73849869e-01  1.41059577e-01 -3.22949812e-02
 -1.82631642e-01  3.14733386e-01 -1.28657326e-01 -1.35423481e-01
  4.32739258e-01  8.21359269e-03  7.07314014e-02 -5.87878466e-01
  7.38684088e-02  2.83079147e-01  1.92593530e-01  1.56737436e-02
  3.29282105e-01  7.90917128e-02 -5.75413704e-01 -2.73423433e-01
  3.98076698e-02  4.79036927e-01 -9.52617824e-02 -5.25784791e-02
 -1.38676345e-01  7.43436590e-02  3.81375253e-01 -1.67276993e-01
  2.16326535e-01 -5.31448185e-01 -8.96267742e-02  8.42257887e-02
 -5.15093729e-02 -1.60265535e-01  6.04499429e-02  1.39830709e-01
  6.58256710e-02 -1.93691403e-02  9.29897726e-02  3.22326243e-01
 -1.36319965e-01  3.94948423e-02 -6.45356417e-01  5.19093871e-03
  1.77347064e-01 -3.25728357e-01  1.74050890e-02 -5.30042015e-02
  1.68238848e-01 -3.95597458e-01  9.37559158e-02 -1.58015620e-02
 -1.20013617e-01 -2.28492498e-01 -7.09178522e-02  2.61582971e-01
  3.31594259e-01 -1.74629688e-01 -2.91908115e-01 -6.49440169e-01
 -8.66109729e-02 -1.82394646e-02  1.97189659e-01 -4.19505328e-01
 -2.88139373e-01  7.32841203e-03 -4.53725636e-01  2.57696658e-01
  7.80595690e-02 -9.83447209e-02 -1.56138092e-04  2.74487942e-01
  2.39596188e-01 -4.31265086e-01  2.86015868e-02 -5.27951956e-01
 -8.82724673e-02 -3.12322192e-03 -3.67087185e-01  1.60657167e-01
  1.08146995e-01  2.99204916e-01  7.30939209e-05 -1.32452976e-02
  1.62560284e-01 -3.29085924e-02  3.05767596e-01  1.11122299e-02
  1.52811874e-03 -1.90113127e-01  7.23171607e-02 -2.11157858e-01
 -5.17423272e-01 -2.52209246e-01 -2.11619154e-01  1.57464013e-01
  1.49600074e-01  4.13732022e-01 -7.53655583e-02  3.87842357e-02
 -2.61731774e-01  5.42003572e-01 -4.98134136e-01  5.19567192e-01
  3.36802870e-01  2.72358842e-02  5.56485891e-01  4.70294617e-02
  1.33186191e-01  2.22765692e-02  2.88437724e-01 -3.71801913e-01
  3.38122547e-01  2.59494185e-01  1.60428300e-01  3.31461608e-01
  1.99513212e-01  4.58529204e-01 -4.65705514e-01  4.03690815e-01
 -2.03081399e-01 -4.03631441e-02 -3.93597446e-02 -1.75647929e-01
  6.92029715e-01 -4.23106104e-01  1.16640486e-01 -1.83774233e-01
  5.41580319e-01 -1.67746365e-01 -1.01236507e-01  2.38670960e-01
  1.48309976e-01  1.45695299e-01  1.56116828e-01  1.16950780e-01
  6.80131540e-02 -4.46597673e-02 -2.20064789e-01 -6.04592502e-01
 -3.79028246e-02  4.52409908e-02 -2.73449481e-01  2.50032812e-01
 -4.61273864e-02 -4.91474718e-02  5.31815514e-02  4.62497994e-02
 -6.64943755e-02 -1.33355767e-01  7.93426409e-02  1.06478728e-01
 -4.76367414e-01  7.10447952e-02  4.16827559e-01 -1.27205580e-01
 -2.90659666e-01 -2.30127890e-02  3.61977845e-01  2.10882366e-01
  5.16541123e-01 -2.75617421e-01  1.71719790e-01 -2.17472672e-01
 -2.26210147e-01  4.26508605e-01 -7.87525922e-02 -3.81896794e-02
 -3.92748445e-01  7.41351604e-01  2.15205848e-01 -8.21973830e-02
  3.42682451e-01 -2.27687448e-01 -3.48639071e-01  1.70021266e-01
  2.33959064e-01 -2.47231394e-01 -2.95337625e-02 -3.75996292e-01
  3.19789618e-01  3.78199100e-01 -1.27710879e-01 -2.73374557e-01
  1.09362435e-02 -3.19869727e-01 -2.67026067e-01  4.32828665e-02
 -1.78753436e-01 -3.52335628e-05 -4.49343994e-02 -1.33644924e-01
 -3.18767965e-01 -1.02674581e-01 -1.44239143e-01 -2.86076427e-01
  3.58787179e-01 -3.47769111e-01  3.42036754e-01  7.60263443e-01
  1.91148102e-01  1.77271530e-01  3.37502994e-02  3.81488986e-02
 -1.01423860e-01 -3.94581333e-02 -4.37867530e-02  2.45414302e-01
 -8.94814208e-02  4.00361512e-03  2.58382261e-01  1.67976737e-01
 -1.13150939e-01  4.95234504e-02 -2.42646977e-01  1.58470407e-01
  5.24095111e-02 -3.25912118e-01 -1.84977591e-01 -2.17730939e-01
  2.86236435e-01  1.06387563e-01  6.57596141e-02  2.74421751e-01
 -2.40361437e-01  4.83234316e-01  6.74444318e-01 -2.91738033e-01
 -6.45138025e-02 -1.49664413e-02  1.06519818e-01 -1.16442755e-01
  5.77664860e-02 -1.38613313e-01  1.13182224e-01  1.68524653e-01]"
TF 2.14 minimum nvidia driver version type:docs-bug stat:awaiting response type:bug type:build/install stale TF2.14,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.14

### Custom code

No

### OS platform and distribution

Tensorflow Docker image

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

450.203.8

### GPU model and memory

_No response_

### Current behavior?

I have a question on if the minimum nvidia driver version has changed (I believe the current docs state `450.80.02` (https://www.tensorflow.org/install/pip)). The below script ran using the 2.13 docker image. Thank you.

When trying to run a test gpu benchmark, I get the following error:
```text
2023-10-02 16:01:32.433368: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:461] Possibly insufficient driver version: 450.203.8
```


### Standalone code to reproduce the issue

```shell
Below is the code being used:

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
import timeit

# Download data and scale
(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()

# scaling image values between 0-1
X_train_scaled = X_train/255
X_test_scaled = X_test/255

# one hot encoding labels
y_train_encoded = keras.utils.to_categorical(y_train, num_classes = 10, dtype = 'float32')
y_test_encoded = keras.utils.to_categorical(y_test, num_classes = 10, dtype = 'float32')

# Define the model
def get_model():
    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(32,32,3)),
        keras.layers.Dense(3000, activation='relu'),
        keras.layers.Dense(1000, activation='relu'),
        keras.layers.Dense(10, activation='sigmoid')
    ])
    model.compile(optimizer='SGD',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
    return model

# GPU Benchmark
def gpuBench():
    # GPU
    #strategy = tf.distribute.MirroredStrategy()
    #with strategy.scope():
    with tf.device('/GPU:0'):
        model_gpu = get_model()
        model_gpu.fit(X_train_scaled, y_train_encoded, epochs = 10)

gpuBench()
```


### Relevant log output

_No response_",True,"[-3.13728154e-01 -3.85360241e-01 -5.35614789e-02  1.34038776e-01
  4.77956712e-01 -1.99881077e-01 -1.50109708e-01  2.98234820e-01
 -1.95534959e-01 -2.37788588e-01 -1.27024353e-01 -1.01646110e-01
 -1.94675148e-01  9.77943912e-02 -2.62865692e-01  3.88596237e-01
 -2.75650799e-01 -2.10115850e-01  2.92643309e-01  2.54144609e-01
 -2.97323704e-01 -7.84453079e-02 -3.24481100e-01  2.48653993e-01
  6.01580068e-02  4.41369303e-02 -3.33021224e-01 -3.56198475e-02
  6.50508329e-03  2.53771663e-01  1.53526202e-01  2.90195286e-01
 -1.23648353e-01  1.05611399e-01  2.43874639e-01 -1.32434309e-01
 -2.09414467e-01 -1.96023643e-01 -4.60534513e-01 -2.00203642e-01
 -2.33521368e-02 -1.07969582e-01  2.95596514e-02 -8.12789947e-02
  3.49416435e-02 -1.32940769e-01  6.28241524e-02 -1.49612114e-01
 -1.24502376e-01 -1.67825855e-02 -1.90597445e-01  3.90479453e-02
 -3.34215611e-01 -2.68485010e-01 -7.12240040e-02 -6.12304211e-02
  1.06596872e-01  1.16557084e-01  3.85104388e-01  6.47099197e-01
  6.64365888e-02 -1.18926622e-01  8.99181813e-02 -1.79030031e-01
  1.31968588e-01  1.56702220e-01  3.23830321e-02 -1.25410572e-01
  2.86132246e-01 -2.02527836e-01  6.71100020e-02 -2.31733382e-01
 -3.44490439e-01 -6.48241267e-02 -1.29374906e-01 -1.74292654e-01
  8.73736292e-02  2.26026505e-01  1.00351378e-01 -1.20313674e-01
 -2.01073259e-01 -1.60695851e-01 -1.61424279e-01 -2.58354783e-01
  7.17132539e-02 -1.67345300e-01  4.28928912e-01  1.62894189e-01
  5.17806530e-01 -3.83288592e-01  3.46994460e-01  4.82062131e-01
  1.75045267e-01  1.18633673e-01  4.53205109e-01 -8.62749517e-02
  2.54350901e-01  3.05955023e-01 -1.19325459e-01 -1.90853700e-02
 -6.74198493e-02 -1.93068683e-01  1.43450469e-01  8.22497904e-02
 -9.63761359e-02 -6.39207214e-02  1.66207552e-01 -3.05421185e-02
 -1.34465814e-01 -2.33242139e-01  3.22633296e-01  2.35329513e-02
  2.65409589e-01 -7.07501248e-02 -1.30270813e-02  7.25832656e-02
 -2.70065874e-01 -7.82562271e-02  4.63064238e-02  5.93598366e-01
  3.84640694e-02 -2.28085577e-01 -5.51279336e-02  5.73138334e-02
  4.54329282e-01 -3.60184647e-02 -1.32257789e-01  6.69483095e-02
  5.72960488e-02 -8.62526745e-02  9.18355025e-03  2.35067457e-01
  7.94947743e-02  3.30690175e-01 -1.33602694e-01  1.87470824e-01
 -2.82213926e-01 -7.86361322e-02 -1.93273216e-01 -1.54399604e-01
 -3.53478491e-01  4.59639803e-02  9.38721597e-02 -4.69033539e-01
  1.28564045e-01  1.19791478e-01 -1.80115432e-01  3.45017672e-01
 -8.27357396e-02  3.13338488e-01  9.17373747e-02 -5.21912239e-03
  1.62073925e-01  4.33225006e-01  1.21811360e-01 -2.39713848e-01
  3.66824925e-01 -1.56534135e-01 -3.26743424e-01 -6.00437284e-01
  1.20194936e-02  2.79987723e-01 -1.93325818e-01 -2.83482403e-01
 -6.85358867e-02  2.98709035e-01 -5.57589293e-01 -4.26479988e-02
  1.48146659e-01  4.26561743e-01 -3.04657742e-02 -2.04626963e-01
  9.79198366e-02  3.14472497e-01  2.59110361e-01 -5.02465069e-02
  3.95025730e-01 -4.34113085e-01 -2.29450405e-01  3.29895228e-01
  5.03425971e-02  1.52713731e-01  1.07219934e-01  1.87031090e-01
  6.37605786e-04  4.33080010e-02  4.54306379e-02  9.00907815e-02
 -2.47616231e-01 -1.59774676e-01 -2.17056632e-01 -1.67992726e-01
  3.49835336e-01  5.12566641e-02 -1.47455305e-01  2.13286251e-01
  1.07860722e-01  7.32563622e-03  1.43998697e-01  4.04011011e-02
 -1.36695579e-01 -5.10722101e-02  1.99767321e-01  1.07700571e-01
  7.18054250e-02 -2.20230222e-01 -2.31815472e-01 -3.90228331e-01
 -5.25529087e-01  5.50955310e-02  1.98383152e-01 -2.91070819e-01
  1.66845188e-01 -2.56827325e-01 -3.22257549e-01  3.92214119e-01
 -4.47456092e-02 -1.40013173e-01 -2.31964022e-01  3.57626885e-01
  1.72994763e-01 -1.50782973e-01  5.82101792e-02 -4.16933596e-01
 -1.99059516e-01  4.71063890e-02 -1.92350328e-01  1.75944865e-01
  5.86462021e-02  2.23656073e-01 -2.83306558e-02  1.12129971e-02
  3.20950925e-01  1.53866470e-01  4.80911732e-01 -2.41649553e-01
 -2.47270018e-01 -1.74743265e-01 -3.08824033e-01 -4.15064767e-03
 -3.04911554e-01 -1.60468668e-01 -8.48466158e-02 -4.16001678e-02
  4.32591110e-01  4.03386831e-01 -1.07948214e-01 -2.03344598e-02
 -2.77424514e-01  4.45381284e-01 -1.75835788e-01  1.33032203e-01
  2.45415732e-01  1.95913285e-01  4.18823659e-01  1.86576061e-02
  2.02537715e-01  2.71378994e-01  4.38619167e-01 -7.55003095e-02
  3.31994951e-01  2.66588211e-01 -1.65520132e-01  6.56950831e-01
  2.18794584e-01  3.28519166e-01 -2.53990382e-01  5.01617670e-01
 -2.35696614e-01 -7.81829208e-02  3.07854265e-04 -4.56471205e-01
  8.24063718e-01 -1.55837879e-01  1.03249669e-01 -1.89275354e-01
  4.46686476e-01  1.35030419e-01 -7.16056749e-02  1.81289129e-02
  2.31135413e-01  4.49181199e-01 -3.18900049e-01  3.61370966e-02
  7.93023109e-02 -1.67192332e-03 -3.13851297e-01 -6.59385920e-01
 -2.26447567e-01 -2.46155774e-03 -3.70105892e-01  9.91040021e-02
 -3.43149751e-02 -1.49554640e-01 -7.48033151e-02  5.16529381e-02
 -2.12271392e-01  4.99046259e-02  1.35526340e-02  1.98407486e-01
 -5.01349047e-02  7.42788911e-02  5.59248149e-01 -4.65202510e-01
 -5.53997979e-03 -2.34113574e-01  3.86273801e-01  3.91056836e-01
  3.52040648e-01 -3.33978921e-01  6.02779053e-02 -7.92655200e-02
  1.70919776e-01  4.58093643e-01  6.46672696e-02  1.66237772e-01
 -4.96143579e-01  5.12873709e-01 -1.32652270e-02 -1.78264767e-01
  1.21172130e-01 -3.11904192e-01 -5.80851495e-01  1.06573917e-01
  3.36733490e-01 -1.74119473e-01 -1.49342760e-01 -1.90642685e-01
 -1.94213480e-01  3.20900410e-01  1.26124024e-01 -2.34290347e-01
 -8.40922147e-02 -1.31703429e-02 -3.59512419e-01 -5.26525676e-02
 -2.87700564e-01  4.21511829e-02 -2.99228542e-02 -3.43039423e-01
 -2.50494361e-01 -1.48698285e-01 -2.74349004e-01 -3.56495798e-01
  1.34503141e-01 -6.01757407e-01  1.68020889e-01  5.64977288e-01
 -1.35107897e-02  2.40919366e-03  9.14819390e-02  1.61405697e-01
 -4.87333536e-01 -1.12460703e-02 -1.71841569e-02  2.33207017e-01
  7.93030336e-02 -7.48184919e-02  5.18434286e-01  2.51404554e-01
 -1.13431364e-01  1.63341746e-01 -2.58704484e-01 -5.06130084e-02
  6.38470724e-02 -3.73585403e-01 -2.68544048e-01 -1.90124914e-01
  1.57956034e-01  2.67593950e-01 -1.66099072e-02  3.09391886e-01
 -1.58398777e-01  3.39087039e-01  4.38654482e-01 -4.80434895e-01
 -2.34558314e-01  1.92900479e-01  2.09907383e-01 -3.04607689e-01
 -1.58377230e-01 -7.90531412e-02  3.18585455e-01  6.33966736e-03]"
Models that trained and saved on Tensorflow2.14 cant be loaded on Tensorflow2.12 stat:awaiting response type:bug stale comp:apis TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Linux mint 21.2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

gtx 1060 6gb

### Current behavior?

I expected that when I load a keras model that I trained on my home computer on tensorflow 2.14 wil be loaded on kaggle notebook which has tensorflow 2.12

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

#the test_model.keras was trained on tensorflow version 2.14
#the current tensorflow version in 2.12

model = tf.keras.models.load_model(""/home/sagi/Desktop/VsCode/Competiton/MODEL/test_model.keras"")
```


### Relevant log output

_No response_",True,"[-3.92757058e-01 -4.76488441e-01 -1.83965921e-01  3.02157626e-02
  3.08624923e-01 -2.44018838e-01 -1.01807401e-01 -3.66634205e-02
 -3.26603711e-01 -3.40328395e-01  2.08349019e-01 -2.48952620e-02
 -1.97489291e-01  1.80366904e-01 -1.56705558e-01  2.04820246e-01
 -1.95282727e-01 -5.90414479e-02  3.68033983e-02  1.42261386e-01
 -1.14423811e-01 -2.31457919e-01 -1.76117644e-01  1.40119046e-01
  1.91487446e-01  2.34536350e-01 -2.61089921e-01 -1.38350129e-01
  8.35210234e-02  1.52971208e-01  2.71955431e-01  3.28496173e-02
 -6.70460537e-02  5.32432273e-02 -7.21328259e-02  3.27444136e-01
 -3.37415159e-01 -2.28404835e-01 -2.36264795e-01  5.70593067e-02
  1.03072092e-01 -1.58023089e-03  1.45459637e-01 -2.65718192e-01
  4.14947979e-04 -2.66373158e-01  5.94492070e-02 -1.36981815e-01
 -8.27912986e-02 -1.60720557e-01  9.30014998e-02 -9.65309963e-02
 -4.26835299e-01 -4.11130041e-01 -2.31414586e-01 -7.73217380e-02
  2.63332814e-01 -2.48687044e-01 -8.99158865e-02  8.98236185e-02
 -1.52387712e-02  3.86029445e-02  2.34220363e-02 -6.79725036e-02
  1.09665781e-01  1.59999758e-01  2.44727254e-01  1.61091909e-02
  6.61627948e-01 -8.22395831e-02  7.74078593e-02 -7.25931078e-02
 -4.46075082e-01  8.21717158e-02 -7.11533576e-02  9.55615342e-02
  1.30725905e-01  1.43073797e-01  2.80980021e-01 -1.46976158e-01
  4.86619174e-02 -4.05729711e-01  8.78961943e-03 -2.24938035e-01
  1.40699476e-01 -1.86008960e-01  4.47257340e-01  7.65800998e-02
  5.38245082e-01 -2.42711887e-01  4.80860472e-01  3.95652711e-01
  3.82898422e-03  7.15789422e-02  4.69104916e-01  3.23624700e-01
  3.96071039e-02  3.82293582e-01  5.27490824e-02 -1.96201071e-01
 -1.23257324e-01 -1.72849894e-01  7.38019347e-02  1.45307630e-01
 -1.70197934e-02 -2.08868936e-01  8.88753533e-02 -2.87529826e-01
  1.30555362e-01 -1.34292692e-01  1.64118737e-01 -1.29026130e-01
  3.26946229e-01  5.34137487e-02  4.53282446e-02 -6.96429014e-02
 -8.84490609e-02  1.50786325e-01 -1.25419289e-01  8.18126857e-01
  4.20945957e-02 -2.53997296e-01  2.66103327e-01  6.42412603e-02
  5.01789212e-01  9.38518420e-02 -1.55365944e-01 -6.86120689e-02
  3.00503373e-02 -2.19902456e-01  2.74817765e-01  1.65314645e-01
  4.10991684e-02  1.66058689e-01 -2.34315898e-02  5.55485226e-02
 -2.36741588e-01 -1.80596471e-01 -2.53542185e-01 -1.96584255e-01
 -3.43985736e-01  2.62085348e-01 -2.11779147e-01 -3.82524788e-01
  1.85015917e-01  7.28258938e-02 -8.91845375e-02  3.38381976e-01
 -1.52482584e-01  1.00247391e-01 -8.73758495e-02  1.76609810e-02
  1.94328371e-02  2.93997675e-01  2.74942935e-01  1.46317124e-01
  3.05690855e-01 -9.33497846e-02 -1.81170702e-02 -6.25907183e-01
 -2.39889547e-02  3.84772420e-01 -1.63746238e-01 -3.17476779e-01
  2.51813442e-01  2.71439314e-01 -2.52721161e-01 -2.76879013e-01
  2.82406896e-01  4.74761724e-01 -7.86056668e-02 -2.73271382e-01
  2.49918997e-02  7.70688578e-02  1.64460912e-01 -9.02954563e-02
  2.63487875e-01 -7.96028972e-01 -8.85071158e-02  3.53635609e-01
  1.55598283e-01  8.88406336e-02  8.82318318e-02  1.57024503e-01
 -5.21679483e-02  6.59053922e-02  1.90489024e-01  1.60599295e-02
 -2.45557308e-01  8.56107920e-02 -3.54524434e-01 -8.94404203e-02
  5.70751369e-01 -1.80394035e-02 -4.03793715e-02  5.77386655e-02
  2.35704765e-01 -5.48832566e-02 -1.69130728e-01  2.65632290e-02
 -1.46248132e-01 -8.02884102e-02 -5.44625781e-02 -1.24067649e-01
  1.00201264e-01 -3.74249160e-01 -1.08741730e-01 -3.21204126e-01
 -2.99103022e-01  3.45328078e-02  7.92966411e-02 -4.69097674e-01
  1.65531993e-01 -4.45724204e-02 -4.69055146e-01  1.09047890e-01
  7.03101009e-02  1.26539335e-01 -1.31494060e-01  3.27818513e-01
  1.79668874e-01 -1.39190987e-01  2.45618001e-02 -3.37032765e-01
 -3.09983373e-01  2.09359169e-01 -3.11090469e-01  7.57415593e-02
  1.01170860e-01  2.56626844e-01  2.18265533e-01  2.28349507e-01
  3.24880421e-01  3.34283650e-01  3.93114984e-01 -2.55292088e-01
 -1.65308744e-01 -1.51148856e-01 -1.18438214e-01 -7.19104894e-03
 -4.26205873e-01 -2.21618995e-01  2.86684260e-02 -1.74647644e-01
  2.65349686e-01  4.87934887e-01 -1.66723028e-01 -6.96500391e-02
 -2.84929782e-01  2.92268693e-01 -4.03713822e-01  1.68462619e-01
  3.63487720e-01  2.30088204e-01  5.01058757e-01  7.46171549e-02
  1.04903832e-01  2.33608529e-01  2.19984517e-01 -2.09762722e-01
  4.64983076e-01  2.91949451e-01 -3.99299338e-03  5.20260572e-01
  3.01967025e-01  2.20531881e-01 -3.67911696e-01  5.86420894e-01
  2.94456929e-02 -1.22149631e-01  1.83413476e-01 -4.59144413e-01
  3.81795615e-01 -4.46152806e-01  1.41524255e-01 -3.94648872e-03
  3.33743781e-01  3.42139415e-02 -1.39655679e-01 -2.88719311e-02
  2.52921343e-01  2.14738131e-01 -3.08615267e-01 -7.17088673e-03
 -1.38879210e-01 -2.17037439e-01 -1.56624764e-01 -5.55849910e-01
 -2.69821495e-01  7.78331934e-03 -3.67320895e-01  1.67012736e-01
 -1.63035747e-02 -5.65476790e-02 -3.96583796e-01  5.03374711e-02
  1.26203805e-01 -1.05131224e-01  1.41277730e-01  2.32103437e-01
 -2.95496285e-01 -4.75126579e-02  4.00475413e-01 -5.87037563e-01
 -2.39343762e-01 -2.31275946e-01  4.70039725e-01  2.29864821e-01
  4.08894360e-01 -4.42835748e-01  5.18574193e-02 -7.72583485e-02
  7.26313144e-02  4.63678598e-01  6.22920170e-02  7.46666715e-02
 -4.09115821e-01  7.62427211e-01  1.53106526e-01 -1.06559999e-01
  1.31977081e-01 -6.26965016e-02 -2.45905399e-01  6.84506148e-02
  2.82226741e-01 -6.75045252e-02  3.47405970e-02 -3.49334359e-01
 -1.52978674e-03  1.28991485e-01 -1.36706322e-01 -1.62649840e-01
 -1.00658655e-01  1.03902899e-01 -1.86644331e-01 -1.07660472e-01
 -4.41264838e-01  2.59535551e-01  8.73747393e-02 -4.48123246e-01
 -6.64560795e-02 -1.79994255e-02 -1.70391083e-01 -3.42730992e-02
  1.01154000e-01 -4.30718273e-01  3.77910435e-01  5.37319243e-01
 -1.08065978e-01  1.54808655e-01  1.81537624e-02  2.18074262e-01
 -4.72397327e-01 -3.20014879e-02 -7.43681192e-02  4.17148501e-01
  1.15596108e-01 -4.13246691e-01  3.33090544e-01  3.52000415e-01
 -1.71612114e-01  1.10189393e-02 -2.74313152e-01  6.02378622e-02
  2.40931392e-01 -2.83506811e-01 -1.30671471e-01 -4.82690811e-01
  2.07217067e-01  4.03391629e-01 -1.79620117e-01  1.46785066e-01
 -1.97259814e-01  2.79015422e-01  6.12362981e-01 -6.44490719e-01
 -3.77660632e-01  1.20664053e-01  2.80344576e-01 -1.26806155e-01
 -7.83691704e-02 -9.87931415e-02  6.40129000e-02 -1.72359258e-01]"
TFlite memory allocation thrashing with FlexDelegates stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.13,"### 1. System information

- OS Platform and Distribution macOS, iOS 16.6.1 iPhone 13 Pro:
- TensorFlow installation (pip package or built from source): 2.13.0
- TensorFlow library (version, if pip package or github SHA, if built from source): tflite built from 2.13.0 tag

### 2. Code

I have a model that converts correctly and flags the following flex delegates:

```python
  tf.AddV2(tensor<1x33xcomplex<f32>>, tensor<complex<f32>>) -> (tensor<1x33xcomplex<f32>>) : {device = """"}
  tf.AddV2(tensor<complex<f32>>, tensor<complex<f32>>) -> (tensor<complex<f32>>) : {device = """"}
  tf.Complex(tensor<1x33xf32>, tensor<1x33xf32>) -> (tensor<1x33xcomplex<f32>>) : {device = """"}
  tf.Complex(tensor<f32>, tensor<f32>) -> (tensor<complex<f32>>) : {device = """"}
  tf.ConcatV2(tensor<1x1x1xcomplex<f32>>, tensor<1x1x64xcomplex<f32>>, tensor<i32>) -> (tensor<1x1x33xcomplex<f32>>) : {device = """"}
  tf.GatherV2(tensor<1x1x33xcomplex<f32>>, tensor<i32>, tensor<i32>) -> (tensor<1x33xcomplex<f32>>) : {batch_dims = 0 : i64}
  tf.Pow(tensor<complex<f32>>, tensor<complex<f32>>) -> (tensor<complex<f32>>) : {device = """"}
  tf.RealDiv(tensor<1x33xcomplex<f32>>, tensor<1x33xcomplex<f32>>) -> (tensor<1x33xcomplex<f32>>) : {device = """"}
  tf.SelectV2(tensor<1x1x1xi1>, tensor<1x1x33xcomplex<f32>>, tensor<1x1x33xcomplex<f32>>) -> (tensor<1x1x33xcomplex<f32>>) : {device = """"}
  tf.StridedSlice(tensor<1x1x33xcomplex<f32>>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> (tensor<1x1x1xcomplex<f32>>) : {begin_mask = 7 : i64, ellipsis_mask = 0 : i64, end_mask = 3 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
  tf.StridedSlice(tensor<1x1x33xcomplex<f32>>, tensor<3xi32>, tensor<3xi32>, tensor<3xi32>) -> (tensor<1x1x64xcomplex<f32>>) : {begin_mask = 3 : i64, ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
  tf.Sub(tensor<complex<f32>>, tensor<complex<f32>>) -> (tensor<complex<f32>>) : {device = """"}
  tf.Transpose(tensor<1x1x33xcomplex<f32>>, tensor<3xi32>) -> (tensor<1x1x33xcomplex<f32>>) : {device = """"}
```

### 3. Failure after conversion
When running this model on iOS, and profiling the model, I can see the application spending a lot of time (30%) in `posix_memalign` that are emitted from the flex library which leads to poor performance of the model executing (highlighted in following image):

![image](https://github.com/tensorflow/tensorflow/assets/192171/f4950106-1b7f-47cc-8025-d3fc9818e643)

Is there a way to:
- figure out which flex delegate is causing this? (If I compile the flex library with symbols, I can't seem to turn on optimization which results in an unusable library for me)
- modify the memory allocation/de-allocation strategy to re-ruse existing memory? I suspect this issue is a side-effect of tflite <-> tensorflow interop but that is only a suspicion
- set something like max allowed persistent memory to prevent de-allocation? I also have a suspicion that it might not be tied to a single delegate but by the allocator itself",True,"[-3.95684749e-01 -3.45213026e-01 -1.39737427e-01 -9.07936785e-03
  1.34198919e-01  2.54202541e-03  9.28284600e-02  5.06777391e-02
 -1.25107765e-01 -1.58852398e-01 -6.80760369e-02  1.97207272e-01
 -1.59774095e-01  2.78188139e-01 -1.92874670e-01  2.05844104e-01
 -1.19726270e-01 -2.63897389e-01  1.84212983e-01  5.78021556e-02
  3.82526815e-02 -3.73445489e-02 -5.15923649e-02  2.41026983e-01
  1.50365531e-01  3.99062455e-01 -1.71011120e-01  3.74769000e-03
  3.80813703e-02  2.10851878e-01 -1.14056841e-03  7.80604631e-02
 -1.42353907e-01  2.59240866e-02 -2.07161829e-02  7.62584731e-02
 -2.19369382e-01 -3.11942939e-02 -3.70550752e-01 -1.71391010e-01
  7.65025690e-02  1.67838618e-01  8.45333859e-02 -4.24910262e-02
  2.40748134e-02  2.42963076e-01  5.50029129e-02  1.40297323e-01
 -1.32534921e-01 -1.98296458e-03  8.83362144e-02 -6.24436066e-02
 -9.98166278e-02 -2.41695493e-01  7.40216076e-02  2.50907272e-01
  1.15542211e-01  1.55796278e-02  5.70209250e-02  1.79495096e-01
  9.57789123e-02 -9.14194733e-02 -8.39987397e-02 -9.46629345e-02
  2.58570254e-01  1.99477032e-01  8.65976661e-02 -5.26139513e-02
  8.18638802e-02 -1.36176139e-01  2.86188535e-02 -1.57904789e-01
 -1.85969129e-01  2.83533521e-02  5.32466546e-02  4.59580198e-02
 -2.90970206e-02  1.06351517e-01  1.28301352e-01  4.14450243e-02
  5.27079925e-02 -2.74068445e-01 -2.96579339e-02  7.33189955e-02
  1.34607598e-01 -1.38494223e-01  2.63738543e-01  1.17155150e-01
  2.85051495e-01 -5.71697392e-02  2.87877589e-01  5.09486198e-01
 -4.81588319e-02  1.36671841e-01  2.45600402e-01  9.95012075e-02
  1.33581415e-01  2.12692037e-01 -2.63585672e-02  4.61053848e-02
  6.54847696e-02 -2.27207854e-01 -4.25949842e-02  4.09067944e-02
 -3.14382240e-02 -3.55246067e-01  1.19759403e-01  8.24393630e-02
  3.27611603e-02  8.65434706e-02  2.02329069e-01  1.84377935e-02
  2.27983460e-01  1.54555872e-01 -1.81226116e-02 -1.61864325e-01
  6.71561733e-02  1.89873219e-01  3.00049961e-01  3.25502485e-01
  3.29067260e-02 -3.07598084e-01 -2.26986371e-02  2.03111023e-01
  3.39565098e-01  6.20329939e-02 -3.37525964e-01 -9.90844890e-02
  5.20438403e-02  1.09136946e-01 -7.95235336e-02  2.74505883e-01
 -3.13053370e-01  6.42420538e-03 -9.12747532e-03 -2.65062936e-02
 -1.33160442e-01 -5.75286224e-02 -3.43011469e-01 -7.35781640e-02
 -2.85283983e-01  1.17592365e-01 -1.00545555e-01 -3.13851118e-01
 -1.15575880e-01  4.33999002e-02 -1.43651932e-01  1.00484595e-01
 -1.47980064e-01 -1.52663514e-03 -4.54331636e-02 -2.77840607e-02
 -1.00091860e-01  1.85133815e-01  1.49639696e-01 -3.08750086e-02
  2.03705832e-01 -7.42222294e-02  1.70974851e-01 -2.81835616e-01
 -1.13950521e-01  7.17487261e-02  8.99457838e-04 -6.08018637e-02
  5.77808395e-02 -1.98007319e-02 -4.05603170e-01 -5.43655828e-02
  5.47943115e-02  2.49728292e-01 -5.91053665e-02 -2.58959055e-01
  1.14778519e-01  1.26415826e-02  1.48415893e-01 -1.32616729e-01
  2.73339331e-01 -4.95590061e-01 -4.19323705e-02 -1.89714968e-01
  3.51031348e-02  7.33282417e-03  3.14597413e-03  3.38520249e-03
 -3.93284336e-02 -2.15525478e-02  2.19113916e-01  1.74553469e-01
 -3.63669485e-01 -8.52541253e-02 -3.56091976e-01 -1.75130159e-01
  2.06844226e-01  2.11832091e-01 -7.85305947e-02 -6.29759133e-02
 -5.86644094e-03  1.26968384e-01 -1.32235996e-02  3.01096402e-02
  5.86082451e-02  2.35875353e-01  3.62430140e-02 -1.08117394e-01
  1.54383302e-01 -1.29628211e-01 -1.22599095e-01 -4.41769123e-01
 -3.23844194e-01  1.43379904e-03  2.67722517e-01 -1.72660723e-01
  1.24753844e-02 -1.72044188e-01 -1.38711751e-01 -7.85158575e-03
 -7.60155469e-02  2.14420818e-02 -2.51609087e-01  3.87923513e-03
 -3.96427959e-02 -1.16742589e-01 -4.94678058e-02 -1.72252685e-01
 -3.48809659e-01 -1.41135007e-01 -2.90934652e-01  2.98810959e-01
 -1.28741831e-01  2.76832998e-01 -1.88757330e-01  1.55202091e-01
  4.80489910e-01 -9.45664793e-02  1.31766215e-01 -8.19680542e-02
  5.58438376e-02 -8.95541906e-02 -2.37513721e-01 -1.03737786e-01
 -1.85556158e-01 -2.24683315e-01  8.42709653e-03 -1.61231667e-01
  3.90164182e-02  4.33361307e-02  7.77885467e-02 -5.24156541e-03
 -3.59328836e-02  2.84577906e-01 -1.60379082e-01  8.48630145e-02
  3.56868535e-01  1.11549333e-01  2.31088147e-01  8.41843635e-02
  1.26679778e-01 -3.88822965e-02  1.46671450e-02 -9.14225727e-02
  1.14185378e-01  2.52063692e-01  1.00604929e-01  6.38001680e-01
  1.86998934e-01  1.94540113e-01 -2.25992560e-01  8.14834237e-02
 -6.82410449e-02 -1.41689345e-01  1.61722116e-02 -1.32112488e-01
  3.36299628e-01 -1.86172232e-01  1.67693898e-01 -1.66640967e-01
  3.20310056e-01 -2.32453227e-01 -7.76109472e-02  5.51393479e-02
  1.56416640e-01  2.14283541e-01 -2.24598750e-01 -9.01404209e-03
  1.02665126e-01 -9.57873091e-02 -1.18582444e-02 -4.94062543e-01
 -1.05261348e-01  1.45819053e-01 -2.20930815e-01 -1.27025440e-01
 -3.55837531e-02  2.01715261e-01 -1.73409790e-01  1.49062097e-01
  2.02848911e-01 -1.63124293e-01  1.08149081e-01  5.48903272e-03
  6.51561376e-03  3.59994620e-01  2.89955974e-01 -2.69575059e-01
 -1.02169670e-01  1.11899823e-01  1.36434078e-01 -1.63868561e-01
  3.35486650e-01 -3.46521497e-01  4.55670133e-02  5.66184185e-02
 -8.41216557e-03  1.97265297e-01  1.34256989e-01  2.03042068e-02
 -2.25745827e-01  2.34957188e-01  5.30559085e-02  8.58202577e-04
  1.59993172e-01 -8.12059641e-02 -2.66698390e-01  1.15629181e-01
 -1.08661745e-02  1.88096032e-01  2.42151409e-01 -1.38885409e-01
  1.79014385e-01 -9.48080793e-02 -1.14597589e-01 -2.24524707e-01
 -7.47531578e-02  6.07126653e-02 -2.39517972e-01  2.20798478e-02
 -5.93859926e-02  1.30342633e-01 -2.46890411e-01 -2.05364287e-01
 -5.69518432e-02 -1.56107172e-01 -1.10842317e-01 -2.06172377e-01
  1.16211832e-01 -1.92572176e-01  6.13492765e-02  1.10966198e-01
  6.39271662e-02  7.23676533e-02 -1.55666143e-01 -7.55509660e-02
  5.29368743e-02  1.29234400e-02 -6.63687512e-02  2.89847195e-01
 -1.87330604e-01 -1.64625436e-01  1.03324160e-01  3.27052057e-01
  3.69533226e-02 -1.17663294e-04 -4.01357114e-01 -2.33114153e-01
  2.20542736e-02 -2.12975577e-01 -2.82366782e-01 -1.47507697e-01
  2.30178088e-02  4.37524438e-01  1.01198576e-01  2.38482118e-01
 -1.26864135e-01  1.18176997e-01  2.05831140e-01 -1.78461790e-01
  5.88767231e-02  7.36282915e-02  9.48516130e-02 -4.28280756e-02
 -2.89104562e-02 -2.56466288e-02 -4.87278402e-03  4.97794896e-02]"
"Checkpoint callback error ""save best only""  in .keras format. stat:awaiting response type:bug stale comp:keras regression issue TF 2.13","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.02

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Saving best model only with callback with .keras format as the whole model which works well when saving every epoch.

Locally I fixed this adding elif option to save best only as in the version with this parameter is False.

```python
 elif filepath.endswith("".keras""):
    self.model.save(filepath, overwrite=True)
```

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1e_imdDFEm-5qARqSbXm8-5JxZP43V_wg?usp=sharing
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/r.kaczmarek/repos/diarization-poc/tmp_train.py"", line 67, in <module>
    model.fit(
  File ""/home/r.kaczmarek/miniconda3/envs/tf-diarization/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/r.kaczmarek/miniconda3/envs/tf-diarization/lib/python3.10/site-packages/keras/src/saving/saving_api.py"", line 142, in save_model
    raise ValueError(
ValueError: The following argument(s) are not supported with the native Keras format: ['options']
```
",True,"[-0.12886918 -0.19678944 -0.39450356 -0.14633659  0.28668332 -0.23486295
 -0.02073967 -0.0349059  -0.40324193 -0.27531737  0.3618567   0.0252939
 -0.06255272  0.02278875 -0.19753969  0.19635034 -0.22375432 -0.04447021
  0.28657335  0.02566632 -0.15039064 -0.14122277 -0.22563344  0.25555474
  0.06136353  0.16783537 -0.1601516  -0.04741419 -0.10596474  0.12062178
  0.27217954  0.16691947 -0.1289545   0.02005226  0.19237128  0.06880413
 -0.23624873 -0.03069451 -0.16293857 -0.04566204  0.19897735 -0.06217407
 -0.06489497 -0.24677715  0.01957516 -0.31672034  0.0188587  -0.17775717
 -0.08035755 -0.3910864  -0.06919595 -0.01556025 -0.33032155 -0.47776473
 -0.15144145 -0.14548758  0.23153901 -0.08867595  0.03071685  0.02946354
  0.0968007  -0.00459843  0.0154325  -0.18538344  0.22791678 -0.18337014
  0.24511313  0.17181692  0.51133585 -0.07909861 -0.04451355 -0.13363253
 -0.43186885 -0.03196481  0.09002036  0.22682354  0.00136522 -0.01814532
  0.31436187 -0.37192738 -0.15299937 -0.33565825 -0.22169757 -0.4302904
  0.14880899 -0.3193407   0.3871761   0.07466996  0.7547846  -0.20334108
  0.5805459   0.51406145 -0.06438192  0.1681231   0.37846014  0.13169014
  0.06719851  0.3260107  -0.19797993 -0.05082049 -0.01653238 -0.17539194
 -0.12644938  0.04787224 -0.05526925  0.01991622  0.07714172 -0.33473673
  0.1459205   0.09508055  0.21047527  0.07345486  0.18560956  0.07882152
  0.14185211  0.10587443  0.07043336  0.04734938 -0.12589553  0.5814945
  0.01931063 -0.04868802  0.16757067  0.41871637  0.31968725  0.17401119
 -0.0382415   0.1942865   0.18373415 -0.23306178 -0.16297965 -0.1102408
  0.275162    0.12375474 -0.27060562 -0.11850373 -0.10331754 -0.14203398
 -0.27048814 -0.42649728 -0.19842368 -0.01914965  0.10040407 -0.29152626
  0.2131119   0.14876649 -0.23259346  0.1793795  -0.19270095  0.15575066
 -0.1477432  -0.05338374 -0.11523055  0.35013124 -0.01399121 -0.07155929
  0.13519503 -0.0277341   0.0753224  -0.41283408  0.10991291  0.44676924
  0.10521208 -0.06362295  0.37462956  0.0912146  -0.3980946  -0.3560298
  0.10480488  0.32951453 -0.07815054 -0.04414434  0.03594899 -0.10143942
  0.24963307  0.00099493  0.19780248 -0.47475547 -0.21272457  0.37758976
 -0.02009508 -0.07081562 -0.18324023  0.32712013  0.15141028 -0.12513614
  0.14563657  0.02799777 -0.36264294  0.03161039 -0.3412857  -0.1233986
  0.38186723 -0.37098926 -0.08961307 -0.02659449  0.25784227 -0.16777903
  0.06004781 -0.10010951 -0.1137572  -0.23162551 -0.0285694  -0.1129276
  0.2305932  -0.3251223  -0.08973231 -0.43936148 -0.19468418  0.18905178
  0.01647886 -0.60901916  0.30210727 -0.15476178 -0.32066184  0.15209772
  0.0586061   0.20083427 -0.00090915  0.25943926  0.2446604  -0.22660509
  0.03986444 -0.34439155 -0.04542214  0.19323178  0.0098499   0.06019223
  0.06770861  0.19125485  0.05125011  0.06046012  0.3499667   0.20827276
  0.28434038 -0.21150196  0.1396859  -0.07366467  0.12703232 -0.06065746
 -0.41276217 -0.0246689   0.04371593 -0.1508413   0.32190916  0.3937813
 -0.31729582  0.04243784 -0.3668279  -0.01295814 -0.2524869   0.2193658
  0.25457835  0.03824091  0.41400957  0.06367175  0.02933214  0.24353206
  0.41607112 -0.09995319  0.50999737 -0.04800318 -0.14583185  0.4027496
  0.2757489   0.26080406 -0.4787895   0.4793455  -0.20958024 -0.14882183
  0.26871502 -0.34930664  0.49488705 -0.33523527  0.29465812 -0.01646093
  0.4721064   0.00629239  0.17119925  0.19645745  0.11377469  0.3520921
 -0.4553831  -0.02733861  0.05797218 -0.1900275  -0.14240174 -0.6927342
 -0.04047396 -0.05829043 -0.16699533  0.17435688 -0.06128221 -0.13342449
 -0.01971096  0.15573412 -0.12820308 -0.19237632  0.23394206  0.04494655
 -0.41738856  0.01086175  0.433536   -0.5151449  -0.28052402 -0.22265446
  0.33418363  0.47247505  0.43365464 -0.34485397  0.1893009   0.05908705
  0.14391635  0.37707824  0.05604583 -0.06354726 -0.26439446  0.5950978
  0.02578424 -0.01349714  0.29924715 -0.00448765 -0.4233825   0.1700618
  0.34096166 -0.09537987 -0.07260472 -0.45926476  0.02455998  0.23568857
 -0.05408815 -0.02613595  0.03701893 -0.00476662 -0.25962332 -0.13511126
 -0.2946337   0.31227842  0.03700294 -0.49789703 -0.401725    0.10004167
 -0.09523416 -0.06417002  0.13294886 -0.23248832  0.25144714  0.78949445
  0.182847    0.03630348 -0.02826772 -0.04438969 -0.2517253   0.10207517
 -0.0059073   0.32468894 -0.06714313 -0.16626188  0.5298199   0.37015313
 -0.27442834 -0.02209454 -0.2436994   0.059581   -0.08401806 -0.11269288
 -0.2794238  -0.32225007  0.08569767  0.05083876 -0.1580109   0.16854528
 -0.07890742  0.21372849  0.38253582 -0.49917072 -0.18040097  0.03638649
  0.2592513  -0.21413392 -0.0205886   0.00927913 -0.04607966  0.03675048]"
TFLite model produces wrong output for constant addition type:bug stale comp:lite TFLiteConverter,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.15.0-dev20230926

### 2. Code

The TensorFlow Lite model in the example below should output `x1+x2+x2=7+1+1=9`. However, it produces a wrong output `x1+x1+x2=7+7+1=15`. This indicates that it confuses the order of the two inputs.

```
import tensorflow as tf
import numpy as np

a = tf.constant(7.0, shape=[1])
b = tf.constant(1.0, shape=[1])
input_data = [a, b]


def _evaluateTFLiteModel(tflite_model, input_data):
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Test model on random input data.
    for i in range(len(input_details)):
        # input_shape = input_details[i]['shape']
        # input_data_i = np.array(np.random.random_sample(input_shape), dtype=np.float32)
        interpreter.set_tensor(input_details[i]['index'], input_data[i])

    interpreter.invoke()

    # The function `get_tensor()` returns a copy of the tensor data.
    # Use `tensor()` in order to get a pointer to the tensor.
    output_data = [interpreter.get_tensor(output_details[i]['index'])
                   for i in range(len(output_details))]
    return output_data


class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()

    @tf.function(input_signature=[tf.TensorSpec(shape=x.shape, dtype=x.dtype) for x in input_data])
    def call(self, x1, x2):
        return ((x1 + x2) + x2)

m = Model()

converter = tf.lite.TFLiteConverter.from_keras_model(m)
tflite_model = converter.convert()

print(_evaluateTFLiteModel(tflite_model, input_data))
```
Output:
```
[array([15.], dtype=float32)]
```
Keras model:
```
import tensorflow as tf
print(tf.__version__)

import tensorflow as tf
import numpy as np


class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()

    def call(self, x1, x2):
        return ((x1 + x2) + x2)



m = Model()

a = tf.constant(7.0, shape=[1])
b = tf.constant(1.0, shape=[1])
input_data = [a, b]
print(m(a, b))
```
Output:
```
tf.Tensor([9.], shape=(1,), dtype=float32)
```

### 3. Failure after conversion
Wrong results.
",True,"[-4.94357914e-01 -7.39017010e-01 -7.01632071e-03  1.32755280e-01
  2.30815500e-01 -8.27253461e-02 -1.14006452e-01 -3.70571092e-02
 -1.90816313e-01 -2.26512045e-01 -9.55878496e-02  2.96612144e-01
 -2.76000768e-01  2.17412606e-01 -2.03085333e-01  1.83585703e-01
 -1.15090504e-01 -4.37872291e-01  1.05255164e-01 -3.05626430e-02
  9.70385969e-02 -1.43983483e-01 -2.55181849e-01  2.71395296e-01
  2.80376166e-01  1.59421697e-01 -1.36573330e-01  4.01066802e-02
  2.81443819e-04  2.13640317e-01  8.84336084e-02  1.66708902e-01
 -1.93504170e-01 -7.67776445e-02 -3.19383740e-02  1.48720145e-01
 -1.46350801e-01 -1.09270968e-01 -3.44773591e-01 -1.06616437e-01
  1.94076970e-02  7.77456984e-02  5.33628650e-03 -5.99727444e-02
 -1.52593940e-01  9.16588753e-02  2.01847494e-01 -8.74551684e-02
 -1.83678806e-01 -1.56403691e-01  6.65663332e-02 -1.04544684e-01
 -1.81038409e-01 -1.81280643e-01  6.36781305e-02 -5.61845377e-02
  2.21340090e-01 -1.43999949e-01 -1.76854990e-02 -1.75785366e-03
 -3.85234654e-02 -1.13906682e-01 -8.09460357e-02  1.88174099e-02
  2.48178780e-01  3.09696883e-01  1.27155185e-01 -1.54227555e-01
  2.39491731e-01 -1.02622360e-01 -9.36885774e-02 -1.04179919e-01
 -1.49535358e-01 -2.31511425e-02  3.79234506e-03 -2.81744748e-02
 -1.74232796e-02  3.73025864e-01  1.75088465e-01  1.70635954e-02
  1.20469332e-01 -3.70519042e-01 -7.05209076e-02 -5.00246882e-03
 -1.14897698e-01 -8.97048041e-02  4.49689068e-02  2.48063393e-02
  2.67093420e-01 -2.87076473e-01  3.00424337e-01  5.32453656e-01
 -1.13903359e-01  2.74626613e-01  3.62139016e-01  1.46679208e-01
  1.30089507e-01  3.19303095e-01  1.12854175e-01 -8.10512602e-02
 -1.08861811e-01 -2.18914568e-01 -1.45973206e-01 -5.51782213e-02
  8.59216005e-02 -1.02672011e-01  1.53504312e-01 -1.22124497e-02
 -1.56441703e-03 -1.41393542e-02  1.89884692e-01 -1.40673518e-01
  2.41325185e-01  9.06450897e-02  3.14108282e-02 -2.39497665e-02
  9.85758752e-02  1.68001264e-01 -5.53757884e-02  4.21082169e-01
 -1.82543695e-01 -1.99180886e-01  9.01800320e-02  1.52132899e-01
  2.35726520e-01  1.17406502e-01 -1.39790013e-01 -6.35187328e-02
  6.69585019e-02  6.35293648e-02  1.09127983e-02  2.73043245e-01
 -1.74490184e-01  1.09435990e-01  1.32856578e-01  5.76425716e-02
 -8.47587883e-02 -1.03225529e-01 -3.17273647e-01  2.25744769e-02
 -3.25490594e-01  2.25318760e-01 -1.98502958e-01 -2.71599889e-01
  9.28978473e-02  1.35222003e-01 -3.11169714e-01  1.03855535e-01
 -1.22483701e-01  2.24740192e-01 -5.76587506e-02  1.11354314e-01
  1.01122111e-01  4.15479511e-01  1.53681517e-01  6.30731210e-02
  4.25394997e-02 -1.11949876e-01  1.87533379e-01 -2.60234863e-01
 -8.28067660e-02  1.83114916e-01 -2.14467689e-01 -1.95599616e-01
  1.57414272e-01  1.76625669e-01 -4.89416629e-01 -2.46632636e-01
  2.41951376e-01  1.94165543e-01 -9.88123268e-02 -1.72828168e-01
  2.04226762e-01 -1.73461828e-02  2.54010916e-01 -1.29214332e-01
  4.53538895e-01 -5.30909956e-01 -9.38145369e-02  4.66747396e-03
  1.98089689e-01  2.25693822e-01  4.79107276e-02 -4.46712524e-02
 -1.66754842e-01 -2.16373205e-02  3.81187201e-01  3.48313265e-02
 -1.70607060e-01  5.17953001e-03 -3.93174231e-01 -3.27356979e-02
  4.87801939e-01  1.51132286e-01 -1.53606966e-01  8.92931819e-02
  2.93194324e-01  5.59263825e-02  2.92591378e-02  5.57129569e-02
 -7.28783682e-02  1.60576016e-01  1.32337183e-01  2.77216360e-02
 -6.16828352e-02 -2.43263572e-01  6.42930120e-02 -2.07174718e-01
 -5.03952384e-01  3.67906317e-02  2.33333930e-01 -2.38893777e-01
 -2.05527879e-02 -1.29467785e-01 -1.88276246e-01  1.57539546e-01
 -2.19763637e-01 -3.48138660e-02 -2.95056581e-01  2.07512781e-01
  1.50521919e-01 -6.02083653e-02  1.75340086e-01 -2.76460737e-01
 -1.45017594e-01 -8.89611244e-02 -3.26723099e-01  2.59423941e-01
  7.79010132e-02  2.89296031e-01 -1.91149637e-02  9.32411402e-02
  5.80671191e-01  2.46684641e-01  2.92365134e-01 -9.67887044e-02
 -1.06043108e-01 -7.87050948e-02 -1.98408753e-01 -1.97825748e-02
 -1.79767862e-01 -3.71133685e-01  1.24995001e-01 -9.29377377e-02
  4.81367037e-02  6.72955960e-02 -1.88787118e-01 -1.80454940e-01
 -1.36425376e-01  2.54837930e-01 -1.70504004e-01 -1.40406758e-01
  1.12918943e-01  2.09789887e-01  3.10478538e-01  1.10735282e-01
  8.05580169e-02  1.04555160e-01  1.31188929e-01  6.36626780e-02
  1.36716276e-01  3.51099849e-01  9.23860744e-02  6.91635609e-01
  2.07920313e-01  2.34309718e-01 -9.87941921e-02  2.16683388e-01
 -2.55670726e-01 -1.73094958e-01 -5.48608229e-02 -2.58747578e-01
  2.47387081e-01 -3.40893686e-01  2.73025572e-01 -7.58036971e-02
  3.02730948e-01 -1.49215609e-01 -8.01334977e-02  1.47413850e-01
  3.84398438e-02  3.33183765e-01 -4.35287714e-01  1.32070079e-01
  1.69255361e-01 -8.83860737e-02 -6.02604523e-02 -6.43203735e-01
 -2.35313430e-01  6.71001077e-02 -1.53876752e-01  5.92858903e-02
  1.02442496e-01  3.77238728e-02 -7.42831826e-02  8.33756849e-02
  1.93918616e-01 -1.53037701e-02  1.90070063e-01 -6.48450702e-02
  2.72147786e-02  2.64190733e-01  3.26365888e-01 -4.28564727e-01
 -2.41580844e-01 -3.56762670e-02  3.12675595e-01  7.05976132e-03
  3.77770543e-01 -4.20775324e-01  1.12221003e-01  9.85321775e-02
  7.47786686e-02  3.37667137e-01  1.33580625e-01  2.54157353e-02
 -3.53094548e-01  3.18650007e-01  1.02021262e-01 -7.68143684e-03
  1.19096108e-01  4.85424921e-02 -4.32948381e-01 -3.73106040e-02
  1.26090854e-01 -6.69387206e-02  7.64896199e-02  1.77918747e-03
 -1.45526126e-01  2.73318775e-03 -2.12578416e-01 -2.39944905e-01
 -2.38897987e-02  2.41509169e-01 -1.50954992e-01 -7.33099729e-02
 -2.05457598e-01  1.65817469e-01 -2.05034226e-01 -3.64919871e-01
 -2.10622624e-02 -2.52754092e-01 -1.14705473e-01 -3.75357270e-01
 -1.08367831e-01 -2.90185690e-01  6.78399652e-02  2.29563206e-01
 -8.36019665e-02  4.61127907e-02 -9.53243747e-02  1.10677175e-01
 -1.67674631e-01  1.69670247e-02  1.64489493e-01  4.82434869e-01
 -1.38890594e-01 -1.81086212e-01  6.63169473e-02  4.58431125e-01
 -4.00408432e-02 -3.16767432e-02 -2.80439556e-01 -1.19176954e-01
  1.37955293e-01 -3.38618234e-02 -2.48312876e-01 -1.61998495e-01
 -3.97910587e-02  6.60217047e-01 -3.23145315e-02  3.71846825e-01
 -2.02944443e-01  9.91924033e-02  4.64179873e-01 -4.40174162e-01
 -1.04765192e-01  1.30142555e-01 -9.87939537e-03 -7.54335970e-02
 -1.97967172e-01  8.64318311e-02 -4.70555797e-02  6.60586506e-02]"
ModuleNotFoundError: No module named 'resource' stat:awaiting response type:bug TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Python test code
`import tensorflow_datasets as tfds`

Produces
ModuleNotFoundError: No module named 'resource'

### Standalone code to reproduce the issue

```shell
Python test code
import tensorflow_datasets as tfds
```


### Relevant log output

```shell
c:\xxxx\Devel Files\Other\Work\Python\image_class_transferlearning>python64.bat test.py
Traceback (most recent call last):
  File ""c:\xxxx\Devel Files\Other\Work\Python\image_class_transferlearning\test.py"", line 1, in <module>
    import tensorflow_datasets as tfds
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\__init__.py"", line 43, in <module>
    import tensorflow_datasets.core.logging as _tfds_logging
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\__init__.py"", line 22, in <module>
    from tensorflow_datasets.core import community
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\community\__init__.py"", line 18, in <module>
    from tensorflow_datasets.core.community.huggingface_wrapper import mock_builtin_to_use_gfile
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\community\huggingface_wrapper.py"", line 31, in <module>
    from tensorflow_datasets.core import dataset_builder
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\dataset_builder.py"", line 44, in <module>
    from tensorflow_datasets.core import split_builder as split_builder_lib
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\split_builder.py"", line 37, in <module>
    from tensorflow_datasets.core import writer as writer_lib
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\writer.py"", line 33, in <module>
    from tensorflow_datasets.core import shuffle
  File ""C:\Users\xxxx\AppData\Local\Programs\Python\Python311\Lib\site-packages\tensorflow_datasets\core\shuffle.py"", line 20, in <module>
    import resource
ModuleNotFoundError: No module named 'resource'
```
",True,"[-3.20197225e-01 -3.21713775e-01 -1.30290061e-01  1.63468540e-01
  3.45401525e-01 -3.10960919e-01 -3.11085097e-02  1.93279654e-01
 -3.66277397e-01 -3.10277402e-01  4.77983207e-02 -1.36959970e-01
 -4.47355956e-02  8.48196596e-02 -1.33537278e-01  3.50974262e-01
 -1.81787342e-01 -2.13241819e-02  1.18746564e-01  1.47011384e-01
 -2.44409651e-01 -1.01830080e-01 -9.28966627e-02  1.69924513e-01
  3.47141683e-01  1.32603049e-01 -4.10176188e-01 -7.52072930e-02
 -1.03961527e-01  3.64110291e-01  3.29208434e-01  1.42202690e-01
 -1.64533496e-01  1.74557030e-01  1.26749277e-01  3.02779675e-01
 -3.06407750e-01 -1.20304286e-01 -2.92982012e-01 -1.28210895e-03
 -1.04251727e-02  1.78900063e-01  2.22471565e-01 -1.69576883e-01
  1.58461779e-01 -2.74599314e-01 -2.45987512e-02 -9.30767134e-02
 -4.15332876e-02 -1.00679740e-01 -1.12401638e-02 -1.86199956e-02
 -4.48972285e-01 -5.67369103e-01 -6.22704178e-02  6.69484399e-03
  1.14132784e-01  7.25563318e-02 -5.82670830e-02  2.57908046e-01
  8.04281384e-02 -1.01014830e-01  5.69807738e-02 -1.28283173e-01
  1.39034331e-01  3.90798487e-02  2.26210907e-01  4.68690917e-02
  5.71892262e-01 -2.84362674e-01  1.96744740e-01 -2.89231278e-02
 -3.60290140e-01  5.05565479e-02  6.77480176e-02  1.14696831e-01
  1.32790031e-02  1.53791726e-01  2.96946943e-01 -2.29865223e-01
 -1.23591356e-01 -1.80791527e-01  3.46924439e-02 -8.35065991e-02
  2.03135669e-01  7.11608529e-02  4.78262037e-01  1.29095063e-01
  4.89504933e-01 -6.11563176e-02  4.86209780e-01  3.76126558e-01
  4.62249033e-02  1.39380470e-01  2.53482163e-01  1.04697526e-01
  3.03729355e-01  2.20581472e-01 -8.54701996e-02 -1.00108564e-01
  3.83817405e-02 -4.19548512e-01 -1.28265359e-02  6.46881089e-02
  1.13091096e-02 -1.89397871e-01  2.81327277e-01 -1.34322584e-01
  2.36702725e-01 -1.07064441e-01  1.86292157e-01  4.33341525e-02
  6.29630834e-02 -1.79422796e-01 -9.54342932e-02 -1.91756189e-01
 -9.73865092e-02  6.89011440e-02 -1.15435943e-01  9.02665019e-01
  1.55475423e-01  1.22949623e-01  1.27256185e-01 -2.48902291e-03
  3.46681714e-01 -2.35531107e-03 -1.75390393e-02  4.21666726e-02
  8.72580558e-02 -9.06782672e-02  3.03938448e-01  5.11765480e-02
 -2.08209455e-01  1.14427596e-01 -1.15753070e-01  9.63836014e-02
 -1.05170250e-01 -3.61137450e-01 -2.47427419e-01 -2.72479922e-01
 -2.67548859e-01  3.89462225e-02 -6.10958487e-02 -5.89935422e-01
  1.37199894e-01  5.76034710e-02 -1.28173292e-01  2.44155630e-01
 -1.84335113e-01 -9.97723546e-03  5.07185943e-02  1.78419948e-01
 -1.97071418e-01  5.12379646e-01  1.57678708e-01  2.70998240e-01
  3.08333635e-01  1.21896520e-01  1.63643390e-01 -6.11000538e-01
  3.74400765e-02  4.51872170e-01 -1.17836393e-01 -3.75308514e-01
 -2.31061839e-02  7.90211409e-02 -4.49732184e-01 -2.05522060e-01
 -1.35744750e-01  3.60856622e-01 -2.03424364e-01 -4.72351909e-02
  1.92526169e-02  1.32652763e-02  2.53825802e-02 -1.20136917e-01
  2.14150757e-01 -4.73117322e-01  3.14951874e-04  3.58161032e-01
  7.82802925e-02  2.38702029e-01 -1.26384437e-01  2.04572663e-01
 -9.05735493e-02  1.69394433e-01  4.02918160e-02  2.54997388e-02
 -5.18699288e-02 -8.14310685e-02 -4.89644796e-01 -1.56325355e-01
  4.62228000e-01 -1.41447470e-01  6.61920384e-02  3.82178389e-02
  2.19548583e-01 -2.37854868e-01 -3.34354676e-03  4.08180803e-02
 -1.50366724e-01  1.46038413e-01  1.02722310e-02 -1.28910560e-02
  2.51283616e-01 -2.18528420e-01 -1.37802005e-01 -3.77397060e-01
 -2.80205548e-01 -4.67427820e-03  1.19699305e-02 -5.25060773e-01
  2.08300501e-01 -1.80829793e-01 -1.32059678e-01  2.93895245e-01
  8.05876255e-02  2.59267390e-01 -1.99458487e-02 -8.92859474e-02
 -1.60100043e-01 -1.75413817e-01 -1.39122963e-01 -4.18977201e-01
  9.28209163e-03  2.21132785e-01 -2.60388792e-01 -1.06102273e-01
 -1.90633666e-02  1.56353444e-01 -6.51065558e-02  2.90126354e-02
  3.89605671e-01  3.69117618e-01  2.86283672e-01 -1.21509530e-01
 -9.57515985e-02 -4.91794422e-02 -1.07734084e-01  1.63222313e-01
 -3.42864752e-01 -6.69262856e-02 -1.09911002e-01  4.47470788e-03
  1.57086134e-01  3.86042356e-01 -9.97902602e-02 -4.25871849e-01
 -4.85572577e-01  1.60292462e-01 -2.83662945e-01  3.47869843e-02
  3.58485997e-01  9.61740986e-02  6.04680300e-01  3.81874859e-01
  1.15018159e-01  8.01543072e-02  1.63784176e-01 -2.73531824e-01
  4.16592240e-01 -7.46471882e-02  1.11380011e-01  1.67177662e-01
  1.97663933e-01  5.36576845e-02 -3.53296459e-01  4.79728281e-01
  1.97919577e-01 -1.73561603e-01  1.42407715e-01 -4.30407763e-01
  4.63809669e-01 -2.55062789e-01  5.96620329e-03 -6.00428134e-02
  2.47588977e-01 -1.77747533e-02  5.64810541e-03 -5.90616465e-02
  3.82995158e-02  2.56576836e-01 -3.09023201e-01 -6.29557949e-03
  8.19734931e-02 -3.04229021e-01 -1.39924735e-01 -3.04224133e-01
 -1.80255160e-01  2.62210995e-01 -2.71737278e-01  2.30831429e-01
  4.06173728e-02 -2.07887694e-01 -2.21012205e-01  1.24889567e-01
  4.51899767e-02 -1.81852922e-01 -5.43808341e-02  4.13129449e-01
 -2.18177497e-01 -5.42609654e-02  2.48200282e-01 -3.42320234e-01
  3.04273311e-02 -1.42327011e-01  3.45013678e-01  1.17386505e-03
  4.17471528e-01 -6.36976719e-01  1.76671028e-01 -1.78347342e-03
 -1.61833987e-01  4.38446760e-01  2.80077551e-02 -1.45241573e-01
 -2.96386838e-01  7.20133662e-01  3.01447570e-01 -1.52770758e-01
  1.10864580e-01 -3.03480327e-02 -3.20185393e-01  9.35112312e-02
  2.15897158e-01  1.44855693e-01 -2.49236420e-01 -2.64000684e-01
  1.14180878e-01  2.77061433e-01 -1.28760204e-01  4.21509892e-02
 -1.68841571e-01 -5.03837392e-02 -6.53772801e-02 -2.28485346e-01
 -4.18542445e-01  1.40581250e-01 -5.65775819e-02 -2.69107223e-01
 -1.55986240e-03  6.37926757e-02 -1.49107212e-03 -2.13570997e-01
  3.91383246e-02 -3.62180233e-01  2.33069599e-01  6.28413081e-01
 -3.96019183e-02 -6.85215965e-02  1.34393767e-01 -1.42740905e-02
 -4.84140754e-01  1.54874790e-02 -3.61881196e-01  4.34192359e-01
 -1.89678296e-02 -1.96499988e-01  4.52059209e-01  2.94898748e-01
 -1.13997534e-01  3.03300433e-02 -2.72118747e-01  8.58775973e-02
  3.49482775e-01 -1.86159208e-01 -3.19328785e-01 -2.22167283e-01
 -4.59929407e-02  6.30957857e-02 -2.49646276e-01  7.43549913e-02
 -2.91303456e-01  3.45806003e-01  4.29881454e-01 -3.21287096e-01
 -1.78077221e-01  1.66461974e-01  1.41155362e-01 -1.59397736e-01
  3.99210081e-02 -2.19258163e-02  2.43975773e-01 -1.40827790e-01]"
tf.math.cumsum weird behaviour  stat:awaiting response type:bug stale comp:ops TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

ython 3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

T4

### Current behavior?

tf.math.cumsum produce weird results on float32 and float64 when device is GPU:
- results are different than on CPU
- results do not match manual cumsum calculation 
- cumsum value can change when adding 0 (last two values in log output). 
- float64 has similar problems as well
- tested on multiple GPUs: T4, GTX1080Ti and tensorflow versions: 2.13, 2.8.4

### Standalone code to reproduce the issue

```shell
with tf.device('/GPU'):
  arr = tf.constant(
        [  0.  ,     0.  ,     0.  ,     0.  ,  9759.35,  9759.35,
          9759.35,  9759.35,  9759.35,  9759.35,  9762.03,  9700.78,
          9700.78,  9700.78,  9700.78,  9700.78,  9700.78,  9660.83,
          9600.46,  9600.46,  9600.46,  9600.46,  9600.46,  9600.46,
          9715.65,  9742.31,  9742.31,  9742.31,  9742.31,  9742.31,
          9742.31,  9774.32,  9750.2 ,  9750.2 ,  9750.2 ,  9750.2 ,
          9750.2 ,  9750.2 ,  9796.23,  9824.72,  9824.72,  9824.72,
          9824.72,  9824.72,  9824.72, 11737.25,  9759.23,  9759.23,
          9759.23,  9759.23,  9759.23,  9759.23,  9551.41,  9551.47,
          9551.47,  9551.47,  9551.47,  9551.47,  9551.47,  9723.25,
          9693.61,  9693.61,  9693.61,  9693.61,  9693.61,  9693.61,
          9629.34,  9658.78,  9658.78,  9658.78,  9658.78,  9658.78,
          9658.78,  9986.66, 10005.04, 10005.04, 10005.04, 10005.04,
          10005.04, 10005.04,  9977.68,  9955.32,  9955.32,  9955.32,
          9955.32,  9955.32,  9955.32,  9875.19,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.  ,
              0.  ,     0.  ,     0.  ,     0.  ,     0.  ,     0.], dtype=tf.float32)

  manual_cumsum = []
  for val in arr:
    if len(manual_cumsum) == 0:
      manual_cumsum.append(val)
    else:
       manual_cumsum.append(manual_cumsum[-1] + val)

  tf_cumsum = tf.math.cumsum(arr)

  for index in range(arr.shape[0]):
    print(""arr_value:"", arr[index].numpy(), ""tensor - manual cumsum diff:"", (tf_cumsum[index] - manual_cumsum[index]).numpy())
```
```


### Relevant log output

```shell
arr_value: 0.0 tensor - manual cumsum diff: 0.0
arr_value: 0.0 tensor - manual cumsum diff: 0.0
arr_value: 0.0 tensor - manual cumsum diff: 0.0
arr_value: 0.0 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9759.35 tensor - manual cumsum diff: 0.0
arr_value: 9762.03 tensor - manual cumsum diff: 0.0
arr_value: 9700.78 tensor - manual cumsum diff: 0.0
arr_value: 9700.78 tensor - manual cumsum diff: -0.0078125
arr_value: 9700.78 tensor - manual cumsum diff: -0.0078125
arr_value: 9700.78 tensor - manual cumsum diff: -0.0078125
arr_value: 9700.78 tensor - manual cumsum diff: -0.0078125
arr_value: 9700.78 tensor - manual cumsum diff: -0.0078125
arr_value: 9660.83 tensor - manual cumsum diff: -0.015625
arr_value: 9600.46 tensor - manual cumsum diff: -0.015625
arr_value: 9600.46 tensor - manual cumsum diff: -0.015625
arr_value: 9600.46 tensor - manual cumsum diff: 0.0
arr_value: 9600.46 tensor - manual cumsum diff: 0.0
arr_value: 9600.46 tensor - manual cumsum diff: 0.0
arr_value: 9600.46 tensor - manual cumsum diff: 0.0
arr_value: 9715.65 tensor - manual cumsum diff: 0.03125
arr_value: 9742.31 tensor - manual cumsum diff: 0.03125
arr_value: 9742.31 tensor - manual cumsum diff: 0.03125
arr_value: 9742.31 tensor - manual cumsum diff: 0.03125
arr_value: 9742.31 tensor - manual cumsum diff: 0.015625
arr_value: 9742.31 tensor - manual cumsum diff: 0.015625
arr_value: 9742.31 tensor - manual cumsum diff: 0.015625
arr_value: 9774.32 tensor - manual cumsum diff: 0.03125
arr_value: 9750.2 tensor - manual cumsum diff: 0.03125
arr_value: 9750.2 tensor - manual cumsum diff: 0.03125
arr_value: 9750.2 tensor - manual cumsum diff: 0.03125
arr_value: 9750.2 tensor - manual cumsum diff: 0.03125
arr_value: 9750.2 tensor - manual cumsum diff: 0.09375
arr_value: 9750.2 tensor - manual cumsum diff: 0.09375
arr_value: 9796.23 tensor - manual cumsum diff: 0.09375
arr_value: 9824.72 tensor - manual cumsum diff: 0.09375
arr_value: 9824.72 tensor - manual cumsum diff: 0.125
arr_value: 9824.72 tensor - manual cumsum diff: 0.125
arr_value: 9824.72 tensor - manual cumsum diff: 0.125
arr_value: 9824.72 tensor - manual cumsum diff: 0.125
arr_value: 9824.72 tensor - manual cumsum diff: 0.125
arr_value: 11737.25 tensor - manual cumsum diff: 0.125
arr_value: 9759.23 tensor - manual cumsum diff: 0.125
arr_value: 9759.23 tensor - manual cumsum diff: 0.125
arr_value: 9759.23 tensor - manual cumsum diff: 0.15625
arr_value: 9759.23 tensor - manual cumsum diff: 0.15625
arr_value: 9759.23 tensor - manual cumsum diff: 0.15625
arr_value: 9759.23 tensor - manual cumsum diff: 0.15625
arr_value: 9551.41 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9551.47 tensor - manual cumsum diff: 0.1875
arr_value: 9723.25 tensor - manual cumsum diff: 0.1875
arr_value: 9693.61 tensor - manual cumsum diff: 0.1875
arr_value: 9693.61 tensor - manual cumsum diff: 0.1875
arr_value: 9693.61 tensor - manual cumsum diff: 0.1875
arr_value: 9693.61 tensor - manual cumsum diff: 0.1875
arr_value: 9693.61 tensor - manual cumsum diff: 0.125
arr_value: 9693.61 tensor - manual cumsum diff: 0.125
arr_value: 9629.34 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.125
arr_value: 9658.78 tensor - manual cumsum diff: 0.25
arr_value: 9986.66 tensor - manual cumsum diff: 0.25
arr_value: 10005.04 tensor - manual cumsum diff: 0.25
arr_value: 10005.04 tensor - manual cumsum diff: 0.25
arr_value: 10005.04 tensor - manual cumsum diff: 0.1875
arr_value: 10005.04 tensor - manual cumsum diff: 0.1875
arr_value: 10005.04 tensor - manual cumsum diff: 0.1875
arr_value: 10005.04 tensor - manual cumsum diff: 0.1875
arr_value: 9977.68 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9955.32 tensor - manual cumsum diff: 0.1875
arr_value: 9875.19 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.1875
arr_value: 0.0 tensor - manual cumsum diff: 0.25
arr_value: 0.0 tensor - manual cumsum diff: 0.25
```
```
",True,"[-0.52379584 -0.29607397 -0.07147279 -0.09704842  0.24347728 -0.37524593
  0.06003782  0.21549383 -0.25160953 -0.36848605  0.00372235  0.01011767
 -0.12044729  0.15098965 -0.1568799   0.17894807 -0.27277672 -0.35552028
  0.06437837  0.02724226 -0.22844586 -0.18375424 -0.25888175  0.16491987
  0.26736984  0.277318   -0.41936934  0.16124718  0.04759326  0.33421326
  0.19648074  0.19161531 -0.11865691  0.09136972  0.04619346  0.20033608
 -0.25397205 -0.04778596 -0.4220787   0.04054875 -0.1312092  -0.06306747
  0.01840461 -0.16974533  0.06184273  0.0268295   0.13742797 -0.10007216
 -0.09036075 -0.13630424 -0.03008201  0.16321631 -0.44708136 -0.19325462
  0.11116625 -0.05333012  0.10550695 -0.15936103 -0.04119008  0.11039761
 -0.01948601  0.02776593 -0.04970807  0.01440655  0.34353632  0.11743841
  0.46201333 -0.00367413  0.1758291  -0.21077436  0.16537187 -0.07800241
 -0.41196537  0.17519176  0.05254241 -0.03004745 -0.20088574  0.14526069
  0.26805282 -0.20093799  0.0899981  -0.32200933 -0.20360112 -0.2279959
 -0.01731461 -0.30883825  0.32314593  0.034351    0.62792015 -0.31207865
  0.49495667  0.5363643  -0.06047388  0.21598011  0.66349745  0.15225703
  0.04306274  0.24856341 -0.06266024  0.04670938 -0.23023687 -0.14985117
 -0.11608516  0.06900778 -0.02169355 -0.11947782  0.05735634 -0.02732296
  0.12561247  0.16971928  0.07706818  0.15077503  0.14063352 -0.14964195
 -0.17058346  0.0112332   0.01976564 -0.03753703  0.14675957  0.4726148
  0.16946806 -0.09896257 -0.16305733  0.23285918  0.46231052  0.00461471
 -0.04337896  0.110256    0.07531092 -0.13822171  0.21096563  0.06836396
  0.01272052  0.24200386  0.10578615  0.05144146 -0.09199814  0.01673516
 -0.28483644 -0.18630753 -0.36116958  0.19234848 -0.20615634 -0.47869802
  0.16667128  0.10243972 -0.3313164   0.5248916  -0.19778568  0.10252647
  0.038202    0.01414342 -0.23314053  0.38839447  0.02441468 -0.09762719
  0.25696796 -0.0666341   0.17650953 -0.3799755   0.0916034   0.3417747
  0.0082058  -0.21096994  0.3638263   0.28925762 -0.4343599  -0.1460052
 -0.03910458  0.38745174 -0.11273748 -0.12120593 -0.0188379  -0.00452535
  0.32578558 -0.27935773  0.27857444 -0.35572731 -0.07445043  0.30363598
  0.18453698  0.15553075  0.07431477  0.17806312  0.01377871 -0.03572055
  0.08896659  0.2207121  -0.4650715  -0.05552586 -0.5169512  -0.15009192
  0.37075555  0.11325777 -0.04072361 -0.02089435  0.39562348 -0.10483225
  0.09585781  0.20261425 -0.17579386 -0.217789    0.07809763 -0.01915678
  0.13418567 -0.2253635  -0.01346808 -0.56002927 -0.24775603  0.12660773
 -0.05897333 -0.3490429   0.12312768 -0.13113263 -0.37316677  0.44103962
 -0.04667606 -0.03587781 -0.05960103  0.20187306  0.32764068 -0.14719108
  0.18225527 -0.43860415 -0.3622793   0.2978038  -0.62240094  0.2320486
 -0.12873414  0.10046807  0.17225005  0.19228394  0.2898451   0.20514305
  0.53350586 -0.12235149 -0.16645259 -0.1280533  -0.0805614   0.01943214
 -0.33324385 -0.21555917 -0.00249888 -0.18076392  0.41125208  0.38127464
 -0.12743634 -0.085711   -0.3124299   0.38689873 -0.46003672  0.12677981
  0.21047154  0.03570746  0.50826377  0.24278831  0.2637813   0.13231337
  0.23453447 -0.27665132  0.11405695  0.24097872  0.24371096  0.35373008
  0.2138105   0.21024433 -0.19722825  0.47430158 -0.18051966 -0.01198302
  0.1078303  -0.48479396  0.8919862  -0.305361    0.18469869 -0.13316762
  0.33109954 -0.24879228 -0.08480462  0.18930422 -0.09940447  0.33805767
 -0.3569017   0.24524775  0.03658234 -0.12074085 -0.06668957 -0.67484415
 -0.1994349  -0.08451639 -0.36267912 -0.03016409 -0.06941693  0.03812423
 -0.31646645  0.06137939  0.03611271 -0.3816401   0.15423986  0.06655204
 -0.11636506  0.13858847  0.43711036 -0.22189629 -0.19762424  0.06671411
  0.4176597   0.16257888  0.21696107 -0.47901577  0.21993402 -0.13078883
 -0.15698458  0.41955    -0.1170662   0.16280396 -0.40244013  0.47282496
  0.1983397  -0.00207137  0.17373534 -0.3290672  -0.3855244   0.09807727
  0.2863608  -0.13477993 -0.00855502 -0.19538361  0.10772887  0.15124452
 -0.0419054  -0.08122224 -0.22753164 -0.06780569 -0.14128095 -0.12749049
 -0.4515339   0.12104339 -0.0350652  -0.38042003 -0.26640832 -0.24372828
 -0.2703449  -0.03925496 -0.06220285 -0.3370048   0.3718057   0.36356264
  0.03699581  0.17236045  0.02723333  0.10897045 -0.22348806  0.08532918
 -0.15766358  0.3287319   0.00793275 -0.09993038  0.34129298  0.01717652
 -0.28095883  0.09274318 -0.2852261   0.1586121   0.09691357 -0.16833225
 -0.26446313 -0.16200268  0.1246963   0.2806388   0.02862844  0.3164612
 -0.3818208   0.14617497  0.643456   -0.6819809  -0.1500919   0.06005074
  0.2716705  -0.17565478 -0.16925387 -0.18072295  0.214468    0.0330263 ]"
AttributeError: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface' stat:awaiting response type:bug stale comp:core TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

2.13.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

OS : Ubuntu 22.04.3
Software: Pycharm
files: LogisticRegression_withTensorflow.ipynb

I dont know what happen, it just work yesterday
when I run this code:
`model.fit(X_train, y_train, epochs=150)`


### Standalone code to reproduce the issue

```shell
# import library
import numpy as np
import tensorflow as tf
from tensorflow.python.keras import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.losses import BinaryCrossentropy
print(tf.__version__)
# 2.13.0
# import data
X = []
y = []
with open('../../Part1/Week2/data/ex2data1.txt') as file:
# with open('/home/wisdom/vs_code_repository/Python/AndrewNG_ML/Part1/Week2/data/ex2data1.txt', 'r') as file:
    for lines in file:
        colums = lines.strip().split(',')
        X.append([float(colums[0]), float(colums[1])])
        y.append(float(colums[2]))
X = np.array(X)
y = np.array(y)
# normalization
X_mean = np.mean(X, axis=0)
X_max = np.max(X, axis=0)
X_min = np.min(X, axis=0)
X = (X - X_mean) / (X_max - X_min)

# split data into training_set and testing_set
X_train = X[:80]
y_train = y[:80]
X_test = X[80:]
y_test = y[80:]
model = Sequential([Dense(units=32, activation='sigmoid'),
                    Dense(units=16, activation='sigmoid'),
                    Dense(units=1, activation='sigmoid')])
model.compile(loss=BinaryCrossentropy())
model.fit(X_train, y_train, epochs=150) # bug appears this line
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)
Cell In[13], line 1
----> 1 model.fit(X_train, y_train, epochs=150)

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/training.py:1138, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1132   self._cluster_coordinator = cluster_coordinator.ClusterCoordinator(
   1133       self.distribute_strategy)
   1135 with self.distribute_strategy.scope(), \
   1136      training_utils.RespectCompiledTrainableState(self):
   1137   # Creates a `tf.data.Dataset` and handles batch and epoch iteration.
-> 1138   data_handler = data_adapter.get_data_handler(
   1139       x=x,
   1140       y=y,
   1141       sample_weight=sample_weight,
   1142       batch_size=batch_size,
   1143       steps_per_epoch=steps_per_epoch,
   1144       initial_epoch=initial_epoch,
   1145       epochs=epochs,
   1146       shuffle=shuffle,
   1147       class_weight=class_weight,
   1148       max_queue_size=max_queue_size,
   1149       workers=workers,
   1150       use_multiprocessing=use_multiprocessing,
   1151       model=self,
   1152       steps_per_execution=self._steps_per_execution)
   1154   # Container that configures and calls `tf.keras.Callback`s.
   1155   if not isinstance(callbacks, callbacks_module.CallbackList):

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:1398, in get_data_handler(*args, **kwargs)
   1396 if getattr(kwargs[""model""], ""_cluster_coordinator"", None):
   1397   return _ClusterCoordinatorDataHandler(*args, **kwargs)
-> 1398 return DataHandler(*args, **kwargs)

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:1152, in DataHandler.__init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1149   self._steps_per_execution = steps_per_execution
   1150   self._steps_per_execution_value = steps_per_execution.numpy().item()
-> 1152 adapter_cls = select_data_adapter(x, y)
   1153 self._adapter = adapter_cls(
   1154     x,
   1155     y,
   (...)
   1164     distribution_strategy=distribute_lib.get_strategy(),
   1165     model=model)
   1167 strategy = distribute_lib.get_strategy()

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:988, in select_data_adapter(x, y)
    986 def select_data_adapter(x, y):
    987   """"""Selects a data adapter than can handle a given x and y.""""""
--> 988   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
    989   if not adapter_cls:
    990     # TODO(scottzhu): This should be a less implementation-specific error.
    991     raise ValueError(
    992         ""Failed to find data adapter that can handle ""
    993         ""input: {}, {}"".format(
    994             _type_name(x), _type_name(y)))

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:988, in <listcomp>(.0)
    986 def select_data_adapter(x, y):
    987   """"""Selects a data adapter than can handle a given x and y.""""""
--> 988   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
    989   if not adapter_cls:
    990     # TODO(scottzhu): This should be a less implementation-specific error.
    991     raise ValueError(
    992         ""Failed to find data adapter that can handle ""
    993         ""input: {}, {}"".format(
    994             _type_name(x), _type_name(y)))

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:707, in DatasetAdapter.can_handle(x, y)
    704 @staticmethod
    705 def can_handle(x, y=None):
    706   return (isinstance(x, (data_types.DatasetV1, data_types.DatasetV2)) or
--> 707           _is_distributed_dataset(x))

File /usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py:1699, in _is_distributed_dataset(ds)
   1698 def _is_distributed_dataset(ds):
-> 1699   return isinstance(ds, input_lib.DistributedDatasetInterface)

AttributeError: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'
```
",True,"[-0.3049636  -0.6201409  -0.26023296  0.1861319   0.2539829  -0.34181783
 -0.06749883  0.04528433 -0.4518995  -0.401744   -0.00585827 -0.1092126
 -0.06318212  0.28367105 -0.1273991   0.38697645 -0.1753028  -0.20464726
  0.17923631 -0.00077238 -0.11663922  0.07550014 -0.10805978  0.2475534
  0.14885566  0.15662622 -0.23879501 -0.23774952 -0.02139059  0.26107514
  0.34451252  0.20143041 -0.28867513  0.15697576 -0.1164186   0.25982463
 -0.26397914 -0.16969548 -0.1686327  -0.05329573  0.06716405  0.25023314
  0.18092617  0.010451   -0.04041519 -0.15185486  0.00360686 -0.1034749
 -0.101686   -0.28808028  0.00622863 -0.08385856 -0.31618044 -0.33753163
 -0.14302981 -0.08685419  0.2399895  -0.01177491 -0.02221257  0.16399072
 -0.03536915  0.00678791  0.09784386  0.06543101  0.07380439  0.1389746
  0.33788273 -0.041981    0.5493475  -0.22800198  0.19059967 -0.02092228
 -0.36413905  0.02494516 -0.05154601  0.04190388 -0.08016524  0.12485065
  0.3223859  -0.12758172 -0.04001746 -0.03184802  0.06151695  0.03551447
  0.09749512 -0.05258124  0.32259732  0.21863088  0.32916215 -0.10979211
  0.6239332   0.43619326 -0.03938563  0.18173435  0.3061927   0.05482136
  0.1130182   0.15564401 -0.05255322 -0.29093236 -0.07555833 -0.41319782
 -0.2192711   0.04607626 -0.19472405 -0.10682916  0.17154753 -0.11189379
  0.06248626  0.04258996  0.14014107 -0.03877382  0.19492097 -0.14896026
 -0.07216859 -0.07983171 -0.1079711   0.07212945 -0.0136317   0.71928424
  0.20733982  0.06429288  0.11398646  0.25279376  0.5443632   0.10455127
  0.04203273 -0.0216555   0.12281542 -0.12517907  0.16525565  0.10614602
 -0.21306998  0.15420616  0.03997127  0.22479054 -0.24440417 -0.21963957
 -0.1924991  -0.01796225 -0.22648849  0.19876496 -0.29680958 -0.48557457
  0.07775183  0.20905285 -0.09448781  0.3569858  -0.17113411  0.09861331
 -0.00197325  0.20540184 -0.2607978   0.4343518   0.04777061  0.36787507
  0.30184674 -0.12267983  0.07820155 -0.54847157 -0.02152602  0.52615845
 -0.09995205 -0.17527139 -0.10929506  0.14557816 -0.39688033 -0.28166008
  0.02335273  0.22881988 -0.1807925  -0.12360096  0.07535279 -0.05905605
 -0.03590985 -0.06845901  0.1243654  -0.6705837   0.02114868  0.25762635
  0.13461047  0.3743354  -0.1006919   0.29173654 -0.00371764  0.03012329
  0.04754887 -0.07717571 -0.13011822 -0.03649434 -0.4249459  -0.16896366
  0.53845584 -0.20225227 -0.0657744   0.03674143  0.18231055  0.02464802
  0.07819979  0.09650347  0.01291737  0.00160675 -0.08031483  0.10856514
  0.11353385 -0.23552284 -0.01677902 -0.38815016 -0.36549214 -0.02915536
 -0.08621488 -0.5405033   0.1603849  -0.08992176 -0.33723336  0.2722561
  0.15936893  0.26532182 -0.16527705 -0.04831482 -0.02935845 -0.17448542
  0.11087778 -0.33865008 -0.15168558  0.12233151 -0.25367877  0.2772466
 -0.00987228  0.17728254 -0.04495137  0.24803175  0.48450798  0.40745753
  0.28639925 -0.16946195 -0.21429214  0.05863408 -0.07852594  0.2714865
 -0.43680522  0.0444784   0.0942817  -0.12203742  0.10114274  0.36812016
  0.00505657 -0.24834281 -0.43108085  0.09536239 -0.2920252  -0.07926069
  0.42744172  0.06845045  0.6582149   0.23261982  0.1420866  -0.03538039
  0.03730419 -0.30328864  0.17105365  0.09088151  0.1252414   0.5323653
 -0.00288384  0.18404958 -0.41202554  0.5644272   0.27109247 -0.25031728
  0.02254198 -0.501559    0.629208   -0.34244838  0.00343432 -0.06627506
  0.33044666 -0.05002476  0.08163875  0.15013567  0.05100729  0.3604161
 -0.2753336  -0.15018228  0.10749251 -0.2114503  -0.20664987 -0.5449004
 -0.23440574  0.17063498 -0.19204146  0.2039306   0.06854231  0.08277715
 -0.2114236   0.04346398  0.07039408 -0.19642502  0.15102743  0.3456579
  0.03632374  0.14426811  0.272588   -0.31135416 -0.12516418 -0.13146107
  0.4074765   0.03907165  0.4239794  -0.53689873  0.06961949 -0.09631824
  0.04138858  0.46468142  0.00487767 -0.13077636 -0.31948343  0.50313735
  0.28593805 -0.12953147  0.16806465 -0.20702729 -0.41079372  0.09409796
  0.20843855 -0.05181473  0.1418225  -0.23059624 -0.10349464  0.12492323
 -0.2192786  -0.07732846 -0.19537742 -0.01606547 -0.02325384 -0.09966802
 -0.54254526  0.15928923 -0.14420222 -0.24185479  0.09034355 -0.15301968
 -0.04130818 -0.34018248 -0.18883395 -0.22795948  0.44228786  0.47120598
 -0.18615273  0.0298659   0.01170471  0.09153722 -0.5255053  -0.04813485
 -0.18179058  0.40370786  0.06611526 -0.31223232  0.41594723  0.41598865
 -0.43472615  0.13116126 -0.23095152  0.01762859  0.21841332 -0.18790764
 -0.29251465 -0.1927186   0.03855804  0.3704952  -0.1723258   0.24758536
 -0.43435842  0.36338416  0.3821265  -0.30091843 -0.20415187  0.1703415
  0.08475386 -0.27062547 -0.09221728  0.03585584  0.33074784 -0.17495608]"
Unable to load TensorFlow saved model (AttributeError: '_UserObject' object has no attribute 'add_slot') stat:awaiting response type:bug comp:apis TF 2.7,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.7.0

### Custom code

Yes

### OS platform and distribution

Windows 10 Enterprise LTSC

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2/8

### GPU model and memory

Nvidia Geforce RTX 3090 24 GB

### Current behavior?

I have several trained TensorFlow models stored on my device in saved_model format. All the models have the same architecture and serve the same purpose, but they were created at different points in time. While I have no problems to load the newest models (created from 5th September 2022 onward) with the command `tensorflow.keras.models.load_model(filepath)`, older models that were created before this date raise the following exception when this method is called:

> AttributeError: '_UserObject' object has no attribute 'add_slot'

The same exception is also raised when trying to load these models with: `tensorflow.saved_model.load(filepath)`, which was one of the suggestions I found to solve the issue. Another possible solution that I took from the related issue #52091 was to use another TF version, I tried it with the older v2.6 and the newest Windows native version v.2.10, but the result was the same.

### Standalone code to reproduce the issue

[EDIT] I could reproduce the exception on Google Colab using the newest TF version. Please find the notebook here: [Model_Load_Problem.ipynb](https://colab.research.google.com/drive/1YyyU6kyn947JCuTHOrjRiDGQb2XBOueu?usp=sharing)


### Relevant log output

```shell
2023-09-25 11:36:20.048929: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-25 11:36:21.096591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18788 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:1a:00.0, compute capability: 8.6
Traceback (most recent call last):
  File ""C:\Users\icon\Desktop\Testing_Models\iCoNet\source\model.py"", line 699, in load_trained_model
    model = tf.keras.models.load_model(model_path)
  File ""C:\Users\icon\.conda\envs\iConNet\lib\site-packages\keras\utils\traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\Users\icon\.conda\envs\iConNet\lib\site-packages\tensorflow\python\saved_model\load.py"", line 466, in _load_nodes
    slot_variable = optimizer_object.add_slot(
AttributeError: '_UserObject' object has no attribute 'add_slot'
```
",True,"[-3.26685786e-01 -5.75748742e-01 -2.21648812e-01  1.29189968e-01
  2.49466434e-01 -2.85581112e-01  1.18990634e-02 -8.23887289e-02
 -3.24330688e-01 -3.18112701e-01  2.17419639e-01 -7.44737312e-02
 -2.14325160e-01  2.27709472e-01 -1.19617447e-01  3.33301187e-01
 -1.60567462e-01 -1.59024503e-02  1.50327533e-01  2.56131262e-01
 -1.14906378e-01 -3.08190167e-01 -1.82910711e-01  1.46357328e-01
  1.45716012e-01  3.48897278e-01 -2.09566623e-01 -1.42506331e-01
  6.17464967e-02  2.21367061e-01  3.42482030e-01 -3.00571732e-02
 -2.32498348e-03  2.41984539e-02  2.13155914e-02  3.91207188e-01
 -3.96463156e-01 -1.99352264e-01 -2.76630223e-01 -2.62802038e-02
  1.49839491e-01  7.05096349e-02  1.56510621e-01 -1.20552793e-01
  2.31279060e-03 -2.10082024e-01  8.60112086e-02 -1.84125677e-01
  1.11835366e-02 -2.29849219e-01  5.94082326e-02 -1.56590551e-01
 -5.14822841e-01 -3.94123435e-01 -3.13603967e-01 -6.37302473e-02
  2.89807916e-01 -1.56350225e-01 -1.49819911e-01  8.07159171e-02
  1.96555629e-02  1.27809823e-01  1.97019707e-02 -9.01729092e-02
  2.88124979e-02  3.07942890e-02  2.09126472e-01  5.31100407e-02
  6.05699301e-01 -3.00029516e-01  3.16867292e-01  8.44188258e-02
 -4.17286515e-01 -2.32229456e-02  5.80869354e-02  1.19730398e-01
  9.97763872e-02  1.62369370e-01  2.61204660e-01 -6.69866381e-03
  7.79897301e-03 -3.21416616e-01 -6.92918003e-02 -1.65133059e-01
  1.91880703e-01 -8.86420608e-02  3.31137478e-01  1.16582401e-01
  4.34919775e-01 -2.30820566e-01  5.08411348e-01  4.52665806e-01
 -8.70855823e-02 -1.34905549e-02  5.62124252e-01  2.09859133e-01
 -1.32262697e-02  2.33012289e-01  6.20730221e-04 -1.81904882e-01
 -2.14271378e-02 -2.15537041e-01 -4.10628095e-02  1.45216703e-01
  6.49167318e-03 -9.02090594e-02  9.84620377e-02 -1.67476416e-01
  1.10179573e-01 -4.55188490e-02  1.25526458e-01 -2.04231620e-01
  3.00508440e-01  6.70851022e-02  5.72696514e-02 -8.79635885e-02
 -2.06355482e-01 -1.34744402e-02 -1.92573726e-01  7.26710200e-01
  1.63085721e-02 -1.01871565e-01  2.75585234e-01  6.01566769e-03
  4.62372422e-01  1.11653730e-01 -9.08467323e-02 -6.17744774e-03
  8.12068954e-02 -2.25311160e-01  2.98298389e-01  1.62817597e-01
  3.10051702e-02  1.82769164e-01 -4.83263209e-02  4.79692407e-02
 -2.82871693e-01 -2.55961269e-01 -2.77071953e-01 -2.07497239e-01
 -1.81268215e-01  1.75875381e-01 -3.43615234e-01 -5.57641625e-01
  1.11891709e-01  1.37254596e-01 -4.51837964e-02  3.30894619e-01
 -9.87950489e-02 -3.81764397e-02 -1.66403964e-01  7.31871501e-02
 -5.94457090e-02  4.92118597e-01  1.68269813e-01  3.59859169e-01
  3.65814269e-01 -5.16058803e-02 -2.84165554e-02 -6.22681081e-01
 -2.52180118e-02  3.62970769e-01 -8.61525238e-02 -2.75954187e-01
  2.24207759e-01  2.13747218e-01 -3.62784117e-01 -2.56078571e-01
  2.52335250e-01  4.68877941e-01 -2.73841560e-01 -1.19990580e-01
 -1.00234754e-01 -5.74902259e-03  1.89474031e-01 -8.18524137e-03
  4.52501237e-01 -8.45059395e-01 -1.26966253e-01  2.71353602e-01
  1.41729087e-01  1.41069978e-01 -1.31614298e-01  7.47411996e-02
  1.35169262e-02 -2.05619931e-02  1.54143155e-01 -5.05751185e-02
 -1.42288417e-01 -6.99433777e-03 -3.53403747e-01 -1.23789579e-01
  6.16002321e-01 -7.19536170e-02  1.86424535e-02  1.04329564e-01
  1.57172471e-01 -1.35577321e-01 -2.64161043e-02  2.94309333e-02
 -1.68708563e-01 -5.26229255e-02 -1.65873319e-01 -3.92900370e-02
  1.23713419e-01 -3.16821218e-01 -3.87728475e-02 -3.79599631e-01
 -3.35436016e-01 -4.62156609e-02 -3.57116163e-02 -5.00543833e-01
  2.16498002e-01  8.43648463e-02 -3.13813448e-01  1.32906929e-01
  6.03891462e-02  7.91182369e-02 -9.97040346e-02  9.29682702e-02
  7.45270699e-02 -1.64189368e-01 -7.72731826e-02 -3.30265671e-01
 -3.00186276e-01  1.17471248e-01 -3.28388721e-01  4.86982837e-02
  1.83461756e-01  2.60451406e-01  1.74221903e-01  2.37873852e-01
  3.41967821e-01  2.38143221e-01  2.26460263e-01 -1.61744922e-01
 -2.32471451e-01 -7.72224665e-02 -1.35638416e-01  1.15977250e-01
 -6.23089373e-01 -3.19092214e-01  7.44095072e-03 -1.87151015e-01
  2.32395187e-01  3.53836566e-01  1.69134662e-02 -1.46497592e-01
 -4.05941486e-01  2.22251922e-01 -4.50215220e-01  7.29579180e-02
  5.43402314e-01  1.53668582e-01  5.42627573e-01  1.71386778e-01
  1.47295773e-01  1.06272742e-01  1.99409276e-01 -2.35154673e-01
  4.40020561e-01  1.91511527e-01  9.60176885e-02  3.47004890e-01
  1.85016990e-01  2.53520489e-01 -3.11495483e-01  4.93524611e-01
  2.02248052e-01 -7.18140453e-02  2.06526160e-01 -4.63614196e-01
  5.75481176e-01 -3.14022362e-01  2.92853564e-02 -1.47739708e-01
  2.88940936e-01  1.28912464e-01 -1.02993011e-01  1.28843635e-01
  2.67446369e-01  2.22146094e-01 -3.94837379e-01 -1.97423771e-02
 -3.81529629e-02 -3.10761273e-01 -7.55287558e-02 -4.62534070e-01
 -3.03325146e-01  1.07086986e-01 -1.71160772e-01  2.15534464e-01
 -3.45025286e-02 -1.51117355e-01 -4.23869967e-01  6.71312138e-02
  6.43366575e-02 -1.05644852e-01  1.51685417e-01  3.41682792e-01
 -2.09424615e-01 -4.88053076e-02  5.12279034e-01 -4.52545762e-01
 -3.09761584e-01 -1.93280816e-01  4.16082054e-01  1.13709636e-01
  3.97719979e-01 -5.52753031e-01  4.70610745e-02 -8.82642418e-02
 -1.12040937e-01  3.87805283e-01 -9.55635458e-02  7.66869858e-02
 -2.38998815e-01  6.87324107e-01  2.00971052e-01 -7.10149035e-02
  7.36175179e-02 -1.22270733e-01 -3.03625822e-01  6.80281073e-02
  1.11306280e-01 -2.52843678e-01  1.41095594e-01 -4.33297753e-01
  8.75713378e-02  1.81073755e-01 -2.56471038e-01 -1.39872700e-01
 -1.43880308e-01  1.27072662e-01 -1.82095900e-01 -2.42553189e-01
 -3.70708346e-01  2.52783030e-01  1.12753130e-01 -4.16708052e-01
  4.33220565e-02 -7.57676810e-02 -5.59109896e-02 -2.37797707e-01
 -1.63504891e-02 -3.33076417e-01  3.21479261e-01  6.32266998e-01
 -9.59270000e-02  2.11311862e-01  1.50005519e-01  1.10096656e-01
 -4.00771916e-01  8.37966427e-03 -2.47879341e-01  4.12938207e-01
  2.68761277e-01 -2.78955996e-01  2.88476706e-01  4.54691023e-01
 -8.00056085e-02 -3.97688709e-02 -1.84912652e-01 -5.52302413e-02
  3.25337052e-01 -2.02068627e-01 -2.36759067e-01 -3.77916992e-01
  1.64731711e-01  3.72329503e-01 -1.98986202e-01  1.83717579e-01
 -2.47128963e-01  1.85281098e-01  6.52673066e-01 -4.16778207e-01
 -3.07122618e-01  2.33131930e-01  3.81466448e-01 -2.00185835e-01
  8.33315179e-02 -1.57907754e-01  9.33411121e-02  2.69017853e-02]"
KeyError: 'min' type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/operation_layers.py"", line 31, in convert_clip
    if params['min'] == 0:
KeyError: 'min'

### Standalone code to reproduce the issue

```shell
from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('ssd_bmv1_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'ssd_bmv1_torch.h5', overwrite=True, save_format=""h5"")

onnx file can be downloaded at https://pan.xunlei.com/s/VNf1O2DqsdqdbTYYpyemSqveA1?pwd=by77#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/operation_layers.py"", line 31, in convert_clip
    if params['min'] == 0:
KeyError: 'min'
```
",True,"[-4.16091681e-01 -4.63917345e-01 -4.30089384e-02  6.98116198e-02
  2.40586996e-01 -4.86990899e-01 -2.76795864e-01 -6.95306957e-02
 -2.76669413e-01 -3.58125985e-01  5.52555844e-02 -5.91752231e-02
 -1.36085346e-01  1.00479543e-01 -1.10029981e-01  4.71000433e-01
 -1.55049011e-01 -1.88996971e-01  1.63206518e-01  2.00398833e-01
 -1.96619362e-01 -1.29750445e-01 -3.81258786e-01  6.22788556e-02
  1.45258844e-01  1.28029794e-01 -2.27382928e-01 -3.90412435e-02
 -3.68140638e-02  2.35619009e-01  5.09443700e-01  7.43909627e-02
 -8.10124204e-02  2.28285789e-01  1.72992617e-01  2.72314966e-01
 -2.17020914e-01 -2.76773572e-01 -2.79500306e-01  1.18401721e-01
 -1.67978272e-01  1.16931513e-01  1.67693168e-01 -1.58479944e-01
  1.03945792e-01 -1.89658210e-01 -7.16006905e-02 -2.68346012e-01
 -9.17606726e-02 -2.09763288e-01  1.80446170e-02  7.43420422e-03
 -4.86450851e-01 -2.28564829e-01 -1.62220895e-01 -7.56481141e-02
 -2.11984038e-01 -1.60485320e-03  4.40070331e-02  2.80938327e-01
  1.23504914e-01 -5.99880666e-02 -3.75539809e-03 -1.52784571e-01
  3.12075056e-02  1.78622663e-01  4.47223246e-01 -1.66147679e-01
  5.64853072e-01 -2.90888250e-01  7.77985156e-03 -7.20486790e-02
 -3.34977865e-01  1.31356612e-01  1.52580813e-01  1.42427534e-02
  6.05448056e-03  1.09530464e-01  1.43879667e-01 -1.01222500e-01
 -5.08112088e-02 -1.43128619e-01  8.77953693e-03 -2.46874020e-01
  1.61388502e-01 -1.84395835e-01  3.52542937e-01  2.02633262e-01
  2.38368809e-01 -2.08939537e-01  3.66436839e-01  2.87094533e-01
 -1.12792561e-02  2.16426194e-01  4.71892238e-01  1.17289223e-01
  1.45754039e-01  6.15647361e-02  1.30466260e-02 -1.63852736e-01
 -3.09605151e-02 -2.63275325e-01 -1.29810750e-01  9.05066282e-02
 -7.64614195e-02 -9.23375264e-02  1.44869298e-01 -2.75176018e-02
  9.36029479e-02 -1.41093418e-01  1.87057227e-01  1.37035966e-01
  2.58269966e-01 -2.79001743e-01  9.19383094e-02 -1.52122006e-01
 -2.24936515e-01 -9.72206593e-02  9.15651023e-02  7.97869444e-01
  8.96503925e-02 -1.20187271e-02 -7.30623156e-02  6.75754547e-02
  4.54957068e-01  3.10692549e-01  3.14325616e-02  7.08199888e-02
 -1.58071574e-02 -2.07654819e-01 -8.40068236e-02  1.18313320e-02
  1.60939485e-01  2.41015255e-01 -4.93192896e-02  2.52876937e-01
 -1.34051487e-01 -2.29195759e-01 -8.73277038e-02 -3.81051689e-01
 -3.42574477e-01  1.50461555e-01 -9.18341428e-02 -6.76893651e-01
  8.38641375e-02  1.56614274e-01 -2.62448490e-01  4.17256027e-01
 -1.87329859e-01 -8.98391753e-02 -3.13618220e-05  1.29261076e-01
 -3.06439530e-02  3.70503783e-01  1.08089089e-01  2.86390930e-01
  4.87491488e-01 -9.06178281e-02  1.24121390e-01 -4.86803234e-01
  6.95933029e-03  3.18255484e-01 -3.26951712e-01 -1.70438841e-01
  1.14135668e-01  8.22425932e-02 -4.05755281e-01 -2.03051582e-01
  1.89537201e-02  4.80947852e-01 -1.95849419e-01 -6.44004866e-02
  1.08316138e-01  1.05250239e-01 -3.77596021e-02  9.63718295e-02
  3.30499023e-01 -6.98851407e-01 -1.21042334e-01  4.67251956e-01
 -1.01239644e-02  1.57469362e-01  1.67093396e-01  1.49598062e-01
  1.38455093e-01  4.38719615e-02  5.45803532e-02  2.76782483e-01
 -1.81070089e-01 -2.18178518e-02 -3.32159519e-01 -1.45507857e-01
  4.95842993e-01 -1.45071760e-01 -8.70439112e-02  1.03103563e-01
  1.47266939e-01 -2.21763760e-01 -7.15333149e-02 -1.31613582e-01
 -1.61808476e-01 -3.37921605e-02 -2.64932454e-01  1.09485663e-01
  1.39582440e-01 -2.58365273e-01 -4.82279398e-02 -4.61120665e-01
 -2.84776390e-01  8.89972597e-02  7.10058659e-02 -4.43083882e-01
  9.90198776e-02 -4.68778610e-02 -3.16848278e-01  2.02951178e-01
  2.06051707e-01  1.23416901e-01 -3.00286651e-01  2.63296485e-01
  6.82371706e-02 -2.31931746e-01  1.19424602e-02 -4.91449773e-01
 -1.94034830e-01  1.18874818e-01 -4.98251587e-01  1.25344813e-01
  4.28341366e-02  2.69557655e-01  2.08611414e-02  1.43295899e-01
  2.94245958e-01  2.51558900e-01  4.33457375e-01 -4.69811037e-02
 -1.56064928e-01 -1.32394969e-01 -1.60130322e-01  9.25759822e-02
 -4.00066257e-01 -1.39492750e-01  8.44131857e-02  9.30658653e-02
  2.46024400e-01  4.32911634e-01 -1.75829381e-01 -1.29182398e-01
 -3.61324728e-01  1.92611620e-01 -1.62954599e-01  2.79449403e-01
  3.18619430e-01 -1.80478003e-02  6.15289211e-01  2.55182147e-01
  3.54529142e-01  1.65847272e-01  3.15056503e-01 -2.34281570e-01
  2.99217880e-01  7.47884065e-02  1.42853588e-01  4.44255590e-01
  2.68954098e-01  2.68182874e-01 -3.50581884e-01  4.36887741e-01
  1.20063238e-01 -2.93144472e-02  1.77109003e-01 -3.71571332e-01
  6.87787890e-01 -2.87966102e-01  1.11776918e-01 -8.64413567e-03
  3.99590284e-01 -3.23433056e-02 -2.09395103e-02  4.38168794e-02
  1.09051261e-02  2.13666081e-01 -2.09460869e-01 -1.99034438e-01
 -1.95177607e-02 -1.14576578e-01 -4.85857576e-02 -6.47979200e-01
 -2.67775953e-01  4.78579402e-02 -2.12940395e-01  1.89232871e-01
  7.68476427e-02 -2.35321932e-02 -2.61691660e-01  1.01425469e-01
  1.27537981e-01 -1.07664973e-01  2.48557091e-01  1.71449929e-01
 -2.09319443e-01  1.84590556e-03  3.41112733e-01 -4.83225942e-01
 -1.60769224e-01  1.24345161e-01  5.21134198e-01  1.62088186e-01
  4.23313707e-01 -4.79170263e-01  2.06817150e-01 -1.15022935e-01
 -2.52032936e-01  4.74236190e-01 -1.08915493e-01  1.83179528e-02
 -3.85027885e-01  5.07218242e-01  2.66649365e-01 -2.27813393e-01
  2.23560214e-01 -1.16676331e-01 -4.42041457e-01 -3.25046852e-02
  2.34079465e-01 -1.37090132e-01 -1.62169442e-01 -2.59395123e-01
  7.33368993e-02  2.18284726e-01 -9.61149633e-02  1.23026147e-01
 -1.93206310e-01  7.77171105e-02 -3.25116992e-01 -2.20143646e-02
 -5.28902769e-01  1.71907082e-01  3.65631282e-02 -2.26579055e-01
 -7.86303580e-02 -1.05723813e-02  5.32512814e-02 -1.82012290e-01
  1.32767260e-01 -2.22800896e-01  3.24725419e-01  6.90550804e-01
 -1.70168012e-01  6.59791604e-02 -2.52574384e-02  7.33074844e-02
 -4.32762027e-01 -1.08438760e-01 -1.13932818e-01  3.89848173e-01
  4.11560461e-02 -1.79376096e-01  6.32043958e-01  2.23383069e-01
 -2.16196805e-01  4.62225862e-02 -3.20573211e-01  6.57066405e-02
  5.62019348e-02 -2.10092768e-01 -3.18723053e-01 -1.74365073e-01
  1.79604709e-01  3.97741199e-01 -1.27461478e-01  9.82417017e-02
 -3.60551059e-01  2.89242029e-01  4.60131764e-01 -2.64248312e-01
 -3.55649948e-01  1.29165635e-01  1.27118796e-01 -1.66020513e-01
 -3.31070721e-02 -2.57825498e-02  2.80326396e-01  1.17596202e-02]"
KeyError: 'ConstantOfShape' type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
KeyError: 'ConstantOfShape'

### Standalone code to reproduce the issue

```shell
from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('patchcore_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'patchcore_torch.h5', overwrite=True, save_format=""h5"")

onnx file can be downloaded at https://pan.xunlei.com/s/VNf1NLiBYfIh_GKpSvMjqJQAA1?pwd=3wzb#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
KeyError: 'ConstantOfShape'
```
",True,"[-2.78852075e-01 -4.56899703e-01 -8.41599703e-02  2.49238349e-02
  2.90354669e-01 -5.61097145e-01 -2.09077269e-01 -3.21519189e-03
 -4.50927198e-01 -2.43664026e-01 -1.69613678e-02 -3.71346101e-02
 -1.22799635e-01  1.16096728e-01 -1.33683905e-01  3.74889404e-01
 -1.68901458e-01 -2.16044456e-01  1.33445919e-01  2.02370733e-01
 -1.97728515e-01  5.28920963e-02 -2.86014289e-01  1.68300003e-01
  1.68042388e-02  1.45075619e-01 -2.36511379e-01 -1.84981078e-01
 -8.05816203e-02  1.62914395e-01  4.79939848e-01 -4.13569249e-02
 -8.48264992e-02  1.38657987e-01  4.08225060e-02  3.16798121e-01
 -1.45712480e-01 -3.28114510e-01 -3.20906699e-01  8.12983438e-02
 -1.78367376e-01  1.65714726e-01  1.08742565e-01 -2.22448692e-01
  1.77392542e-01 -2.44073927e-01 -2.06428319e-01 -1.86234042e-01
 -1.26255732e-02 -6.75264299e-02 -7.78211802e-02  1.68268252e-02
 -4.08460200e-01 -2.16786832e-01 -9.85593274e-02 -1.23880558e-01
 -2.44897157e-01  4.90250923e-02  1.23259015e-01  1.58418149e-01
  1.87256902e-01 -1.28001004e-01  8.07267278e-02 -1.04048729e-01
  1.23841465e-01  9.84643996e-02  4.65860516e-01 -3.90198082e-02
  5.37635565e-01 -1.41386628e-01 -4.61497419e-02  4.32833731e-02
 -3.39314163e-01 -3.41670848e-02  3.13874066e-01  6.20064028e-02
 -1.94398314e-01  5.54241091e-02  1.65480971e-01 -1.18638337e-01
 -4.29807976e-02 -1.65898010e-01 -6.96592107e-02 -3.41578364e-01
  1.66782081e-01 -4.42036614e-02  3.07035416e-01 -5.64301293e-03
  3.76453817e-01 -1.58303782e-01  3.40028226e-01  2.42736325e-01
 -3.73285674e-02  3.40193421e-01  4.25559998e-01 -5.77002689e-02
  1.22988269e-01  6.06124550e-02  5.78559116e-02 -2.16060281e-01
 -1.36705160e-01 -2.62308151e-01  5.28181642e-02 -3.62565666e-02
 -2.33790539e-02 -1.84868053e-01  1.38372168e-01  7.56456554e-02
  1.10225387e-01 -1.37980685e-01  1.79184467e-01  1.03839666e-01
  1.80181041e-01 -2.70844489e-01  8.45059380e-02 -1.12722725e-01
 -1.14387065e-01 -2.02661887e-01 -6.01612814e-02  6.32289529e-01
  1.71602249e-01  4.18433212e-02 -6.54001832e-02  1.68148786e-01
  3.98982286e-01  2.10219175e-01  1.04290023e-01  1.13920607e-01
  3.13862003e-02 -3.43111217e-01 -1.52329262e-02 -7.23944381e-02
  1.20438159e-01  2.01983333e-01  3.33240330e-02  3.03793937e-01
 -2.10455656e-01 -3.61874372e-01 -9.78900939e-02 -1.47285879e-01
 -3.54023755e-01  2.41421387e-01 -2.09168792e-01 -7.53917217e-01
  1.91347480e-01  1.93877041e-01 -2.16645330e-01  2.03790098e-01
 -2.91724533e-01 -3.63280252e-02  2.39378214e-02 -2.80005485e-02
  5.16777299e-02  3.38043481e-01  1.21777438e-01  3.38315696e-01
  3.83139551e-01 -8.82714689e-02  1.97278097e-01 -5.49199462e-01
  4.05161977e-02  4.68885332e-01 -2.79653728e-01 -5.14379740e-02
  1.21220760e-01  1.85772896e-01 -3.58309209e-01 -1.47608861e-01
 -1.00498728e-01  4.19014037e-01 -1.17733426e-01 -4.03952636e-02
  1.55811742e-01 -8.04918185e-02 -6.03863411e-02  9.83107314e-02
  1.93280399e-01 -5.40707231e-01 -3.63090001e-02  3.50895166e-01
 -1.39763772e-01  3.39454114e-01 -2.92332713e-02  2.49658674e-01
 -2.34908834e-02  5.69259562e-02  7.92644098e-02  2.81190157e-01
 -1.99709177e-01 -4.81391884e-03 -3.11198294e-01 -1.20001808e-01
  3.72667819e-01 -2.18271166e-01 -2.13882606e-03  1.25458241e-01
  1.61154985e-01 -1.17419086e-01  1.52133405e-03 -8.34804848e-02
 -1.06948972e-01 -8.19503516e-02 -6.37109950e-02  1.17887504e-01
  1.22123711e-01 -1.99572027e-01 -1.16083641e-02 -3.86476517e-01
 -1.78162277e-01  1.59750044e-01  7.67684504e-02 -3.14579219e-01
 -1.27973706e-01 -1.99922156e-02 -2.48693869e-01  1.84970647e-01
  1.25973910e-01  1.28832161e-01 -1.50819466e-01  1.09584302e-01
  1.95964381e-01 -3.35347831e-01  1.38836622e-01 -5.05270839e-01
  1.42521054e-01 -2.18058005e-04 -2.50317752e-01  2.07924649e-01
 -1.88234597e-02  2.72023141e-01  1.22198261e-01 -4.39900421e-02
  2.49586716e-01  9.77624357e-02  6.28361225e-01  1.64447986e-02
 -1.61018446e-01 -6.39310330e-02 -2.05719382e-01  3.27784121e-02
 -3.58648866e-01 -7.67780244e-02  1.47010058e-01  1.26720471e-02
  3.04781169e-01  3.27172577e-01 -1.11602373e-01 -1.77678108e-01
 -2.75912642e-01  5.30076027e-02 -3.18528324e-01  2.97994018e-01
  2.10497260e-01 -2.17993204e-02  5.70053399e-01  3.07057947e-01
  2.95458674e-01  1.90035179e-01  2.49990031e-01 -1.81711882e-01
  2.10097939e-01  2.25961462e-01  2.10008577e-01  4.11636561e-01
  1.90387800e-01  2.33357966e-01 -3.45233679e-01  2.62696266e-01
  1.20234720e-01 -2.91692019e-02  1.08114332e-01 -3.02024424e-01
  5.96833408e-01 -2.53905147e-01  2.00107589e-01 -8.43300484e-03
  3.93170178e-01 -1.38495713e-01 -4.70307991e-02  2.14738071e-01
 -9.09297019e-02  2.98327118e-01 -2.36849934e-01 -1.82798773e-01
  9.98839736e-02 -1.10436074e-01 -3.13717760e-02 -4.68379050e-01
 -1.86746061e-01 -6.40883148e-02 -1.04903646e-01  8.28370973e-02
  1.30895793e-01 -1.60571188e-01 -2.11064041e-01  1.15675338e-01
  8.44313949e-02 -4.34805602e-02  1.91055313e-01  1.55635476e-01
 -7.14792013e-02 -1.99811995e-01  2.75141507e-01 -3.72593880e-01
 -1.82617128e-01 -2.02231817e-02  4.81231838e-01  2.37260252e-01
  3.85692954e-01 -4.94563758e-01  2.18963265e-01 -1.37336075e-01
 -3.15709233e-01  5.86834550e-01 -1.93577424e-01  7.25943521e-02
 -2.69883215e-01  4.37632322e-01  1.66476592e-01 -1.22324362e-01
  9.89428088e-02 -2.47842520e-01 -4.65101272e-01 -7.28136674e-02
  3.18902165e-01 -1.74497589e-01 -1.52430683e-01 -2.46803910e-01
 -3.94376107e-02  1.32789060e-01 -2.54351869e-02  1.31383985e-01
 -1.92374408e-01 -9.56874564e-02 -3.46473128e-01 -4.46513109e-03
 -5.18752158e-01  1.25434175e-01  4.27605063e-02 -1.65893897e-01
 -1.60108417e-01 -1.70038566e-02 -6.71230555e-02 -2.68128008e-01
  1.50161281e-01 -1.87632531e-01  3.54450405e-01  6.69543743e-01
 -1.38974041e-01  2.47639194e-02  5.78857064e-02  1.00385502e-01
 -4.49752569e-01 -1.16799816e-01 -1.25174910e-01  5.25091648e-01
 -5.13678156e-02 -1.64427876e-01  2.93584079e-01  3.11599970e-01
 -1.57229424e-01  1.57249331e-01 -2.40068018e-01  5.90005070e-02
 -6.24493808e-02 -1.70148730e-01 -2.31540293e-01  7.99340308e-02
  5.37856147e-02  4.15214956e-01 -7.26216733e-02  2.09455684e-01
 -2.78289080e-01  2.82179356e-01  3.90087664e-01 -3.80141705e-01
 -2.95401692e-01  4.54207137e-02  1.01598516e-01  1.58422124e-02
 -1.40815675e-01  9.40302014e-03  3.38931978e-01  1.43083066e-01]"
AttributeError: Number of inputs is not equal 1 for unsqueeze layer type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['onnx::Unsqueeze_0'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 210, in convert_unsqueeze
    raise AttributeError('Number of inputs is not equal 1 for unsqueeze layer')
AttributeError: Number of inputs is not equal 1 for unsqueeze layer

### Standalone code to reproduce the issue

```shell
please run the below codes to reproduce:

from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('textcnn_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['onnx::Unsqueeze_0'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'textcnn_torch.h5', overwrite=True, save_format=""h5"")
```
onnx file can be downloaded at https://pan.xunlei.com/s/VNf1M2KIkqx6PBbrBdTiuTkMA1?pwd=wxqf#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['onnx::Unsqueeze_0'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 210, in convert_unsqueeze
    raise AttributeError('Number of inputs is not equal 1 for unsqueeze layer')
AttributeError: Number of inputs is not equal 1 for unsqueeze layer
```
",True,"[-0.36778948 -0.46262097 -0.13903612  0.06855279  0.15005137 -0.4107712
 -0.12113774  0.00595967 -0.38887572 -0.20738521 -0.03708407 -0.19832502
  0.04680441  0.15123661 -0.0648253   0.58355725 -0.17771333 -0.17831981
  0.2312357   0.04888054 -0.05237459 -0.11651872 -0.369618    0.08125349
  0.0297393   0.1024846  -0.3427871  -0.00509221 -0.07683551  0.13190046
  0.3375675   0.00513603 -0.24346039  0.18968071 -0.11775382  0.25433266
 -0.23660709 -0.25582188 -0.30676925  0.1792611  -0.12512596  0.18321854
  0.1094415  -0.25100952  0.02020335 -0.2695594  -0.10134104 -0.14913219
 -0.08958188 -0.25626093  0.02275866  0.12142383 -0.45291176 -0.18158577
 -0.21056637 -0.17618632 -0.03997946 -0.10462375 -0.076225    0.1290056
 -0.04581238  0.05164659  0.07148248 -0.1862461   0.03661972  0.12981683
  0.35335904  0.10457659  0.49864268 -0.11579747  0.10284108 -0.02065201
 -0.28817862  0.01637341  0.14076594  0.06277858 -0.10012577  0.18225488
  0.14429143 -0.27341193 -0.21081726 -0.13840894 -0.09500771 -0.08308862
  0.11766967 -0.08671633  0.20547146  0.16537371  0.44113338 -0.09595375
  0.59376     0.31513146 -0.16169454  0.26150894  0.5428331   0.17596835
  0.07049884  0.07780387  0.06174273 -0.12748942 -0.03458386 -0.323429
 -0.23826234 -0.01859725  0.07704443 -0.08053595  0.19731572 -0.09240454
  0.03349051 -0.10177605  0.15306392 -0.00545862  0.19231366 -0.21999621
  0.02624061 -0.0744902   0.0429028  -0.19290292 -0.08690691  0.6060991
  0.34051284  0.12391064  0.05371448  0.29279783  0.594682    0.07707311
  0.13620698  0.03707435  0.06712165 -0.08949845  0.1656945   0.10900468
  0.04225171  0.28066567  0.05940257  0.24593568 -0.24466011 -0.06480628
 -0.34539616 -0.21612053 -0.298986    0.13910884 -0.03133475 -0.6504362
  0.06912819  0.13266194 -0.27500358  0.35035482 -0.17268546 -0.051589
 -0.02618669  0.1688695  -0.1597246   0.41130885  0.04148827  0.30910188
  0.36654824 -0.01065664  0.288824   -0.5795644   0.04726869  0.37709677
 -0.25134632 -0.14543928  0.14142141  0.14360824 -0.35380054 -0.17652518
  0.11525685  0.35158032 -0.07364994 -0.02187094  0.12683573 -0.01225379
  0.07283476 -0.01934353  0.11118107 -0.6035242   0.01656336  0.3874088
  0.02393137  0.1922411   0.07370742  0.1409507  -0.01467781  0.14185654
  0.14724958  0.27341312 -0.0886799  -0.0022487  -0.36829114 -0.22384873
  0.46961486 -0.320328    0.06085309 -0.04767769  0.09117694 -0.29489562
 -0.02445216 -0.07389608 -0.13845977 -0.0554533  -0.16197217  0.08303665
  0.1625365  -0.18913242  0.27641138 -0.4723362  -0.4392059   0.2137601
 -0.11865235 -0.59732866  0.17639412 -0.03873013 -0.22632769  0.14933941
  0.08080059  0.18918431 -0.22813278  0.04337397  0.0612064  -0.30778235
  0.09662214 -0.39697024 -0.27335018  0.24496596 -0.32368568  0.11906426
  0.01969827  0.18276656  0.15117835  0.0215491   0.33074942  0.3196277
  0.44699323  0.04279916 -0.12303776  0.0027324  -0.03767906  0.08090314
 -0.4275027  -0.06107172  0.03596847 -0.07913814  0.24337797  0.4340801
 -0.06056005 -0.23698626 -0.44793195  0.2495721  -0.3460747   0.18603177
  0.5010699   0.00294432  0.60594666  0.27021635  0.4018181   0.23025711
  0.2570771  -0.18477508  0.13725957  0.1134546   0.23905659  0.33683023
  0.14251675  0.13930494 -0.21681374  0.42690748  0.04813771 -0.11967918
  0.03541278 -0.32278168  0.5660635  -0.34921962 -0.00943428 -0.03332025
  0.32810158 -0.07154321  0.03251754  0.14865452 -0.20597294  0.32738757
 -0.1489078  -0.05155546  0.14922038 -0.0709398  -0.06636613 -0.6313217
 -0.12228779  0.00186491 -0.1352838   0.10181178  0.02122073 -0.12280218
 -0.18187419  0.11044063  0.07987297 -0.27632236  0.1013882   0.17445931
 -0.10089625  0.11262448  0.2356345  -0.4367144  -0.20434374  0.07516409
  0.59770834  0.14937416  0.37965074 -0.5987036   0.19811913 -0.11090519
 -0.09652723  0.4937446  -0.13762198  0.0092045  -0.2248146   0.5735809
  0.2657261  -0.09109572  0.12642902 -0.23263419 -0.35819417 -0.1334658
  0.134415   -0.1258149  -0.18013248 -0.2751707  -0.03187581  0.18502708
 -0.01727272 -0.04691984 -0.17923898  0.01422497 -0.08496341 -0.10153503
 -0.53739417  0.14252011 -0.04635228 -0.27761436 -0.115915    0.00194387
 -0.04679643 -0.19745229  0.02723452 -0.16813666  0.15419978  0.56514657
 -0.14282417  0.06964961  0.26599556  0.0478856  -0.4283948   0.06534316
 -0.10713987  0.4001292  -0.04029015 -0.24453518  0.3408063   0.2165667
 -0.18038127  0.06440749 -0.278022    0.11819634  0.1671871  -0.14672142
 -0.3208339  -0.20477551  0.11759801  0.2589653  -0.30129212  0.14194793
 -0.43638712  0.33889726  0.3776926  -0.37091675 -0.2660023   0.08635347
  0.22127335 -0.13779409 -0.0210585   0.04533207  0.2420876   0.01842392]"
AttributeError: Can't gather from tf tensor. type:bug wsl2 TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['onnx::Cast_0', 'onnx::Cast_1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 87, in convert_gather
    raise AttributeError('Can\'t gather from tf tensor.')
AttributeError: Can't gather from tf tensor.

### Standalone code to reproduce the issue

```shell
please run the codes below to reproduce:

from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('fasttext_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['onnx::Cast_0', 'onnx::Cast_1'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'fasttext_torch.h5', overwrite=True, save_format=""h5"")
```
onnx file can be downloaded at https://pan.xunlei.com/s/VNf1LHoMqVe2TuzMTA8uhjO5A1?pwd=ibm3#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['onnx::Cast_0', 'onnx::Cast_1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 87, in convert_gather
    raise AttributeError('Can\'t gather from tf tensor.')
AttributeError: Can't gather from tf tensor.
```
",True,"[-3.88516188e-01 -5.27623236e-01 -6.41570762e-02  5.87474108e-02
  2.82460093e-01 -3.33946705e-01 -1.81516409e-01 -9.84587669e-02
 -3.57315898e-01 -3.56085062e-01 -6.71601668e-02 -1.97812721e-01
 -8.61811787e-02  2.23403662e-01 -3.44987884e-02  4.89296585e-01
 -1.32024944e-01 -1.62120759e-01  1.85743988e-01 -1.73065718e-02
 -2.32774958e-01 -1.42375350e-01 -2.56455123e-01  1.97972044e-01
  4.86833826e-02  8.82440209e-02 -3.51675987e-01 -3.52623723e-02
 -7.72222281e-02  2.90089250e-01  3.74435365e-01 -3.18896137e-02
 -2.90874124e-01  1.50010243e-01 -7.84396976e-02  3.04664195e-01
 -2.17641667e-01 -1.12675965e-01 -2.46804208e-01  1.03851669e-01
 -8.51034895e-02 -4.78823856e-02  1.30144775e-01 -1.13549799e-01
 -1.41802475e-01 -1.78473949e-01  1.52992696e-01 -9.24575776e-02
 -2.14588881e-01 -1.08033553e-01  3.07502151e-02  5.12488633e-02
 -4.31206822e-01 -2.50468373e-01 -1.38252795e-01  3.49625796e-02
  3.93060446e-02  4.46660221e-02  3.20198201e-03  1.69654071e-01
 -1.65616125e-02  1.70024455e-01  1.79017171e-01 -1.27197266e-01
  1.36411354e-01  2.17838556e-01  3.69780391e-01  1.01717710e-01
  5.26606381e-01 -3.03146064e-01  1.58047795e-01 -3.07558458e-02
 -3.37592572e-01  7.28723258e-02  3.51960361e-02  5.90115748e-02
 -6.79243207e-02  1.11522570e-01  3.46918970e-01 -1.26607224e-01
 -1.36762053e-01 -5.66622168e-02 -1.25181094e-01 -4.80811074e-02
  7.98839927e-02 -1.74750209e-01  2.90555209e-01  2.47477680e-01
  4.55322951e-01 -1.32751316e-01  6.49155796e-01  2.25912333e-01
 -4.87322882e-02  1.54322013e-01  4.70547318e-01  2.42010549e-01
  5.73975742e-02  2.65342951e-01  1.14164837e-01 -1.26940310e-01
 -2.21047595e-01 -4.70518410e-01 -2.22122431e-01  3.50686610e-02
 -6.15821220e-03 -1.90732941e-01  1.45133853e-01  1.77973658e-02
  8.87814462e-02 -1.19106211e-01  1.03955515e-01  3.89892980e-02
  2.66415000e-01 -2.47356266e-01 -5.83324358e-02 -1.03782237e-01
 -1.43204689e-01 -6.34422153e-02  9.15652215e-02  6.11326456e-01
  2.85056293e-01  6.33434858e-04  1.22495383e-01  1.58898085e-01
  5.11802018e-01  1.39464557e-01 -9.20957625e-02  6.17744401e-02
  1.80240244e-01 -4.07863632e-02  9.60808918e-02  1.30481854e-01
 -1.18645485e-02  2.07518697e-01  4.18223366e-02  1.61597013e-01
 -3.39442909e-01 -1.97016925e-01 -2.19632119e-01 -2.17950195e-01
 -4.12240922e-01  3.76604795e-02  1.18523967e-02 -5.12577891e-01
  9.27378237e-02  1.71499863e-01 -1.21335939e-01  4.42865938e-01
 -2.74522185e-01  8.75369161e-02 -1.95687041e-02  1.90408379e-01
 -5.01953177e-02  3.16816598e-01  2.87229903e-02  2.54163891e-01
  4.85704631e-01 -1.16728768e-01  1.83117956e-01 -4.99681592e-01
 -2.94550937e-02  5.04104674e-01 -1.17991075e-01 -2.02164575e-01
  4.94608618e-02  2.03227475e-01 -4.78831202e-01 -1.70999140e-01
  1.73713356e-01  4.08236653e-01  2.56895944e-02 -1.10967204e-01
  1.03183411e-01  3.74773033e-02  9.33503956e-02 -1.12097152e-01
  2.72161633e-01 -6.36454701e-01  9.69804674e-02  4.22091991e-01
  5.84758818e-05  1.85849875e-01  1.65543124e-01  2.12265626e-01
  2.71142051e-02  5.03933467e-02  1.48221403e-01  2.84236729e-01
 -2.39943072e-01 -1.15018291e-02 -2.35778883e-01 -1.41388208e-01
  4.82376933e-01 -2.39625692e-01 -7.27874115e-02 -2.35798825e-02
  1.95102915e-01 -1.54934406e-01  1.15693331e-01  2.03285832e-02
 -7.92581588e-02 -7.42026716e-02 -2.34121740e-01  1.32170349e-01
  2.44326815e-01 -2.38051742e-01  1.36409700e-01 -5.48660100e-01
 -3.37162346e-01  3.15680951e-02  1.01099879e-01 -3.71354669e-01
  1.00675106e-01 -5.44794984e-02 -2.91307390e-01  8.69999751e-02
  5.47690988e-02  9.24495161e-02 -2.68722177e-01  2.71084439e-02
 -1.14170955e-02 -2.70786464e-01 -3.98426279e-02 -4.31030780e-01
 -2.47346804e-01  7.20952451e-02 -3.71354759e-01  2.30060220e-02
  5.07606827e-02  2.84350902e-01  8.46351311e-03  1.20884866e-01
  2.25695699e-01  3.29624236e-01  4.39458728e-01 -1.80079609e-01
 -2.15154231e-01 -2.56058037e-01 -1.04907699e-01  6.67258874e-02
 -3.42893362e-01 -5.13512306e-02 -9.23365951e-02 -1.64361954e-01
  2.63405144e-01  5.05452454e-01 -3.49683315e-02 -2.05291778e-01
 -3.51047277e-01  2.98379630e-01 -2.45818675e-01  1.73158363e-01
  3.15318108e-01  4.72911522e-02  5.97183585e-01  2.30076671e-01
  3.20925355e-01  2.55684018e-01  2.72439957e-01 -2.61494458e-01
  1.76581472e-01  1.17691442e-01  2.19256878e-01  3.74435276e-01
  1.99949890e-01  2.43275523e-01 -1.95928663e-01  3.87367666e-01
 -1.11150227e-01 -1.89664923e-02 -2.22013425e-02 -3.14501852e-01
  7.92337418e-01 -3.22931588e-01  1.97321903e-02 -9.23345089e-02
  4.49224412e-01 -1.39278576e-01 -4.87881377e-02  1.79072678e-01
 -5.61490022e-02  3.96744221e-01 -1.80583268e-01 -6.67736009e-02
  4.49719541e-02 -1.24913920e-02 -9.03134346e-02 -6.29840672e-01
 -1.59970284e-01 -4.84915776e-03 -3.58095020e-01  2.97719575e-02
 -2.08915621e-01 -2.08144486e-01 -2.28086948e-01 -3.50045785e-02
  7.49920532e-02 -1.89709440e-01  1.36204511e-01  6.67860061e-02
 -1.61870003e-01  1.94747150e-01  4.05899763e-01 -3.39082003e-01
 -5.36913425e-02 -1.94639750e-02  5.70494473e-01  2.17095420e-01
  3.50533009e-01 -6.07316077e-01  1.80113077e-01 -3.74323651e-02
  1.55715533e-02  4.82371837e-01 -4.55551520e-02  8.50189030e-02
 -3.15011173e-01  5.62671423e-01  2.62991101e-01 -1.81214616e-01
  1.59842610e-01 -2.49806911e-01 -4.91957515e-01 -1.19560942e-01
  1.39365017e-01  4.59907837e-02 -1.08639851e-01 -2.98030108e-01
  2.37899628e-02  2.16367066e-01 -2.36763030e-01 -8.24192911e-02
 -2.63609439e-01 -5.24757244e-02 -2.02942342e-01 -1.79853886e-01
 -5.25912404e-01  4.24844921e-02  2.46489979e-03 -3.54166090e-01
 -1.75815433e-01 -1.02311164e-01 -6.51841238e-03 -1.64038524e-01
 -1.81266107e-03 -1.03846043e-01  2.03557119e-01  4.74354476e-01
 -7.43443668e-02  1.72902972e-01  8.99948478e-02  2.92891525e-02
 -3.78270984e-01 -6.23472184e-02 -6.59336150e-02  3.67396057e-01
 -1.05465174e-01 -3.20358992e-01  4.02007222e-01  3.02714765e-01
 -1.93965524e-01  1.06861942e-01 -2.04937190e-01  1.38824314e-01
  1.80890828e-01 -2.45595455e-01 -3.90942872e-01 -1.40062243e-01
  2.58852661e-01  3.67320597e-01 -1.71300545e-01  2.27536112e-01
 -4.98355746e-01  2.57570624e-01  3.75833511e-01 -3.27425659e-01
 -1.96686402e-01  1.11510493e-01  2.46605992e-01 -2.85782486e-01
  3.05841975e-02 -1.15082875e-01  3.17706227e-01  1.83718335e-02]"
AttributeError: Not implemented type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 294, in convert_slice
    raise AttributeError('Not implemented')
AttributeError: Not implemented

### Standalone code to reproduce the issue

```shell
reproduce by running the following code:

from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('deeplabv3_torch.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'deeplabv3_torch.h5', overwrite=True, save_format=""h5"")
```

onnx file can be downloaded at https://pan.xunlei.com/s/VNf1JfHu9m6WG6yE2IuzWtKpA1?pwd=ux7b#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['input.1'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/reshape_layers.py"", line 294, in convert_slice
    raise AttributeError('Not implemented')
AttributeError: Not implemented
```
",True,"[-0.43621966 -0.45584038 -0.00104742  0.06447522  0.2844699  -0.4132607
 -0.25544864 -0.08485961 -0.4118181  -0.3405507   0.05664257 -0.15254638
 -0.09298514  0.18755579 -0.02568923  0.54534006 -0.12734856 -0.12368393
  0.28440416  0.15581366 -0.17684668 -0.09290437 -0.31799877  0.04107163
  0.0437057   0.07836607 -0.29180056 -0.0584884  -0.03376624  0.24612358
  0.53346133 -0.04879285 -0.21440735  0.22053751 -0.00306362  0.21565786
 -0.2813322  -0.21343307 -0.24846028  0.15725657 -0.16035198  0.10286754
  0.14755744 -0.2688079  -0.026104   -0.2543835  -0.00671331 -0.19738536
 -0.07895826 -0.28978252  0.06701557 -0.05813815 -0.43240148 -0.27746063
 -0.1911385   0.01813652 -0.08498257  0.09497771 -0.021788    0.288711
  0.07351161 -0.01750742  0.08158845 -0.13236274  0.04172891  0.13666895
  0.35896996  0.01975634  0.60915506 -0.35506153  0.08889823 -0.03461286
 -0.21342658  0.12269793  0.10423581  0.03009527 -0.02817414  0.15811142
  0.22046736 -0.15482214 -0.14495829 -0.10080806 -0.09668757 -0.050386
  0.15173611 -0.13544658  0.22871542  0.16833852  0.32074577 -0.07949974
  0.4844191   0.31333575 -0.01103007  0.15655856  0.46023664  0.14147162
  0.1485134   0.14540154  0.11784978 -0.18301122 -0.08125002 -0.3770725
 -0.1580294   0.03606363 -0.07752857 -0.14847255  0.1662805  -0.07261972
  0.10344375 -0.1653972   0.15888533  0.05276226  0.19549051 -0.34681043
 -0.03778894 -0.14034218 -0.22989818 -0.1612293   0.01377799  0.63774556
  0.27796406  0.00595685  0.07199989  0.18779002  0.58451015  0.18232152
 -0.02214468  0.03333776  0.01073486 -0.11520609  0.0450699   0.04941623
  0.08267268  0.19189823  0.08927023  0.22968325 -0.31232852 -0.28494304
 -0.16268879 -0.33623177 -0.3665767   0.17954892 -0.0378763  -0.67809314
  0.05266272  0.12349321 -0.1568159   0.41027236 -0.17405373 -0.03081522
  0.02414013  0.1368519  -0.1154002   0.4608766   0.09266259  0.32788482
  0.5204956  -0.09003203  0.22672643 -0.510705    0.04025162  0.4491549
 -0.20089316 -0.17838648  0.02993997  0.05906667 -0.4243046  -0.13561249
  0.09135456  0.4301322  -0.2033939  -0.04835238  0.14013818 -0.0170166
 -0.05947088  0.05311668  0.27708408 -0.67138517  0.01225995  0.46186692
  0.0285655   0.1320079   0.1661738   0.18393698  0.03974716  0.12927255
  0.11662276  0.180489   -0.16236782 -0.00394469 -0.40743414 -0.08055604
  0.40800834 -0.23865154 -0.03185485  0.04665997  0.0898912  -0.17539588
  0.04121365 -0.09884004 -0.11823985 -0.01979281 -0.19228569  0.18953337
  0.09771922 -0.28825766  0.06381027 -0.536461   -0.40397155  0.08472309
 -0.05989031 -0.4243083   0.2030921   0.00232804 -0.29465908  0.20927754
  0.18639594  0.26134884 -0.21534915  0.10060404 -0.07499644 -0.27638596
  0.08268966 -0.4292826  -0.10216998  0.22252473 -0.42380744  0.02352745
  0.04410653  0.26802412 -0.00561391  0.01805036  0.35820815  0.30937967
  0.39870363 -0.04365169 -0.20308852 -0.12262398 -0.09702259  0.16404964
 -0.41440487 -0.02160314 -0.02592796 -0.04302827  0.2902559   0.46805912
 -0.06414323 -0.14077726 -0.40881613  0.17814876 -0.23623943  0.2565741
  0.47849575 -0.04543196  0.65595627  0.31689632  0.37077224  0.18321535
  0.24298397 -0.1931552   0.17887586  0.02816819  0.22677891  0.27176273
  0.16916461  0.14850488 -0.39999917  0.39287186  0.11992874 -0.06846405
  0.17539887 -0.31272304  0.6708927  -0.31549752  0.04679551 -0.03664618
  0.37761024 -0.0899157   0.00668652  0.06066748  0.01572237  0.27110985
 -0.288707   -0.1819765   0.05435659 -0.16968627 -0.03617498 -0.6405412
 -0.33097214  0.02798237 -0.27249306  0.19317377 -0.05253039 -0.09458476
 -0.27558976  0.02926071  0.12236176 -0.21424821  0.11907335  0.19881096
 -0.20123255 -0.00765602  0.31028956 -0.47573018 -0.08791777  0.00281038
  0.61536694  0.09806587  0.4364772  -0.6330416   0.15097709 -0.11606881
 -0.15728277  0.53457105 -0.07347102  0.05924135 -0.28958744  0.5208081
  0.22305815 -0.17879477  0.19674759 -0.19070959 -0.37705684 -0.12141653
  0.19756396 -0.09827785 -0.16504192 -0.28700075  0.07661159  0.192915
 -0.16946822  0.10617854 -0.14246288  0.04156144 -0.17653616 -0.07287611
 -0.44871742  0.17672582  0.00369789 -0.25539458 -0.15403911  0.03548148
 -0.05138749 -0.11738165  0.01813299 -0.16864398  0.29758048  0.5502812
 -0.1590202   0.17738941  0.11754668  0.0974905  -0.44583064 -0.01199235
 -0.09932797  0.36359066  0.07738055 -0.23163125  0.53457737  0.2186309
 -0.2172926   0.14019    -0.32531148  0.04701275  0.19610748 -0.22964424
 -0.40816352 -0.19793934  0.16759679  0.41315025 -0.24158356  0.07072487
 -0.5139295   0.29972708  0.4558921  -0.23955344 -0.29537067  0.0791669
  0.20445603 -0.11847194 -0.01930469 -0.0207455   0.4980272   0.00381988]"
"ValueError: Exception encountered when calling layer ""13"" (type Lambda). type:bug","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

5.15.90.1-microsoft-standard-WSL2

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/pengg/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/pengg/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
.font5
	{color:windowtext;
	font-size:9.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:;
	mso-generic-font-family:auto;
	mso-font-charset:134;}
tr
	{mso-height-source:auto;
	mso-ruby-visibility:none;}
col
	{mso-width-source:auto;
	mso-ruby-visibility:none;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:11.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:;
	mso-generic-font-family:auto;
	mso-font-charset:134;
	mso-number-format:General;
	text-align:general;
	vertical-align:middle;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl65
	{text-align:center;}
.xl66
	{text-align:center;
	white-space:normal;}
ruby
	{ruby-align:left;}
rt
	{color:windowtext;
	font-size:9.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:;
	mso-generic-font-family:auto;
	mso-font-charset:134;
	mso-char-type:none;
	display:none;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">



ValueError:   Exception encountered when calling layer ""13"" (type Lambda).          Dimensions must be equal, but are 204 and 206 for '{{node 13/Add}} =   AddV2[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes:   [?,64,204,204], [?,64,206,206].          Call arguments received by layer ""13"" (type Lambda):       inputs=['tf.Tensor(shape=(None,   64, 204, 204), dtype=float32)', 'tf.Tensor(shape=(None, 64, 206, 206),   dtype=float32)']       mask=None       training=None
--




</body>

</html>


### Standalone code to reproduce the issue

```shell
just run the following code to reproduce:

from onnx2keras import onnx_to_keras
import keras
import onnx
import sys
# sys.path.append(""/root/MR"")
onnx_model = onnx.load('yolov3_darknet53.onnx')
onnx_inputs = onnx_model.graph.input
print(""==========================="")
print(onnx_inputs)
# onnx_model = onnx.load('vgg11.onnx')
k_model = onnx_to_keras(onnx_model, ['x'], name_policy='renumerate', verbose=True)
keras.models.save_model(k_model, 'ssd_resnet50fpn_torch.h5', overwrite=True, save_format=""h5"")
```
the onnx file can be downloaded at https://pan.xunlei.com/s/VNf1FtBh2v6mP_QXJWVailBmA1?pwd=g43e#
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""o2k.py"", line 11, in <module>
    k_model = onnx_to_keras(onnx_model, ['x'], name_policy='renumerate', verbose=True)
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/converter.py"", line 175, in onnx_to_keras
    AVAILABLE_CONVERTERS[node_type](
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/elementwise_layers.py"", line 83, in convert_elementwise_add
    layers[node_name] = lambda_layer([input_0, input_1])
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/root/miniconda3/envs/onnx/lib/python3.8/site-packages/onnx2keras/elementwise_layers.py"", line 76, in target_layer
    layer = tf.add(
ValueError: Exception encountered when calling layer ""LAYER_12"" (type Lambda).

Dimensions must be equal, but are 204 and 206 for '{{node LAYER_12/Add}} = AddV2[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [?,64,204,204], [?,64,206,206].

Call arguments received by layer ""LAYER_12"" (type Lambda):
   inputs=['tf.Tensor(shape=(None, 64, 204, 204), dtype=float32)', 'tf.Tensor(shape=(None, 64, 206, 206), dtype=float32)']
   mask=None
   training=None
```
",True,"[-0.41594505 -0.48767093 -0.0132966   0.03741739  0.18204267 -0.37858486
 -0.13308473  0.05727968 -0.4024652  -0.21888214  0.08803896 -0.25947824
 -0.01465136  0.19071268  0.05294671  0.6817633  -0.22101131 -0.26209417
  0.22624579  0.00857633 -0.15197967 -0.14089036 -0.30994335  0.07311715
  0.15363191 -0.00208464 -0.3119169  -0.17285612 -0.00505728  0.23394357
  0.3663567  -0.05051945 -0.22040714  0.11846262  0.14244902  0.25354728
 -0.28488636 -0.3474971  -0.2885594   0.20659503 -0.17788538  0.13279487
  0.01553615 -0.22575548 -0.08655161 -0.33525777 -0.00444145 -0.15987077
 -0.15631522 -0.22679977  0.13948163  0.05599641 -0.4039713  -0.24259004
 -0.12634243 -0.00131158 -0.16986138  0.08259545  0.06448506  0.44919732
  0.0160624   0.0296327   0.07572415 -0.130528   -0.0082952   0.12951893
  0.39431915  0.05906912  0.62927467 -0.4073692   0.15815058 -0.0233614
 -0.39861736  0.06344935  0.24330139 -0.04950127  0.01137458  0.07048094
  0.18201202 -0.19329482 -0.11311095 -0.05589942 -0.02129588  0.03384741
  0.19240987 -0.19544889  0.17218432  0.14746165  0.42571646 -0.00454791
  0.5625018   0.10745482 -0.24672046  0.26008704  0.38177884  0.06031749
  0.16877262  0.18821687  0.01820311 -0.14811048  0.00069646 -0.50467813
 -0.33271432  0.0355655   0.03349979 -0.03512379  0.2846352  -0.06544816
 -0.02095009 -0.18657881  0.15443122  0.0731885   0.21649441 -0.3962104
  0.00412294 -0.09567002 -0.06942452 -0.14851211 -0.04219692  0.62721115
  0.25948143  0.09864277  0.07410908  0.24937113  0.50613403  0.15290865
  0.03544259  0.05343504  0.16438401 -0.13871552  0.18605721  0.19659203
  0.12441855  0.22319679  0.05370952  0.26880357 -0.3446591  -0.25058293
 -0.2121401  -0.41568473 -0.33758885  0.09917244  0.07194122 -0.67212474
  0.13388047  0.09287262 -0.06316516  0.45820785 -0.2061967  -0.15450567
  0.08101726  0.25415146 -0.28792745  0.52574515  0.04780228  0.29946253
  0.60441244 -0.06775686  0.30922186 -0.6451584   0.12982884  0.49422264
 -0.20773992 -0.0071213   0.28181255  0.17880657 -0.4701702  -0.21058184
  0.04288028  0.40837985 -0.16754697  0.03263042  0.1378905  -0.06758076
  0.10180859 -0.08672407  0.17992194 -0.54687285 -0.02258335  0.43992072
  0.04648967  0.2235285   0.11746478  0.31428832  0.03315678  0.17543867
  0.14622787  0.16075253 -0.05332081  0.00755161 -0.5049254   0.04485638
  0.47373998 -0.22110698  0.00713356  0.06846191  0.14417748 -0.3666916
 -0.015596    0.07934552 -0.12319505 -0.17152849 -0.12919569  0.20209178
  0.03994894 -0.25163585  0.17417127 -0.54030806 -0.24897869  0.02182715
 -0.07070021 -0.5658201   0.18352306 -0.10867637 -0.31634423  0.18005854
  0.16628146  0.24519767 -0.15121606 -0.0582212   0.03386037 -0.19982009
  0.02680789 -0.41075242 -0.13789912  0.1498023  -0.416601   -0.04569114
  0.1193767   0.21816343  0.02439778  0.04703938  0.19352588  0.26532575
  0.43479425 -0.11796972 -0.10264075 -0.00331617 -0.23373751  0.09757924
 -0.50625944 -0.01705536 -0.06464648 -0.0394787   0.19117184  0.43347842
 -0.13402864 -0.17906636 -0.39424917  0.26618734 -0.34142143  0.2494698
  0.58812535 -0.04687887  0.5742277   0.29069704  0.35103515  0.21420854
  0.25253567 -0.06867042  0.00953802  0.17195158  0.20410936  0.19856676
  0.03561848  0.09794792 -0.19248834  0.37929153  0.04173937  0.02912236
  0.17693534 -0.3594277   0.5196712  -0.2859592  -0.08268708 -0.01759891
  0.40811563 -0.11668368  0.07810187  0.01208097 -0.23575637  0.36255383
 -0.20078233 -0.04518057  0.08804779 -0.1058567  -0.01254618 -0.6139525
 -0.19634041  0.07230501 -0.26931033  0.12831843  0.06892733 -0.15556955
 -0.26752102  0.13280943  0.0328673  -0.32790816  0.12312557  0.06404012
 -0.28202972  0.07879183  0.24484643 -0.35100955 -0.09955816 -0.10735203
  0.6877928   0.16366881  0.45445597 -0.6440518   0.26120013 -0.13260713
 -0.2764777   0.6716771  -0.13610122 -0.03792198 -0.41567075  0.5147532
  0.38874954 -0.1627942   0.202694   -0.10618868 -0.42424494 -0.28965974
  0.31328404  0.03013022 -0.12628353 -0.19816473  0.04987486  0.23040472
 -0.2192533   0.18375607 -0.10245392  0.03962967 -0.19882599 -0.08764304
 -0.48986435  0.15675598  0.05006883 -0.27108312 -0.17553881  0.03580832
  0.04694828 -0.15372199 -0.00424924 -0.2573869   0.1652124   0.6170114
 -0.16767073  0.16079071  0.01521332  0.02236715 -0.5531734  -0.05879026
 -0.20356861  0.5035516  -0.08497377 -0.31863874  0.53176117  0.1431033
 -0.21814835  0.19248402 -0.2867818   0.04485949  0.24694437 -0.18269096
 -0.3653945  -0.118331    0.13130072  0.3669492  -0.3386315   0.02118278
 -0.52998924  0.35850435  0.41662586 -0.2836836  -0.17558128  0.0545296
  0.09052198 -0.09479763 -0.09896143 -0.06285098  0.30825126 -0.00351229]"
ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported. stat:awaiting response type:bug stale comp:keras TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Window 10

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to implement [this](https://keras.io/examples/vision/image_classification_with_vision_transformer/) in tensorflow 2.13 but getting this error ""**ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported**"" after searching a lot I got to know we have to use -1 in case of None but even after trying that it was not working  I am getting this error in custom Patches layer 

patches = tf.reshape(patches, [batch_size, -1, patch_dims])


can anybody please help?

### Standalone code to reproduce the issue

```shell
class Patches(layers.Layer):

    def __init__(self, patch_size, **kwargs):
        super(Patches, self).__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0] # Get the Batch Size
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1], # only along the Height and Width Dimension
            strides=[1, self.patch_size, self.patch_size, 1], # The next patch should not overlap the previus patch
            rates=[1,1,1,1],
            padding='VALID'
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        
        return patches
    
    def get_config(self):
        config = super().get_config()
        config.update({
            ""path-size"": self.patch_size,
        })
        return config
```


### Relevant log output

```shell
history = model.fit(
     27     train_generator,
     28     validation_data=valid_generator,
     29     batch_size=BATCH_SIZE,
     30     epochs=NUM_EPOCHS,
     31     callbacks=[
     32         checkpoint_callback, 
     33         tf.keras.callbacks.EarlyStopping(patience=5, monitor='val_Accuracy', mode='max' ,restore_best_weights=True)
     34     ],
     35 )
     37 model.load_weights(checkpoint_filepath)
     38 _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)

File ~\AppData\Roaming\Python\Python39\site-packages\keras\src\utils\traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~\AppData\Local\Temp\__autograph_generated_filelwsbm473.py:15, in outer_factory.<locals>.inner_factory.<locals>.tf__train_function(iterator)
     13 try:
     14     do_return = True
---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16 except:
     17     do_return = False

File ~\AppData\Local\Temp\__autograph_generated_filewl0d9m3r.py:13, in outer_factory.<locals>.inner_factory.<locals>.tf__call(self, images)
     11 patches = ag__.converted_call(ag__.ld(tf).image.extract_patches, (), dict(images=ag__.ld(images), sizes=[1, ag__.ld(self).patch_size, ag__.ld(self).patch_size, 1], strides=[1, ag__.ld(self).patch_size, ag__.ld(self).patch_size, 1], rates=[1, 1, 1, 1], padding='VALID'), fscope)
     12 patch_dims = ag__.ld(patches).shape[-1]
---> 13 patches = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)
     14 try:
     15     do_return = True

ValueError: in user code:

    File ""C:\Users\aksh1\AppData\Roaming\Python\Python39\site-packages\keras\src\engine\training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""C:\Users\aksh1\AppData\Roaming\Python\Python39\site-packages\keras\src\engine\training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""C:\Users\aksh1\AppData\Roaming\Python\Python39\site-packages\keras\src\engine\training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""C:\Users\aksh1\AppData\Roaming\Python\Python39\site-packages\keras\src\engine\training.py"", line 1080, in train_step
        y_pred = self(x, training=True)
    File ""C:\Users\aksh1\AppData\Roaming\Python\Python39\site-packages\keras\src\utils\traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""C:\Users\aksh1\AppData\Local\Temp\__autograph_generated_filewl0d9m3r.py"", line 13, in tf__call
        patches = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(patches), [ag__.ld(batch_size), -1, ag__.ld(patch_dims)]), None, fscope)

    ValueError: Exception encountered when calling layer 'patches_3' (type Patches).
    
    in user code:
    
        File ""C:\Users\aksh1\AppData\Local\Temp\ipykernel_11744\2749589268.py"", line 17, in call  *
            patches = tf.reshape(patches, [batch_size, -1, patch_dims])
    
        ValueError: Tried to convert 'shape' to a tensor and failed. Error: None values not supported.
    
    
    Call arguments received by layer 'patches_3' (type Patches):
       images=tf.Tensor(shape=(None, None, None, None), dtype=float32)
```
",True,"[-3.23032975e-01 -4.22672331e-01 -2.43815303e-01  2.26430237e-01
  1.13268942e-01 -4.63736504e-01 -5.18002324e-02 -1.61708929e-02
 -3.42145503e-01 -3.29631865e-01  1.17576353e-01 -2.63679326e-01
 -2.05640927e-01  7.11755827e-02 -1.21752396e-01  3.50720793e-01
 -3.99163306e-01 -1.30118951e-02  2.35052124e-01  8.30212906e-02
 -2.32900590e-01  2.78603174e-02 -2.09305838e-01  1.96225792e-01
  2.42904313e-02  1.26983538e-01 -1.99123681e-01 -1.51740313e-01
 -6.62948098e-03  8.62095505e-02  3.01398039e-01 -3.13809104e-02
 -1.84985444e-01  1.05362073e-01 -7.29869306e-02  1.71545833e-01
 -2.46113896e-01 -1.53364753e-02 -2.06431210e-01  4.92521226e-02
  4.69170213e-02  7.39725828e-02  1.94987521e-01 -2.98635483e-01
  2.35270470e-01 -1.05929382e-01  1.14205942e-01 -2.72715241e-01
 -4.26017717e-02 -1.66309252e-01 -1.17199473e-01 -8.17465484e-02
 -4.33827758e-01 -4.80362535e-01 -2.37556159e-01  2.16109380e-02
  1.05903678e-01 -4.63007875e-02 -8.18434358e-02 -8.74181390e-02
  3.93930003e-02  4.94390056e-02  1.34617418e-01 -1.22644931e-01
 -3.53836268e-02  2.08339700e-03  3.33641708e-01 -3.91965285e-02
  4.95591640e-01 -1.96275875e-01  2.68989950e-01 -1.28429811e-02
 -2.87919521e-01  2.93404069e-02  6.81356490e-02  1.79524153e-01
 -3.72449942e-02  2.20768914e-01  3.45962584e-01 -3.22254241e-01
 -8.53849575e-02 -1.69949353e-01 -1.06477082e-01 -2.66646147e-01
  3.04484993e-01 -1.36175573e-01  4.16087508e-01  9.18758959e-02
  3.95778775e-01 -1.76430568e-01  5.62023878e-01  4.61631060e-01
 -9.77854878e-02  1.52043998e-01  4.77968603e-01  1.64915413e-01
  9.18711200e-02  1.75743774e-01  3.92223224e-02 -2.58356035e-01
 -2.41323858e-01 -1.35795087e-01 -9.02241915e-02  1.86301023e-01
 -6.90755546e-02  3.61925783e-03  2.17935801e-01 -1.15120947e-01
  1.87944204e-01 -1.13778897e-01  2.40221545e-01 -1.39490083e-01
  2.98116505e-01 -5.43402322e-02  6.83055073e-02 -6.36241511e-02
 -1.66078806e-02  6.24007061e-02 -1.06805250e-01  7.43025184e-01
  5.34628890e-03 -1.05293766e-01  4.26005572e-02  3.28709126e-01
  3.56265038e-01  5.64690270e-02 -1.48608670e-01  5.42735793e-02
  8.52709115e-02 -2.02626735e-03  1.38128996e-01  5.32570183e-02
  2.06264630e-02  2.47693032e-01 -1.48365334e-01  2.05436535e-03
 -2.05907464e-01 -1.74831465e-01 -2.34489113e-01 -2.88308263e-01
 -1.21833548e-01  1.01778761e-01 -2.17886880e-01 -4.93092030e-01
  1.80598229e-01  5.44011556e-02 -2.11736351e-01  1.31613165e-01
 -1.86837688e-01  2.24260345e-01 -1.49627715e-01  8.47042724e-02
 -1.98951527e-01  4.50509071e-01  1.16246983e-01  1.79526210e-01
  2.26023078e-01 -1.85396314e-01  4.78023067e-02 -6.72011495e-01
  2.31606707e-01  3.74575973e-01 -2.05773592e-01 -1.46416277e-01
  1.73478305e-01  2.32801124e-01 -4.06612098e-01 -1.70921609e-01
  1.33621812e-01  3.30567330e-01 -6.06781989e-03 -1.43420603e-02
 -3.99340577e-02  4.29128259e-02  1.39227390e-01  1.06472865e-01
  1.88086778e-01 -6.40149236e-01 -2.28793919e-01  3.79910558e-01
  1.30848289e-01  1.46260709e-01  1.74856186e-02  1.68371677e-01
 -1.99413579e-03  7.21008051e-03  1.98638700e-02 -2.56792642e-03
 -1.67180270e-01  7.04130083e-02 -3.15315157e-01 -1.84310257e-01
  2.38180920e-01 -1.67278558e-01 -3.24405804e-02  1.39428705e-01
  1.45571619e-01 -3.28607857e-01  9.90823470e-03  3.02335471e-02
 -1.87330961e-01 -2.32615218e-01 -3.76267582e-02 -2.53838375e-02
  7.78510123e-02 -5.82821488e-01 -5.19249737e-02 -3.16839874e-01
 -4.08408403e-01  3.24745893e-01  9.04622972e-02 -7.66609907e-01
  1.69249550e-01 -7.37205222e-02 -3.42041492e-01  2.17982739e-01
  3.28684717e-01  1.48352638e-01 -2.08796099e-01  1.99642405e-01
  6.03295453e-02 -1.88041151e-01 -2.82670055e-02 -3.62960130e-01
 -2.27346718e-01  1.67167693e-01 -2.93024212e-01  5.39330244e-02
  2.08390988e-02  3.31001133e-01  1.10069484e-01  3.22035700e-03
  2.25731820e-01  2.38934219e-01  4.04397100e-01 -1.71123892e-01
 -2.04933524e-01 -7.39203468e-02 -5.89585751e-02 -5.70655093e-02
 -6.42527461e-01  3.04292664e-02 -2.03902274e-02 -1.59056246e-01
  2.02875897e-01  3.70585978e-01 -1.46569759e-01 -5.03938980e-02
 -5.24303019e-01  2.25675493e-01 -2.86074638e-01  1.29396856e-01
  3.98049951e-01  2.29881048e-01  6.42161965e-01  6.45793229e-02
  3.14947844e-01  1.40384972e-01  1.96448684e-01 -1.63993478e-01
  3.68987560e-01  3.75050716e-02  4.37231995e-02  3.68855029e-01
  9.19928700e-02  2.90401459e-01 -3.08707118e-01  5.56097746e-01
 -2.89047211e-02 -1.28976941e-01  3.23322415e-01 -3.66137475e-01
  5.59767246e-01 -3.55731577e-01  7.70826042e-02 -2.21292421e-01
  2.00451314e-01  1.30555943e-01 -4.31470796e-02  1.80326644e-02
  1.08547583e-01  3.55646729e-01 -4.66186792e-01 -9.22279730e-02
 -1.08647294e-01 -7.07821399e-02 -9.64144468e-02 -6.91447318e-01
 -3.78198512e-02  1.47496745e-01 -1.61225691e-01  3.10184956e-01
  1.42680496e-01 -7.26559535e-02 -1.76018193e-01  1.15262337e-01
  9.56748128e-02 -1.65077448e-01  1.22346856e-01  2.75801867e-01
 -1.46322101e-01  1.18731290e-01  3.72313440e-01 -3.69558334e-01
 -2.11343095e-02 -1.89946443e-01  5.50763786e-01  1.87914714e-01
  3.72651696e-01 -3.97089362e-01  8.18046927e-02 -1.27585188e-01
  1.53687214e-02  6.82044923e-01 -4.89863940e-02 -1.31653678e-02
 -1.25502482e-01  7.44253278e-01  2.82417625e-01  8.49485397e-04
  8.45869333e-02 -4.33720797e-02 -2.88580775e-01  4.49987948e-02
  1.08836159e-01 -9.95548368e-02  2.44091198e-01 -4.97859299e-01
 -5.47511429e-02  1.69491068e-01 -3.46649349e-01 -2.63889253e-01
 -1.33789778e-01 -7.81409889e-02 -3.93909663e-02 -1.65636390e-01
 -3.92090440e-01  3.24672669e-01  5.42745069e-02 -3.31858516e-01
 -2.53916949e-01 -7.95393735e-02 -7.83487260e-02 -1.68805540e-01
  4.80113141e-02 -4.32051897e-01  2.93617070e-01  5.58041811e-01
 -1.61192700e-01  1.51493400e-01  3.70075461e-03  1.13016874e-01
 -2.78194308e-01  1.26473894e-02 -8.76568109e-02  1.81817263e-01
 -2.11356431e-02 -8.79496336e-02  4.77941573e-01  5.05289257e-01
 -2.75094330e-01  1.08872510e-01 -1.92418009e-01  5.53301908e-03
  2.24013776e-01 -1.92925751e-01 -2.71072030e-01 -2.74155259e-01
  1.60303950e-01  3.38777035e-01 -1.34472445e-01  2.68240690e-01
 -2.78368443e-01  2.95394272e-01  5.29637337e-01 -2.64095634e-01
 -3.30133617e-01  4.56970602e-01  1.78281754e-01 -2.29549885e-01
  1.74843706e-04 -2.66289338e-03  1.28328413e-01 -4.50712666e-02]"
SpectralNormalization layer is not trainable. Please help (OperatorNotAllowedInGraphError: Exception encountered when calling layer 'spectral_normalization' (type SpectralNormalization).) type:bug comp:keras TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.3 LTS and Google Colab

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cudatoolkit=11.8.0, nvidia-cudnn-cu11==8.6.0.163

### GPU model and memory

_No response_

### Current behavior?

SpectralNormalization layer is not trainable. Whenever I try to use the ""model.fit"" method, TF outputs the error ""Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.""

Please help

Best

Kav

### Standalone code to reproduce the issue

```shell
LINK TO COLAB NOTEBOOK:
https://colab.research.google.com/drive/1TYoNIrrpk-bLBqpzNJXq5VOI6Mpln5Ty?usp=sharing


STANDALONE CODE:
batch = 1
height = 10
width = 10
channels = 1
filters = 4
kernel_size = 3

x_input = Input(shape=(height, width, channels))
conv2d = SpectralNormalization(Conv2D(filters, kernel_size))
x_output = conv2d(x_input)
model = Model(x_input, x_output)
model.compile(loss='mse')

x = np.random.rand(batch, height, width, channels)
y = np.random.rand(batch, height, width, filters)

model.fit(x, y)
```


### Relevant log output

```shell
---------------------------------------------------------------------------

OperatorNotAllowedInGraphError            Traceback (most recent call last)

<ipython-input-6-d3dc977168f5> in <cell line: 1>()
----> 1 model.fit(x, y)

1 frames

/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py in autograph_handler(*args, **kwargs)
     50     except Exception as e:  # pylint:disable=broad-except
     51       if hasattr(e, ""ag_error_metadata""):
---> 52         raise e.ag_error_metadata.to_exception(e)
     53       else:
     54         raise

OperatorNotAllowedInGraphError: in user code:

    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function  *
        return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step  **
        outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1080, in train_step
        y_pred = self(x, training=True)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None

    OperatorNotAllowedInGraphError: Exception encountered when calling layer 'spectral_normalization' (type SpectralNormalization).
    
    Using a symbolic `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.
    
    Call arguments received by layer 'spectral_normalization' (type SpectralNormalization):
       inputs=tf.Tensor(shape=(None, 10, 10, 1), dtype=float32)
       training=True
```
",True,"[-0.22816813 -0.43699908 -0.14570644 -0.09857053  0.2073689  -0.31451702
 -0.0988514  -0.02588435 -0.4485085  -0.10535771  0.2242203  -0.21957898
 -0.10235341  0.22196662 -0.09346161  0.6590531  -0.21755497 -0.05143081
 -0.02251218  0.07884045  0.0839877   0.0453589  -0.05580702 -0.02594057
  0.11553049 -0.06907442 -0.09302128 -0.37697193  0.15730096  0.17134455
  0.16592748  0.07456183 -0.26481196  0.17114484  0.00627871  0.08350032
 -0.23777314 -0.04454007 -0.27021632  0.08713219  0.15158036  0.21788904
 -0.14540389 -0.12325129 -0.22282311 -0.35359365 -0.06346706  0.05122604
 -0.20045236 -0.2061927   0.13672273 -0.07063087 -0.4809193  -0.28668022
 -0.13637865 -0.05268387 -0.13910118  0.03930648 -0.02200398  0.10663339
  0.10062784 -0.09782213  0.032635   -0.16289765  0.13045947  0.10541953
  0.24683599 -0.04914784  0.5475381  -0.36336493 -0.02772267 -0.01605746
 -0.26565516  0.26332963  0.301915    0.30890545 -0.1148892   0.25209162
  0.00995657 -0.11191515  0.03090258 -0.09506398 -0.02487747 -0.1992065
  0.16953935  0.05723099  0.08895692  0.09480314  0.15894505 -0.06120835
  0.56009424  0.17151949 -0.29016396  0.19141883  0.26010108 -0.14753707
  0.09602877  0.05841684  0.04904774 -0.14102817  0.07165033 -0.43587288
 -0.29257143  0.5050268   0.04820366 -0.00500166  0.26822686 -0.09542905
 -0.05772801  0.05055611  0.20176649 -0.19859919  0.05796543 -0.13056478
  0.09211317 -0.02315465  0.06146061  0.36426342 -0.12953739  0.53332704
  0.01985934 -0.08072785  0.10745011  0.07213674  0.46641564  0.15839976
 -0.02635066  0.05484445  0.21942878  0.18396196  0.02564954  0.16940367
  0.19334789  0.09598243  0.02734392  0.16614518 -0.03978711 -0.0506603
 -0.18033162 -0.0557993  -0.23248577  0.27087235  0.00185123 -0.32246774
  0.20795    -0.07158111 -0.25869903  0.13581571 -0.10128304  0.07266276
  0.01989734 -0.01092234 -0.29354703  0.3498897   0.03626564  0.13940218
  0.38792413 -0.04289535  0.09691234 -0.48604333  0.30381703  0.2594934
 -0.08773873 -0.01653198  0.37008628  0.0306958  -0.22611459 -0.24665809
 -0.16831385  0.06258447  0.11111373 -0.04851508  0.11591619  0.02193774
  0.10431631 -0.14444375  0.1271882  -0.5548513  -0.07416204  0.06303163
  0.1674732   0.23880488  0.12973931  0.21990372  0.01149282 -0.16480123
  0.20890167  0.05507997 -0.17199427  0.01643514 -0.26971442 -0.39971805
  0.2858693  -0.3611257  -0.04725824 -0.10184116  0.15504643 -0.43412212
 -0.08426365  0.08788605  0.16169304 -0.03040734  0.08587655  0.10846221
  0.03923214 -0.1370027  -0.02541147 -0.2646454  -0.63900495  0.33679315
 -0.03749971 -0.6679824   0.0992284  -0.20908183 -0.30600804  0.2606126
 -0.08688315  0.18434039 -0.2127055   0.3404032   0.11043724 -0.07266116
 -0.16481799 -0.23752877 -0.22072504  0.03184075 -0.20392922 -0.06979084
 -0.09715046  0.18424675 -0.1181382   0.06883265  0.13570717  0.15847185
  0.137665   -0.0235782  -0.1219627  -0.07493274 -0.17514852 -0.14618033
 -0.4656083   0.24423341  0.05205755 -0.2461893   0.02686405  0.17823735
 -0.12043124 -0.18886805 -0.2797902   0.10181877 -0.37258774  0.05971254
  0.39682212  0.12665284  0.40469357  0.28742623  0.17514715 -0.2538768
  0.18685836  0.1180755   0.25947466  0.15510729 -0.00951847  0.37394214
  0.02133182  0.3487717  -0.18142864  0.34598285  0.08920453 -0.35565913
  0.19995001 -0.33117938  0.28005695 -0.367832    0.21474232  0.07679178
  0.32506    -0.1152253  -0.02668728  0.07865323 -0.00788615  0.27716637
 -0.1536835  -0.2318052  -0.00602958 -0.11387693 -0.2787779  -0.52810496
  0.01024327  0.13517798  0.13884017  0.1765087   0.2381726   0.04710991
 -0.34811687  0.14273533 -0.0170227  -0.11173554  0.26448178 -0.02602393
 -0.05155437  0.09700267  0.24897528 -0.35818654 -0.08308056 -0.03710092
  0.35035366  0.01298314  0.64146817 -0.46524215  0.30030283  0.01476696
 -0.08591969  0.42489398  0.00627052  0.01451732 -0.39439374  0.5620092
  0.3256071   0.08073656  0.09595188 -0.02695434 -0.29072532 -0.18936636
  0.32435766 -0.13531041 -0.03526404 -0.26304936 -0.03699079  0.10083531
 -0.14273445  0.01226831 -0.05645416  0.10299251  0.01574332  0.01851656
 -0.3861761   0.34068662  0.0868935  -0.3104426  -0.1343717  -0.07146714
 -0.19928275 -0.3484013   0.05225245 -0.243418    0.28808668  0.41663587
  0.0464978   0.24880311 -0.07670684  0.11226413 -0.27674     0.0244283
 -0.41887245  0.37291494 -0.02939233 -0.26910847  0.23366703  0.36591992
 -0.33515072  0.09633866 -0.13825029 -0.06144867  0.2825278  -0.10783002
  0.06567991 -0.06363288  0.03682917  0.61257386 -0.25066048  0.21163346
 -0.20807189  0.01651119  0.1668919  -0.42150837 -0.11603619 -0.06004252
 -0.07995204 -0.02353576 -0.1347306   0.03947146 -0.08241311 -0.08841629]"
TFLite benchmark tool with default cmake build shows weird time duration on DEPTHWISE_CONV_2D  stat:awaiting response type:bug stale comp:lite TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.9.3, tf 2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.0

### GCC/compiler version

gcc 9.4.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the guide for [build_cmake](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md)
`cmake ../tensorflow_src/tensorflow/lite`
then build the benchmark tool & label_image:
```
cmake --build . -j -t benchmark_model
cmake --build . -j -t label_image
```
I tested using the model [here](https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_1.0_224_quant_and_labels.zip).

The results shows:
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       15	    38.265	    58.415%	    58.415%	     0.000	       15
	       DEPTHWISE_CONV_2D	       13	    27.230	    41.569%	    99.985%	     0.000	       13
	         AVERAGE_POOL_2D	        1	     0.007	     0.011%	    99.995%	     0.000	        1
	                 SOFTMAX	        1	     0.003	     0.005%	   100.000%	     0.000	        1
	                 RESHAPE	        1	     0.000	     0.000%	   100.000%	     0.000	        1

In theory, 13 x DEPTHWISE_CONV_2D should consume MUCH less time than 15 CONV_2, but it is not the case here.

However, if I use the pre-built binary from [here](https://storage.googleapis.com/tensorflow-nightly-public/prod/tensorflow/release/lite/tools/nightly/latest/linux_x86-64_benchmark_model), the result is correct.


### Standalone code to reproduce the issue

```shell
benchmark_model --graph=./mobilenet_quant_v1_224.tflite --enable_op_profiling=true
```


### Relevant log output

```shell
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	       15	    38.265	    58.415%	    58.415%	     0.000	       15
	       DEPTHWISE_CONV_2D	       13	    27.230	    41.569%	    99.985%	     0.000	       13
	         AVERAGE_POOL_2D	        1	     0.007	     0.011%	    99.995%	     0.000	        1
	                 SOFTMAX	        1	     0.003	     0.005%	   100.000%	     0.000	        1
	                 RESHAPE	        1	     0.000	     0.000%	   100.000%	     0.000	        1
```
",True,"[-3.39298785e-01 -3.60174924e-01 -2.99933225e-01  8.62650424e-02
  2.23882571e-01 -2.23031059e-01 -2.40640506e-01  3.16809773e-01
 -3.80470514e-01 -4.87476468e-01 -6.19375370e-02 -5.35175130e-02
 -1.08669400e-01 -4.69206274e-02 -3.60443056e-01  2.52478719e-01
 -2.98553929e-02 -2.89428055e-01  2.93069869e-01 -5.11977412e-02
 -2.78025568e-01  1.06144562e-01 -3.07999790e-01  3.00641775e-01
  2.04640895e-01  1.59741178e-01 -2.15585083e-01  2.06381440e-01
 -1.23688146e-01  2.19659626e-01  9.17082727e-02  2.41113111e-01
 -3.12565267e-01  8.84396210e-02  1.29147023e-01  1.50477409e-01
 -2.46325731e-01  3.65903489e-02 -4.22957391e-01 -1.06899023e-01
 -2.90211830e-02  1.71346977e-01  1.75836887e-02  6.84645250e-02
 -9.77965444e-02  1.12508163e-01  1.23532772e-01  1.42882884e-01
 -2.11059868e-01  1.21512990e-02  1.53016709e-02 -9.33748111e-03
 -2.78272629e-01 -4.27374721e-01 -1.73492655e-02  1.79506108e-01
 -5.34907952e-02 -1.02588192e-01  3.25897336e-01  7.21634030e-02
  1.14065915e-01 -1.51124686e-01 -7.31204972e-02 -1.16179921e-02
  4.14930284e-01  1.03360146e-01  2.32825369e-01 -1.39703855e-01
  1.94946021e-01 -8.09133723e-02 -1.32760331e-02 -9.31522474e-02
 -3.56593937e-01 -3.31033990e-02  1.47841573e-01  7.46963844e-02
 -2.45705560e-01  5.23258112e-02  9.32550356e-02 -2.44762152e-01
  8.20134804e-02 -4.59007025e-01 -5.96227646e-02 -1.46771252e-01
  1.62844032e-01 -1.84120774e-01  4.30430561e-01  1.51991084e-01
  2.70970970e-01 -3.01353246e-01  5.17831624e-01  6.90973759e-01
 -1.37016803e-01  3.16924632e-01  2.92947114e-01  1.64993666e-03
  1.67537257e-01  3.34062457e-01  6.49413094e-03  2.24649712e-01
 -1.01834603e-01 -2.80964375e-01 -1.31771818e-01  2.79449672e-01
 -8.86351615e-02 -2.11035997e-01  3.01553816e-01  6.17806129e-02
  4.19952124e-02  1.88737273e-01  2.45671928e-01  1.45287156e-01
  2.60791004e-01 -9.37086046e-02  2.01525450e-01  4.60323319e-02
 -2.47277524e-02  9.56925675e-02  1.76711515e-01  4.53915775e-01
  8.39974433e-02 -6.32951781e-02 -3.12838197e-01  1.04777440e-02
  4.43507433e-01  2.50542492e-01 -4.30881940e-02  9.00673643e-02
  3.27279806e-01 -2.26959996e-02 -1.41762838e-01  1.42384648e-01
 -1.37426257e-01  2.94997573e-01 -1.02083229e-01  2.19875798e-01
  1.13231346e-01  5.85923381e-02 -9.88402814e-02 -2.06124097e-01
 -3.64376187e-01  9.08623412e-02 -1.27703324e-01 -5.08714259e-01
  1.50744200e-01  1.37623712e-01 -2.64690995e-01  1.33396134e-01
 -2.48042211e-01 -9.64132920e-02 -1.27932861e-01  2.18628898e-01
 -1.45093977e-01  5.44806600e-01  1.49596155e-01  2.91135237e-02
  2.25716144e-01 -7.47857913e-02  2.16390312e-01 -3.82184684e-01
  5.23917452e-02  2.86183238e-01 -2.78612860e-02 -7.28438273e-02
  3.00002396e-01  5.80098107e-02 -3.56369436e-01 -1.24585658e-01
  1.95103839e-01  3.98097396e-01 -1.50376111e-01 -2.38880679e-01
 -4.19818237e-02  6.56679943e-02  1.92513645e-01 -6.73231035e-02
  1.96402937e-01 -3.70016813e-01 -1.51305214e-01  9.27862898e-02
  1.39826648e-02  1.31775811e-02 -9.42837000e-02  1.88175365e-01
 -2.53950357e-01 -1.05940953e-01  2.07956403e-01  2.58260131e-01
 -1.80994883e-01 -1.40643969e-01 -4.80068296e-01 -1.52926102e-01
  4.81120437e-01  1.69712245e-01  1.84102990e-02 -1.77565143e-02
  2.86220998e-01 -9.21528712e-02  2.02967197e-01  1.20679744e-01
  6.06365204e-02 -8.11669976e-03  7.33766854e-02 -1.01044856e-01
  3.54358375e-01 -1.82503402e-01  2.00591348e-02 -3.64053965e-01
 -2.19214112e-01 -1.70924351e-01 -3.23792472e-02 -3.15221012e-01
  4.89844643e-02 -3.90115023e-01 -2.79379457e-01  8.94561633e-02
 -1.57543927e-01 -1.41506553e-01 -2.91261405e-01  1.81502163e-01
  5.32448478e-02 -1.41394451e-01 -1.57128312e-02 -4.65346247e-01
 -5.93558028e-02  3.84534746e-02 -6.08462930e-01  3.74964595e-01
  4.77774329e-02  1.15645960e-01 -1.81177855e-01  2.46101543e-01
  4.71610516e-01  6.25785813e-02  3.15086275e-01 -1.38257608e-01
 -2.26791158e-01 -2.91532606e-01 -1.96879625e-01  1.00269571e-01
 -2.97029346e-01 -2.89631277e-01 -6.87082186e-02 -1.17557004e-01
  4.47470397e-01  9.16592777e-02 -2.69316822e-01 -1.01579420e-01
  4.26433906e-02  4.11608934e-01 -2.28500366e-01  8.79232287e-02
  4.07288224e-02  1.46751314e-01  3.77776593e-01  3.20513189e-01
  2.54438519e-01  1.80253088e-01  1.63269475e-01 -1.43523917e-01
  3.07828665e-01  2.59578288e-01  1.92774981e-01  7.38378406e-01
  2.73490608e-01  4.66125369e-01 -2.91830599e-01  2.57127911e-01
 -7.27772992e-03 -1.45826824e-02  7.97790959e-02 -3.48081857e-01
  5.26049495e-01 -1.64751709e-01  1.32514328e-01 -3.44303668e-01
  4.65968847e-01 -2.12802008e-01 -9.66030806e-02  1.66906625e-01
  2.02978328e-02  1.32127166e-01 -1.00711524e-01  1.39121622e-01
 -9.65599343e-03  1.29320338e-01 -1.76051795e-01 -5.74848354e-01
 -2.56876469e-01  1.97935924e-01 -2.56069183e-01  5.60584627e-02
  9.21612326e-03  2.19608128e-01 -3.56996685e-01  2.43487269e-01
 -3.13571543e-02 -2.75671780e-01  1.65344644e-02 -4.03077975e-02
  4.12364155e-02  3.36606979e-01  1.87656313e-01 -3.61304402e-01
 -1.76026255e-01  2.22923905e-01  3.49010587e-01 -3.20479739e-04
  3.51301581e-01 -2.61167467e-01 -6.90753385e-02  9.37751010e-02
 -2.04008847e-01  4.15125102e-01 -2.04083659e-02 -2.50236485e-02
 -4.19596016e-01  4.61301684e-01 -4.29480337e-02 -8.47427845e-02
  1.66332662e-01  2.17581168e-03 -4.93625820e-01  2.91962735e-02
  2.92503119e-01  4.91003655e-02  2.39158049e-01 -1.17580786e-01
  1.20484233e-01  3.52995209e-02 -5.05344309e-02 -3.24375033e-01
 -2.61789709e-01 -1.79645628e-01 -2.51870513e-01 -1.03317790e-01
 -3.02264631e-01  7.59541988e-02 -2.68397540e-01 -2.98756659e-01
 -1.00473687e-01 -2.00452238e-01  1.47026703e-02 -3.68904412e-01
 -7.12068826e-02 -1.85353935e-01  6.28125370e-02  3.28784227e-01
  1.27100006e-01 -8.07569996e-02  5.20840622e-02  7.09654167e-02
 -1.05648607e-01 -5.37311435e-02 -8.01061615e-02  2.76876926e-01
 -3.57935399e-01 -5.54750562e-02  2.79661328e-01  4.36543435e-01
 -2.98753619e-01  1.64533302e-01 -3.27433616e-01 -3.19274515e-02
 -8.54899585e-02 -1.64529040e-01 -4.48000401e-01  4.37697396e-02
  1.56118587e-01  1.19424626e-01  3.23447622e-02  4.11309361e-01
 -1.29845738e-01  9.79405344e-02  4.56389666e-01 -1.67825490e-01
  1.82438046e-01  1.52612448e-01 -5.63371275e-03 -3.70149225e-01
  1.67031772e-02 -4.88113351e-02 -1.74850151e-01 -6.72005415e-02]"
A call of the model which is reloaded from SavedModel format produces a TypeError stat:awaiting response type:bug stale TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

_No response_

### Current behavior?

A call of the model which is reloaded from SavedModel format produces a `TypeError: '_UserObject' object is not callable`.
All necessary code is in [this tutorial](https://www.tensorflow.org/text/tutorials/transformer). Below I show code lines from there, where the error occurs.

### Standalone code to reproduce the issue

```shell
reloaded = tf.saved_model.load('translator')
reloaded('este  o primeiro livro que eu fiz.').numpy()
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/molokanov/tensorflow/check.py"", line 13, in <module>
    reloaded('este  o primeiro livro que eu fiz.').numpy()
TypeError: '_UserObject' object is not callable
```
",True,"[-5.58667183e-01 -3.23994011e-01 -3.36740255e-01  1.22364298e-01
  3.01872909e-01 -3.15061152e-01  1.40557950e-02  2.01302432e-02
 -2.61679947e-01 -1.74215734e-01  2.63064831e-01 -2.66305488e-02
 -1.05996132e-01 -2.77911127e-03 -2.35767663e-01  2.61732697e-01
 -2.51126170e-01 -8.40816945e-02  1.62429750e-01  3.24961275e-01
 -7.41298273e-02 -1.42065972e-01 -1.52594149e-01  1.69269174e-01
  2.15744823e-01  2.04298198e-01 -1.68801650e-01 -5.70810959e-02
 -6.49760216e-02  1.51831746e-01  2.08305329e-01  5.14117181e-02
 -1.97087780e-01  5.67843989e-02  1.82949483e-01  2.94976234e-01
 -4.30098593e-01 -1.25241131e-01 -3.21996778e-01  1.43439397e-02
 -6.01875037e-02  1.88965909e-02  7.07390904e-02 -1.53346747e-01
 -2.09536776e-02 -1.72508180e-01 -7.48133473e-03 -8.11780617e-03
 -1.21494327e-02 -2.80511498e-01  7.70307779e-02 -1.19334109e-01
 -4.74359393e-01 -3.65207076e-01 -2.15868488e-01 -7.36311823e-02
  2.40442961e-01 -1.52311504e-01 -8.89981389e-02  1.19642220e-01
  6.00313582e-03  4.96187583e-02  3.56821977e-02 -1.96683377e-01
  1.37551025e-01  1.49078310e-01  1.77627310e-01  5.46334721e-02
  5.65903783e-01 -3.00978899e-01  2.08112434e-01  1.64134484e-02
 -2.93586254e-01  7.55999982e-03  1.64906934e-01 -1.32061332e-01
  8.85106064e-03  1.44313514e-01  2.97294140e-01  2.40539927e-02
 -4.93619684e-03 -5.15693009e-01 -2.93161541e-01 -1.94182307e-01
  2.04600804e-02 -8.57072547e-02  4.48994637e-01  1.02987111e-01
  6.46540880e-01 -1.54893920e-01  4.36227024e-01  4.87329572e-01
 -1.43720940e-01  1.61290452e-01  2.59787977e-01  2.66631603e-01
  7.66267069e-04  3.93135160e-01  6.32523149e-02 -7.16942996e-02
  1.60528067e-02 -1.93392485e-01  7.94099271e-02  6.28618002e-02
 -2.05196384e-02 -2.64281750e-01  1.72253102e-01 -1.86686784e-01
  5.41896895e-02 -5.69370985e-02 -2.41891295e-02 -9.94425565e-02
  2.54697382e-01 -4.40725684e-02  4.95468155e-02 -1.72398295e-02
  3.65819447e-02 -1.00529015e-01 -1.95289925e-01  3.52298617e-01
  2.46401519e-01 -1.68749616e-01  1.24201924e-01  6.23445436e-02
  5.49612463e-01  8.30732137e-02 -1.37842238e-01  1.57953799e-03
  7.95767456e-02 -1.28493279e-01  3.21289092e-01  2.19845533e-01
  1.72383077e-02  1.04967944e-01 -1.31396621e-01  2.21532118e-02
 -2.55089223e-01 -1.08736157e-01 -3.86467874e-01 -2.83166409e-01
 -2.66482919e-01  1.49047598e-01 -1.38919085e-01 -6.71201169e-01
 -4.03677151e-02  9.79888961e-02 -1.32570103e-01  3.34562629e-01
 -2.28996158e-01  2.95543373e-01  6.23321999e-03 -5.39878979e-02
 -2.40060866e-01  4.45645154e-01  9.11129117e-02  2.07295328e-01
  3.40989858e-01 -4.91183177e-02  1.14348814e-01 -5.88220477e-01
 -5.20795062e-02  3.56898814e-01  1.80884823e-03 -1.76793247e-01
  1.53964669e-01  2.46550113e-01 -3.55904877e-01 -4.15756404e-02
  2.54967332e-01  5.30403376e-01 -1.16345905e-01 -1.37879342e-01
  9.39764455e-02 -1.34042472e-01  2.01007575e-01 -4.57791612e-05
  1.89998776e-01 -6.91916823e-01 -1.71932667e-01  3.73949260e-01
  3.42156917e-01  1.60856605e-01 -2.30420120e-02  1.07354775e-01
  7.18934089e-02  1.07947715e-01  1.39554009e-01  5.64851761e-02
 -2.79469728e-01 -5.75271808e-02 -2.79750854e-01  5.71089871e-02
  5.29536605e-01 -4.15335819e-02 -1.11000814e-01  3.55824605e-02
  9.95879769e-02 -1.31067634e-01  9.88493115e-02  4.22475301e-02
 -7.04918057e-02 -1.49806291e-01 -8.09079483e-02 -2.76638623e-02
  1.64119080e-01 -1.67548478e-01 -7.56518096e-02 -5.26210904e-01
 -1.53431356e-01  8.29955116e-02 -2.30979890e-01 -5.52810192e-01
  2.33633906e-01 -3.24794240e-02 -3.37603718e-01  9.21239778e-02
 -5.19501604e-02  1.31857619e-02 -1.28049925e-01  2.03421600e-02
  2.25942954e-01 -3.00716698e-01  1.64236739e-01 -4.12272751e-01
 -1.83695614e-01  2.60819107e-01 -3.30483973e-01  2.16043182e-02
  1.71961069e-01  1.93133697e-01  7.33225569e-02  1.81386918e-01
  2.51939207e-01  3.31957698e-01  3.34321499e-01 -1.77453727e-01
 -1.49343848e-01 -1.17315248e-01 -6.03550151e-02  7.83369243e-02
 -4.16122139e-01 -3.98308754e-01 -1.41510248e-01 -1.93462223e-01
  2.95497745e-01  4.79105562e-01 -2.88944483e-01  1.83315203e-02
 -3.93268108e-01  1.86775833e-01 -4.07090068e-01  3.21307719e-01
  6.05236411e-01 -1.11366073e-02  5.56738734e-01  4.44905311e-02
  3.42535883e-01  3.29490989e-01  1.72930986e-01 -1.98685229e-01
  4.63901609e-01  2.55138040e-01  2.33181715e-01  3.09070289e-01
  2.20522583e-01  1.91216081e-01 -4.17558193e-01  4.98178959e-01
 -1.30783180e-02 -6.37383685e-02  2.17888251e-01 -5.14804542e-01
  6.14341140e-01 -2.87198424e-01  1.73289120e-01 -2.30978131e-01
  3.61525387e-01 -4.38138396e-02  7.30785057e-02  6.79724365e-02
  5.07011823e-02  1.83481157e-01 -2.96520233e-01  1.32093161e-01
  9.33809113e-03 -1.69884861e-01 -6.39493689e-02 -5.25422812e-01
 -3.50425452e-01 -5.99017814e-02 -2.24005073e-01  1.58809274e-01
 -2.59471759e-02 -1.90069526e-01 -3.54806781e-01  1.10681951e-01
 -1.00719295e-02 -1.82323456e-01  1.99088007e-01  2.10219711e-01
 -2.50909328e-01 -4.55624908e-02  4.28553939e-01 -3.65073383e-01
 -2.13113129e-01 -2.87446290e-01  4.60098386e-01  1.26707703e-01
  3.57751727e-01 -5.56152105e-01  7.60662854e-02 -9.32606533e-02
 -1.68331683e-01  3.95143688e-01 -1.00678913e-02  6.35715425e-02
 -3.15438211e-01  6.52785540e-01  8.24637562e-02 -4.86232638e-02
  2.03197390e-01 -1.57835662e-01 -1.76935494e-01  4.88319099e-02
  2.35086977e-01 -1.83480009e-01  1.97898954e-01 -3.75930667e-01
  1.14875168e-01  1.00835748e-01 -3.40649784e-01 -7.42695779e-02
 -7.76008144e-03  2.34646406e-02 -2.48139471e-01 -1.83193058e-01
 -3.90564084e-01  2.42112279e-01  1.97836533e-02 -3.66658866e-01
 -1.40243337e-01 -1.26165420e-01 -1.63008764e-01 -1.74349844e-01
  1.02880254e-01 -2.57008284e-01  1.62786573e-01  4.91880655e-01
 -1.11118987e-01  2.32846409e-01  1.43153802e-01  3.08515485e-02
 -2.89971352e-01  6.49232119e-02 -1.88305110e-01  3.89749467e-01
  2.43185282e-01 -1.47524685e-01  3.15988392e-01  2.84574568e-01
  8.00977498e-02  1.25571460e-01 -3.42002332e-01  1.38900965e-01
  9.51478407e-02 -2.46552303e-01 -3.60531330e-01 -4.30569470e-01
  2.23639786e-01  1.50650233e-01 -1.08774766e-01  2.28328004e-01
 -2.07805067e-01  2.63298720e-01  4.97932792e-01 -2.95602620e-01
 -1.95200399e-01  2.33802050e-01  3.60386789e-01 -5.02744503e-02
  1.31500602e-01 -2.00419024e-01  1.98946640e-01  7.31706247e-02]"
XLA compiled `Embedding` could work for out-of-bound input stat:awaiting response type:bug stale comp:keras comp:xla TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230921

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA compiled `Embedding` could work for out-of-bound input. In the following code, the model has an embedding layer using `tf.keras.layers.Embedding(64, 128)`, which means it has 64 possible tokens (usually representing words or items, indexed from 0 to 63).

The input to it is `tf.constant([64], dtype=tf.int32)`, which is out of range because 64 exceeds the highest valid token index 63.

If we run the model without XLA compilation, it will raise an error as expected. However, after XLA compilation, the model could process such invalid input.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
""""""
XLA compiled
""""""
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(64, 128)

    @tf.function(jit_compile=True)
    def call(self, x1, x2):
        x3 = self.embedding(x1)
        return (x3 * x2)

tf.random.set_seed(42)
m = Model()
input_1 = tf.constant([64], dtype=tf.int32)
input_2 = tf.constant([[[[10.0]]]], dtype=tf.float32)
print(m(input_1, input_2))

""""""
Without XLA
""""""
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(64, 128)

    def call(self, x1, x2):
        x3 = self.embedding(x1)
        return (x3 * x2)

tf.random.set_seed(42)
m = Model()
input_1 = tf.constant([64], dtype=tf.int32)
input_2 = tf.constant([[[[10.0]]]], dtype=tf.float32)
print(m(input_1, input_2))

""""""
InvalidArgumentError: Exception encountered when calling layer 'embedding_4' (type Embedding).

{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0] = 64 is not in [0, 64) [Op:ResourceGather] name: 
""""""
```


### Relevant log output

_No response_",True,"[-5.60441911e-01 -1.49115831e-01 -6.94507658e-02 -5.13147116e-02
  1.22009695e-01 -3.46644670e-01 -2.14812234e-02 -1.34929851e-01
 -1.69001698e-01 -3.95128369e-01  1.14893228e-01  9.31541771e-02
  2.72904262e-02 -3.27700585e-01 -2.06196100e-01  4.02587324e-01
 -1.70483544e-01 -2.35068705e-02  9.36450437e-02  1.20994449e-01
  1.40500590e-02 -3.37767541e-01 -2.74636179e-01  1.07742399e-01
  3.23176324e-01  1.56993896e-01 -3.85296702e-01  1.18509710e-01
 -5.97912148e-02  6.57267198e-02  5.42320073e-01  2.34269559e-01
 -8.48324820e-02  2.02597603e-01 -1.08421475e-01  2.45647237e-01
 -1.87526628e-01 -1.70712397e-01 -3.60081255e-01  1.29322857e-01
 -3.22544500e-02  8.11354816e-02 -1.12190112e-01 -3.15687478e-01
  4.38327730e-01 -2.12045014e-01  2.67038159e-02 -3.37302327e-01
 -1.11852661e-02  1.12936512e-01 -4.31310423e-02  8.24207962e-02
 -2.68254369e-01 -4.78278697e-01 -2.12973297e-01  6.36193380e-02
 -9.27728489e-02 -1.53082490e-01 -8.93879309e-03  3.33426058e-01
  7.28463382e-02 -1.02626622e-01 -1.52375624e-02 -7.49427825e-02
  2.00199246e-01  3.14133018e-02  2.64673859e-01  1.28860444e-01
  2.95443147e-01 -5.47972322e-02  8.27641040e-02 -8.16306025e-02
 -7.45091796e-01 -5.51097691e-02 -4.09975722e-02  3.17637652e-01
 -1.08194120e-01 -2.20627010e-01  1.32117420e-01 -4.03892517e-01
  2.25176364e-01 -1.35749549e-01 -1.66642144e-02 -2.34027393e-03
  3.90820324e-01 -1.27396598e-01  3.77250493e-01  1.33038670e-01
  2.95970231e-01 -1.04952663e-01  3.31319571e-01  4.35611427e-01
 -9.19858739e-02  3.67603242e-01  5.47384262e-01  1.07894510e-01
  3.42612505e-01  1.55807137e-01 -1.74423859e-01  2.01101452e-02
 -2.15900034e-01  1.46005183e-01  7.41127692e-03 -1.99320525e-01
 -5.50590046e-02 -3.07473093e-01  2.87662983e-01 -1.59232125e-01
 -1.64712951e-01 -1.81737199e-01  7.17826039e-02 -3.04797500e-01
  1.86469853e-01  1.00369588e-01 -2.60032713e-04  5.27983718e-03
  6.58626705e-02 -2.25997288e-02  6.39289096e-02  6.14748597e-01
  3.41458209e-02 -4.41877246e-02 -1.49259657e-01  5.59791267e-01
  5.10186076e-01  2.18690112e-02 -3.35114971e-02  2.32780613e-02
  4.33747470e-03 -1.05540141e-01  1.28924996e-01  1.88941538e-01
  1.43206716e-01  2.70076990e-01 -6.04652129e-02  2.43115332e-02
 -2.34153107e-01 -2.29798570e-01 -3.36180210e-01 -2.05730349e-01
 -3.13329667e-01  3.24095517e-01  4.65983078e-02 -3.88119102e-01
  2.09794700e-01  1.71475470e-01 -2.62582541e-01  1.24918483e-03
 -2.88477703e-03 -3.67951810e-01 -1.61927104e-01 -1.46303713e-01
 -3.54726493e-01  4.59848613e-01  1.43882036e-01  5.99299781e-02
  1.31494537e-01  3.64252329e-02  6.79254904e-02 -5.27833998e-01
  3.12604487e-01  3.68976235e-01 -1.39555186e-01 -1.62292719e-01
  6.80117831e-02  2.67335832e-01 -3.70631158e-01 -1.64633453e-01
  7.50457197e-02  3.76553357e-01 -2.75275707e-02 -1.59107476e-01
 -4.94224913e-02  7.26396441e-02  2.58550882e-01 -2.09325537e-01
  1.51047170e-01 -4.32872236e-01 -1.75206885e-01  4.02981758e-01
  1.49449140e-01  5.21471053e-02  8.72883797e-02  1.51632816e-01
 -1.02926493e-01  1.11038551e-01 -3.93498763e-02  2.21139595e-01
 -2.15029970e-01  2.19972104e-01 -3.63466561e-01 -1.14780165e-01
  2.50855237e-01 -2.79308092e-02 -7.35307634e-02 -1.56717543e-02
  5.12322709e-02 -2.86994338e-01  3.35496292e-03  4.58027497e-02
 -2.39272326e-01 -1.22260384e-01  2.05630049e-01 -1.07102729e-01
  1.61102846e-01 -3.30179036e-01 -1.05356932e-01 -4.46823269e-01
 -3.30532521e-01 -7.36719370e-02  3.60121299e-03 -3.51116121e-01
  1.21529490e-01  1.54904202e-02  3.50028127e-02  4.45228547e-01
  2.66634263e-02  2.44450197e-01 -2.48921037e-01  9.11985636e-02
  1.07442439e-01 -2.06941187e-01  1.15984961e-01 -4.10414070e-01
 -1.87430561e-01  2.69739389e-01 -2.25170404e-01 -4.54523340e-02
  2.15048343e-03  5.05626082e-01  2.93554604e-01  2.88904402e-02
  5.27597487e-01  3.26877624e-01  3.23246151e-01 -1.98969692e-01
  8.57582018e-02 -1.04134716e-01 -3.74771804e-02 -1.44569486e-01
 -5.77803373e-01 -2.11176395e-01 -1.40924640e-02 -1.49817407e-01
  1.76194429e-01  1.61814630e-01 -6.69078901e-03 -2.04738632e-01
 -1.37835652e-01  3.18010271e-01 -2.75463283e-01  1.88544065e-01
  2.63532043e-01  9.43967849e-02  8.86123478e-02  3.45772445e-01
  2.25938618e-01  1.69750065e-01  2.99158961e-01 -1.77813977e-01
  4.43822652e-01  6.71569556e-02 -1.23212114e-01  3.80112588e-01
  9.10138637e-02  1.65319741e-01 -4.82439607e-01  4.40144211e-01
  2.34851032e-01  2.52075903e-02  7.16101900e-02 -4.00475264e-01
  6.45381391e-01 -3.29577595e-01  1.13886133e-01  2.53117681e-01
  2.82477409e-01  1.88364595e-01  2.90989429e-01 -4.99537587e-03
 -1.64457530e-01  8.99558067e-02 -4.75372702e-01 -1.61125481e-01
 -1.10107385e-01 -3.35328460e-01 -5.78144118e-02 -6.70161128e-01
 -9.61706266e-02 -1.28178775e-01 -4.88121398e-02 -2.94338092e-02
  7.70679414e-02  1.34280443e-01 -2.53041714e-01  6.86755180e-02
  8.21142495e-02 -1.70697227e-01  2.36064211e-01  1.80755988e-01
 -2.98387647e-01 -2.03811303e-02  1.79981217e-01 -5.21113336e-01
 -1.88504025e-01 -2.36752182e-01  3.79356802e-01  2.84835875e-01
  5.81075311e-01 -4.27700788e-01  1.85835868e-01  5.19987121e-02
 -9.47643593e-02  5.88640273e-01  1.04099214e-01  1.22547105e-01
 -5.53030014e-01  8.05475593e-01  1.23318516e-01  1.09848827e-02
  2.12131497e-02 -2.38260224e-01 -2.77992785e-01  5.87472618e-02
  2.12433591e-01 -3.46466303e-02 -2.50318408e-01 -9.97810513e-02
  1.04805902e-01 -1.28609687e-03 -5.77312708e-03  1.73181128e-02
 -1.36724174e-01 -4.62179594e-02 -3.06104064e-01 -1.91743925e-01
 -2.58450359e-01  1.62842125e-01 -9.59915966e-02 -3.77580225e-01
 -2.87317157e-01  2.26248115e-01 -2.30421618e-01 -3.42980295e-01
  1.64657921e-01 -4.85439241e-01  1.61243752e-01  5.25194645e-01
 -1.41912848e-01  1.83898807e-01  2.74417520e-01  6.97408020e-02
 -2.33688191e-01  3.96964401e-02 -2.42065102e-01  3.08687329e-01
  1.98629245e-01 -4.57670353e-02  2.35397905e-01  2.55850911e-01
 -1.39614986e-03 -1.74360931e-01 -4.21610713e-01  9.96045098e-02
  3.00608039e-01 -1.29853666e-01 -4.00433466e-02 -3.43654275e-01
  1.45295978e-01  2.25263208e-01 -9.41646844e-02  1.95285350e-01
 -2.22815990e-01  4.69952792e-01  4.00799930e-01 -5.13250291e-01
 -3.46447349e-01 -6.04988169e-03  3.02751780e-01  5.51008619e-03
 -5.95810786e-02  3.58880796e-02  4.35054116e-02 -7.01199323e-02]"
The Depthwise Convolution operation result is incorrect from interpreter stat:awaiting response type:bug comp:lite TFLiteConverter TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am testing network inference using Interpreter for the '.tflite'  that supports dynamic input shape.

However, the operational results of Depthwise Convolution are different from what was expected.

I simply reproduced the issue.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1WCOZBuWTYU7kn4EcjHHrG_fZm7mZcojN?usp=drive_link
```


### Relevant log output

```shell
# input : Inputs corresponding to output [0, 31, 33, 24] 
0) input[0, 61:64, 65:68, 24] 
[[0.978893   0.03620292 0.9301994 ]
 [0.34403667 0.4492776  0.8247143 ]
 [0.8075199  0.77361435 0.1642472 ]]

# weight: corresponding to output [0, 31, 33, 24]
1) weight[..., :24]
[[[ 0.09494528 -0.0925059   0.07752301]
  [-0.07614094  0.06238136 -0.03187032]
  [ 0.06566823  0.09477414 -0.05954333]]]

# bias: corresponding to output [0, 31, 33, 24]
2) bias[24]
0.024211471900343895

# tflite output: Interpreter result
# test output: Formula result of Depthwise Convolution
3) Deptwise Convoultion output
tflite output[0, 31, 33, 24] : 0.0
test output[0, 31, 33, 24]: 0.2780301570892334
```
",True,"[-0.5152595  -0.3157112  -0.21782434 -0.02798725  0.22761032 -0.53750736
 -0.01074817  0.11063736 -0.19851255 -0.24527085  0.02731191 -0.10841311
 -0.00910557  0.27651036 -0.14108086  0.36662278 -0.2622436  -0.05658711
  0.04174989 -0.10041978 -0.13189045 -0.01155149 -0.279033    0.22973797
  0.21456832  0.11948505 -0.3693084  -0.02564843 -0.0673411   0.21463749
  0.27987158  0.17669801 -0.05727296 -0.02299191 -0.11398605  0.36242056
 -0.21152021 -0.15994439 -0.4412049   0.06993787 -0.1919564   0.04251857
  0.07069038 -0.19615626 -0.03354333 -0.03487502  0.07482903 -0.01784198
  0.00752464 -0.12804003  0.01874803 -0.04459241 -0.46195933 -0.3594647
 -0.14933553  0.00108652  0.05297903 -0.06626366 -0.05216266  0.10340461
  0.18893513 -0.07067839 -0.0204647  -0.10274427  0.08915426  0.10754854
  0.31128773 -0.10362769  0.3480412  -0.14618987  0.0714031  -0.10154699
 -0.28194195  0.01486861  0.12966827  0.06945821  0.02699091  0.17055535
  0.29962906 -0.24580625  0.09416817 -0.29866594 -0.22774434 -0.1670739
  0.18668607 -0.07281508  0.24520168  0.09620489  0.52516216 -0.1895889
  0.38311338  0.23124933 -0.06513003  0.08408006  0.5494933   0.13008535
  0.08314728  0.16256848  0.10647731 -0.09679471 -0.22989562 -0.28180405
 -0.08024599 -0.01005576 -0.04431558 -0.1771113   0.15335487  0.09335011
  0.19632585 -0.0497561   0.11854465  0.12352751  0.2721327  -0.18476623
 -0.02002737 -0.01619917  0.06272504 -0.15939532 -0.12217019  0.71747154
  0.07748578  0.02493224 -0.06871605  0.23498224  0.5593139   0.15924737
 -0.07618081 -0.00579647  0.17668732  0.074367    0.17797759  0.08399108
 -0.0622956   0.2912886  -0.14129628  0.09028898 -0.07697695 -0.05217542
 -0.2504016  -0.23447204 -0.28589463  0.10243509 -0.2408651  -0.6385324
  0.14609647  0.00439744 -0.35964078  0.18597603 -0.19585502  0.07938686
 -0.05355995  0.20318286  0.00751409  0.40006253  0.19755095  0.10646345
  0.3823352  -0.0281637   0.14278613 -0.51327336 -0.0304706   0.2770846
 -0.02404478 -0.2440936   0.27259842  0.22843054 -0.46098563 -0.18262234
  0.04481218  0.39833096 -0.18616135 -0.04134969  0.10883144  0.14328521
  0.12301724  0.05411335  0.35011852 -0.57253563 -0.14092998  0.30672908
  0.03026605  0.20464627  0.16127923  0.15633261 -0.0157239   0.06433483
  0.19691727  0.25684166 -0.37938115  0.07451994 -0.5410051  -0.13245258
  0.445214   -0.2256101  -0.09761792  0.04072443  0.12234858 -0.31477505
 -0.0100185   0.1923033  -0.2410553  -0.01262511 -0.08501595 -0.07975748
  0.07803604 -0.21169949  0.12035488 -0.45913798 -0.37352702  0.20140006
  0.16576958 -0.48539123  0.02603846 -0.11144708 -0.24246679  0.08045787
 -0.05854065  0.01800259 -0.21288116  0.28150463  0.10633913 -0.19543846
  0.138217   -0.4346623  -0.32771224  0.20436044 -0.41617483  0.11878595
 -0.01552803  0.20976706  0.25957072  0.0277011   0.29118037  0.23931426
  0.4125321  -0.11450922 -0.26976928 -0.11202359 -0.0806716   0.11014537
 -0.38312942 -0.23274486 -0.06389695  0.04898286  0.28940243  0.34821075
 -0.19575968 -0.21493855 -0.3516203   0.2219566  -0.26012206  0.13912955
  0.28605774  0.1681985   0.41390997  0.28819594  0.24208722  0.18089534
  0.14936902 -0.2068029   0.25197077  0.20603096  0.11692834  0.42959136
  0.25007933  0.20792666 -0.12553227  0.46945006 -0.17037866 -0.08262508
  0.11970881 -0.47558105  0.5652157  -0.4544335   0.20147449 -0.16986637
  0.2873962  -0.04530061  0.00261124  0.1613144   0.09410418  0.20204672
 -0.30438086  0.04555402 -0.0050128  -0.14937504  0.02222552 -0.71639067
 -0.10570215  0.02070762 -0.3175221   0.29099274 -0.02660084 -0.08348772
 -0.10374808 -0.05381498  0.18568552 -0.23644933  0.16797525  0.05081523
 -0.13137624  0.17732759  0.33518636 -0.3435396  -0.18226404  0.02351873
  0.5421613   0.16724256  0.41161907 -0.48705015  0.20480031 -0.07854284
 -0.04312149  0.53233767 -0.11101505  0.08523163 -0.19136861  0.52744603
  0.26034832  0.00256467  0.1553465  -0.22820452 -0.2920077  -0.0939246
  0.24580555 -0.08883684  0.07390558 -0.19984154 -0.01121735  0.07954247
 -0.24703807 -0.08965161 -0.24181868  0.03356594 -0.1468535  -0.13131772
 -0.37608042  0.21116456 -0.06207506 -0.18663174 -0.25886947  0.06135618
 -0.16476795 -0.20758884  0.06458431 -0.3056512   0.14768167  0.43061602
 -0.10494038  0.08334136 -0.04506632  0.19864842 -0.40812707 -0.06950621
 -0.19223526  0.4602636  -0.19959544 -0.10288483  0.32389534  0.19205049
 -0.12940659  0.17699328 -0.27824184  0.26175153  0.22292541 -0.12187926
 -0.24758065 -0.23729394  0.16508232  0.2925076  -0.17931862  0.35132143
 -0.30569202  0.46738398  0.5706779  -0.45199877 -0.16688609 -0.0354985
  0.17692077 -0.15881008 -0.21809506 -0.00912952  0.06471697 -0.02478493]"
Training Vanilla Transformer on TPU gives InternalError type:bug comp:tpus TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.2 (Google Colab)

### Mobile device

Colab

### Python version

3.10.12

### Bazel version

Colab

### GCC/compiler version

Colab

### CUDA/cuDNN version

Colab

### GPU model and memory

Colab

### Current behavior?

Training Transformer model on TPU gives Internal Error 

I have some idea on the error that there's a incompatible tensor ops thats causing the problem but i can't pinpoint it.

I had already done a bigger model which is using pretrained embeddings and it went off without a hitch i tried to replicate the same but with different tfds dataset

If this is already solved please direct me to the relevant links

### Standalone code to reproduce the issue

```
[This is the notebook](https://colab.research.google.com/drive/1y3VEuaYXnsoB42TaVd8UBB-18U8UHFBt#scrollTo=Y0hKZ9yRC3FU)

[This notebook worked fine](https://colab.research.google.com/drive/1DJU058LhhyCfNsuHZ74kZ0E2ziHw-VYo)

Thankyou in advance. I will respond asap
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
[<ipython-input-12-013fa12d9e3a>](https://localhost:8080/#) in <cell line: 1>()
----> 1 model.fit(
      2     train_ds,
      3     validation_data=valid_ds,
      4     epochs=EPOCHS,
      5     steps_per_epoch=train_steps,

1 frames
[/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py](https://localhost:8080/#) in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

[/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/capture/capture_container.py](https://localhost:8080/#) in capture_by_value(self, graph, tensor, name)
    120         graph_const = self.by_val_internal.get(id(tensor))
    121         if graph_const is None:
--> 122           graph_const = tensor._capture_as_const(name)  # pylint: disable=protected-access
    123           if graph_const is None:
    124             # Some eager tensors, e.g. parallel tensors, are not convertible to

InternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:35437: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:35437: Failed to connect to remote host: Connection refused {created_time:""2023-09-19T08:56:09.694479753+00:00"", grpc_status:14}
Executing non-communication op <MultiDeviceIteratorInit> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.
```
",True,"[-5.98172784e-01 -3.76793861e-01 -8.77487659e-02 -1.13092422e-01
 -7.47292340e-02 -4.09344703e-01  6.26585484e-02 -1.14157721e-02
 -4.18906480e-01 -1.65560529e-01 -6.58951476e-02 -1.58849150e-01
 -3.00503194e-01  3.14473569e-01 -6.32268144e-03  4.46880013e-01
 -2.77694404e-01  1.39690533e-01  3.49441290e-01 -2.12806880e-01
 -1.51207477e-01  6.07269164e-03 -3.04332137e-01  2.26744503e-01
 -3.04860324e-02  1.64797843e-01  1.09787900e-02 -9.55478922e-02
  4.04682010e-03  1.69772983e-01 -1.98225975e-01 -7.69192576e-02
 -8.88314098e-02  1.44859748e-02 -2.89877295e-01  1.72994018e-01
 -3.26721609e-01  9.05357003e-02 -2.70170808e-01 -1.34998932e-01
 -1.05215825e-01 -7.61100128e-02  4.83890623e-02 -2.70930678e-02
 -1.24103844e-01 -1.50433760e-02 -1.16456658e-01  3.18664201e-02
 -1.88549086e-01 -2.36930892e-01  2.85525978e-01 -1.87026501e-01
 -2.76109874e-01 -1.71270818e-01  4.73110899e-02  4.43711877e-04
  2.46345580e-01  1.65153712e-01  3.21305424e-01  1.13312870e-01
 -3.18991914e-02  4.61594015e-02  7.54621327e-02 -1.62556872e-01
 -1.01545043e-01  3.98306213e-02  1.60137415e-01 -4.85441322e-03
  1.40970945e-01 -2.88139939e-01  9.36837774e-03 -2.67101794e-01
 -2.17766851e-01  1.37539849e-01  9.60771739e-02 -1.66128695e-01
  1.38662793e-02  2.42784917e-01  1.33740604e-01  8.26700032e-03
  5.37978411e-02 -9.47615802e-02 -2.56732702e-01 -3.77697498e-02
  1.96962252e-01 -1.83061674e-01  7.02384263e-02  1.21030949e-01
  5.22510350e-01 -7.46861920e-02  4.15315419e-01  4.16989207e-01
 -1.07887704e-02 -6.36752546e-02  2.53403723e-01  4.55005765e-02
  2.26965711e-01  1.08934186e-01  1.91636980e-01 -3.19626182e-04
 -5.77969216e-02 -3.26817513e-01 -3.17087710e-01  9.72885713e-02
 -7.15806708e-03 -1.69644386e-01  2.04282820e-01 -4.44796905e-02
  2.03867406e-01  1.10085800e-01  8.34629610e-02  9.47344676e-02
  5.48519865e-02 -1.61377847e-01  1.00508265e-01  6.07520714e-03
 -2.73288965e-01  4.46927771e-02  1.11856252e-01  5.91717362e-01
  1.77073747e-01 -1.59569591e-01 -1.24544449e-01  3.01776886e-01
  6.16325557e-01  6.78554922e-02  2.58281380e-02  2.10607991e-01
  2.25580603e-01  1.99219316e-01  4.63149324e-02  3.29847515e-01
  1.18082792e-01  1.73732072e-01  1.44912928e-01  3.17504942e-01
 -8.16449225e-02 -1.22476742e-01 -1.45727515e-01 -2.05909595e-01
 -5.21316409e-01  3.64815414e-01 -1.38903968e-02 -5.02764106e-01
  2.99955998e-03 -1.61137298e-01 -2.76886284e-01  3.39638829e-01
 -1.22246802e-01  1.36602595e-01  7.41039887e-02  1.73791826e-01
  5.24581820e-02  2.88129330e-01  1.03999376e-01 -8.73083398e-02
  3.37663651e-01 -8.83227289e-02 -8.31162184e-02 -4.24050629e-01
  5.95146604e-02  2.35076100e-01 -5.94876520e-03 -1.79162636e-01
  2.33064324e-01  1.68115392e-01 -5.78158379e-01 -1.89241514e-01
  1.00336544e-01  3.22318733e-01 -5.25737554e-02 -3.25372815e-01
  3.36841568e-02  7.81683326e-02  2.54313111e-01 -9.74116176e-02
  2.78889984e-01 -3.90129209e-01 -2.46756256e-01  2.31722400e-01
 -8.23699757e-02  4.22837399e-02  1.78441092e-01  1.49786413e-01
 -1.44105759e-02  2.02141732e-01  3.84941697e-01  2.09993914e-01
 -2.97037065e-01  8.64514858e-02 -3.10778439e-01  4.58010472e-03
  1.66110963e-01 -1.28299564e-01 -1.82884872e-01 -7.91754946e-02
  2.52153248e-01 -2.81106502e-01 -1.33439004e-01  1.16852969e-01
 -3.38599235e-01 -8.17119181e-02 -1.35520518e-01  7.86411539e-02
  1.42937630e-01 -6.33118814e-03 -8.25404674e-02 -7.11497664e-01
 -6.36607766e-01  2.18120202e-01  6.84733167e-02 -2.93520480e-01
  3.02382469e-01 -2.05977678e-01 -5.52206859e-02  1.86059996e-01
 -7.30358809e-02  2.35610716e-02 -2.78004229e-01  1.95141569e-01
 -1.91524714e-01 -4.94841486e-02  1.94829240e-01 -4.55687046e-01
 -3.41449320e-01  1.70069449e-02 -4.28491473e-01  4.86074463e-02
 -2.48670168e-02  5.40602580e-02 -5.06379679e-02  1.14965066e-01
  2.85490632e-01  2.96047419e-01  3.38408947e-01  5.20188883e-02
 -1.39137954e-01 -1.20443150e-01 -1.58413798e-01 -2.77414061e-02
 -3.46152097e-01  9.39793736e-02 -1.11310616e-01  2.61192024e-02
  2.15100497e-01  8.39952230e-01 -1.16947897e-01 -1.31430641e-01
 -3.58363926e-01  5.42014480e-01 -4.17616069e-01  2.46373102e-01
  4.11558241e-01  2.44815156e-01  4.62384641e-01  1.49919897e-01
  1.96646392e-01 -2.52560005e-02  1.35061473e-01 -3.37320089e-01
  9.70121101e-02  3.28487486e-01  1.91857547e-01  4.80922490e-01
  2.18288749e-01  3.72405648e-01 -1.99092597e-01  6.54515982e-01
 -3.33346575e-01 -2.54514396e-01  2.27309242e-01 -4.32283998e-01
  5.74427187e-01 -1.18768379e-01  3.11922371e-01 -2.06151769e-01
  1.21398643e-01 -5.38965128e-02 -2.13761285e-01  7.56501034e-02
  1.26456439e-01  2.96534449e-01 -1.77813262e-01  1.07649356e-01
 -1.56239480e-01  7.78304785e-02 -1.15682840e-01 -8.15906763e-01
 -1.93241954e-01 -1.49714902e-01 -2.01650664e-01  2.73591101e-01
  1.85167074e-01 -1.97685927e-01 -3.90050113e-01  5.90026043e-02
  1.54573724e-01 -2.31328249e-01  1.64082050e-01 -8.49859267e-02
 -5.39498739e-02  1.77193761e-01  2.78667539e-01 -4.11657363e-01
 -2.06413016e-01 -1.29644513e-01  5.47054946e-01  9.97067541e-02
  4.37951595e-01 -4.76445168e-01  1.54388443e-01 -2.09015943e-02
  8.58798921e-02  3.70210856e-01  4.26307134e-03  3.33949506e-01
 -3.97425413e-01  4.86746848e-01  1.20167270e-01 -1.57003850e-02
  1.47642002e-01 -1.45747304e-01 -4.22677964e-01  5.87904602e-02
  2.31983647e-01 -1.89762607e-01  1.13684468e-01 -1.63568243e-01
 -9.73707736e-02  5.81601560e-02 -2.60143965e-01 -5.47955893e-02
 -9.43230540e-02 -1.06475130e-01 -7.78145790e-02  5.01584634e-02
 -2.49430194e-01  3.76589835e-01 -7.15672076e-02 -2.53774494e-01
 -4.36739773e-01 -1.56849161e-01 -2.37283617e-01 -1.08891308e-01
 -1.15704000e-01 -3.87648225e-01  4.45536561e-02  2.57712990e-01
  7.40473792e-02 -7.98953511e-03  5.23739383e-02  2.66156614e-01
 -2.90414602e-01  8.92167613e-02 -2.77641285e-02  4.38154727e-01
 -4.81331311e-02 -1.43810466e-01  5.33151865e-01  1.90566242e-01
 -2.51926303e-01 -1.07386289e-02 -4.26535308e-01  5.70580773e-02
 -1.08216912e-01 -1.05502740e-01 -2.82263130e-01 -3.28296363e-01
 -5.86951151e-02  3.65583956e-01 -2.23731417e-02  3.54794383e-01
 -1.15173236e-01  3.28710049e-01  7.42257118e-01 -3.39103997e-01
  3.87785956e-02  8.25074762e-02  1.42633438e-01 -1.57873496e-01
 -2.03623064e-02  5.05021811e-02  3.31164271e-01 -6.54036030e-02]"
How to compile tflite-runtime to include gpu part? type:bug comp:lite TF 2.13,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04.2 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 2.13


**Provide the text output from tflite_convert**

```
SUBCOMMAND: # @XNNPACK//:operators [action 'Compiling src/operators/convolution-nchw.c', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt/bin/external/XNNPACK/_objs/operators/convolution-nchw.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/XNNPACK/_objs/operators/convolution-nchw.pic.o' -fPIC '-DXNN_IGNORED_PLATFORM_JIT=0' '-DXNN_LOG_LEVEL=0' -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_JIT=0' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=0' -iquote external/XNNPACK -iquote bazel-out/k8-opt/bin/external/XNNPACK -iquote external/pthreadpool -iquote bazel-out/k8-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/k8-opt/bin/external/FXdiv -iquote external/FP16 -iquote bazel-out/k8-opt/bin/external/FP16 -iquote external/cpuinfo -iquote bazel-out/k8-opt/bin/external/cpuinfo -Ibazel-out/k8-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/k8-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/k8-opt/bin/external/FP16/_virtual_includes/FP16 -Ibazel-out/k8-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -isystem external/XNNPACK/include -isystem bazel-out/k8-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/k8-opt/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/k8-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/k8-opt/bin/external/FXdiv/include -isystem external/FP16/include -isystem bazel-out/k8-opt/bin/external/FP16/include -isystem external/cpuinfo/include -isystem bazel-out/k8-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/k8-opt/bin/external/cpuinfo/src -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' -Iinclude -Isrc -Os '-std=c99' -O2 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/XNNPACK/src/operators/convolution-nchw.c -o bazel-out/k8-opt/bin/external/XNNPACK/_objs/operators/convolution-nchw.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,222 / 1,232] 2 actions, 1 running
    Compiling absl/strings/internal/charconv_bigint.cc; 0s local
    [Prepa] Compiling src/operators/convolution-nchw.c
[1,222 / 1,232] 2 actions running
    Compiling absl/strings/internal/charconv_bigint.cc; 0s local
    Compiling src/operators/convolution-nchw.c; 0s local
SUBCOMMAND: # @com_google_absl//absl/time:time [action 'Compiling absl/time/format.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/com_google_absl/absl/time/_objs/time/format.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/com_google_absl/absl/time/_objs/time/format.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_absl/absl/time/format.cc -o bazel-out/k8-opt/bin/external/com_google_absl/absl/time/_objs/time/format.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,223 / 1,232] 2 actions, 1 running
    Compiling absl/strings/internal/charconv_bigint.cc; 0s local
    [Prepa] Compiling absl/time/format.cc
[1,223 / 1,232] 2 actions running
    Compiling absl/strings/internal/charconv_bigint.cc; 1s local
    Compiling absl/time/format.cc; 0s local
[1,224 / 1,232] 2 actions, 1 running
    Compiling absl/strings/internal/charconv_bigint.cc; 2s local
    [Scann] Compiling src/unpool-config.c
SUBCOMMAND: # @XNNPACK//:microkernel_configs [action 'Compiling src/unpool-config.c', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/k8-opt/bin/external/XNNPACK/_objs/microkernel_configs/unpool-config.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/XNNPACK/_objs/microkernel_configs/unpool-config.pic.o' -fPIC '-DXNN_IGNORED_PLATFORM_JIT=0' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=0' '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_JIT=0' '-DXNN_LOG_LEVEL=0' -DPTHREADPOOL_NO_DEPRECATED_API -iquote external/XNNPACK -iquote bazel-out/k8-opt/bin/external/XNNPACK -iquote external/FXdiv -iquote bazel-out/k8-opt/bin/external/FXdiv -iquote external/pthreadpool -iquote bazel-out/k8-opt/bin/external/pthreadpool -iquote external/cpuinfo -iquote bazel-out/k8-opt/bin/external/cpuinfo -iquote external/FP16 -iquote bazel-out/k8-opt/bin/external/FP16 -Ibazel-out/k8-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/k8-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/k8-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/k8-opt/bin/external/FP16/_virtual_includes/FP16 -isystem external/XNNPACK/include -isystem bazel-out/k8-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/k8-opt/bin/external/XNNPACK/src -isystem external/FXdiv/include -isystem bazel-out/k8-opt/bin/external/FXdiv/include -isystem external/pthreadpool/include -isystem bazel-out/k8-opt/bin/external/pthreadpool/include -isystem external/cpuinfo/include -isystem bazel-out/k8-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/k8-opt/bin/external/cpuinfo/src -isystem external/FP16/include -isystem bazel-out/k8-opt/bin/external/FP16/include -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' -Iinclude -Isrc '-std=c99' -O2 -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/XNNPACK/src/unpool-config.c -o bazel-out/k8-opt/bin/external/XNNPACK/_objs/microkernel_configs/unpool-config.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,224 / 1,232] 2 actions, 1 running
    Compiling absl/strings/internal/charconv_bigint.cc; 2s local
    [Prepa] Compiling src/unpool-config.c
SUBCOMMAND: # //tensorflow/lite/kernels:builtin_op_kernels [action 'Compiling tensorflow/lite/kernels/add.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/add.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/add.pic.o' -fPIC -DTFLITE_KERNEL_USE_XNNPACK -DPTHREADPOOL_NO_DEPRECATED_API '-DEIGEN_NEON_GEBP_NR=4' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DXNNPACK_DELEGATE_ENABLE_QS8=1' '-DXNNPACK_DELEGATE_ENABLE_QU8=1' '-DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0' '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' '-DXNN_IGNORED_PLATFORM_JIT=0' '-DXNN_LOG_LEVEL=0' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_JIT=0' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=0' '-DXNN_ENABLE_SPARSE=1' '-DXNN_ENABLE_MEMOPT=1' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/ruy -iquote bazel-out/k8-opt/bin/external/ruy -iquote external/cpuinfo -iquote bazel-out/k8-opt/bin/external/cpuinfo -iquote external/gemmlowp -iquote bazel-out/k8-opt/bin/external/gemmlowp -iquote external/pthreadpool -iquote bazel-out/k8-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/k8-opt/bin/external/FXdiv -iquote external/arm_neon_2_x86_sse -iquote bazel-out/k8-opt/bin/external/arm_neon_2_x86_sse -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/flatbuffers -iquote bazel-out/k8-opt/bin/external/flatbuffers -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/XNNPACK -iquote bazel-out/k8-opt/bin/external/XNNPACK -iquote external/FP16 -iquote bazel-out/k8-opt/bin/external/FP16 -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -Ibazel-out/k8-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/k8-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/k8-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/k8-opt/bin/external/flatbuffers/_virtual_includes/flatbuffers -Ibazel-out/k8-opt/bin/external/flatbuffers/src/_virtual_includes/flatbuffers -Ibazel-out/k8-opt/bin/external/flatbuffers/_virtual_includes/runtime_cc -Ibazel-out/k8-opt/bin/external/FP16/_virtual_includes/FP16 -isystem external/cpuinfo/include -isystem bazel-out/k8-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/k8-opt/bin/external/cpuinfo/src -isystem external/pthreadpool/include -isystem bazel-out/k8-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/k8-opt/bin/external/FXdiv/include -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem tensorflow/lite/schema -isystem bazel-out/k8-opt/bin/tensorflow/lite/schema -isystem tensorflow/lite/experimental/acceleration/configuration -isystem bazel-out/k8-opt/bin/tensorflow/lite/experimental/acceleration/configuration -isystem external/XNNPACK/include -isystem bazel-out/k8-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/k8-opt/bin/external/XNNPACK/src -isystem external/FP16/include -isystem bazel-out/k8-opt/bin/external/FP16/include -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -DFARMHASH_NO_CXX_STRING -msse4.2 -O3 -fno-exceptions '-Wno-error=reorder' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/lite/kernels/add.cc -o bazel-out/k8-opt/bin/tensorflow/lite/kernels/_objs/builtin_op_kernels/add.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,225 / 1,232] 2 actions, 1 running
    Compiling absl/strings/internal/charconv_bigint.cc; 2s local
    [Prepa] Compiling tensorflow/lite/kernels/add.cc
SUBCOMMAND: # //tensorflow/lite:stderr_reporter [action 'Compiling tensorflow/lite/stderr_reporter.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/lite/_objs/stderr_reporter/stderr_reporter.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/lite/_objs/stderr_reporter/stderr_reporter.pic.o' -fPIC -DTFLITE_KERNEL_USE_XNNPACK -iquote . -iquote bazel-out/k8-opt/bin -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -DFARMHASH_NO_CXX_STRING -msse4.2 -O3 -fno-exceptions -Wall -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/lite/stderr_reporter.cc -o bazel-out/k8-opt/bin/tensorflow/lite/_objs/stderr_reporter/stderr_reporter.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,226 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 0s local
    [Prepa] Compiling tensorflow/lite/stderr_reporter.cc
SUBCOMMAND: # @com_google_absl//absl/strings:cord [action 'Compiling absl/strings/cord.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/com_google_absl/absl/strings/_objs/cord/cord.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/com_google_absl/absl/strings/_objs/cord/cord.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_absl/absl/strings/cord.cc -o bazel-out/k8-opt/bin/external/com_google_absl/absl/strings/_objs/cord/cord.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,227 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 0s local
    [Prepa] Compiling absl/strings/cord.cc
[1,227 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 0s local
    Compiling absl/strings/cord.cc; 0s local
[1,227 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 1s local
    Compiling absl/strings/cord.cc; 1s local
[1,227 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 2s local
    Compiling absl/strings/cord.cc; 2s local
[1,227 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 3s local
    Compiling absl/strings/cord.cc; 3s local
[1,227 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 4s local
    Compiling absl/strings/cord.cc; 4s local
[1,228 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 5s local
    [Scann] Compiling src/google/protobuf/stubs/statusor.cc
SUBCOMMAND: # @com_google_protobuf//:protobuf_lite [action 'Compiling src/google/protobuf/stubs/statusor.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/statusor.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/statusor.pic.o' -fPIC -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_protobuf/src/google/protobuf/stubs/statusor.cc -o bazel-out/k8-opt/bin/external/com_google_protobuf/_objs/protobuf_lite/statusor.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,228 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 5s local
    [Prepa] Compiling src/google/protobuf/stubs/statusor.cc
[1,228 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 6s local
    Compiling src/google/protobuf/stubs/statusor.cc; 0s local
[1,229 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 6s local
    [Scann] Compiling absl/synchronization/internal/create_thread_identity.cc
SUBCOMMAND: # @com_google_absl//absl/synchronization:synchronization [action 'Compiling absl/synchronization/internal/create_thread_identity.cc', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/external/com_google_absl/absl/synchronization/_objs/synchronization/create_thread_identity.pic.d '-frandom-seed=bazel-out/k8-opt/bin/external/com_google_absl/absl/synchronization/_objs/synchronization/create_thread_identity.pic.o' -fPIC -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-march=native' '-std=c++17' -Wall -Wextra -Wcast-qual -Wconversion-null -Wformat-security -Wmissing-declarations -Woverlength-strings -Wpointer-arith -Wundef -Wunused-local-typedefs -Wunused-result -Wvarargs -Wvla -Wwrite-strings -DNOMINMAX -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/com_google_absl/absl/synchronization/internal/create_thread_identity.cc -o bazel-out/k8-opt/bin/external/com_google_absl/absl/synchronization/_objs/synchronization/create_thread_identity.pic.o)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,229 / 1,232] 2 actions, 1 running
    Compiling tensorflow/lite/kernels/add.cc; 6s local
    [Prepa] Compiling absl/synchronization/internal/create_thread_identity.cc
[1,229 / 1,232] 2 actions running
    Compiling tensorflow/lite/kernels/add.cc; 6s local
    Compiling .../synchronization/internal/create_thread_identity.cc; 0s local
[1,230 / 1,232] Compiling tensorflow/lite/kernels/add.cc; 7s local
[1,230 / 1,232] Compiling tensorflow/lite/kernels/add.cc; 9s local
[1,230 / 1,232] Compiling tensorflow/lite/kernels/add.cc; 10s local
[1,230 / 1,232] Compiling tensorflow/lite/kernels/add.cc; 11s local
[1,231 / 1,232] [Prepa] ...r_wrapper:_pywrap_tensorflow_interpreter_wrapper.so
SUBCOMMAND: # //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper.so [action 'Linking tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so', configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2, execution platform: @local_execution_config_platform//:platform]
(cd /root/.cache/bazel/_bazel_root/44af44c54090ffd8c730879fc5d7b491/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib \
    PATH=/opt/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF2_BEHAVIOR=1 \
  /usr/bin/gcc @bazel-out/k8-opt/bin/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so-2.params)
# Configuration: 6bf13bb09c259727d5061837858294f1092bec30d275f05710212182ee5e1ce2
# Execution platform: @local_execution_config_platform//:platform
[1,231 / 1,232] [Prepa] ...r_wrapper:_pywrap_tensorflow_interpreter_wrapper.so
[1,231 / 1,232] ...wrapper:_pywrap_tensorflow_interpreter_wrapper.so; 0s local
Target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper up-to-date (nothing to build)
[1,232 / 1,232] checking cached actions
INFO: Elapsed time: 1504.620s, Critical Path: 75.80s
[1,232 / 1,232] checking cached actions
INFO: 1232 processes: 204 internal, 1028 local.
[1,232 / 1,232] checking cached actions
INFO: Build completed successfully, 1232 total actions
INFO: Build completed successfully, 1232 total actions
+ cp /kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/../../../../bazel-bin/tensorflow/lite/python/interpreter_wrapper/_pywrap_tensorflow_interpreter_wrapper.so /kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime
+ chmod u+w /kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so
+ cd /kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ case ""${TENSORFLOW_TARGET}"" in
+ [[ -n '' ]]
+ python3 setup.py bdist bdist_wheel
/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` directly.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
        ********************************************************************************

!!
  self.initialize_options()
/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/dist.py:947: SetuptoolsDeprecationWarning: setup.py install is deprecated.
!!

        ********************************************************************************
        Please avoid running ``setup.py`` directly.
        Instead, use pypa/build, pypa/installer or other
        standards-based tools.

        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
        ********************************************************************************

!!
  command.initialize_options()
+ echo 'Output can be found here:'
Output can be found here:
+ find /kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/MANIFEST.in
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite-runtime-2.13.0.linux-x86_64.tar.gz
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/tflite_runtime-2.13.0-cp310-cp310-linux_x86_64.whl
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/bdist.linux-x86_64
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime/metrics_interface.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime/__init__.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime/interpreter.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/build/lib.linux-x86_64-cpython-310/tflite_runtime/metrics_portable.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian/changelog
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian/copyright
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian/rules
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian/control
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/debian/compat
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/numpy.cc
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/interpreter_wrapper.h
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/python_utils.h
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/numpy.h
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/interpreter_wrapper_pybind11.cc
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/python_error_reporter.cc
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/python_error_reporter.h
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/python_utils.cc
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/interpreter_wrapper.cc
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/interpreter_wrapper/BUILD
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info/PKG-INFO
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info/dependency_links.txt
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info/requires.txt
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info/top_level.txt
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime.egg-info/SOURCES.txt
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/metrics_interface.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/__init__.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/interpreter.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime/metrics_portable.py
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/setup.py
+ [[ '' != \y ]]
+ exit 0
total 7748
-rw-r--r-- 1 root root 3960944 Sep 19 03:23 tflite-runtime-2.13.0.linux-x86_64.tar.gz
-rw-r--r-- 1 root root 3966013 Sep 19 03:23 tflite_runtime-2.13.0-cp310-cp310-linux_x86_64.whl
/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist
Processing ./tflite_runtime-2.13.0-cp310-cp310-linux_x86_64.whl
Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from tflite-runtime==2.13.0) (1.23.5)
**Installing collected packages: tflite-runtime
Successfully installed tflite-runtime-2.13.0**
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Cell In[5], line 17
     15 get_ipython().run_line_magic('cd', '/kaggle/working/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/dist/')
     16 get_ipython().system('pip install tflite_runtime-2.13.0-cp310-cp310-linux_x86_64.whl')
**---> 17 import tflite_runtime.interpreter as tflite**

File /opt/conda/lib/python3.10/site-packages/tflite_runtime/interpreter.py:33
     30   from tensorflow.python.util.tf_export import tf_export as _tf_export
     31 else:
     32   # This file is part of tflite_runtime package.
---> 33   from tflite_runtime import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper
     34   from tflite_runtime import metrics_portable as metrics
     36   def _tf_export(*x, **kwargs):

**ImportError: /opt/conda/lib/python3.10/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.so: undefined symbol: _ZN6tflite29farthestpointsamplingLauncherEiiiPKfPfPi**
    
```

**Standalone code to reproduce the issue** 
I tried to add custom OP and built tflite-runtime with shim, see issue https://github.com/tensorflow/tensorflow/issues/61521.
And the custom OP includes some GPU code. 
I built tflite-runtime successfully and the installation seems successfully as well. 
But when tried to import tflite-runtime in python there raised errors about GPU API farthestpointsamplingLauncher. 
Missing the share library of GPU part?

[code and change]
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/shim/test_op/sampling_op.h
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/shim/test_op/sampling_tflite_op.cc
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/shim/test_op/sampling_tflite_op.h
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/shim/test_op/tf_sampling_gpu.cu.cc
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/shim/test_op/BUILD#L159
https://github.com/kuangzy2011/tensorflow/blob/main/tensorflow/lite/kernels/BUILD#L744

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
",True,"[-4.93661433e-01 -4.46226358e-01 -2.67008603e-01  1.01830758e-01
  3.44034918e-02 -8.70825797e-02 -8.76005739e-02 -7.99567066e-03
 -1.92452401e-01 -1.43137157e-01 -1.62157997e-01  3.46631780e-02
 -2.94819832e-01  2.67314643e-01 -2.62368262e-01  2.44069636e-01
  4.27779518e-02 -2.03423887e-01  4.03579414e-01  1.69629306e-02
  4.81775589e-03 -8.96291584e-02 -3.79280567e-01  4.09344256e-01
  3.06503534e-01  3.78162861e-01 -6.01747893e-02  1.10516429e-01
 -5.10514528e-02  2.39836007e-01  1.70277074e-01  1.92156404e-01
 -1.01351678e-01  9.05787293e-03 -1.84823111e-01  7.70739168e-02
 -1.29830867e-01 -1.36270270e-01 -3.18793952e-01 -3.59198339e-02
  1.13537520e-01  7.78511763e-02 -1.45943657e-01  2.17147663e-01
 -4.57566157e-02  2.31490284e-02  1.37659729e-01  1.13598071e-01
 -2.12865278e-01 -8.60459805e-02 -1.27040297e-02 -1.64141908e-01
 -3.12520206e-01 -2.00135857e-01  2.58700587e-02  7.05932528e-02
  4.50075537e-01  7.02268407e-02  1.84245825e-01  7.51982853e-02
 -5.71969151e-03 -2.77512148e-02 -9.18948501e-02  4.35583889e-02
  1.70749485e-01  2.02010691e-01 -1.03311487e-01 -1.29350096e-01
  2.89026111e-01 -2.26464674e-01 -4.83720973e-02 -1.25769705e-01
  1.72395278e-02 -1.83774278e-01 -3.76502387e-02  3.70489471e-02
  6.92126155e-02  2.60201365e-01  3.78875919e-02  1.10357981e-02
  2.87138283e-01 -6.66058958e-02 -2.80338787e-02  1.39885932e-01
  1.96391761e-01  1.70552880e-01  1.54409811e-01  3.43655586e-01
  2.90641338e-01 -3.12124431e-01  4.02559817e-01  4.55544353e-01
 -2.49612778e-01 -8.70593823e-03  2.90212691e-01  1.31553322e-01
  1.28840894e-01  1.60796359e-01  8.95309746e-02 -7.45367631e-03
 -8.65938216e-02 -3.75266194e-01  6.40055537e-02  3.46426293e-03
 -8.45806450e-02 -1.99388608e-01  2.17825145e-01  2.11476654e-01
 -6.21089600e-02 -9.24318731e-02  2.09515125e-01  1.47569343e-01
  1.57782301e-01  3.66275162e-02  1.32078648e-01 -6.12164661e-02
 -2.23445714e-01  1.58108413e-01 -7.59962574e-03  5.58222890e-01
 -1.01261839e-01 -1.24103673e-01  2.34078735e-01  6.61090687e-02
  5.26240051e-01  8.58869404e-02 -2.66255170e-01 -4.92351539e-02
  2.74211764e-01  1.26389787e-01 -1.97122265e-02  2.32297957e-01
 -2.30582923e-01  1.01021729e-01  1.18903466e-01 -6.33665696e-02
 -3.20636481e-01 -7.76235461e-02 -1.71734989e-01  7.21230358e-02
 -1.32969931e-01  5.29848933e-02 -1.95655957e-01 -4.20184344e-01
  8.95511955e-02  1.68814853e-01 -2.64620006e-01  1.37579948e-01
 -1.03813015e-01  3.61053467e-01 -2.37853914e-01  5.11263944e-02
 -2.58562304e-02  2.52293468e-01  5.68082146e-02  2.66360231e-02
  2.75179148e-01 -1.94435090e-01  4.09996137e-05 -3.73926461e-01
 -2.20289215e-01  1.13499135e-01  3.17863375e-03 -4.36159857e-02
  1.76945087e-02  9.10626203e-02 -4.19964880e-01 -2.22149864e-01
  3.78886759e-01  2.00120971e-01  9.79048163e-02 -3.84329073e-02
  6.93499669e-02  3.44474167e-01  1.91215992e-01 -2.73808502e-02
  4.65743631e-01 -6.39639735e-01 -1.32741302e-01  1.56348050e-02
  8.83631781e-02 -3.72222327e-02  4.98536266e-02 -1.63238216e-03
  8.47758427e-02 -7.33364671e-02  3.00280601e-01  1.27279490e-01
 -3.91318619e-01 -1.21840224e-01 -2.50234693e-01 -2.59202987e-01
  1.87096506e-01 -2.59888083e-01 -4.97842431e-01  1.84917346e-01
  1.81029454e-01  1.20070353e-01  2.30709277e-02  2.03532100e-01
 -2.06497684e-01 -9.23622847e-02  1.36813521e-01  5.03765792e-03
  8.08944777e-02 -6.47679269e-02 -2.75246859e-01 -2.48923063e-01
 -4.76275563e-01 -1.75851658e-02  2.01575518e-01 -2.25648135e-01
  1.13104708e-01  2.87573691e-03 -1.24762222e-01 -1.67496696e-01
 -9.11788344e-02 -3.30078036e-01 -4.35245812e-01  8.28438848e-02
  7.95247182e-02 -2.16785476e-01  4.84021008e-02 -3.24098527e-01
 -3.31421375e-01 -1.89313829e-01 -2.37193793e-01  2.40230888e-01
 -3.22535113e-02  3.79578054e-01 -2.37059936e-01 -1.75544709e-01
  3.83198112e-01  1.29476652e-01  2.62378752e-01  4.17448729e-02
  1.00235254e-01 -3.34911287e-01 -3.11520249e-01  2.60616869e-01
 -8.38026255e-02 -2.31666923e-01  1.05420366e-01 -5.77569269e-02
  8.15674812e-02 -3.17910314e-03 -1.04458243e-01 -8.22203532e-02
 -6.88270330e-02  5.23740411e-01  4.87007275e-02 -1.57120869e-01
  2.16200680e-01  3.01009297e-01  3.17425370e-01  1.52153671e-01
 -7.69585446e-02 -2.01599691e-02 -3.02318465e-02  2.03993432e-02
  5.71813509e-02  3.72589707e-01 -7.56323338e-02  6.03904903e-01
  4.49034631e-01  3.31426799e-01 -9.56474990e-02  1.42688245e-01
 -2.83994615e-01 -9.04923975e-02 -5.33724725e-02  3.78904864e-02
  1.81629121e-01 -3.35547209e-01  1.49102397e-02 -4.09655184e-01
  4.03931081e-01 -2.90470660e-01  1.81875005e-02  4.13647257e-02
  3.09579909e-01  2.93997884e-01 -2.15197116e-01 -8.47121775e-02
  2.28816330e-01  5.23186103e-02 -1.43842399e-01 -6.18879735e-01
 -2.99712837e-01  1.09378070e-01 -2.02514440e-01  1.43551733e-02
  1.99672990e-02 -5.36433533e-02 -1.13011912e-01  1.85126029e-02
  2.23701358e-01  1.16553158e-04  7.12083876e-02 -5.45169786e-03
  3.31150368e-05  2.50385106e-01  2.20569327e-01 -2.37633482e-01
  6.68418407e-02 -3.41678709e-02  1.69915706e-01 -2.48823352e-02
  4.46994573e-01 -2.97147542e-01  1.16513856e-01  1.43907607e-01
  2.02168107e-01  1.51243746e-01  1.60611421e-03  2.49026835e-01
 -1.42008588e-01  4.00220156e-01  1.58215895e-01 -1.79023519e-01
  2.74242461e-01 -1.94211721e-01 -3.78767252e-01  4.85003814e-02
 -1.04334224e-02 -1.96828991e-01 -1.17054932e-01 -1.62044495e-01
  1.15390360e-01  3.81562412e-02 -2.40770802e-01 -4.34651285e-01
 -9.90887210e-02 -1.11350138e-02 -1.20536208e-01 -5.03326170e-02
  6.50169626e-02  1.63407683e-01 -5.25715277e-02 -2.79329240e-01
 -1.50956303e-01 -2.44753689e-01 -9.06521529e-02 -3.81067038e-01
 -3.25453021e-02 -7.10178465e-02 -6.41710218e-03  2.39665896e-01
  1.11494780e-01  1.19071126e-01  1.85772367e-02 -3.88856232e-03
 -2.49500200e-01  1.55587107e-01 -5.62494174e-02  2.01251477e-01
 -2.81710386e-01  3.41018960e-02  2.99158275e-01  5.30626416e-01
 -1.59086660e-02  2.10893098e-02 -2.74538934e-01 -2.38377139e-01
 -1.65195376e-01 -2.89763331e-01 -3.19378555e-01  7.56848156e-02
  2.19169587e-01  5.28572261e-01  5.38549796e-02  4.49709833e-01
 -2.64697313e-01  7.60750771e-02  4.95733351e-01  3.10077937e-03
  2.40497440e-02 -1.67097867e-01  6.90333843e-02 -4.53276008e-01
 -1.05659537e-01 -3.08683813e-02  3.29695866e-02  4.49877568e-02]"
Results with error set when running tf.raw_ops.StatelessParameterizedTruncatedNormal stat:awaiting tensorflower type:bug comp:ops TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Maybe a better error message should be raised here? 
```
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
```

### Standalone code to reproduce the issue

```shell
results = dict()
import tensorflow as tf
import numpy as np
try:
  try:
    with tf.device('/CPU'):
      shape = []
      seed_0 = -0.28041645497635637
      seed_1 = -434
      seed = [seed_0,seed_1,]
      means = -382
      stddevs_tensor = tf.saturate_cast(tf.random.uniform([], minval=0, maxval=2, dtype=tf.int64), dtype=tf.uint64)
      stddevs = tf.identity(stddevs_tensor)
      minvals = []
      maxvals = []
      name_tensor = tf.random.uniform([], dtype=tf.bfloat16)
      name = tf.identity(name_tensor)
      out = tf.raw_ops.StatelessParameterizedTruncatedNormal(shape=shape,seed=seed,means=means,stddevs=stddevs,minvals=minvals,maxvals=maxvals,name=name,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      shape = []
      seed = [seed_0,seed_1,]
      stddevs = tf.identity(stddevs_tensor)
      stddevs = tf.cast(stddevs, tf.uint64)
      minvals = []
      maxvals = []
      name = tf.identity(name_tensor)
      name = tf.cast(name, tf.bfloat16)
      tf.raw_ops.StatelessParameterizedTruncatedNormal(shape=shape,seed=seed,means=means,stddevs=stddevs,minvals=minvals,maxvals=maxvals,name=name,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))

print(results)
```
```


### Relevant log output

```shell
2023-09-18 23:45:51.224040: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-18 23:45:51.345060: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-09-18 23:45:51.934234: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-09-18 23:45:51.934291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-09-18 23:45:51.934301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-09-18 23:45:52.423299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-09-18 23:45:52.450695: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2023-09-18 23:45:52.450716: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2023-09-18 23:45:52.451038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
Error:<class 'tensorflow.python.framework.ops.EagerTensor'> returned a result with an error set
{}
```
```
",True,"[-4.30000573e-01 -4.24739063e-01 -1.46087825e-01  1.28902942e-01
  3.45261097e-01 -4.04098094e-01 -6.60984516e-02  7.13068545e-02
 -3.84241313e-01 -2.93609887e-01  1.74419895e-01 -1.15423091e-01
 -1.64748281e-01  1.03018425e-01 -2.87163407e-01  2.85518169e-01
 -1.77522317e-01 -2.47811377e-02  2.10172147e-01  1.30219311e-01
 -1.67852551e-01 -5.21104671e-02 -2.26960182e-01  1.75392002e-01
  1.51094973e-01  2.09178314e-01 -4.51541662e-01  8.32180753e-02
 -7.46831447e-02  3.20226878e-01  3.61235321e-01 -6.31633326e-02
  5.62346354e-03  1.42076001e-01  8.61679167e-02  1.44079030e-01
 -2.78937161e-01 -1.18036687e-01 -3.39283764e-01  8.36203694e-02
  1.69025008e-02 -1.78180993e-01  6.54452890e-02 -1.63793251e-01
 -9.69132222e-03 -5.60673885e-02 -2.59182658e-02 -1.31569773e-01
 -1.24550477e-01 -7.39749819e-02 -4.04098742e-02  8.75968710e-02
 -4.84556794e-01 -3.83318126e-01  1.59631073e-01  1.00604631e-02
  2.71591209e-02  6.84573203e-02  8.77984986e-03  1.64743587e-01
  1.07434586e-01 -7.64189512e-02 -6.20241538e-02 -1.40938908e-01
  1.97190166e-01  1.58569843e-01  3.79179418e-01 -4.06568348e-02
  4.70910370e-01 -2.86064506e-01  1.71801150e-01 -3.26917134e-03
 -2.90145814e-01  1.26200765e-01 -5.97328767e-02  2.34836131e-01
 -9.81399640e-02  1.37647122e-01  2.83914089e-01 -2.40944654e-01
 -3.17873843e-02 -7.99001977e-02 -1.31973475e-01 -1.47237107e-01
  1.85407504e-01 -2.23443016e-01  4.52476710e-01  8.04484338e-02
  5.16638100e-01 -1.78508013e-01  5.06501436e-01  3.82798165e-01
 -1.78025424e-01 -5.74062616e-02  5.36625624e-01  1.78080171e-01
  1.65938064e-01  2.68280029e-01  6.74540773e-02  3.81630734e-02
 -1.07121602e-01 -3.04939687e-01 -1.56144202e-01  1.11988395e-01
 -1.40525520e-01 -6.70006573e-02 -9.09203570e-03 -1.22474812e-01
  8.09659883e-02 -2.84745842e-02  2.88640618e-01  1.46492332e-01
  2.03753144e-01 -1.43183887e-01 -9.39301699e-02 -1.13746747e-01
  9.88735706e-02  1.16901860e-01  8.86749476e-02  6.40530348e-01
  8.09402019e-02 -1.61449730e-01 -8.04527849e-02  2.12442815e-01
  4.96047854e-01  1.64407611e-01 -6.09279945e-02  1.05426654e-01
  1.51668102e-01 -1.95775013e-02  2.01523781e-01 -1.99786365e-01
 -1.47157520e-01  2.45573729e-01 -1.90076798e-01  2.79302504e-02
  1.51042715e-02 -5.94485067e-02 -2.91475803e-01 -1.11499228e-01
 -2.35694692e-01  1.35315388e-01 -1.34850428e-01 -5.77803791e-01
  2.07160562e-01  1.79317892e-01 -1.37227163e-01  2.88862228e-01
 -1.57111987e-01  1.82259247e-01 -2.28805393e-02 -4.94129173e-02
 -2.32223332e-01  5.23852110e-01 -3.61489318e-03  1.84627622e-02
  4.03340191e-01  4.94612306e-02  1.38676867e-01 -5.13859749e-01
 -2.66862325e-02  3.47966254e-01 -2.17825130e-01 -9.20524150e-02
  2.25982398e-01  9.01218057e-02 -4.66518492e-01 -1.73593372e-01
 -4.37818170e-02  4.40265357e-01 -1.61252022e-01 -7.51082003e-02
 -9.29490700e-02  2.82561351e-02  8.55203792e-02 -1.09035157e-01
  2.46781170e-01 -4.07723129e-01 -1.15221612e-01  1.01673231e-01
  2.03852087e-01 -1.07507572e-01  1.34130359e-01  3.02675486e-01
  7.56722018e-02 -2.50935405e-02  1.08609676e-01  1.48015305e-01
 -3.86584044e-01 -7.13751763e-02 -5.76107979e-01 -2.39322856e-01
  3.72430325e-01  1.13739371e-01 -5.13254106e-02 -8.29036012e-02
  1.28399387e-01 -2.14488044e-01  1.53371677e-01  1.41486496e-01
 -1.72979504e-01 -1.20957687e-01 -4.21924703e-02  1.24634035e-01
  1.64906070e-01 -2.32478708e-01 -2.43932545e-01 -6.38957858e-01
 -5.71818590e-01  2.07041830e-01  1.64885577e-02 -5.43762922e-01
  1.55834675e-01  8.45220685e-03 -3.47425044e-01  2.39809245e-01
 -1.22912504e-01  6.81108516e-03 -5.54543771e-02  1.42035216e-01
  6.91883732e-03 -6.64201975e-02  4.82868403e-04 -3.67233634e-01
 -3.53639007e-01  2.47303754e-01 -5.49809456e-01  9.96081159e-02
 -2.35740505e-02  1.83813661e-01 -9.19152051e-04 -3.00542489e-02
  2.10050434e-01  2.52018064e-01  4.18079108e-01 -2.51740128e-01
 -2.84217447e-01 -2.67794847e-01 -9.96746570e-02  4.20616940e-02
 -5.11699736e-01 -7.26210773e-02 -1.00765802e-01 -1.51741859e-02
  3.52647394e-01  4.98848170e-01 -8.35412145e-02  3.33225206e-02
 -3.54100347e-01  3.48021924e-01 -2.33116269e-01  7.73238316e-02
  1.84698194e-01 -1.51694575e-02  5.23524344e-01  2.15660304e-01
  2.34455317e-01  2.81491756e-01  2.54153371e-01 -2.96834916e-01
  2.71542132e-01 -4.46702540e-03  2.50948787e-01  4.45952564e-01
  1.92802638e-01  3.58379513e-01 -3.00456375e-01  4.77822065e-01
 -1.56311870e-01 -1.90882027e-01  1.69340670e-01 -2.07978338e-01
  7.10820615e-01 -3.76929253e-01 -6.85505942e-02 -1.60063297e-01
  2.39565641e-01 -2.48648413e-03 -1.46715820e-01  5.59880957e-02
  1.29320890e-01  3.09734702e-01 -3.70387316e-01  5.92955500e-02
 -1.45570934e-01 -2.08573312e-01 -1.97053939e-01 -3.98815274e-01
 -2.19961643e-01  1.33127838e-01 -2.92148530e-01  4.65453789e-02
  9.97208711e-03  7.37249106e-03 -2.40235373e-01  2.11679488e-01
  1.62206560e-01 -2.90409267e-01  1.04599088e-01  1.02985069e-01
 -1.18527755e-01  1.29748195e-01  4.44157749e-01 -2.51727462e-01
 -5.34220114e-02 -1.36286076e-02  5.08183300e-01  1.48120597e-01
  4.47350442e-01 -4.39828247e-01  1.11335009e-01 -1.36793211e-01
 -1.29811823e-01  4.47326928e-01  2.93371007e-02  1.09979182e-01
 -2.74631947e-01  6.34239435e-01  1.19466960e-01 -4.79710698e-02
  2.67713249e-01 -2.24170730e-01 -2.91235119e-01  1.10489532e-01
  2.99172699e-01 -9.08505619e-02 -7.60418922e-02 -3.09485376e-01
  1.98183414e-02  7.02987686e-02 -1.19743399e-01 -1.81336641e-01
 -1.65098935e-01 -4.36333455e-02 -1.24197707e-01 -1.91186294e-01
 -2.14574128e-01  1.86446056e-01 -2.58246996e-03 -2.35033020e-01
 -2.42929608e-01 -1.41526863e-01 -2.36550584e-01 -1.74432352e-01
  1.43636931e-02 -3.10450494e-01  3.45804423e-01  3.54969800e-01
 -6.18099347e-02  3.42038691e-01  9.15291607e-02  9.94593874e-02
 -2.78867841e-01  1.24327466e-01 -1.42175823e-01  4.02699679e-01
  2.06541717e-02 -2.89200488e-02  2.08020478e-01  2.53315091e-01
 -3.47339630e-01  1.75139159e-01 -3.76307338e-01  3.43008712e-02
  2.66423762e-01 -2.13673025e-01 -2.92830288e-01 -2.49209240e-01
  2.73264170e-01  2.12517351e-01 -7.60727152e-02  1.67084843e-01
 -2.32389033e-01  2.33456075e-01  4.75646049e-01 -3.01142722e-01
 -9.06275436e-02  9.54497755e-02  1.62199110e-01 -1.51329264e-01
  5.93403354e-03 -2.14890301e-01  6.60520867e-02  7.77726099e-02]"
FPE in BatchMatMul stat:awaiting response type:bug stale comp:lite TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 18.04.6

### Mobile device

_No response_

### Python version

Python 3.8.3

### Bazel version

bazel 5.3.0

### GCC/compiler version

gcc 7.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?
Construct a malicious BatchMatMul operator model with a shape of 4x0x0x0. 
The `input_shape.Dims(i)`  variable may be equal to 0, leading to a division by zero error.
```cpp
// transpose_utils.cc
size_t Flatten(const RuntimeShape& input_shape,
               const RuntimeShape& output_shape, const TransposeParams& params,
               RuntimeShape* non_flatten_input_shape,
               RuntimeShape* non_flatten_output_shape,
               TransposeParams* non_flatten_params) {
  // Calculate the total size of non-flatten dimensions.
  int skip_dims_cnt = 0;
  size_t flat_size = input_shape.FlatSize();
  for (int i = 0; i < params.perm_count; ++i) {
    if (params.perm[i] == i) {
      flat_size /= input_shape.Dims(i);  // FPE
```
[BatchMatMul_FPE.zip](https://github.com/tensorflow/tensorflow/files/12655488/BatchMatMul_FPE.zip)


### Standalone code to reproduce the issue

```shell
When I use the benchmark tool for PoC validation, it causes the TensorFlow Lite inference process to be subjected to a DOScoredump).


 ./benchmark_model --graph=../poc/BatchMatMul_FPE.tflite
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [../poc/BatchMatMul_FPE.tflite]
INFO: Loaded model ../poc/BatchMatMul_FPE.tflite
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: The input model file size (MB): 0.0006
INFO: Initialized session in 155.728ms.
INFO: Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
[1]    26298 floating point exception (core dumped)  ./benchmark_model --graph=../poc/BatchMatMul_FPE.tflite
```
```


### Relevant log output

_No response_",True,"[-4.62599337e-01 -1.99977487e-01 -2.32298508e-01  3.83194648e-02
  2.72945046e-01 -2.76597530e-01  7.35462084e-03  1.94319248e-01
 -3.35702926e-01 -1.54419452e-01 -6.58411235e-02 -6.41050041e-02
 -2.33510107e-01 -8.38111937e-02  3.93718891e-02  1.59142122e-01
 -1.72848120e-01 -2.13564724e-01  1.14636451e-01  6.09916821e-02
 -1.98918998e-01 -9.14161056e-02 -2.19454169e-01  5.26729450e-02
  2.92570651e-01  1.20525174e-01 -4.14205372e-01 -7.75982589e-02
 -1.10434353e-01  1.33079708e-01  1.68812424e-01  3.00538719e-01
 -6.10277392e-02  3.09451018e-02  9.05493647e-02  8.14837664e-02
 -6.35188296e-02 -1.18614078e-01 -4.09078598e-01  5.54193929e-03
  2.50420570e-02  1.03707939e-01  8.83413702e-02 -1.67816937e-01
  3.54563504e-01 -1.98616147e-01 -7.33635109e-03 -6.01219982e-02
 -1.09587923e-01 -1.12760320e-01 -1.38940457e-02 -6.95767030e-02
 -2.98336178e-01 -2.95049787e-01 -1.48394313e-02 -1.63786620e-01
  1.72402591e-01 -5.40871844e-02  7.92147219e-02  2.50352114e-01
 -6.13074116e-02 -2.49399953e-02  9.52579379e-02 -1.58635482e-01
  1.11220002e-01  5.07562049e-02  1.11896835e-01  2.96611488e-02
  5.57537496e-01 -2.74295300e-01 -4.91079502e-03 -3.85929830e-02
 -3.92288655e-01  5.61344996e-03  1.09762065e-01  6.25572652e-02
  3.81852463e-02  7.34454244e-02  2.66514003e-01 -1.96241885e-01
 -4.08683345e-02 -3.99256915e-01 -2.03828782e-01 -2.02577055e-01
  1.72903225e-01 -6.64386004e-02  4.03013736e-01  2.08361074e-01
  4.62438285e-01 -1.11626960e-01  2.55217284e-01  6.41771078e-01
 -1.66640077e-02  7.21190497e-02  4.35063988e-01  1.70260787e-01
  2.41197824e-01 -3.61402407e-02  1.26065388e-02 -1.62551627e-02
 -1.49242401e-01 -4.14412051e-01  1.41067490e-01  2.98248269e-02
 -1.52027369e-01 -2.29618207e-01  1.86621442e-01  1.33339509e-01
  2.24452108e-01 -2.67213732e-01  1.74373165e-01 -1.10674715e-02
  2.21755534e-01 -2.66255379e-01  2.13415995e-02 -7.28079006e-02
 -1.72321230e-01 -1.24689266e-01  9.71486419e-02  5.28307378e-01
  8.87964368e-02 -1.09040201e-01  1.22582197e-01  1.10882800e-02
  4.29889500e-01 -3.63560170e-02  1.32904828e-01 -2.79149786e-03
 -9.39021111e-02 -8.13188106e-02  1.67855680e-01  1.53395653e-01
  8.49437341e-02  1.57585412e-01  1.75222665e-01 -2.81552859e-02
 -4.28383723e-02 -1.73826143e-01 -9.12727863e-02 -2.26951167e-01
 -3.82417083e-01  9.51670408e-02 -1.50490418e-01 -4.84160841e-01
  4.72118147e-02  2.44910300e-01 -4.12981480e-01  1.62113085e-01
 -2.45148242e-01  1.98802620e-01 -2.93108076e-02 -5.32212853e-02
 -4.30918299e-02  3.94313008e-01  1.95178151e-01  1.22655883e-01
  2.84829080e-01 -6.79560006e-04  2.67627656e-01 -3.35480690e-01
 -5.23222759e-02  4.88949001e-01 -1.30435914e-01 -2.60376096e-01
  1.05733387e-02  2.00218916e-01 -2.89803088e-01 -1.66880086e-01
  3.69194485e-02  5.76921761e-01 -1.37924999e-01 -8.88611227e-02
  7.43625015e-02 -1.93671003e-01 -2.58077197e-02 -4.38835137e-02
  3.10363114e-01 -3.76180321e-01  3.33845615e-02  3.96902025e-01
  1.52718008e-01  4.24268812e-01  1.28444910e-01  1.96849018e-01
 -6.31090626e-03  2.39129379e-01  2.81485796e-01  2.16242313e-01
 -6.89057633e-02 -1.71887074e-02 -5.08853972e-01 -6.08642623e-02
  3.91402245e-01 -2.27288350e-01 -7.18285292e-02  8.97222608e-02
  1.91474870e-01  4.78979200e-02  1.00475699e-01  7.17677474e-02
 -2.01576464e-02 -2.77555138e-01  9.42901298e-02 -6.34668246e-02
  3.17916334e-01 -9.34375226e-02 -2.58886993e-01 -3.51823926e-01
 -3.07961628e-02  2.91743129e-02 -1.47549212e-01 -4.85152453e-01
  6.22039437e-02 -1.07727170e-01 -2.03197181e-01  2.59201884e-01
  1.58198506e-01 -1.99441239e-02 -1.75855428e-01 -1.75093170e-02
  1.05209261e-01 -1.15710929e-01 -8.70205462e-02 -5.02413929e-01
 -2.43899189e-02  1.96186408e-01 -2.65158653e-01  1.50385708e-01
  2.20694095e-02  1.32822692e-01  2.25168332e-01  1.51586920e-01
  3.56523305e-01  2.21851051e-01  4.77733910e-01 -2.02016950e-01
 -1.06709562e-01  1.43286943e-01 -3.79042983e-01  1.04101554e-01
 -3.32013547e-01 -2.93383002e-01 -1.00172296e-01 -1.07411504e-01
  1.20763823e-01  2.01268852e-01 -1.70852363e-01 -5.55449165e-02
 -4.32507575e-01  2.36379802e-01 -4.23392951e-01  6.58561587e-02
  2.82321721e-01  4.13295999e-03  3.97055566e-01  1.76080376e-01
  5.82869016e-02  1.68189913e-01  3.03969622e-01 -1.68388635e-01
  3.60093653e-01  8.96295011e-02  1.04001194e-01  4.24728513e-01
  2.97959387e-01  1.54571980e-01 -4.47430104e-01  3.99316728e-01
 -9.71090049e-02  2.51520332e-02  2.67310351e-01 -2.54461080e-01
  5.69544792e-01 -8.73650089e-02  2.06713989e-01 -7.86105618e-02
  2.75238037e-01 -9.93183106e-02 -3.97497118e-02  1.49001256e-01
  1.60305157e-01  3.72303694e-01 -4.61168528e-01  1.57142252e-01
  5.05493172e-02 -2.26390362e-01  2.00859029e-02 -6.41712070e-01
 -1.07239872e-01  5.07380627e-02 -2.25447863e-01  2.21734092e-01
 -1.94015633e-03 -1.62092149e-01 -1.87831670e-01  1.47262827e-01
  8.12790617e-02 -1.80920720e-01  1.31139055e-01  5.32359034e-02
 -1.11334942e-01 -3.05934161e-01  4.10155296e-01 -3.25757086e-01
 -6.23144023e-02 -1.67385250e-01  3.79489601e-01  3.13702226e-01
  3.31152588e-01 -3.82752180e-01  9.66875777e-02 -7.22431242e-02
 -2.15175450e-02  2.61764616e-01 -1.04580641e-01 -5.41962609e-02
 -3.71626079e-01  7.55082965e-01  1.51782915e-01 -9.25413594e-02
  1.61401868e-01 -2.27470964e-01 -2.88309127e-01  1.70135424e-01
  2.65364885e-01 -1.43660866e-02 -1.26995385e-01 -3.84210229e-01
 -3.29660550e-02  9.59997252e-02 -9.01921932e-03  2.23847199e-03
 -2.41753310e-01 -2.90550947e-01 -9.92170051e-02 -2.29673147e-01
 -4.71198827e-01  1.94271028e-01  2.35557482e-02 -4.62666392e-01
 -1.94750607e-01  2.49454305e-02 -2.09196195e-01 -1.92610934e-01
 -7.87453279e-02 -2.63944149e-01  2.75093734e-01  4.32569176e-01
 -2.75934283e-02  1.26510588e-02  9.85368043e-02  1.59066588e-01
 -2.94694245e-01 -9.34266746e-02 -1.64042100e-01  3.19215298e-01
 -6.37042969e-02 -4.58555110e-02  4.28216547e-01  1.27642542e-01
 -1.09846592e-01  2.77658880e-01 -3.17589879e-01 -1.27953235e-02
 -3.61103155e-02 -2.47629732e-01 -3.44352961e-01 -1.65740907e-01
 -3.39824855e-02  2.32513547e-01 -1.16567295e-02  2.17175335e-01
 -2.84212977e-01  3.52595955e-01  4.63487208e-01 -3.23142469e-01
 -2.52945423e-01  4.87587005e-02  3.74721169e-01 -1.30975053e-01
 -1.37268752e-01 -6.97972849e-02  3.79121184e-01  1.46067470e-01]"
module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface' stat:awaiting response type:bug stale comp:apis TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Very new to coding so am unfamiliar with what to say. I used tensor flow to run a model to find the best fitted line. Last week the model run perfectly. This week the script incurs an error upon running the model. Ran a script that was even older and incurred that same problem.

Had read that the issue is to do with conflict between _tensorflow_ and _keras_ but I don't understand what that means with relation to my code. 

I also read not to use the term 'python' when importing a library.

from tensorflow.**python**.keras.models import Sequential

I have to use 'python' as part of the syntax as google colab reports this message if I don't 
_Import ""tensorflow.keras.models"" could not be resolved(reportMissingImports)_

Is this something to so with my google colab environment? I updated tensorflow to the latest version but the problem still exists. 

I have no idea what to do and am quite stuck.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.activations import linear
from tensorflow.python.keras.optimizers import adam_v2
from tensorflow.python.keras.losses import MeanSquaredError

x = [ [23], [45], [78], [12]]
y = [ [12], [22], [36], [6]]

model = Sequential([
    tf.keras.layers.Dense(units=25, activation='relu', name='layer1'),
    tf.keras.layers.Dense(input_shape=(25,), units=15, activation='relu', name='layer2'),
    tf.keras.layers.Dense(input_shape=(15,), units=1, activation='linear', name='layer3')
] ,name=""Model1""
)

loss=MeanSquaredError()
opt=adam_v2.Adam(learning_rate=0.001)

model.compile(
    loss=loss,
    optimizer=opt
    )
model.fit(x, y, epochs=100)
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)
<ipython-input-27-a1b963ff7586> in <cell line: 26>()
     24     optimizer=opt
     25     )
---> 26 model.fit(x, y, epochs=100)

6 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1136          training_utils.RespectCompiledTrainableState(self):
   1137       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.
-> 1138       data_handler = data_adapter.get_data_handler(
   1139           x=x,
   1140           y=y,

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in get_data_handler(*args, **kwargs)
   1396   if getattr(kwargs[""model""], ""_cluster_coordinator"", None):
   1397     return _ClusterCoordinatorDataHandler(*args, **kwargs)
-> 1398   return DataHandler(*args, **kwargs)
   1399 
   1400 

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1150       self._steps_per_execution_value = steps_per_execution.numpy().item()
   1151 
-> 1152     adapter_cls = select_data_adapter(x, y)
   1153     self._adapter = adapter_cls(
   1154         x,

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in select_data_adapter(x, y)
    986 def select_data_adapter(x, y):
    987   """"""Selects a data adapter than can handle a given x and y.""""""
--> 988   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
    989   if not adapter_cls:
    990     # TODO(scottzhu): This should be a less implementation-specific error.

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in <listcomp>(.0)
    986 def select_data_adapter(x, y):
    987   """"""Selects a data adapter than can handle a given x and y.""""""
--> 988   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]
    989   if not adapter_cls:
    990     # TODO(scottzhu): This should be a less implementation-specific error.

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in can_handle(x, y)
    705   def can_handle(x, y=None):
    706     return (isinstance(x, (data_types.DatasetV1, data_types.DatasetV2)) or
--> 707             _is_distributed_dataset(x))
    708 
    709   def __init__(self,

/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _is_distributed_dataset(ds)
   1697 
   1698 def _is_distributed_dataset(ds):
-> 1699   return isinstance(ds, input_lib.DistributedDatasetInterface)

AttributeError: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'
```
",True,"[-0.48235115 -0.5250945  -0.21600276  0.20766222  0.22461638 -0.3038907
 -0.06085888  0.02357262 -0.47809806 -0.363914    0.01102531 -0.16722372
 -0.09491104  0.31875116 -0.09804586  0.39598963 -0.29752564 -0.11288199
  0.19462332  0.00702702 -0.00437069  0.03223607 -0.20391263  0.29249728
  0.08254324  0.23484796 -0.2792637  -0.27619606  0.01673512  0.26791182
  0.35530308  0.24767646 -0.16093566  0.0761672  -0.04209455  0.27426875
 -0.34381124 -0.2448936  -0.1433889  -0.10897147  0.0969419   0.21035518
  0.1786272  -0.05135951  0.03874471 -0.23068564 -0.035443   -0.15800357
 -0.1239125  -0.28541726 -0.02513832 -0.1192599  -0.4181655  -0.28503633
 -0.18661493 -0.14401641  0.11322004 -0.06388041 -0.06301234  0.23563106
 -0.12821847  0.07424484  0.04192069  0.0223576   0.07941467  0.18317908
  0.28071624 -0.02055064  0.50803924 -0.20163527  0.191593   -0.02237832
 -0.39108154  0.05053434 -0.04706843  0.07846288 -0.07016949  0.10408965
  0.30824968 -0.03865393 -0.05752654 -0.17157    -0.02385708 -0.0504822
  0.10381736 -0.12372994  0.31586844  0.15822941  0.3947097  -0.11475569
  0.6029525   0.5188795  -0.0550173   0.16669185  0.34432286  0.07447742
  0.13598707  0.13711734  0.03673508 -0.2789634  -0.08564509 -0.3839155
 -0.16189963 -0.02489997 -0.18841973 -0.08405381  0.13870117 -0.12323071
  0.16732453  0.0466281   0.17766137 -0.0669349   0.30431357 -0.110126
 -0.12460221  0.00319597 -0.08081302  0.00927378 -0.0435446   0.7437724
  0.10803275  0.10868947  0.10840522  0.23876533  0.45255673  0.16487285
  0.00800626 -0.03602988  0.10359863 -0.13126811  0.1659504   0.12639242
 -0.15384325  0.2853008  -0.03855303  0.19909394 -0.2087856  -0.24024524
 -0.24303626 -0.00309382 -0.20058784  0.12053062 -0.29365867 -0.5505991
  0.12740311  0.1436603  -0.05496065  0.38271037 -0.14431338  0.06608228
 -0.01996991  0.2020632  -0.29364967  0.4925498   0.07079994  0.22855547
  0.3233028  -0.04373719  0.10905717 -0.58513224 -0.04085596  0.5061965
 -0.14238857 -0.26851937 -0.17921853  0.15191644 -0.4219785  -0.29119265
  0.02858914  0.34175372 -0.14847541 -0.11819059  0.03709732 -0.03056211
  0.00574248 -0.07096332  0.21679468 -0.7022631  -0.02892634  0.33822173
  0.06674284  0.4011894   0.01361633  0.21711573  0.05069834  0.0486424
 -0.02332249 -0.03698389 -0.01915601  0.03444989 -0.44886348 -0.21518636
  0.5966245  -0.11815359 -0.05381941  0.18978313  0.22636631 -0.13783127
  0.20329408  0.00831738 -0.03817776 -0.06702737 -0.10384539  0.05868452
  0.10425358 -0.27067876 -0.01682147 -0.39436305 -0.30167133 -0.05776888
 -0.12890303 -0.62160116  0.10201104 -0.05960544 -0.3313561   0.39498347
  0.17933506  0.23686343 -0.20373529 -0.0231209   0.0078685  -0.15865205
  0.1246139  -0.34421313 -0.2039069   0.21022254 -0.22178257  0.23046573
 -0.03536851  0.14848256  0.06799313  0.2652666   0.59420484  0.4038623
  0.31062254 -0.22371483 -0.24956375  0.04715491 -0.15453048  0.19254683
 -0.4067735  -0.01079927  0.0625463  -0.07290322  0.06044664  0.51988757
 -0.10586789 -0.23735392 -0.49953067  0.10697412 -0.2584548   0.01476244
  0.43495005  0.1476706   0.6832473   0.16442692  0.19680609  0.07290733
  0.12507923 -0.38014564  0.31929868  0.12637962  0.07735875  0.5104022
  0.11926697  0.10983218 -0.45256388  0.59793025  0.26712337 -0.21907052
  0.00390388 -0.54886335  0.6640394  -0.27318966 -0.05947525 -0.0226136
  0.24224801 -0.00640214  0.06303224  0.14119229  0.06176813  0.41667253
 -0.34541696 -0.15714325  0.0430428  -0.2747286  -0.13762015 -0.6666409
 -0.18639585  0.1933545  -0.19268793  0.25146806  0.13240337  0.01071464
 -0.17514458  0.07913389  0.12920994 -0.19673559  0.17425027  0.48359454
 -0.09052563  0.16220856  0.32227328 -0.43547446 -0.1917961  -0.10095285
  0.422145    0.13222629  0.44318402 -0.56047547  0.05836156 -0.15294287
  0.09573901  0.5334318   0.00165472 -0.10012345 -0.34249902  0.64119446
  0.33612517 -0.08237567  0.20865466 -0.17371993 -0.4410378   0.1424897
  0.26494354 -0.06721147  0.12557673 -0.29310113 -0.08333272  0.1307258
 -0.193701    0.04937971 -0.15668285 -0.03419529 -0.06141148 -0.25161958
 -0.6261935   0.18851498 -0.05604074 -0.27396315 -0.04478417 -0.2395609
 -0.02315458 -0.2696336  -0.14340645 -0.33992785  0.48830566  0.5331408
 -0.24063781  0.00556044  0.08472233  0.05782651 -0.5723332  -0.04344991
 -0.20133159  0.35614103  0.10378137 -0.37928107  0.43719912  0.42214006
 -0.47595093  0.03287997 -0.20255399 -0.00721929  0.24261552 -0.24392848
 -0.20696057 -0.29123905  0.11197302  0.4258016  -0.21781713  0.23057115
 -0.43290251  0.36514804  0.4185877  -0.38571376 -0.16565499  0.20486215
  0.13080436 -0.24399924 -0.03756681 -0.02267896  0.35393643 -0.24795619]"
TFLite inference order is not the same as TensorFlow model stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.13,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code

This is a custom depth-first inference layer, I try it using a (1, 7, 7, 4) tensor, the output is the same as the two layer cnn model and the output size is (1, 3, 3, 1).

```
create_conv_layer1 = Conv2D(filters=4, kernel_size=(3, 3), activation='relu', trainable=False, kernel_initializer=tf.initializers.Constant(0.5))
create_conv_layer2 = Conv2D(filters=1, kernel_size=(3, 3), activation='relu', trainable=False, kernel_initializer=tf.initializers.Constant(0.5))
```

```
class PatchBasedConv2D(Layer):
    def __init__(self, **kwargs):
        super(PatchBasedConv2D, self).__init__(**kwargs)
        self.layer_number = 2
        self.output_size = (3, 3, 1)
        self.expand_size = (1, 1)
        self.patch_stride = None
    
    @staticmethod
    def compute_last_patch_size(output_size, kernel_size, stride):
        input_height = (output_size[0] - 1) * stride[0] + kernel_size[0]
        input_width = (output_size[1] - 1) * stride[1] + kernel_size[1]
        return (input_height, input_width)
    
    def calculate_patch_count(self, input_size, patch_size, stride):
        return ((input_size[0] - patch_size[0]) // stride[0] + 1) * ((input_size[1] - patch_size[1]) // stride[1] + 1)
    
    def get_current_patch_possition(self, input_size, patch_size, stride, current_round):
        return ((current_round // ((input_size[1] - patch_size[1]) // stride[1] + 1)) * stride[0],
              (current_round % ((input_size[1] - patch_size[1]) // stride[1] + 1)) * stride[1])

    def build(self, input_shape):
        with tf.device('/CPU:0'):
            self.conv1 = create_conv_layer1
            self.conv2 = create_conv_layer2
            self.patch_size_tmp = self.compute_last_patch_size(self.expand_size, self.conv2.kernel_size, self.conv2.strides)
            self.patch_size = self.compute_last_patch_size(self.patch_size_tmp, self.conv1.kernel_size, self.conv1.strides)
            self.patch_stride = self.conv1.strides

    def call(self, inputs):
        if self.conv1.padding == 'same':
            inputs = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='CONSTANT')
            
        num_patch = self.calculate_patch_count((inputs.shape[1], inputs.shape[2]), self.patch_size, self.patch_stride)
        
        number_patch_in_row = int(self.output_size[1] // self.expand_size[1])
        output_feature_map_tmp = None 
        output_feature_map = None
        
        for current_round in range(num_patch):
            position = get_current_patch_possition((inputs.shape[1], inputs.shape[2]), self.patch_size, self.patch_stride, current_round)
            patch_output = self.conv1(inputs[:, position[0]: position[0] + self.patch_size[0], position[1]: position[1] + self.patch_size[1], :])
            patch_output = self.conv2(patch_output)
            
            if output_feature_map_tmp is None:
                output_feature_map_tmp = patch_output
            else:
                output_feature_map_tmp = tf.concat([output_feature_map_tmp, patch_output], axis=2)
            
            if (current_round + 1) % number_patch_in_row == 0 and current_round != 0:
                if output_feature_map is None:
                    output_feature_map = output_feature_map_tmp
                else:
                    output_feature_map = tf.concat([output_feature_map, output_feature_map_tmp], axis=1)
                output_feature_map_tmp = None
        
        return output_feature_map
```

### 3. Failure after conversion
I am working on implementing depth-first inference on Tflite micro, above is my custom layer. You can see that in the call function, I picked out a patch of the input image and did 2 layers of Conv first, then went on to the next patch. It works just fine in my jupyter-notebook. However, when I convert it to tflite using TFLiteConverter, I found that it first expand all the patches, then do 2 Conv on all the patches. This turns out to be an inference layer-by-layer and uses up even more memory, why is that?


![ 2023-09-18 7 33 19](https://github.com/tensorflow/tensorflow/assets/68526411/e3768de1-b03f-44a8-8a61-51318c8b8f92)",True,"[-5.32162964e-01 -7.02388406e-01 -8.23086575e-02  1.83790587e-02
  1.86144739e-01 -3.36377062e-02 -1.02330700e-01 -1.43392131e-01
 -3.49983536e-02 -2.33404294e-01 -1.23964995e-02  7.10827187e-02
 -1.93618596e-01  2.75263011e-01 -3.21101487e-01  3.62860560e-02
 -5.26268333e-02 -3.39878440e-01  1.76305816e-01 -8.22786391e-02
  5.17735481e-02  3.31107825e-02 -1.93489969e-01  3.31799865e-01
  2.73128688e-01  4.91109416e-02 -1.07176274e-01  4.01387662e-02
  6.07558712e-02  7.52709061e-03  1.86684757e-01  1.92690015e-01
 -1.24603018e-01 -8.25058669e-02 -2.96488285e-01  1.71249449e-01
 -1.10215254e-01 -8.76134038e-02 -2.53824174e-01 -4.96844947e-02
  4.51876149e-02  1.76169544e-01  4.33793962e-02  9.27227065e-02
 -2.36156583e-02  1.42408729e-01  3.06541294e-01 -9.70145222e-03
 -1.79069281e-01  8.53824914e-02 -1.05277009e-01 -1.00536950e-01
 -3.02998513e-01 -2.08275244e-01 -2.19560508e-02  1.60106540e-01
  2.78824151e-01 -6.75172433e-02 -1.35318711e-01  5.15774116e-02
  1.70575697e-02 -1.31343398e-02 -3.60358581e-02  6.88543171e-02
  2.93966353e-01  3.63882244e-01  1.36143118e-01 -2.56556123e-01
  3.26813340e-01 -2.20354110e-01  3.95117700e-02 -8.40078071e-02
 -3.01664561e-01  6.72998875e-02 -3.72136235e-01  2.65386626e-02
 -5.70426658e-02  2.09301203e-01  2.00933352e-01 -6.47876561e-02
  7.44382292e-02 -2.05671012e-01 -7.85387820e-04  1.17352501e-01
 -2.37141326e-02 -1.18668854e-01  1.31299675e-01  2.34924406e-01
  2.41962433e-01 -1.68869659e-01  3.83694768e-01  2.59530962e-01
 -1.33283585e-01  1.02818862e-01  3.99849355e-01  2.04882219e-01
  8.80741850e-02  3.35683942e-01  3.34968984e-01  7.97471479e-02
 -1.96471483e-01 -1.61404088e-01 -3.23815405e-01 -1.74128637e-01
  2.94718802e-01 -1.67528868e-01  4.67384607e-02  2.10687350e-02
  2.61028826e-01  6.29609376e-02  5.28381541e-02  4.35971692e-02
  2.20920190e-01  3.90429422e-02 -1.16648391e-01 -8.82791281e-02
  2.28567332e-01  2.28557840e-01  1.42345071e-01  3.33210647e-01
 -1.76833905e-02 -2.41140231e-01  1.51598826e-01  8.46184790e-02
  4.35993850e-01  1.20355226e-02 -2.42963403e-01 -1.07047975e-01
  2.45585501e-01  6.34600297e-02  2.02218682e-01  1.62349075e-01
 -2.91953325e-01  2.04059891e-02  4.97334376e-02 -6.51152059e-02
 -1.21737756e-01 -1.33605506e-02 -3.73810470e-01  9.94618982e-02
 -2.06607044e-01 -1.14387557e-01 -2.77858943e-01 -1.02186173e-01
 -1.14890367e-01  6.04800768e-02 -1.74483940e-01 -6.79818913e-04
 -1.94859684e-01  4.06048968e-02 -1.29355252e-01  7.81594291e-02
 -2.65463591e-02  1.87944233e-01  1.36572585e-01  5.35367662e-03
  1.07569218e-01 -1.03948582e-02  9.33719128e-02 -1.89536765e-01
 -1.32228538e-01  1.83010563e-01 -1.06002539e-01 -1.26189530e-01
  2.05150560e-01  2.70825028e-01 -4.54916835e-01 -1.98255852e-01
  2.73265749e-01  1.46517977e-01 -3.29764038e-02 -3.55331123e-01
 -7.66781121e-02 -9.44131389e-02 -2.78666914e-02 -1.55838922e-01
  2.48628199e-01 -3.95608246e-01  3.72459628e-02 -2.63014734e-01
  1.79629236e-01  7.42596984e-02  1.26029737e-02  6.48106411e-02
 -2.17199579e-01  2.16140896e-02  1.62796587e-01  1.81743354e-01
 -2.81874895e-01  8.36010575e-02 -2.14666784e-01 -5.29106483e-02
  3.38264763e-01  1.49745822e-01 -2.34988362e-01 -6.41865283e-02
  3.67295384e-01 -7.84954205e-02  9.72932503e-02  1.19425744e-01
 -1.09226331e-01 -6.39833510e-02  7.55764395e-02 -5.70391938e-02
 -9.00196880e-02 -2.24767566e-01  9.19571072e-02 -2.71820724e-01
 -3.88012201e-01 -3.92841324e-02  1.80174470e-01 -2.59585649e-01
 -1.85166985e-01 -3.90125141e-02 -8.65842625e-02  5.43664582e-02
 -2.99862117e-01 -1.64882988e-01 -3.04383099e-01  2.36846954e-01
  6.87572658e-02 -3.21098976e-02  2.21945733e-01 -1.35959506e-01
 -4.06701714e-01 -1.06518097e-01 -2.54294336e-01  3.41454506e-01
 -8.83284807e-02  2.35274166e-01  1.07582491e-02  1.42355740e-01
  3.55911255e-01  8.71919170e-02  1.53052151e-01 -3.79965231e-02
  8.56392533e-02 -1.72699481e-01 -1.22294202e-02  4.96208146e-02
 -2.05654383e-01 -2.56226271e-01 -2.21594591e-02  4.43374813e-02
  4.58911061e-03 -9.28811077e-03 -1.03864565e-01 -1.31951451e-01
 -1.40114203e-01  2.49807015e-01 -1.75498426e-01 -1.80842325e-01
  2.99603999e-01  1.69364184e-01  1.62335709e-01  1.50595188e-01
  2.34643929e-04  1.11166254e-01 -3.27673294e-02  5.34977019e-02
  1.67706281e-01  2.94185430e-01  1.57130182e-01  7.67261267e-01
  1.23582631e-01  2.26121426e-01 -1.49013817e-01  1.11470833e-01
 -2.17757285e-01 -1.67053074e-01  7.94771686e-03 -1.30187407e-01
  3.49472135e-01 -3.01316500e-01  1.54715195e-01  2.25564271e-01
  2.17820138e-01 -7.63292313e-02 -5.40626794e-02  1.56033143e-01
 -2.03915089e-02  3.13960433e-01 -4.17661488e-01  8.71831179e-02
  1.16493419e-01 -2.36916617e-02 -1.15427420e-01 -3.88135970e-01
 -1.77807823e-01  1.03108943e-01 -3.18349779e-01  7.63109624e-02
 -3.11162658e-02 -4.25778516e-02  5.83543908e-03 -1.53644076e-02
  2.82378703e-01 -2.20248461e-01  3.35615337e-01 -4.77294624e-02
 -5.94804063e-03  2.46062726e-01  1.46591485e-01 -1.75600916e-01
 -3.71139981e-02  1.38177216e-01  3.70323449e-01  1.08948246e-01
  2.65274793e-01 -1.43392697e-01  2.20137835e-01 -2.53536999e-02
  1.29500970e-01  3.40440810e-01 -1.45362228e-01 -4.88821641e-02
 -1.36059538e-01  2.89433479e-01  9.35254395e-02  1.03893839e-01
  1.18838064e-01 -2.56360948e-01 -1.80366620e-01  9.98397768e-02
  1.09049276e-01  1.55983061e-01  3.90232578e-02 -4.31316160e-02
 -1.54013932e-01 -9.74175110e-02 -2.36328349e-01 -1.44389838e-01
 -1.61804825e-01  4.78107035e-02 -1.04668915e-01  5.66230267e-02
 -1.56192496e-01  6.28579929e-02 -1.16689175e-01 -1.74519062e-01
 -2.66539995e-02 -3.44059050e-01  3.68237197e-02 -2.64328301e-01
  4.29323018e-02 -9.03681517e-02  4.89406548e-02  1.16485566e-01
  5.38563393e-02  1.80733070e-01 -8.28235969e-02  1.28733516e-01
 -2.21713930e-01 -1.08763427e-01  8.65615532e-03  3.83658230e-01
 -1.55644655e-01 -4.79619019e-02  1.54282361e-01  2.66722769e-01
 -1.95068330e-01 -1.81480378e-01 -3.03514600e-01 -1.69071764e-01
  1.47988185e-01 -6.65693879e-02 -6.86132014e-02 -3.49161088e-01
  1.63713723e-01  4.96331900e-01 -1.33920051e-02  2.02351451e-01
 -1.77340820e-01  2.09578067e-01  3.30172956e-01 -2.06690103e-01
  8.98032188e-02 -1.30654216e-01  3.68059501e-02 -2.15055242e-01
 -4.19285223e-02 -1.11917583e-02 -1.80516183e-01 -8.75508860e-02]"
tensorflow.tf concurrency issue type:bug comp:ops TF 2.11,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

unknown 2.11.0 (from nvcr.io/nvidia/tensorflow:23.03-tf2-py3))

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda_12.1.r12.1/compiler.32415258_0

### GPU model and memory

_No response_

### Current behavior?

We found that running tensorflow.concat operation will have concurrency issue only in the Nvidia tensorflow image. We also tried with other image but the issue cannot reproduce.

We expect that tensorflow.concat work fine for multi-thread environment. But it's not. Is it expected? 
FYI, adding a lock for concat can avoid such issue. What's the best practice?

Our environment:
GPU: A100
image: nvcr.io/nvidia/tensorflow:23.03-tf2-py3
driver: NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from concurrent.futures import ThreadPoolExecutor, as_completed

client_num = 10
repeat = 10000
executor = ThreadPoolExecutor(max_workers=client_num)
future_2_input_shapes = {}

# import threading
# lock = threading.Lock()

test_cases = [
    [[1, 2] * 31, [2, 3]],
    [[1, 2, 8, 90] * 31, [2, 3, 3, 1]],
    [[[7, 4, 3], [8, 4, 3]], [[2, 10, 3], [15, 11, 3]] * 63],
]


def do_concat(t1, t2):
    # with lock:
    #     return tf.concat([t1, t2], 0)
    return tf.concat([t1, t2], 0)


print(""creating task"")
for _ in range(repeat):
    for test_case in test_cases:
        a = tf.constant(test_case[0])
        b = tf.constant(test_case[1])
        future = executor.submit(do_concat, a, b)
        future_2_input_shapes[future] = a.shape, b.shape

print(""waiting task"")
count = 0
for future in as_completed(future_2_input_shapes.keys()):
    print(f""{count}: {future_2_input_shapes[future]}"")
    data = future.result()
    count = count + 1




```
```


### Relevant log output

```shell
...
17086: (TensorShape([62]), TensorShape([2]))
17087: (TensorShape([2, 3]), TensorShape([126, 3]))
17088: (TensorShape([124]), TensorShape([4]))
17089: (TensorShape([124]), TensorShape([4]))
17090: (TensorShape([2, 3]), TensorShape([126, 3]))
17091: (TensorShape([124]), TensorShape([4]))
17092: (TensorShape([2, 3]), TensorShape([126, 3]))
Traceback (most recent call last):
  File ""x.py"", line 37, in <module>
    data = future.result()
  File ""/usr/lib/python3.8/concurrent/futures/_base.py"", line 437, in result
    return self.__get_result()
  File ""/usr/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/usr/lib/python3.8/concurrent/futures/thread.py"", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""x.py"", line 22, in do_concat
    return tf.concat([t1, t2], 0)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 7215, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__ConcatV2_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} ConcatOp : Ranks of all input tensors should match: shape[0] = [124] vs. shape[1] = [126,3] [Op:ConcatV2] name: concat
```
",True,"[-0.41224194 -0.6235925  -0.2247626  -0.14154325  0.09567621 -0.3053259
 -0.05405764 -0.01655694 -0.23958284 -0.39620167  0.09288146  0.16784026
 -0.11060401  0.19834414 -0.13865945  0.3394654  -0.01386579 -0.22012033
  0.10640852 -0.02763148 -0.2521555  -0.21295771 -0.21189833  0.20064798
  0.06586875  0.2673537  -0.33715284 -0.18277931 -0.09973217  0.34588802
  0.3874521   0.08921898 -0.15051532 -0.07857089  0.08562526  0.24201545
 -0.36639264 -0.24452186 -0.21975748 -0.09093527  0.01204164  0.0503735
  0.05452992 -0.21579948 -0.21771829 -0.14157686 -0.00511755  0.03399591
 -0.17621389 -0.06387537  0.13232136  0.0848013  -0.4650186  -0.23582675
  0.07216786  0.04800773  0.0898912   0.04356813  0.10599466  0.2575885
  0.08144167 -0.06119078  0.12695362  0.06816366  0.18692262  0.09630197
  0.35576856 -0.08740368  0.4525665  -0.35423124  0.04646388 -0.12721992
 -0.39369133  0.05317579  0.07986729  0.0754796  -0.05979606  0.18236801
  0.24991345 -0.10115522  0.09530977 -0.05317995  0.09026454 -0.11849731
  0.20883378 -0.098445    0.32640696  0.19821346  0.4758817  -0.22141762
  0.5155039   0.3176627   0.02485211  0.01621003  0.5010407   0.03202906
  0.0103519   0.2675209  -0.1669359  -0.09088163 -0.09199416 -0.38165367
 -0.07559503  0.20618139 -0.14730594  0.03096745  0.00678396 -0.18131332
 -0.03450612  0.04536283  0.06654043  0.06153972  0.33121562 -0.20007607
 -0.05773387 -0.1346379  -0.08771756  0.04081208  0.1580922   0.62099946
  0.12297033 -0.26956952 -0.10166158  0.21888502  0.44040427  0.08120353
 -0.1706455   0.07221473  0.16514546 -0.13760944  0.11667594  0.06869835
 -0.10071659  0.23545918  0.04334237  0.15696949 -0.20315209 -0.31039977
 -0.22903371 -0.23432717 -0.1359581   0.08092641 -0.03680553 -0.5045057
  0.31040406  0.20173165 -0.11129604  0.38817027 -0.11051751  0.17358358
  0.05621295  0.09940045 -0.01810086  0.3791303  -0.08896197  0.00166324
  0.49307093 -0.1511473   0.04908204 -0.5252334  -0.00317932  0.42713326
 -0.0480473  -0.09007634  0.06727517  0.12264117 -0.4264374  -0.32453203
 -0.08359309  0.37624192 -0.17705925 -0.19085093 -0.06317934  0.04541444
  0.17200944 -0.06345046  0.3168124  -0.5013949   0.00135684  0.29817116
  0.00732666  0.17035764  0.0900574   0.10195301  0.16384694  0.04363057
  0.19455099  0.27076787 -0.38990563 -0.15099442 -0.3328558  -0.28573543
  0.4701902  -0.10059401 -0.0953109   0.01629825  0.37602592  0.08082189
  0.00733709  0.19553487  0.0609774   0.00513239 -0.01859912  0.01234448
  0.0865812  -0.2706472  -0.26569906 -0.3476694  -0.3165751  -0.06075416
  0.15690017 -0.265573    0.0316504  -0.08115573 -0.36857417  0.20144305
  0.02885761  0.2305638  -0.04448368  0.09486981  0.238946   -0.04387156
  0.01122102 -0.26462197 -0.35108453  0.06885467 -0.4885275   0.06737009
 -0.054169    0.17803463 -0.20552078  0.05507136  0.21632372  0.18757758
  0.245835   -0.2044856  -0.08600092 -0.16274294 -0.18180612 -0.0705729
 -0.2398252  -0.1274839  -0.02473237 -0.15784799  0.43609464  0.25812525
  0.0702693  -0.07618722 -0.25007066  0.3319081  -0.23082949 -0.01002559
  0.38358808  0.07174293  0.47502726  0.32584667  0.22558703  0.06228538
  0.35156775 -0.13495533  0.26479602  0.20937796  0.06678604  0.47646725
  0.35216755  0.30091682 -0.15285417  0.36255533 -0.0443057  -0.06177921
 -0.01730182 -0.24043183  0.6666554  -0.23888515  0.15226275 -0.20236368
  0.412825   -0.15600392 -0.09813704  0.21624662  0.13542932  0.30048248
 -0.2786436  -0.06138657  0.03992844 -0.26997775 -0.13031116 -0.5416291
 -0.16654262  0.01976493 -0.31722423 -0.06273688 -0.10507279  0.03231311
 -0.30367294  0.19858687 -0.02092388 -0.11209495  0.21523635  0.1717166
 -0.06049285 -0.19535105  0.2949529  -0.322232   -0.06813857 -0.03586812
  0.42158598  0.20730236  0.23634276 -0.5731226   0.15478216 -0.02479596
 -0.08310191  0.31569624  0.08914533  0.16853863 -0.36431426  0.52125823
  0.36354446 -0.01689694  0.15704864 -0.28058836 -0.49980608 -0.04851041
  0.20605797 -0.1215722  -0.01034746 -0.29515362  0.06107607  0.11468172
 -0.13939843 -0.01703454 -0.10429385  0.02296468 -0.15628994 -0.07324336
 -0.2662478  -0.02188791  0.02249798 -0.42093578 -0.09060875 -0.2167781
 -0.21204549 -0.2568487  -0.20015892 -0.24408087  0.3526317   0.5460625
 -0.04936415  0.09425521 -0.0499836  -0.01582072 -0.2926008  -0.07667536
 -0.19790757  0.31942946 -0.13088901 -0.17444046  0.33504492  0.13762164
 -0.26912367  0.21800575 -0.22099563 -0.00461755  0.22885561 -0.26076263
 -0.2951894  -0.05801145  0.10077462  0.4313326  -0.20979695  0.27614474
 -0.21881643  0.04503538  0.5593004  -0.46683374 -0.11877435 -0.08892432
  0.2550886  -0.28021795 -0.05487165  0.00672186  0.20313345  0.01740267]"
Tensorflow failed build due to ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed. stat:awaiting response type:bug type:build/install stale subtype:windows,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

master branch, commit: a442440

### Custom code

No

### OS platform and distribution

Windows Server 2022

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.3.2

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 22, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 37, in <module>
    from tensorflow.python.tpu import api
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\tpu\api.py"", line 22, in <module>
    from tensorflow.python.tpu import bfloat16
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\tpu\bfloat16.py"", line 20, in <module>
    from tensorflow.python.framework import dtypes
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\framework\dtypes.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Users\CPPTES~1\AppData\Local\Temp\2\Bazel.runfiles_pr3ptqbl\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: F:/tensorflow/tensorflow/tensorflow/tools/pip_package/BUILD:252:10 Middleman _middlemen/tensorflow_Stools_Spip_Upackage_Sbuild_Upip_Upackage.exe-runfiles failed: (Exit 1): bash.exe failed: error executing command (from target //tensorflow:tf_python_api_gen_v2)

### Standalone code to reproduce the issue

```shell
git clone https://github.com/tensorflow/tensorflow.git F:\Tensorflow\tensorflow
cd /d F:\Tensorflow\tensorflow
pip3 uninstall -r tensorflow/tools/ci_build/release/requirements_common.txt --yes
pip3 install -r tensorflow/tools/ci_build/release/requirements_common.txt --upgrade
set PATH=F:\Tensorflow\tensorflow\..\tools;%path%
set PATH=F:\Tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
yes """" 2>nul | python ./configure.py
C:\Python39\python.exe -m pip install --upgrade pip
set TF_PYTHON_VERSION=3.9
set BAZEL_VC=C:\Program Files\Microsoft Visual Studio\2022\Enterprise\VC
set BAZEL_VC_FULL_VERSION=14.37.32822
set PATH=F:\Tensorflow\tensorflow\..\tools;%path%
set PATH=F:\Tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
bazel --output_user_root F:\bazelTemp build --jobs 8 --config=opt --local_ram_resources=4096 --host_cxxopt=""/D_DISABLE_CONSTEXPR_MUTEX_CONSTRUCTOR"" --subcommands //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_",True,"[-0.34298766 -0.6603499  -0.03404419  0.10888791  0.15348017 -0.40418118
 -0.26712555  0.09221599 -0.3914559  -0.23534329  0.01732248 -0.0414484
 -0.13410515  0.13803586 -0.17566651  0.5044768  -0.35248017 -0.17300792
  0.11884218  0.12888695 -0.15828568  0.03689182 -0.1774346   0.11363831
  0.12105402  0.09086983 -0.1721921  -0.13673934  0.17826204  0.11387123
  0.42815393 -0.16232026 -0.1097106   0.13376139  0.01859531  0.3766176
 -0.17505047 -0.19333771 -0.21186219 -0.03660395  0.212006    0.23211712
 -0.0196136  -0.07033654  0.024885   -0.0558295   0.07664151 -0.10575384
  0.02740467 -0.18039456 -0.18490304  0.01131341 -0.45862564 -0.26565218
 -0.22453603 -0.11794376  0.06035703  0.09953259 -0.01312683  0.17907941
 -0.09542318 -0.1304264   0.09394316 -0.13216251  0.09627315  0.079705
  0.47076333  0.1182941   0.465051   -0.16933292  0.15181643  0.01638511
 -0.5026985   0.16401824  0.08269954  0.30686644 -0.12340737  0.11070198
  0.41076195 -0.07837898 -0.1404932  -0.07201073  0.16452956 -0.07462247
  0.2953372   0.01219979  0.24816525  0.0703872   0.2884292  -0.19687198
  0.4847653   0.18431738 -0.18363822  0.18346953  0.5296062   0.16239624
  0.07189976  0.20851168 -0.0329228  -0.24706666 -0.09687332 -0.1963341
  0.06483464  0.12135111 -0.00995226 -0.11940835  0.2629656   0.05075426
  0.1602712  -0.09508815  0.1589186  -0.09428415  0.18161245 -0.08023965
 -0.13232924  0.10460825 -0.19003335  0.01278449 -0.19101086  0.7812961
  0.00484797  0.03207825  0.10746368  0.1443443   0.18052402  0.04351171
 -0.1287154  -0.03606391 -0.02188497 -0.12328124  0.21982723  0.34447017
  0.00322467  0.12875047  0.09072351  0.09109555 -0.26345032 -0.2879015
 -0.3485189  -0.27324012 -0.25386518  0.07926822 -0.23768927 -0.6613536
  0.07520828  0.13974333 -0.14587216  0.26652116 -0.06896683 -0.17338769
  0.12485941  0.15921846 -0.26292348  0.48083794  0.1544972   0.27480105
  0.40822476 -0.02765046 -0.17933789 -0.7461802   0.09654414  0.36619627
 -0.23010749 -0.0768987   0.23995371  0.08506788 -0.34661186 -0.2043628
 -0.0359009   0.31451896 -0.3452727  -0.02123787 -0.07750519 -0.11621577
  0.29725367 -0.02398781  0.16706792 -0.6149695  -0.2501984   0.2426984
  0.20221671  0.22804153 -0.12067289  0.23260257  0.0044548  -0.0231404
  0.01066956  0.05106699 -0.08126642  0.20015085 -0.42764288  0.21886143
  0.4589746  -0.19868714  0.00486639  0.17990068  0.19309747 -0.16371301
  0.07607726 -0.02363246  0.02507047  0.14875168 -0.12567313  0.19846672
 -0.01300354 -0.39952478  0.0505917  -0.37607816 -0.3710705  -0.12245443
 -0.25221655 -0.4274711   0.15726197  0.07194713 -0.3564462   0.09923436
  0.06798916  0.06535058 -0.14040323  0.05062996 -0.00991248 -0.14821972
 -0.09205672 -0.3517209   0.09592848  0.06364204 -0.19775242 -0.00295747
  0.19311859  0.35161996  0.07267761  0.0362721   0.36505473  0.2675987
  0.36799264 -0.13597034 -0.12520766  0.02506155 -0.04069157  0.04646628
 -0.35108805 -0.11378556  0.13125882 -0.00108681  0.15955128  0.3121766
 -0.24840583 -0.17982616 -0.4409964   0.4119016  -0.224858    0.0262449
  0.52239835  0.3606487   0.4776779   0.2880833   0.0597407   0.09196398
  0.25317705 -0.02651135  0.24520753  0.22898479  0.18141854  0.14900286
  0.18854116  0.21806714 -0.42176858  0.30865332  0.33558667 -0.09672034
  0.04417543 -0.38926977  0.35948002 -0.40498325  0.08193053  0.08801915
  0.4010619   0.01296005 -0.10426643 -0.00754175 -0.05157065  0.35262758
 -0.42414242  0.01973348 -0.00747547 -0.29317516 -0.10281332 -0.49630734
 -0.41542262  0.03075323 -0.06795688 -0.00158901  0.12141411  0.09478439
 -0.33449364  0.27543065 -0.02085935 -0.1474203   0.0495108   0.35483834
 -0.20323354  0.12513149  0.36179826 -0.37415892 -0.15441492 -0.13665581
  0.32637107  0.13404971  0.57098746 -0.5538496   0.12327521  0.14970863
 -0.16618285  0.5818331  -0.13567701 -0.13452655 -0.42243046  0.8696066
  0.04946062 -0.28190687  0.12348735  0.02227042 -0.23706898  0.07638974
  0.12933592  0.00200754 -0.12424748 -0.11449914 -0.20774718  0.28162298
 -0.23644307  0.03648039 -0.05912859  0.1360077  -0.20715715 -0.0204123
 -0.46395645  0.34854364 -0.04329162 -0.48688766  0.10573523  0.08440291
  0.2000593  -0.3144731  -0.00234204 -0.4916103   0.39817488  0.41311803
 -0.3037603   0.0988109   0.04492173  0.17264831 -0.6331537  -0.04601043
 -0.2320703   0.27624625  0.1437226  -0.15939577  0.30631965  0.52527606
 -0.26140475  0.16765338 -0.24043064 -0.13078693  0.41027617 -0.04817487
 -0.31401166 -0.1685265   0.02210266  0.3416101  -0.12156782  0.1026653
 -0.42490822  0.25321987  0.4742214  -0.24612054 -0.44194046  0.26924807
  0.02920536 -0.10787316 -0.1141718  -0.01009728  0.07587363 -0.15794809]"
Cast from int32 to float32 changes the value. stat:awaiting response type:bug stale comp:ops TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 Here what I see is that the bug actually happens b/c as we are doing the conversion from int32 to float32 the value changes, as there is a rounding error that occurs when casting an integer to a floating-point type as float32 is not the high precision floating point type like float64, and the bug will also remain the same if we change them even to int64 from int32.

### Standalone code to reproduce the issue

```shell
> The code is below 
>> tf.cast(tf.constant(52479845, tf.int64), tf.float32)
<tf.Tensor: shape=(), dtype=float32, numpy=52479844.0>
>> tf.cast(tf.constant(52479843, tf.int64), tf.float32)
<tf.Tensor: shape=(), dtype=float32, numpy=52479843.0>
```


### Relevant log output

```shell
### Relevant log output

_No response_
```
",True,"[-5.33724904e-01 -3.44878882e-01 -2.47849181e-01  7.60695934e-02
  1.24134421e-01 -4.85976934e-01 -2.58793999e-02  1.57855228e-01
 -2.71217257e-01 -2.92791367e-01  1.23393618e-01 -2.00240850e-01
 -2.89772809e-01 -1.35729499e-02 -1.53740853e-01  1.23909809e-01
 -2.73444295e-01  3.16608772e-02  2.42784053e-01  2.19934434e-01
 -1.52346611e-01  1.33016691e-01 -3.54195684e-01  2.21150815e-01
  1.69520497e-01  2.42003381e-01 -3.29997897e-01  1.80397600e-01
 -9.20190364e-02  2.69075572e-01  2.61556208e-01 -2.34220996e-02
  5.86192906e-02  6.13178536e-02  9.14796144e-02  2.47191966e-01
 -4.54493761e-01 -3.12366784e-01 -4.60672736e-01  9.18547623e-04
 -1.27520775e-02  2.81174660e-01  1.35730937e-01 -2.52787858e-01
  2.04866603e-01  6.65490180e-02 -1.93382859e-01 -1.10818766e-01
  6.31419271e-02 -1.53610140e-01 -6.09370545e-02  1.38565063e-01
 -4.91871387e-01 -2.96821296e-01 -2.17605948e-01 -1.37682617e-01
 -1.40854586e-02 -1.49115562e-01  3.11181657e-02  7.97564536e-02
  1.76568329e-01 -9.19651985e-02  1.16791807e-01 -1.11063816e-01
  1.49725512e-01  3.42386328e-02  4.32314277e-01 -9.35461372e-04
  4.35286522e-01 -4.47140962e-01  1.50192976e-01  6.38204142e-02
 -2.91141242e-01  1.03050373e-01 -3.75442579e-02  5.42997196e-02
 -2.38780260e-01  1.40562337e-02  2.92519420e-01 -3.02809179e-01
  3.14623564e-02 -2.66332865e-01 -1.18188918e-01 -2.54812717e-01
  2.47321442e-01 -1.23324879e-01  5.27174592e-01  7.56648555e-03
  3.89858723e-01  3.06447987e-02  4.45466220e-01  6.29023194e-01
  3.85986157e-02  1.74513683e-01  3.67063433e-01  3.35120857e-01
  2.10530192e-01  9.09341201e-02  3.84563357e-02 -1.02998324e-01
  2.17601359e-02 -2.18774170e-01 -1.04411423e-01  1.07144967e-01
 -2.13233635e-01 -2.67913222e-01  1.14368573e-01 -1.79650318e-02
  6.92188442e-02 -9.09276120e-03  2.32434154e-01  2.88841352e-02
  1.79841936e-01 -1.10051282e-01 -1.74434990e-01  3.77197675e-02
 -7.90922865e-02 -6.66990057e-02 -4.04956788e-02  5.46850562e-01
  2.32581764e-01 -1.74817458e-01 -2.21357554e-01  4.25815642e-01
  6.49134159e-01  1.21864647e-01 -1.66442752e-01  5.22756018e-02
  8.78036767e-02 -1.59757480e-01  2.41160452e-01 -1.00419872e-01
 -1.24176368e-01  5.52613810e-02 -1.56186268e-01  2.48865038e-01
 -2.59255648e-01 -1.67238250e-01 -4.32455719e-01 -3.52447689e-01
  9.75683704e-03  1.02928557e-01 -2.83208460e-01 -7.39651144e-01
  2.61523336e-01  1.94144785e-01 -7.91821256e-02  2.53896624e-01
 -2.61297882e-01  2.57366121e-01  9.07870382e-02  1.37649640e-01
 -4.44853425e-01  5.68360567e-01 -1.76563233e-01 -9.38533694e-02
  3.91930014e-01  4.63776365e-02  3.94467153e-02 -6.75130248e-01
 -5.47383912e-04  2.75387317e-01  5.78785222e-03 -2.54515052e-01
  2.30261624e-01  1.04494318e-01 -3.85990649e-01 -2.44185030e-01
 -1.43135311e-02  5.65882802e-01 -3.73425841e-01 -1.82869866e-01
  3.83678749e-02 -5.19362651e-02  1.46794260e-01  9.15995240e-02
  8.12439546e-02 -2.93965489e-01 -2.41171196e-02  3.83359790e-01
  3.07971776e-01  2.33867571e-01  1.29401833e-01  3.36077780e-01
  1.15618743e-01 -5.11344597e-02 -8.55611172e-03  1.91317901e-01
 -2.84863591e-01  6.81138635e-02 -5.11701345e-01  1.35440081e-02
  3.85140926e-01  9.57770944e-02 -1.21223234e-01  3.37044187e-02
  1.17786512e-01 -6.71638250e-02  7.53815472e-02 -2.79714689e-02
 -1.33866876e-01  1.89016759e-02 -1.32929564e-01 -7.30057135e-02
  4.86166850e-02 -3.99525702e-01 -9.74298939e-02 -4.84293371e-01
 -1.91078782e-01  3.78783107e-01 -1.48711920e-01 -6.55862689e-01
 -3.61736044e-02 -1.11020021e-01 -3.17950249e-01  9.01324898e-02
  1.50749251e-01  1.71081901e-01 -1.17907614e-01  6.09337687e-02
  1.38008669e-01 -2.87567139e-01  3.12976509e-01 -3.94646704e-01
 -2.09425107e-01  2.28252932e-01 -5.88861585e-01  8.91876295e-02
 -6.18582964e-02 -1.05910581e-02  1.76664293e-01  1.44651577e-01
  4.27007467e-01  3.06925505e-01  3.32318336e-01 -8.97551849e-02
 -2.85149008e-01 -7.14341849e-02  4.77954373e-03 -1.29923001e-01
 -5.05415916e-01 -1.71102375e-01  4.68054712e-02 -1.33708820e-01
  4.25066799e-01  5.16925812e-01 -7.66431913e-03  5.70564419e-02
 -3.08087409e-01  6.99559599e-02 -2.40875453e-01  2.62437999e-01
  4.39717799e-01  6.48419280e-03  4.94081378e-01  1.49924219e-01
  3.42366934e-01  2.09009349e-01  1.83197886e-01 -3.13407004e-01
  3.91766101e-01  1.60048872e-01  1.76951274e-01  2.90948689e-01
  1.79389074e-01  1.74588591e-01 -3.98445785e-01  4.22599345e-01
  1.92538410e-01 -1.59421176e-01  3.27991307e-01 -1.03723101e-01
  8.09971988e-01 -3.83956015e-01  1.27601266e-01 -2.90324330e-01
  3.28919172e-01 -3.35504077e-02 -1.64118428e-02  6.26586080e-02
  1.40959859e-01  2.39050299e-01 -2.64224857e-01  8.02011788e-02
  6.18565306e-02 -3.29792470e-01 -1.29651219e-01 -5.89197755e-01
 -1.92092597e-01 -5.13755530e-02 -2.52311110e-01 -4.32989374e-02
 -7.18512908e-02  1.15099460e-01 -3.22806060e-01  1.23905659e-01
  1.33761823e-01 -3.24390322e-01  7.20014870e-02  2.94240743e-01
 -2.04147607e-01  6.98286667e-02  2.88366854e-01 -3.92698526e-01
 -1.29656047e-01 -1.12937450e-01  4.85526919e-01  3.36271465e-01
  5.07804692e-01 -3.19250077e-01  7.93625861e-02 -1.06871419e-01
 -3.24815869e-01  5.26194692e-01 -1.07108675e-01 -4.75435257e-02
 -1.98018000e-01  5.52045345e-01  2.62094513e-02 -5.65546528e-02
  1.98443443e-01 -2.48571366e-01 -1.97053775e-01  2.94022918e-01
  2.67319381e-01 -7.03517720e-02  2.06644069e-02 -4.23681498e-01
  1.79675594e-01  1.58713937e-01 -7.70218149e-02 -2.49851234e-02
  2.35776938e-02 -1.47682860e-01 -1.99414566e-01 -2.11118728e-01
 -3.26347888e-01  1.00600675e-01 -7.34968260e-02 -2.81268597e-01
 -2.60872960e-01 -2.47619092e-01 -1.20911263e-01 -2.84272820e-01
  3.89300883e-02 -4.59964275e-01  5.41755199e-01  6.10745728e-01
 -5.68760522e-02  2.59737313e-01  8.25178772e-02  1.51846195e-02
 -1.73830509e-01  1.83310896e-01 -6.06119409e-02  1.75214633e-01
 -8.16373248e-03 -9.84364450e-02  3.34402323e-01  1.77548617e-01
 -4.37545300e-01  3.86561722e-01 -4.74036217e-01  4.43676785e-02
  1.39895350e-01 -2.60707259e-01 -1.11422509e-01 -1.85890004e-01
  2.65061587e-01  8.44537653e-03 -2.25782469e-01  1.92068473e-01
 -4.12677586e-01  4.02830124e-01  6.87159956e-01 -4.65147227e-01
 -1.41140325e-02  2.03585893e-01  4.07968193e-01  3.92228514e-02
  1.74112380e-01 -1.68803632e-01  9.49229002e-02 -1.67058147e-02]"
Cast from int32 to float32 changes the value. stat:awaiting response type:bug stale comp:ops TF 2.12,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.12.0 through tf-nightly (v1.12.1-99894-g5bef7ce6955 2.15.0-dev20230915)

### Custom code

Yes

### OS platform and distribution

nvcr.io/nvidia/tensorflow:23.07-tf2-py3 docker image

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Casting some int32 values to float32 causes a change in value.

### Standalone code to reproduce the issue

```shell
>>> tf.cast(tf.constant(52479843, tf.int32), tf.float32)
<tf.Tensor: shape=(), dtype=float32, numpy=52479844.0>
>>> tf.cast(tf.constant(52479845, tf.int32), tf.float32)
<tf.Tensor: shape=(), dtype=float32, numpy=52479844.0>
```
```


### Relevant log output

_No response_",True,"[-3.14301729e-01 -5.06773710e-01 -3.02169740e-01 -2.41216607e-02
  1.31118864e-01 -3.73571634e-01  4.90971953e-02  2.03029305e-01
 -1.82109445e-01 -2.81090319e-01 -1.52541131e-01  6.68771937e-03
 -3.17500472e-01 -5.76298758e-02 -2.31152296e-01  6.26005083e-02
 -1.44409448e-01 -5.30228578e-02  1.31976008e-01  1.49615988e-01
 -2.06659704e-01 -4.90909815e-02 -3.58755559e-01  1.67898208e-01
  1.52907073e-01  1.83935747e-01 -1.75107613e-01  1.92995906e-01
 -1.29287735e-01  3.50799799e-01  1.74141586e-01  3.13366875e-02
  1.47595957e-01  9.47262496e-02 -3.25134248e-02  1.00310013e-01
 -3.23579222e-01 -2.68198729e-01 -4.31019306e-01 -1.13941580e-01
  6.07851893e-02  3.08766007e-01  1.10654511e-01 -9.41470936e-02
  1.01513088e-01  1.91640988e-01 -1.71098039e-01 -1.63280293e-02
  1.16160661e-01  7.55792437e-03 -1.10051408e-03  7.68922269e-02
 -3.63454044e-01 -2.16476828e-01 -1.60334870e-01  1.94949694e-02
  1.73270762e-01 -1.55721739e-01  1.24652952e-01  1.84381187e-01
  2.11754113e-01 -3.04303795e-01  2.25703329e-01  1.30651221e-02
  1.40138999e-01 -1.23398034e-02  3.33795130e-01 -2.60087512e-02
  3.95951986e-01 -2.89707392e-01  1.09796479e-01  7.00080469e-02
 -2.03790799e-01  5.22145294e-02  1.27005279e-02  1.98934041e-03
 -3.10380697e-01  1.90425534e-02  3.18952858e-01 -1.80388391e-01
 -4.77917306e-02 -9.80093479e-02  3.57788950e-02 -1.76834419e-01
  2.28341326e-01  3.59556042e-02  5.00891924e-01 -1.08382637e-02
  3.45026761e-01  2.37239189e-02  4.23069477e-01  5.42720616e-01
  9.76538360e-02  3.71916071e-02  3.05358887e-01  2.80161411e-01
  2.91385710e-01  2.78098062e-02  7.74234384e-02 -4.33270037e-02
 -1.48634046e-01 -2.17421189e-01 -1.50059491e-01  2.32629165e-01
 -1.62038848e-01 -2.24064395e-01  1.22198798e-01  3.69531326e-02
 -5.10159284e-02 -6.33706078e-02  3.38558793e-01  2.22439691e-02
  1.04394585e-01  2.48607025e-02 -9.07498896e-02  6.97710440e-02
 -4.78326716e-02  1.36402369e-01 -1.85430050e-04  4.78503436e-01
  1.85408786e-01 -2.20345169e-01 -1.79091886e-01  3.80524457e-01
  5.81018269e-01 -6.37931526e-02 -4.45433967e-02  1.61816448e-01
  6.04570396e-02 -2.32100844e-01  2.27343187e-01 -4.80538234e-03
 -6.32765964e-02 -4.43462431e-02  1.16589162e-02  1.70191020e-01
 -2.04287976e-01 -1.23507336e-01 -3.70959997e-01 -2.83270657e-01
 -3.05286255e-02  2.51721442e-01 -1.95429936e-01 -4.94111776e-01
  2.80824482e-01  2.55239218e-01 -1.55778676e-01  2.17867926e-01
 -6.21190667e-02  1.87398762e-01  1.71094894e-01  3.77647988e-02
 -1.95663363e-01  5.69452047e-01 -1.97114572e-01  4.48252670e-02
  3.23403180e-01 -4.91168201e-02 -3.30964699e-02 -7.26350307e-01
 -3.44631337e-02  2.39827558e-01  5.11549152e-02 -3.14175904e-01
  3.31041157e-01  1.20020807e-01 -3.48972887e-01 -8.78959298e-02
  9.24833715e-02  3.80112886e-01 -3.56882244e-01 -3.11724216e-01
 -1.12646908e-01  3.04341409e-02  2.60262311e-01  1.09813996e-01
  1.32777005e-01 -2.14772746e-01 -1.06566131e-01  1.86771870e-01
  2.42484421e-01  9.96709466e-02  7.56327808e-02  3.05062175e-01
 -2.60635074e-02 -1.14983171e-02  6.75052479e-02  8.25264752e-02
 -3.80700707e-01 -1.01393551e-01 -2.96557128e-01  8.35716873e-02
  2.86737502e-01  1.21929511e-01 -4.88998704e-02 -1.64657444e-01
  7.29149953e-02  5.87260649e-02 -2.97209844e-02  3.89186330e-02
 -1.30013511e-01  8.62259418e-02 -1.16984285e-02 -6.70933127e-02
  1.01821348e-01 -3.23987752e-01 -1.90394104e-01 -4.02151823e-01
 -2.70092010e-01  3.39425445e-01 -9.45483223e-02 -3.03619146e-01
 -1.28037944e-01  1.02807544e-02 -1.28695980e-01 -3.65685448e-02
 -5.90378605e-02  1.28415555e-01 -2.75748163e-01  1.23425201e-01
  2.01426983e-01 -3.18570465e-01  1.55927822e-01 -3.39206308e-01
 -2.66704649e-01 -4.69942428e-02 -5.25630295e-01  7.51274228e-02
 -3.90061848e-02  8.95798579e-02 -5.93857318e-02  9.80821550e-02
  2.52056956e-01  1.92289054e-01  2.93038815e-01 -4.11223248e-02
 -4.33543682e-01 -9.17425305e-02 -1.27339959e-01 -4.62994054e-02
 -4.28309530e-01 -5.34301512e-02 -2.93051004e-02 -2.93407381e-01
  3.13419282e-01  4.10789698e-01  4.22621630e-02 -1.03175230e-02
 -6.97811171e-02  8.28836113e-02 -2.69335508e-01  1.87754370e-02
  2.53298819e-01  9.87738371e-02  2.91533947e-01  1.65188581e-01
  2.79926836e-01  1.04957953e-01  2.35935032e-01 -1.54050574e-01
  4.37528551e-01  7.00204074e-02  1.18102193e-01  4.43108976e-01
  1.86018378e-01  2.70059645e-01 -2.65227109e-01  3.05753767e-01
 -5.20832166e-02 -1.56995043e-01  1.26568943e-01  1.01066813e-01
  7.88780689e-01 -2.39200234e-01  2.96881258e-01 -3.92996877e-01
  2.43278205e-01  6.52019866e-03 -2.26670027e-01  8.42342526e-02
  1.87413439e-01  2.34270811e-01 -2.53714770e-01  2.34865993e-02
  5.58474846e-02 -2.60671437e-01 -1.98201597e-01 -4.30751920e-01
 -1.04353808e-01 -9.77862030e-02 -1.94361120e-01 -1.29113346e-01
  5.90163283e-02 -4.86357510e-03 -2.38056332e-01 -2.63162199e-02
  5.72350696e-02 -1.24070525e-01 -3.75091434e-02  1.19452253e-01
 -1.59928441e-01 -4.44071069e-02  2.95978427e-01 -2.17079297e-01
 -5.36500327e-02 -8.80833864e-02  4.18031812e-01  3.43417525e-01
  4.34837520e-01 -2.32030839e-01  2.30756655e-01  1.03851438e-01
 -2.19845086e-01  3.94896090e-01 -1.21544771e-01  4.78819460e-02
 -2.93263227e-01  3.22909236e-01 -1.29219562e-01 -2.17030942e-03
  1.55232370e-01 -3.04950058e-01 -2.25919217e-01  3.54462564e-01
  4.74156737e-02 -1.22254893e-01 -7.30787963e-03 -2.92007923e-01
  4.96569555e-03  8.14309344e-02 -4.00162190e-02 -1.95162058e-01
 -1.60031207e-02 -5.87408617e-02 -1.06873475e-02 -2.27183014e-01
 -2.44226813e-01  4.40908149e-02 -7.39409253e-02 -3.24571431e-01
 -1.95427492e-01 -2.04697192e-01 -1.25798672e-01 -3.68159473e-01
 -2.72549293e-03 -3.75284374e-01  4.10658300e-01  5.07566869e-01
  1.20391071e-01  2.98474580e-01  1.09701538e-02 -2.89188549e-02
  1.58640929e-02  1.59542248e-01 -1.62234485e-01  2.67346472e-01
 -8.59039649e-02  3.23770754e-02  2.86803305e-01  1.22361556e-01
 -4.17590111e-01  3.54939818e-01 -4.57748413e-01 -6.93402290e-02
  1.18709937e-01 -2.37957299e-01 -1.69503525e-01 -1.46176636e-01
  2.55985528e-01  1.28948271e-01 -1.44843504e-01  2.43352234e-01
 -2.73505270e-01  3.32970142e-01  5.00910282e-01 -5.22755504e-01
 -1.17296375e-01  1.27086282e-01  3.49898666e-01 -8.36797506e-02
  1.12947430e-02 -7.91863725e-02 -1.00140676e-01 -3.60372216e-02]"
Fails to build on AARCH64 type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

/tensorflow/lite/kernels/rng_util.h:26:12: error: use of undeclared identifier 'uint32_t'

### Standalone code to reproduce the issue

```shell
bazel test --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_lang_filters=py --flaky_test_attempts=3 --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --action_env=PYTHON_BIN_PATH=/usr/local/bin/python3 --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py39,-no_oss_py310 --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py39,-no_oss_py310 --local_test_jobs=64 --build_tests_only -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/compiler/xrt/... -//tensorflow/core/tpu/... -//tensorflow/go/... -//tensorflow/java/... -//tensorflow/python/integration_testing/... -//tensorflow/tools/toolchains/... -//tensorflow/lite/... -//tensorflow/core/kernels/image:resize_bicubic_op_test -//tensorflow/core/grappler/optimizers:auto_mixed_precision_test_cpu -//tensorflow/core/grappler/optimizers:remapper_test_cpu
```


### Relevant log output

```shell
ERROR: /workspace/tensorflow/lite/kernels/BUILD:497:11: Compiling tensorflow/lite/kernels/rng_util.cc failed: (Exit 1): clang failed: error executing command (from target //tensorflow/lite/kernels:rng_util) 
  (cd /tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    CACHEBUSTER=20220325 \
    CLANG_COMPILER_PATH=/usr/lib/llvm-16/bin/clang \
    LD_LIBRARY_PATH='' \
    PATH=/home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-arm64/bin:/home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    *** \
    PYTHON_BIN_PATH=/usr/local/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/_objs/rng_util/rng_util.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/_objs/rng_util/rng_util.pic.o' '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/aarch64-opt/bin -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions -Wno-gnu-offsetof-extensions '-mtune=generic' '-march=armv8-a' -O3 -flax-vector-conversions '-std=c++17' -DFARMHASH_NO_CXX_STRING -DEIGEN_ALLOW_UNALIGNED_SCALARS -Wno-sign-compare -O3 -fno-exceptions '--sysroot=/dt10' -c tensorflow/lite/kernels/rng_util.cc -o bazel-out/aarch64-opt/bin/tensorflow/lite/kernels/_objs/rng_util/rng_util.pic.o)
# Configuration: 91cacbf6409fd17883ece1a0e16168e33815822fe5d36a44e64642fa9b0e32ee
# Execution platform: @local_execution_config_platform//:platform
In file included from tensorflow/lite/kernels/rng_util.cc:15:
./tensorflow/lite/kernels/rng_util.h:26:12: error: use of undeclared identifier 'uint32_t'
std::array<uint32_t, 2> Threefry2x32(uint32_t key_0, uint32_t key_1,
           ^
./tensorflow/lite/kernels/rng_util.h:26:38: error: unknown type name 'uint32_t'
std::array<uint32_t, 2> Threefry2x32(uint32_t key_0, uint32_t key_1,
                                     ^
./tensorflow/lite/kernels/rng_util.h:26:54: error: unknown type name 'uint32_t'
std::array<uint32_t, 2> Threefry2x32(uint32_t key_0, uint32_t key_1,
                                                     ^
./tensorflow/lite/kernels/rng_util.h:27:49: error: use of undeclared identifier 'uint32_t'
                                     std::array<uint32_t, 2> ctr);
                                                ^
./tensorflow/lite/kernels/rng_util.h:32:12: error: use of undeclared identifier 'uint32_t'
std::array<uint32_t, 4> Philox4x32(uint32_t key_0, uint32_t key_1,
           ^
./tensorflow/lite/kernels/rng_util.h:32:36: error: unknown type name 'uint32_t'
std::array<uint32_t, 4> Philox4x32(uint32_t key_0, uint32_t key_1,
                                   ^
./tensorflow/lite/kernels/rng_util.h:32:52: error: unknown type name 'uint32_t'
std::array<uint32_t, 4> Philox4x32(uint32_t key_0, uint32_t key_1,
                                                   ^
./tensorflow/lite/kernels/rng_util.h:33:47: error: use of undeclared identifier 'uint32_t'
                                   std::array<uint32_t, 4> ctr);
                                              ^
8 errors generated.
```
",True,"[-4.61093426e-01 -3.57758999e-01 -1.99878529e-01  6.82685375e-02
  2.10412502e-01 -3.80070746e-01 -1.74390242e-01 -3.73366848e-02
 -2.76846886e-01 -2.18151554e-01  1.50185879e-02  1.54054731e-01
 -2.13678420e-01  1.48125857e-01 -3.81087475e-02  5.03715932e-01
 -1.41004384e-01 -9.61708277e-02  1.09142356e-01  2.54002780e-01
 -1.38369218e-01 -1.79829702e-01 -2.19099224e-01  5.10458276e-02
  1.40977025e-01  1.81287795e-01 -2.32450336e-01 -8.84417295e-02
  5.31819724e-02  9.26132053e-02  4.68184710e-01  1.55204469e-02
  1.14227951e-01  1.23987339e-01  5.84330410e-04  3.64434481e-01
 -1.06418096e-01 -3.49068940e-01 -2.77933359e-01  1.18529752e-01
 -1.80483997e-01  1.23060711e-01  2.36178368e-01 -2.69784361e-01
  6.39695004e-02 -2.81558573e-01  6.34481460e-02 -2.27504492e-01
  3.41015011e-02 -2.69874096e-01  4.44781668e-02 -1.15757763e-01
 -3.66556585e-01 -3.60898137e-01 -2.07465887e-01  1.35475725e-01
  3.49727422e-02 -1.91776693e-01 -9.04096365e-02  1.43583953e-01
  1.96224719e-01 -8.48823637e-02 -2.73896493e-02 -1.28633544e-01
  1.66170597e-01  1.76313892e-01  3.53800684e-01 -6.17142320e-02
  5.60542762e-01 -1.62537321e-01  1.51552223e-02 -2.17894301e-01
 -2.74839401e-01  1.68779239e-01  1.51552811e-01 -3.44053954e-02
  2.27234200e-01  1.47445515e-01  2.56390661e-01 -1.02013648e-02
  3.13648880e-02 -2.07190275e-01 -3.60700861e-03 -3.40264201e-01
  1.37338668e-01 -9.17202681e-02  4.41323459e-01  1.58423245e-01
  4.17762339e-01 -9.99098048e-02  3.89980435e-01  3.86492193e-01
 -3.54391858e-02  5.03892004e-02  4.77935165e-01  1.14691161e-01
  1.77137867e-01  1.45274669e-01  3.08875181e-02 -2.22657636e-01
 -4.94495258e-02 -2.01202780e-01  1.63794905e-01  1.82052702e-02
 -2.11834013e-01 -1.86592355e-01  1.94218010e-01 -1.79541260e-02
 -9.76513699e-02 -2.41428822e-01  1.86205357e-01  7.98896253e-02
  1.15429088e-01 -3.08720052e-01  5.01387427e-03 -2.23729789e-01
 -2.77198076e-01 -1.62057832e-01  4.35059816e-02  7.40732908e-01
  1.07973479e-01 -1.38153076e-01  1.44151211e-01  3.45748276e-01
  4.54808295e-01  2.56435543e-01 -3.25629674e-02  8.17614421e-03
  9.65023693e-03 -2.34436631e-01  3.53664346e-02  5.34700863e-02
  2.68229008e-01  3.24280620e-01  7.74551034e-02  1.53360635e-01
 -1.85237184e-01 -1.75436810e-01 -6.02546446e-02 -2.91314572e-01
 -3.44715536e-01  2.64476806e-01 -1.56550616e-01 -7.11816370e-01
  2.43670084e-02  7.35167712e-02 -3.77484202e-01  2.55459756e-01
 -1.88486621e-01  2.15905234e-02 -4.09158915e-02  3.22975777e-02
  2.49422155e-02  3.96430016e-01  1.77986503e-01  3.01477373e-01
  3.31836045e-01 -1.85673326e-01  2.13980228e-01 -5.15052319e-01
 -1.25287145e-01  3.21612656e-01 -1.27299547e-01 -1.53411373e-01
  8.90156180e-02  1.08469680e-01 -3.38562369e-01 -2.50299990e-01
  5.37510738e-02  5.27460396e-01 -1.83233500e-01 -1.28223836e-01
  2.27990553e-01  1.01674207e-01  9.85121876e-02  6.80064335e-02
  1.27430290e-01 -6.80263877e-01 -2.58261375e-02  5.26942611e-01
  4.32894379e-02  2.39764780e-01  1.48495436e-01  2.23198026e-01
  3.63442153e-02  1.09868072e-01  1.92531273e-01  1.55279115e-01
 -2.11679399e-01 -2.26708278e-02 -3.55822593e-01 -1.17150076e-01
  3.33137304e-01 -3.19060504e-01 -1.26781892e-02  1.27062380e-01
  1.59768656e-01  1.57386631e-01 -1.33102864e-01 -1.07089370e-01
 -1.76322058e-01 -5.90783656e-02 -1.77239001e-01  7.68362135e-02
  6.97709620e-02 -1.81858078e-01 -2.37111449e-01 -2.94216394e-01
 -4.11919862e-01  2.32776597e-01  9.63984877e-02 -3.99166107e-01
  1.88987613e-01  6.05086908e-02 -2.66631961e-01 -2.34636609e-02
  2.25368962e-01  1.70249045e-01 -3.19129705e-01  2.67490327e-01
  1.46572948e-01 -3.65552187e-01  3.06929648e-02 -3.87203664e-01
  6.74997568e-02  1.23615749e-03 -4.17336404e-01 -5.75084910e-02
 -7.54875913e-02  1.61294073e-01 -2.05355510e-02  1.15872458e-01
  2.12780595e-01  2.50058234e-01  5.45344412e-01 -7.67360032e-02
 -1.48451954e-01 -1.00847997e-01 -1.24019101e-01  9.20956954e-02
 -2.77452111e-01 -2.32233331e-01  1.52389705e-01 -3.45799923e-02
  2.75898099e-01  4.82363641e-01 -9.07091647e-02 -4.31217328e-02
 -3.45621377e-01  9.77497846e-02 -2.69779831e-01  2.51147419e-01
  3.96238267e-01  1.21351611e-03  4.07152176e-01  2.35905141e-01
  2.32512742e-01  1.63121551e-01  1.89235806e-01 -1.27189547e-01
  2.77691603e-01  6.81160390e-02  1.23299751e-02  4.45847988e-01
  3.92915070e-01  3.16290498e-01 -3.87040675e-01  4.45290327e-01
  1.37095869e-01 -1.35203689e-01  2.72932380e-01 -1.43248439e-01
  6.15247905e-01 -3.75041395e-01  2.90764272e-01 -1.45374686e-01
  3.49434137e-01 -1.92068785e-01 -6.80322424e-02 -1.14521734e-01
  1.92733169e-01  1.39273673e-01 -2.96644926e-01 -5.42534888e-02
 -8.97674263e-02 -2.70077467e-01 -7.45638907e-02 -7.99882770e-01
 -3.34982872e-01 -1.51543260e-01 -3.56815726e-01  1.13285616e-01
 -2.87365243e-02 -6.11284859e-02 -3.07959557e-01 -2.52131838e-03
  1.70947164e-01  4.31439728e-02  4.50608954e-02  6.95634708e-02
 -1.37891665e-01 -1.10603020e-01  3.59527290e-01 -6.16379619e-01
 -1.95853978e-01 -1.61058828e-01  6.24208927e-01  1.41162977e-01
  4.59276199e-01 -5.01835167e-01  1.16132990e-01 -1.00660890e-01
 -2.65775055e-01  4.03144807e-01 -2.32072026e-02  9.53179821e-02
 -3.79183233e-01  6.82337523e-01  1.41253650e-01 -1.49911180e-01
  1.17726643e-02 -1.28693193e-01 -3.61734182e-01  6.66249171e-02
  8.85590538e-02 -1.46598935e-01 -5.86000159e-02 -3.20796818e-01
  1.88162476e-02  1.52279347e-01 -1.54445633e-01  7.12317377e-02
 -1.59499586e-01  2.32219443e-01 -2.90893763e-01  7.83378780e-02
 -4.40702260e-01  2.60571778e-01 -8.68197978e-02 -2.60841280e-01
 -1.89868078e-01  1.68930829e-01 -1.91068739e-01 -1.48203939e-01
  5.35092279e-02 -2.48171329e-01  3.62164885e-01  4.26320851e-01
  4.37096786e-03  9.31319147e-02 -5.72132617e-02  2.61914670e-01
 -3.27430040e-01 -6.20270446e-02 -5.29671274e-02  5.22926569e-01
  6.40064105e-02 -2.80574799e-01  4.84044641e-01  7.08344728e-02
 -2.89559755e-02  1.70245856e-01 -3.54902267e-01  5.75950816e-02
 -2.96169706e-03 -2.63554394e-01 -3.02801400e-01 -2.63896644e-01
  1.21857166e-01  3.98544610e-01 -1.41798466e-01  8.27828422e-02
 -2.94546127e-01  3.55495542e-01  5.23613930e-01 -4.27540749e-01
 -3.80218089e-01 -1.02654546e-02  2.06224144e-01 -8.25837404e-02
  1.49753131e-02 -7.28134960e-02  3.09572071e-01  1.27763301e-03]"
Gather out-of-range read will return different value with `jit_compile` stat:awaiting tensorflower type:bug comp:ops comp:xla TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the index is out of range, `gather` will return different values with and without `jit_compile=True`.

The return values are expected to be consistent. Another solution is to throw errors in both cases.

### Standalone code to reproduce the issue

```shell
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True)
  def call(self, input):
    x = tf.gather(input, indices=[5, 8, 7, 16, 256, 123], axis=0) # 256 is out of range
    return x

m = Model()

input_shape = [256]
x1 = tf.ones(input_shape)

# Call model
y = m(x1)
print(y)
# tf.Tensor([1. 1. 1. 1. 1. 1.], shape=(6,), dtype=float32), with jit_compile=True
# tf.Tensor([1. 1. 1. 1. 0. 1.], shape=(6,), dtype=float32), without jit_compile=True
```


### Relevant log output

_No response_",True,"[-5.81206083e-01 -4.08232987e-01  3.42926569e-02  1.08206332e-01
  2.06531614e-01 -4.57308739e-01  3.17817740e-03 -7.09774159e-03
 -3.03725898e-01 -3.71118724e-01  2.04780951e-01 -2.47773975e-02
 -1.38750702e-01 -1.31085500e-01 -1.20724514e-01  2.55239427e-01
 -1.10400572e-01 -3.17294076e-02  1.27053708e-01  2.36178622e-01
 -2.37809718e-01 -2.74034262e-01 -3.59229207e-01  1.42005056e-01
  1.88540667e-01  2.11879849e-01 -3.53396833e-01  2.59216338e-01
 -3.25242579e-02  2.47192681e-01  5.83240509e-01  2.03970313e-01
 -1.51817918e-01  2.01883942e-01 -1.28919482e-01  2.27872729e-01
 -2.49552131e-01 -2.57366061e-01 -2.00833380e-01  1.10615864e-01
  1.55221354e-02  2.11688399e-01  3.09433520e-01 -1.39695942e-01
  2.07266256e-01 -1.06174406e-02 -1.77006245e-01 -1.25162750e-01
 -4.54334840e-02 -1.91351950e-01 -8.91385972e-02  4.16104533e-02
 -5.37877798e-01 -3.46300900e-01 -1.57252297e-01  4.53204811e-02
  8.35567117e-02 -5.13230339e-02 -1.21408649e-01  2.44285762e-01
  1.00081936e-01 -1.04693860e-01 -8.45330581e-03 -6.69326782e-02
  4.38993648e-02  1.32587954e-01  3.54354560e-01 -1.11583062e-02
  6.09377265e-01 -1.29279807e-01  2.41470397e-01  3.56225073e-02
 -3.91055614e-01  4.30455059e-02  5.10872081e-02  2.18658209e-01
 -1.14441058e-02  5.39851449e-02  3.21832001e-01 -3.86141926e-01
  3.15193199e-02 -1.77718416e-01  1.32800564e-02 -1.80090442e-01
  4.18085456e-01 -1.91499680e-01  4.13130671e-01  1.07815713e-01
  3.61680388e-01 -2.55763769e-01  4.38959599e-01  4.53141540e-01
 -1.51401043e-01  1.11378450e-02  4.59737659e-01  3.23407441e-01
  3.40341300e-01  6.19524345e-02  3.70886922e-03 -5.15748002e-02
 -6.58742040e-02 -6.11822456e-02 -5.69179691e-02 -9.87652689e-03
 -1.32865921e-01 -1.68681473e-01  2.59212434e-01 -1.24890804e-01
  1.03207588e-01 -8.79299864e-02  2.17683747e-01  5.58003560e-02
  1.18133888e-01 -8.76646042e-02 -1.07956842e-01 -2.49270648e-01
 -1.16067901e-01 -4.13238443e-02  1.57941431e-01  7.55054116e-01
 -2.26479732e-02 -1.03923805e-01 -1.66212708e-01  1.42006487e-01
  4.86103833e-01  2.58734912e-01 -1.90418959e-02 -6.39964342e-02
  2.69265901e-02 -8.52277651e-02  6.48109168e-02 -5.36104478e-02
 -9.79069993e-02  1.77046433e-01 -1.45740137e-01  2.86511481e-02
 -2.26671427e-01 -3.67146768e-02 -3.65923762e-01 -1.86101571e-01
 -3.37388456e-01  1.53073654e-01  8.21696594e-05 -5.54751396e-01
  2.44051069e-01  5.99464811e-02 -2.19785020e-01  1.91236153e-01
 -1.39804453e-01 -2.05666453e-01 -1.44827142e-01 -1.55064575e-02
 -2.06323832e-01  5.43598652e-01  1.21994294e-01  1.70849562e-01
  2.73729146e-01  5.87087721e-02  6.09521680e-02 -5.53538740e-01
  1.82699617e-02  3.59729648e-01 -3.07211220e-01 -1.94658309e-01
  4.86874022e-02  1.00129925e-01 -3.47474754e-01 -1.15429178e-01
 -4.23182510e-02  4.42611367e-01 -1.55000344e-01 -8.63198191e-02
 -1.43858939e-01  1.16801076e-01  7.71673620e-02  1.58595238e-02
  2.44971931e-01 -4.53850389e-01 -2.35340461e-01  1.12214342e-01
  1.35639459e-01  9.63999107e-02  1.09598115e-01  2.85316706e-01
  9.74342972e-02  5.21667860e-02  1.27068579e-01  3.55693847e-01
 -2.44822040e-01 -8.94486606e-02 -5.47804892e-01 -6.21610060e-02
  5.31784415e-01  2.08238158e-02 -5.88628352e-02 -2.17180122e-02
  8.38198885e-02 -5.68007268e-02 -1.80633850e-02  2.30014715e-02
 -2.33886048e-01 -3.04690171e-02 -3.52301188e-02 -1.41344249e-01
  1.92325339e-01 -2.92385221e-01 -6.81564063e-02 -5.22347808e-01
 -3.92576039e-01  6.76794425e-02 -4.13860902e-02 -3.16313475e-01
  3.50795574e-02 -2.09308881e-03 -2.12157041e-01  3.14264059e-01
  1.97939426e-01  4.03031260e-02 -1.32808879e-01  1.83741108e-01
  1.79748863e-01 -2.58614004e-01  5.53664193e-03 -4.09470975e-01
 -3.71982515e-01  3.01911771e-01 -3.40343624e-01  2.47445256e-02
  3.81595902e-02  1.62228286e-01  4.97099496e-02  1.71389803e-02
  5.84292710e-01  2.80924141e-01  3.83538604e-01 -1.56706691e-01
 -1.64135367e-01 -1.30712450e-01 -1.01509541e-01  1.29035950e-01
 -5.09477139e-01 -2.76753485e-01  1.66758940e-01  1.42803947e-02
  2.27707565e-01  3.33569705e-01 -5.38876764e-02 -2.27231428e-01
 -3.10858190e-01  2.76933193e-01 -2.98654288e-01  1.07818097e-02
  2.14196205e-01  1.55033618e-01  5.36776781e-01  8.69082212e-02
  2.31845364e-01  2.06826195e-01  2.16054633e-01 -3.60027671e-01
  4.69327450e-01 -2.32997909e-02  1.15601439e-03  4.29607570e-01
  2.19741240e-01  2.57175267e-01 -2.89579272e-01  4.86761451e-01
  1.20956764e-01 -1.54121006e-02  1.55590042e-01 -9.73804444e-02
  6.46491766e-01 -1.53319612e-01  5.19015193e-02 -2.84192748e-02
  1.79024011e-01  6.50807023e-02 -1.79751873e-01 -3.65767144e-02
  9.36658606e-02  1.24602787e-01 -4.90943074e-01 -1.50673598e-01
  7.70192686e-03 -3.50941837e-01 -1.33756369e-01 -3.94700706e-01
 -1.97835803e-01  1.30981863e-01 -3.37865114e-01  1.37616098e-01
  4.47438024e-02  7.87988156e-02 -2.16277927e-01  6.85950071e-02
  1.57080159e-01 -3.08779418e-01  7.21662045e-02  2.72478968e-01
 -1.26640528e-01  7.61633813e-02  4.59733605e-01 -3.59810978e-01
 -1.24540269e-01  9.82907861e-02  4.20318782e-01  8.09121430e-02
  4.25734758e-01 -4.17786300e-01  4.11640108e-03 -3.58675122e-02
 -5.51327504e-02  3.75892282e-01 -5.38501628e-02 -2.09143478e-03
 -2.71452278e-01  8.50172758e-01  2.58425266e-01 -1.10963807e-01
  2.13483065e-01 -2.08374545e-01 -3.72860551e-01  1.23824045e-01
  2.35633224e-01 -1.88925028e-01 -2.32855365e-01 -3.68416965e-01
  6.65297061e-02  2.50334978e-01 -7.26227313e-02 -1.20165385e-02
 -2.16328621e-01 -1.93311363e-01 -1.32501453e-01 -2.01263458e-01
 -2.74042845e-01  1.83040977e-01 -1.83316946e-01 -2.68719941e-01
 -1.65059507e-01 -4.11443785e-03  2.49351077e-02 -3.59192431e-01
  6.54266924e-02 -2.39603296e-01  2.67220616e-01  4.70332325e-01
 -2.31297642e-01  2.20154643e-01  7.22703636e-02  7.45949000e-02
 -3.49408627e-01  3.28818075e-02 -2.98586003e-02  3.58771235e-01
 -4.53513041e-02  1.18140830e-02  3.87166083e-01  3.19348246e-01
 -2.58903503e-01 -2.72095539e-02 -3.20683300e-01  1.50583774e-01
  2.84699857e-01 -2.14112863e-01 -1.04290262e-01 -4.30568248e-01
  1.81775331e-01  1.88865393e-01 -1.66595280e-01  1.30948156e-01
 -2.48124689e-01  3.04713011e-01  4.30827320e-01 -3.64934951e-01
 -2.63876617e-01  1.90629393e-01  1.90471172e-01 -2.05055952e-01
  6.57793730e-02 -1.31333321e-01  7.81162828e-02  9.24929082e-02]"
Body function of `while_loop` cannot access the external variable after compilation  type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The body function of `while_loop` cannot access the external variable after compilation. It will raise the error `UnboundLocalError: local variable 'x' referenced before assignment`

However, if I run the model without the `@tf.function(jit_compile=True)`, the model can be executed without any error.

### Standalone code to reproduce the issue

```shell
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True) # Comment this line, it will succeed
  def call(self, x):

    def cond(i, _):
      return i < 10

    def body(i, y):
      y = tf.math.add(y, 2.0)
      x = tf.math.multiply(x, 2.0)
      return [tf.math.subtract(i, 1), y + x]

    i = tf.constant(10)
    y = tf.constant(1.0)
    _, final_y = tf.while_loop(cond, body, [i, y], shape_invariants=[i.shape, y.shape])
    return final_y

m = Model()
input_shape = [1,2]
x = tf.constant([4.,5.], shape=input_shape)

y = m(x)
```


### Relevant log output

```shell
UnboundLocalError: Exception encountered when calling layer 'model_28' (type Model).

in user code:

    File ""<ipython-input-31-1eb50a9c2c75>"", line 16, in body  *
        x = tf.math.multiply(x, 2.0)

    UnboundLocalError: local variable 'x' referenced before assignment


Call arguments received by layer 'model_28' (type Model):
   x=tf.Tensor(shape=(1, 2), dtype=float32)
```
",True,"[-0.4586361  -0.28468513 -0.2670311   0.18861128  0.2074085  -0.66714036
 -0.05714292  0.04509311 -0.48972166 -0.2988396   0.2433506  -0.4796787
 -0.31060106  0.08138307 -0.02947831  0.20753917 -0.11650302  0.0952578
  0.10264447  0.11886428 -0.15690592 -0.0289987  -0.22190401  0.31952596
 -0.04607283  0.23703939 -0.31435382 -0.15449739 -0.01868359  0.0196957
  0.4529575  -0.05719999 -0.21739882 -0.01726444  0.08247305  0.40158045
 -0.17590603 -0.45880473 -0.36471564 -0.09132551 -0.21410275  0.02100629
  0.10603245 -0.29652384  0.23281159 -0.14669839 -0.14585775 -0.08265577
  0.0831238  -0.3271572  -0.00807413 -0.06975777 -0.41901118 -0.3476569
 -0.25117126 -0.01067168  0.14175667 -0.30178252  0.04377946  0.11473235
  0.06104491 -0.09407587 -0.1318205  -0.08174072  0.25441134  0.0296488
  0.39420965  0.04910033  0.39859217 -0.07421756  0.16455606  0.03732961
 -0.5058379   0.05795762  0.32486996  0.07803781 -0.05719791  0.2187489
  0.16950637 -0.17407061 -0.0209283  -0.18388224  0.12371068 -0.28267992
  0.01123844  0.05131194  0.39554423  0.16404584  0.29200447 -0.14777735
  0.19749656  0.17023566 -0.15463594  0.086436    0.34056312  0.11091044
  0.14581937  0.26626194  0.02297173 -0.28033644 -0.05108432  0.06046223
  0.05163096 -0.01796314 -0.19763106 -0.03075244  0.18541175  0.0182475
  0.2733159  -0.29569444 -0.01451107  0.03971395  0.18330866 -0.21191984
  0.0834399   0.0950062  -0.07723077 -0.2506054  -0.06249829  0.56121504
 -0.06677096  0.03512286  0.01028816  0.21570183  0.5683289   0.07749604
 -0.14032242  0.10048029  0.14056833 -0.40451556  0.24496013  0.06110982
  0.17274833  0.16630885  0.04214136  0.12226865 -0.1589299  -0.16676357
 -0.00167679 -0.52661383 -0.21138963  0.04438899 -0.17159218 -0.80485153
  0.22188528  0.05610652 -0.29840368  0.23223546 -0.17627785  0.17908424
  0.12140256  0.14189525  0.04761886  0.4237991   0.08253843  0.33658123
  0.42801267 -0.13591802  0.07673372 -0.64854264  0.01191528  0.64023995
 -0.28340223 -0.10641875  0.21359259  0.08706729 -0.33929783 -0.24891424
 -0.04951917  0.31759655 -0.24974874 -0.0401752  -0.01167795 -0.02078764
  0.27806154 -0.03824885  0.18944837 -0.7403724  -0.09806027  0.6694101
  0.07106619  0.02954975  0.09686496  0.24814552  0.09840219  0.19100586
 -0.0896492   0.16095018  0.01641525  0.16843423 -0.31751883 -0.04755944
  0.3618501  -0.24087405 -0.00665257  0.05223002  0.36165085 -0.11912729
 -0.01415743 -0.13034362 -0.11828014 -0.18425213  0.04741003  0.09949414
  0.04136809 -0.54922986 -0.15641911 -0.35362074 -0.12304227  0.2668963
  0.02472352 -0.46830255  0.17173034 -0.07255819 -0.44076306  0.1035357
  0.14230299  0.18961312 -0.2535805   0.184302    0.1956712  -0.14137468
  0.19193149 -0.46484697  0.25049192  0.191688   -0.33501932 -0.01255198
  0.07202704  0.17871262  0.13811314 -0.19124144  0.28930283  0.44125265
  0.5071052   0.0321627  -0.18133992 -0.07407717  0.03456973  0.05736505
 -0.49305236 -0.43974835  0.06910628  0.04841766  0.22157484  0.5066948
  0.0488314  -0.17677632 -0.49872935  0.18027988 -0.2959045   0.2650541
  0.50630915  0.01426392  0.58557063  0.16243406  0.40231565  0.14649412
  0.34561938 -0.42843956  0.21144484  0.26677987  0.18184909  0.178171
  0.22386882  0.32282358 -0.23342001  0.3497406   0.22896707 -0.06665976
  0.14333127 -0.37542856  0.41421407 -0.3516248   0.23702028 -0.3910486
  0.41591406 -0.1160806   0.14683212  0.25305653  0.12940884  0.05538172
 -0.02884448  0.10333325  0.10610026 -0.14646652  0.13328704 -0.59844327
 -0.24973637 -0.01879241 -0.3799314   0.17670748  0.16880612 -0.05547954
 -0.15476982  0.2317339   0.06680387  0.04089229  0.11593943  0.5294663
 -0.10623952 -0.24488932  0.28343257 -0.5849259  -0.28297952 -0.0726793
  0.51441497  0.35246664  0.3746912  -0.6485627   0.08111421 -0.16193853
 -0.35419467  0.37006894  0.04355315  0.05000201 -0.17852734  0.7149663
  0.20408706 -0.11121446  0.01087925 -0.328768   -0.34454286  0.10239786
  0.14809793 -0.2312592  -0.00126857 -0.491984    0.03532816  0.33160603
 -0.21850318  0.01538859  0.14460485 -0.02310133 -0.32549858 -0.242959
 -0.41792414  0.29959658 -0.1309365  -0.38414204 -0.2645774  -0.13197875
 -0.07506178  0.03078151  0.05566643 -0.45356202  0.25288814  0.68616825
 -0.06724755  0.06461327  0.21427786  0.21669933 -0.5045935   0.11480933
  0.11381624  0.3038479  -0.03157064 -0.26314223  0.3558209   0.34371504
 -0.1487767   0.1683681  -0.29187644  0.21648231  0.07479718 -0.21559036
 -0.21660426 -0.111229    0.09180384  0.16625169 -0.20271926  0.07412225
 -0.33799756  0.29341862  0.7153075  -0.30620164 -0.27394664  0.13233882
  0.33484423 -0.04033371 -0.02240614 -0.00399381  0.38134003 -0.00595001]"
restoring checkpoint loads weights partially stat:awaiting response type:bug stale comp:keras TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am following this [tfa seq2seq tutorial](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt) for building a seq2seq network with LSTMs. I trained my model and got great accuracy. I saved my model with `tf.train.Checkpoint`. Then, I tried to reload my model with `checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))`. 

However, the model gets restored partially.

My encoder weights get restored, however the decoder does not.

How to resolve?

### Standalone code to reproduce the issue

```shell
from typing import Tuple

import tensorflow as tf
import tensorflow_addons as tfa

from tensorflow.keras.preprocessing.text import tokenizer_from_json

import json

import numpy as np

import unicodedata
import re
import os
import io
import time
from collections import Counter

MAX_SEQUENCE_LENGTH = 30

def math_tokenizer(expression):
    regex = r'(\d+|[\+\-\*\/\(\)\^]|[a-zA-Z]+|.)'

    # Use the regex to split the expression into tokens
    _tok = re.findall(regex, expression)

    # Split numbers into individual digits
    ret_tokens = []
    for token in _tok:
        if token.isdigit():
            ret_tokens.extend(list(token))
        else:
            ret_tokens.append(token)

    # Filter out empty strings
    ret_tokens = [token for token in ret_tokens if token.strip()]

    return ret_tokens

def change_variable(expression):
    # Define regular expressions to match different tokens
    tokenstream = math_tokenizer(expression)
    variable = None
    for idx, tok in enumerate(tokenstream):
        if len(tok) == 1 and tok.isalpha():
          variable = tok
          tokenstream[idx] = ""var""
    return ' '.join(tokenstream), variable

def text_cleaning(x):
  modified_text = [None] * len(x)
  tokens = set()
  tok_list = []

  variables = []

  for idx, dx in enumerate(x):
      lhs, rhs = dx.split(""="")
      tokenstream, v = change_variable('='.join([lhs[2:-4], rhs[:-1]]))
      tokens.update(tokenstream)
      tok_list.extend(tokenstream)
      variables.append(v)
      modified_text[idx] = ''.join(tokenstream)

  return modified_text, variables

def generate_train_test_dataset(data, TRAIN_SIZE):
  modified_text, variables = text_cleaning(data)
  inputs = []
  targets = []

  for idx, dx in enumerate(modified_text):
      inp, tgt = dx.split(""="")
      inputs.append(inp)
      targets.append(tgt)

  train_inputs = inputs[:TRAIN_SIZE]
  train_targets = targets[:TRAIN_SIZE]
  train_variables = variables[:TRAIN_SIZE]

  test_inputs = inputs[TRAIN_SIZE:]
  test_targets = targets[TRAIN_SIZE:]
  test_variables = variables[TRAIN_SIZE:]

  return train_inputs, train_targets, train_variables, test_inputs, test_targets, test_variables

  class MyDataset:
    def __init__(self, problem_type='calculus'):
        self.problem_type = 'calculus'
        self.inp_lang_tokenizer = None
        self.targ_lang_tokenizer = None


    def unicode_to_ascii(self, s):
        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')

    def preprocess_sentence(self, w):
        w = w.strip()

        w = 'start ' + w + ' end'
        return w

    def tokenize(self, lang, func):

        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=None, oov_token='<OOV>', analyzer=func)
        lang_tokenizer.fit_on_texts(lang)

        tensor = lang_tokenizer.texts_to_sequences(lang)

        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')

        return tensor, lang_tokenizer

    def load_dataset(self, dataset, func):
        # creating cleaned input, output pairs
        targ_lang, inp_lang = dataset

        targ_lang = [self.preprocess_sentence(w) for w in targ_lang]
        inp_lang = [self.preprocess_sentence(w) for w in inp_lang]

        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang, func)
        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang, func)

        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer


    def call(self, dataset, BUFFER_SIZE, BATCH_SIZE, func):
        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(dataset, func)

        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor))
        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

        return train_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer

class Encoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
    super(Encoder, self).__init__()
    self.batch_sz = batch_sz
    self.enc_units = enc_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)

    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')


  def call(self, x, hidden):
    x = self.embedding(x)
    output, h, c = self.lstm_layer(x, initial_state = hidden)
    return output, h, c

  def initialize_hidden_state(self):
    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]

class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, max_length_input, max_length_output, attention_type='luong'):
    super(Decoder, self).__init__()
    self.batch_sz = batch_sz
    self.dec_units = dec_units
    self.attention_type = attention_type
    self.max_length_output = max_length_output

    # Embedding Layer
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)

    #Final Dense layer on which softmax will be applied
    self.fc = tf.keras.layers.Dense(vocab_size)

    # Define the fundamental cell for decoder recurrent structure
    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)



    # Sampler
    self.sampler = tfa.seq2seq.sampler.TrainingSampler()

    # Create attention mechanism with memory = None
    self.attention_mechanism = self.build_attention_mechanism(self.dec_units,
                                                              None, self.batch_sz*[max_length_input], self.attention_type)

    # Wrap attention mechanism with the fundamental rnn cell of decoder
    self.rnn_cell = self.build_rnn_cell(batch_sz)

    # Define the decoder with respect to fundamental rnn cell
    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)


  def build_rnn_cell(self, batch_sz):
    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell,
                                  self.attention_mechanism, attention_layer_size=self.dec_units)
    return rnn_cell

  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):
    # ------------- #
    # typ: Which sort of attention (Bahdanau, Luong)
    # dec_units: final dimension of attention outputs
    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)
    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)

    if(attention_type=='bahdanau'):
      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)
    else:
      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)

  def build_initial_state(self, batch_sz, encoder_state, Dtype):
    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)
    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)
    return decoder_initial_state


  def call(self, inputs, initial_state):
    x = self.embedding(inputs)
    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[self.max_length_output-1])
    return outputs

def loss_function(real, pred):
  # real shape = (BATCH_SIZE, max_length_output)
  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )
  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')
  loss = cross_entropy(y_true=real, y_pred=pred)
  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1
  mask = tf.cast(mask, dtype=loss.dtype)
  loss = mask* loss
  loss = tf.reduce_mean(loss)
  return loss

import json
functions = '8exp^(9e)'

f = open('modified_train.txt', 'r')
modified_text = f.readlines()
f.close()

modified_text = [m[:-1] for m in modified_text]

BUFFER_SIZE = 32000
BATCH_SIZE = 64
# Let's limit the #training examples for faster training
num_examples = 30000

inputs = []
targets = []

N_TRAIN = 800000

for dx in (modified_text):
    try:
      inp, tgt = dx.split(""="")
      inputs.append(inp)
      targets.append(tgt)
    except:
      print(f""Error at: {dx}"")

train_inputs = inputs[:N_TRAIN]
train_targets = targets[:N_TRAIN]

test_inputs = inputs[N_TRAIN:]
test_targets = targets[N_TRAIN:]

data = (train_targets, train_inputs)

print(""Creating the dataset"")
dataset_creator = MyDataset('calculus')

# print(""Training the tokenizer"")
# train_dataset, inp_lang, targ_lang = dataset_creator.call(data, BUFFER_SIZE, BATCH_SIZE, math_tokenizer)

f = open('./inp_lang_tokenizer.json')
inp_json = f.read()
inp_json = json.loads(inp_json)
f.close()

f = open('./targ_lang_tokenizer.json')
targ_json = f.read()
targ_json = json.loads(targ_json)

inp_lang = tokenizer_from_json(inp_json)
targ_lang = tokenizer_from_json(targ_json)


# example_input_batch, example_target_batch = next(iter(train_dataset))
# example_input_batch.shape, example_target_batch.shape
vocab_inp_size = len(inp_lang.word_index)+1
vocab_tar_size = len(targ_lang.word_index)+1
max_length_input = 31
max_length_output = 31

embedding_dim = 128
units = 256
steps_per_epoch = num_examples//BATCH_SIZE

train_dataset, inp_lang, targ_lang = dataset_creator.call(data, BUFFER_SIZE, BATCH_SIZE, math_tokenizer)

## Test Encoder Stack
example_input_batch, example_target_batch = next(iter(train_dataset))
print(""Creating encoder"")
encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)
sample_hidden = encoder.initialize_hidden_state()
sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)

sample_hidden = encoder.initialize_hidden_state()

# Test decoder stack

print(""Creating decoder"")
# vocab_size, embedding_dim, dec_units, batch_sz, max_length_input, max_length_output, attention_type='luong'
decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, max_length_input, max_length_output, 'luong')
sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))
decoder.attention_mechanism.setup_memory(sample_output)
initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)

print(""Creating the optimizer"")
optimizer = tf.keras.optimizers.Adam()

checkpoint_enc_dir = './training_checkpoints/encoder'
checkpoint_enc_prefix = os.path.join(checkpoint_enc_dir, ""ckpt"")

checkpoint_dec_dir = './training_checkpoints/decoder'
checkpoint_dec_prefix = os.path.join(checkpoint_dec_dir, ""ckpt"")

checkpoint_enc = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder)
checkpoint_dec = tf.train.Checkpoint(optimizer=optimizer,
                                 decoder=decoder)

# restoring the latest checkpoint in checkpoint_dir
checkpoint_enc.restore(tf.train.latest_checkpoint(checkpoint_enc_dir))
checkpoint_dec.restore(tf.train.latest_checkpoint(checkpoint_dec_dir))

print(decoder.embedding.variables)
```


### Relevant log output

```shell
[]
```
",True,"[-3.72615039e-01 -6.19931072e-02 -3.81860852e-01 -7.22955614e-02
  6.72439262e-02 -4.66187268e-01  1.19733393e-01  1.26339301e-01
 -5.77908039e-01 -3.23415577e-01  2.86423236e-01 -5.43616563e-02
  5.92502281e-02 -6.19572736e-02 -3.11250806e-01  2.08684593e-01
 -1.93644643e-01 -8.93435255e-03  2.34208494e-01  1.13228172e-01
 -3.31293792e-01 -1.85061410e-01 -3.66694808e-01  4.19045597e-01
  1.57507330e-01  2.80502856e-01 -2.37244248e-01  1.61970198e-01
 -1.27134129e-01  6.27313852e-02  2.07924753e-01  1.99197143e-01
 -4.01795089e-01  9.84096080e-02  1.97580785e-01  1.64413258e-01
 -2.80462235e-01 -1.86285436e-01 -2.51773328e-01 -6.64917678e-02
  2.08925232e-01  3.37011702e-02  4.47016433e-02 -1.75017908e-01
  9.56586525e-02 -2.99614161e-01 -6.97799176e-02 -1.05919436e-01
 -4.99841943e-02 -6.14378929e-01  1.79302514e-01  1.95502311e-01
 -2.77137458e-01 -4.40874368e-01 -3.15448403e-01 -1.17876582e-01
  3.48943651e-01 -6.26950115e-02  2.69705169e-02  2.34681189e-01
  1.80826247e-01 -4.67973538e-02 -1.86904110e-02 -1.81193024e-01
  3.01832736e-01 -1.89476758e-01  2.27208063e-01  8.15794766e-02
  5.08278251e-01 -6.36110678e-02 -1.38378022e-02 -1.51277691e-01
 -5.90639591e-01 -5.76892346e-02  1.83423281e-01  1.48008436e-01
 -8.03375244e-02  3.57135870e-02  2.31517673e-01 -4.82435644e-01
 -1.98805183e-01 -4.14522290e-01 -1.49137527e-01 -4.80829537e-01
  2.22617507e-01 -2.26051629e-01  4.41662192e-01  1.46513343e-01
  7.37036228e-01 -1.60772651e-01  4.69429314e-01  5.46663165e-01
 -8.85788053e-02  1.35927260e-01  3.90084803e-01  9.96183082e-02
  1.45617872e-01  3.35903674e-01 -1.28177315e-01  5.40615693e-02
  2.11355165e-01 -4.71835062e-02 -1.03125721e-01  2.77079642e-04
 -1.99121743e-01  7.74890333e-02  2.01305658e-01 -2.54120708e-01
  6.47051111e-02  6.58845305e-02  2.92272449e-01  1.86340660e-01
  3.09930801e-01 -9.70638692e-02  1.16816312e-01  1.43866271e-01
 -9.25894380e-02 -1.16718948e-01 -1.60583198e-01  6.21698737e-01
 -4.36760262e-02 -1.30294532e-01  1.54323056e-01  2.33174518e-01
  3.64502013e-01  1.61840037e-01 -5.64263389e-02  2.27858588e-01
  9.73674431e-02 -1.86813742e-01 -5.27799949e-02 -7.18275085e-03
  3.25848401e-01  1.66419029e-01 -9.53959301e-02  9.71934646e-02
 -9.49718356e-02 -1.83456525e-01 -2.83368528e-01 -2.39472836e-01
 -3.06509376e-01 -2.98337452e-02  1.17105886e-01 -4.33464587e-01
  2.66311914e-01  1.60181373e-01 -2.51021743e-01  1.26833424e-01
 -2.99204409e-01  1.75963685e-01 -6.76684082e-02 -2.41793450e-02
 -5.87340966e-02  4.25394565e-01  1.89243294e-02  4.14952226e-02
  1.74859852e-01 -1.67400874e-02  1.15807593e-01 -4.82866704e-01
  5.20419423e-03  5.07601678e-01 -1.59603264e-02 -5.11690527e-02
  2.21463189e-01  6.54084682e-02 -4.62058455e-01 -3.98980021e-01
  1.52479082e-01  2.65805870e-01 -8.21746662e-02 -2.61001706e-01
 -1.19249657e-01 -1.28662452e-01  2.93587774e-01 -1.14512183e-01
  1.37400359e-01 -4.91405815e-01 -1.15840532e-01  3.97008061e-01
 -8.42642784e-02 -1.16688952e-01 -1.45340487e-01  2.27734223e-01
  1.13917157e-01  9.62844789e-02  1.87777784e-02  2.89051890e-01
 -4.32812989e-01  1.21313501e-02 -5.15195966e-01  1.40620679e-01
  3.81412029e-01 -2.16851294e-01 -2.97501571e-02 -5.35184145e-02
  1.81978852e-01 -1.18305534e-03  1.26272649e-01 -8.61512944e-02
 -3.79246138e-02 -1.95244551e-01 -1.23416372e-02  1.34762097e-02
  5.60189337e-02 -3.39647770e-01 -2.60455579e-01 -5.04360914e-01
 -7.76752457e-02  1.70502290e-01 -5.89071698e-02 -6.00224137e-01
  2.69107580e-01 -3.08005929e-01 -3.19894373e-01  1.48736879e-01
  6.02395833e-02  2.21327215e-01 -6.94663078e-02  7.04466105e-02
  3.00174654e-01 -2.82716513e-01  1.08769022e-01 -3.53289038e-01
 -3.01379897e-02  4.20975745e-01 -2.24297851e-01  5.72524294e-02
 -2.15440840e-02  1.51141539e-01  8.19133893e-02  2.14271754e-01
  2.51601279e-01  3.38666141e-01  2.41897717e-01 -1.59357727e-01
  9.62910876e-02 -4.48296219e-02 -3.02432254e-02  2.90465150e-02
 -4.42809463e-01 -2.78260887e-01 -3.88180986e-02 -7.60628879e-02
  3.50054085e-01  4.97837365e-01 -2.98980772e-01  1.12687051e-01
 -4.79791582e-01  4.58636507e-03 -1.99147120e-01  2.47768193e-01
  2.89700449e-01 -5.81018552e-02  5.58601856e-01 -3.60398181e-03
  2.10342824e-01  2.47024864e-01  3.05226237e-01 -3.09409797e-01
  5.14723361e-01  1.07405908e-01 -1.29154786e-01  3.89172614e-01
  3.12339038e-01  2.47267514e-01 -3.72826338e-01  5.24210513e-01
 -1.29755139e-01 -2.87457705e-01  3.12510490e-01 -5.36193490e-01
  4.55603957e-01 -3.10911417e-01  2.45668262e-01 -1.97602391e-01
  6.19885206e-01  1.45906508e-01  2.76952703e-02  4.10749167e-01
  1.32691413e-01  4.56205189e-01 -3.77857208e-01  1.69206768e-01
  1.22801661e-01 -2.74036646e-01 -1.74359441e-01 -5.80552995e-01
 -1.91390395e-01 -1.17498398e-01 -1.47921473e-01  1.06149651e-01
 -1.83078051e-01 -1.13664091e-01 -1.34080518e-02  2.03033328e-01
  1.29886553e-01 -2.51331657e-01  2.21545905e-01 -3.42766121e-02
 -4.37527537e-01 -7.46607408e-02  4.08128202e-01 -5.54641247e-01
 -1.81782812e-01 -3.73420954e-01  3.16717774e-01  4.83523637e-01
  4.45514321e-01 -5.22804439e-01  2.51256138e-01 -1.18435763e-01
  9.45225507e-02  3.25396627e-01  1.51544437e-01 -1.43413395e-01
 -3.57800692e-01  7.71796703e-01  6.61768243e-02 -1.55311152e-02
  3.80828381e-01  8.16101432e-02 -4.74264085e-01  1.37886614e-01
  2.33111501e-01  1.01609072e-02  1.69764794e-02 -4.65707541e-01
 -5.65723404e-02  3.48323703e-01 -1.89381577e-02 -1.44836485e-01
  1.66891441e-02 -5.54165244e-02 -2.35194117e-01 -1.48388267e-01
 -4.10327852e-01  2.41189599e-01  1.31640863e-02 -4.60181713e-01
 -3.80747736e-01  1.15842625e-01  7.36859366e-02  1.88147724e-01
  1.53569221e-01 -3.09053451e-01  2.47699857e-01  7.53214300e-01
  3.21882397e-01 -1.73969343e-02  2.01734286e-02 -9.92691666e-02
 -1.98274240e-01  1.36445746e-01  7.75564462e-02  3.37951869e-01
  1.02254767e-02 -1.10887781e-01  6.42213821e-01  4.16747212e-01
 -2.94244707e-01 -4.38072197e-02 -2.79154480e-01  9.08698961e-02
 -1.16330341e-01 -1.85127914e-01 -5.58776796e-01 -3.14201385e-01
  1.12238824e-01 -2.69352831e-02 -9.30791721e-02  8.34632814e-02
 -1.55910581e-01  3.65560472e-01  4.83184755e-01 -5.16457438e-01
 -3.07302237e-01  1.06865786e-01  2.43156493e-01 -2.67999202e-01
  1.26724541e-01  7.40543008e-03 -1.05719492e-02  2.74459347e-02]"
ImportError : cannot import name 'context from 'tensorflow.python.eager' stat:awaiting response type:bug stale TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
[~\AppData\Local\Temp\ipykernel_23420\1827115418.py](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/go4av/OneDrive/Desktop/Dissertation/Code/2021fc04746/notebook/~/AppData/Local/Temp/ipykernel_23420/1827115418.py) in <module>
----> 1 from tensorflow import keras
      2 from tensorflow.keras.models import Sequential
      3 from tensorflow.keras.layers import Input, Dense, Activation, Dropout
      4 from tensorflow.keras.optimizers import Adam
      5 from keras import layers

[c:\Program](file:///C:/Program) Files\Python310\lib\site-packages\tensorflow\__init__.py in <module>
     36 import typing as _typing
     37 
---> 38 from tensorflow.python.tools import module_util as _module_util
     39 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 

[c:\Program](file:///C:/Program) Files\Python310\lib\site-packages\tensorflow\python\__init__.py in <module>
     35 
     36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---> 37 from tensorflow.python.eager import context
     38 
     39 # pylint: enable=wildcard-import

ImportError: cannot import name 'context' from 'tensorflow.python.eager'

### Standalone code to reproduce the issue

```shell
from tensorflow import keras 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Dense, Activation, Dropout
from tensorflow.keras.optimizers import Adam
from keras import layers
from keras import regularizers
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
[~\AppData\Local\Temp\ipykernel_23420\1827115418.py](https://file+.vscode-resource.vscode-cdn.net/c%3A/Users/go4av/OneDrive/Desktop/Dissertation/Code/2021fc04746/notebook/~/AppData/Local/Temp/ipykernel_23420/1827115418.py) in <module>
----> 1 from tensorflow import keras
      2 from tensorflow.keras.models import Sequential
      3 from tensorflow.keras.layers import Input, Dense, Activation, Dropout
      4 from tensorflow.keras.optimizers import Adam
      5 from keras import layers

[c:\Program](file:///C:/Program) Files\Python310\lib\site-packages\tensorflow\__init__.py in <module>
     36 import typing as _typing
     37 
---> 38 from tensorflow.python.tools import module_util as _module_util
     39 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 

[c:\Program](file:///C:/Program) Files\Python310\lib\site-packages\tensorflow\python\__init__.py in <module>
     35 
     36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
---> 37 from tensorflow.python.eager import context
     38 
     39 # pylint: enable=wildcard-import

ImportError: cannot import name 'context' from 'tensorflow.python.eager'
```
",True,"[-0.34613574 -0.5266453  -0.36756048  0.17807494  0.28001234 -0.41748783
 -0.04113523  0.03277131 -0.41941485 -0.4090005   0.05889174 -0.29667094
 -0.17647329  0.06432746 -0.131783    0.34330368 -0.3164683  -0.12459774
  0.07839391 -0.08618803 -0.05127113 -0.06303064 -0.14624959  0.06575399
  0.2704649   0.13142347 -0.1435903  -0.10388634  0.10056905  0.32061923
  0.3680061   0.01906464 -0.13732721  0.24241212  0.05790899  0.39198816
 -0.3627289  -0.13140851 -0.14056319 -0.14929754 -0.01298732  0.11064138
  0.05895066 -0.06287289  0.01810822 -0.18788217  0.16443655 -0.10753524
 -0.01477922 -0.14748496 -0.3251636  -0.05427562 -0.6572499  -0.5670287
 -0.13508198 -0.01532098  0.22312115  0.12513961 -0.15686099 -0.01590057
 -0.13404404 -0.08970496  0.09336412  0.00721173  0.03649817 -0.01261716
  0.1289268   0.08220962  0.45024008 -0.37117296  0.10897151  0.09509843
 -0.51964784  0.13957387 -0.01160623  0.0954138   0.06423609  0.15164252
  0.36405385 -0.17859063 -0.0285086  -0.10230266 -0.09253055 -0.21864629
  0.09479585 -0.00741273  0.29801464  0.02824813  0.39402974 -0.11247435
  0.759868    0.08493024 -0.02639154  0.17593995  0.47533834  0.07295658
  0.17125964  0.38922435 -0.05897538 -0.24732134 -0.14632232 -0.31362143
 -0.13668433  0.00118405  0.10630511  0.12577961  0.07427609 -0.14696333
  0.15824352 -0.13565178  0.01972461 -0.00380983  0.31601086 -0.17805752
 -0.13308619  0.10726887 -0.16752121  0.06441019  0.03862857  0.70841444
 -0.04736422  0.1300211   0.03003114  0.16620061  0.39250347  0.07514141
 -0.02627979  0.03826807  0.05640756 -0.14606589  0.27231884  0.12272657
 -0.09680304  0.38358173 -0.03498899  0.08257081 -0.42388266 -0.24965893
 -0.19013731 -0.25085092 -0.24976495  0.22665103 -0.05417551 -0.62146354
  0.10840548  0.12482424 -0.0615201   0.3188256  -0.39352903 -0.0475805
 -0.02471071  0.17571092 -0.3627798   0.4191726   0.05628286  0.25810355
  0.3983524  -0.02314466 -0.02170133 -0.7087333   0.04910083  0.48492065
 -0.23752919 -0.1287613   0.05545443  0.21550173 -0.5074769  -0.24161896
  0.0969322   0.41048983 -0.09818237  0.03136677 -0.10313679  0.01180502
  0.27712607  0.08165742  0.1490717  -0.59977674 -0.12262495  0.30747756
  0.29657573  0.2186599  -0.22491157  0.21130319 -0.06858986  0.07225755
  0.01033211 -0.10697594 -0.17479208  0.12627237 -0.15627855  0.13199914
  0.64463264 -0.1282218  -0.02262805  0.25091708  0.38030052 -0.18845487
  0.06023088  0.03923055 -0.18374097 -0.14154607 -0.04337635  0.10757695
  0.1087413  -0.39565748  0.14341654 -0.3984663  -0.247547    0.2731263
 -0.0118539  -0.56179     0.07855265 -0.15536001 -0.33563706  0.10964068
  0.27035263  0.10175122 -0.24759641 -0.05921067  0.00617833 -0.15242705
  0.09340089 -0.4303809   0.12172205  0.1725563  -0.3322326   0.01240025
  0.02463938  0.278835    0.16001683  0.18251157  0.44175476  0.32182
  0.31010562 -0.12075941  0.12135096 -0.17401639  0.07941805  0.16991927
 -0.3190945   0.21372503 -0.1017994  -0.14537299  0.03479085  0.18628971
 -0.24150777 -0.28691816 -0.5064045   0.11105466 -0.3667159   0.06626837
  0.27913916  0.35365242  0.5130454   0.32962912  0.09331577  0.29134095
  0.0319751  -0.18196933  0.3428536   0.05505898  0.12932399  0.44947252
  0.05735441  0.14530785 -0.4590975   0.6727059   0.17277792 -0.11966519
  0.04549867 -0.4247306   0.5239005  -0.3485704  -0.05637689  0.05174329
  0.35477862 -0.10684781 -0.03394639  0.23320815 -0.04935092  0.1807011
 -0.31337804  0.03288781  0.14413857 -0.0796729  -0.11475109 -0.54415965
 -0.21543898  0.05760534 -0.27219725  0.27426666  0.09004143 -0.26934046
 -0.12117858 -0.05553941  0.07955997 -0.12057675 -0.01474479  0.3609278
 -0.0028942   0.2658714   0.06699999 -0.30567774 -0.0516855  -0.17875999
  0.41946024  0.23578498  0.28929493 -0.34431353  0.2595523  -0.04559602
 -0.09243727  0.60905373 -0.11969218 -0.05463047 -0.10270682  0.7284911
  0.33656815 -0.05012791  0.02168695 -0.30551088 -0.35597587  0.12087969
  0.13058189 -0.03058537 -0.09418097 -0.11270214 -0.08083007  0.29304227
 -0.30157548  0.04392445 -0.2721013   0.06282976 -0.25876278  0.03057887
 -0.45047188  0.26323497 -0.00584495 -0.20044576  0.03397647 -0.13788776
 -0.02942316 -0.1399768   0.03184375 -0.5241877   0.28429008  0.6421504
 -0.12224453  0.02220907  0.1511622   0.11451012 -0.5209973   0.08483401
 -0.05942236  0.31894675  0.03744003 -0.30662277  0.23654076  0.2772469
 -0.34209883  0.04278585 -0.1564675   0.00212499  0.18249208 -0.10109536
 -0.23885423 -0.23428717  0.06635781  0.24119121 -0.19218983  0.25267363
 -0.4489993   0.36685684  0.68425167 -0.34085813 -0.48210347  0.36637023
  0.33246398 -0.21026969 -0.08381686  0.09055869  0.21370494 -0.16499363]"
Could not find matching concrete function to call loaded from the SavedModel type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.10

### Custom code

Yes

### OS platform and distribution

win10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

How to match data of signature?

### Standalone code to reproduce the issue

```shell
...

model.save(filepath=save_model_dir, save_format='tf', signatures=None)
local_model = tf.keras.models.load_model(filepath=save_model_dir)

y_local_pred = local_model.predict(x_test)
y_model_pred = model.predict(x_test)
print('y_local_pred == y_model_pred:', numpy.allclose(y_local_pred, y_model_pred))

user_inputs = [
    tf.TensorSpec.from_tensor(tf.convert_to_tensor(user_inputs[0]), name='inputs/0'),
    tf.TensorSpec.from_tensor(tf.convert_to_tensor(user_inputs[1]), name='inputs/1'),
    tf.TensorSpec.from_tensor(tf.convert_to_tensor(user_inputs[2]), name='inputs/2'),
]

user_outputs = local_model.user_fn(user_inputs)
```


### Relevant log output

```shell
Could not find matching concrete function to call loaded from the SavedModel. Got:
  Positional arguments (1 total):
    * [<tf.Tensor 'inputs/0:0' shape=(150, 5) dtype=float32>,
 <tf.Tensor 'inputs/1:0' shape=(150, 10) dtype=int32>,
 <tf.Tensor 'inputs/2:0' shape=(150, 3, 5) dtype=int32>]
  Keyword arguments: {}
 Expected these arguments to match one of the following 1 option(s):
Option 1:
  Positional arguments (1 total):
    * (TensorSpec(shape=(None, 5), dtype=tf.float32, name='inputs/0'),
 TensorSpec(shape=(None, 10), dtype=tf.int32, name='inputs/1'),
 TensorSpec(shape=(None, 3, 5), dtype=tf.int32, name='inputs/2'))
  Keyword arguments: {}
```
",True,"[-8.09374213e-01 -7.03403801e-02 -2.48599410e-01  1.19659059e-01
  3.94512475e-01 -3.73781621e-01  1.46646291e-01 -1.30174994e-01
 -4.10411060e-01 -3.83880734e-01  3.01526964e-01 -1.36958435e-01
 -3.43792178e-02  2.18205117e-02 -8.11018236e-03  2.32265860e-01
 -2.19869837e-01  1.20423399e-01  6.38362467e-02  5.96004762e-02
 -9.39383358e-02 -1.12335287e-01 -1.76848561e-01  1.65503144e-01
  2.59957790e-01  3.36040080e-01 -1.50550961e-01 -4.79444787e-02
  1.85215563e-01  1.43106490e-01  5.80734849e-01 -4.69658896e-03
 -1.41094849e-02  1.44519851e-01  3.92598987e-01  4.46805924e-01
 -4.14831161e-01 -1.17937997e-01 -1.07890479e-01 -5.05372584e-02
 -1.54121846e-01 -8.53950977e-02  2.75851607e-01 -2.75452677e-02
 -1.55825198e-01 -2.56160617e-01 -1.07722595e-01 -1.62842199e-01
  1.98291745e-02 -2.58373559e-01  3.00724983e-01 -1.52493194e-01
 -7.58939385e-01 -3.68265331e-01 -1.54370904e-01  1.81944855e-02
  9.30044055e-02 -1.09722301e-01  8.59894380e-02  3.88843450e-03
  2.34459624e-01  1.64675072e-01 -1.77058429e-01 -1.70704424e-01
 -1.56095341e-01  3.02210838e-01  1.13512158e-01 -3.13751064e-02
  5.35692990e-01 -1.82941288e-01  3.92211109e-01  3.03025186e-01
 -3.26199591e-01 -1.99222565e-01  2.84519672e-01  1.19739622e-01
 -9.00595337e-02  3.31386298e-01 -1.62371360e-02 -1.65172011e-01
 -2.89052725e-04 -4.62522000e-01 -1.03286937e-01 -1.69510245e-01
  2.86445916e-01 -6.30068779e-02  4.20533180e-01  2.39902660e-01
  5.18941402e-01 -2.31833905e-01  1.46817684e-01  2.71832138e-01
 -2.34172583e-01  1.05122790e-01  2.30527237e-01  2.68480003e-01
  1.28462493e-01  1.56905204e-01  1.73773654e-02 -1.22954138e-01
 -5.92850447e-02 -3.15844044e-02  3.66983712e-02 -2.31364910e-02
 -3.72865014e-02 -1.04145603e-02 -1.00029901e-01 -1.63671732e-01
  1.93591177e-01 -2.67703444e-01  6.27131835e-02 -1.46444470e-01
  2.46737510e-01  3.30626518e-02 -1.64092749e-01 -1.77921489e-01
  6.12463616e-02 -3.34119260e-01 -9.02930796e-02  2.19805866e-01
  2.15303540e-01  1.94895715e-01  5.35907485e-02  2.71865606e-01
  3.34623694e-01 -3.62991169e-02 -3.80212784e-01 -4.01103124e-02
  4.16095629e-02 -1.49873659e-01 -6.52811304e-03  1.98919311e-01
  3.80357414e-01  3.77788674e-03  8.36814493e-02 -2.18511000e-01
 -5.99150538e-01 -1.42008424e-01 -1.23374060e-01 -2.96855330e-01
 -1.08192697e-01  2.65981525e-01 -1.18209183e-01 -6.64697528e-01
  2.53688097e-01  4.77010943e-02 -2.96031237e-01  4.27032799e-01
 -2.48202458e-01 -1.06661156e-01 -2.58861780e-01 -1.12121865e-01
 -1.30458564e-01  3.51133406e-01  2.09724754e-01  2.67500639e-01
  5.92774630e-01 -1.78380340e-01  1.32077679e-01 -3.63156587e-01
  2.47722492e-02  5.69301724e-01 -1.62480742e-01 -5.24945110e-02
  1.66460633e-01  1.43815190e-01 -3.87388170e-01 -3.02186340e-01
  2.61708468e-01  4.10572112e-01 -2.97144830e-01 -2.50997424e-01
  2.20675692e-01  1.60403177e-03  1.25131980e-01 -1.15459710e-01
  4.05280769e-01 -9.49972391e-01 -1.25466749e-01  6.62688971e-01
  9.67584848e-02  5.75990453e-02 -1.97764467e-02 -3.09802175e-01
  1.63086891e-01  3.70331377e-01  3.35728005e-02  2.43229419e-01
 -2.08455771e-01 -7.43297040e-02 -4.68354225e-01  4.17778753e-02
  1.62104785e-01 -1.49796098e-01 -1.56431392e-01  1.43404335e-01
  2.53971308e-01 -9.64960158e-02 -2.96593681e-02  2.29500994e-01
 -2.47876048e-01 -1.97479159e-01 -6.08913973e-02 -5.81972152e-02
  3.06591559e-02 -2.99086329e-02 -7.39980265e-02 -4.75932479e-01
 -3.13666575e-02  1.81521595e-01 -1.64292723e-01 -7.09255099e-01
  1.50155321e-01 -7.80024379e-02 -2.93089867e-01  1.58397153e-01
 -4.67740372e-03  9.69685763e-02 -2.93936729e-01  1.45240694e-01
  3.98556888e-01 -2.32218668e-01  2.21839905e-01 -3.29137623e-01
 -1.38001829e-01  2.52905309e-01 -1.64636403e-01  9.11385566e-03
  8.74284208e-02  3.24824631e-01  2.26792172e-01 -3.17376703e-01
  3.05026382e-01  1.27921522e-01  5.58076143e-01 -6.98565915e-02
 -1.65207580e-01 -2.13308081e-01 -1.55817987e-02  1.57276988e-01
 -5.19994259e-01 -3.81702662e-01  5.51312789e-02 -1.88773172e-03
  3.63422483e-01  4.97039735e-01 -3.05771679e-01  1.33145645e-01
 -4.61580336e-01 -3.27674411e-02 -5.19894779e-01  1.44123554e-01
  5.93818665e-01  3.93960662e-02  3.43124807e-01  2.57378757e-01
  2.56001174e-01  4.19368386e-01 -4.91462275e-02 -3.05450678e-01
  3.01811397e-01  1.57388732e-01 -1.13993250e-02  2.60678709e-01
  3.30711961e-01  1.39547959e-01 -2.73432434e-01  4.73130465e-01
  2.38090325e-02 -2.00830162e-01  3.39426458e-01 -4.48072284e-01
  4.28391993e-01 -3.84119570e-01  2.38946676e-01 -1.60091091e-02
  2.48706564e-01 -1.07417814e-01  2.42124438e-01  3.22822303e-01
  8.35765675e-02  7.30298758e-02 -3.02307546e-01  1.27777189e-01
 -5.76270670e-02 -1.42216027e-01  1.37323737e-01 -4.38231885e-01
 -3.79964590e-01 -1.69236921e-02 -3.66318315e-01  1.58913732e-01
 -1.32998630e-01  3.03483363e-02 -1.95942000e-01  1.69568673e-01
  6.60259128e-02 -1.05131827e-01  3.90837163e-01  4.18228984e-01
 -8.73305798e-02  2.57073827e-02  1.62003428e-01 -3.54630947e-01
 -1.08603194e-01 -1.71515644e-01  3.39844584e-01  1.97899729e-01
  3.92164111e-01 -5.04118562e-01  1.79605216e-01 -1.33278430e-01
 -1.44871891e-01  4.84220415e-01 -1.35689095e-01  2.29159057e-01
 -2.55299240e-01  5.73905110e-01  1.85756564e-01  1.32480398e-01
 -1.39070861e-02 -2.80407965e-01 -4.43810403e-01 -2.26836771e-01
  1.43650398e-02 -2.37397820e-01  5.24684310e-01 -5.84106207e-01
 -4.82474491e-02  1.06639802e-01 -6.13505483e-01 -1.78027019e-01
 -1.22358337e-01  1.86352938e-01 -3.21427971e-01 -2.57388234e-01
 -3.67076814e-01  9.75860581e-02  1.47925958e-01 -2.26487234e-01
 -4.20165360e-01  4.87937778e-03  2.56291255e-02  2.11694464e-02
 -1.57686457e-01 -3.42937112e-01  2.45939851e-01  5.97564995e-01
 -2.51751810e-01  3.99000317e-01  1.90150976e-01  2.43830234e-02
 -4.58611637e-01 -1.06245540e-01  8.05922896e-02  2.54070520e-01
  3.95232290e-01 -2.25028723e-01  2.37375051e-01  1.39964372e-01
  6.09155521e-02 -3.36348563e-02 -2.44975090e-01  1.09518051e-01
  1.07491940e-01 -6.88617229e-02 -1.91657335e-01 -5.03283679e-01
  1.63415819e-01  3.62261534e-01 -1.77505910e-02  1.84485212e-01
 -4.02908564e-01  1.63175970e-01  6.85282588e-01 -1.90566778e-01
 -1.97959334e-01  1.41055673e-01  3.89207244e-01 -2.71174937e-01
 -1.71636969e-01 -1.03866227e-01  4.51568663e-01  1.45677924e-01]"
load_associated_files does not load a txt file stat:awaiting response type:bug stale comp:apis,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On ""load_associated_files"", it emit longest `does not exist` error.
All the lines below reproduce the same result.

```
populator.load_associated_files([os.path.abspath(""./label_file.txt"")])

populator.load_associated_files([])

populator.load_associated_files(['label_file.txt'])
```

The label_file.txt exist in the same folder as main.py.
<img width=""981"" alt="" 2023-09-12 12 15 38"" src=""https://github.com/tensorflow/tensorflow/assets/52132649/178ef07f-e6a3-446e-856e-865b12a36962"">



Also, what information am I supposed to put in the txt file ?
Is there any documentation for that ?

### Standalone code to reproduce the issue

```shell
# Creates model info.
model_meta = _metadata_fb.ModelMetadataT()
model_meta.name = ""Cup classifier""
model_meta.description = (""Identify a cup"")
model_meta.version = ""v1""
model_meta.author = ""Integro""
model_meta.license = (""Apache License. Version 2.0 ""
                      ""http://www.apache.org/licenses/LICENSE-2.0."")

# Creates input info.
input_meta = _metadata_fb.TensorMetadataT()

# Creates output info.
output_meta = _metadata_fb.TensorMetadataT()

input_meta.name = ""image""
input_meta.description = (
    ""Input image to be classified. The expected image is {0} x {1}, with ""
    ""three channels (red, blue, and green) per pixel. Each value in the ""
    ""tensor is a single byte between 0 and 255."".format(160, 160))
input_meta.content = _metadata_fb.ContentT()
input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()
input_meta.content.contentProperties.colorSpace = (
    _metadata_fb.ColorSpaceType.RGB)
input_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.ImageProperties)
input_normalization = _metadata_fb.ProcessUnitT()
input_normalization.optionsType = (
    _metadata_fb.ProcessUnitOptions.NormalizationOptions)
input_normalization.options = _metadata_fb.NormalizationOptionsT()
input_normalization.options.mean = [127.5]
input_normalization.options.std = [127.5]
input_meta.processUnits = [input_normalization]
input_stats = _metadata_fb.StatsT()
input_stats.max = [255]
input_stats.min = [0]
input_meta.stats = input_stats



# Creates output info.
output_meta = _metadata_fb.TensorMetadataT()
output_meta.name = ""probability""
output_meta.description = ""Probabilities of the 1001 labels respectively.""
output_meta.content = _metadata_fb.ContentT()
output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()
output_meta.content.contentPropertiesType = (
    _metadata_fb.ContentProperties.FeatureProperties)
output_stats = _metadata_fb.StatsT()
output_stats.max = [1.0]
output_stats.min = [0.0]
output_meta.stats = output_stats
label_file = _metadata_fb.AssociatedFileT()
label_file.name = 'label_file.txt'
label_file.description = ""Labels for objects that the model can recognize.""
label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS
output_meta.associatedFiles = [label_file]



# Creates subgraph info.
subgraph = _metadata_fb.SubGraphMetadataT()
subgraph.inputTensorMetadata = [input_meta]
subgraph.outputTensorMetadata = [output_meta]
model_meta.subgraphMetadata = [subgraph]

b = flatbuffers.Builder(0)
b.Finish(
    model_meta.Pack(b),
    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)
metadata_buf = b.Output()

populator = _metadata.MetadataPopulator.with_model_file(tflite_model)
populator.load_metadata_buffer(metadata_buf)
populator.load_associated_files([os.path.abspath(""./label_file.txt"")])
populator.populate()
```


### Relevant log output

```shell
.....{q2\xbd\xf6\\\x9c\xbd\x80\xa1\x0c=\xa6\xfa\x1f\xbd\xc2\xdd\x99\xb9:x\xa9\xbb]B\xe4=\xd4\xef\x14\xbdm\xcb\x0c\xbdO8\xfb\xbdk\x8dl=}s\x11=\xb6\xcf\xab=\x07x\x0c=\xf2t\xe5\xbc:\x01\x8a\xb9\xdb+\xa0\xbd\x0f\xf5\xea\xbd\xf8\xaf\x1d\xbd\xe3s\xde\xbcm\x86\x87\xbd\x11\xa3\x18=\x1e\x0bM=\xb6\x00\x17=\x97:\xd0=\x10\xda\xf2;\x89\x1b>=.0\xa4\xbde\xd7\xad\xbd\x88\x00R=\x0f\xe7\x85=_\xda\x9b=\xd2v\xfe<\xa9\xe9\x07=\xba\x91\xaf=\xaa\xad\x98\xbc)\x9c\xbe\xbc""\x17s=0n\xfc\xbdB\x9c\xd1=\x9er\t=R\\\xad=\xac\xc9x=\xe9R\x94\xbdR\xd4\x13=\xb9\xc7D=\x91\x18T=J\x8e\x94=\xe2\xeb\x8a\xbd\xe3\xf8\x00\xbd6\xa2\x9b\xbc\xfa\t\x98=\xf9\x18\xdf<C\x1e\xa7=\xe2\x9a\xa0=]k\x0b\xbd\x86.7\xbc\xbar\xe4;\xcb\x9b\x0c>\xc2#\xd9=\xaaB\x1f\xbc\xa4\x03\xbc\xbd6\xea\xf0\xbd\xca\xae\xa8\xbd(\xb7\xfa\xbd\xff\x92D=\xa66\xd7\xbd\xf7\xea)=`\xf1\xa2=\x92\x120\xbd\x8eb\xa9=\x13\x8e\xfd\xbd\xe0\x9d)\xbd\n\xd1$=\xd7K\x12\xbd\xb6X\xa0<\xaeL\xd1=\x053\xa2=\x01X\xb3\xbd8\x9df\xbd\\\xef&\xbd+k&\xbc\xc8\xa0\x8a=G\x81\xc8\xbd\xeal5=\xd4`\xbf\xbd\xca\ny=T\xb0P\xbd<e\x7f\xbc\xcaU^<\x7f\xc2\xd1;p\xa3\xb8\xbd\x16\xefF\xbd%r\x87\xbd\x81{\xb1=\xf9\x13\xfb\xbc\xc1\xf8\xce;\x06\x8a\x87\xbd\r\x8fK\xbd\xb2\xd4\x14=\x9d\xecL\xbd\x0c\xe6""=F:o\xbd\xd8_\x9c\xbd\x18\xd6:=\xa6\x1b<=\xde\xd3\xa1=fY\xfd\xbcw),=\xec\xa4\x80=\x89\xdf\x9b\xbd\x8c\xf6\x06\xbe\xc9\xc7\xab\xbdO\x00\x93\xbd+\xf8\x98\xbd\x84\xeff\xbb5\xa3\xd2=v4\x19\xbc\xd3\x7f\xe7\xbd\xa3}\x84\xbd\xd6\xb2C\xbd\xda\xc6`\xbd\xf5\xde\xb5\xbdr\xed5<uo\x84<\x03\xd7\xdb\xbd\xa9\x00\x94\xbd\x06(\x88\xbdM\xa4\x82\xbd\x1a\xdb\xc9=\xcf\x9d\xcd\xbc~SG\xbd\x18\xeb~\xbd\xd0Wv<\x01[\x98=\x92\xba\x98= \x83\xd5\xbd\x10\x85b\xbc\x95+\x92=\x8a\x1f\x11\xbb\x18b$=<\xe9\xaf\xbdjfr=\x958\xed\xbd\xd8\xb2\xc2<Q[\x97\xbd\x14\x9e\x9e=\xdd\x19\x18\xbb\x8c\xa0\x81\xbb^0\x1a<\x7fW\x00\xbd\x1c\xc9\xd4=\x83\xa8\x04\xbd\xd7\x99k=-\xe5U=\x16\x83\x9b\xbcf\x90\xb9\xbdJ\x1f\xab=\x82\x00\xe3\xbd@3.=\x1e\x98\xac\xbdX\x8e\x1c\xbd\x91\x83\xca=\xed\xedJ\xbd@\x96\xd8=\x87\xb6\x0e=?q\xb4\xbd\xc4\xe9\xac=q\xe8\xbd\xbd\x7fe\xed<:\xb4\x1a=G\x9a\xc9\xbd\xf0\xc4\xc2=NN\xe8\xbd\xfdb\xf7<\xacs\xb7\xbd\xa6\xd9\xf1\xbcg\xff\x0b<\xdc\x8ea=u>\x98\xbb\xc2f\x86\xbd\xaf\xd8I\xbd|\xc4R=Pk\x04=\xde\xbfL=\xb8&\xa8\xbd:\xe0\xf0\xbdKo\xc9\xbd\xac\x1e-=\xfe\x0b\xa5=2\x02.\xbdk\xac\x8c\xbd\xf8\n;<w\xca)<\xc8\xae\xd9\xbdj\xd9\x9d\xbd\xd4.B\xbd\x80]\x02\xbd\x9a\xe1\xa3\xbdGLe\xba\xac\xf3N\xbd0\x81\x12=\xc0\xa0\xa5\xbd\x0c-\xf3<\xeb\xca\xda\xbd>\x0bE<}\xcf\x84=\xb1\xf9\xac=\xf5\x87X\xbdP""_\xbc\x8c?\xf4<\xb7\x94\x1d=\xd2\xdbG\xbb\xe5?<={Y\xe3=\xbf?\x83\xbd\xfe\xe8\xbb=*\xc2\x9f\xbd\xbd\xb3\xd0=\x18j\xf9=\x9b\x8d\xd0\xbc\x08\xb4^<\xa74*\xbc\xe7\x12\xea\xbc\x81\x88Y=o\r\x7f\xbdP\x8f\x8d<YH\xa8\xbba\x0c\xf9\xbc_\x87\xac\xbd7<_=\x05\xe53=\x01\xabH<%\xfe\x9f<%yP<w\xd3\x83\xbd\x9aL\xbe<&\xd3\xb9=\x93\x94\xbe=\x85\x11o\xbd\x11F\xc1<""\x8c\xa2=\xb5\x13\xb9\xbb\xae\xe4\xef\xbd\xb1y =\x92\xe5\xc5=\x10\xcb\xf8\xbd\xce\x8b\xd0\xbd\\\xceX=$\xb5\xe7<v\x10\x9f\xbd\xd3^\xbf=n\xff\x9f<\x83\xcb\x85\xbd\x14\x7f\x94=\xfb\xda\x95=\xa07q;\x8f\xe9\xb7\xbd\xdc16\xbc\xed_\xb3\xbd\xdf\xf1E\xbd_\xa1\x02\xbe`B\xb5\xbd\x1f\x15\x80=\xc9\xba\x9b<6\xa7\x83\xbd\x8c\x11\xd2\xbd\xe2\xaa\x94\xbc\xafHO=D\xeb\xcb\xbd\x19\xf6\x12\xbb9\xbf\x98=e\xa9\xb4\xbd\xde\x89\xa0\xbc\n\xe6\xcb<\xa5\x99\x89:\xc1z\xd6=\xb7h\xc0\xbde\xde\x13=Fo\xf3\xbd\xc2\xa5\xaf\xbd\x8b\x1bw=7\xf0\xac\xbd\x0e\x9a\x9d<4\x00\x9e\xbb\x8d?\xd1\xbd\x84\xf0w=]Y\'\xbd\x80Dz=\xe0""\xc2=bG?\xbc_7\xc3=\x83=\xc5=;\x11\xea\xbd\x84|\xbc=\xdc\xcf""<\x02\x01\xa8\xbc}\x96\xa0\xbd\xe5\xf9\xd7\xbd\xdaS\xa8=y\xd1\x96<\xe82U\xbdS\xbb\x9f\xbdr\xf7\xca=\x87\xb2\xa7\xbc\xde\x9c\xfa\xbd\xb0\x12\x01=\xef\\\xa3\xbc\x00.\xb4=\xdf\xfdS=\x87\xc3h=\xae \xa9\xbc\x08\xd8\xc9\xbd\xd2P\xcf\xbc\x991\xa4=\xa1s*\xbd\x07\x16\xee;\xb4\xbeO\xbd\x8b\x95\xfd<\xe8H\xb7<\xdd\xe4\xd7\xbd\x87\xefo=\x9f\xc8\xce=\x04A\xa6\xbd\x88\x95q=\xcd\xf9\xd7\xbdZ\xc4\xb3\xbd\xcap~<\xbd\xa8\x86\xbd\xf5\xf2\x83=\xac3^=\x1c\xc6%\xbdn\xd6\x9b\xbd|\xa5\xda\xbd\x12\xec\x98=\xd8\xd2q=H\x1f\x01=\xe7\xcf\xec<\xb6G\xd5\xbc\xf3>w=\x10\x05\xff\xbd\x92\xa2E;\xd4\x8fm\xbd\x07\xce\x1c=\xc3O:\xbd%\xca=\xbcKi\x8d\xbd;\xef,=\xee\x1d\x91=\xca=Z\xbd\x166w\xbc\x1a<\x83=\xbeG\x9e\xbd\xbcw\x04=\xf3$\x96\xbd\xdf\xd5\xa6<\xd8j\xd9=e\xe7\x9b\xbd3s\x81\xbd\x04\x15\xb7\xbc\xc8\x9e\xb0\xbd\xe8\xad\x9b<{\xab\x94<\xa1=\x9a<\x81\xfe\xcf\xbda\xbbr\xbc\x95\x9f\x8d=\r\x9e\xba\xbd\x84(&\xbd-L\xad=\x10)^=\xb1\xefQ=u\xde\xb68\xb7\xe2\xab<\x88\x1c\x9a=R\\\x00\xbd\xa2q\x96\xbd\xda\xc3,\xbd\xaf\xc4\xf3\xbd\xed<:=\xd3\xc1\xe8\xbd\x9d\x12\xa3\xbdG$\xb2\xbdC2\x1e\xbc\xb7o\x05\xbd\x10\xe0Q;\x08\x19\xe7\xbd\xd7\xf4\x94\xbdB\x85\x97\xbd\xbf\xf8\xab\xbd\xb9|m=/h\xa5\xbd\xbe7\x15=\xfd\xd8\xa5\xbd\xbb\xbc\xa8\xbd\x8c\xe3\x02\xbdx\x05\xfe<q}\xcd=G\xd8\xcf\xbc\r\x84\xb1\xbd\xea\n\x84=\xbe\xfai=\x80b\t=\xc5i\xa2=\xf5<""<+K~=_\xcb#\xbd\xfa\xe3N\xbd\x82(\xe1<\x19hs\xbd\xd6\xc9\x0f=\xad\\\x1e<\xb5\xdbL=&\r\x1e=I\x9f\xef<\x05\xbc\xeb=\x90\x8e\xa9\xbb\x1c\x88\xca\xbaT(?\xbd\xa0\x8e\xc7\xbd\x99\xa7 \xbd\x0e\xa8\x94=\xa5*\xea<\xd8\xc2g<\xb26\xc8\xbd\x92\xd4\xee\xbcPQ\xeb\xbd\xdf\x00\x14=\xc2s\xa6\xbd\xd6\xf0\x02\xbd~\x15\xc5=\xa4\x90\xce<\x88\x0b\xb8\xbc\xe2C\x8b\xbcV\xf5\xb2=\x97\xcc\xc7\xbd\xe9\xf7P=\xa3\xceL;\x92\xdb\xd8\xbdsn\x89= u\xf2\xbb\xcc\xf4\xd2<\x01\xfe\x96=\x04\x8c\xa8\xbc\xbca\xd3\xbdUg\xa2=\xd5\xda\xeb=\xb0:\xd0\xbbj\x18=\xbd\x00\xaf\x9a:R\xa0<=\xe0Af;\x98r\xbb\xbd\xb5\x0c\x96\xbd\x0c\xcb\x08\xbc6\xa2\xef\xbd$W\xa9\xbd]\x16\x9b\xbd\x16T\x9b\xbc$\xad\xa0<\x03\x83J\xbd\xf7\xac\xbe\xbda\x9cY<l_\xb3\xbdE_\xb6=5Z\x0b\xbd:N\x8a<o\x1e\x9b=\xc16\x87=\xe2[\x1c\xbc\xc0\xb0\xf3\xbcKo\xb3=\xd1\x04.\xbd\xb6c\xc2\xbd\x16w}=\x8ezY\xbd\x93\xf2\xc8\xbd\xa1]\xac\xbd\xd2\xaf_=\xad\xf0z\xbd\xca\x98\x99\xbdG\x8a\xc0\xbd\xbb\xfa\x9c=\x93\x00n=\xa8Q\xc1\xbc\'\xc8\x95\xbd\x97\xc3\x99=~\x98\x17=\x83\xdc\xcf\xbd\xd9\x04\xbd=\x05\xee\x94\xbd\xbcv\x9b\xbc\xf3\x88\xa2=*\xc9\xab\xbdd\x9c\xa2\xbd\x80d\x13\xbcaB\xd6=hgg\xbc\x90\xd0\xce\xbc\x03\x17\xc2=\n\x83\x93\xbd\xccac\xbd$\xcd\x83=P\xdab\xbc\x11\x1e\xb0\xbcmP\xb9\xbd\x10\xeb\xe6\xbc\x83\xa1\x93\xbd8\xdf\xad\xbd\x08\xf5J<\xfbW\xfe<\x1eF\xc2=\xd1\x11\xea;\xa5\x9e\xcc=\x9d[\r=\xc87\xf9\xbd\xf0\xafH\xbc W\xda<<x+<\x88\x86\x08<\xd7\xd5\x97<w\x99\xb5\xbc\xcd""B\xbd\xa1:\xfe\xbd\xd1m\x96\xbd\x12\xe2\xb0\xbd]~7=}\x80V=\xf0\xed\x94\xbdB\xcb\x17=\xe1\xfb\x8c=\xaa_\xba=\xb6)g\xbd\xde/\x9b\xbd\x94\x18\xc8\xbc<\xde\xc1<\x91\x9a\xa9=r\x04x\xbdZ\xfb|\xbd\x04\x15\x96<\x19C\xad=\xc7\xca\xc3\xbd[\x95\xe9\xbd\xd2.T\xbd\xb0_\t\xbd\x8e\xe2\x04=\xfbM\xbe=\x18g\xe3\xbcdf\x8f\xbd\xb6\x8d\xab\xbd&\x8c\xb9\xbd\x98\xc2\xee\xbd\xdd\x9a4=\x0e\xa65<\xfa,\x0f\xbd\x1dX\xe7<\x7f\x0c\xdc=!\x08\xb0\xbd>\xa2\xce\xbc7\xd7\xb9\xbd1K\xbf=\x98d\xcf\xbdt\xa2\xb7=\'\x18\xa9=\x92\xe7\xb7\xbd\xfe\xb7\xcf\xbd\x9bn\x85\xbd\xcd\x0ca=\xa1Cy=\x84\xb5\xe3<\xc7X_<\x8ey%=bK\xbe=\xab\x01\xb6=h\x88\xd4\xbc2\x92I\xbd\xfc\xee<=62\xef<!\x1c\x15\xbc\xf8$+\xbd|\xf6z=\x8b\xc6\x89=z\x8dp\xbd\x8a\xe0/\xbd\xf2g\xcf\xbd\x16\x18\x01\xbe{_\xc3=\x9b\xf7\xaf\xbc\xda\xa3\xbc;\x0bM\xe4\xbdv\xbf\xd9\xbcD:I=\xd4\xcc\xbc\xbd\xc6x\xdc=\xcdG\xa2=\x1e|\xdd\xbd\x0e#\x9c=\xb3\xf0\x8d=->\x83\xbd\xa1\xd2N\xbdVD\x84\xbc\xe2\x1c\xc8=\xa6\xab\xec<\x1a\xd4\x81=\xe0\x99\xdb<\xeb,\xc6\xbd1\xef\xcf=\x1f\x1f\xbf\xbd\xc2\x84\x19<\xd0\x8an=\xdbg\x86=fs\x97=\xe0=\x02=\x99\xe3\n\xbdX\x13\xe1<)\xe1u\xbc\xba\x9f\x03<\x8a\xc8Z\xbd^[\xbd=\xdfx\x9a\xbdR\x18\x9e=\xc7\x8f\x9c=\x98P\x80\xbd\xa65\x07\xbb\xbe|\xa1=\x04{\xc8=\x06\x94\x84\xbd\x86p4\xbc\xa7l9\xbc\x89\x0e\xa9\xbdh\x0f\xb8\xbc\x02\x9a$<\xfb6\x84\xbd\t\xbe\xc1=F\xbe\x14\xbc@\xb2\xb0=\x11s+=\n\xfb[=4\xdfM\xbb\xf1\x8f\x89\xbd\xad\xc1\x9a<3\xa1/\xbdz\xf8\x9a\xbc^\x96\xd9\xbc\x8c{\xa8=\x02\xff\xa3={\x9a\xa4=Z\xc5\x05=:\x81\x0b\xbd\x02\x95\xcd=a\xcc\xfc=A\xf6\xcc\xbdA\xa0\xd7=""\xa4\x9d=\xdd\xee\xcb\xbd\x8d\x0e\xce;Y\xc7\xfb\xbc\x1e\xa0\x8b=|\xa6\xf6\xbdMS\xa3\xbdE7\x10\xbd\xd5\xc0\x80\xbd\xf5\x8a\x8a=\xb7\'\x9d=\xb8\xea\xe1\xbcJF\xc0<p\x99P\xbd\xe0Uz\xbc\xd2\xf1\x80=\x01\x01,\xbdx\x87@\xbd\xef\x8b_=\x90\x99\xbb\xbdP\x81\xc1=~\xf6\xa6=\xa1\xa3\x99=\xd3mM\xbd\xb96\xe2\xbdP\x8e\xbd\xbd\x90fb\xbdH\x13\xcb\xbd\xd3&\xd0<;\xea\xb3<\x02\xc0\xd2\xbd\xef\xe4&<B.\xbf=\xf0\xc6\xaa\xbd\n\x16*=T\xcc\xae=\xcd\xd7<=\x94\xfdv\xbc\xb9g\x90\xbdp\x1e\xc8\xbd\xb3#\xef<\xb0\xf8\x15=\x9a\xf3Q\xba\xa9B\x8f\xbd\xf1j\x9b\xbd\xb1\x8f\xc6\xbd^\x07\x88=\xc6sj\xbd\xed\xf3\xea\xbd\xf0\xaa\xd8\xbd\xd5\x80e\xbd\xb5\xfch<\xc55\x83\xbd\xc9\xc5\xaa=\x06\x8d\x98\xbd\x9d\xa4\xc7;I\xab\xcf<\x02\xdcU\xbd\x83B\xd3=\x86\x18\xa1;\x1f`\x89=b\x16\xa6:l\xf9\xb0\xbdN\x16\\\xbd>|i\xbc>J\xf1\xbd\xd3\xe0\xb8\xbd\x90lz\xbd_\xae\xe2\xbd\xee\x84\xab=\xcc@\x83<\x99\xaf\xa5=\xe0Y\xcf\xbdH\xaf\x1e=\x16\x12\xd2\xbd\xfd\xcc\xcc=\xd17\xa3\xbcG\xf7\xfb=\xd1\xe9\x9b\xbc\xdc\x16\xf6:\xf4\xb4\xb3\xbd\xd4f\x81=\xf6\x98\x9a;\xf2I\x14\xbda\x98D\xbd\x16eS=\x9cB\x92\xbd\xd5\x07m\xbd)j\x1f=\xa0\x1aW=\xb1\xe8_=\x06\x84\x9b8n\xe7H=\x86\x83T\xbd\x85H\xbc\xbd\xdb\xda1<\x81N\x1f\xbd\x9ff\x1b=\x96\xa07=vCB\xbdL\xad\xbd\xbdjc\xce\xbd\xb3))<\x16$\xdd\xbd\xde~\x19\xbd\xd4\xb5\xa0<y\xfd\xda\xbc\xec\xc8\x84=\xf5\x1au\xbb\xdd\xe9\x0c\xbd\x10\xfd\x92=\x02\xe4\x01=z\x1f^\xbd\x8e\xef\xc9\xbd\x99\xf0\x85=\x90\xba\xc9\xbd\t\xa5\xc7\xbd\xc4\xaf\xe1<\xbf\xae\xc5\xbd\xbdu\xcc\xbcUI\x94=\xa0\xf9\xa5=\x93\x91\x02\xbdY\xed\x88<\xd0L\x16=\x07}a=\xc0\xab\xcc\xbc\x96\xbf\t\xbd\xe9]\x82\xbd\x1c6\xab=\x1d\x0e\x81\xbdm\xb1\xda<\x13`\x03<\xb9\x9d\x14=\xed*\xbc<\xd8V\x96=\xcf\xc1\x90\xbc\xf8\x81\x91=\x1bn\x9e=c\xf9\x93\xbc\xcf\x9d\xe3\xbc\x91\x04\x85\xbd\xb0\xbe\xb2=C%\xa2\xbc\x82\x8e\x99\xbd%\xe4\x86\xbd\'\xe6Z=\xfc\x89V\xbbI\xaf\xda<\x82\xec\x17\xbdp\xeer\xbd\n\x8e\x9c\xbd\x10&\x1f=\xfa:\x14\xbd\xea\xbf%\xbd\xdfB\x02\xbes\xeb\xff;\xfb \xc4\xbd\xcf\xcd\xed\xbd\xe7\x96\x83\xbdK0\x81=\xc0\xb0\xcc\xbd\x81\x05\x9d\xbd\xa4\xff\xc7=Lv7\xbdJ\x04\xf9\xbdp\\\xa3=\x0e\xa9 \xbd-\xc3\x9d\xbcy\xbeb=\xc9\x1b\x92=\x01\x97\xbb\xbd\xd6\xba\xb2=\xda\xee\xeb\xbc\xbb\x16\x94<\xdew\x03\xbe\t+8\xbd\x05<\x88<\xc3l\x02=\xad\xac\x91<\xde\x1f6\xbd\xf2U4\xbc\xab\xddx\xbd\x03\xf4\xc1\xbd\xaci\x10\xbd?\x9c\xc0\xbd\xffMh=dM\x14\xbd?\xa6n\xbd]j\x04\xbd\xa5]\x05\xbb\x0cs\xac\xbd\xf6%\xbd\xbd\xd4s\xa9<\xa8\x1f\xbd=\x19\xdb\xac<\x01\xda\xdb\xbd\x0c\xa6\xdd\xbc\xdcz\x85=<\xde}\xbd\r\xda:\xbd\xee\xa3\xcb=\x9bM\xac\xbdz\x8aT\xbcHF\xc3=!}\x7f\xbd\x1d>\xb2\xbd\xe9\xbe\xba=^\x0c\x9b=\xef-M\xbd\x11\x8f\xb9\xbd\xb9>\x8c\xbb8\xbf\x1d=\xd5\xdc!\xbd\x1a\xb0\xae\xbd\xb7E\xe5;.\x8fN\xbd\xaf&\xfc\xbd\xd2}\xfa;u\xa4;\xbc\xb9-\x93\xbd\x0880\xbc]\xba\xc5=[\xbc\x87\xbd8]\xe5\xbdz\xa4\x08\xbc\xe4\x87\x95=\x8f\xf7\xa6=3\xc9y=\x1a&b\xbd\xc4\x88\xd8=t\xde\xbb\xbb_m\xd1<\r\xa5\x9b=\xf8x\xc7=\xc4\xc1\x0c=w\xcf\xe0\xbd9\xc3P\xbb\xf5J\x8c\xbd\'\xc82=:\xe3\xb8=\rp\x7f\xbd<\xf0T\xbd\xdb3\xcd=\xfd\r\xcd=\x9d\x7f\xad=\xea\xc6\xe1\xbd-\x98\xa1\xbd\xde\xc6\xce=\xcc\x8b|\xbd\xd6\xb4\xc3<\x1ax/\xbdV,\xce=\xb9o\x1d\xbd\x1b\xac\x84\xbdtF\xd0;\xe2\x1f\xb0=\xf7\r\xb7\xbcb\xf6\xa4\xbdt/\xd3\xbd\x9f\xb4\xad\xbd\xd9\x06\x1c\xbdt\n\xce=\xa6\xc5\r=}t\xf1<;]\x93\xbd\xbb5\xee\xbd\x14\x8b\x8d=I2\xa9\xbd\xa7\xd4\t\xbc\xb7\n\xe2\xbb\xaca\xc6=\xc9!\xb7=\xef\x9c\xe2\xbc_,\xd2;\x1c\xbe\'<\x9d\xc5\x8b=\xd57\x1d\xbd\x9b\xd2\xbb=1X\xb6\xbd\xcc\xe6Q\xbd]E\xcb=\x87\xab(=g\xdf\xb0\xbd\xcb\x1d\x82\xbd\x0fb\xae\xbb\xb34\xb3=C&`\xbdw+a\xbd\x96m\x19\xbd)s5=\xfdF\xc3=\xf5F\xb5=\xa46\x9f\xbc\xc7\x96\x8b<u9\x05\xbd;\xe5\x88\xbdQN\x86\xbd\xa9zy<\xfa\x80H=\x15PJ=\xd8\xa4\xa1=\xb2\x0f\xa2=vl_\xbd\x93\x82\xbf\xbdrP3=k#\xc6<U]\x81\xbd\x9d\x7f\x99\xbd)\xe8\x7f\xbd\x82Y\xda=`:\x92=D\xf1O\xbd\x89\xf8 \xbd\x81|`\xbb\xd6\xa6\xea;6t\xa8\xbd\x9f.\xc7\xbc\xe0\xbe\xbe<\x90\xcd\x93\xbd2A&=:\xecm\xbbn\xaa\xa7\xbd\x1e\xe9\xfb<\xae=\x1a=g$\xde\xbdX\xa2Z\xbd\xd4\xde\x00\xbd\xe9)\xf2\xbc\xe4!\xdf\xbc\xea\xcc\x8a\xbc\x05\x94\x8c\xbd\xf6P\xcd=\x16\x12\x8f\xbd\xa6\xac\x15=\x8b$\x1c\xbd\xcas}=\xca\x92\xc7=\x1a\xb0\x12\xbd\xf0\xeck\xbd\xf03\x0b\xbd7\x89\x02>\'\xabM\xbd\xc5+r\xbc\xd8\xd7\xa6=\xa6\x0c\xb5=\x95\t\xb0=\xd5\xc1\xdb;\xc3\xbb\x86\xbd\x89\xe6\xde=\x87\xc0\x1e\xbd\x80A\xef=\x87\xfb\xc3\xbd\x06k\n\xbd""R8=\x99\xddK\xbd\r\xbf\xf9\xbda\xc9\xd2=\xca\xfb\x17=\xa5\xac\xdb\xbd\x12\xfd9\xbd2\xb4%=\x8c\x93\x92\xbdX\xb3\xa7\xbd\xf2\xef\x97\xbd\xbf\xad\xf0\xbd\x8f\x96\xda=\xe8\xc8\xf6\xbb\xafY\x11\xbd\x9d\x12\x07>\x9fE\xa2<M\xf0(=\x8a\xb1\xdc\xbb\x7f\xaa\xb6\xbd\xb9p\x8f\xbd\xb8\xe8\xb3\xbdA\x87\xbd=w+\xe4=\x8d\xcc\x1d=\x049\xad\xbd\x9f\xa4\xd8\xbd\xf6W\xf2<\x028H\xbc\x90\xb1\x82=r\xe2>\xbc\xf4\x8c\r\xbd4\xf3\xab\xbdd\x95\xde<\xcb`\n\xbe\\\xb38=\x81\xef\xa5\xbdq\xd2\x82\xbd<\xb2\xb9;\xb3\x82\x1a\xbd]\xad#=\x10\xb3\xbc\xbd7\xee\x84\xbd\xf037=\xfd\xaa[\xbd\x86\x92\xbb=\xc6\xcf==\xb6b;\xbd\xec\x01\xcd<g%\xbd=d\x11\xc2<\xca\xf7\xa7\xbd\xc39\x9b\xbd\xfaI\xaa\xbc:\x1a\xf5\xbc\x84F\x1e\xbd%;\x8a\xbd\xb6\xbb\xa9\xbd\xd3\x83\xb1\xbd\xf8\xf3\xdf=\x03~\xeb\xbc\xe1\xe8\xf1=\xe9k\xc2\xbd\xcaa\x91\xbc\xd1\x81\xe9\xbc\x06\xbaK=!0Y\xbdy\xba#=\xefL\xe9=\xea\xd6\xd0\xbb\xf3\r\x00\xbd\x0f\xc4\xf3<bX\xc2<\x8ai\xb3\xbd\x95&3=\xbb\xfbh=\xb9\xe1\xa3={\x06\x04>J/\xcb=\xb0\x8a\xaf\xbd\xd2\x18M\xbbm\xa4\x1b=\x99\xc8\xe8\xbds\x0b-\xbd\x8c\xed@=)\xe0\x8c\xbd\xef\x17\xed=\xdf4\x84\xbd\xb4\x08\x8b\xbc\'m[;\xc85\xbe\xbd\x82_\xd7<UJ{=\xadf\xd5\xbd\xb4F\xdc\xbdS=\x9d\xbd\xb7x\x96\xbc\n6\xc3\xbd\xf0\x8f2\xbd\x0cM\x97=\x9b5^=\xbbW}\xbdE\xdfM=8\xa5\xa5\xbb\xfd\x8a\xe9=r\x91\xc7=\x9d\\\xdc\xbd\xddV\x1c\xbd\xa7\xc7\xc1\xbd(\xad\xd9\xbc\xb8Y\xba\xbd\x1c\x9a?\xbd\x7f3\x8e\xbd\xb7\x11\x1d<\x03\xc5\xc6=#{\xa7\xbc\x94\xcf\xd1=\xb6\xf9\xc9\xbc\xbd!\xd1\xbd\xba8\xce\xbd\xaa/2=\xffm!=\x01\xc9\xd7=n\x852<\x8fU\xbf\xbc\x8b|\xa3<V}\xc0<\xa9\xbd\x9f=j\xbe&<\xb3$8=\xd6 \xb2\xbd\x98\xda\xf3\xbd\xce\xe7\xd9\xbd\xc3\x1e\xee\xbde%K=0\x9fZ=\xac\x85\xd6=&p\xab\xbd\xac{\x00<t7g\xbd{\x01\xd5=\xc4eI=A\xf7a\xbc\x18\x15F\xbd\x7fT\xff=\x08\xf4\x10=#l\xd7\xbd\\p\x9f\xbdxm\x0c\xbd\x8f\x92\x1e;O\xde$=\x8ev\xfb\xba\xc9|\x9e\xbc\xef\x8a\xdc=\xa3\n\xae=\xb9\xe4o\xbd\xfa9\xab\xbd\xa8\x1f\xd9=\x06\x15t\xbd\x9c.y\xbd\xc6BW=\xa2*u=\xda\x07d<q\xeb-=\xd3\xf2\x92=\xd8i^=e\x83\xd2=\x12\xa2\xbc:\xb6\xaa+\xbd.\xe7\xeb=]\x9f\x85=<\t\xa7\xbdbZ\xa3\xbd\xd2Z\x9a\xbc\x07\x87\xc5=\x84y\xb2\xbd\x97\xcc\xe6\xbc\xfb\xe2\x11\xbd\x1bG\xb4<\xa9\xfc\xb9=\xdam\x95=v\xf0\xdb<\x06\x1e\xb5\xbd\xbf\x87\xbd=\xa0\x1dA=\x8e\xfb\xcc\xbb#\x19\x15=Bb\xb2\xbd\xf3\xfd\xf0\xbd\xe8\xb9\xe7\xbc\xd5\x86\xc0=4|\x18\xbd\x82\xed\xfe\xbc\xc1\n\xf0=&6\xdb\xbc:\xe5E\xbd\xbd!\xd6=,\xa5\x89=\xae\xfcg\xbdk\xafT=\x15\x8d|\xbcb@\x81<Z\x00\xd8\xbd\x17@\x88\xbc\xaa\x84\x9c=\xc5\xa4\xeb\xbd\x08AX=\xa5t\xdc=\x8f\xb2\x88=\xda\xe0\xab\xbd\xefi\xeb\xbdJ\x05\x94\xbdD\xd9\xda\xbd\xebs\xb4\xbc\x19\xb9\xaa\xbdU^\xb2=\xae\x8f\x89<\xada^=\x93\x0e\x10=\x9f\xe8\x8a\xbc\xc4\xd0\x86=GR\xac;\x12\xd4\x9d\xbd\x95\xbf\xf3\xbcgk\x01\xbd\xacM\xc6=0\xd9\xec\xbc\xba\x02\x9a\xbc\xcc\xe5\xa8\xbbg\xe51\xbd\xee\x1e\xf1<\x04\xf6\xa0\xbd\xf7\xe5\xf3\xbd\xd3\xb4T\xbc\x7fI\x84=""\xea\x12=f\x89\xf7<\xd9\x00\xb7<\xe7K\xe7=\xb3\xcb\xb7=\xd1\xef\xc6\xbc.\xaf\x18\xbcb\x97\x01>a\xe0b=\xed\xbe\xa3\xbc\xb2a\xa1=e\x19T=NX\x95\xbd\xd3J\x8b=3\xf4\xd0<\xe6I\x96\xbdg4\x80\xbd\xf4\x82\xf7\xbd\xc1\x9d\xd8\xbc\xa1\x0e{\xbd\x8e\xf4\xfe\xbd\xa3\x16\xf3=3\xbc\x8b\xbc\xeb\xac\xb6=\x8f\xec\x99<q\xa2\xe3<\x14\xccA\xbd\x02\x9a\xc8\xbdf\xf0:=\xfe\x89\xc0\xbbj\xbe\xb0\xbc\x8bC\x80\xbc!\'5=\xdb\x83\xb2=\x1b\xae\\\xbd\xf3\xb2\xd5=\xe6i\xa6\xbdSi\x95=A\xb6G=\x1f\x95\x94\xbdf\xac\xf5;\x1b\x9c\xad\xbd:C8\xbc\xab\x95\x88\xbc\xe6\x83\x87\xbd\x92\x13\xee\xbd\x88\x11\xb8\xbd\xb1]\xcc=\x9doM\xbd\xde\xa2)=>\xc2v=\x8a\x97\x06\xbcj\xc8\xc0<\xfd*\x1f\xbc\xa8\xd2#=5|\x15\xbd\x05\x10\xcd\xbc\xf1\xa8\xb6\xbd\x85\x94z\xbd=\xf0x\xbdj[\x92\xbci\x03(=\xfeI~\xbd\xbdh\xa4=\x0e=\x06>\xdd\xf4\xba\xbd\x10\xa6\x9d=j\xe0\x96\xbd\xcb\x03\x91=C\xb3\xb1\xbdE\x8c \xbb0\x14\xee;\xc7\xf9&\xbb\\\xea\xf6\xbd\xdd\x1e\xe5\xbb\xd2\xd6\xa1\xbc\x96k\x06\xbd\xa2\xc5+:\x845\x1b\xbd{\xe0&\xbd\xb1\x83Q=L\xa5\xb4\xbd\xa2\x82\xcf=\x19\'\xce:\x9e\x96\xc8=\xec\xa7\xb4\xbbW\x0f\xc3=\n\x00\x9f\xbc\xb7-\xbc=hj\x98=\xd1s\xb1=d`s=\xbb\xd6 \xbc\xee\xd9\xa3\xbc\x16\x9d\xa7=\xc2\xfa\x05=2\x1b\xff\xba\xca\xd6g=\x92\xec\x92\xbdX\xb5)\xbbB\xb8\xc7\xbd\xacf\x95=A\xbf\xc2\xbc\xb7\xb5\xb7=\xa5\xa2>\xbd\xb2\x04\x9f=\x06m\xbb<\xe2\xc6\t\xbd\xc9\x19\xfd\xbd!\xa1;=~M7\xbc{\xe4\x03\xbc+\xdd\xbd=B\xaf \xbc\x82\x95\xcf;c\tS=\x1eU\xe5\xbd\xff\x86l\xbd\xf3\x06\xa5;w\xd6\x16\xbd\x900\x89<\xd5;?\xbd\x9d[\x98\xbd\xf2\xff\x08=\x84\x92\\;\xe3\xa5\x87\xbc\x84\xf4@=\xe0\x03&\xbb\xb9Q\xbe\xbd\x9d0!=\xc8_\x88\xbd{\xa5\xc8=\xaafw\xbdP\x15\xe5\xbc\x99\x03\x89\xbd\xe1b,==\x7f9\xbd\xccK\xe4\xbdp<\x95=\xf0\xc2\xcd=\xf5:\x88=}FF\xbdF4\x1f=\x80\xe4b\xbd\xec\x93\xc8\xbc\x0e\t\xff;Ga\xd7\xbd<l\xaa\xbc\n\xa9}\xbde\xb9w\xbd\xdb\x90\xb9;\x11\xb7\xab\xbd@A\x02\xbd)\x85_\xbd\x87C*\xbd\xd9\xbc\x85<5r\x9f\xbc_\xfa\xf3\xbdGJ\x00\xbe\xc7\xb6\x97\xbd\xf8M\x08=e?\xd4=\xdf\x9cG\xbb6\x8e7=m\x16\x08=\xe5J\r\xbd\xecS\x1c=\x83\xa5\xc0=\xfc\xe1\xcd\xbd\xba\xde\xa6\xbdy\xd7N\xbd\x98{B\xbc\x1a\xfe\xff\xff\x04\x00\x00\x00\x00\x01\x00\x00\xb6_)\xbc\xe4\x81,\xbc\x9b\xcdv\xbb\xf9\x90\x07=\xee\x96""\xbcH\x177\xbc\t\xb4@\xbc\xe2S\x1b\xbc\xd7kq\xbc:\xd3-\xbc\xbe\r\xc9<b\xbd\x92<\x98\xc5%\xbcE6\x13\xbc\x1d)8\xbc\x94N""\xbc!\xe4P\xbcnDG<\x02\xec-\xbb\xbb\x8c\xd7;&\xaa#\xbc\x1b\xec\x02=\xa33T\xbc5\xf5\r\xbc\x10\xf7Q\xbc\xf2p3\xbc[\xa7\x1d\xbco(\x01\xbcE\xe4B\xbc\xf6O\x0f\xbc\x8fJ\xfb\xbb\x03g\xf6;\r\x98$\xbc\xb9\xb9\x83;\xb6%Y\xbc\x82o\x96\xbb\xa6\xe3N\xbc8O\xf2<{U(\xbcg \xf1\xbbOp#\xbc\x8b&3\xbcnW\xdd;\x00\x00\x00\x00\x9b\x92\'\xbc+a|:\xb4\xb5\x98<\x9e\xc9\x8d\xbc\x06\\\xcd\xbb\xff\xc3\xe2\xbb\xcf\x1e\x10\xbc\xff :\xbc\x8es\xc4\xbb\x1b\xf8#\xbc\x0fD}\xbc\xf6a1\xbc\xbf\xca\x7f\xbcE\xcc\x13=]\xa2\x18\xbc\n\xe40\xbc6\xd8%\xbc\xfew\xc3\xbb\xc8\xb0G\xbc\xec\xfdQ\xbc&\xff\xff\xff\x04\x00\x00\x00\x80\x00\x00\x00\xf6c^=5\xd6\xbc<\xa5j4\xbcosb\xba\x16\xe0C\xbc\r\x179\xbc\x8e|\x1a\xbchlT\xbc\xab\x90\x16\xbc<t\x1e\xbc\xcd\x04h;C-\xfb\xbb\x0c\x90\x84<H,\x18\xbaZ\xa0/\xba\xaef\xfd\xb9W1\x07\xbc\xc0\xa37<\xcf\x19\xf4\xbb\xc8{\x14\xbc\xa7\xbbM\xbc\x1e\xc3\x0f=,\x0c.\xbc\xb1\xe26\xbc)\x87\x13\xbcoA\xf8\xbb\x1b\x03<\xbc\x1519\xbc\xebz\x14\xbcE\x89\x03;\xa9S\xab\xb8\xb5~\x15\xbc\xb2\xff\xff\xff\x04\x00\x00\x00@\x00\x00\x00K\x1bo\xbbKj?\xbb6\xee@;]r(\xbc\'\x83\xc1\xba\x1b*\x84<;\xde\xc8\xbc\xef\xd0\x9c\xbaWt%\xbc\xc8\'\x1b=K\xaa\x0c=*\xd4p<\xa3\x9d\x93\xbcJ0\x83; \x9a.<\xa8\x10h\xbc\x00\x00\x06\x00\x08\x00\x04\x00\x06\x00\x00\x00\x04\x00\x00\x00\xc0\x06\x00\x00\x9a\x07\xc7\xb9\xb4\x1f=\xba;,\xb9\xb8\xa0T\x0b\xb8`\xbb\x1b:/$\xc99?\xc8\x808\xbf2\xc89\xe1x_\xb7\xc3\xcd-:1""\xe59K\x00\xdf\xb9\x0e\xf7\x10\xba\xf8\x13\x1c\xb91]\xd6\xb6\xef\x04\x0e:\x87|\xb8\xb83\xfc\x949H\xfcd9\xab]\xe79Di\xf98\x84\xac!:\xe2\xb4\x85\xb7\x04\x8f\x8a\xb9\xa2\x94\x1e:\xfc\xca\x18:\x18\\r\xb7\x8d\xb1\x039\x8d\x0c(9\x1au@\xba[\xc7a96!^\xb9\x94\xbf\xc1\xb9F\xc0\n:\xf7\x84\xd89\x1fJD\xba&\xe0#\xba\xdcY\x16\xbaE\xf7,:\x18\xc3\x198HM\xeb\xb8a\xa8\x199`\x8c\xca\xb9m\'\x13\xb9\xc4\x05\xf79\xee*\xae\xb86g :rs\'9S\xe8\x1c\xba\xfc}o\xb8\xc3<\xd19\x1a\x0b\x8a\xb9-\xc8\x15:\xbd\xf0\x1d\xba\xf2\'$\xba\xa8\x1al\xb7\xcfZZ9\xb4L\xe09h\x16\xcc\xb9#\x00=9\xaa\xa6\x959Y\x8e6\xb9?\xaa\x9a\xb9?\xd0\xd09*M99\xaa\xac\xb1\xb9\xf3\xa30:o(n9\x8d\x82\n:\xc5\x15$:\x93\xd3""\xba\x11\x16\xfd\xb9\x16:\xca9\x90\xcaq\xb9\x06\xfa%:K{3\xba\x9bK\x8d\xb9\x03K\xbe\xb9\xac\x05\x16\xba\x06\x00)\xba\xf9]\x9d9\x85!\x1f\xba:\x142\xbat\x9a""\xb9\xb0\xd4*\xb9V\x90\x1b:~\xc5v9,e\xf99\xd6X]\xb9\xa2s\x1f\xba7X7\xb9\x8f\xe5\x05\xbar6\xcc\xb7%\xcd\x909x\x98\xa59\xc0\xfd\xb8\xb9\xc16\x1b:\xcbe2:W\xe2\xa39w}\xca\xb9\xf4\n\xd07\x89V""\xba\xfa\xbc\x05\xba\x97\xb6t\xb80,J\xb7\xc4\x00\xf19 M\xcc\xb9\x02%V8\xc7\x18\xce\xb9\x8d\x932:\xc6\xa9X:w!\x04\xbaJ\xbb\xd8\xb9\xba\xe7V:\xa8{\x869\x1c\x06\x0e\xbaC\x8b\xd19s\xc3\xa28\xbbT\n\xba\xe3x\x18:\xbe\xa7\xa3\xb9,\xb5\xae\xb9\x84\xb4\xee\xb9\xe5n\xa19\x9a\xca\xed8\xb9\x13\x869\x0bh8\xb9_\x13\x11\xba\xd7\xcb\xea70_f\xb9\xab\xdd\xd4\xb8\xaa\xfc\x8b9\xb0*#:]E\xc0\xb9\x9e\xa5S\xb9\x8f{\xc09p\xe6@9)M\xe8\xb7\x15\xe8r9:$9\xb9\x0b\xb3\x17\xb9\x9f\xc4\xff\xb8\x82\x03\xf2\xb9I\xa6+:\x13\x82\xf69\xe5}\xf3\xb7\xcf\x1c\xa1\xb90\xbc%:Sx""\xbaA\xf0\xb3\xb9\x11E\xc1\xb8t\xa7L\xb9\xc3M\x1a\xba\xcb1\x1c\xb6J\xf5\xb9\xb9\x80K\xfb\xb9\xa9\xe2\x8f9\xdb\x038:\xf4\xd3\xb48\x99\xb8\xcb9\x9f\xaf\xd4\xb9\x98""#\xb9^\xfc0:d\xfbR\xb8\x8c\x0e\x829C\xd8\xc0\xb97\x909:\x8f\xb4\xfe\xb9\x8d\xa1\xca\xb8\x04L@\xba\xf3\xc9\x9e9O\xd6\xc3\xb8\xb8_\x0c9<\x8a\xe17\xb8\xc7\xad\xb9\xd0b\xcb9\x8f\xd4\x9f9\xaed\x0e:0\xc4\xf59\xc3\x0f\xdd93!\xe78\xa0\xbeS9\x81\x88\xd39\xca\xf0:\xb9\t\x86w\xb9lV\x9f9\x05\x08\xb79\x1c\x03T8[%\xba9M-o\xb8\x9f\xa8\x12\xba\\l\x13:\xf1\x9f{9\x88\xc7\xd99\x140\x05\xbagaF\xb9]\x8d\xfb4\x18e\xa5\xb9\xa4\x87\xe0\xb9\x85\x0cQ7\x8aY\x079\x9f\xd6\xe9\xb9n\r\xe1\xb9Iw*\xb9\x88\xf9m7I\xce!:\x06`\x8c\xb9k\xdf\xc6\xb9\x83\xa9\xd89\xd8T\x8d\xb9;\xf9\xaf9\xf8\x95\xd6\xb9\xfc\x98\r\xba\x97\x19Q9\x83\xa9\xe0\xb8\xbd\xd7\x9f\xb9\xdb\xfaX\xb9\xc1\x10\xb09\xe8\xe7$:\xba\x8dP\xb8\x8b8\xd57\x95\xc1(:$6\xcd\xb8\xf3\xf2\x9d\xb9\x8a\x0e\x07\xba\x1cXP9x\xd1\x83\xb8\xc5!^9h\xe9\xa39\x01m>\xba\x03\xb8\x04\xb9H\xf3/\xba\xaf\xf3\x10\xba\xbe3\xad\xb9%&\xe09\xc7A29\x7f\xb4\x8b\xb8\xeb8\x1e\xbaN&\xe8\xb9D\xb2,\xbaw\xa3r\xb9!\xa5\x85\xb9B\xf6u\xb9\xe7\xcf.\xba\x95#\xd5\xb9Ps\xae8\x0c|\xce9U\x05\xa88g\xe8\x1b\xba\xab\x8e\x83\xb7\x07[\xb6\xb9@\x0e^8\xad\x1b\xad\xb9\xd5\x90""\xba\x99(\xe39a\x01\xea9/=P\xb8\xf9\xde<:P\xcf\x1e:\xcc\xde?\xba\xe4\xf0\xfc\xb9Ho\x1c\xb8I\n\x14\xba\x04\xd2\xa08`\x9f\xec8\xca\x1b\x1c\xba\x9ea\xf39\xc7\xdfM:\xbf\x9f\xc58*\xfc\x12\xb9\xbf\xe1\x84\xb8zF#\xba@\t\':k\xe9\xbd\xb9Z+\xe49\xbe\x11\xce\xb9.\xe8$:\x88:\xf1\xb9\xa3\xef\x10\xba\xcar\xe0\xb9\xa4I;\xb9\xae\xa3Z\xb9x\x0f\x1b:O4\x08\xba:x\x17\xba\xdb! :]{""\xba\xeaC\x9d\xb9\xae\xdc\xbe\xb9?i3\xba /\xc7\xb9\n\xb95:\x95\x01\x0e\xba\xe1M\xa09\xa6C(:\xdcD\xeb9\xf7$\xf58/[%\xbak\xb12\xba\x96r\xc49#D\xd79\x17\xd5<:.\x1cA7\xbf\xc6\xdf\xb6r\xe0\x089\xe7\xc7\x05:\x11\xa4/\xba\xf8\x8c?:\x02B@:ky\x969\xaa\xa6=:J\xad\xae9>H\xa2\xb9sl\x18:\xc6N\n\xbaq~;\xba9\x98}8\xfe\xf9""9\xf3d:\xba\xe3\t\xd49\xf6\xcc\xd0\xb9zs3\xba\\\x02\x0b\xbaS\xa4\x95\xb9G\xa8\xb09\xd7\xdd,:O\xfc\x0c\xba\x94\xc2\xe2\xb9<\x97R9\xa5\xf6\xa1\xb6\xf1\x8b\xe0\xb9\xf1\x89\xd2\xb9+f\xdf8\x12g\x04\xb9\xda\xd2\xda9~\x14\xf37\x8c<$:\xf53\x19:\xfcKx7\xae\x9c9\xba\x1f\x8d\xee\xb8\x0f\x19\x1e\xb9\x12\xa5\t:\xe6\xb0z9\xab&\xa6\xb7\xb5\xbcX\xb9\xa2\x9c\x129\x12G\x80\xb9\x91\x92\xf69\x92\x03\x01\xba\xfb\xfd\x0b:\xb4\xf4f9\xcf\x80\x148M\x0b\xb8\xb9\xeb\xebW8,@\xf8\xb9f\x00\xe78\xc64\xfd\xb9+[\x8c8\xec\xca\xd89\x89\xa6%\xb8\xeff\x13\xba\x11_\xf8\xb9)\xd0\xc29\xda4\x1f\xb9O<\x10:\xe3P\x0c:%f\xb8\xb9\x18\xe6\xfa9\x0b/|9\xd8\x8b\xcb9\x8e_\'\xba\x8f\xb5\xbf91r\xf5\xb9\xdf\xfb)\xbaYb\x17\xb8O=\xdf8Q\xea`\xb9\x88+\x058u\xba\x13\xbaG\x1f\x18981\xd09\xb3\x96\xc29\x99I\x1e\xb8G\xb3\x0e\xba|\x0e\x879\xb06\x049\xb1F,:o\x99@\xba\x1b\xa5#9\xf4\x00\xc7\xb7\xa6\x17Q\xba\x07Hp9Z\xc7&:\xe8\xa4\x02:k\xbd\xb19\xffU 8\x0c:R\xba\xb2\xcc\xe49\xfef\x9c9\xf3p6\xba\xe4\x10\xc1\xb9\x1cq9\xb8$\xa9\x06\xba)w\x189%\x18\x00:\xeat\xc9\xb8\x86+!\xb7\xe2\x10\x06\xbaMx48\x06\xf0\xf2\xb9S\x186\xb9\xdc\xc0\x16:\xb6\xb9O\xb9\n\n2\xbaKP\x8e9\xaf\x88\xc39\xb4\x9e6:\xba\xc8\x1a\xb9\x80\xb5\x0f:\xf1\xb8\xc39\xccy\x1f:\xbc\xbf\x1f\xba\xc8\xaf\xed9\x12\x82\x19:*\xfe\xff8\x10\x02\xfa9-\x11a90}5:\x82\x96\xa49\xf8N\x0f:I\xde:\xb9p\xf3\xff\xfft\xf3\xff\xff\x0f\x00\x00\x00MLIR Converted.\x00\x01\x00\x00\x00\x14\x00\x00\x00\x00\x00\x0e\x00\x18\x00\x14\x00\x10\x00\x0c\x00\x08\x00\x04\x00\x0e\x00\x00\x00\x14\x00\x00\x00\x1c\x00\x00\x00\x94\x02\x00\x00\x98\x02\x00\x00\x9c\x02\x00\x00\x04\x00\x00\x00main\x00\x00\x00\x00\t\x00\x00\x000\x02\x00\x00\xcc\x01\x00\x00|\x01\x00\x008\x01\x00\x00\xf8\x00\x00\x00\xb4\x00\x00\x00\x8c\x00\x00\x00<\x00\x00\x00\x04\x00\x00\x00b\xfe\xff\xff\x14\x00\x00\x00\x00\x00\x00\x08\x10\x00\x00\x00\x14\x00\x00\x00\x03\x00\x00\x00\x08\xf4\xff\xff\x01\x00\x00\x00\x14\x00\x00\x00\x03\x00\x00\x00\x13\x00\x00\x00\x0b\x00\x00\x00\x08\x00\x00\x00\x96\xfe\xff\xff\x1c\x00\x00\x00\x00\x00\x00\x08\x1c\x00\x00\x00 \x00\x00\x00\x03\x00\x00\x00\x00\x00\x06\x00\x08\x00\x07\x00\x06\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\x13\x00\x00\x00\x03\x00\x00\x00\x12\x00\x00\x00\n\x00\x00\x00\t\x00\x00\x00\x00\x00\n\x00\x10\x00\x0c\x00\x08\x00\x04\x00\n\x00\x00\x00\x0c\x00\x00\x00\x10\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x12\x00\x00\x00\x02\x00\x00\x00\x11\x00\x00\x00\x07\x00\x00\x00\x06\xff\xff\xff\x14\x00\x00\x00\x00\x00\x00\x05$\x00\x00\x00(\x00\x00\x00\x01\x00\x00\x00\xf6\xfe\xff\xff\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\x11\x00\x00\x00\x01\x00\x00\x00\x10\x00\x00\x00\xe6\xfe\xff\xff\x10\x00\x00\x00\x00\x00\x00\x01\x18\x00\x00\x00\x1c\x00\x00\x00\xd8\xfe\xff\xff\x00\x00\x00\x01\x01\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x10\x00\x00\x00\x03\x00\x00\x00\x0f\x00\x00\x00\x06\x00\x00\x00\x04\x00\x00\x00\x82\xff\xff\xff\x14\x00\x00\x00\x00\x00\x00\x05$\x00\x00\x00(\x00\x00\x00\x01\x00\x00\x00r\xff\xff\xff\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\x0f\x00\x00\x00\x01\x00\x00\x00\x0e\x00\x00\x00b\xff\xff\xff\x10\x00\x00\x00\x00\x00\x00\x01\x18\x00\x00\x00\x1c\x00\x00\x00T\xff\xff\xff\x00\x00\x00\x01\x01\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x0e\x00\x00\x00\x03\x00\x00\x00\r\x00\x00\x00\x05\x00\x00\x00\x03\x00\x00\x00\x00\x00\x0e\x00\x1a\x00\x14\x00\x10\x00\x0c\x00\x0b\x00\x04\x00\x0e\x00\x00\x00$\x00\x00\x00\x00\x00\x00\x054\x00\x00\x008\x00\x00\x00\x01\x00\x00\x00\x00\x00\x0e\x00\x18\x00\x17\x00\x10\x00\x0c\x00\x08\x00\x04\x00\x0e\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\r\x00\x00\x00\x01\x00\x00\x00\x0c\x00\x00\x00\x00\x00\x0e\x00\x14\x00\x00\x00\x10\x00\x0c\x00\x0b\x00\x04\x00\x0e\x00\x00\x00\x1c\x00\x00\x00\x00\x00\x00\x01$\x00\x00\x00(\x00\x00\x00\x0c\x00\x10\x00\x00\x00\x0c\x00\x08\x00\x07\x00\x0c\x00\x00\x00\x00\x00\x00\x01\x01\x00\x00\x00\x01\x00\x00\x00\x01\x00\x00\x00\x0c\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x14\x00\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x15\x00\x00\x00x\t\x00\x00\xac\x08\x00\x00<\x08\x00\x00\xe0\x07\x00\x00\x84\x07\x00\x00,\x07\x00\x00\xd4\x06\x00\x00\x88\x06\x00\x00\x18\x06\x00\x00\xc0\x05\x00\x00t\x05\x00\x00(\x05\x00\x00\\\x04\x00\x00\xe8\x03\x00\x00\x14\x03\x00\x00\x9c\x02\x00\x00\xc8\x01\x00\x00P\x01\x00\x00\xf0\x00\x00\x00`\x00\x00\x00\x04\x00\x00\x00\xf2\xf6\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00\x1c\x00\x00\x00\x1c\x00\x00\x00\x15\x00\x00\x004\x00\x00\x00\x02\x00\x00\x00\xff\xff\xff\xff\x0b\x00\x00\x00\xd4\xf6\xff\xff\x19\x00\x00\x00StatefulPartitionedCall:0\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x0b\x00\x00\x00J\xf7\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00\x1c\x00\x00\x00\x1c\x00\x00\x00\x14\x00\x00\x00h\x00\x00\x00\x02\x00\x00\x00\xff\xff\xff\xff\x80\x00\x00\x00,\xf7\xff\xffL\x00\x00\x00sequential_1/dense/MatMul;sequential_1/dense/Relu;sequential_1/dense/BiasAdd\x00\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x80\x00\x00\x00\xd6\xf7\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00\x1c\x00\x00\x00\x1c\x00\x00\x00\x13\x00\x00\x008\x00\x00\x00\x02\x00\x00\x00\xff\xff\xff\xff\x00y\x00\x00\xb8\xf7\xff\xff\x1c\x00\x00\x00sequential_1/flatten/Reshape\x00\x00\x00\x00\x02\x00\x00\x00\x01\x00\x00\x00\x00y\x00\x002\xf8\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\x12\x00\x00\x00H\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xff\x16\x00\x00\x00\x16\x00\x00\x00@\x00\x00\x00\x1c\xf8\xff\xff$\x00\x00\x00sequential_1/max_pooling2d_2/MaxPool\x00\x00\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00\x16\x00\x00\x00\x16\x00\x00\x00@\x00\x00\x00\xa6\xf8\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\x11\x00\x00\x00\xa4\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xff-\x00\x00\x00-\x00\x00\x00@\x00\x00\x00\x90\xf8\xff\xff\x82\x00\x00\x00sequential_1/conv2d_2/Relu;sequential_1/conv2d_2/BiasAdd;sequential_1/conv2d_2/Conv2D;sequential_1/conv2d_2/BiasAdd/ReadVariableOp\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00-\x00\x00\x00-\x00\x00\x00@\x00\x00\x00v\xf9\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\x10\x00\x00\x00H\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xff-\x00\x00\x00-\x00\x00\x00 \x00\x00\x00`\xf9\xff\xff$\x00\x00\x00sequential_1/max_pooling2d_1/MaxPool\x00\x00\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00-\x00\x00\x00-\x00\x00\x00 \x00\x00\x00\xea\xf9\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\x0f\x00\x00\x00\xa4\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xffZ\x00\x00\x00Z\x00\x00\x00 \x00\x00\x00\xd4\xf9\xff\xff\x82\x00\x00\x00sequential_1/conv2d_1/Relu;sequential_1/conv2d_1/BiasAdd;sequential_1/conv2d_1/Conv2D;sequential_1/conv2d_1/BiasAdd/ReadVariableOp\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00Z\x00\x00\x00Z\x00\x00\x00 \x00\x00\x00\xba\xfa\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\x0e\x00\x00\x00D\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xffZ\x00\x00\x00Z\x00\x00\x00\x10\x00\x00\x00\xa4\xfa\xff\xff""\x00\x00\x00sequential_1/max_pooling2d/MaxPool\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00Z\x00\x00\x00Z\x00\x00\x00\x10\x00\x00\x00*\xfb\xff\xff\x00\x00\x00\x01\x14\x00\x00\x00$\x00\x00\x00$\x00\x00\x00\r\x00\x00\x00\x9c\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xff\xb4\x00\x00\x00\xb4\x00\x00\x00\x10\x00\x00\x00\x14\xfb\xff\xff{\x00\x00\x00sequential_1/conv2d/Relu;sequential_1/conv2d/BiasAdd;sequential_1/conv2d/Conv2D;sequential_1/conv2d/BiasAdd/ReadVariableOp1\x00\x04\x00\x00\x00\x01\x00\x00\x00\xb4\x00\x00\x00\xb4\x00\x00\x00\x10\x00\x00\x00\xba\xfc\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x0c\x00\x00\x00(\x00\x00\x00\xc4\xfb\xff\xff\x1b\x00\x00\x00sequential_1/dense_1/MatMul\x00\x02\x00\x00\x00\x0b\x00\x00\x00\x80\x00\x00\x00\x02\xfd\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x0b\x00\x00\x00(\x00\x00\x00\x0c\xfc\xff\xff\x19\x00\x00\x00sequential_1/dense/MatMul\x00\x00\x00\x02\x00\x00\x00\x80\x00\x00\x00\x00y\x00\x00J\xfd\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\n\x00\x00\x008\x00\x00\x00T\xfc\xff\xff)\x00\x00\x00sequential_1/dense/BiasAdd/ReadVariableOp\x00\x00\x00\x01\x00\x00\x00\x80\x00\x00\x00\x9e\xfd\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\t\x00\x00\x008\x00\x00\x00\xa8\xfc\xff\xff+\x00\x00\x00sequential_1/dense_1/BiasAdd/ReadVariableOp\x00\x01\x00\x00\x00\x0b\x00\x00\x00\x00\x00\x16\x00\x1c\x00\x18\x00\x17\x00\x10\x00\x0c\x00\x08\x00\x00\x00\x00\x00\x00\x00\x07\x00\x16\x00\x00\x00\x00\x00\x00\x01\x14\x00\x00\x00\x14\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x02(\x00\x00\x00\x18\xfd\xff\xff\x1a\x00\x00\x00sequential_1/flatten/Const\x00\x00\x01\x00\x00\x00\x02\x00\x00\x00R\xfe\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x07\x00\x00\x00,\x00\x00\x00\\\xfd\xff\xff\x1c\x00\x00\x00sequential_1/conv2d_2/Conv2D\x00\x00\x00\x00\x04\x00\x00\x00@\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00 \x00\x00\x00\xa6\xfe\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x06\x00\x00\x00,\x00\x00\x00\xb0\xfd\xff\xff\x1c\x00\x00\x00sequential_1/conv2d_1/Conv2D\x00\x00\x00\x00\x04\x00\x00\x00 \x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x10\x00\x00\x00\xfa\xfe\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x05\x00\x00\x00<\x00\x00\x00\x04\xfe\xff\xff,\x00\x00\x00sequential_1/conv2d_2/BiasAdd/ReadVariableOp\x00\x00\x00\x00\x01\x00\x00\x00@\x00\x00\x00R\xff\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x04\x00\x00\x00<\x00\x00\x00\\\xfe\xff\xff,\x00\x00\x00sequential_1/conv2d_1/BiasAdd/ReadVariableOp\x00\x00\x00\x00\x01\x00\x00\x00 \x00\x00\x00\xaa\xff\xff\xff\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x03\x00\x00\x008\x00\x00\x00\xb4\xfe\xff\xff*\x00\x00\x00sequential_1/conv2d/BiasAdd/ReadVariableOp\x00\x00\x01\x00\x00\x00\x10\x00\x00\x00\x00\x00\x16\x00\x18\x00\x14\x00\x00\x00\x10\x00\x0c\x00\x08\x00\x00\x00\x00\x00\x00\x00\x07\x00\x16\x00\x00\x00\x00\x00\x00\x01\x10\x00\x00\x00\x10\x00\x00\x00\x02\x00\x00\x00\x88\x00\x00\x00 \xff\xff\xffz\x00\x00\x00sequential_1/conv2d/Relu;sequential_1/conv2d/BiasAdd;sequential_1/conv2d/Conv2D;sequential_1/conv2d/BiasAdd/ReadVariableOp\x00\x00\x04\x00\x00\x00\x10\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x03\x00\x00\x00\x00\x00\x16\x00\x1c\x00\x18\x00\x00\x00\x14\x00\x10\x00\x0c\x00\x00\x00\x00\x00\x08\x00\x07\x00\x16\x00\x00\x00\x00\x00\x00\x01\x14\x00\x00\x00(\x00\x00\x00(\x00\x00\x00\x01\x00\x00\x00H\x00\x00\x00\x04\x00\x00\x00\xff\xff\xff\xff\xb4\x00\x00\x00\xb4\x00\x00\x00\x03\x00\x00\x00\x04\x00\x04\x00\x04\x00\x00\x00""\x00\x00\x00serving_default_sequential_input:0\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00\xb4\x00\x00\x00\xb4\x00\x00\x00\x03\x00\x00\x00\x04\x00\x00\x00@\x00\x00\x00$\x00\x00\x00\x14\x00\x00\x00\x04\x00\x00\x00\xdc\xff\xff\xff\t\x00\x00\x00\x00\x00\x00\t\xe8\xff\xff\xff\x16\x00\x00\x00\x00\x00\x00\x16\xf4\xff\xff\xff\x11\x00\x00\x00\x00\x00\x00\x11\x0c\x00\x0c\x00\x0b\x00\x00\x00\x00\x00\x04\x00\x0c\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x03'', does not exist.
```
",True,"[-6.78271592e-01 -3.58006209e-01 -2.58533694e-02  2.98638791e-01
  4.35446411e-01 -4.07672167e-01 -1.95701286e-01  5.67830317e-02
 -4.00810242e-01 -2.54202843e-01  2.10587293e-01 -1.16945237e-01
 -1.35941505e-01  5.32536395e-02 -1.53600216e-01  2.78435290e-01
 -2.12107942e-01 -2.06740469e-01  2.90282071e-01  1.60421237e-01
 -2.24098310e-01  9.70871896e-02 -1.75095305e-01  7.09879100e-02
  1.98371127e-01  1.79230466e-01 -4.15659785e-01  2.35404037e-02
 -4.30246480e-02  2.48647586e-01  5.18156111e-01 -5.19913249e-02
 -2.23149121e-01  1.83892176e-01  2.76876867e-01  2.41608053e-01
 -4.20588106e-02 -1.32816568e-01 -3.26166630e-01  3.36909592e-02
  3.32468413e-02  1.03348315e-01  1.91027686e-01 -2.59529471e-01
  1.75649196e-01 -2.57688999e-01 -8.64485502e-02 -2.82608151e-01
 -2.86559667e-02 -8.20176899e-02 -1.37225121e-01  1.20278299e-01
 -5.84999681e-01 -3.19097310e-01 -3.20643723e-01 -1.11170411e-01
 -1.67290159e-02 -3.56087200e-02 -1.68111861e-01  2.84223557e-01
  3.13434377e-02 -1.60682455e-01  9.25836563e-02 -1.53532639e-01
  2.51491740e-03  4.74594980e-02  3.59482229e-01  4.81208973e-02
  5.21901727e-01 -2.97464132e-01  4.24257219e-02  9.26419348e-02
 -4.83437270e-01  1.27118081e-01  1.52995050e-01  9.52046514e-02
 -1.50465816e-01  1.96338207e-01  1.78698584e-01 -2.94819713e-01
  2.24819407e-03 -2.40568176e-01 -4.74916846e-02 -3.70515555e-01
  9.15994495e-03 -1.54620975e-01  4.38470989e-01  1.11803196e-01
  3.67283285e-01 -1.88211024e-01  3.69189680e-01  5.06149709e-01
 -2.46838592e-02  2.77341217e-01  5.29153943e-01  1.52475566e-01
  2.96537541e-02  2.05482170e-01 -7.59752654e-03 -2.34131247e-01
 -2.39800930e-01 -1.66201591e-01  1.70124695e-02  2.42306888e-02
 -1.16433918e-01 -1.71157569e-01  1.41771376e-01  3.06568071e-02
  1.73229009e-01 -1.81405246e-01  1.78869098e-01 -2.64627635e-02
  5.70678897e-03 -2.53639758e-01 -6.34209812e-02  5.46111986e-02
 -1.49342716e-01 -1.10076115e-01 -7.52536356e-02  6.87166393e-01
  2.64976397e-02 -2.06642509e-01 -1.10324189e-01  1.59239292e-01
  5.06791472e-01  9.55579337e-04  2.70804279e-02  7.42567796e-03
 -3.23375687e-02 -2.51929671e-01  1.53534412e-01  4.20422405e-02
  4.54544611e-02  2.27132142e-01 -1.68646216e-01 -1.50334844e-02
 -2.01380491e-01 -1.04016714e-01 -3.01802039e-01 -3.93044859e-01
 -2.53321826e-01  1.68974757e-01 -1.88459158e-01 -7.06291795e-01
  7.70568848e-02  2.37779468e-02 -2.12830052e-01  1.73670799e-01
 -3.19146693e-01  2.33582750e-01  1.07868075e-01  4.71963249e-02
 -2.22467378e-01  5.31126320e-01  8.39685351e-02  1.13812417e-01
  3.96852612e-01 -1.17525399e-01  1.05662063e-01 -6.30757689e-01
  1.83443397e-01  3.62774014e-01 -2.11010829e-01 -1.86260521e-01
  8.18859600e-03  7.68172890e-02 -4.24030423e-01 -1.83695287e-01
 -1.88972522e-02  4.00444418e-01 -2.90421188e-01 -1.10135064e-01
 -1.42471492e-03 -3.69191915e-02  1.96359187e-01  6.03518523e-02
  3.08095545e-01 -4.83349890e-01 -2.79608101e-01  4.65571254e-01
  2.29911774e-01  1.65173784e-01 -7.92877600e-02  1.83331519e-01
 -3.92652936e-02 -1.80046409e-02 -9.71702710e-02  2.29069829e-01
  2.21151933e-02 -5.73092178e-02 -4.85958070e-01 -1.84338391e-01
  4.96278793e-01 -1.00920588e-01  1.63081169e-01  5.46073690e-02
  2.97592342e-01 -7.68803060e-02  1.62861258e-01 -1.60645857e-01
 -1.45423144e-01 -6.09827116e-02 -7.59894773e-02  1.28213227e-01
  2.28339583e-01 -3.01737547e-01 -3.24919224e-02 -2.29170918e-01
 -3.72393191e-01 -2.18848065e-02 -1.07656397e-01 -5.90472102e-01
  2.03627050e-01 -3.50464173e-02 -3.99955690e-01  4.07824278e-01
  1.20083898e-01  1.08650312e-01 -7.18939677e-02  2.68815160e-01
  1.58789203e-01 -2.28393435e-01  1.41431153e-01 -4.25998062e-01
 -9.03379917e-03  4.42153990e-01 -2.93076158e-01  1.87115714e-01
  2.82640345e-02  1.53334469e-01  3.69679444e-02  5.46704512e-03
  4.87135500e-01  3.21178198e-01  3.72689307e-01 -2.68948823e-01
 -1.72981143e-01 -1.61694825e-01  1.30399223e-02  1.42366976e-01
 -6.30016923e-01 -4.87324655e-01 -3.34331132e-02 -4.21405882e-02
  3.51783037e-01  4.04270709e-01 -8.01253319e-02 -5.64217716e-02
 -5.00204206e-01  5.07394448e-02 -3.03393036e-01  1.57178953e-01
  4.06030178e-01  3.33333202e-02  5.22358716e-01  3.12916428e-01
  1.57402247e-01  7.53453001e-02  3.15412521e-01 -3.54911268e-01
  4.90492284e-01  2.78029323e-01  2.10690245e-01  3.38549763e-01
  3.55086684e-01  2.28403762e-01 -4.87569124e-01  4.22483981e-01
  1.02039836e-01 -6.79769367e-02  2.04833373e-01 -4.71327752e-01
  4.73674715e-01 -3.07838857e-01  2.73716524e-02  1.62221849e-01
  3.98112714e-01  2.64557153e-02  2.06842601e-01  1.19088173e-01
  8.30421038e-03  4.19610381e-01 -5.12339354e-01  7.82100111e-03
  1.34135652e-02 -1.90602720e-01 -1.51174486e-01 -6.04970515e-01
 -3.36038291e-01  1.92836881e-01 -7.06059188e-02 -4.24642004e-02
  2.75861789e-02  6.45888299e-02 -1.57023519e-01  6.04013167e-03
  5.10406904e-02 -3.98674905e-01  1.85448527e-01  3.44303012e-01
 -1.82942465e-01  5.63047454e-02  5.56790233e-01 -3.45305145e-01
 -1.74327880e-01 -1.19851148e-02  4.10337150e-01  2.93622166e-01
  4.89631921e-01 -4.70633566e-01  1.66136369e-01 -5.00744209e-02
 -9.91975293e-02  6.02421284e-01 -5.96279800e-02 -9.58470479e-02
 -3.38552296e-01  8.33081067e-01  1.60091911e-02 -1.52229026e-01
  2.17218637e-01 -1.95843130e-01 -3.11831892e-01  1.39201760e-01
  3.99807662e-01 -1.11484349e-01 -9.11435336e-02 -2.40739807e-01
  8.18792954e-02  3.58367920e-01  1.19699668e-02 -9.55287665e-02
 -1.15663894e-01 -7.03415275e-02 -2.65961617e-01 -1.98075980e-01
 -4.87712562e-01  2.87303329e-01  8.32754821e-02 -3.42364609e-01
 -1.00120030e-01 -3.61090899e-02  5.07665016e-02  2.34239157e-02
  1.80808023e-01 -3.77888620e-01  4.90273476e-01  4.66342747e-01
 -1.99620515e-01  1.99290663e-01  1.77349180e-01  7.96250105e-02
 -6.02023125e-01 -8.73730183e-02 -1.51541978e-02  1.52535290e-01
  2.42802352e-01 -1.89041287e-01  2.85772800e-01  3.99790853e-01
 -2.83204969e-02  2.97968447e-01 -1.86450273e-01  1.50455907e-03
  2.30535597e-01 -2.77289748e-01 -2.24546596e-01 -3.05762410e-01
  1.74998552e-01  8.93686861e-02 -2.13464350e-01  7.08287805e-02
 -4.70252812e-01  3.73813391e-01  4.64522272e-01 -5.38743734e-01
 -2.89565861e-01  2.78319299e-01  1.45465791e-01  1.09793603e-01
  2.09066905e-02 -1.04047403e-01  2.75915980e-01 -6.92056492e-04]"
ValueError: Unable to create dataset (name already exists) type:bug comp:apis TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Epoch 1/10
1/1 [==============================] - ETA: 0s - loss: 6.8405 - accuracy: 0.3250
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-57-865e41f45523> in <cell line: 1>()
----> 1 transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds,batch_size=BATCH_SIZE,callbacks=[early_stop, checkpoint_call, plot_losses])

2 frames
/usr/local/lib/python3.10/dist-packages/h5py/_hl/dataset.py in make_new_dset(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)
    161         sid = h5s.create_simple(shape, maxshape)
    162 
--> 163     dset_id = h5d.create(parent.id, name, tid, sid, dcpl=dcpl, dapl=dapl)
    164 
    165     if (data is not None) and (not isinstance(data, Empty)):

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/h5d.pyx in h5py.h5d.create()

ValueError: Unable to create dataset (name already exists)


i want to save each epoch as a checkpoint two weeks back back without any any error each checkpoint will save as a checkpoint but suddenly now getting error

### Standalone code to reproduce the issue

```shell
import matplotlib.pyplot as plt

from tensorflow.keras.callbacks import Callback


import os

checkpoint_dir = '/model/checkpoints_m_1'

if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)
from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss', patience=3)
# Set up the model checkpoint callback
checkpoint_call = ModelCheckpoint(filepath=checkpoint_dir+""/checkpoint_{epoch}.hdf5"",
                                  monitor='val_loss',
                                  save_best_only=True,
                                  save_weights_only=False,
                                  mode='min',
                                  save_freq='epoch')
transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds,batch_size=BATCH_SIZE,callbacks=[early_stop, checkpoint_call)
```


### Relevant log output

_No response_",True,"[-4.32067066e-01 -6.02935016e-01 -1.81928456e-01  1.44139275e-01
  2.63121754e-01 -5.27994275e-01 -1.94171250e-01 -3.91887762e-02
 -4.77266014e-01 -1.84006453e-01  8.09832066e-02 -2.84361213e-01
 -1.62642092e-01  5.13838120e-02 -2.09956974e-01  4.35161352e-01
 -2.01011062e-01 -2.09317952e-01  2.90341318e-01  4.13749255e-02
 -1.73527852e-01 -7.15038776e-02 -3.65101814e-01  1.46821916e-01
  7.62263462e-02  9.01927277e-02 -3.11520278e-01 -5.47904149e-02
  3.51478718e-03  2.77624786e-01  4.18023735e-01  1.18545815e-02
 -1.41414881e-01  2.39077911e-01  1.63976640e-01  3.30889344e-01
 -3.42395157e-01 -1.17551729e-01 -3.60629082e-01 -1.08036697e-02
 -8.03056359e-02  1.19400628e-01  1.75956070e-01 -1.59904063e-01
  6.34485483e-02 -2.20357418e-01 -1.03010237e-01 -3.79645079e-02
 -1.57469779e-01 -1.15294971e-01  4.76990640e-03  5.75761199e-02
 -6.66553676e-01 -3.79819661e-01 -3.13272029e-01 -1.09645545e-01
 -6.58319443e-02  8.24481547e-02 -2.70910747e-03  2.12896928e-01
  2.53331773e-02 -2.64280438e-02  2.12553754e-01 -3.09779793e-02
  1.44090369e-01  1.44151032e-01  2.54780889e-01 -4.89417985e-02
  5.79935789e-01 -2.44195968e-01  1.92791805e-01  2.28352249e-02
 -3.32727909e-01  1.09871738e-01  3.79238538e-02  3.14745307e-02
 -6.95905760e-02  2.77166605e-01  2.44266003e-01 -2.12932482e-01
 -9.50175822e-02 -2.83943210e-02  4.45142016e-02 -2.24268422e-01
  2.05310583e-01 -1.70263305e-01  3.80480766e-01  1.00082785e-01
  3.96495640e-01 -1.71767384e-01  4.44606781e-01  2.82066733e-01
 -1.12006113e-01  2.78193891e-01  3.25798273e-01  2.29682148e-01
  1.41592234e-01  2.86406934e-01  4.91009653e-02 -9.19334143e-02
 -1.12417266e-01 -3.18183422e-01 -1.20948851e-01  1.82640225e-01
 -8.68183188e-03 -4.04796489e-02  1.74156591e-01 -1.24213971e-01
  1.95667475e-01 -3.54399905e-02  7.62537643e-02  6.79547936e-02
  2.03734487e-01 -3.14424783e-01  6.41441941e-02 -1.12685896e-01
 -1.77079573e-01 -8.20533186e-02 -1.00295253e-01  6.44042492e-01
  9.87877399e-02 -2.06324793e-02  9.18276422e-03  3.76455858e-02
  5.89284718e-01  2.57510096e-01 -1.03955761e-01  5.34165278e-03
  1.66667908e-01 -8.94672275e-02  2.60064125e-01  1.39827028e-01
  4.62588742e-02  2.59941161e-01 -5.60149178e-02  1.78558454e-01
 -4.36637133e-01 -1.43409446e-01 -2.20578045e-01 -3.76962334e-01
 -1.73518181e-01  1.99448645e-01 -1.93866380e-02 -6.52836442e-01
  2.37099752e-01  9.62060019e-02 -1.60280675e-01  2.94980824e-01
 -3.94885063e-01 -7.48217106e-04 -2.24059932e-02  2.57509261e-01
 -2.25968540e-01  5.39309144e-01  6.60548359e-02  3.16232383e-01
  5.11782765e-01 -1.65796906e-01  3.17470916e-02 -7.11824834e-01
  2.72977576e-02  4.03585792e-01 -3.06933343e-01 -3.06996644e-01
  1.34410366e-01  1.13809966e-01 -4.04889166e-01 -2.07067639e-01
  2.23457888e-01  3.45143676e-01 -9.28397700e-02 -1.21870108e-01
 -2.58357357e-02  7.27894902e-02  1.51860505e-01  7.51011968e-02
  2.56896913e-01 -6.51644886e-01 -1.60823375e-01  5.16265869e-01
  3.50721478e-01  1.52123809e-01  5.10929972e-02  1.87684193e-01
  1.01091482e-01 -7.62076527e-02  9.51763242e-02  2.70638555e-01
 -1.28078163e-01 -8.03157017e-02 -3.43686342e-01  2.36975737e-02
  5.54761529e-01 -1.84096634e-01 -1.20106712e-01  3.23875137e-02
  2.67996460e-01 -3.12481403e-01  1.16627038e-01  2.41826531e-02
 -1.55559763e-01 -1.62502676e-02 -1.36746407e-01  4.80138958e-02
  8.80217254e-02 -2.69269764e-01 -1.55499607e-01 -3.28533918e-01
 -3.83256674e-01  1.83100015e-01 -1.44989014e-01 -4.83243406e-01
  1.86654180e-01 -1.98594779e-01 -2.79017121e-01  1.16778746e-01
  3.09524655e-01  1.44950435e-01 -3.12532127e-01  1.25444591e-01
  1.25665277e-01 -1.89017743e-01  2.06200108e-02 -4.95117098e-01
 -3.52645516e-02  2.90927649e-01 -4.87084180e-01  5.51042333e-03
  1.39263898e-01  1.55389428e-01  1.27320379e-01  4.68628034e-02
  2.83808887e-01  3.69682431e-01  3.55504751e-01 -3.75939235e-02
  2.79158391e-02 -4.09183800e-01 -1.21424451e-01  2.19186172e-01
 -5.07711589e-01 -8.85589048e-02 -7.04735219e-02 -1.13450870e-01
  2.23012090e-01  4.00616705e-01 -2.66940087e-01 -2.12477118e-01
 -5.70580900e-01  2.42327362e-01 -2.01078042e-01  1.80043921e-01
  3.32184851e-01  6.30450621e-02  4.49737638e-01  3.00787598e-01
  3.39782447e-01  1.91469759e-01  2.47242615e-01 -3.57283115e-01
  4.09553528e-01 -1.34485900e-01  1.08112305e-01  3.59196246e-01
  2.01286644e-01  2.86069036e-01 -3.36448729e-01  4.99662310e-01
  1.70303509e-01 -1.22342989e-01  1.82349369e-01 -1.92785263e-01
  6.38057590e-01 -2.90495396e-01 -4.17186916e-02 -6.25890344e-02
  3.95632029e-01  2.96365265e-02  9.09809470e-02 -3.05051692e-02
 -6.96167350e-02  2.62397766e-01 -2.67673850e-01  1.11158222e-01
  2.26617306e-02 -9.85364616e-02 -8.63247737e-02 -4.65323478e-01
 -3.70550811e-01 -1.06077949e-02 -3.22660625e-01  2.96543360e-01
  1.15479976e-01 -6.10961020e-02 -2.54594624e-01 -4.08404320e-02
  1.00841351e-01 -1.18505940e-01  6.62757643e-03  1.90158844e-01
 -1.50403515e-01 -4.39295396e-02  2.74779201e-01 -4.20696110e-01
 -1.18508756e-01 -1.22848436e-01  6.10941112e-01  2.09397644e-01
  5.29365063e-01 -5.21523476e-01  1.42391026e-01 -7.72949457e-02
 -2.04471767e-01  7.22754061e-01 -1.04340523e-01 -8.43927264e-02
 -3.37386191e-01  7.15217113e-01  3.14512402e-01 -1.70364946e-01
  6.44009933e-02 -2.19078779e-01 -4.27534163e-01 -2.78086513e-02
  2.92306721e-01 -3.41280997e-02 -7.88804963e-02 -1.93639278e-01
  2.66750399e-02  2.01066643e-01 -1.67425871e-01 -5.20327538e-02
 -2.45500624e-01  8.95102471e-02 -2.41198152e-01 -1.74369350e-01
 -4.43102837e-01  2.23542243e-01  5.88367879e-02 -1.93990588e-01
 -6.80413246e-02 -4.95873690e-02 -9.76017267e-02 -2.49459416e-01
  6.11074902e-02 -3.69725645e-01  2.84614682e-01  7.02869654e-01
 -4.08281326e-01  1.81146264e-01  6.74762279e-02  5.95025457e-02
 -4.01670933e-01  1.11277485e-02  9.35714617e-02  2.74397314e-01
 -9.59451795e-02 -1.19773582e-01  4.24744040e-01  2.26548240e-01
 -2.67226607e-01  2.51973838e-01 -1.98508397e-01 -5.56023046e-02
  1.70023724e-01 -1.52958184e-01 -4.69016999e-01 -1.31410331e-01
  3.08294177e-01  2.43784800e-01 -3.73686433e-01  1.74326941e-01
 -3.30168426e-01  2.83261061e-01  5.93772173e-01 -3.21590126e-01
 -2.63519257e-01  3.08814883e-01  4.46037948e-01 -3.13257277e-02
  1.10065699e-01 -6.36015087e-04  2.88657427e-01 -1.20397180e-01]"
tf.data.Dataset.list_files(): You must feed a value for placeholder tensor type:bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

No

### OS platform and distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Reading a dataset obtained with `tf.data.Dataset.list_files()` prints incomprehensible warnings.

Create two files:
```bash
touch a.txt
touch b.txt
```
Run this python program:
```python
import tensorflow as tf
dataset = tf.data.Dataset.list_files(['a.txt', 'b.txt'])
for f in dataset: 
    print(f)
```

Prints some incomprehensible warnings:
```
tf.Tensor(b'b.txt', shape=(), dtype=string)
tf.Tensor(b'a.txt', shape=(), dtype=string)

2023-09-07 17:19:04.634978: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]
	 [[{{node Placeholder/_0}}]]
2023-09-07 17:19:04.635273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]
	 [[{{node Placeholder/_0}}]]
```
This is a ~duplicate of https://github.com/tensorflow/tensorflow/issues/41648 that was marked as resolved 3 years ago.

### Standalone code to reproduce the issue

```shell
With tensorflow==2.12.0:  
https://colab.research.google.com/drive/1_kjUH6BzcLnlM4rc8mY7JcnhE5NdaRGy?usp=sharing

No warning with tensorflow==2.11.1  
https://colab.research.google.com/drive/1QhatrE7hdJIxIUAIrYw5yDPQ50SeqFrI?usp=sharing
```


### Relevant log output

```shell
2023-09-07 17:19:04.635273: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]
	 [[{{node Placeholder/_0}}]]
```
",True,"[-0.3716026  -0.730747   -0.166192    0.19890994  0.3415628  -0.45843863
 -0.07124343 -0.05560212 -0.35648227 -0.24306616  0.10729447 -0.11137544
 -0.33130404  0.26331705 -0.28654706  0.33895454 -0.01996631 -0.13957626
  0.17574272  0.02769981 -0.19919822 -0.24873671 -0.38534826  0.24066603
  0.11405586  0.23404393 -0.30882287 -0.16393596  0.0539728   0.15249932
  0.20613304  0.09140088  0.0087952   0.10201187 -0.04016875  0.35402292
 -0.14896815 -0.20816424 -0.29738718 -0.08848535  0.06746966 -0.0179157
  0.14302464 -0.2048099   0.03759494 -0.25266394  0.19954473 -0.1210136
 -0.15971904 -0.03914136 -0.12335788  0.00324297 -0.58681977 -0.29452175
 -0.10290193 -0.02757254  0.23350671  0.04347415  0.06718944  0.20098853
  0.03893692 -0.07460237  0.11537328  0.07805412  0.15883906  0.2488715
  0.27652806 -0.11952848  0.49461287 -0.2791797   0.26123753  0.08700176
 -0.29686242  0.08839624 -0.03553031  0.13957097  0.09607994  0.16681114
  0.26739648 -0.0512868  -0.0479903  -0.14870515  0.09350993 -0.17378801
  0.01226922 -0.18198644  0.35130715  0.20751026  0.38407603 -0.30624008
  0.7219593   0.4476153   0.04459121  0.06813699  0.3776048   0.07576855
  0.17536408  0.21533799  0.05632108 -0.08028528 -0.21154499 -0.32123625
 -0.02611594  0.20818256 -0.01072273 -0.0237719   0.0864121  -0.1009252
  0.1232752   0.04779221  0.04738127  0.09847748  0.13214539 -0.17589626
 -0.17759831 -0.29694235 -0.21617748 -0.00200405  0.16631424  0.6282015
  0.06778216 -0.2388719   0.05165309  0.11814707  0.5823067   0.15680875
 -0.19233876  0.0704603   0.06367188 -0.03371534  0.25336015  0.12289412
 -0.08955725  0.06657422 -0.12491097 -0.00240421 -0.27904135 -0.17033133
 -0.23424366 -0.12463437 -0.22992614  0.2565208  -0.11173727 -0.47853005
  0.20471719  0.18246491 -0.04360207  0.45208916 -0.29389203  0.25011605
  0.06752245  0.16226597  0.13218418  0.47747084 -0.05258209  0.20160058
  0.33550015 -0.15872324  0.10259634 -0.61744225  0.02918373  0.24867943
 -0.16691697 -0.38859913  0.0992979   0.25891164 -0.40599543 -0.4285341
  0.41972435  0.33209312 -0.14360808 -0.09875397  0.07930173  0.27217537
  0.24077998 -0.10105163  0.4022578  -0.7999437  -0.06348686  0.21868564
  0.10468867 -0.10778417  0.10344405  0.03346673  0.03149942 -0.0522413
  0.18232767  0.07353195 -0.07745083 -0.11808041 -0.23456404 -0.21849501
  0.6014896  -0.10443169 -0.20006971  0.04252873  0.20628823 -0.12472218
 -0.05691536  0.10812551 -0.03311401 -0.12331413 -0.08713794  0.09856309
  0.1689668  -0.3706477  -0.05623586 -0.4032419  -0.3585743   0.01614852
  0.15759018 -0.408396    0.01783105 -0.02942155 -0.2115397   0.04513944
  0.1658004   0.15026757 -0.48414716  0.32847193  0.01204198 -0.17803931
  0.04724977 -0.49246782 -0.18842526  0.03491772 -0.33267725  0.07316978
 -0.10172845  0.12432619 -0.09230075  0.12002934  0.2769476   0.40691763
  0.30505073 -0.20577931 -0.15833762 -0.26812923 -0.18361217  0.3948509
 -0.50964516  0.00489982 -0.22096929 -0.34852332 -0.05088169  0.4235371
 -0.04499054  0.0068618  -0.46976167  0.3457396  -0.252782   -0.00521209
  0.28210858  0.11859748  0.22853681  0.18136889  0.23801745  0.08289899
  0.34743863 -0.05467905  0.32315192 -0.01691578  0.11694677  0.7715879
  0.24693958  0.5201442  -0.25914556  0.4581018  -0.09421195 -0.15347773
 -0.06138658 -0.14678252  0.5694343  -0.4063863   0.12852915 -0.2187324
  0.46917433 -0.13550219 -0.06600153  0.19135061  0.2429294   0.2642977
 -0.25539705 -0.01069064 -0.13096717 -0.10932979 -0.24358228 -0.67574894
 -0.16478436  0.06571491 -0.24807519  0.1037777   0.00958128 -0.21006599
 -0.23626241 -0.12353444  0.1847491  -0.03019063  0.08012418  0.18950453
 -0.06437986 -0.08561888  0.28819507 -0.3907123   0.09518523  0.01680863
  0.5881102   0.26198     0.36553842 -0.28924808  0.24772798 -0.07294997
 -0.08222837  0.42588574  0.06060629 -0.02366818 -0.4075576   0.624171
  0.38334113 -0.16946422  0.02729143 -0.36958677 -0.3482947   0.09223884
  0.15697545 -0.10268673  0.02785551 -0.23924485  0.11681741  0.33242995
 -0.34816447 -0.21098498 -0.18207994  0.06861781 -0.14335245 -0.1789945
 -0.5586202   0.08453821 -0.05087614 -0.1940275  -0.00912225 -0.21902221
 -0.1883788  -0.29203853  0.07400535 -0.35857573  0.38650733  0.50171876
 -0.12323702  0.2703211   0.12262624  0.10265579 -0.3305745   0.05313857
  0.03806641  0.31340513 -0.13050121 -0.12971759  0.14599317  0.301488
 -0.16291346  0.21717842 -0.14026192  0.08509106  0.14464118 -0.21923617
 -0.45953494 -0.05075524  0.38294616  0.46649766 -0.12408722  0.34453857
 -0.25973463  0.37124538  0.5144596  -0.60128033 -0.19889745  0.17812422
  0.36870396 -0.14639881  0.12823945 -0.05354094  0.05909334 -0.14216357]"
Linux tensorflow build from source: bash failed genrule-setup.sh has carriage return type:docs-bug stat:awaiting response type:bug type:build/install stale TF 2.13,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/compiler version

LLVM = CLang = 16

### CUDA/cuDNN version

CPU only

### GPU model and memory

N/A

### Current behavior?

bazel build fails with ""bash failed: error executing command"", ""/bin/bash: line 1: $'\r': command not found""
(bash failed, genrule-setup.sh has carriage return (windows line endings))
modifying this .sh file does not work, because bazel won't even start the build, saying that file is modified, it might be corrupt.

Docs need to tell us we should check out tf repo after 
`git config --global core.autocrlf input`
Thanks.

### Standalone code to reproduce the issue

```shell
# git config --global core.autocrlf input  # very important!!!  otherwise your bazel build will fail, because an invoked shell script will have Windows line endings.
# git config --global core.eol lf
git clone https://github.com/tensorflow/tensorflow.git &
cd tensorflow
git checkout -b r2.13 origin/r2.13
apt install python3.10-venv
cd ..

python3 -m venv tf_venv
tf_venv/bin/pip install -U pip numpy wheel packaging requests opt_einsum
tf_venv/bin/pip install -U keras_preprocessing --no-deps

wget https://github.com/bazelbuild/bazelisk/releases/download/v1.18.0/bazelisk-linux-amd64
mv bazelisk-linux-amd64 bazel
chmod +x bazel
mv -v bazel /usr/local/bin

#install clang-16 

cd tensorflow
../tf_venv/bin/python3 configure.py   # (dl clang: N, opti flag: -msse4.1)
bazel build  --local_ram_resources=2048 --jobs=4 --verbose_failures  //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_",True,"[-4.95335668e-01 -3.15374672e-01 -1.95694506e-01 -2.64844093e-02
  1.50229394e-01 -4.54495192e-01 -9.13711488e-02  7.13206753e-02
 -2.60169327e-01 -1.79581031e-01 -1.11256644e-01  5.02395853e-02
 -1.55621573e-01  2.66358495e-01 -1.37784511e-01  7.03170896e-04
 -2.81592250e-01 -2.98905969e-01  3.01747084e-01  5.02397977e-02
 -2.44054332e-01  1.49215281e-01 -7.46271685e-02  1.24585092e-01
  1.35359503e-02  3.10396135e-01 -2.58106530e-01 -6.62429333e-02
  3.64013910e-02  2.36310422e-01  3.27719778e-01  3.17575037e-01
 -3.10872495e-01 -1.14458539e-01  1.02302507e-01  3.00314814e-01
 -6.98724985e-02 -1.03467524e-01 -4.12663132e-01 -8.94064084e-02
  5.63381203e-02 -3.88967767e-02  9.78440195e-02 -1.00775510e-01
  2.18016114e-02 -1.39058739e-01  1.02623910e-01 -1.49923921e-01
 -8.16552714e-02 -4.74524826e-01 -1.56348392e-01  1.17192000e-01
 -3.10285181e-01 -2.93724120e-01 -2.09904104e-01 -2.30766147e-01
  1.77694023e-01  3.20462883e-01  5.45810461e-02  1.40445307e-03
 -3.28810886e-02  1.29120678e-01  9.70130265e-02 -1.23387605e-01
  1.48901179e-01  4.45498377e-02  2.89227337e-01 -7.36558139e-02
  4.77919638e-01 -1.21471018e-01  9.91711095e-02 -1.86029166e-01
 -2.23917112e-01 -6.33709580e-02  1.61435176e-02  2.91241974e-01
  1.97457343e-01  1.10184565e-01  2.63866931e-01 -2.56423950e-01
 -1.37368724e-01 -1.76406205e-01 -2.21744895e-01 -1.20221063e-01
  6.34089857e-02  5.79599254e-02  2.10236520e-01 -1.15224674e-01
  5.76870561e-01 -7.59818256e-02  6.36369228e-01  3.37617069e-01
  4.64526303e-02  8.48752707e-02  4.81087983e-01  1.17306858e-01
 -3.68820131e-02  2.77387440e-01  1.20897144e-01 -4.76875156e-02
 -6.28468096e-02 -1.24071240e-01  1.37335330e-01  2.74252370e-02
 -1.06078014e-01 -1.55409604e-01  1.21152133e-01 -3.13694365e-02
  1.50184616e-01 -3.55791152e-02  1.20956823e-01 -7.40933698e-04
  2.70529687e-01 -8.05547982e-02  4.13966179e-02 -1.24763250e-02
 -3.25039685e-01 -2.25260004e-01 -5.81112541e-02  6.89776480e-01
  8.91391933e-03 -1.41617030e-01  4.18657996e-03  1.31292731e-01
  2.83103466e-01  1.26216441e-01 -1.09763987e-01  3.25668044e-03
  2.19739601e-02  3.54802087e-02  2.55220383e-02  3.35558653e-01
 -1.64410859e-01  3.62390339e-01 -2.20529176e-03  2.01451302e-01
 -1.27240837e-01 -1.60714537e-01 -2.39523336e-01 -1.52169585e-01
 -2.39727765e-01  2.91543305e-02 -1.44210860e-01 -6.13912702e-01
  1.25915617e-01 -1.09952092e-01 -2.63558835e-01  1.68251544e-01
 -2.61069059e-01  1.87982425e-01 -3.68509889e-02  1.99118420e-03
 -3.79198268e-02  3.88760835e-01  3.21675241e-01 -9.30532888e-02
  2.31107667e-01  7.21134245e-03 -1.59750983e-01 -5.13166845e-01
  4.97992858e-02  2.60727286e-01 -7.32551962e-02 -7.53033310e-02
  4.16454412e-02  2.00139940e-01 -3.30042481e-01 -2.16743499e-01
  3.24007198e-02  3.53798419e-01 -6.77837059e-02 -2.10913513e-02
  2.40273014e-01  1.58505756e-02  4.20424849e-01 -1.00743800e-01
  3.48625630e-01 -4.78260249e-01  1.43325841e-02  2.95272946e-01
  1.88105673e-01  2.30918244e-01  1.27924651e-01  3.27377081e-01
  1.70172811e-01 -1.65712312e-02  9.76871997e-02  1.50355965e-01
 -2.38091528e-01  2.90816545e-01 -2.82763153e-01 -1.80907533e-01
  3.31204206e-01 -1.06766112e-01 -2.57882774e-01  2.08573908e-01
  2.25256145e-01  1.00688815e-01  5.11510074e-02  4.94180806e-03
 -9.12826285e-02 -6.73493296e-02  5.59961945e-02  8.37272555e-02
  1.48037657e-01 -3.40439498e-01  1.28775626e-01 -4.48196948e-01
 -4.20577466e-01  3.31225106e-03 -3.51390764e-02 -4.45787936e-01
  6.61141574e-02 -2.33050719e-01 -2.45237499e-01  8.48561302e-02
 -2.92141875e-03 -1.56502366e-01 -9.86731872e-02  5.91193289e-02
  1.15016982e-01 -2.47381449e-01  7.27387145e-02 -3.74180466e-01
 -1.20631374e-01  9.00325775e-02 -3.71083915e-01  9.34767872e-02
  1.33401761e-02  1.07696079e-01  8.92867595e-02  4.72994410e-02
  4.63921219e-01  1.21412285e-01  3.70731950e-01 -2.86821455e-01
  3.80403586e-02 -2.89229423e-01 -1.97904170e-01  4.97486442e-02
 -3.86197507e-01 -3.12404573e-01  1.22526167e-02  2.80146860e-02
  2.89778501e-01  3.15788627e-01 -1.18861228e-01  6.42040968e-02
 -3.21724832e-01  1.24877468e-01 -1.82866424e-01  1.29271641e-01
  2.47606248e-01  1.92173779e-01  4.84572351e-01  1.14773735e-01
  5.79768717e-02  4.70697880e-01  2.24493533e-01 -1.65564463e-01
  4.11352038e-01  4.31100786e-01 -2.74264608e-02  1.60818815e-01
  2.71461844e-01  2.34612167e-01 -3.54099840e-01  5.24064779e-01
 -6.78117126e-02  2.27128506e-01  1.33960843e-01 -3.05294067e-01
  7.33942688e-01 -2.50338376e-01  1.67156234e-01 -7.30056763e-02
  2.72884429e-01  1.13655552e-01  7.21303821e-02  1.44737452e-01
  4.75192480e-02  4.21114981e-01 -5.82678378e-01  1.37981012e-01
  2.21505567e-01 -1.10237919e-01 -1.51352704e-01 -7.74845302e-01
 -2.07590565e-01 -1.81386918e-01 -1.03857152e-01  3.77195626e-02
 -1.10062666e-01 -6.40608445e-02 -3.63798141e-02 -2.16141157e-02
  1.01865560e-01 -2.55074978e-01  1.89953119e-01  2.50240743e-01
 -1.34919956e-01 -1.91830903e-01  3.24595690e-01 -1.58903033e-01
 -1.10719264e-01 -2.55739987e-02  1.96195006e-01  2.52690703e-01
  4.14575845e-01 -3.69770169e-01  1.79704249e-01  6.20389879e-02
 -6.26037568e-02  4.93036121e-01 -3.05018853e-03  5.79129159e-02
 -4.78018969e-01  6.36105478e-01  1.19786963e-01 -1.78087443e-01
  2.83849388e-01 -2.69008756e-01 -2.63453364e-01  5.65180257e-02
  3.00873995e-01 -1.96387872e-01 -3.17869484e-02 -2.93039024e-01
 -1.33413821e-01  1.29731104e-01 -2.30601832e-01 -2.15883300e-01
 -2.49014765e-01  1.64099872e-01 -1.10231191e-01  1.83607638e-02
 -2.71196187e-01  2.21673906e-01 -1.91291407e-01 -4.21815991e-01
 -1.17370144e-01 -5.12624308e-02 -6.71296194e-03 -3.17920029e-01
 -6.31794631e-02 -3.34624887e-01  3.75930905e-01  2.29773968e-01
 -2.71262705e-01 -2.04910859e-02 -5.52622899e-02  1.72969162e-01
 -4.50895965e-01 -1.23182647e-01 -1.62115186e-01  1.83839977e-01
  2.22068001e-03  3.66985612e-03  3.26884866e-01  2.87763000e-01
 -1.84140533e-01  1.44574851e-01 -3.35072011e-01  4.26437333e-02
  4.45125103e-02 -1.50946915e-01 -2.58999497e-01  6.44572750e-02
 -1.38101518e-01  2.26489261e-01  3.02769039e-02  2.37393156e-01
 -4.00013685e-01  3.31905186e-01  7.11530328e-01 -3.86991978e-01
 -4.44870770e-01  1.05970331e-01  2.55031466e-01 -1.33095443e-01
 -1.32834136e-01  2.08822191e-02  3.30552995e-01 -6.47168010e-02]"
bazel  test failed with some ctest type:bug subtype: ubuntu/linux TF 2.12,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.12

### Custom code

Yes

### OS platform and distribution

centos7.6

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/compiler version

9.3.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

After compiling tensorflow based on the source code, bazel test is executed for unit testing, some test items are passed, but there are many failures, and the reasons for the error are as follows
So I want to know if I'm executing the statement incorrectly, how should I set it up?
bazel test -c opt  --config=cuda  --test_sharding_strategy=disabled  //tensorflow/core/kernels/...
output information:
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/core/kernels/image:resize_ops_test_gpu

![image](https://github.com/tensorflow/tensorflow/assets/30514703/bc876890-814c-4488-8c74-ee9f46960ada)


![image](https://github.com/tensorflow/tensorflow/assets/30514703/befa4291-9404-42d1-956a-11250f384cc4)


### Standalone code to reproduce the issue

```shell
bazel test -c opt  --config=cuda  --test_sharding_strategy=disabled  //tensorflow/core/kernels/...
```


### Relevant log output

_No response_",True,"[-7.38710701e-01 -1.63162023e-01 -1.68839797e-01  1.08645551e-01
  1.82402693e-02 -5.35018682e-01  3.30797993e-02  1.58367991e-01
 -2.58099943e-01 -5.49096823e-01  6.13918826e-02 -3.90287310e-01
 -2.53334492e-01 -7.16607422e-02 -1.15516827e-01  1.56625018e-01
 -5.47845196e-03  2.22960673e-02  3.41251343e-01 -8.57060105e-02
 -3.41238976e-01 -2.46031821e-01 -1.54664636e-01  3.54879975e-01
  9.48913172e-02  2.13179260e-01 -1.30238369e-01 -1.73339128e-01
 -1.36202388e-02  8.29878896e-02  2.23734930e-01  2.41645440e-01
  1.05017625e-01 -1.91055685e-02  2.82394201e-01  1.56800583e-01
 -4.90815639e-02 -2.44289622e-01 -3.57148826e-01 -2.01816410e-01
  2.18980182e-02 -7.22131208e-02  4.44929928e-01 -2.20829561e-01
  2.37177536e-01 -4.05773759e-01 -1.60870224e-01  2.39720736e-02
 -1.07692972e-01 -2.18976945e-01  5.51344380e-02 -3.06197286e-01
 -2.23764449e-01 -5.59597135e-01 -1.00521073e-01 -6.11675903e-02
  4.94801328e-02 -1.95896663e-02 -5.39587513e-02  3.16109449e-01
 -4.25517187e-03  4.02501412e-02 -1.35971442e-01  7.19769299e-03
  4.71717149e-01  1.26595080e-01  5.01704328e-02 -1.40351593e-01
  4.72105265e-01 -1.03175968e-01  2.00502336e-01 -8.40063673e-04
 -4.21781361e-01  1.19510889e-01  1.10529266e-01  4.20539714e-02
  4.29649651e-01  2.22739756e-01  1.91048443e-01 -1.44224539e-01
  1.10978849e-01 -2.48085022e-01 -7.70990178e-02 -3.24432552e-01
  1.66488037e-01  5.35367094e-02  3.28801662e-01 -2.14416403e-02
  4.25881594e-01 -1.36836320e-01  3.86519134e-01  3.68432403e-01
 -2.26640284e-01  3.09606671e-01  5.67512810e-02  2.87438691e-01
  4.28227037e-01  4.19466011e-02 -1.26850978e-02  2.66925879e-02
  1.02212746e-02 -3.28985214e-01  6.32186532e-02  6.11539222e-02
 -1.57324702e-01 -3.21782976e-01  4.58529830e-01 -1.27697811e-01
 -5.28884605e-02 -2.25867465e-01  4.77874205e-02  2.71320567e-02
  5.95451072e-02 -1.76509127e-01  7.99705014e-02 -9.95842218e-02
 -3.51915695e-03  1.09797835e-01  2.14149971e-02  6.36034846e-01
 -2.02686518e-01 -5.98441958e-02 -1.97412342e-01 -1.39417857e-01
  7.74213195e-01  6.63258955e-02 -1.90696225e-01  1.99866351e-02
  2.96918392e-01 -2.36402825e-03  1.84960682e-02  2.20266476e-01
 -9.34967622e-02  3.57512087e-01 -3.43818702e-02  2.43121207e-01
 -2.78940558e-01  8.57857317e-02  5.05792648e-02 -3.43686819e-01
 -2.38310874e-01 -3.99224125e-02  2.42489591e-01 -5.46512961e-01
 -1.06194884e-01 -2.54058093e-01 -4.55269516e-01  1.06983155e-01
 -4.12659734e-01  9.51348804e-03 -3.03978741e-01  1.04593653e-02
 -7.92450458e-02  5.95178485e-01  7.80566782e-02 -1.16535947e-02
  2.74153858e-01  8.11038241e-02  1.95596933e-01 -2.15220690e-01
 -5.27189560e-02  5.23438096e-01 -1.49095953e-01  1.29914004e-02
 -3.52317393e-02  1.91053361e-01 -2.57768184e-01 -3.61028090e-02
  2.63388336e-01  4.81309384e-01 -2.26314381e-01 -3.98415029e-02
  1.73850209e-01 -1.57489523e-01  1.37960553e-01 -2.17365682e-01
  3.40789557e-01 -1.59373313e-01  7.97095671e-02  2.73457289e-01
 -1.56977877e-01  1.62503392e-01  1.02256827e-01  1.90812677e-01
  2.31307745e-01  1.19358316e-01  1.09072156e-01  2.39942104e-01
 -7.58277625e-03 -1.52924694e-02 -4.43784535e-01 -1.30684406e-01
  3.65816712e-01 -3.71298701e-01 -2.43075699e-01  3.36253196e-02
  3.09334159e-01 -1.29944921e-01 -1.44064724e-02  1.11650117e-03
 -1.75535649e-01  2.44266838e-02  2.39570096e-01  1.55057348e-02
  8.09658468e-02 -9.88218188e-02  1.37849048e-01 -2.45155618e-01
  9.82761160e-02  9.92621928e-02 -5.05317748e-02 -3.36278826e-01
 -7.85465389e-02  2.65529044e-02 -2.32275665e-01  2.22923160e-01
  4.05484498e-01 -7.02137351e-02 -1.21881496e-02  2.81500578e-01
  2.28828833e-01 -3.55452657e-01  3.49654555e-02 -3.61486793e-01
 -1.01277210e-01  2.44018614e-01 -1.18325584e-01 -8.95361975e-02
 -2.65902519e-01 -2.31443986e-01  2.03530669e-01  3.34027223e-02
  3.97530556e-01  4.46896017e-01  4.23033148e-01  1.16208300e-01
 -8.54878426e-02 -8.34856853e-02 -3.78322601e-01  5.18234000e-02
 -4.55995202e-01 -5.39078593e-01  2.07411751e-01 -1.37233660e-01
  2.21367761e-01  4.22115505e-01 -5.38854264e-02 -1.21119186e-01
 -4.54536110e-01  2.82589972e-01 -4.06999767e-01  2.34195367e-01
  2.77074695e-01  1.84054554e-01  3.49007845e-01  2.79391587e-01
  2.15483740e-01  2.67826617e-01  2.14498967e-01 -4.90603924e-01
  5.14549732e-01  3.06592286e-01  8.83660018e-02 -3.02298889e-02
 -1.67553484e-01  2.99388468e-01 -2.91777730e-01  3.84417057e-01
  2.14075327e-01 -1.68400928e-01  5.63576102e-01 -2.87000120e-01
  5.28967500e-01 -1.08410671e-01  8.66153538e-02 -3.15552682e-01
  8.52857232e-02 -7.51091614e-02 -1.37194782e-01  8.45651701e-02
  1.69730723e-01 -1.62750468e-01  1.06457278e-01 -6.08682521e-02
  1.18243039e-01  4.05043140e-02  8.58534724e-02 -7.53427386e-01
 -1.25797212e-01 -2.57443637e-02 -2.48665079e-01  3.65983337e-01
 -3.16523723e-02 -9.97137800e-02 -3.81007671e-01  9.91198421e-02
  9.68488306e-02 -2.33094871e-01  8.96983445e-02  2.27833658e-01
 -4.07027781e-01 -1.56843066e-01  5.19357845e-02 -3.10300261e-01
 -9.52432081e-02 -2.83578988e-02  7.98555091e-02  2.26319045e-01
  5.20355046e-01 -2.71739334e-01 -2.30532549e-02 -1.74632624e-01
 -4.42145169e-01  2.25005627e-01 -1.53898656e-01  3.80715728e-01
 -2.44948089e-01  5.47205329e-01  3.50647628e-01  4.02710065e-02
 -1.38725825e-02 -5.30549765e-01 -4.70657587e-01  6.04740307e-02
  3.47113371e-01 -1.05391502e-01 -3.38209927e-01 -5.93437195e-01
  1.92663416e-01  2.10420370e-01 -3.56844187e-01  9.74794775e-02
 -2.35805720e-01 -7.39443600e-02 -1.76174670e-01  1.10878289e-01
 -1.94231465e-01 -1.19272619e-04 -1.27024069e-01 -2.18767062e-01
 -2.95613706e-01 -8.25734884e-02 -2.35058770e-01 -1.68664992e-01
 -1.75840668e-02 -9.95171964e-02  2.42171995e-02  4.90006298e-01
 -1.85859110e-02 -5.95811680e-02  8.71464461e-02  1.93550512e-01
 -1.02600977e-01  7.25907832e-02  2.60545611e-02  3.66660058e-01
 -4.39048260e-02 -2.12156922e-02  6.10231698e-01  3.95488441e-02
  7.81575218e-02 -1.97217137e-01 -2.72558033e-01  2.00983495e-01
  8.94359499e-02 -2.99929798e-01 -3.31237316e-01 -2.22780645e-01
  1.16105173e-02  1.87988654e-01 -2.07538411e-01 -1.13378771e-01
 -9.51737314e-02  2.52934903e-01  7.21844196e-01 -5.08173585e-01
  9.97457281e-02  1.47477731e-01  1.20261125e-01  1.89892694e-01
 -8.82108808e-02 -1.33549124e-01  4.19776142e-01  6.95274621e-02]"
Cannot create interpreter when using GPU-Delegate or NNAPI-Delegate stat:awaiting response type:bug stale comp:lite TFLiteNNAPIDelegate TFLiteGooglePlayServices,"**System information**
- Android Device information: samsung/a14mnseea/a14m:13/TP1A.220624.014/A145RXXU2AWG3:user/release-keys
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
  - com.google.android.gms:play-services-tflite-java:16.1.0
  - com.google.android.gms:play-services-tflite-support:16.1.0
  - com.google.android.gms:play-services-tflite-gpu:16.2.0
- Google Play Services version: 23.33.16

**Standalone code to reproduce the issue**

        var useGpu = Tasks.await(TfLiteGpu.isGpuDelegateAvailable(context));
        var optionsBuilder = TfLiteInitializationOptions.builder();

        optionsBuilder.setEnableGpuDelegateSupport(useGpu);

        Tasks.await(TfLite.initialize(context, optionsBuilder.build()));

        var options =  new InterpreterApi.Options();
        if(useGpu){
            options.addDelegateFactory(new GpuDelegateFactory());
        }

        /*delegate = new NnApiDelegate();
        options.addDelegate(delegate);
        options.setUseNNAPI(true);*/

        options.setRuntime(InterpreterApi.Options.TfLiteRuntime.FROM_SYSTEM_ONLY);

        //load Model from App assets
        interpreter = InterpreterApi.create(new File(modelPath), options);

**Any other info / logs**
I oriented my code on the official documentation [on here](https://www.tensorflow.org/lite/android/delegates/gpu)

Logcat-Output:
java.lang.IllegalArgumentException: Internal error: Cannot create interpreter: 
                                                                                                    	at com.google.android.gms.tflite.NativeInterpreterWrapper.createInterpreter(Native Method)
                                                                                                    	at com.google.android.gms.tflite.NativeInterpreterWrapper.zzl(com.google.android.gms:play-services-tflite-java@@16.1.0:34)
                                                                                                    	at com.google.android.gms.tflite.NativeInterpreterWrapper.<init>(com.google.android.gms:play-services-tflite-java@@16.1.0:6)
                                                                                                    	at com.google.android.gms.tflite.zzd.<init>(com.google.android.gms:play-services-tflite-java@@16.1.0:1)
                                                                                                    	at com.google.android.gms.tflite.InterpreterFactoryImpl.create(com.google.android.gms:play-services-tflite-java@@16.1.0:2)
                                                                                                    	at org.tensorflow.lite.InterpreterApi.create(InterpreterApi.java:336)
                                                                                                    	at com.example.tfliteaudio.TFLiteEngine.initialize(TFLiteEngine.java:83)
                                                                                                    	at com.example.tfliteaudio.MainActivity.lambda$transcribeAudio$5(MainActivity.java:143)
                                                                                                    	at com.example.tfliteaudio.MainActivity.$r8$lambda$1xqJ9hAvPXTc26gXgWfy8QcV0VE(Unknown Source:0)
                                                                                                    	at com.example.tfliteaudio.MainActivity$$ExternalSyntheticLambda2.run(Unknown Source:2)
                                                                                                    	at java.lang.Thread.run(Thread.java:1012)
",True,"[-0.1971954  -0.49908376 -0.51584244 -0.24797839 -0.05323266  0.03489468
  0.02811811 -0.03465653 -0.09514572  0.00284562 -0.1814681  -0.33163694
  0.10799901  0.41324455  0.13039882  0.2634778  -0.14856665 -0.10941878
  0.14222135 -0.10818394 -0.05843533 -0.08920346 -0.15438613  0.12512079
  0.18422326  0.15995541  0.02479051 -0.25903413  0.07769781  0.32838026
  0.1571444   0.17928009  0.08090682 -0.01094521 -0.18665645  0.09106977
 -0.43786377 -0.21210244 -0.23704591 -0.0835906   0.08189987  0.41555393
 -0.07619715  0.24976623 -0.27262533  0.12400451 -0.13486022  0.18635498
 -0.35563883  0.1333463  -0.14225534  0.07601673 -0.4801364  -0.25326714
 -0.02306233  0.16516373  0.0440059   0.33049423  0.11232152  0.39172447
  0.04581157 -0.05821668 -0.00968866 -0.10018194 -0.26915473  0.3017677
  0.04629722 -0.34270176  0.4143387  -0.3022619  -0.07134035 -0.18553749
  0.16894442 -0.07731041  0.21550849 -0.02485638 -0.10292214  0.33198833
  0.18196063 -0.0995096  -0.0886526  -0.04636107  0.02004481  0.15428406
  0.4579753   0.0064732   0.00532544  0.05576869  0.3640078   0.3104378
  0.21304396  0.19952215  0.20783196  0.13879675  0.14616567  0.16641292
  0.02289864  0.14657989 -0.16257142 -0.00845966 -0.13680682 -0.18381585
 -0.24440327  0.23230681  0.12748855 -0.46634752  0.21119419  0.0662989
  0.06579424  0.32677716  0.25721186 -0.04421448  0.10539904  0.06466323
  0.01336772  0.27340335 -0.18147321  0.1028053  -0.04753565  0.31319147
 -0.07964389 -0.07982428  0.01212122  0.00195933  0.16870704  0.06418256
 -0.3230525  -0.12839474  0.03760842  0.03987692  0.12862438  0.33276844
 -0.28087145 -0.15419623  0.05600328  0.05024386 -0.11732544 -0.37569818
 -0.3268424  -0.06784908 -0.07924671  0.24879563 -0.00083685 -0.06102666
 -0.11525731  0.20887686 -0.15730421  0.2761371   0.03916997  0.04492312
 -0.1784322  -0.00404284 -0.14000562  0.33749422  0.13459502  0.10196289
  0.3765394  -0.03797872 -0.02868598 -0.68531823 -0.12914419 -0.02392391
  0.22040686 -0.10388049  0.12329566  0.07546957 -0.29859322 -0.15206882
 -0.09743748  0.20083302 -0.2386486  -0.07859364 -0.20946187  0.10139543
  0.23829755 -0.18616197  0.1869078  -0.41396165 -0.04301566 -0.11228518
  0.24747635  0.21948671  0.07893293 -0.21349522 -0.36003187  0.09157626
 -0.00807868  0.08505219 -0.16308628 -0.05242528 -0.19808796  0.0728334
  0.17172295 -0.13862947 -0.44692978 -0.21122624  0.31423062 -0.24420139
  0.3888822   0.251029    0.14012042  0.15574226 -0.1651406  -0.20846608
  0.20402128 -0.11412331 -0.29479492 -0.3074763  -0.01161946  0.07552989
  0.14688528 -0.32564253 -0.24502352 -0.18519822  0.01119196 -0.37984717
 -0.10332753  0.15019669 -0.40330815  0.19640018  0.06793629 -0.1192463
 -0.10871838 -0.16837966 -0.5758556  -0.3238826   0.05998467 -0.03122671
 -0.09480917  0.15199943 -0.04047494 -0.20167717  0.41230574 -0.1242187
 -0.04516619 -0.11716648  0.3269608  -0.23963398 -0.23595971 -0.2399163
 -0.02440613 -0.00109474  0.04212598 -0.17070654  0.02180517  0.03134725
 -0.06613021  0.10074737 -0.0415733   0.3154887   0.19839317 -0.0354616
  0.16664627  0.2537102   0.26359853  0.07929596 -0.14984787 -0.42654264
  0.07491266 -0.13000852  0.07955934  0.43376565 -0.1983098   0.46810922
  0.4371989   0.02905283 -0.11577225  0.24666579 -0.11444557 -0.09277007
  0.07081366 -0.27175897  0.26733017 -0.3701403   0.16856869 -0.17382677
  0.39509454 -0.1395009  -0.32439715  0.2957508   0.2313186   0.2678728
  0.05274121 -0.17274787  0.21453509 -0.43294027 -0.09224673 -0.3124419
 -0.11014243  0.07008478  0.08328369  0.1355258   0.37650102 -0.04087388
 -0.05342439 -0.11865775 -0.06402524  0.13814747  0.05946031  0.10767624
 -0.09168614  0.3148054   0.18503045 -0.21195632  0.16850768  0.08776702
 -0.02329477 -0.12834603  0.4864225  -0.30858865  0.35018358  0.16603854
 -0.02168154  0.3351359  -0.27521878  0.15408519 -0.25032943  0.30952546
  0.10310435  0.14908487  0.19493921 -0.25847077 -0.16235384  0.19224113
  0.05078273  0.04296403 -0.2890513  -0.21837851 -0.04245731 -0.07621996
  0.06120064 -0.10622355 -0.0320633   0.11920088  0.14347714 -0.1103579
 -0.14761564  0.31609863 -0.00212374 -0.16549057  0.01269658 -0.22169492
 -0.0681911  -0.293512   -0.1704555  -0.19005272  0.24997637  0.44869924
  0.16794261  0.21655816 -0.12313288  0.0946609  -0.02993148 -0.1660235
 -0.24237846  0.3168481  -0.24177918 -0.09140347  0.2592297   0.46215346
 -0.1259847  -0.00276365 -0.3066663  -0.180222   -0.15241933 -0.30885303
 -0.06488705 -0.05778275  0.35882303  0.73684883 -0.20556624  0.265307
 -0.34091955 -0.15796697  0.26751474 -0.05849623  0.3082342  -0.2437635
 -0.1972011  -0.01318517  0.01410041  0.09832413  0.09735534  0.04952788]"
Memory leak when using tf.Model and tf.Model.fit() in a loop. clear_session() does not help stat:awaiting response type:bug stale comp:keras TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes (tf-nightly = ""2.15.0.dev20230904"")

### Source

source

### TensorFlow version

2.13, 2.12, 2.11

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.1 LTS (GNU/Linux 5.16.10 x86_64)

### Mobile device

_No response_

### Python version

3.10.0, 3.9.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuDNN 8600

### GPU model and memory

NVIDIA RTX A5000, 24GB

### Current behavior?

Memory usage steadily increases when using tf.keras.Model and tf.keras.Model.fit() in a loop, and leads to Out Of Memory exception saturating the memory eventually. clear_session() does not help. The same code with TF version == 2.9.2 has an almost constant memory usage instead, and works as expected.
I've also opened [this issue](https://github.com/keras-team/tf-keras/issues/286) on Keras's GitHub, months ago, with no solutions from the Keras team.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf # same issue with tf-nightly = ""2.15.0.dev20230904""
import time
import gc
import psutil # psutil == ""5.9.5""
import subprocess as sp

gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(
        device=gpu, enable=True
    )


def get_cpu_memory():
    memory_info = psutil.virtual_memory()
    # you can have the percentage of used RAM
    memory_percent = 100.0 - memory_info.percent
    memory_free_values = memory_info.available / (1024 * 1024)  # in MB
    # you can calculate percentage of available memory
    return memory_free_values, memory_percent


def get_gpu_memory():
    command = ""nvidia-smi --query-gpu=memory.free --format=csv""
    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\n')[:-1][1:]
    memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)][0]
    memory_percent = (memory_free_values / 24564) * 100  # my gpu has 24564 MB of memory
    return memory_free_values, memory_percent


class MyModel(tf.keras.Model):

    def __init__(self):
        super().__init__()
        self.dense1 = tf.keras.layers.Dense(1000, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(10000, activation=tf.nn.relu)
        self.dense3 = tf.keras.layers.Dense(10000, activation=tf.nn.relu)
        self.dense4 = tf.keras.layers.Dense(1000, activation=tf.nn.softmax)

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        x = self.dense3(x)
        x = self.dense4(x)
        return x


if __name__ == '__main__':
    print(f""Starting.."")
    memory_free_val_initial, memory_perc_initial = get_cpu_memory()
    print(f""[Memory monitoring] Free memory CPU {memory_free_val_initial} MB, {memory_perc_initial} %."")
    memory_free_val_initial_gpu, memory_perc_initial_gpu = get_gpu_memory()
    print(f""[Memory monitoring] Free memory GPU {memory_free_val_initial_gpu} MB, {memory_perc_initial_gpu} %."")

    for r in range(0, 1000):
        model = MyModel()
        # ds = tf.data.Dataset.from_tensor_slices((tf.random.uniform((64*4, 1000)), tf.ones((64*4))))
        ds = (lambda: tf.data.Dataset.from_tensor_slices((tf.random.uniform((64 * 20, 1000)), tf.ones((64 * 20)))))
        model.compile(optimizer='sgd', loss=tf.keras.losses.SparseCategoricalCrossentropy())

        model.fit(ds().batch(64), verbose=0)
        model.evaluate(ds().batch(64), verbose=0)
        tf.keras.backend.clear_session()

        if r % 5 == 0:
            # print every 5 model.fit
            print(f""Round: {r}"")
            memory_free_val, memory_perc = get_cpu_memory()
            print(f""[Memory monitoring] Free memory CPU {memory_free_val} MB, {memory_perc} %."")
            memory_free_val_gpu, memory_perc_gpu = get_gpu_memory()
            print(f""[Memory monitoring] Free memory GPU {memory_free_val_gpu} MB, {memory_perc_gpu} %."")
            if r == 0:
                memory_free_first = memory_free_val
                memory_free_first_gpu = memory_free_val_gpu
            # time.sleep(2)

        del model
        gc.collect()
        del ds

    print(f""[Memory monitoring CPU] Memory usage increased by {memory_free_first - memory_free_val} MB, ""
          ""during the process."")
    print(f""[Memory monitoring GPU] Memory usage increased by {memory_free_first_gpu - memory_free_val_gpu} MB, ""
          ""during the process."")
```


### Relevant log output

```shell
Round: 0
[Memory monitoring] Free memory CPU 56633.48046875 MB, 88.5 %.
[Memory monitoring] Free memory GPU 21180 MB, 86.2237420615535 %.


Round: 995
[Memory monitoring] Free memory CPU 49866.3046875 MB, 77.9 %.
[Memory monitoring] Free memory GPU 21156 MB, 86.12603810454324 %.

[Memory monitoring CPU] Memory usage increased by 6767.17578125 MB, during the process.
[Memory monitoring GPU] Memory usage increased by 24 MB, during the process.
```
",True,"[-0.16927683 -0.32220227 -0.25179368 -0.20844279  0.24598469 -0.19077104
  0.0651847   0.08103301 -0.21117178 -0.20299055  0.05144693 -0.04199604
 -0.17518446  0.09888913 -0.12185818  0.17235309  0.06377569 -0.06818537
 -0.0406177   0.13362993 -0.0588288  -0.19325724 -0.10528964  0.25546592
 -0.13462219  0.30668762 -0.01466811 -0.04847445 -0.09867294  0.05482978
 -0.0805631   0.16591924 -0.18792026  0.04358936  0.08696961  0.1104722
 -0.22297564 -0.29079458 -0.225795   -0.04566185  0.01216471 -0.0136383
  0.12438956 -0.17941846  0.09192673 -0.08616747 -0.22655502 -0.02085206
 -0.13667426 -0.06956255  0.18438527  0.05966992 -0.12280796 -0.38865846
 -0.25545073  0.12521428  0.09173129 -0.32049942 -0.16823183  0.13864535
  0.09451308  0.04582337  0.04288778  0.10596035  0.4951886   0.0892028
  0.1869291   0.08574863  0.52786255 -0.15631172  0.0426718  -0.0359295
 -0.39342397 -0.00549488  0.23012912  0.03286034  0.07715965  0.00701517
  0.32111448 -0.06140193 -0.04157852 -0.2625224  -0.12854552 -0.3825354
 -0.12392426 -0.11681844  0.4696806  -0.02986607  0.4237453  -0.22415142
  0.4131731   0.36101213 -0.1784881   0.16488227  0.42729634  0.13849169
 -0.07958102  0.08874197 -0.2655671  -0.07511653 -0.05372773 -0.02757371
  0.1059347   0.17135885 -0.03833878 -0.1612229   0.13477647 -0.12640887
  0.09080131  0.06936242  0.21591413 -0.10105911  0.35737908  0.24624075
 -0.0351136  -0.0119218   0.14481702  0.26106793 -0.10725223  0.49350512
 -0.02764967 -0.3838781   0.03911354 -0.07995841  0.28117055  0.18146214
 -0.18584546  0.1299236   0.05266863 -0.2002097   0.03990234  0.10870271
  0.08298551 -0.05162882 -0.13805425  0.0709013  -0.10351186  0.05647182
 -0.15812922 -0.15465342 -0.22511156  0.19019051 -0.02386599 -0.29060388
  0.01086228  0.25891674 -0.19318447  0.23471723 -0.05770236  0.02794286
  0.08051321 -0.10380195  0.10505681  0.28963232 -0.13003449  0.10448024
  0.336603   -0.06701271  0.09315777 -0.42712742 -0.0329251   0.3902864
  0.13570014 -0.35337257  0.45974562  0.17681828 -0.06751504 -0.12465607
  0.17069025  0.30578947  0.11926584 -0.20543513 -0.153653   -0.05439936
  0.49590236 -0.18026279  0.18888083 -0.5074631  -0.06295705  0.01374236
  0.05352657 -0.05360535  0.08568461  0.2626634   0.08548392 -0.11265372
  0.21158062  0.18356115 -0.34164515 -0.09483309 -0.2513864  -0.23749787
  0.37855974 -0.13695115 -0.03690611 -0.15943196  0.35864192 -0.15444814
 -0.07450105 -0.12250426  0.16347894 -0.1410805  -0.13488096 -0.14565529
  0.3763331  -0.17165047 -0.21857207 -0.4360784   0.01535411 -0.08982303
  0.06843298 -0.3092087  -0.11418736 -0.00455363 -0.19833264  0.02737425
 -0.02028699 -0.09735909 -0.04335167  0.42175055  0.33740237 -0.2614767
 -0.11091791 -0.44529283 -0.2701869  -0.12921697 -0.27297065  0.15210763
  0.15993644 -0.008978    0.07384465  0.2365658   0.2524348   0.13400945
  0.2704942  -0.02039693 -0.2378026  -0.13290852 -0.18844077 -0.30390292
 -0.22340882 -0.18382305 -0.0933069  -0.15321416  0.3262296   0.3227945
 -0.16475746 -0.05794673 -0.13818477  0.28490007 -0.5486334   0.06457014
  0.08197519 -0.03648048  0.24312565  0.12047321  0.22058557  0.0831393
  0.5070052  -0.17614731  0.42529142  0.2636538   0.08208421  0.45578015
  0.18771614  0.32176894 -0.38060898  0.1972748  -0.09568468 -0.16336018
  0.09742    -0.35359108  0.64248174 -0.14685565  0.2497669  -0.34081444
  0.41304034 -0.12386546 -0.27103484  0.13575113  0.2139116   0.12882082
 -0.17688024  0.00981226  0.08625539 -0.08556884 -0.05420694 -0.56911236
 -0.06146548 -0.26193327 -0.12509567  0.13642983  0.22958559 -0.06780356
 -0.06318802  0.10834524  0.01224163  0.16615644  0.15769015 -0.03189831
 -0.4384994  -0.17862271  0.535011   -0.31795973 -0.27379164  0.03796671
  0.26265648  0.17160322  0.36564755 -0.27269116  0.10791689  0.02633917
 -0.17908265  0.23981957 -0.02991675  0.07525317 -0.31205225  0.57280946
 -0.10327492 -0.0647338   0.19097915 -0.3203057  -0.23744461  0.14708015
  0.13065395 -0.24089114  0.07368909 -0.12955289  0.1648126   0.18549767
 -0.09902247 -0.1269218  -0.09974231  0.00782886 -0.05097679 -0.12169211
 -0.33262658  0.3741351  -0.08421696 -0.43749142 -0.18576045 -0.08725917
 -0.25513905 -0.27715743  0.26930913 -0.17967027  0.28110382  0.59992284
  0.11002479  0.2379317   0.06307432  0.07942447  0.07061633  0.05062563
 -0.07910924  0.4033231   0.09143589 -0.10664219  0.21268567  0.35663238
 -0.08371434  0.04520446 -0.2595416   0.13698573 -0.10937373 -0.2594979
 -0.07190031 -0.06938018  0.22750485  0.2201947  -0.14330606  0.16644199
 -0.21336886  0.02498067  0.43129647 -0.488896   -0.1877481   0.09065782
  0.22627668 -0.10361256  0.04407671 -0.08874968 -0.04824337  0.03440915]"
"Unable to open file (truncated file: eof = 2568306, sblock->base_addr = 0, stored_eof = 9406464) type:bug subtype:macOS TF 2.13","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

No

### OS platform and distribution

mac os 13.5.1

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
Cell In[15], line 3
      1 # Create the base model from the pre-trained model MobileNet V2
      2 IMG_SHAPE = IMG_SIZE + (3,)
----> 3 base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=IMG_SHAPE,
      4                                                include_top=False,
      5                                                weights='imagenet')

File [~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/keras/src/applications/mobilenet_v2.py:481], in MobileNetV2(input_shape, alpha, include_top, weights, input_tensor, pooling, classes, classifier_activation, **kwargs)
    477         weight_path = BASE_WEIGHT_PATH + model_name
    478         weights_path = data_utils.get_file(
    479             model_name, weight_path, cache_subdir=""models""
    480         )
--> 481     model.load_weights(weights_path)
    482 elif weights is not None:
    483     model.load_weights(weights)

File [~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70], in filter_traceback..error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb
...
File h5py/_objects.pyx:55, in h5py._objects.with_phil.wrapper()

File h5py/h5f.pyx:106, in h5py.h5f.open()

OSError: Unable to open file (truncated file: eof = 2568306, sblock->base_addr = 0, stored_eof = 9406464)

### Standalone code to reproduce the issue

```shell
https://tensorflow.google.cn/tutorials/images/transfer_learning

# Create the base model from the pre-trained model MobileNet V2
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,
                                               include_top=False,
                                               weights='imagenet')
```


### Relevant log output

_No response_",True,"[-4.01217341e-01 -3.56934071e-01  1.59628130e-03  4.68756184e-02
  3.42527092e-01 -4.39058363e-01 -1.85224384e-01  1.38725340e-01
 -3.08042824e-01 -2.47327358e-01 -3.97574194e-02 -2.05394756e-02
 -4.17406112e-01 -1.73329730e-02 -3.28057289e-01  2.54684389e-01
 -1.16859153e-01 -2.73098707e-01  1.51086450e-01  7.94150978e-02
 -1.61468953e-01  1.54983494e-02 -9.96600911e-02 -1.55400438e-02
  2.60144509e-02  1.56314224e-01 -6.22688085e-02 -6.34624660e-02
 -8.94451067e-02  2.04416469e-01  3.08461010e-01 -9.88314524e-02
  1.50953338e-01 -6.78513348e-02  3.07379901e-01  3.53476882e-01
  2.36314610e-01 -2.06046283e-01 -5.26051819e-01 -1.11905128e-01
 -9.32563469e-02  1.79786950e-01  1.92280844e-01 -1.22108169e-01
  9.99941230e-02 -2.86184132e-01  4.06157635e-02 -2.29327708e-01
  2.04726964e-01 -1.23627990e-01  4.92024496e-02  1.33386375e-02
 -1.99710056e-01 -3.55375290e-01 -8.31623971e-02  1.01764984e-01
 -2.39118993e-01  1.10581324e-01  1.16463631e-01  2.94142604e-01
  3.56181890e-01 -5.55466525e-02  1.05296679e-01 -3.13158482e-02
  4.59961116e-01  5.22214547e-02  3.00367475e-01  1.05980225e-02
  4.12132859e-01 -2.87829578e-01  3.16460691e-02  9.89086479e-02
 -3.06739509e-01  1.73737168e-01  3.49630296e-01  1.39337391e-01
 -1.84674010e-01 -6.98654447e-03  7.97726437e-02 -4.32525426e-02
 -1.97924495e-01 -1.61219180e-01  6.07947782e-02 -2.89686680e-01
 -8.30479264e-02  6.61681592e-02  2.51557022e-01 -9.10457075e-02
  4.37638432e-01 -4.65967320e-02  3.65162849e-01  1.10645987e-01
 -1.64845929e-01  2.21158043e-02  3.45423162e-01 -1.88843161e-01
  2.26230174e-01  2.75735617e-01 -4.25583497e-02 -1.10157579e-02
 -6.63462058e-02 -1.11453079e-01  3.23278576e-01  1.67902797e-01
 -7.54102841e-02 -5.99817075e-02  2.65904456e-01 -1.15146581e-02
 -2.26798989e-02 -2.65495420e-01  2.03891784e-01  3.39556485e-04
  9.27921385e-02  4.62609120e-02  1.37196541e-01  1.52273104e-03
 -1.92850113e-01 -4.02734503e-02 -1.35246158e-01  5.15317738e-01
 -1.07351476e-02 -3.50767851e-01 -1.57697171e-01  1.68262377e-01
  1.78546667e-01  6.01326786e-02 -2.73462117e-01  8.65420178e-02
 -3.57460491e-02 -2.59572178e-01 -4.63998690e-02 -6.09935038e-02
  3.90083581e-01  2.41194479e-03  1.02401473e-01  1.60666108e-01
 -2.53336310e-01 -1.74929753e-01 -2.01159522e-01 -2.37260938e-01
 -7.45233940e-03  2.87744403e-02 -1.25385404e-01 -6.47072732e-01
  1.59768701e-01 -1.75778717e-02 -3.15382540e-01  1.91298395e-01
 -2.02555701e-01 -5.34174033e-02 -8.62825587e-02 -2.22623855e-01
  9.09008756e-02  3.65879506e-01  9.32197869e-02  2.53999054e-01
  5.94088852e-01 -1.07057773e-01 -1.44094408e-01 -5.46054304e-01
  2.48182900e-02  2.86963642e-01 -1.80556417e-01 -2.12689847e-01
  4.06956732e-01  2.54923142e-02 -3.48881394e-01 -2.66953230e-01
 -4.76399623e-02  1.93151206e-01 -1.56051010e-01 -1.32631034e-01
 -5.62763959e-02 -8.39507729e-02  1.68936968e-01 -1.32903516e-01
  4.30754930e-01 -5.01050234e-01 -5.35497330e-02  3.01628530e-01
  2.17909947e-01  3.69877405e-02 -1.55125093e-03  3.58193159e-01
 -3.79618481e-02  1.30579516e-01 -1.07467286e-01  1.89612731e-01
 -1.02276333e-01  3.46963227e-01 -8.38365406e-02 -7.57577866e-02
  4.51882452e-01 -4.98874746e-02  9.23466831e-02  2.29319595e-02
  8.64854679e-02  2.40970775e-03 -2.35923275e-01  1.91491857e-01
  8.61895531e-02 -4.02369015e-02 -6.03247657e-02  6.30089268e-02
  3.19827162e-02 -1.34431124e-01 -9.90615040e-02 -2.94674814e-01
 -1.00709312e-01 -3.79956029e-02  2.85666436e-02 -2.12104633e-01
 -1.20645240e-01  1.50368810e-01 -2.53617227e-01  6.03963062e-03
  1.19081177e-01  7.38504529e-02 -2.41774485e-01  2.37648785e-01
  8.38550180e-02 -2.86004037e-01 -1.46565437e-01 -2.13432491e-01
  1.19107507e-01 -1.81889698e-01 -3.47872794e-01 -2.00140357e-01
 -2.17981771e-01  3.47344935e-01  9.61623192e-02 -9.34079066e-02
  2.84575582e-01 -3.72496992e-02  4.42639589e-01 -6.05168864e-02
 -1.46385014e-01 -1.33347109e-01 -2.53286362e-01  7.80195296e-02
 -5.88290811e-01  7.95104057e-02  7.39821047e-02 -1.55587986e-01
  2.42403984e-01  7.51282945e-02  2.08341151e-01 -1.51948601e-01
 -1.70602277e-01 -4.76278439e-02 -2.47332796e-01  4.43670094e-01
 -2.52653882e-02  7.77874291e-02  2.25839794e-01  5.28056622e-01
  2.88285673e-01 -2.59626806e-02 -5.24128117e-02 -2.89214551e-02
  1.83290824e-01  2.53001630e-01  3.25583577e-01  3.86016726e-01
  1.11823156e-01  4.81209844e-01 -2.34691575e-01  2.37528101e-01
  2.92632394e-02  7.91056827e-02  2.05038369e-01 -1.26254112e-01
  5.44447362e-01 -4.41961825e-01  1.99306324e-01 -5.46287335e-02
  2.84178078e-01 -1.31138146e-01  8.83126818e-03  1.80619419e-01
 -4.07677889e-02  2.86804110e-01 -1.16787151e-01 -9.02067646e-02
  1.86253607e-01 -1.07824124e-01 -2.34212816e-01 -6.67333364e-01
 -3.89213860e-02 -1.29611224e-01 -1.69167191e-01 -1.47999614e-01
  2.22482830e-02 -1.02748796e-01 -1.00725859e-01  5.59992902e-02
  2.03126281e-01  2.57475913e-01  1.48655087e-01  9.08680782e-02
 -1.05375141e-01 -3.62886071e-01  6.46683425e-02 -2.17669532e-01
 -9.30718184e-02  7.37764016e-02  2.16635913e-01  1.66402638e-01
  5.99637628e-01 -1.81214750e-01  2.78174400e-01  2.16815665e-01
 -4.20445502e-01  4.19138879e-01 -1.41253620e-02  1.16574965e-01
 -4.09416497e-01  4.47177708e-01  9.36532617e-02  3.72120738e-03
  4.03302908e-03 -3.70758027e-01 -3.76008004e-01  1.06777817e-01
  1.34488821e-01 -1.35190651e-01 -3.45021099e-01 -3.80864859e-01
  1.87692493e-02 -1.67620536e-02 -2.77800739e-01 -7.31258988e-02
 -2.91733831e-01  1.04460128e-01 -3.73070270e-01  5.01428545e-02
 -3.01431239e-01  2.25163415e-01 -8.92104656e-02 -3.73612344e-01
 -5.84354624e-03 -2.44133323e-01 -5.58989011e-02 -2.08893657e-01
  1.29787371e-01 -2.99971521e-01  3.19218636e-01  3.10476214e-01
 -1.62755065e-02  6.77965581e-02 -2.62487121e-02  1.71326488e-01
 -1.20185457e-01  2.78303735e-02 -5.03459796e-02  4.66371417e-01
 -1.75962150e-01  1.23108231e-01  2.01878827e-02  1.81686461e-01
 -1.42267391e-01  1.74953163e-01 -3.84525955e-01  1.05244078e-01
 -6.58352748e-02  8.50632414e-02 -3.23299676e-01  2.16016710e-01
  1.29298836e-01  3.50925684e-01  2.21678764e-02  2.27079485e-02
 -1.34126633e-01  2.43800342e-01  4.00328875e-01 -3.24113369e-01
 -1.89141095e-01 -2.58468203e-02  2.09044311e-02  1.32822096e-01
  7.83640221e-02  9.85365063e-02 -7.07018189e-03  6.19599149e-02]"
The model does not save and load correctly when containing `tf.keras.layers.experimental.preprocessing.StringLookup` layer stat:awaiting response type:bug stale comp:keras,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0-rc1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The model does not save and load correctly when containing `tf.keras.layers.experimental.preprocessing.StringLookup` layer.
It seems that the `vocabulary` is not saved or loaded correctly, which is empty when loading the model.
This behavior may relate to #61369, but different API endpoint.


### Standalone code to reproduce the issue

```python
import pickle
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)

model_input = tf.keras.Input(shape=(1,), dtype=tf.int64)
lookup = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=['a', 'b'])(model_input)
output = tf.keras.layers.Dense(10)(lookup)
full_model = tf.keras.Model(model_input, output)

# this part works
try:
    model_bytes = pickle.dumps(full_model)
    model_recovered = pickle.loads(model_bytes)
except Exception as e:
    print(""Failed! Error:"", e, flush=True)
else:
    print(""Success!"", flush=True)

# this part throws an error
try:
    full_model.save(""/tmp/temp_model"")
    full_model_loaded = tf.keras.models.load_model(""/tmp/temp_model"")
    model_bytes = pickle.dumps(full_model_loaded)
    model_recovered = pickle.loads(model_bytes)
except Exception as e:
    print(""Failed! Error:"", e, flush=True)
else:
    print(""Success!"", flush=True)
```


### Relevant log output

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
Success!
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
Failed! Error: Error when deserializing class 'StringLookup' using config={'name': 'string_lookup', 'trainable': True, 'dtype': 'int64', 'invert': False, 'max_tokens': None, 'num_oov_indices': 1, 'oov_token': '[UNK]', 'mask_token': None, 'output_mode': 'int', 'sparse': False, 'pad_to_max_tokens': False, 'idf_weights': None, 'vocabulary': [], 'vocabulary_size': 3, 'encoding': 'utf-8'}.

Exception encountered: Cannot set an empty vocabulary, you passed [].
```
",True,"[-0.47088796 -0.36770153 -0.18435872  0.01885058  0.25553203 -0.32357657
 -0.21764737 -0.02647397 -0.30646196 -0.16882387  0.38078856 -0.15905192
 -0.13641003  0.20880094 -0.2751915   0.27854705 -0.20009424 -0.02002248
  0.08996023  0.3753366  -0.07714288 -0.27820304  0.00415339  0.11651087
  0.2320996   0.11407559 -0.10532513 -0.01006452 -0.01266501  0.08945649
 -0.00228664  0.05797579 -0.18534485  0.03971491  0.10541809  0.21823771
 -0.27518964 -0.19170691 -0.38545382  0.09459316  0.08349354 -0.03678549
  0.02098353 -0.2981557   0.00357857 -0.16424504  0.05421021 -0.27312565
 -0.14197503 -0.32933992 -0.0299096  -0.0554678  -0.45814037 -0.61583877
 -0.14216256 -0.07824861  0.04732678 -0.13715431 -0.14060369  0.02640661
  0.00571815  0.06556551 -0.04840168 -0.03786964  0.36525095  0.17095822
  0.2066141   0.15283388  0.5441077  -0.23261271  0.04270364 -0.07315814
 -0.38550287  0.07877033  0.08226535  0.1439021   0.09475452  0.02852104
  0.26651645 -0.17896697  0.05689434 -0.463958   -0.33601695 -0.1387493
  0.06764532 -0.15148328  0.5615258  -0.14162958  0.3630833  -0.16691
  0.53724563  0.3270512  -0.10147803  0.11287098  0.38998345  0.23101604
 -0.01760075  0.26308954 -0.04660299 -0.1005397   0.12338129 -0.17941263
  0.04117751  0.16813792 -0.03971898 -0.26856774  0.10227847 -0.2767246
  0.1733361   0.07440621  0.24680181 -0.02623765  0.10293423 -0.08899847
  0.00160271 -0.14646403  0.04808863  0.215675   -0.09891481  0.7262401
  0.11670341 -0.28992182  0.15309833  0.06777215  0.4781613   0.20404297
 -0.19878912  0.08990507  0.00817948 -0.20705734  0.3302335   0.05226819
  0.03876751 -0.01599286 -0.18522161  0.07456385 -0.17342344 -0.07192544
 -0.49492675 -0.1534954  -0.34557632  0.1065194  -0.20419809 -0.4684966
  0.17750397  0.21210173 -0.2008082   0.24821232 -0.08429256  0.02380021
  0.0108981  -0.12168097 -0.03870609  0.23990445  0.21839783 -0.06821886
  0.1468635   0.11960548  0.1327863  -0.43660325 -0.0972894   0.2614709
 -0.11973796 -0.19191214  0.37162843  0.18121776 -0.20219713 -0.21321076
  0.12670228  0.3691501   0.05826677  0.06209819  0.11924528 -0.14336559
  0.28587145 -0.07859719  0.08918952 -0.59106886 -0.03022929  0.11029714
  0.26603264  0.21030022  0.06972263  0.10752726  0.07749949 -0.2737588
  0.21100324  0.06529817 -0.29069382  0.06099391 -0.474912   -0.16365308
  0.4847633  -0.02425782 -0.01398189 -0.04351118  0.12193353 -0.15065704
 -0.16468999 -0.09839405 -0.16860472 -0.22181657 -0.10786323 -0.12910283
 -0.07236527 -0.37910056 -0.11324184 -0.41333595 -0.17949697  0.18771508
 -0.0536092  -0.7617863   0.13499242  0.01237249 -0.2793231   0.3561609
  0.09137164  0.09481837  0.12783013  0.30668855  0.26312768 -0.13142921
  0.0547869  -0.39483017 -0.24875778  0.17838836 -0.25571415  0.12957749
  0.11843576  0.08428353  0.1751032   0.11079574  0.2321985   0.18774053
  0.3185212  -0.2538579   0.00240123 -0.04944667 -0.12553805 -0.09766345
 -0.60590494 -0.20832282  0.09193745 -0.29970983  0.1437847   0.34367007
 -0.0641435  -0.0202656  -0.36388272  0.23268014 -0.48372024  0.21779236
  0.4939374   0.1904854   0.426978    0.06886076  0.14389671  0.15162128
  0.2805     -0.02498754  0.6059332   0.13572502  0.20310779  0.26538554
  0.22394744  0.2173851  -0.4660191   0.47480363 -0.0597282  -0.1934433
  0.25736904 -0.5247138   0.5881029  -0.33283162  0.24049105  0.03026332
  0.34137207  0.07637796 -0.06573941  0.13852438  0.02178423  0.2724456
 -0.49180248  0.05382604  0.07426894 -0.3447359  -0.05892324 -0.6067622
 -0.06305628  0.01749335 -0.10944025  0.13338053 -0.00225443 -0.1448071
 -0.3341925   0.17187041  0.05118231 -0.19308718  0.24705815  0.18054223
 -0.3507119   0.01278841  0.57200897 -0.22853081 -0.29523996 -0.05741461
  0.40693516  0.24940847  0.42903954 -0.3763821   0.07728858  0.07883459
 -0.03769545  0.40752175  0.02652763  0.02687701 -0.2249333   0.6542728
  0.03289244 -0.04417063  0.24095082 -0.08034151 -0.19114524  0.12195196
  0.25291336 -0.09700224  0.09240189 -0.3824754   0.25844124  0.08759786
 -0.14147404  0.08519369 -0.04749326  0.25668725 -0.09690625 -0.18030804
 -0.3667295   0.39575577 -0.04115032 -0.3333782  -0.19613507 -0.01498634
 -0.25834325 -0.19585696  0.12973663 -0.26267466  0.11707358  0.71214646
 -0.04285406  0.25035965  0.09904902  0.05268784 -0.4314973   0.15429997
 -0.17956796  0.43407315  0.04095723 -0.18632802  0.28965092  0.40811434
 -0.16242601 -0.10348488 -0.23369534  0.06219022  0.2579869  -0.13420202
 -0.03802588 -0.44958135  0.01253077  0.31366816 -0.11633352  0.16029507
 -0.14808832  0.13623968  0.4507087  -0.48716658 -0.12562217  0.179172
  0.30491608  0.13333425  0.09206543 -0.23326525 -0.09901442 -0.00956131]"
"XLA compiled tf.where with known output shape error (set_shape, tf.concat / tf.stack) type:bug comp:xla TF 2.13","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Output of `tf.where`, when used inside `tf.function` with `jit_compile=True`, can sometimes be used correctly (as with sum), and sometimes raises shape mismatch error (as with concatenation). This error is present even if output shape is set manually with `set_shape`. 

The code below runs without `jit_compile` or with sum instead of `tf.concat`, and only fails if concatenating inside a compiled function. 

Note: `autoclustering` solves the issue on the toy example, but _not_ on the codebase I am working on.

### Standalone code to reproduce the issue

Colab: https://colab.research.google.com/drive/1FuboVMSao8eCZLcZ2F7Fdsa1UvlnHQmG?usp=sharing

```python
import tensorflow as tf

def fun(x, y):
    x = tf.where(x == 1)
    print(f'Shape before (unknown): {x.shape}') 
    x.set_shape(shape=[y.shape[0], 2])
    print(f'Shape after (known): {x.shape}') 
    return tf.concat([x,y], axis=1)  # Concatentation fails
    # return x + y  # Sum would succeed

x = tf.constant([[0,0,1,1,0],
                  [0,1,0,1,0],
                  [1,0,0,0,1],
                  [1,0,1,0,0],
                  [0,1,1,0,0],], dtype=tf.int32)
y = tf.expand_dims(tf.range(x.shape[0] * 2, dtype=tf.int64), axis=-1)

fun(x, y)

tf.function(fun)(x,y)

tf.function(fun, jit_compile=True)(x,y)  # Fails as described above
```


### Relevant log output

```shell
Shape before (unknown): (None, 2)
Shape after (known): (10, 2)
InvalidArgumentError: Cannot concatenate arrays that differ in dimensions other than the one being concatenated. Dimension 0 in both shapes must be equal: s64[<=25,2] vs s64[10,1].
```
",True,"[-6.71460271e-01 -3.17543238e-01  8.26896913e-03 -1.56806439e-01
  1.10746183e-01 -2.92414010e-01 -5.47404066e-02  3.90230790e-02
 -2.71210402e-01 -2.22534984e-01 -9.69163701e-02 -1.45360291e-01
 -1.14009649e-01 -1.53970391e-01 -2.09082171e-01  4.03725207e-01
 -3.16115826e-01 -1.82229817e-01  2.37018287e-01  4.13461067e-02
  4.29668128e-02 -2.15079278e-01 -2.83401310e-01  3.24980229e-01
  1.75224096e-01  1.79471284e-01 -2.96766281e-01 -4.48648706e-02
 -1.02870576e-01  4.54765037e-02  1.18400224e-01  2.19088435e-01
 -1.37477949e-01  1.03009880e-01 -3.93273681e-02  2.43747607e-01
 -2.66315550e-01 -1.28950000e-01 -3.64063263e-01  4.42209803e-02
 -1.36331636e-02 -6.27288371e-02  8.61428678e-02 -3.66932064e-01
  2.80659676e-01 -8.69942978e-02  4.81750630e-02 -9.74419564e-02
 -7.45921880e-02  1.66283399e-01  3.92593294e-02  1.00788381e-02
 -2.98272550e-01 -3.59404802e-01 -9.12578776e-03  2.40534723e-01
 -9.95381474e-02  1.05659114e-02  2.27253586e-01  3.65873516e-01
  1.67511612e-01  7.17373937e-02 -2.48346813e-02  2.48691533e-03
  3.42166483e-01  1.48545429e-01  2.29763880e-01  1.31347865e-01
  3.18701923e-01 -2.71046221e-01  1.47092372e-01  7.27827661e-03
 -4.92624432e-01  7.20282197e-02  9.85620543e-03  1.13472849e-01
 -1.74845587e-02  6.36696070e-02  6.00382313e-02 -3.92558396e-01
  8.79315883e-02 -4.66781557e-02  6.24210872e-02  1.03567846e-01
  4.08025593e-01 -1.80619776e-01  3.66976678e-01  1.61287799e-01
  4.90527451e-01 -2.50912365e-02  3.27596188e-01  2.74616122e-01
 -1.05445236e-01  2.96891361e-01  4.00356174e-01  8.68605226e-02
  1.11771196e-01  1.66046247e-01  5.91776893e-02  5.70904054e-02
 -1.52902275e-01 -4.96425927e-02  2.04283595e-01 -3.24555598e-02
  1.85293448e-03 -1.48294508e-01  2.20297247e-01  1.70918554e-03
 -2.45385300e-02 -1.03745289e-01  4.60175797e-02 -1.44939691e-01
  2.60865957e-01 -8.11925083e-02 -4.27854657e-02  3.60516366e-03
 -2.02951580e-02 -1.96280241e-01  7.67637938e-02  4.50007081e-01
  7.50208423e-02 -2.74288356e-01 -6.07881397e-02  4.49084967e-01
  3.59053612e-01  2.50881780e-02 -3.49333465e-01  1.74973104e-02
 -4.60462943e-02  5.14106266e-03  1.46346390e-01  1.84254706e-01
  6.14367723e-02  1.88125461e-01 -3.42206582e-02  1.66315675e-01
 -4.61764127e-01 -2.34796792e-01 -2.40709439e-01 -1.75108910e-01
 -2.61867225e-01  2.89514124e-01 -9.51999500e-02 -4.16433156e-01
  3.70636880e-01  1.08390421e-01 -3.61464649e-01  2.75631636e-01
 -1.66685998e-01 -2.57432878e-01 -6.07010797e-02  1.33570984e-01
 -2.23647416e-01  5.97971201e-01 -6.68044463e-02  1.06114857e-01
  2.42295578e-01 -3.36968452e-02  6.97618499e-02 -7.22555101e-01
  3.43948036e-01  2.97121108e-01 -4.71793525e-02 -1.29299402e-01
  3.83486552e-03  1.65858239e-01 -5.30629456e-01 -5.80907613e-02
  1.18837178e-01  3.20899606e-01  2.01245070e-01 -2.88459063e-01
  3.95256653e-03  2.80133843e-01  2.21454844e-01 -3.18214506e-01
  2.75341183e-01 -4.41970229e-01 -3.07791624e-02  3.84878099e-01
  1.93082929e-01  3.51384580e-02  1.74775422e-01  1.72877729e-01
 -1.53640091e-01  1.03025764e-01  8.50790739e-02  3.34101379e-01
 -1.05601296e-01  2.66415387e-01 -4.67141151e-01  4.20256518e-03
  1.48058653e-01  8.81352127e-02 -1.55342117e-01  1.14182360e-01
  1.76917762e-01 -3.20171297e-01  6.41931593e-02  1.70346692e-01
  8.57634842e-03 -4.48831245e-02  1.67517960e-01 -1.81137864e-02
  3.77139337e-02 -3.38643670e-01 -8.65688846e-02 -4.33012456e-01
 -3.57956707e-01 -5.12270853e-02  1.52464017e-01 -6.06295884e-01
 -2.99864225e-02 -4.07408215e-02  5.80849797e-02  3.56320918e-01
 -3.30436192e-02  1.18739143e-01 -4.25567031e-01  1.22753695e-01
  7.44783580e-02 -5.77378124e-02  5.91410473e-02 -4.12515402e-01
 -9.20988098e-02  1.00939982e-01 -3.01680803e-01 -1.85187995e-01
 -1.50821870e-02  3.93976212e-01  2.51272291e-01  9.28638726e-02
  4.69760358e-01  2.52389073e-01  5.97683966e-01 -1.30734503e-01
 -1.76360905e-01 -3.24280024e-01 -1.31173119e-01 -9.06529874e-02
 -5.19599736e-01 -3.24514806e-01 -2.24628180e-01 -1.83889210e-01
  2.88403064e-01  1.51955068e-01  3.06786187e-02 -1.51857018e-01
 -2.67578423e-01  5.27881742e-01 -2.71086812e-01  1.93857744e-01
  1.17359504e-01  2.47512668e-01  1.99497193e-01  3.29677701e-01
  3.17722380e-01  1.62239522e-01  2.73857594e-01 -1.17675513e-01
  3.48849297e-01  1.90280557e-01  1.68630853e-02  3.31212342e-01
 -9.00816359e-03  3.27587515e-01 -1.81460768e-01  3.82323146e-01
 -3.50557379e-02  1.55393273e-01  1.87411577e-01 -3.85212004e-01
  7.61129141e-01 -3.36302757e-01  1.50077254e-01  4.73436527e-02
  3.42447102e-01 -2.43652612e-03  1.92006618e-01  9.38717723e-02
 -1.35233954e-01  3.72560978e-01 -4.12245214e-01 -4.82835397e-02
 -9.79800224e-02 -1.13646194e-01 -2.60843933e-01 -8.60233665e-01
 -9.66072157e-02 -2.01857895e-01 -1.50487244e-01 -2.99543180e-02
  3.27317491e-02 -4.57668491e-02 -2.64004141e-01  1.65671960e-01
 -8.93808901e-05 -1.98051706e-03  1.10612452e-01  3.80566344e-02
 -2.40053892e-01  1.34798512e-02  1.94609329e-01 -3.21332693e-01
 -1.07110348e-02 -2.32047454e-01  4.25955057e-01  3.37366819e-01
  4.04314816e-01 -3.86143744e-01  1.99789047e-01 -1.44400448e-01
 -2.01566383e-01  6.24247074e-01  3.01133953e-02  2.50795752e-01
 -4.69343722e-01  6.75395012e-01  7.88092464e-02  5.32645099e-02
  4.71528061e-02 -4.52307522e-01 -5.07502198e-01 -2.29895264e-01
  1.41837195e-01 -7.74727091e-02 -6.68499619e-03 -1.11755058e-01
 -5.90595119e-02 -2.15855185e-02 -3.71573389e-01 -9.40287635e-02
 -1.72293678e-01 -1.20154887e-01 -3.45217526e-01 -3.62439275e-01
 -2.91642338e-01  6.24713004e-02 -8.10548142e-02 -4.10672963e-01
 -3.32927346e-01 -1.26506776e-01 -2.30917841e-01 -3.09752584e-01
  2.90814936e-02 -5.66898227e-01  2.61039734e-01  2.81937182e-01
 -1.03817835e-01  2.86779821e-01  2.33266994e-01  1.70701206e-01
 -2.19717741e-01 -2.82177906e-02 -4.95582297e-02  2.65931547e-01
 -1.97494216e-03 -5.19810207e-02  2.86864191e-01  2.01436982e-01
  5.81697412e-02  1.36776790e-02 -2.55813569e-01  9.05867368e-02
  2.72437423e-01 -1.65631711e-01 -2.17013955e-01 -2.50341117e-01
  2.86617935e-01  2.66896367e-01 -8.26502219e-02  2.84050345e-01
 -3.21723133e-01  3.13740313e-01  5.60279012e-01 -4.50426370e-01
 -1.17630847e-01  1.28598973e-01  4.35931802e-01 -1.72696233e-01
 -8.59803557e-02 -1.73649620e-02  1.95318371e-01 -9.03581679e-02]"
incompatible shapes error stat:awaiting response type:bug stale comp:model TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm trying to train a model to do binary classification.  I am getting an error about incompatible shapes, but my data seems to be in the correct shape. 









































































































































































































































































































































































### Standalone code to reproduce the issue

```shell
!pip install transformers

import tensorflow as tf
import tensorflow_hub as hub
from transformers import LongformerTokenizer, TFLongformerForSequenceClassification
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import LearningRateScheduler
import math
import numpy as np

positive_sentences = [
    ""The weather is absolutely gorgeous today."",
    ""I'm so grateful for the support of my friends and family."",
    ""I achieved my personal best in the race!"",
    ""The new cafe in town serves amazing coffee."",
    ""I love spending time with my adorable pets."",
    ""I received a surprise gift from a dear friend."",
    ""The sunrise this morning was breathtaking."",
    ""I'm excited about the upcoming vacation."",
    ""The concert last night was incredibly entertaining."",
    ""I'm proud of my hard work paying off."",
    ""The park is a peaceful place to relax."",
    ""I found a great book that I can't put down."",
    ""The team's collaboration led to a successful project."",
    ""I'm enjoying learning a new skill."",
    ""Spending time with loved ones always brightens my day."",
    ""I got a promotion at work, and it's a fantastic feeling."",
    ""The movie I watched last night was heartwarming."",
    ""I'm making positive changes in my daily routine."",
    ""The delicious aroma of home-cooked food fills the air."",
    ""I'm surrounded by inspiring and supportive people.""
]

negative_sentences = [
    ""The constant rain is making me feel gloomy."",
    ""I'm disappointed that my plans got canceled."",
    ""The traffic was horrendous this morning."",
    ""I made a mistake on the important presentation."",
    ""I'm feeling overwhelmed with work and tasks."",
    ""The internet connection is frustratingly slow."",
    ""The food I ordered was cold and tasteless."",
    ""I'm exhausted after a long and stressful day."",
    ""My phone battery died at the worst time."",
    ""The store was out of stock of the item I needed."",
    ""I lost my wallet and it's been a hassle."",
    ""The loud construction noise is giving me a headache."",
    ""I'm struggling to meet my deadlines."",
    ""The movie I was looking forward to was disappointing."",
    ""I'm not feeling well and it's affecting my mood."",
    ""My computer crashed and I lost my unsaved work."",
    ""The rude customer service ruined my experience."",
    ""I'm frustrated with the constant delays."",
    ""The rainy weather is putting me in a bad mood."",
    ""I'm stressed about the upcoming exams.""
]

sentences = positive_sentences + negative_sentences

labels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
    ,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1
]

df = pd.DataFrame({'sentences':sentences,'labels':labels})

data = df.copy()

display(data.head(5))

tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')
longformer_model = TFLongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096')

X_train, X_test, y_train, y_test = train_test_split(df['sentences'],df['labels'], stratify=df['labels'])
X_train.head(4)
display(y_train)

x_train_tokens = tokenizer(list(X_train), padding=True, truncation=True, return_tensors=""tf"")
x_test_tokens = tokenizer(list(X_test), padding=True, truncation=True, return_tensors=""tf"")

y_train_encoded = np.array(y_train)
y_test_encoded = np.array(y_test)

display(x_train_tokens)

input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)
attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32)
outputs = longformer_model(input_ids, attention_mask=attention_mask)[0]
output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name=""output"")(outputs)

model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[output_layer])

def lr_schedule(epoch):
    initial_lr = 0.001  # Set your initial learning rate here
    drop = 0.75
    epochs_drop = 5  # Adjust this value based on your preference
    lr = initial_lr * math.pow(drop, math.floor((1 + epoch) / epochs_drop))
    return lr

lr_scheduler = LearningRateScheduler(lr_schedule)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(
    [x_train_tokens['input_ids'], x_train_tokens['attention_mask']]
    , y_train
    , epochs=5
    , batch_size=5
    , validation_data=([x_test_tokens['input_ids'], x_test_tokens['attention_mask']], y_test)
    , callbacks=[lr_scheduler]
)
```


### Relevant log output

```shell
Epoch 1/5

---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

<ipython-input-12-24f08ed0005f> in <cell line: 3>()
      1 model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
      2 
----> 3 history = model.fit(
      4     [x_train_tokens['input_ids'], x_train_tokens['attention_mask']]
      5     , y_train

1 frames

/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Graph execution error:

Detected at node 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1' defined at (most recent call last):
    File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
      handle._run()
    File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
      lambda f: self._run_callback(functools.partial(callback, future))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
      self.ctx_run(self.run)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
      self.do_execute(
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
      result = self._run_cell(
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-12-24f08ed0005f>"", line 3, in <cell line: 3>
      history = model.fit(
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1742, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1084, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1'
Detected at node 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1' defined at (most recent call last):
    File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py"", line 16, in <module>
      app.launch_new_instance()
    File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
      self.io_loop.start()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
      self.asyncio_loop.run_forever()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
      handle._run()
    File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
      lambda f: self._run_callback(functools.partial(callback, future))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
      ret = callback()
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
      self.ctx_run(self.run)
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
      yielded = self.gen.send(value)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
      yield gen.maybe_future(dispatch(*args))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
      yield gen.maybe_future(handler(stream, idents, msg))
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
      self.do_execute(
    File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
      yielded = ctx_run(next, result)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
      res = shell.run_cell(code, store_history=store_history, silent=silent)
    File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
      result = self._run_cell(
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
      return runner(coro)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
      coro.send(None)
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
      if (await self.run_code(code, result,  async_=asy)):
    File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-12-24f08ed0005f>"", line 3, in <cell line: 3>
      history = model.fit(
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1742, in fit
      tmp_logs = self.train_function(iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1338, in train_function
      return step_function(self, iterator)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1322, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1303, in run_step
      outputs = model.train_step(data)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py"", line 1084, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 543, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File ""/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/optimizer.py"", line 276, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1'
2 root error(s) found.
  (0) INVALID_ARGUMENT:  Incompatible shapes: [5,512,12,514] vs. [5,512,12,513]
	 [[{{node gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1}}]]
	 [[model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._6/attention/self/cond_2/pivot_t/_676/_1027]]
  (1) INVALID_ARGUMENT:  Incompatible shapes: [5,512,12,514] vs. [5,512,12,513]
	 [[{{node gradient_tape/model/tf_longformer_for_sequence_classification/longformer/encoder/layer_._0/attention/self/BroadcastGradientArgs_1}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_131688]
```
",True,"[-6.56136632e-01 -2.80091316e-01 -1.49784803e-01  7.43823946e-02
  1.23993345e-01 -3.55168998e-01 -7.72526115e-02  1.67847704e-02
 -3.25646818e-01 -3.89962345e-01  9.23461467e-02 -1.71528578e-01
 -1.84283257e-01 -6.62517026e-02 -8.98339376e-02  2.33766556e-01
 -3.27747464e-01 -5.90951443e-02  3.18083882e-01  1.85421675e-01
 -3.18453491e-01 -3.62864994e-02 -3.37506324e-01  1.83593318e-01
  1.03273451e-01  1.35732815e-01 -2.27034271e-01  9.77624878e-02
  2.76575033e-02  8.95356536e-02  2.77580917e-01  1.03198364e-01
 -9.13587511e-02  2.00302005e-01  8.29187185e-02  2.88469959e-02
 -3.56710017e-01 -2.16438562e-01 -2.83957273e-01 -8.84864852e-02
 -1.29027188e-01  3.56792659e-02  2.34379038e-01 -4.42508966e-01
  3.44154567e-01 -1.08022459e-01 -1.91161394e-01 -1.05035275e-01
 -2.87755430e-02 -6.51737452e-02  3.62177119e-02 -2.05136001e-01
 -4.80050802e-01 -3.79985809e-01 -1.33813620e-01  8.11259672e-02
  9.86874327e-02 -2.82344539e-02 -7.37241805e-02  1.97190985e-01
  1.69341937e-01 -3.26472335e-02 -1.60525888e-02 -1.92990124e-01
 -1.79879777e-02  1.96883947e-01  3.13200116e-01 -7.91430026e-02
  4.36046422e-01  1.00458041e-02  1.75537944e-01 -6.94069341e-02
 -3.80071223e-01 -2.54872125e-02  2.31119514e-01  1.00265648e-02
  7.19322562e-02  1.79550767e-01  2.61872560e-01 -2.50832111e-01
 -1.96874499e-01 -3.62170517e-01 -1.65913701e-01 -2.76675999e-01
  3.22621346e-01 -3.09275031e-01  3.67684990e-01  5.91313727e-02
  4.11264479e-01 -1.62613958e-01  3.96999180e-01  4.88034785e-01
 -8.94733667e-02  3.60034704e-01  4.20115709e-01  2.73496658e-01
  1.39089692e-02  1.51522368e-01  1.77939519e-01 -4.98380959e-02
 -2.68409967e-01 -1.29524514e-01  1.17278099e-01  3.72059256e-01
 -6.90593943e-02 -8.69368166e-02  1.34097904e-01 -1.37660295e-01
  9.15092081e-02  3.13850679e-02  1.98318332e-01 -6.77060932e-02
  2.34723687e-01 -6.62336797e-02 -8.76188800e-02 -4.79407385e-02
 -5.75241223e-02 -3.90769169e-02 -6.78110719e-02  5.38324773e-01
 -5.05278781e-02 -1.17063411e-01  1.13958232e-02  1.56413302e-01
  5.32599390e-01 -2.20131576e-01 -3.34892392e-01 -1.90269928e-02
 -1.19569050e-02  1.83424018e-02  2.11957276e-01  1.12214319e-01
  1.04225338e-01  2.61390448e-01 -2.47920960e-01 -8.32835399e-03
 -2.75912941e-01  2.09761132e-02 -2.12341905e-01 -2.47024745e-01
 -2.97903776e-01  2.87784100e-01 -1.81101263e-03 -5.06588221e-01
  2.84683734e-01  3.11683938e-02 -4.19912934e-01  2.59959280e-01
 -2.72511125e-01  9.62038934e-02  1.03975259e-01  5.39765731e-02
 -2.06701875e-01  6.00958228e-01  1.60082370e-01  2.16001540e-01
  3.34514380e-01 -8.74626786e-02  5.66519648e-02 -7.81794786e-01
  3.20342809e-01  4.08842087e-01 -8.56639072e-03 -2.75314569e-01
  2.30777323e-01  2.16058359e-01 -4.25052732e-01 -7.10797161e-02
  2.15953737e-01  5.10211825e-01 -3.67962420e-02 -1.14636824e-01
  2.71849111e-02  2.33269066e-01  4.23552722e-01 -9.60696414e-02
  2.54062206e-01 -6.06969357e-01 -2.25362420e-01  4.12292659e-01
  6.55624196e-02  8.51494372e-02  1.43444002e-01 -2.96919532e-02
 -4.85890806e-02  7.83013776e-02 -3.20093520e-02  1.40808702e-01
 -2.05525160e-01  1.26429321e-02 -3.89759421e-01 -1.19229004e-01
  1.82656214e-01 -1.79788973e-02 -6.70813695e-02  1.33570671e-01
  2.04542279e-01 -3.13779175e-01  1.70292556e-01  5.03547639e-02
 -2.75062680e-01 -6.36187196e-02  1.60486382e-02 -1.63787097e-01
  1.27810329e-01 -3.88478249e-01  8.54316503e-02 -2.61842906e-01
 -3.66941154e-01  3.97198200e-01  3.69190350e-02 -8.08300614e-01
  7.47930706e-02 -1.00701466e-01 -1.95616871e-01  1.35323599e-01
  2.66712338e-01  1.41208887e-01 -3.31273735e-01  2.83489525e-01
  1.52180016e-01 -1.67485386e-01  1.84967861e-01 -4.32296157e-01
 -1.92779630e-01  2.70971924e-01 -2.93014824e-01 -7.42560476e-02
 -3.07043735e-02  1.40007228e-01  1.99832246e-01  1.69229001e-01
  4.22771811e-01  3.36120188e-01  4.17337418e-01 -1.56827480e-01
 -1.23329014e-01 -1.88470766e-01 -3.01921889e-02 -6.83268905e-02
 -4.69245076e-01 -3.36318552e-01 -6.76591545e-02 -9.25069302e-02
  5.65409541e-01  4.19396162e-01 -2.72666991e-01 -1.28677301e-02
 -4.32714403e-01  4.07318264e-01 -4.31343079e-01  9.02778059e-02
  3.06564867e-01  2.55740434e-01  3.09824616e-01  1.25728488e-01
  4.00320590e-01  2.43341416e-01  2.68641323e-01 -2.39677101e-01
  3.09644520e-01  2.58853525e-01  1.21667683e-01  1.29859611e-01
  2.61588395e-01  4.01564419e-01 -3.43868166e-01  5.49748898e-01
 -8.58913958e-02 -1.81493998e-01  2.71846890e-01 -3.77492845e-01
  5.18870175e-01 -5.50178826e-01  1.54030293e-01  5.60476258e-02
  2.83212364e-01 -9.73919183e-02 -2.49866962e-01  7.48991221e-02
  1.10222716e-02  2.10808769e-01 -3.92624795e-01  2.78152168e-01
 -1.23862568e-02 -1.83733717e-01 -8.67714137e-02 -7.90486157e-01
 -1.65676445e-01 -2.64807371e-03 -1.33356184e-01  1.54881746e-01
  1.56956777e-01 -1.66566610e-01 -2.58766502e-01  1.25028580e-01
 -6.88937679e-03 -1.34016752e-01  1.29693836e-01  1.44979805e-01
 -2.44166896e-01  2.72504359e-01  3.99760008e-01 -5.45624852e-01
 -9.77087207e-03 -2.38134623e-01  5.18254399e-01  4.76292521e-01
  2.67336637e-01 -4.08169836e-01  6.77052364e-02 -1.29601747e-01
 -1.53829902e-01  7.52180696e-01  2.51612719e-03  1.39808178e-01
 -2.04328120e-01  8.21047366e-01  7.13974461e-02  2.01491788e-02
  2.40793407e-01 -2.23302722e-01 -2.51325965e-01 -2.18010813e-01
  7.69214556e-02 -1.54753610e-01  3.95374484e-02 -3.64933491e-01
 -8.57953280e-02  3.29960793e-01 -1.57015175e-01 -1.56688690e-01
  5.25457636e-02 -2.28355061e-02 -2.24479303e-01 -2.40064248e-01
 -4.19840008e-01  3.53031933e-01  7.97255710e-02 -2.78629720e-01
 -4.03553545e-01 -8.76323581e-02 -2.44133174e-01 -1.56647354e-01
 -4.19945419e-02 -3.75407696e-01  2.12640598e-01  4.25758779e-01
 -1.34685844e-01  4.07267570e-01  1.47351280e-01  8.21536034e-02
 -3.79012853e-01  2.88786329e-02 -6.52126968e-04  4.31481972e-02
  7.40854666e-02 -4.47067358e-02  4.10033584e-01  3.29956889e-01
  8.58044438e-03  2.10354373e-01 -2.74890423e-01  4.26489599e-02
  1.12275265e-01 -3.14179361e-01 -1.76262915e-01 -4.48836505e-01
  6.50799498e-02  2.82328665e-01 -1.71887144e-01  2.67232031e-01
 -3.73450696e-01  2.05729783e-01  5.69306910e-01 -4.79057431e-01
 -8.62509012e-02  3.27868700e-01  1.79567471e-01  6.13208488e-02
  3.09605487e-02 -2.48042822e-01  1.98458761e-01 -6.32697567e-02]"
rnn with initial_state model can't be loaded with load_model  stat:awaiting response type:bug comp:keras TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A simple RNN with LSTMcell model.
I want to initialize the states with `initial_state_h` and `initial_state_c`. 
```
batch_size= 16
inputs = tf.keras.layers.Input(shape=(20,5),batch_size=batch_size)
units = 8
lstm_cell_fw = tf.keras.layers.LSTMCell(units)

initial_state_h = tf.random.normal(shape = (batch_size,units), mean=0., stddev=10., dtype=tf.dtypes.float32)
initial_state_c = tf.random.normal(shape = (batch_size,units), mean=0., stddev=10., dtype=tf.dtypes.float32)
lstm_layer_fw = tf.keras.layers.RNN(lstm_cell_fw, stateful=True, return_state=True, return_sequences=False)
outputs,states_h_fw, states_c_fw= lstm_layer_fw(inputs,initial_state = [initial_state_h,initial_state_c])

lstm_dense1 = tf.keras.layers.Dense(16, activation = 'relu')
lstm_dense2 = tf.keras.layers.Dense(2, activation = 'softmax')
out=lstm_dense2(lstm_dense1(outputs))

model = tf.keras.models.Model(inputs, out)
```
After compile and train, the model is saved with `model.save('my_model_test.keras')`.

```
model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])
model.summary()

xTrain = np.random.rand(96,20,5)
yTrain = np.random.rand(96,2)

for i in range(10):
  model.fit(xTrain, yTrain,batch_size=batch_size)

model.save('my_model_test.keras')

```
But when I try to load it with `load_model = tf.keras.models.load_model('my_model_test.keras')`, it gives error:
```
13 frames
[/usr/local/lib/python3.10/dist-packages/keras/src/backend.py](https://localhost:8080/#) in int_shape(x)
   1530     """"""
   1531     try:
-> 1532         shape = x.shape
   1533         if not isinstance(shape, tuple):
   1534             shape = tuple(shape.as_list())

AttributeError: 'float' object has no attribute 'shape'

```
I tried to save in other format, `.h5`, `.json`, etc. All give the same error.

But, if I don't use `initial_state` in `outputs,states_h_fw, states_c_fw= lstm_layer_fw(inputs)`, everything goes well. No problem with `load_model`.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1uKEpnddzeYSRG_1vKjtcQ4OLNwLuQeqy?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-7-8e0130abf25e> in <cell line: 1>()
----> 1 load_model = tf.keras.models.load_model('my_model_test.keras')

13 frames
/usr/local/lib/python3.10/dist-packages/keras/src/backend.py in int_shape(x)
   1530     """"""
   1531     try:
-> 1532         shape = x.shape
   1533         if not isinstance(shape, tuple):
   1534             shape = tuple(shape.as_list())

AttributeError: 'float' object has no attribute 'shape'
```
```
",True,"[-0.72245765 -0.46740615 -0.09699793  0.01795732  0.3594277  -0.09762038
 -0.30682302  0.03939809 -0.2544691  -0.21388839  0.17295241 -0.01866243
 -0.1866565   0.06430493 -0.26697522  0.07106534 -0.29210827 -0.05249185
  0.23838814  0.41903418 -0.00294211 -0.05692077 -0.23824374  0.14653128
  0.22312842  0.1390191  -0.18242788  0.14877257 -0.07774159  0.14582929
  0.21628237  0.06569719 -0.03684227  0.2538498   0.00382103  0.07375628
 -0.23466149 -0.29375115 -0.5482531   0.15156648  0.24499427 -0.12408674
  0.10360457 -0.23446448  0.09851831 -0.15280715  0.12994713 -0.26180243
  0.05188528 -0.321381   -0.14167887 -0.0174105  -0.43449658 -0.44262797
 -0.05250303 -0.30792975  0.18156452 -0.16770655 -0.21048847  0.00256432
 -0.14821878  0.03216413  0.01984454 -0.08746659  0.4335224   0.00332747
  0.22077069  0.15237674  0.49961707 -0.18262132  0.09046008 -0.11779687
 -0.45167792  0.1608479   0.07107519  0.07221912  0.05450473  0.24252518
  0.50300086 -0.10526513 -0.09746135 -0.28904545 -0.2953467  -0.18095382
 -0.15490954 -0.13656968  0.4621169   0.02378885  0.48477656 -0.26129004
  0.37328592  0.49932024 -0.30072308  0.14925458  0.6431285   0.25165582
 -0.14967106  0.12923531 -0.04654247 -0.10569437 -0.17066465 -0.09469034
 -0.04081062  0.20589855 -0.08347866 -0.4061427   0.11656764 -0.38066727
  0.05809792  0.05455086  0.22414792  0.15678465  0.16761968 -0.24789444
 -0.10412426 -0.14175352  0.05960873  0.0839466  -0.26689643  0.58538485
 -0.05593745 -0.31562752  0.04179029  0.11390215  0.42692167  0.34114537
 -0.28798184 -0.05139787  0.02865035 -0.27691516  0.3724401   0.08045846
  0.0508654   0.24463485 -0.10304914  0.09052469 -0.06619798  0.09430072
 -0.5944731   0.00613282 -0.27795756 -0.06203698 -0.29024124 -0.6874113
 -0.03193855  0.15721187 -0.09447463  0.10976321 -0.20608401  0.11940113
  0.0744117   0.11080667 -0.16564275  0.23798639  0.0688085  -0.05744009
  0.16619226 -0.06664108  0.07209395 -0.39138454 -0.10193207  0.19019575
 -0.20445997 -0.24462849  0.5262128   0.13272905 -0.2841314  -0.21634603
  0.25386295  0.42056042 -0.21138692  0.04952346 -0.12769493 -0.04182007
  0.13996431 -0.07631445  0.23332256 -0.62492836 -0.2433806   0.24317741
  0.18022361 -0.05180378  0.12387367  0.23351628 -0.0232498  -0.14403974
  0.07825866  0.09729355 -0.37745893 -0.01283255 -0.2778228  -0.08724535
  0.7179012   0.12337124 -0.1103788   0.05598344  0.3833238  -0.08496755
  0.07897937 -0.1856099  -0.29486406 -0.05471534  0.08334527  0.11399682
  0.23645358 -0.4331174  -0.1578389  -0.41002268 -0.34077334  0.04699734
  0.06543876 -0.34952337  0.17865941  0.10838781 -0.3989457   0.3608437
 -0.08310898 -0.09330228 -0.03121951  0.17824282  0.36919022 -0.18424243
  0.1476751  -0.4033187  -0.1957284   0.2287811  -0.46060458  0.02154227
  0.20740885  0.05980645  0.19900654  0.16884267  0.34373772  0.3063987
  0.5167529  -0.17197306 -0.02573591 -0.06453356 -0.0474868  -0.05640112
 -0.597013   -0.5792378   0.07541338  0.06695612  0.29671523  0.4621081
 -0.38028547  0.08571752 -0.38091362  0.22025156 -0.23983495  0.25527304
  0.3790822   0.12306826  0.5086837   0.05322371  0.340962    0.37166578
  0.27251613 -0.3111371   0.53909194  0.15551752  0.3426823   0.24110243
  0.49170703  0.1666266  -0.41703984  0.39876366  0.04018697 -0.28869894
  0.33911133 -0.46482474  0.64533895 -0.40970707  0.04892153 -0.04845443
  0.36396903  0.02184824 -0.03422263 -0.07712863  0.1150443   0.20139027
 -0.269432    0.27830946  0.11661639 -0.26152834 -0.24171332 -0.66432565
 -0.3641665   0.02486588 -0.13115248  0.05823775  0.27021068  0.1291749
 -0.31881788  0.21115524  0.29903376 -0.25472116  0.17519729  0.3043235
 -0.27592745 -0.00507106  0.6166283  -0.40151495 -0.5580691  -0.0701116
  0.3261681   0.22189298  0.57806844 -0.32985538  0.15309685  0.02965555
 -0.0641949   0.6837892  -0.14313689 -0.18636599 -0.30713236  0.83480173
 -0.10757399 -0.19164403  0.26904023 -0.16418353 -0.08864143  0.32040218
  0.36266983 -0.03837869 -0.00189746 -0.4092021   0.21577969  0.18967971
  0.01050169 -0.03171074  0.02039533 -0.02084512 -0.09636489 -0.10490681
 -0.219715    0.516437    0.00697904 -0.3617529  -0.24695878 -0.01925761
 -0.19942094 -0.23645984  0.11320816 -0.43389058  0.13454702  0.42134985
 -0.12869589  0.233928    0.03800534  0.11987005 -0.38085824  0.06412349
 -0.19043261  0.43811864 -0.07232985 -0.14524601  0.36063418  0.29362062
 -0.11033629  0.17817986 -0.2409063   0.05730693  0.04256144  0.02521969
 -0.22224262 -0.43443608  0.11693593  0.2507484  -0.0361426   0.0991522
 -0.03923861  0.18808356  0.40062946 -0.16397801 -0.24530263  0.3235346
  0.25407642  0.04804204 -0.04555405 -0.21238661 -0.03556941 -0.06467232]"
Will there a MLP model in the future version? type:bug,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When building deep learning models like Multi-Layer Perceptrons (MLPs), code reusability and conciseness are crucial factors. Currently, using `tf.keras.Sequential` in TensorFlow allows for convenient creation of sequential models. However, manually adding common operations such as Batch Normalization or Dropout to each layer can lead to code redundancy and an increased burden in terms of coding and maintenance. Therefore, proposing the addition of a feature in TensorFlow to directly create MLPs with Batch Normalization and Dropout is highly beneficial.

Here are several reasons why this feature would be advantageous for TensorFlow users:

1. **Simplified Code**: Users won't need to manually add Batch Normalization and Dropout operations to each layer, resulting in cleaner, more readable, and maintainable code.

2. **Reduced Error Rate**: Manual copy-pasting of code is error-prone, especially as model complexity increases. Automating the integration of Batch Normalization and Dropout operations can reduce issues arising from code errors.

3. **Increased Productivity**: Developers can build and iterate on models more swiftly, focusing on design and parameter tuning rather than rewriting the same code segments for every new model.

4. **Education and Learning**: For newcomers to TensorFlow, this feature can provide a quicker onboarding process, lowering the learning curve and enabling them to grasp and apply deep learning concepts faster.

Certainly, here's the additional information:

I also believe that PyTorch has implemented MLP functionality quite effectively. An example of this can be found in the following URL: [PyTorch MLP](https://pytorch.org/vision/main/generated/torchvision.ops.MLP.html). PyTorch's approach to creating MLPs provides a good reference for how TensorFlow could potentially integrate similar features.

### Standalone code to reproduce the issue

```shell
origin

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10),
])


with MLP model
```python
model = tf.keras.MLP(
    hidden_channels=[128, 64, 32, 10],
    norm_layer=tf.keras.layers.BatchNormalization,
    activation_layer=tf.keras.layers.ReLU,
    dropout=0.2,
)
```
```


### Relevant log output

_No response_",True,"[-4.70199287e-01 -3.64802003e-01 -5.53065315e-02  6.42097555e-03
  2.18961671e-01 -3.44896615e-01 -4.43298638e-01 -1.22575626e-01
 -3.23922306e-01 -3.53550971e-01  2.09526464e-01 -1.03540123e-01
 -1.63951427e-01  3.13093700e-02  3.41580212e-02  3.16758037e-01
 -1.89112574e-01  9.42183938e-03  1.19270779e-01  3.00051957e-01
 -1.25662923e-01 -1.96572185e-01 -1.93711638e-01  1.86852977e-01
  3.47830988e-02  6.08988404e-02 -3.53581280e-01 -8.29373151e-02
 -2.45265979e-02  2.25442827e-01  6.82312608e-01  2.32581139e-01
 -4.39852923e-02  9.96566415e-02  1.45321399e-01  1.96023762e-01
 -1.68387651e-01 -3.36429179e-01 -3.73315364e-01  1.08556077e-01
  1.57938469e-02  1.90129355e-01  8.39860439e-02 -1.96174160e-01
  1.52928680e-01 -1.63865298e-01 -3.22890431e-02 -3.76220644e-01
 -1.00737616e-01 -3.05251211e-01  1.32745713e-01 -3.67413163e-01
 -5.06565332e-01 -3.45303953e-01 -3.73672247e-01 -1.26963884e-01
  1.25254452e-01 -2.93111414e-01 -1.27930604e-02  1.91133469e-01
 -2.32447181e-02 -4.29345220e-02  8.30244571e-02 -1.39133736e-01
  1.18002504e-01  1.64813280e-01  3.29053253e-01 -3.04463841e-02
  5.82831383e-01 -2.29529381e-01 -2.59440839e-02 -1.88863128e-01
 -5.01268566e-01  3.47675756e-04  5.00687137e-02  1.81944266e-01
  2.57998228e-01  7.92691037e-02  4.15731281e-01 -8.98367912e-02
  1.87841907e-01 -2.56490320e-01 -8.24058279e-02 -3.20134491e-01
  1.48479566e-01 -8.83037224e-02  5.14587760e-01  2.28454173e-01
  2.55294323e-01 -3.17041248e-01  1.66217536e-01  3.83872926e-01
 -1.28457367e-01  1.68146491e-01  5.17692983e-01  4.36520189e-01
  5.02954125e-02 -1.15309194e-01  2.22374216e-01 -2.15454057e-01
 -2.89832473e-01 -1.51604891e-01 -4.43367437e-02 -1.77141279e-01
 -1.44419715e-01 -1.66715220e-01  5.78675345e-02 -1.24099806e-01
  2.57144570e-01 -1.38748974e-01  1.73108131e-01  1.18939765e-01
  2.87082613e-01 -2.59707212e-01 -5.39025478e-02 -1.23222247e-01
 -4.50964868e-02  2.37006471e-02 -8.65723044e-02  7.39788651e-01
  1.12189665e-01 -2.30266511e-01  2.18720973e-01  5.81923649e-02
  6.66957498e-01  3.75794590e-01 -2.81930208e-01  1.12534814e-01
 -3.36509608e-02 -3.76976728e-01  2.79059350e-01  5.46481051e-02
  2.79137224e-01  3.86231661e-01 -1.68849416e-02  1.15291707e-01
  9.39264745e-02 -2.84329355e-01 -1.99203968e-01 -3.72557849e-01
 -4.44425523e-01  2.17970759e-01 -2.39796698e-01 -7.06925869e-01
  1.32502168e-01  1.36596262e-01 -2.23720491e-01  1.59080267e-01
 -6.17765337e-02 -1.59078762e-01  2.49847583e-02  1.64167911e-01
  1.60801351e-01  3.30710590e-01  3.81376475e-01  2.08826736e-01
  3.43297541e-01 -1.59088403e-01  8.60156491e-03 -4.73201424e-01
 -1.55745417e-01  2.60239959e-01 -2.11127162e-01 -2.88954735e-01
 -3.67495418e-03  1.70238376e-01 -2.65249789e-01 -2.79781371e-01
  1.49567500e-01  5.84420562e-01 -2.45363727e-01 -1.29447743e-01
  8.34929347e-02 -2.50425562e-02  1.27664506e-01 -1.02746256e-01
  1.75290972e-01 -7.69786358e-01 -1.81328416e-01  4.05877948e-01
 -6.78916797e-02  1.47381693e-01  3.27397287e-01  5.86566702e-02
  2.44816914e-02 -2.36405190e-02  1.55024752e-01  1.15864754e-01
 -1.99100241e-01  7.18170218e-03 -4.86330748e-01 -1.30165741e-01
  6.48332357e-01 -7.83233196e-02 -8.94323140e-02  1.70773774e-01
  2.11484909e-01 -1.25830024e-01  9.95387696e-03 -1.06813490e-01
 -4.16052580e-01 -2.63806522e-01 -2.46050477e-01  1.19335130e-01
  1.19354293e-01 -2.67546296e-01 -5.70461676e-02 -1.99802712e-01
 -3.98918211e-01  1.82004884e-01 -3.30789499e-02 -5.25374413e-01
  2.93954879e-01  1.42328233e-01 -2.80674845e-01  3.14556062e-01
  1.51861668e-01  9.73180458e-02 -1.98383451e-01  1.59328520e-01
  5.29473051e-02 -1.27103597e-01  2.34888121e-02 -3.43336493e-01
 -1.87825724e-01  3.64040673e-01 -4.52162087e-01  2.03965724e-01
  7.10809324e-03  2.36020267e-01  5.18873148e-02  3.22105527e-01
  4.48568761e-01  3.62113535e-01  4.38443214e-01 -1.21269047e-01
 -1.10887669e-01 -1.31079465e-01 -1.48534894e-01 -1.08355917e-01
 -5.25420547e-01 -3.21984828e-01 -5.05802631e-02  8.59106332e-02
  7.28888735e-02  6.21413052e-01 -4.80905175e-01 -6.44259006e-02
 -3.26425731e-01  1.02240220e-02 -3.41101915e-01  1.85247093e-01
  4.25445795e-01 -5.49207851e-02  5.12851954e-01  2.18904585e-01
  3.15731198e-01  3.43201071e-01  5.31331539e-01 -1.88309073e-01
  5.88606119e-01  1.34314876e-03  4.90370244e-02  5.38168490e-01
  4.53275889e-01  2.61857688e-01 -4.73752946e-01  4.61508870e-01
  3.23876664e-02 -2.71442443e-01  2.41076946e-01 -4.46321040e-01
  7.43334055e-01 -3.66033673e-01  1.60693556e-01  1.05090097e-01
  2.59936959e-01  6.82624504e-02 -7.96340555e-02  8.04491490e-02
  2.71449327e-01  8.57562795e-02 -4.91335571e-01  1.89803354e-02
 -1.94683403e-01 -4.21753496e-01 -8.45931470e-03 -7.77059376e-01
 -3.44189167e-01  1.70964092e-01 -1.76826686e-01  3.07928383e-01
  9.51351225e-02  3.89744453e-02 -8.92953724e-02  1.69457287e-01
  1.25075385e-01 -1.66195631e-01  1.50048137e-01  7.56022185e-02
 -2.07146943e-01 -9.58431140e-02  4.33972955e-01 -6.89396143e-01
 -3.21555734e-01  1.80197582e-02  6.17164731e-01  2.02692688e-01
  5.01793861e-01 -4.45426166e-01  7.04675913e-03 -4.35273737e-01
 -1.89570576e-01  4.82996196e-01 -2.21380353e-01 -8.21252316e-02
 -5.04453897e-01  7.95666993e-01  1.88616395e-01  5.87968677e-02
  3.09459507e-01  1.03317291e-01 -1.46785587e-01  1.76148444e-01
  1.67439699e-01 -8.45407993e-02 -9.27677602e-02 -5.10550559e-01
  1.42717853e-01  2.56693363e-01 -2.92254072e-02  1.60854757e-01
  1.34278834e-01  9.80643779e-02 -2.50251383e-01 -3.58995721e-02
 -4.39617783e-01  2.98663020e-01  1.98146738e-02 -3.74567956e-01
 -3.31337303e-01  3.28779519e-02 -2.47516278e-02 -7.45905414e-02
  4.99545895e-02 -3.07812959e-01  3.92856687e-01  6.97511911e-01
 -2.77083069e-01  9.74611044e-02  1.81787863e-01  2.19094440e-01
 -7.38135636e-01 -1.43550009e-01 -1.06602199e-01  3.12390983e-01
  1.40421540e-01 -1.90488845e-01  5.66076338e-01  2.27743804e-01
 -1.84261143e-01  1.86838865e-01 -2.74543881e-01  1.35313898e-01
  2.33451694e-01 -3.18819970e-01 -6.10990077e-02 -6.46778941e-01
 -9.69166458e-02  3.37896883e-01 -4.12405580e-02  1.07528150e-01
 -3.20382595e-01  4.00141716e-01  5.80181897e-01 -3.77534807e-01
 -3.36134315e-01  1.70440108e-01 -2.07397230e-02 -6.34766743e-02
 -3.09705362e-03 -3.66278231e-01  1.67488813e-01 -1.18519545e-01]"
Issues with Running Custom TensorFlow Lite Model in C++ stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.11,"### 1. System information

- Platform and Linux distribution kubuntu 22.04:
- TensorFlow is built from C++ source code:
- Tensorflow 2.11:

### 2. Code

- Link to models that I trained and tried but they don't work in C++ - https://github.com/asuemg1/models_hub/tree/main/Tensorflow%20Lite/Object%20Detection/my_ssd_mobnet/Optimized%20Models
- Link to the model that works in C++ - https://github.com/ankdesh/tflite/blob/master/Android-TensorFlow-Lite-Example/app/src/main/assets/mobilenet_quant_v1_224.tflite
- Link to C++ code (mainwindow.cpp file):
https://drive.google.com/file/d/1u87yK-1qqKeHBjUKq-Lkxi0LMQKkdrPg/view?usp=sharing

### 3. Crash after conversion

- The model does not work in C++.

Please tell me how you can run the Tensorflow Lite model (tflite format) for object detection or image classification in C ++.

My steps:
- Trained the model for object detection using Tensorflow 2 API object detection.
- After training, I converted the model to the savedmodel format, and then to tflite.
- Next, I needed to embed this model into a C++ project. In order to use it in the future on low-power devices such as rasberry pi

My actions:
- Compiled the Tensorflow Lite library for C++.
- Found a test case using the mobilenet_quant_v1_224.tflite model. In this test case, the model runs successfully. However, when trying to use my own model, it does not work, although it has been tested and works in Python.

What was found out:

- The mobilenet_quant_v1_224.tflite model was quantized and had no metadata and no internal labelmap.txt file.
- TensorFlow Lite API 2 for C++ does not currently support metadata.

If you have any information on how to get my tflite model to work in C++ please share.",True,"[-0.48323858 -0.74976695 -0.3564997   0.15716031  0.13327481 -0.02666099
 -0.17943859 -0.01387471 -0.20968698 -0.28239924  0.02504439  0.10934882
 -0.25636256  0.23241603 -0.13004443  0.08057043 -0.17687038 -0.24087185
  0.04222577  0.00571779 -0.03650679 -0.19693202 -0.19608662  0.18939427
  0.29302096  0.16372573 -0.16160072 -0.1476221   0.02458584  0.18228546
  0.35521668  0.06622124 -0.16335669  0.05570034 -0.2381884   0.21865767
 -0.24715813 -0.16743499 -0.29278046  0.09745927  0.2221895   0.04436789
  0.00212772  0.03205733  0.05998535 -0.0320279   0.21343608 -0.09780264
 -0.02090372 -0.24469793  0.04544676 -0.06951964 -0.2811809  -0.3215549
 -0.00643852  0.02567738  0.31272006 -0.11067499  0.00754496 -0.0044981
  0.08299625  0.07826205  0.08769464 -0.01824694  0.03826363  0.17363144
  0.26777545 -0.04930168  0.5445943  -0.24592423  0.03799907  0.025666
 -0.20596778  0.0156114  -0.00457931  0.0095953  -0.10614456  0.26714075
  0.39701247 -0.04702111  0.14781517 -0.2518081   0.06168825 -0.0808559
  0.09924486 -0.07658987  0.26875126 -0.02739024  0.4001732  -0.10246309
  0.3837877   0.30349314 -0.1754709   0.02899897  0.45218247  0.15242653
 -0.038091    0.395844    0.08274074 -0.04834633 -0.15363833 -0.1988329
  0.01778504  0.20025587  0.16730714 -0.2281997   0.12466668  0.0969679
 -0.01477733 -0.10071019  0.2843307  -0.16435093  0.27997017 -0.00154012
  0.05564467  0.03395929 -0.04515922  0.2058487  -0.05719377  0.7320006
 -0.18714592 -0.21653026  0.19223765  0.1042726   0.41142458  0.05114198
 -0.37272483 -0.05242249 -0.051461    0.00976684  0.21064872  0.27115455
 -0.26605278  0.08425897 -0.0040635   0.0449357  -0.14412221  0.01947826
 -0.2578578   0.05056321 -0.18818161  0.3072446  -0.38750723 -0.3992163
  0.06425458  0.03746017 -0.17067114  0.11691692 -0.05886108  0.02670019
 -0.11155502  0.04609923 -0.06966844  0.19926761  0.21489969  0.24938685
  0.1722708  -0.13757595 -0.00134131 -0.42580175  0.07025182  0.38241106
 -0.05663267 -0.1018291   0.44643778  0.11227836 -0.39705408 -0.3157955
  0.24190584  0.34150004  0.08502046 -0.10630929 -0.11999476  0.2475826
  0.24300092 -0.090848    0.40942714 -0.8292799  -0.1242093   0.05990231
  0.1705921   0.05669405  0.06156407  0.09890632 -0.13637432  0.01382779
  0.22379605 -0.07931523 -0.38810885  0.05211516 -0.48092556 -0.06917604
  0.35857445  0.03003115 -0.2574695  -0.00167509  0.22731404 -0.06386526
 -0.05398844  0.22179532 -0.16078344 -0.1385285  -0.00738874  0.0352897
  0.11681868 -0.3185196  -0.23477934 -0.16935056 -0.5006639  -0.04536333
  0.21097904 -0.22160605 -0.07891666  0.02501728 -0.38908744 -0.04597976
 -0.01921632 -0.13454217 -0.3805048   0.42917258  0.2809437  -0.08188889
  0.06126501 -0.2549248  -0.4656881  -0.15618879 -0.30533952  0.12443519
  0.14334635  0.53149116 -0.01782384  0.21049526  0.29738218  0.24242239
  0.3784423  -0.20495486 -0.02741698 -0.16861811 -0.09705082 -0.21640308
 -0.37489253 -0.24448672  0.0589205   0.00672322  0.0018395   0.11315216
 -0.21404788 -0.01807727 -0.12247579  0.45159635 -0.2975925  -0.0061833
  0.33379728  0.25925732  0.34552407  0.14579582 -0.03747259 -0.03960866
  0.08148082  0.09990995  0.23205778  0.32064313 -0.00488505  0.591483
  0.23559642  0.34591573 -0.25278962  0.23973694 -0.10856579  0.02735586
  0.07744615 -0.22810835  0.22087716 -0.5278199  -0.07541607 -0.15684162
  0.31412405 -0.12621063 -0.28343385  0.01878743  0.2898195   0.29672587
 -0.35363027 -0.01406161  0.0957621  -0.2154361  -0.05393988 -0.5315963
 -0.17320967 -0.01395115 -0.15930432  0.00859983  0.19460203 -0.03313535
 -0.20768234  0.06709875  0.19000222  0.0323032   0.07104924 -0.03824978
 -0.07628804  0.02692018  0.3441155  -0.33924526 -0.09761697 -0.03027093
  0.39175263 -0.08823572  0.5423381  -0.24951217  0.23194213  0.0446017
  0.01484289  0.57533014 -0.07176425  0.05883264 -0.29001433  0.46706218
  0.0614041  -0.1306378  -0.00956497  0.04280488 -0.2476729  -0.11249484
  0.02254005 -0.06373926 -0.05316339 -0.23186597 -0.15700288  0.08729407
 -0.2375031  -0.2671886  -0.00513304  0.03501594 -0.07023679 -0.07277414
 -0.2906342   0.271226    0.04225153 -0.32154965  0.06693268 -0.19413236
 -0.24126637 -0.37202564  0.03128373 -0.3061331   0.2380438   0.4898648
 -0.01319489  0.21078014 -0.08935284  0.28133142 -0.17008033 -0.03342323
 -0.03850711  0.40979195 -0.08443018 -0.09414364  0.10628521  0.50809914
 -0.08918556  0.0671654  -0.262901   -0.1389342   0.1150764  -0.06169061
 -0.02440234 -0.22793621  0.3635826   0.7016767  -0.02432615  0.24178612
 -0.30595434  0.10568943  0.508216   -0.42772412 -0.22565909  0.05456137
  0.03331508 -0.17702767 -0.21589074 -0.11840046 -0.19413239 -0.04347049]"
"extensions eglQueryDevicesEXT, eglQueryDeviceAttribEXT and eglGetPlatformDisplayEXT not available stat:awaiting response type:bug stale TF 1.13","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

1.13.1

### Custom code

Yes

### OS platform and distribution

Unbuntu 22

### Mobile device

_No response_

### Python version

Python 2.7

### Bazel version

_No response_

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

10

### GPU model and memory

GTX 1060 

### Current behavior?

Hi, I got this error when I run Dirt model with my tensorflow-gpu 1.13.1 . 

`2023-08-25 15:23:37.711796: F /home/engineer1/dirt/csrc/gl_common.h:46] extensions eglQueryDevicesEXT, eglQueryDeviceAttribEXT and eglGetPlatformDisplayEXT not available`

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf
import dirt

canvas_width, canvas_height = 128, 128
centre_x, centre_y = 32, 64
square_size = 16


def get_non_dirt_pixels():
    xs, ys = tf.meshgrid(tf.range(canvas_width), tf.range(canvas_height))
    xs = tf.cast(xs, tf.float32) + 0.5
    ys = tf.cast(ys, tf.float32) + 0.5
    x_in_range = tf.less_equal(tf.abs(xs - centre_x), square_size / 2)
    y_in_range = tf.less_equal(tf.abs(ys - centre_y), square_size / 2)
    return tf.cast(tf.logical_and(x_in_range, y_in_range), tf.float32)


def get_dirt_pixels():

    # Build square in screen space
    square_vertices = tf.constant([[0, 0], [0, 1], [1, 1], [1, 0]], dtype=tf.float32) * square_size - square_size / 2.
    square_vertices += [centre_x, centre_y]

    # Transform to homogeneous coordinates in clip space
    square_vertices = square_vertices * 2. / [canvas_width, canvas_height] - 1.
    square_vertices = tf.concat([square_vertices, tf.zeros([4, 1]), tf.ones([4, 1])], axis=1)

    return dirt.rasterise(
        vertices=square_vertices,
        faces=[[0, 1, 2], [0, 2, 3]],
        vertex_colors=tf.ones([4, 1]),
        background=tf.zeros([canvas_height, canvas_width, 1]),
        height=canvas_height, width=canvas_width, channels=1
    )[:, :, 0]


def main():

    if '.' in tf.__version__ and int(tf.__version__.split('.')[0]) < 2:

        session = tf.Session()
        with session.as_default():

            non_dirt_pixels = get_non_dirt_pixels().eval()
            dirt_pixels = get_dirt_pixels().eval()

    else:

        non_dirt_pixels = get_non_dirt_pixels().numpy()
        dirt_pixels = get_dirt_pixels().numpy()

    if np.all(non_dirt_pixels == dirt_pixels):
        print('successful: all pixels agree')
    else:
        print('failed: {} pixels disagree'.format(np.sum(non_dirt_pixels != dirt_pixels)))


if __name__ == '__main__':
    main()
```


### Relevant log output

```shell
2023-08-25 15:23:37.181664: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2023-08-25 15:23:37.186756: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2496000000 Hz
2023-08-25 15:23:37.189184: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x24d6bf0 executing computations on platform Host. Devices:2023-08-25 15:23:37.189205: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2023-08-25 15:23:37.478413: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node
Your kernel may have been built without NUMA support.
2023-08-25 15:23:37.478518: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x233ae60 executing computations on platform CUDA. Devices:2023-08-25 15:23:37.478540: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): NVIDIA GeForce GTX 1060 6GB, Compute Capability 6.1
2023-08-25 15:23:37.478666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:
name: NVIDIA GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7085
pciBusID: 0000:01:00.0
totalMemory: 6.00GiB freeMemory: 5.09GiB
2023-08-25 15:23:37.478681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2023-08-25 15:23:37.478801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-08-25 15:23:37.478813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
2023-08-25 15:23:37.478817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
2023-08-25 15:23:37.478968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1194] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2023-08-25 15:23:37.479033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4913 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
2023-08-25 15:23:37.711796: F /home/engineer1/dirt/csrc/gl_common.h:46] extensions eglQueryDevicesEXT, eglQueryDeviceAttribEXT and eglGetPlatformDisplayEXT not available
Aborted
```
",True,"[-0.43139672 -0.2830157  -0.15912834  0.1140662   0.40902868 -0.10487901
 -0.0073874  -0.09462487 -0.16224837 -0.35184234  0.09492037  0.11466986
 -0.16690299  0.09800135 -0.00540768  0.43112975 -0.31023493  0.10803074
 -0.03803746 -0.12881713 -0.01037484 -0.04747517 -0.20252797  0.06483427
  0.26799077  0.12174372 -0.2545196  -0.18229364  0.06413013  0.14171526
  0.40089047  0.04801639 -0.06515789  0.17634207  0.19840543  0.06340896
 -0.33111432 -0.09684086 -0.34923476 -0.0451448   0.11663657  0.03778215
 -0.06025545 -0.1327272  -0.03736311 -0.30046085 -0.0176506  -0.18619916
 -0.08156379  0.1683792   0.05644901 -0.2815264  -0.4619074  -0.37591052
 -0.12010276 -0.08387305  0.03754077 -0.04700309  0.11458009 -0.01077416
  0.1765593   0.04745173 -0.20425454 -0.06254924 -0.20388946  0.08152258
  0.27210805 -0.07700536  0.40084252 -0.2383686   0.04528261  0.08706754
 -0.48046577  0.06505854  0.03725797  0.11841341  0.01815666  0.04135332
  0.29453027 -0.06309405  0.03784198 -0.39711198 -0.11067259 -0.17244396
  0.14490224 -0.11576556  0.26784176  0.00597764  0.40899602 -0.11902486
  0.422929    0.42816317  0.00270261  0.16753727  0.5945456   0.01373171
  0.03716226  0.1115436   0.09917143 -0.13381803 -0.17844078 -0.4123519
  0.02544696  0.28627697 -0.13106322 -0.22575097  0.06793519 -0.05335366
  0.16819869 -0.12277084  0.09580709 -0.23577477  0.25247842 -0.09112091
 -0.01904567 -0.08432487 -0.06622447  0.16600144 -0.02174332  0.6553681
  0.2238526   0.04226896  0.14625636  0.24360143  0.4092612   0.2359008
 -0.04385334  0.11411888  0.04640007 -0.06979676  0.13424245  0.16079514
 -0.09977555  0.24228974 -0.06074375 -0.05046829 -0.40921354 -0.1130435
 -0.22802751 -0.06299375 -0.35844117  0.13367802 -0.2544244  -0.35840774
  0.13620496  0.23338538 -0.2011869   0.41070354 -0.235253    0.09321699
 -0.07783078  0.04256712 -0.35218126  0.49540442  0.05546413  0.24710253
  0.36515734 -0.06202844  0.06048545 -0.57622457 -0.00957822  0.48310804
  0.06532975 -0.19162095  0.23761103  0.00448429 -0.33561784 -0.14812526
  0.18709819  0.47677597 -0.23998788 -0.06846443  0.20724496  0.01515463
  0.17640391  0.07557837  0.3904     -0.5148716  -0.07010029  0.5046283
  0.12274821  0.14417085 -0.10005094  0.145069    0.04227463 -0.03581405
  0.1724941  -0.0063321  -0.14572054  0.01208578 -0.35439175 -0.2837338
  0.42242223 -0.24486339  0.04953797  0.10982792 -0.03086079 -0.20675924
  0.14330256  0.04840756 -0.08158263  0.07646111  0.01325171 -0.16884406
  0.05025341 -0.02733105  0.00243848 -0.3915673  -0.2580288  -0.07518855
  0.11027414 -0.5496269   0.03172496 -0.3390937  -0.4269688   0.25940865
  0.22050202  0.20530668 -0.14051609  0.19824871 -0.02809078 -0.15806842
  0.09878919 -0.3375002  -0.1767244   0.00650832 -0.39853874  0.26553172
  0.00768939  0.27027583  0.04986424  0.33884013  0.5186436   0.2713499
  0.41272688 -0.3037475  -0.13281572 -0.11771549 -0.2011446   0.04030067
 -0.26675585 -0.07523844  0.01003431 -0.11826893  0.19614887  0.23345721
 -0.20519751 -0.20300448 -0.17432216  0.3269563  -0.3272257  -0.01874252
  0.24636564  0.10000755  0.3298471   0.10739308  0.1436879   0.08398496
  0.04512209 -0.2119607   0.28157672 -0.05627872  0.04832774  0.32668698
  0.10480274  0.1808821  -0.23645326  0.39231962  0.19042455 -0.18315296
  0.11915561 -0.4223407   0.71184343 -0.32871413 -0.02859099  0.08877987
  0.26891106 -0.2260935   0.01630741 -0.03855004  0.1434412   0.22154334
 -0.4031769   0.11301071 -0.08021127 -0.00793359  0.14442562 -0.5117677
 -0.19347933  0.18329868 -0.10130793  0.09414044  0.0460883  -0.05088702
 -0.39835268  0.22979188 -0.11669198 -0.2852214   0.04637337  0.13997519
 -0.08615454  0.17559144  0.3006072  -0.43378988 -0.00180549 -0.32476237
  0.42765605  0.09742511  0.29633945 -0.4282453  -0.04487144  0.2006047
  0.03976133  0.4719395   0.06814219 -0.12141146 -0.4865271   0.5637935
  0.09521742  0.02984236  0.08676617  0.00091293 -0.4585904  -0.03866556
  0.12053831 -0.02723362  0.190864   -0.14708248  0.07109078  0.05548556
 -0.14285098 -0.02954093 -0.1648733   0.03836002 -0.08821817 -0.28354543
 -0.3723156   0.0749827   0.1066184  -0.16241644 -0.14474547  0.10823035
 -0.13772175 -0.2948228  -0.03167711 -0.5567587   0.2383732   0.5142917
 -0.05153837  0.05293111  0.06684913  0.07638088 -0.37693748  0.0350894
 -0.15271048  0.4049085   0.1722689  -0.1100326   0.2662956   0.43665338
 -0.10604407  0.28519875 -0.19169617 -0.01298226  0.19931939 -0.14856327
 -0.20361015 -0.34209606  0.02875591  0.24117368 -0.1735088   0.14943147
 -0.48913515  0.16309188  0.40865248 -0.36724642 -0.09102683  0.11702444
  0.16371265 -0.09204485 -0.22235033 -0.29112864  0.29803067  0.11397341]"
JIT yields inconsistent results using tf.math.top_k stat:awaiting response type:bug comp:ops,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15.0-dev20230824

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda_11.8.r11.8/compiler.31833905_0 / cuDNN version 8700

### GPU model and memory

NVIDIA GeForce RTX 2080 Ti

### Current behavior?

JIT yields inconsistent results using `tf.math.top_k` when `index_type=tf.int32` (no issue with `index_type=tf.int64`).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


@tf.function(jit_compile=True)
def tf_func(shape):
    x = tf.random.stateless_normal(shape, seed=(1, 2))
    x = tf.transpose(x, perm=[1, 0])
    topk_max, indices = tf.math.top_k(x, 1, sorted=False, index_type=tf.int32)
    reduce_max = tf.reduce_max(x, axis=1, keepdims=True)
    return topk_max - reduce_max


def check(shape):
    should_be_all_zero = tf_func(shape)
    print(f""should_be_all_zero shape {shape}:\n{should_be_all_zero}"")


check((1024, 3))
check((1023, 3))
check((1024, 2))
```


### Relevant log output

```shell
should_be_all_zero shape (1024, 3):
[[-0.7787831 ]
 [ 0.47324872]
 [ 0.30553436]]
should_be_all_zero shape (1023, 3):
[[0.]
 [0.]
 [0.]]
should_be_all_zero shape (1024, 2):
[[0.]
 [0.]]
```
",True,"[-7.84602046e-01 -3.76843274e-01  5.24132177e-02  9.85046923e-02
  1.33887395e-01 -5.13912201e-01  3.39736566e-02  9.18849856e-02
 -1.46903798e-01 -3.88037860e-01  7.98513219e-02 -1.77867617e-03
 -1.16189115e-01 -2.18142241e-01 -2.79924452e-01  4.12905693e-01
 -4.24493253e-01 -2.37296522e-01  1.79697275e-01  6.26448020e-02
 -3.30388546e-01 -4.27252084e-01 -3.42811137e-01  7.62304813e-02
  4.81345356e-01  2.09852278e-01 -2.49304712e-01  3.59115422e-01
  6.06994852e-02  1.13403730e-01  4.09262538e-01  4.43577319e-01
 -6.17525913e-03  1.37996078e-01 -9.00427476e-02  1.31293967e-01
 -2.30364904e-01 -2.43662819e-01 -2.92991489e-01 -1.21045858e-02
 -5.90625107e-02  1.12254843e-01  3.05822790e-01 -2.83474296e-01
  1.94646075e-01 -1.47891650e-02  4.55326065e-02 -1.17599003e-01
 -1.54425114e-01 -3.19688290e-01 -1.18102036e-01  2.50060081e-01
 -3.80656719e-01 -1.83635324e-01 -6.38750428e-03 -8.74764249e-02
 -4.33153175e-02 -1.96020231e-01 -6.03834130e-02  4.91294444e-01
 -5.39450571e-02  9.73979980e-02 -1.00403756e-01 -6.30041957e-02
  8.79326761e-02  9.69363004e-03  5.82288146e-01 -1.92941859e-01
  4.27038729e-01 -2.91236401e-01  4.99167331e-02 -4.82373871e-02
 -4.23376262e-01  1.68639272e-01  1.24905936e-01  2.84679234e-02
 -3.47187556e-02  2.24429250e-01  4.39793736e-01 -2.76507616e-01
  1.17527008e-01 -2.41747484e-01 -2.93940529e-02 -2.47079581e-01
  2.34693795e-01 -2.22586960e-01  2.38017082e-01  4.14006040e-02
  3.82900476e-01 -3.34513128e-01  2.65863448e-01  5.11644006e-01
 -3.28917384e-01  1.00551009e-01  4.99618053e-01  2.38838002e-01
  2.75996685e-01  3.63955647e-02 -1.45924002e-01  8.23386312e-02
 -4.75680195e-02  1.05512716e-01 -1.84800506e-01  8.92674476e-02
 -6.78896159e-02  2.68878322e-02  3.91890138e-01  3.57938819e-02
  1.57600939e-01 -4.70362902e-02  1.32923365e-01  2.11981177e-01
  3.18255201e-02 -4.07230437e-01 -7.59120882e-02 -1.03865698e-01
 -7.52402097e-02 -8.18123072e-02  1.35920927e-01  6.83014393e-01
 -1.30895332e-01  6.85913786e-02 -3.36178571e-01  4.47997916e-03
  5.65539002e-01  1.86856225e-01 -4.58455645e-02 -1.06763579e-02
  1.05430603e-01 -6.85012639e-02 -4.26617041e-02  4.17702161e-02
 -1.08290985e-02  9.71873105e-02  7.43074119e-02 -8.46165717e-02
 -7.34534636e-02  1.36178851e-01 -2.64211863e-01 -2.69090891e-01
 -2.60127246e-01  8.23255405e-02  1.63777083e-01 -4.13768500e-01
  1.46915719e-01  5.09096980e-02 -2.72173911e-01  3.22264075e-01
 -3.61933000e-02 -3.18515599e-01 -2.38867849e-01  8.32050387e-03
 -1.16070099e-01  5.45774460e-01  1.68923110e-01  1.23427927e-01
  4.41097260e-01  7.42429942e-02  1.04459442e-01 -2.78643757e-01
  1.56682372e-01  2.74354428e-01 -2.81423539e-01 -3.33767831e-01
  2.68694639e-01  1.34333909e-01 -4.50878084e-01  1.79085005e-02
 -6.78131878e-02  5.50276399e-01 -3.03061247e-01 -2.82718301e-01
 -4.19729613e-02 -4.04600054e-04  2.75263548e-01 -5.83831687e-03
  3.91151488e-01 -2.41542026e-01 -1.47824943e-01  3.00015837e-01
  2.05569312e-01  2.95295149e-01  2.45236441e-01  1.56668440e-01
  2.45042425e-02 -8.31954107e-02  2.21454240e-02  6.27910376e-01
 -5.10296643e-01 -2.78165132e-01 -9.23921943e-01  2.71898080e-02
  4.80287254e-01  1.80077404e-02  7.91666508e-02 -3.54163274e-02
  8.70807320e-02 -2.35454470e-01 -1.36123925e-01  5.79163879e-02
 -2.22868204e-01 -1.84318215e-01 -2.24725485e-01 -1.63628966e-01
  1.38513401e-01 -8.21867585e-02 -9.89994630e-02 -3.60929012e-01
 -3.55792254e-01  3.92489880e-02 -2.26512030e-02 -4.64154720e-01
  1.96495831e-01 -7.19006360e-02 -1.94146976e-01  2.96741605e-01
  2.60719121e-01 -1.83769181e-01 -1.69386953e-01  2.92987049e-01
  4.22008276e-01 -2.07688197e-01  1.05390951e-01 -4.29245234e-01
 -3.28545094e-01  3.08763683e-01 -5.36799669e-01  2.14478612e-01
 -2.18971260e-03  1.36591107e-01  2.62453735e-01 -1.31545663e-01
  5.14093399e-01  1.16844773e-01  3.75659287e-01 -1.21091209e-01
 -1.31266877e-01 -1.83038592e-01 -2.40090102e-01 -1.03295688e-02
 -5.21668553e-01 -4.33977306e-01  2.66061246e-01  5.62831610e-02
  3.28058422e-01  3.86433065e-01 -1.33926183e-01 -4.26314443e-01
 -1.95493579e-01  3.78962576e-01 -3.47013652e-01  1.04277596e-01
  1.28916696e-01  3.55212927e-01  5.05049467e-01  6.87621236e-02
  2.78465390e-01  4.52289060e-02  2.96098948e-01 -3.08271796e-01
  2.67736286e-01  2.16484636e-01  1.81664646e-01  4.30994689e-01
  1.55878335e-01  2.83944666e-01 -2.50633687e-01  4.89316821e-01
 -9.60953981e-02  1.21422723e-01 -3.11607067e-02 -2.34389246e-01
  7.17807293e-01 -1.11868843e-01  1.09481126e-01  5.75048551e-02
  2.93710411e-01 -3.15152816e-02 -2.28083909e-01 -8.81120935e-02
  8.70826319e-02  2.50886768e-01 -3.69966298e-01 -2.76263803e-02
  8.57505947e-02 -2.61730492e-01 -1.06203154e-01 -5.54957211e-01
 -1.07410282e-01  2.03652769e-01 -2.43191063e-01  1.71364129e-01
  2.29017287e-01 -1.44512327e-02 -1.86940536e-01 -3.02787796e-02
  4.82446291e-02 -2.85942852e-01  1.81763023e-01  1.68334842e-01
 -2.73157746e-01  1.01351179e-01  5.90123296e-01 -2.22133398e-01
 -1.43579006e-01  2.28418186e-01  3.88593912e-01  2.13901907e-01
  1.94901049e-01 -2.96635509e-01  2.33919457e-01  5.86181395e-02
  6.53423667e-02  2.74179339e-01 -3.10113788e-01  6.18474185e-02
 -2.66164780e-01  7.16450274e-01  2.74898589e-01 -3.53737846e-02
  1.73597127e-01 -1.65801436e-01 -6.23116851e-01 -5.25407940e-02
  2.23471895e-01 -1.02625012e-01 -2.38467649e-01 -2.17782825e-01
  1.01966292e-01  3.32392156e-01  1.29700214e-01  2.01406002e-01
 -3.95858377e-01  3.37530263e-02 -9.25621614e-02 -2.79381067e-01
 -2.70475328e-01  2.15940699e-01 -1.97862506e-01 -5.19147515e-01
 -2.17061460e-01  7.90577382e-02 -1.53678179e-01 -3.40202689e-01
 -5.43380566e-02 -2.04969883e-01  2.20864713e-01  5.89065433e-01
 -3.53674620e-01  2.55487025e-01 -1.59585431e-01  3.68644632e-02
 -3.35743874e-01 -1.78677700e-02 -1.02718852e-01  3.90627444e-01
 -2.49722198e-01 -2.34266482e-02  5.27318060e-01  3.96602675e-02
 -3.33114624e-01  1.31823584e-01 -2.45105520e-01  1.25889003e-01
  1.40234753e-01 -3.41696329e-02 -2.14238971e-01 -2.69937962e-01
  2.66922832e-01  1.20571971e-01 -7.98807144e-02  2.09499747e-01
 -3.37496430e-01  2.26602063e-01  4.86522079e-01 -5.98928869e-01
 -3.85354161e-01  1.21192805e-01  2.28358820e-01 -3.04848611e-01
 -2.70259768e-01 -1.22729670e-02  1.11195400e-01  1.79768175e-01]"
Batch matmul imprecision stat:awaiting response type:bug stale comp:ops TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13 / 2.14.0-dev20230706

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

```
$ mamba list | grep cu
cuda-nvcc                 12.2.128                      0    nvidia
cudatoolkit               11.8.0              h4ba93d1_12    conda-forge
nvidia-cublas-cu11        2022.4.8                 pypi_0    pypi
nvidia-cublas-cu117       11.10.1.25               pypi_0    pypi
nvidia-cuda-nvrtc-cu11    2022.4.8                 pypi_0    pypi
nvidia-cuda-nvrtc-cu117   11.7.50                  pypi_0    pypi
nvidia-cudnn-cu11         8.9.4.25                 pypi_0    pypi
```
### GPU model and memory

NVIDIA GeForce RTX 3060 - 12 GB
Driver Version: 520.61.05

### Current behavior?

I'm trying to do a batch matrix multiply (i.e. I've got a bunch of m x n matrices, all in one tensor, and I want to do a matrix multiply of each of them with some other matrix). However, I'm getting slightly different results than I get from Numpy (and previous TensorFlow versions, this script passed for me in 2.9). 

One interesting thing is that the value 25 (the second dimension of `x`) is significant. If I reduce this to 16 or below, it passes. Also, if I change the second dimension of `c` from 2 to 1, it also passes.

### Standalone code to reproduce the issue

```python
import numpy as np
import tensorflow as tf

print(f""file: {tf.__file__}"")
print(f""git version: {tf.version.GIT_VERSION}"")
print(f""version: {tf.__version__}"")

rng = np.random.RandomState(0)
x = rng.uniform(-1, 1, size=(1, 25, 5)).astype(np.float32)
c = rng.uniform(-1, 1, size=(5, 2)).astype(np.float32)

y = x @ c

x2 = np.tile(x, (2, 1, 1))
y2 = x2 @ c

for y2i in y2:
    np.testing.assert_allclose(y2i, y.squeeze(0))

tols = dict(atol=1e-7, rtol=1e-5)

z = tf.matmul(x, c).numpy()
# z = tf.einsum(""...tq,...qr->...tr"", x, c)
np.testing.assert_allclose(z, y, **tols)

z2 = tf.matmul(x2, c).numpy()
# z2 = tf.einsum(""...tq,...qr->...tr"", x2, c)
np.testing.assert_allclose(z2, y2, **tols)
```


### Relevant log output

```shell
file: /home/ehunsber/mambaforge/envs/tf213/lib/python3.8/site-packages/tensorflow/__init__.py
git version: v1.12.1-96406-gfa4d29bfef8
version: 2.14.0-dev20230706
Traceback (most recent call last):
  File ""test_batch.py"", line 26, in <module>
    np.testing.assert_allclose(z2, y2)
  File ""/home/ehunsber/mambaforge/envs/tf213/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 1592, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/home/ehunsber/mambaforge/envs/tf213/lib/python3.8/contextlib.py"", line 75, in inner
    return func(*args, **kwds)
  File ""/home/ehunsber/mambaforge/envs/tf213/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 862, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0

Mismatched elements: 100 / 100 (100%)
Max absolute difference: 0.00046098
Max relative difference: 0.01047752
 x: array([[[-0.187503,  0.005696],
        [-0.255552, -0.84404 ],
        [ 0.268836, -1.250793],...
 y: array([[[-0.187499,  0.005691],
        [-0.255404, -0.844322],
        [ 0.268935, -1.251033],...
```
",True,"[-0.46747673 -0.0975676   0.00667897 -0.11260487  0.3304971  -0.4489907
  0.01665057  0.14093849 -0.26606154 -0.30402446  0.13299973  0.0804357
  0.10201205  0.00901839 -0.12797225  0.16381896 -0.37514436 -0.20056619
  0.13536344  0.22091702 -0.23092464 -0.16512808 -0.26655018  0.11478769
  0.32208735  0.3495009  -0.4151576   0.1402992  -0.00196012  0.33564255
  0.41684723  0.2142739   0.15707207  0.13322824  0.12805584  0.17379159
 -0.18830007 -0.13938275 -0.240325    0.16898265 -0.01170932  0.04730069
  0.22439161 -0.16463645  0.26004368 -0.1948366  -0.11413284 -0.21769504
 -0.00570277 -0.13649593  0.09083762  0.25032192 -0.20828333 -0.31061524
 -0.05799966 -0.13710833  0.1263259  -0.23574623 -0.14539951  0.33980277
  0.05955174 -0.14062466 -0.04933561 -0.08667444  0.1735732   0.09313838
  0.32435238  0.06691356  0.5534151  -0.10699911  0.03616498 -0.0356129
 -0.2538972   0.11420164  0.00204536 -0.04503046 -0.10942581 -0.00662924
  0.17701164 -0.30373567 -0.12460943 -0.18621701 -0.08814657 -0.29801232
 -0.05356229 -0.03760196  0.48376507  0.16509205  0.66399527 -0.29713905
  0.48707503  0.47238606 -0.12079444  0.13141759  0.5175855   0.23380908
  0.11187451  0.17455605 -0.04144191 -0.04382124 -0.20556006 -0.14272949
 -0.210201   -0.00710606 -0.12853621 -0.18124554 -0.0070861  -0.06371627
 -0.09946322  0.00276886  0.3211129   0.085942    0.06560915 -0.07154386
 -0.14704879  0.10245739  0.10912958 -0.00938043  0.07397334  0.6282511
  0.17205602 -0.05256507 -0.18397847  0.37914726  0.47174084 -0.00673136
  0.09685865  0.05443262  0.04796418 -0.21272622  0.14253111 -0.09487557
  0.07773472  0.18533966 -0.07405515 -0.05510816  0.15952891  0.01293444
 -0.14536944 -0.1040011  -0.386158    0.0320293  -0.08443226 -0.565007
  0.22406882  0.10163407 -0.42305428  0.16399123 -0.20402765  0.08295293
 -0.06863396 -0.06005305 -0.19468239  0.5196826   0.0868233   0.03522663
  0.4084373   0.01302329  0.24364045 -0.4905017  -0.00254825  0.53266406
  0.04404382 -0.13716418  0.1956099   0.22078249 -0.401442   -0.18662196
  0.01226489  0.48240337 -0.10219707 -0.08521842  0.05382814 -0.22088979
  0.17725846 -0.12244731  0.08453343 -0.31359667  0.12641893  0.3973714
  0.12003151  0.2562129   0.15178245  0.16336781  0.03897323  0.00578124
  0.04695663  0.27929735 -0.38667232 -0.19088304 -0.6170264   0.02174569
  0.44033098 -0.18305679 -0.10005736  0.02936115  0.19020458  0.18618649
 -0.00711844  0.20646712 -0.28729832 -0.05586232 -0.09457981 -0.05258717
  0.3683317   0.06860132 -0.14417374 -0.49676758 -0.28286803  0.21133512
 -0.05286675 -0.47461298  0.19209336 -0.01655158 -0.34019256  0.38349867
 -0.03993949 -0.10627598 -0.09486036  0.16616371  0.16050875 -0.22801983
 -0.06443486 -0.37752414 -0.12338167  0.23963222 -0.40721232 -0.00293256
 -0.07019444 -0.00130758  0.07655431  0.19474468  0.29941133  0.19359851
  0.31301543 -0.06500317 -0.12864213  0.03305252 -0.02401706  0.20045063
 -0.32918292 -0.24614057 -0.08414579 -0.06235582  0.36274016  0.49338382
 -0.05051632 -0.22864209 -0.34122056 -0.01298408 -0.19158009  0.05131263
  0.19126469 -0.07337196  0.26695275  0.08320269  0.16334881  0.26182705
  0.21181661 -0.4012579   0.36531848  0.12684757  0.15849699  0.40017787
  0.16224346  0.09736136 -0.50705284  0.5338066  -0.06559588 -0.15397166
  0.15024795 -0.3610931   0.65789104 -0.32970142  0.02829099 -0.1089603
  0.35803878 -0.25493842 -0.11503358  0.06139947 -0.06415937  0.21194093
 -0.34675315  0.15884992  0.16923946 -0.27217358 -0.17116474 -0.6322477
 -0.10165054  0.04243841 -0.3146402   0.1265889  -0.28953534  0.04350215
 -0.22520971  0.21331063  0.05325562 -0.29489663  0.07298256  0.03266221
 -0.17828734 -0.03333491  0.36002213 -0.35188547 -0.25758466 -0.04263603
  0.37636036  0.33932427  0.3401959  -0.2470154   0.16563001 -0.01768902
 -0.16438879  0.338814    0.04233809 -0.00803759 -0.27409926  0.73904026
  0.02196222 -0.0891614   0.1775245  -0.22795671 -0.28654748  0.1431375
  0.41500488  0.04747334 -0.34584185 -0.29501855  0.02689003  0.07563276
  0.05034016  0.02856036 -0.2614063  -0.0688803  -0.20550013 -0.22453877
 -0.37691382  0.25312063 -0.11460941 -0.44613856 -0.25749007 -0.01437975
 -0.17392686 -0.17650771 -0.1136813  -0.14152023  0.29959926  0.5808171
 -0.04137019  0.17732573  0.03601327  0.09630543 -0.2770912  -0.02645895
 -0.13929817  0.48543864  0.14096057 -0.17052817  0.2516033   0.0964913
 -0.17816743  0.1661563  -0.35185337  0.16325349  0.08985174 -0.24246237
 -0.21178223 -0.35806194  0.07190888 -0.01669179 -0.06502438  0.09740059
 -0.30110532  0.40773794  0.37890428 -0.49261358 -0.3135636  -0.08327316
  0.16006133  0.03364599 -0.24542852 -0.03882505  0.08415905  0.17585929]"
Unit test failure when built with clang awaiting review type:bug type:support subtype: ubuntu/linux,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/compiler/mlir/lite/sparsity:sparsify_model_test throws a segfault

### Standalone code to reproduce the issue

```shell
bazel test --cache_test_results=no --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --build_tests_only -- //tensorflow/compiler/mlir/lite/sparsity:sparsify_model_test
```


### Relevant log output

```shell
==================== Test output for //tensorflow/compiler/mlir/lite/sparsity:sparsify_model_test:
2023-08-23 14:23:54.976080: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-08-23 14:23:54.977641: W tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.
Running main() from gmock_main.cc
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from SparsifyModelTest
[ RUN      ] SparsifyModelTest.MetadataIsAddedToOutputModel
================================================================================
Target //tensorflow/compiler/mlir/lite/sparsity:sparsify_model_test up-to-date:
  bazel-bin/tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test
INFO: Elapsed time: 557.635s, Critical Path: 355.66s
INFO: 4267 processes: 707 internal, 3560 local.
INFO: Build completed, 1 test FAILED, 4267 total actions
//tensorflow/compiler/mlir/lite/sparsity:sparsify_model_test             FAILED in 1.0s
  /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test/test.log

Executed 1 out of 1 test: 1 fails locally.
andrew@8bde10e59b61:/workspace$ gdb bazel-bin/tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test
GNU gdb (Ubuntu 9.2-0ubuntu1~20.04.1) 9.2
Copyright (C) 2020 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
Type ""show copying"" and ""show warranty"" for details.
This GDB was configured as ""aarch64-linux-gnu"".
Type ""show configuration"" for configuration details.
For bug reporting instructions, please see:
<http://www.gnu.org/software/gdb/bugs/>.
Find the GDB manual and other documentation resources online at:
    <http://www.gnu.org/software/gdb/documentation/>.

For help, type ""help"".
Type ""apropos word"" to search for commands related to ""word""...
Reading symbols from bazel-bin/tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test...
(gdb) run
Starting program: /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test 
warning: Error disabling address space randomization: Operation not permitted
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/aarch64-linux-gnu/libthread_db.so.1"".
2023-08-23 14:26:40.332993: W tensorflow/tsl/lib/monitoring/collection_registry.cc:81] Trying to register 2 metrics with the same name: /tensorflow/core/bfc_allocator_delay. The old value will be erased in order to register a new one. Please check if you link the metric more than once, or if the name is already used by other metrics.
Running main() from gmock_main.cc
[==========] Running 1 test from 1 test suite.
[----------] Global test environment set-up.
[----------] 1 test from SparsifyModelTest
[ RUN      ] SparsifyModelTest.MetadataIsAddedToOutputModel

Program received signal SIGSEGV, Segmentation fault.
0x0000ffff80a09040 in ?? () from /lib/aarch64-linux-gnu/libc.so.6
(gdb) bt
#0  0x0000ffff80a09040 in ?? () from /lib/aarch64-linux-gnu/libc.so.6
#1  0x0000ffff8b3df258 in std::__fill_a1<unsigned char> (__first=0xaaaaf5cbc9c1 """", __last=0xaaaaf5cbc9c0 """", __c=@0xaaaaf5cbc9c0: 0 '\000')
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_algobase.h:893
#2  std::__fill_a<unsigned char*, unsigned char> (__first=0xaaaaf5cbc9c1 """", __last=0xaaaaf5cbc9c0 """", __value=@0xaaaaf5cbc9c0: 0 '\000')
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_algobase.h:914
#3  std::__fill_n_a<unsigned char*, unsigned long, unsigned char> (__n=18446744073708562752, __value=@0xaaaaf5cbc9c0: 0 '\000', 
    __first=<optimized out>) at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_algobase.h:1065
#4  std::fill_n<unsigned char*, unsigned long, unsigned char> (__n=18446744073708562752, __value=@0xaaaaf5cbc9c0: 0 '\000', 
    __first=<optimized out>) at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_algobase.h:1094
#5  std::__uninitialized_default_n_1<true>::__uninit_default_n<unsigned char*, unsigned long> (__first=0xaaaaf5cbc9c0 """", __n=<optimized out>)
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_uninitialized.h:598
#6  std::__uninitialized_default_n<unsigned char*, unsigned long> (__first=0xaaaaf5cbc9c0 """", __n=<optimized out>)
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_uninitialized.h:632
#7  std::__uninitialized_default_n_a<unsigned char*, unsigned long, unsigned char> (__first=0xaaaaf5cbc9c0 """", __n=<optimized out>)
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_uninitialized.h:698
#8  std::vector<unsigned char, std::allocator<unsigned char> >::_M_default_initialize (this=0xffffe8e7e728, __n=<optimized out>)
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:1606
#9  std::vector<unsigned char, std::allocator<unsigned char> >::vector (this=0xffffe8e7e728, __n=0, __a=...)
    at /dt10/usr/lib/gcc/aarch64-unknown-linux-gnu/10/../../../../include/c++/10/bits/stl_vector.h:512
#10 (anonymous namespace)::Translator::BuildCustomOperator (this=this@entry=0xffffe8e801b8, inst=<optimized out>, op=..., 
    operands=std::vector of length 1, capacity 1 = {...}, results=std::vector of length 1, capacity 1 = {...})
    at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1248
#11 0x0000ffff8b3d7ca0 in (anonymous namespace)::Translator::BuildOperator (this=this@entry=0xffffe8e801b8, inst=inst@entry=0xaaaaf5d48ec0, 
    operands=std::vector of length 1, capacity 1 = {...}, results=std::vector of length 1, capacity 1 = {...}, 
    intermediates=std::vector of length 0, capacity 0) at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1434
#12 0x0000ffff8b3d0774 in (anonymous namespace)::Translator::BuildSubGraph (this=this@entry=0xffffe8e801b8, name=""main"", region=0x0, 
    index=index@entry=0) at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2036
#13 0x0000ffff8b3c7934 in (anonymous namespace)::Translator::TranslateInternal[abi:cxx11]() (this=<optimized out>, this@entry=0xffffe8e801b8)
    at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2429
#14 0x0000ffff8b3c5bb8 in (anonymous namespace)::Translator::Translate (module=..., toco_flags=..., Python Exception <class 'gdb.error'> No type named std::__detail::_Hash_node<class std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, true>.: 
tags=std::unordered_set with 0 elements, 
    op_or_arg_name_mapper=0xffffe8e7fbc0, metadata=Python Exception <class 'AttributeError'> 'NoneType' object has no attribute 'pointer': 
std::map with 1 element) at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2331
#15 tflite::MlirToFlatBufferTranslateFunction (module=..., options=..., serialized_flatbuffer=serialized_flatbuffer@entry=0xffffe8e80a00)
    at tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2838
#16 0x0000ffff8b443498 in mlir::lite::SparsifyModel (input_model=..., builder=builder@entry=0xffffe8e80c08, 
    error_reporter=error_reporter@entry=0xffffe8e80c00) at tensorflow/compiler/mlir/lite/sparsity/sparsify_model.cc:91
#17 0x0000aaaae03023c0 in mlir::lite::(anonymous namespace)::SparsifyModelTest_MetadataIsAddedToOutputModel_Test::TestBody (
    this=<optimized out>) at tensorflow/compiler/mlir/lite/sparsity/sparsify_model_test.cc:67
#18 0x0000ffff81043fd4 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void> (
    method=&virtual testing::Test::TestBody(), location=0xffff80fef955 ""the test body"", object=<optimized out>)
    at external/com_google_googletest/googletest/src/gtest.cc:2599
#19 testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void> (object=0xaaaaf5c74320, 
    method=(void (testing::Test::*)(class testing::Test * const)) 0x20, location=0xffff80fef955 ""the test body"")
    at external/com_google_googletest/googletest/src/gtest.cc:2635
#20 0x0000ffff81043e6c in testing::Test::Run (this=0xaaaaf5c74320) at external/com_google_googletest/googletest/src/gtest.cc:2674
#21 0x0000ffff810454f8 in testing::TestInfo::Run (this=0xaaaaf5c67960) at external/com_google_googletest/googletest/src/gtest.cc:2853
#22 0x0000ffff81046480 in testing::TestSuite::Run (this=0xaaaaf5c740b0) at external/com_google_googletest/googletest/src/gtest.cc:3012
#23 0x0000ffff81057db4 in testing::internal::UnitTestImpl::RunAllTests (this=0xaaaaf5c73d30)
    at external/com_google_googletest/googletest/src/gtest.cc:5870
#24 0x0000ffff81057844 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (
    method=(bool (testing::internal::UnitTestImpl::*)(class testing::internal::UnitTestImpl * const)) 0xffff810579e8 <testing::internal::UnitTestImpl::RunAllTests()>, location=0xffff80fee5f4 ""auxiliary test code (environments or event listeners)"", object=<optimized out>)
--Type <RET> for more, q to quit, c to continue without paging--q
```
",True,"[-0.44464093 -0.31966078 -0.02160631  0.07769825 -0.06874637 -0.630358
 -0.13446982  0.10062164 -0.436103   -0.4029569   0.15279792 -0.14626037
 -0.08435688 -0.01907466 -0.18571755  0.16168055  0.01601361 -0.083599
  0.19902726  0.2961051  -0.2760003  -0.16506815 -0.3600242   0.03451067
  0.28412235 -0.07107147 -0.45982522  0.04612995  0.10652076 -0.02033908
  0.3555215   0.20324232  0.02202577 -0.12446614  0.21689147  0.586036
  0.02726286 -0.5735178  -0.34110174  0.06848995 -0.07570906  0.08159857
  0.3018622  -0.1975135   0.03415285  0.02408807  0.07923555 -0.15734039
  0.10309523 -0.4053204   0.02316222  0.05402544 -0.16243233 -0.49079597
 -0.04475126  0.06020111  0.30821264 -0.11559572 -0.05001236  0.20193318
  0.25904715 -0.04980417 -0.10921221 -0.21185544  0.25686103  0.30772647
  0.3379264  -0.14780664  0.4053322  -0.09055761  0.26493728 -0.10462558
 -0.4462365   0.14764598  0.02503803  0.16268578  0.16575186  0.05906447
  0.440424   -0.21506014 -0.1525543  -0.02409164  0.07925142 -0.39996475
  0.27532798  0.06476414  0.5182806   0.18222067  0.28865105 -0.03067213
  0.14118357  0.5014062  -0.17718868  0.06208216  0.46743137  0.22229253
  0.17374979  0.14027525 -0.13279387 -0.06342262 -0.10425623 -0.11653608
  0.16480146  0.17857924 -0.40743822 -0.32980573  0.13596943  0.04663816
  0.22960585 -0.2644753   0.23177192  0.18362252 -0.16882563  0.00521316
 -0.00785612  0.02483733 -0.06035383 -0.32510033 -0.09317678  0.89685345
  0.17060691 -0.12753293  0.09049237  0.2520718   0.49421525  0.0690154
 -0.02746737  0.02987935  0.09906567 -0.4204722   0.21575765  0.06758416
  0.10734124  0.16308323 -0.02531978  0.10164334  0.05720211  0.01754757
 -0.03480043 -0.33116123 -0.30894148  0.19167548  0.10333475 -0.60493255
  0.0345441  -0.2064722  -0.42542922  0.05912468 -0.22904593  0.24820088
  0.23759739  0.11538361  0.01250052  0.21238247  0.23345281  0.00390954
  0.33497626 -0.10738692  0.03951696 -0.7001544   0.01442219  0.5294974
 -0.24343243 -0.06838562  0.18884131  0.12461804 -0.5218787  -0.34340525
 -0.05674813  0.42423758 -0.29509127 -0.05562253  0.40742016  0.01851072
  0.21974628 -0.26145178 -0.03345208 -0.21899644 -0.06726854  0.7087126
  0.02145453  0.03541995  0.11457115  0.485904    0.07382471  0.34945428
 -0.04935358  0.12817502 -0.13695365 -0.28422284 -0.56368893  0.02180157
  0.06780801 -0.37313822 -0.0686662  -0.16972698  0.16881032 -0.0999497
 -0.06167513 -0.2715966  -0.3390083  -0.06406906 -0.10122043  0.17521748
 -0.00246241 -0.15782353 -0.3486126  -0.17608097 -0.16338679  0.28034568
 -0.2023555  -0.45510477  0.20355979  0.09524507  0.01862014 -0.09969115
  0.18531564  0.03616183  0.04030168  0.44000888  0.15224925 -0.3350659
  0.21323162 -0.3511291   0.05469375  0.00303486 -0.22855915 -0.03061169
 -0.1052559  -0.19640145  0.03733219  0.18422857 -0.08437671  0.15980291
  0.5846313  -0.02620183 -0.36936888 -0.10612972 -0.29221767  0.00426482
 -0.36754283 -0.51073074  0.19146527  0.17543687  0.38329834  0.6510843
  0.25022972  0.07913151 -0.4263077   0.20060174 -0.3296895   0.11791026
  0.41089118 -0.28309888  0.4060779   0.2788595   0.39544523  0.3463378
  0.37283462 -0.42551923  0.29313186  0.25883687  0.00097405  0.04077666
  0.31961125  0.44911152 -0.18492879  0.4241886  -0.07081369 -0.21466677
  0.44951943 -0.23662642  0.4410913  -0.3596882   0.18172991 -0.2589
  0.41508228 -0.17457941 -0.07558782 -0.08901168  0.3344139   0.1615938
 -0.25023723  0.42126864 -0.03395362 -0.18572366  0.02730549 -0.64889747
 -0.32941502 -0.10011225 -0.4848044   0.06797516  0.1628997   0.08685621
 -0.33275583  0.1471555   0.08403125 -0.10601822  0.13634129  0.14351362
 -0.44484213 -0.28775936  0.5088482  -0.68942624 -0.19763939 -0.18477228
  0.40823913  0.39722055  0.5007787  -0.6915535   0.1662161   0.22346577
 -0.1455718   0.36885294 -0.04428074  0.01199819 -0.30664068  0.9026032
  0.17353109 -0.1466199   0.16903904 -0.13260156 -0.43440485 -0.01423642
  0.08782253 -0.19092605 -0.10706515 -0.4625257  -0.05192369  0.28442866
  0.02106301 -0.12250327 -0.2824148  -0.04006938 -0.3124467  -0.05327319
 -0.38032594  0.37080574  0.00400058 -0.3346697  -0.18907896  0.09854712
 -0.17810135  0.0318701  -0.2371139  -0.1430514   0.18190481  0.09608571
  0.02465419  0.05766304  0.08409052  0.40614083 -0.4855146  -0.08074698
 -0.30289698  0.6065509  -0.23326164  0.04226648  0.62646747  0.09219384
  0.0154941   0.05858052 -0.2297366  -0.04719826  0.21619594 -0.24253897
 -0.07681467 -0.27038342  0.1801978   0.19662602 -0.16596434  0.01571764
 -0.41518933  0.27479738  0.59032595 -0.33379507 -0.19084242  0.03738788
  0.05902525  0.04397357  0.08010451 -0.29674667  0.31897622  0.0164657 ]"
Please help !  type:bug TF 1.13 subtype: ubuntu/linux,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

1.13.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.0

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

10.5.0

### CUDA/cuDNN version

10

### GPU model and memory

RTX 3060 12G

### Current behavior?

I  am running the Octopus repo (https://github.com/thmoa/octopus) which use tensorflow-gpu version 1.13.1 . When I run that model with python, I got some errors from tensorflow. Please help me. 

### Standalone code to reproduce the issue

```shell
import os
import argparse
import tensorflow as tf
import keras.backend as K




from glob import glob

from lib.io import openpose_from_file, read_segmentation, write_mesh
from model.octopus import Octopus



def main(weights, name, segm_dir, pose_dir, out_dir, opt_pose_steps, opt_shape_steps):
    segm_files = sorted(glob(os.path.join(segm_dir, '*.png')))
    pose_files = sorted(glob(os.path.join(pose_dir, '*.json')))

    if len(segm_files) != len(pose_files) or len(segm_files) == len(pose_files) == 0:
        exit('Inconsistent input.')

    K.set_session(tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))))

    model = Octopus(num=len(segm_files))
    model.load(weights)

    segmentations = [read_segmentation(f) for f in segm_files]

    joints_2d, face_2d = [], []
    for f in pose_files:
        j, f = openpose_from_file(f)

        assert(len(j) == 25)
        assert(len(f) == 70)

        joints_2d.append(j)
        face_2d.append(f)

    if opt_pose_steps:
        print('Optimizing for pose...')
        model.opt_pose(segmentations, joints_2d, opt_steps=opt_pose_steps)

    if opt_shape_steps:
        print('Optimizing for shape...')
        model.opt_shape(segmentations, joints_2d, face_2d, opt_steps=opt_shape_steps)

    print('Estimating shape...')
    pred = model.predict(segmentations, joints_2d)

    write_mesh('{}/{}.obj'.format(out_dir, name), pred['vertices'][0], pred['faces'])

    print('Done.')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument(
        'name',
        type=str,
        help=""Sample name"")

    parser.add_argument(
        'segm_dir',
        type=str,
        help=""Segmentation images directory"")

    parser.add_argument(
        'pose_dir',
        type=str,
        help=""2D pose keypoints directory"")

    parser.add_argument(
        '--opt_steps_pose', '-p', default=5, type=int,
        help=""Optimization steps pose"")

    parser.add_argument(
        '--opt_steps_shape', '-s', default=15, type=int,
        help=""Optimization steps"")

    parser.add_argument(
        '--out_dir', '-od',
        default='out',
        help='Output directory')

    parser.add_argument(
        '--weights', '-w',
        default='weights/octopus_weights.hdf5',
        help='Model weights file (*.hdf5)')

    args = parser.parse_args()
    main(args.weights, args.name, args.segm_dir, args.pose_dir, args.out_dir, args.opt_steps_pose, args.opt_steps_shape)

```


### Relevant log output

```shell
Processing sample...
> Optimizing for pose...
  0%|                                                                                                                                                                                                                 | 0/10 [00:00<?, ?it/s]2023-08-22 16:24:18.296359: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2023-08-22 16:25:50.156420: I tensorflow/core/kernels/cuda_solvers.cc:159] Creating CudaSolver handles for stream 0x55fa094fdcf0
2023-08-22 16:26:08.284736: E tensorflow/stream_executor/cuda/cuda_blas.cc:698] failed to run cuBLAS routine cublasGemmBatchedEx: CUBLAS_STATUS_EXECUTION_FAILED
2023-08-22 16:26:08.284773: E tensorflow/stream_executor/cuda/cuda_blas.cc:2620] Internal: failed BLAS call, see log for details
2023-08-22 16:26:08.326578: I tensorflow/stream_executor/stream.cc:5014] [stream=0x55fa0950bb90,impl=0x55fa093dbf20] did not memcpy device-to-host; source: 0x813bc6700
2023-08-22 16:26:08.326623: F tensorflow/core/framework/op_kernel.cc:1408] Check failed: nullptr == ctx->op_kernel().AsAsync() (nullptr vs. 0x55fa38108400)Use OP_REQUIRES_ASYNC in AsyncOpKernel implementations.
Aborted
```
",True,"[-7.91988432e-01 -5.34777522e-01 -7.76372552e-02  2.06817426e-02
  2.49139965e-01 -5.01438260e-01 -1.34817362e-02 -1.26693845e-01
 -2.65635103e-01 -3.34629267e-01  1.27942145e-01 -1.44376811e-02
 -2.61023700e-01  9.09726024e-02 -1.98841006e-01  5.12750447e-01
 -2.26992443e-01 -7.64882714e-02  2.11872995e-01  1.24875270e-03
 -2.82761216e-01 -1.46588162e-01 -4.17968750e-01  3.02226275e-01
  1.08505011e-01  2.01031625e-01 -3.29526067e-01  4.40285914e-02
 -9.32244770e-03  6.73020333e-02  5.24534822e-01  1.99442089e-01
 -4.60911393e-02  1.75014511e-02  5.26317358e-02  3.92234683e-01
 -1.42521769e-01 -2.59072334e-01 -3.35385442e-01 -2.12288275e-03
 -1.47155195e-01  1.17423888e-02  1.86713874e-01 -2.70755798e-01
  9.11864936e-02 -2.73326039e-01 -4.43269312e-02 -1.55143932e-01
  3.88005301e-02 -3.22405398e-01  1.13418087e-01 -4.77408320e-02
 -5.59589505e-01 -2.51727164e-01 -1.00906149e-01 -4.14690487e-02
  3.20203677e-02 -9.91442502e-02  4.00383864e-03  2.37284705e-01
  1.37772709e-01  7.08843991e-02  2.42595561e-02  8.74718279e-03
  1.28674805e-01  1.66907743e-01  4.02802229e-01 -1.22138523e-01
  5.37797928e-01 -2.81664550e-01  9.24606174e-02 -1.00377955e-01
 -3.05781335e-01  1.78141475e-01  1.22340634e-01  2.75745131e-02
  1.45623803e-01  2.02663273e-01  3.18414658e-01 -9.59637314e-02
  1.36114925e-01 -1.62697569e-01  7.97541253e-03 -3.77463013e-01
  5.09988628e-02 -8.49758275e-04  4.05877501e-01  4.80146408e-02
  4.72769231e-01 -2.42382392e-01  3.76502007e-01  3.13485384e-01
 -7.19259381e-02  9.02358368e-02  4.29639161e-01  5.54572903e-02
  1.91539794e-01  1.82036832e-01  2.01768205e-02 -1.04597703e-01
 -1.46806479e-01 -2.89544910e-01  6.24869801e-02  5.94865605e-02
 -2.04129055e-01 -1.57682121e-01  3.01792681e-01 -7.70930946e-02
  9.62497592e-02 -1.80209056e-01 -1.61904357e-02  1.37198810e-02
  2.36908689e-01 -1.38585299e-01  2.70427577e-02 -1.52503580e-01
 -1.35520339e-01 -1.20339207e-01  5.49606830e-02  8.04760218e-01
  6.27326313e-03 -2.04324886e-01 -2.44209319e-02  2.44910970e-01
  6.71343923e-01  2.62873828e-01 -1.67004362e-01 -3.01640294e-03
  2.22741991e-01 -7.06339255e-04  1.51147991e-01  7.32698441e-02
  1.71308428e-01  3.98507953e-01 -2.47672107e-03  1.46724045e-01
 -2.78420895e-01 -9.91600286e-03 -9.46121514e-02 -3.98282230e-01
 -2.79121041e-01  2.68898100e-01 -2.39428997e-01 -7.23057687e-01
  7.61717185e-02 -9.54954177e-02 -4.67890948e-01  2.89055586e-01
 -3.57556462e-01  2.41885230e-01 -3.11180241e-02  1.02634497e-01
  4.71861698e-02  3.42574447e-01  2.21653078e-02  2.26017714e-01
  4.99534518e-01 -1.74296081e-01  1.07591093e-01 -6.15202487e-01
  5.99926747e-02  5.16292095e-01 -8.98754150e-02 -1.89104438e-01
  2.01396868e-01  1.58380687e-01 -4.89744365e-01 -3.01238537e-01
  1.92579180e-01  5.33574820e-01 -1.41753793e-01 -1.32349819e-01
  2.81608522e-01  3.03722918e-01  2.02291638e-01 -8.64258111e-02
  3.39251995e-01 -6.60997927e-01 -1.13224149e-01  6.73249364e-01
  9.51549485e-02  4.49366122e-03  8.49945024e-02  2.99199581e-01
  1.27236634e-01  1.02172479e-01  1.20742097e-01  2.00313449e-01
 -2.40121543e-01 -4.58019823e-02 -4.85729367e-01 -1.65518403e-01
  5.56664348e-01 -2.46930197e-01 -1.17569484e-01  2.07473487e-01
  2.60266632e-01 -1.43524095e-01  8.39874707e-03 -1.63989794e-03
 -2.68572271e-01 -4.21889275e-02 -2.52699494e-01  1.93268239e-01
  3.43861654e-02 -2.82324553e-01 -4.18052450e-03 -3.46292377e-01
 -5.05761206e-01  1.42926663e-01  2.18680948e-01 -4.45731163e-01
  1.26613796e-01 -9.30023417e-02 -2.73825407e-01  2.95894742e-02
  2.41425470e-01 -4.70020100e-02 -2.86712170e-01  3.32951784e-01
  2.69495517e-01 -3.28500509e-01  1.70168623e-01 -3.64523143e-01
 -1.99608594e-01  4.76924665e-02 -4.95359987e-01  1.23033458e-02
 -1.62513167e-01  3.14394474e-01  2.66815573e-02  1.51791766e-01
  3.30993056e-01  3.27942193e-01  5.71850955e-01 -7.80263096e-02
 -2.78172910e-01 -2.86949694e-01 -1.81048051e-01  9.39531624e-02
 -4.18697953e-01 -1.80492252e-01 -4.99369502e-02  8.20133686e-02
  3.00511241e-01  4.74968255e-01 -2.11652532e-01 -1.29727483e-01
 -4.68131423e-01  3.14756781e-01 -1.94382608e-01  2.52174646e-01
  2.60335505e-01  4.20879945e-02  4.10131872e-01  3.66086125e-01
  3.45306575e-01  1.56012043e-01  2.56598532e-01 -2.90903658e-01
  2.79697269e-01  1.11687288e-01  7.46680796e-02  3.74525517e-01
  1.88843817e-01  4.28117037e-01 -2.50619531e-01  5.07652342e-01
  3.63046974e-02 -1.40384048e-01  7.61811249e-03 -2.90731639e-01
  6.46191776e-01 -3.90581548e-01  1.20216802e-01 -3.98864746e-01
  3.50513756e-01 -1.75253615e-01  9.00269079e-04 -1.76525339e-02
  1.39530867e-01  2.68098682e-01 -1.68681502e-01  8.38060230e-02
 -5.84707819e-02 -1.96226481e-02 -1.25424325e-01 -9.13112223e-01
 -2.53981918e-01 -1.18014038e-01 -3.30682904e-01  1.10337883e-01
  5.86238466e-02 -2.26606146e-01 -2.54997730e-01  1.24047197e-01
  1.52990982e-01 -6.15997054e-02  1.91543233e-02  8.26383680e-02
 -8.33220482e-02 -1.26075357e-01  3.95816356e-01 -5.11732936e-01
 -4.93785888e-02 -7.32275322e-02  4.12954241e-01  2.22347975e-01
  4.24971908e-01 -4.87885416e-01  1.28046170e-01 -1.21279374e-01
 -2.69882262e-01  3.35101783e-01 -1.19200293e-02  1.88258290e-01
 -4.57021147e-01  6.97481453e-01  2.31934547e-01 -1.50887191e-01
  2.43110359e-01 -3.15497994e-01 -5.08033931e-01 -2.46050898e-02
  1.65813908e-01 -2.59675980e-01 -5.23764193e-02 -3.86067808e-01
  8.33676979e-02  2.02622741e-01 -1.95493191e-01 -1.23488810e-03
 -5.97181134e-02 -1.86434723e-02 -3.66308033e-01 -7.01769441e-02
 -4.57085431e-01  2.12008953e-01 -7.32129589e-02 -2.93901861e-01
 -1.90700203e-01 -3.58533859e-02 -1.97144032e-01 -2.40617543e-01
  7.09757656e-02 -3.94263744e-01  3.29835355e-01  5.13323903e-01
 -1.51072413e-01  1.24858201e-01 -2.06292421e-03  2.48948589e-01
 -4.26569372e-01  6.82492703e-02  7.72793517e-02  5.36830783e-01
 -2.68978588e-02 -1.16741747e-01  4.61884618e-01  2.04913855e-01
 -1.23543348e-02  1.67687178e-01 -3.63676250e-01  1.15032546e-01
  1.27399594e-01 -2.25309566e-01 -3.30102682e-01 -1.38577878e-01
  2.46464655e-01  3.11997056e-01 -8.23088288e-02  2.21579731e-01
 -3.07410896e-01  3.73569787e-01  6.24423623e-01 -5.27970910e-01
 -3.27518404e-01  1.36882290e-01  2.62178242e-01 -1.25348389e-01
  6.94202781e-02 -1.64301038e-01  3.32176656e-01  2.62595695e-02]"
TF Quantizer breaks pip package type:bug comp:core,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

16.0.6

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

The commit https://github.com/tensorflow/tensorflow/commit/fee881376914381062deb767759d144d0f07d760 added pywrap_quantize_model.so to the pip package but the build fails to set the RPATH for _pywrap_tensorflow_internal.so in the new .so leading to a failure in auditwheel when attempting to 'repair' to make it manylinux2014 compatible.

### Standalone code to reproduce the issue

```shell
$ python3 -m auditwheel repair --plat manylinux2014_aarch64 ./tensorflow-pkg/tensorflow_aarch64-2.15.0-cp311-cp311-linux_aarch64.whl --wheel-dir ./whl/
```


### Relevant log output

```shell
$ python3 -m auditwheel repair --plat manylinux2014_aarch64 ./tensorflow-pkg/tensorflow_aarch64-2.15.0-cp311-cp311-linux_aarch64.whl --wheel-dir ./whl/
INFO:auditwheel.main_repair:Repairing tensorflow_aarch64-2.15.0-cp311-cp311-linux_aarch64.whl
Traceback (most recent call last):
  File ""<frozen runpy>"", line 198, in _run_module_as_main
  File ""<frozen runpy>"", line 88, in _run_code
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/__main__.py"", line 6, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/main.py"", line 59, in main
    rval = args.func(args, p)
           ^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/main_repair.py"", line 173, in execute
    out_wheel = repair_wheel(
                ^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/repair.py"", line 80, in repair_wheel
    raise ValueError(
ValueError: Cannot repair wheel, because required library ""_pywrap_tensorflow_internal.so"" could not be located

Also

$ ldd venv_bad/lib/python3.11/site-packages/tensorflow/compiler/mlir/quantization/tensorflow/python/pywrap_quantize_model.so
	linux-vdso.so.1 (0x0000ffffb8af9000)
	libtensorflow_framework.so.2 => /workspace/venv_bad/lib/python3.11/site-packages/tensorflow/compiler/mlir/quantization/tensorflow/python/../../../../../libtensorflow_framework.so.2 (0x0000ffffb6800000)
	_pywrap_tensorflow_internal.so => not found
	libdl.so.2 => /lib/aarch64-linux-gnu/libdl.so.2 (0x0000ffffb8923000)
	libpthread.so.0 => /lib/aarch64-linux-gnu/libpthread.so.0 (0x0000ffffb88f2000)
	libm.so.6 => /lib/aarch64-linux-gnu/libm.so.6 (0x0000ffffb8847000)
	libstdc++.so.6 => /lib/aarch64-linux-gnu/libstdc++.so.6 (0x0000ffffb661b000)
	libgcc_s.so.1 => /lib/aarch64-linux-gnu/libgcc_s.so.1 (0x0000ffffb8823000)
	libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000ffffb64a8000)
	/lib/ld-linux-aarch64.so.1 (0x0000ffffb8ac9000)
	librt.so.1 => /lib/aarch64-linux-gnu/librt.so.1 (0x0000ffffb8809000)

Compare with

$ ldd venv_bad/lib/python3.11/site-packages/tensorflow/python/saved_model/pywrap_saved_model.so
	linux-vdso.so.1 (0x0000ffffa40a2000)
	libtensorflow_framework.so.2 => /workspace/venv_bad/lib/python3.11/site-packages/tensorflow/python/saved_model/../../libtensorflow_framework.so.2 (0x0000ffffa1e00000)
	_pywrap_tensorflow_internal.so => /workspace/venv_bad/lib/python3.11/site-packages/tensorflow/python/saved_model/../_pywrap_tensorflow_internal.so (0x0000ffffa1c0a000)
	libdl.so.2 => /lib/aarch64-linux-gnu/libdl.so.2 (0x0000ffffa3ee5000)
	libm.so.6 => /lib/aarch64-linux-gnu/libm.so.6 (0x0000ffffa3e3a000)
	libpthread.so.0 => /lib/aarch64-linux-gnu/libpthread.so.0 (0x0000ffffa3e09000)
	libstdc++.so.6 => /lib/aarch64-linux-gnu/libstdc++.so.6 (0x0000ffffa1a25000)
	libgcc_s.so.1 => /lib/aarch64-linux-gnu/libgcc_s.so.1 (0x0000ffffa3de5000)
	libc.so.6 => /lib/aarch64-linux-gnu/libc.so.6 (0x0000ffffa18b2000)
	/lib/ld-linux-aarch64.so.1 (0x0000ffffa4072000)
	librt.so.1 => /lib/aarch64-linux-gnu/librt.so.1 (0x0000ffffa3dcd000)
	libtensorflow_cc.so.2 => /workspace/venv_bad/lib/python3.11/site-packages/tensorflow/python/saved_model/../../libtensorflow_cc.so.2 (0x0000ffff8be00000)
	libml_dtypes.so.so => /workspace/venv_bad/lib/python3.11/site-packages/tensorflow/python/saved_model/../../tsl/python/lib/core/libml_dtypes.so.so (0x0000ffffa3d99000)
	libomp.so.5 => /usr/lib/llvm-16/lib/libomp.so.5 (0x0000ffff8bcc0000)
```
",True,"[-0.5721873  -0.4026791   0.00830667 -0.15364751  0.25976694 -0.21013997
 -0.01861858 -0.01995921 -0.24892125 -0.2009083   0.01950095 -0.03513832
 -0.24324074  0.22853674 -0.04998313  0.1651276  -0.1682242  -0.06653534
  0.10345696 -0.03351852 -0.24414167  0.0978191  -0.33383685  0.39022285
  0.29500204  0.07975465 -0.32353666 -0.1973055   0.03195824  0.13125426
  0.23964521  0.17495151 -0.06728754  0.10729926  0.2335622   0.25282964
 -0.07773606 -0.14945126 -0.38168275 -0.11540005  0.00576644  0.01062026
 -0.01240765 -0.11299527 -0.05630268 -0.31150216 -0.08292897 -0.2557406
 -0.14036049 -0.04124806 -0.14225572  0.29986182 -0.543863    0.0256631
  0.05004906 -0.28536904  0.24798396  0.24479836  0.09245531  0.33596757
 -0.10851533  0.02546118 -0.16328183  0.12462408  0.18106428  0.18126819
  0.2856482  -0.11069772  0.23254944 -0.4312256   0.01739323 -0.2658152
 -0.46841657  0.07119478  0.16501778 -0.02562864 -0.07812662  0.41367364
  0.23668024 -0.00835321  0.10065411 -0.22290003  0.03667171  0.04892319
 -0.08193178  0.04772446  0.02391659 -0.00551037  0.5200761  -0.2849236
  0.12639761  0.41291407  0.2786273   0.18592624  0.63247484  0.00783723
  0.02878376  0.24029964  0.02522677 -0.02010051 -0.1023114  -0.37889302
 -0.34443814 -0.0123736  -0.14462566 -0.05811815  0.26019207  0.08746426
  0.27719158 -0.07161035  0.08163719  0.26104528  0.0642966  -0.42471108
 -0.20001972 -0.03888366  0.10952385 -0.09663001  0.10497043  0.66076297
  0.25048137 -0.13446243 -0.1333912   0.26583767  0.4220897   0.19222894
  0.1830984   0.11341037  0.14548862 -0.1135961   0.2503047   0.18229517
  0.08871444  0.1233114   0.25997043 -0.07149041 -0.31706125 -0.16904368
 -0.26453277 -0.1375334  -0.6815164   0.34527838 -0.18454142 -0.6610118
  0.08638175  0.19248995 -0.36109573  0.42141443 -0.19727106  0.15833662
 -0.06089941  0.08928916  0.22986741  0.53923464  0.23615247  0.08573526
  0.28108364 -0.01863154 -0.07148986 -0.33296335 -0.21045813  0.29485142
 -0.20785835 -0.15006372  0.06054558  0.24061045 -0.6691748  -0.12631689
  0.07754441  0.27450415 -0.44563633 -0.16991296  0.25121227 -0.10308325
  0.13449727 -0.02328636  0.35554382 -0.31549728  0.09385594  0.39527762
  0.14621046  0.26378068  0.15718706  0.0503157   0.20117643  0.03457
  0.15735438  0.22486176 -0.1082284   0.03476119 -0.3640927  -0.11935769
  0.3947842  -0.12356669 -0.23674446  0.2160984   0.2843611   0.0323865
 -0.13850719  0.12614192 -0.13023919 -0.0941363   0.06499575  0.07291436
 -0.03428832 -0.07826696  0.04057388 -0.5631229  -0.4881171   0.05425319
  0.1221624  -0.20179373 -0.00705855 -0.4117248  -0.29521883  0.13605854
 -0.12562218 -0.25293815 -0.13646841  0.18319449  0.10218655 -0.1958557
  0.04520287 -0.43212405 -0.10786875  0.07082377 -0.50173175  0.44321162
 -0.2061688  -0.0252156  -0.05497506  0.18041119  0.44994068  0.37745106
  0.5951346  -0.1535778  -0.22170591 -0.12464347 -0.29628247  0.14527784
 -0.06305383 -0.32863706  0.01508125 -0.06916036  0.05825286  0.33559984
 -0.11038582 -0.13320047 -0.44844094  0.3653996  -0.3468064  -0.03117534
  0.36227298  0.18787643  0.42320365  0.16811043  0.18581004  0.15849598
  0.3114608  -0.26847118  0.1595121   0.43924874  0.12406486  0.3130375
  0.3098575   0.15388961 -0.23918152  0.5269836  -0.21979438 -0.10721014
  0.26036853 -0.29449314  0.57195854 -0.15501893  0.22608785 -0.04370994
  0.4625275  -0.13071468 -0.07958879  0.1530147   0.00706413  0.4212098
 -0.5120455   0.3248245   0.0037196   0.05150463 -0.03467612 -0.53243756
 -0.15590936 -0.09211654 -0.46701068  0.07220799 -0.15657663  0.04457689
 -0.04434957  0.12904823  0.02821543 -0.03238194  0.26350006  0.02058765
 -0.0387283   0.04880704  0.24234684 -0.08303377 -0.15683338 -0.07513648
  0.14079946  0.234683    0.42747578 -0.4616835  -0.00233576  0.01906559
 -0.14150201  0.4403466  -0.17857856  0.14046541 -0.4890045   0.5319723
 -0.02856774 -0.14857395  0.32194024 -0.20430167 -0.71846175  0.02675896
  0.12603283 -0.00428016 -0.1657293  -0.22979778 -0.01353884  0.29256773
 -0.12533656  0.15917215 -0.3231048   0.11989388 -0.2533495  -0.14522417
 -0.24193116  0.15155996 -0.05619606 -0.34073046 -0.17941734 -0.00782236
 -0.03384796 -0.26875773 -0.10891207 -0.48071486  0.22615594  0.35105282
  0.1306613   0.22375996 -0.16599289  0.13602991 -0.5608247   0.1584963
 -0.09809916  0.49891084 -0.2392598  -0.02013435  0.38551176  0.14831148
 -0.23555739  0.16788247 -0.42543814  0.01242591  0.03597777 -0.16702671
 -0.24652246 -0.16068844 -0.12636763  0.5417681  -0.0748753   0.42374814
 -0.520846    0.26769876  0.5761114  -0.35630882 -0.0509851   0.02662419
  0.11500038 -0.27280408 -0.26979297 -0.06291653  0.46668258  0.08063365]"
python3.11.3 mismatch with tensorflow 2.13.0 stat:awaiting response type:bug stale subtype:macOS TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS Venture 13.4

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

![image](https://github.com/tensorflow/tensorflow/assets/11846497/3459b246-e377-499d-9d9f-e5f666fb7957)


### Standalone code to reproduce the issue

```shell
import tensorflow
```


### Relevant log output

_No response_",True,"[-0.4415298  -0.49283034 -0.1688447   0.24288446  0.28509218 -0.42988485
 -0.06124553  0.0327094  -0.34077555 -0.46870756  0.07669966 -0.13239054
 -0.23892072  0.11060736 -0.19584718  0.2681266  -0.3581019  -0.13314608
  0.16346875  0.00374243 -0.10784015 -0.06306052 -0.27414933  0.3030893
  0.17236438  0.1874764  -0.2361288  -0.13996667 -0.12434451  0.20751253
  0.3576528   0.24694102  0.0225779   0.00845789  0.09718356  0.2416437
 -0.22649434 -0.34572914 -0.39731565  0.05371841  0.00466772  0.09763552
  0.24401763 -0.13862851  0.0347534  -0.2405806  -0.05212504 -0.2153937
  0.0131052  -0.25916997 -0.10461724  0.02965911 -0.31617123 -0.47405165
 -0.16353971 -0.07743237  0.01387578 -0.03778022  0.03178253  0.06749727
  0.07676888  0.07639625  0.10097101 -0.05319613  0.34113088  0.18630323
  0.27442557 -0.1740214   0.519069   -0.00929256  0.1878874  -0.12383202
 -0.4947154   0.20168513 -0.11726637  0.11428597  0.0871664   0.04956376
  0.31936464 -0.1400374  -0.1488759  -0.2312038   0.01884324 -0.17448735
  0.22439912 -0.16348329  0.37510103 -0.00801939  0.42904615 -0.07696731
  0.5988962   0.28776646  0.07411771  0.09752649  0.49371767 -0.03104239
  0.22150183  0.29619905 -0.02035399 -0.06298441 -0.08171524 -0.04604042
  0.09082894  0.03677455  0.13063335 -0.10042191  0.25964692 -0.2609883
  0.21443409 -0.04407572  0.08827464 -0.07529053  0.38546628 -0.04565375
 -0.05238974 -0.00118843 -0.14205939  0.2263246   0.06646578  0.9864718
  0.01415141 -0.06987118 -0.10977608  0.29180735  0.36954618  0.1793159
 -0.22067028 -0.08264349  0.11036276 -0.13156465  0.21158049  0.00281687
 -0.16509488  0.2830193  -0.03310516  0.03081795 -0.16067804 -0.14703675
 -0.306447   -0.03408043 -0.17485118  0.08343206 -0.21282169 -0.5471552
  0.18544838  0.11542653 -0.30980137  0.33669233 -0.21099448  0.0921993
 -0.02605881  0.04721925 -0.22540416  0.41874382  0.14832833  0.11838616
  0.38699946 -0.0619487  -0.00616212 -0.49951226  0.16749492  0.57159376
 -0.21857552 -0.17094035  0.19013551  0.14962064 -0.39065468 -0.2427803
 -0.01128653  0.4285194  -0.05795111 -0.1324169   0.11755487  0.06814197
  0.11484727 -0.16888174  0.16640575 -0.6389096   0.04644452  0.24502206
 -0.00558707  0.23075977 -0.03989576  0.15595998  0.00310872  0.07254483
  0.05514622  0.13511515 -0.18544292  0.19113997 -0.33714592 -0.17027643
  0.59142065 -0.07273964  0.00463116  0.14848346  0.13114649 -0.14650877
 -0.00160728  0.11560515 -0.18072063 -0.09553304 -0.05550038 -0.03152213
  0.1136037  -0.5377062  -0.11840515 -0.27273142 -0.3282038   0.0756232
  0.1541967  -0.5469669  -0.00304964 -0.06355308 -0.4025442   0.14465697
  0.31858006  0.14060307 -0.22769728  0.16749477  0.03831076 -0.132662
 -0.02650121 -0.3286008  -0.18968695  0.06408383 -0.3138245   0.0550372
 -0.26398396  0.2175723   0.16889286  0.1406584   0.43309772  0.27674192
  0.36535665 -0.2226466  -0.09281766 -0.19771942  0.01195379  0.08449349
 -0.3228078   0.02008869  0.00757507 -0.02269525  0.26191455  0.246322
 -0.1975896  -0.24014318 -0.49267554  0.2808512  -0.26864663  0.21526405
  0.30826238  0.08125249  0.44751936  0.26570666  0.23575564  0.06761063
  0.2197108  -0.19900505  0.2564429   0.13307592  0.0959255   0.3849461
  0.04668389  0.2923318  -0.37161475  0.5913      0.2975419  -0.13112164
  0.00993065 -0.3359314   0.68734634 -0.47657984 -0.00891024 -0.14120403
  0.2972908   0.05167066 -0.10071594  0.01014355 -0.02855573  0.3719551
 -0.50080717  0.06718034 -0.07462598 -0.14429155 -0.06333295 -0.6147338
 -0.03409965  0.1364857  -0.335402    0.10543065  0.13210382 -0.02695058
 -0.12794301  0.24390951  0.05003288 -0.15039831  0.05139114  0.29363036
 -0.18862756  0.07291218  0.2253267  -0.46924382 -0.00579662 -0.08051465
  0.38786477  0.21351895  0.23421524 -0.36601722  0.19691    -0.16495632
 -0.13815573  0.5104383   0.10564174  0.0265312  -0.4358685   0.7243382
  0.3129179  -0.0562918   0.03470526 -0.19983713 -0.365945    0.1152328
  0.44026342 -0.12613615 -0.04075247 -0.35715383 -0.06174319  0.06509012
 -0.1613449  -0.05729204 -0.19481531 -0.01294198 -0.09080662  0.01400837
 -0.50723135  0.13651723 -0.01852598 -0.26397425 -0.14282776 -0.08820808
 -0.18293679 -0.21817374  0.03916117 -0.38030034  0.53269374  0.6566955
 -0.23724926 -0.06940228  0.07521144  0.15893891 -0.47385815 -0.07269555
 -0.07290967  0.39354998  0.05303704 -0.27958402  0.34968537  0.17548536
 -0.34675753  0.01643723 -0.4267792   0.13924633  0.14455085 -0.30439678
 -0.23851457 -0.11709204  0.14184871  0.21907625 -0.10214716  0.07781596
 -0.2717117   0.41094887  0.5516356  -0.4772035  -0.34754497  0.1303479
  0.02385499 -0.1299841   0.01607573 -0.11006196  0.22428936 -0.04955402]"
"  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/lite/python/interpreter.py"", line 915, in invoke     self._interpreter.Invoke() RuntimeError: tensorflow/lite/kernels/concatenation.cc:158 t->dims->data[d] != t0->dims->data[d] (1 != 2)Node number 304 (CONCATENATION) failed to prepare. stat:awaiting response type:bug stale comp:lite TFLiteConverter TF 2.9","### 1. System information
![image](https://github.com/tensorflow/tensorflow/assets/35233355/cfc8e9d3-9790-4a89-86db-062b97d3bbbc)

- OS Platform and Distribution (Linux Ubuntu 20.04):
- TensorFlow installation (2.9.1+nv22.9):

### 2. Code
`import numpy as np
import tensorflow as tf

def main():
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path=""model.tflite"")
    interpreter.allocate_tensors()
    
    #return

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    # Test the model on random input data.

    input_shape = input_details[0]['shape']
    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
    #interpreter.set_tensor(input_details[0]['index'], input_data)

    signatures = interpreter.get_signature_list()
    print(signatures)

    interpreter.invoke()
    return
    # The function `get_tensor()` returns a copy of the tensor data.
    # Use `tensor()` in order to get a pointer to the tensor.
    output_data = interpreter.get_tensor(output_details[0]['index'])
    print(output_data)


if __name__ == '__main__':
    main()`

**How to get the model:**
I used a library here to get a unet with efficientnetb0 as encoder
https://github.com/qubvel/segmentation_models.
So you can try to use my script to get the model:
`    BACKBONE = 'efficientnetb0'
    BATCH_SIZE = 1
    CLASSES = [""background"", ""target"", ""others""]
    LR = 0.0001
    EPOCHS = 10

    preprocess_input = sm.get_preprocessing(BACKBONE)

    # define network parameters
    n_classes = 1 if len(CLASSES) == 1 else (len(CLASSES) + 1)  # case for binary and multiclass segmentation
    activation = 'sigmoid' if n_classes == 1 else 'softmax'

    #create model
    model = sm.Unet(BACKBONE, classes=n_classes, activation=activation)`

",True,"[-2.69058228e-01 -4.81091917e-01 -3.29311877e-01  1.81190878e-01
 -3.66367698e-02 -3.25163335e-01 -3.36548313e-04  8.49317685e-02
 -3.73672187e-01  2.48942133e-02 -1.09536618e-01 -1.38861477e-01
 -5.53987324e-02  9.45687145e-02 -4.63819019e-02  1.85995623e-01
 -2.87933201e-02 -3.51797044e-01  7.57166222e-02 -4.63168100e-02
  1.84528857e-01  1.20279059e-01 -2.07231149e-01  2.55841538e-02
  1.39913902e-01  5.45317121e-03 -9.09486786e-02 -1.28941491e-01
  7.65840486e-02  4.24153358e-02  2.94648021e-01  1.03583410e-01
 -2.61006624e-01  2.28747591e-01  7.31800403e-03  2.83345759e-01
  3.84580530e-03 -2.50432253e-01 -6.35564551e-02  5.29568121e-02
  7.70478845e-02  2.43424892e-01  6.44141883e-02  8.49679783e-02
  6.42712042e-02 -2.94120554e-02 -2.89451256e-02  4.30825129e-02
 -4.18305993e-02 -6.16076104e-02 -1.49241388e-01  2.91851342e-01
 -1.76447093e-01 -2.20993370e-01  1.33154795e-01 -1.11451566e-01
  5.17024659e-02  2.01882616e-01 -6.85470998e-02 -4.43531871e-02
  7.08758831e-02 -1.23861268e-01 -7.12032989e-02  7.91231021e-02
 -1.97643191e-01  2.20432371e-01  2.52919286e-01 -1.00660324e-01
  4.38037068e-01 -3.24919045e-01 -1.25270963e-01  1.01116523e-01
 -3.22682470e-01  5.60816675e-02  7.84358382e-02  1.63253322e-01
 -2.56144404e-01  2.07293972e-01  9.88797694e-02 -1.92986935e-01
  4.47224453e-02 -9.66555774e-02  1.95391566e-01 -1.08207509e-01
  2.22837865e-01  1.58418305e-02  1.85212567e-01  3.51767913e-02
  2.63817519e-01  1.25645220e-01  3.39998126e-01  1.43790081e-01
 -2.15730026e-01  2.48242110e-01  2.67108455e-02 -5.50049916e-02
  2.30242804e-01 -6.60854280e-02 -1.23488776e-01 -1.01177722e-01
 -1.26755804e-01 -2.53702819e-01 -1.85745388e-01  1.06977105e-01
  1.12790033e-01 -4.26963344e-02  3.57607961e-01  2.20304862e-01
  8.81438553e-02 -1.10562474e-01  2.04564244e-01 -1.49890840e-01
  2.05260009e-01 -9.54161659e-02  1.52695969e-01 -6.33173436e-02
 -1.71194896e-02  6.03832006e-02  1.06926616e-02  3.25543404e-01
 -4.23045047e-02  2.11556226e-01  1.02123385e-02  2.02272639e-01
  2.41977870e-01 -1.90646518e-02  2.16737524e-01 -2.53580511e-04
 -1.70989912e-02 -1.48860356e-02  1.89565733e-01  1.89021915e-01
 -2.40171239e-01  1.37058198e-01  1.42341793e-01 -1.23071605e-02
 -2.12114900e-01 -2.26715505e-01  6.25660941e-02 -8.88300166e-02
 -6.34080470e-02 -1.25950336e-01 -2.95657404e-02 -3.59759361e-01
  9.81114283e-02  1.00910224e-01 -2.81857312e-01  1.91136330e-01
 -1.91220175e-03 -8.28569382e-02 -7.20585659e-02  1.42906547e-01
 -3.80068243e-01  3.21144342e-01 -7.84764998e-04  3.64832640e-01
  4.04762506e-01 -7.15010613e-02 -1.15497671e-02 -3.61625820e-01
  8.60995147e-03  2.67436564e-01 -2.07834214e-01  7.55616464e-03
  1.02967754e-01  5.72689101e-02 -3.19480419e-01 -5.24066016e-02
 -1.35746896e-01  7.45683759e-02  4.07840218e-03 -5.24561629e-02
 -1.71927005e-01 -1.19199961e-01  1.66567743e-01  1.42904386e-01
  1.60483316e-01 -3.76157403e-01 -1.50270388e-01  2.21800357e-01
  2.75996685e-01  3.22649240e-01  2.84750164e-02  1.39126912e-01
 -1.04796618e-01  6.18297011e-02  1.17402770e-01  5.23139387e-02
 -1.62603453e-01  6.30342513e-02 -2.82638162e-01 -3.97761837e-02
  1.24958567e-01 -3.03098738e-01 -1.48704320e-01 -8.16613138e-02
  1.22011662e-01  1.43107861e-01  7.96263143e-02  2.08672419e-01
 -3.46060172e-02 -8.55264813e-02  1.45041183e-01  4.97417338e-02
  2.77171314e-01 -1.04291953e-01 -2.60904610e-01 -2.87850559e-01
 -7.36464933e-03  2.66769361e-02 -1.99409902e-01 -3.04521441e-01
 -4.90182862e-02  2.06790529e-02 -2.65114427e-01 -1.93081647e-01
 -1.57524589e-02 -3.27250957e-02 -1.90424874e-01  4.09920812e-02
  5.19890562e-02 -9.34025198e-02 -4.30480093e-02 -4.30599213e-01
  1.20115250e-01 -8.38208012e-03 -1.35176361e-01  1.34298325e-01
  8.40548426e-02  1.31626293e-01  1.27841324e-01 -1.99996814e-01
  3.58735412e-01  4.38161120e-02  2.21494973e-01 -3.20666023e-02
 -9.41105485e-02 -2.28701383e-02 -1.56937152e-01  1.41021758e-01
 -1.97681502e-01 -8.23288411e-02  1.83339611e-01 -1.32848695e-01
 -1.66626140e-01 -9.10779685e-02 -1.59976661e-01 -1.36906669e-01
 -3.27816725e-01  3.49329561e-01 -1.48633078e-01 -6.86374307e-02
  1.30984098e-01  1.30407840e-01  3.23436528e-01  2.14329451e-01
  3.74678187e-02 -4.57035601e-02 -3.62808295e-02  5.10974452e-02
  9.44680646e-02  1.40425444e-01 -9.02439058e-02  4.76338685e-01
  1.59727871e-01  1.91079512e-01 -3.09310526e-01  2.01960966e-01
  4.72411960e-02  3.15694399e-02  1.17149316e-01 -2.28397459e-01
  2.35748082e-01 -8.55726004e-02  6.79344684e-02 -1.21830180e-01
  2.57617742e-01 -2.21391022e-03 -1.54770821e-01 -1.52220622e-01
 -1.36804655e-02  3.46987069e-01 -3.56546104e-01 -1.63310885e-01
  1.55581504e-01 -2.53233433e-01 -7.72874802e-04 -1.71509951e-01
 -2.09017590e-01 -5.45140281e-02  1.42604662e-02  3.25458705e-01
  5.39624751e-01 -2.14346666e-02 -5.53658605e-02 -2.42911559e-02
  1.33422345e-01 -5.94550930e-02  1.49881572e-01  1.32403195e-01
 -7.90232047e-02 -2.27618128e-01  7.08531886e-02  5.50703704e-02
  1.71619326e-01 -9.93425697e-02  2.98649967e-01  1.84174292e-02
  2.93202460e-01 -2.98419535e-01  3.65963817e-01  2.39087045e-01
 -5.61155267e-02  4.67683315e-01 -2.43104011e-01 -2.21511498e-02
 -8.62582028e-02  4.69937831e-01  1.85540751e-01 -1.25593439e-01
 -9.56812352e-02 -1.86422706e-01 -3.24841797e-01  5.99344075e-02
  2.75785904e-02  2.29408219e-02 -1.88184440e-01 -1.02424435e-01
 -4.08633947e-01 -6.40861839e-02 -9.70951766e-02 -1.27454743e-01
 -2.89976060e-01 -8.88561830e-02  2.83342779e-01 -8.12079236e-02
 -3.35942149e-01  3.88386369e-01 -5.94886839e-02 -1.40885890e-01
  1.09566242e-01 -1.15213633e-01 -9.84851830e-03 -3.55619878e-01
 -1.36484772e-01 -1.22042492e-01  2.02614516e-01  2.17803031e-01
 -1.78485900e-01  7.02671800e-03  1.66525487e-02  4.57302369e-02
 -1.40668496e-01 -6.56730011e-02 -2.51451284e-02  3.07327330e-01
 -1.78624481e-01  3.13840687e-01  8.04350078e-02  5.38893104e-01
 -4.02834862e-01  2.70252109e-01 -2.94801652e-01 -1.94319248e-01
 -3.15442644e-02 -1.36502072e-01 -1.59259945e-01  4.75163423e-02
  1.89396024e-01  5.10240793e-01 -4.71200943e-02  1.93295389e-01
 -3.11892033e-01  2.03360140e-01  2.14742348e-01 -2.01440938e-02
 -1.81696713e-01  1.38816088e-01 -6.65642023e-02 -1.61244407e-01
 -1.78215891e-01  2.70704985e-01  1.23236082e-01 -7.16210902e-02]"
Unable to serialize VariableSpec stat:awaiting response type:bug stale TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

windows 11

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A graph

### Standalone code to reproduce the issue

```shell
from models import *
from conftest import DDPGAgent
import matplotlib as plt
import pytest
import time

# Just disables the warning, doesn't take advantage of AVX/FMA to run faster
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# setting for hidden layers
Layer1 = 400
Layer2 = 300


class MecTer(object):
    """"""
    MEC terminal parent class
    """"""

    def __init__(self, user_config, train_config):
        self.rate = user_config['rate']
        self.dis = user_config['dis']
        self.id = user_config['id']
        self.state_dim = user_config['state_dim']
        self.action_dim = user_config['action_dim']
        self.action_bound = user_config['action_bound']
        self.data_buf_size = user_config['data_buf_size']
        self.t_factor = user_config['t_factor']
        self.penalty = user_config['penalty']

        self.sigma2 = train_config['sigma2']
        self.init_path = ''
        self.isUpdateActor = True
        self.init_seqCnt = 0

        if 'model' not in user_config:
            self.channelModel = MarkovModel(self.dis, seed=train_config['random_seed'])
        else:
            n_t = 1
            n_r = user_config['num_r']
            self.channelModel = ARModel(self.dis, n_t, n_r, seed=train_config['random_seed'])

        self.DataBuf = 0
        self.Channel = self.channelModel.getCh()
        self.SNR = 0
        self.Power = np.zeros(self.action_dim)
        self.Reward = 0
        self.State = []

        # some pre-defined parameters
        self.k = 1e-27
        self.t = 0.001
        self.L = 500

    def localProc(self, p):
        return np.power(p / self.k, 1.0 / 3.0) * self.t / self.L / 1000

    def localProcRev(self, b):
        return np.power(b * 1000 * self.L / self.t, 3.0) * self.k

    def offloadRev(self, b):
        return (np.power(2.0, b) - 1) * self.sigma2 / np.power(np.linalg.norm(self.Channel), 2)

    def offloadRev2(self, b):
        return self.action_bound if self.SNR <= 1e-12 else (np.power(2.0, b) - 1) / self.SNR

    def getCh(self):
        return self.Channel

    def setSNR(self, snr):
        self.SNR = snr
        self.sampleCh()
        channel_gain = np.power(np.linalg.norm(self.Channel), 2) / self.sigma2
        self.State = np.array([self.DataBuf, snr, channel_gain])

    def sampleData(self):
        data_t = np.log2(1 + self.Power[0] * self.SNR)
        data_p = self.localProc(self.Power[1])
        over_power = 0

        self.DataBuf -= data_t + data_p
        if self.DataBuf < 0:
            over_power = self.Power[1] - self.localProcRev(np.fmax(0, self.DataBuf + data_p))
            self.DataBuf = 0

        data_r = np.random.poisson(self.rate)
        self.DataBuf += data_r
        return data_t, data_p, data_r, over_power

    def sampleCh(self):
        # self.Channel = self.channelModel.sampleCh()

        # Calculate channel gain using channel quantization
        raw_channel_gain = np.linalg.norm(self.channelModel.sampleCh())
        min_val = np.min(self.Channel)
        max_val = np.max(self.Channel)

        # Quantize the channel gain into 10 levels
        quantized_channel_gain = min_val + (max_val - min_val) * (raw_channel_gain - min_val) / (max_val - min_val)
        quantized_channel_gain = np.clip(quantized_channel_gain, min_val, max_val)
        self.Channel = quantized_channel_gain

        return self.Channel

    def reset(self, rate, seqCount):
        self.rate = rate
        self.DataBuf = np.random.randint(0, self.data_buf_size - 1) / 2.0
        self.sampleCh()

        if seqCount >= self.init_seqCnt:
            self.isUpdateActor = True

        return self.DataBuf


class MecTermRL(MecTer):
    """"""
    MEC terminal class using RL
    """"""

    # rate:packet poisson arrival, dis: distance in meters
    def __init__(self, user_config, train_config):
        MecTer.__init__(self, user_config, train_config)
        self.agent = DDPGAgent(user_config, train_config)

        if 'init_path' in user_config and len(user_config['init_path']) > 0:
            self.init_path = user_config['init_path']
            self.init_seqCnt = user_config['init_seqCnt']
            self.isUpdateActor = False

    def feedback(self, snr, done):
        isOverflow = 0
        self.SNR = snr

        # update the data buffer
        [data_t, data_p, data_r, over_power] = self.sampleData()

        # get the reward for the current slot
        self.Reward = -self.t_factor * np.sum(self.Power) * 10 - (1 - self.t_factor) * self.DataBuf

        # estimate the channel for next slot
        self.sampleCh()

        # update the actor and critic network
        channel_gain = np.power(np.linalg.norm(self.Channel), 2) / self.sigma2
        next_state = np.array([self.DataBuf, snr, channel_gain])

        self.agent.update(self.State, self.Power, self.Reward, done, next_state, self.isUpdateActor)

        # update system state
        self.State = next_state
        # return the reward in this slot
        sum_power = np.sum(self.Power) - over_power
        return self.Reward, sum_power, over_power, data_t, data_p, data_r, self.DataBuf, channel_gain, isOverflow

    def predict(self, isRandom):
        power, noise = self.agent.predict(self.State, self.isUpdateActor)
        self.Power = np.fmax(0, np.fmin(self.action_bound, power))

        return self.Power, noise


class MecSvrEnv(object):
    """"""
    Simulation environment
    """"""

    def __init__(self, user_list, num_att, sigma2, max_len):
        self.user_list = user_list
        self.num_user = len(user_list)
        self.num_att = num_att
        self.sigma2 = sigma2
        self.count = 0
        self.seqCount = 0
        self.max_len = max_len

        # specially designed for Greedy agent training

    #         self.data_set = []

    def init_target_network(self):
        for user in self.user_list:
            user.critic.init_target_network(path='data_set_OGD.npz')

    def plot_channel_gains_histogram(self):
        # Get the channel gains for all users
        channel_gains = [np.abs(user.getCh()) for user in self.user_list]

        # Flatten the channel gains to a 1D array
        flat_channel_gains = np.concatenate(channel_gains)

        # plot a histogram for the channel gains
        plt.hist(np.abs(flat_channel_gains), bins=20, edgecolor='black')
        plt.title(""Channel Gains Histogram"")
        plt.xlabel(""Channel Gain Magnitude"")
        plt.ylabel(""Frequency"")
        plt.show()

    def step_transmit(self, isRandom=True):
        # get the channel vectors
        channels = np.transpose([user.getCh() for user in self.user_list])
        # get the transmit powers
        powers = []
        noises = []

        for i in range(self.num_user):
            p, n = self.user_list[i].predict(isRandom)
            powers.append(p.copy())
            noises.append(n.copy())
        # compute the snr for each user

        powers = np.array(powers)
        noises = np.array(noises)
        snr_list = self.compute_snr(channels, powers[:, 0])

        rewards = np.zeros(self.num_user)
        powers = np.zeros(self.num_user)
        over_powers = np.zeros(self.num_user)
        data_ts = np.zeros(self.num_user)
        data_ps = np.zeros(self.num_user)
        data_rs = np.zeros(self.num_user)
        data_buf_sizes = np.zeros(self.num_user)
        next_channels = np.zeros(self.num_user)
        isOverflows = np.zeros(self.num_user)

        self.count += 1
        # feedback the snr to each user
        for i in range(self.num_user):
            [rewards[i], powers[i], over_powers[i], data_ts[i], data_ps[i], data_rs[i], data_buf_sizes[i],
             next_channels[i], isOverflows[i]] = self.user_list[i].feedback(snr_list[i], self.count >= self.max_len)

        return rewards, self.count >= self.max_len, powers, over_powers, noises, data_ts, data_ps, data_rs, data_buf_sizes, next_channels, isOverflows

    def compute_snr(self, channels, powers):
        # FDD - Computing SNR
        H_inv = np.linalg.pinv(channels)
        total_signal_power = np.power(np.linalg.norm(channels, axis=1), 2)
        noise = np.power(np.linalg.norm(H_inv, axis=1), 2) * self.sigma2
        snr_list = total_signal_power / noise

        return snr_list

    def reset(self, isTrain=True):
        self.count = 0

        if isTrain:
            init_data_buf_size = [user.reset(user.rate, self.seqCount) for user in self.user_list]
            # get the channel vectors
            channels = np.transpose([user.getCh() for user in self.user_list])
            # get the transmit powers to start
            powers = [np.random.uniform(0, user.action_bound) for user in self.user_list]
            # compute the snr for each user
            snr_list = self.compute_snr(channels, powers)
        else:
            init_data_buf_size = [0 for user in self.user_list]
            snr_list = [0 for user in self.user_list]

        for i in range(self.num_user):
            self.user_list[i].setSNR(snr_list[i])

        self.seqCount += 1
        return init_data_buf_size


# Create the environment
# def env():
#     envi = MecSvrEnv(user_list, NUM_R, SIGMA2, MAX_EPISODE_LEN)
#     return envi

# env = MecSvrEnv(user_list, NUM_R, SIGMA2, MAX_EPISODE_LEN)
# env.init_target_network()

train_config = {
    'sigma2': 0.01,
    'minibatch_size': 64,
    'actor_lr': 0.0001,
    'tau': 0.001,
    'critic_lr': 0.001,
    'gamma': 0.99,
    'buffer_size': 250000,
    'random_seed': int(time.perf_counter() * 1000 % 1000),
    'noise_sigma': 0.12
}

# Define user_list_info with user information
user_list_info = [
    {'state_dim': 3,
     'action_dim': 1,
     'id': '1',
     'action_bound': 1,
     'model': 'AR',
     'num_r': 4,
     'rate': 3.0,
     'dis': 100,
     'data_buf_size': 100,
     't_factor': 1.0,
     'penalty': 1000, }
]

# sess = tf.compat.v1.Session()
# Create instances of the User class from the dictionary in user_list
user_list = [
    MecTermRL(user_config=user_info, train_config=train_config)
    for user_info in user_list_info
]

# Initialize variables
for user in user_list:
    user.agent.init_target_network()
    # (
    #     path=""C:/Users/USER/PycharmProjects/mec_drl-masterr/mec_drl-master/mec_drl-master/data_set_OGD.npz""
    # )

@pytest.fixture
def env():
    # Create and return the environment object
    # Make sure to adjust this to properly create your environment instance
    return MecSvrEnv(user_list, NUM_R, SIGMA2, MAX_EPISODE_LEN)
```


### Relevant log output

```shell
WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.__operators__.add_1), but are not present in its tracked objects:   <tf.Variable 'dense_5/bias:0' shape=(300,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.
Traceback (most recent call last):
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py"", line 207, in get_json_type
    type_spec_name = type_spec_registry.get_name(type(obj))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\tensorflow\python\framework\type_spec_registry.py"", line 75, in get_name
    raise ValueError(""TypeSpec %s.%s has not been registered."" %
ValueError: TypeSpec tensorflow.python.ops.resource_variable_ops.VariableSpec has not been registered.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\USER\PycharmProjects\mec_drl-masterr\mec_drl-master\mec_drl-master\test.py"", line 303, in <module>
    user_list = [
                ^
  File ""C:\Users\USER\PycharmProjects\mec_drl-masterr\mec_drl-master\mec_drl-master\test.py"", line 304, in <listcomp>
    MecTermRL(user_config=user_info, train_config=train_config)
  File ""C:\Users\USER\PycharmProjects\mec_drl-masterr\mec_drl-master\mec_drl-master\test.py"", line 125, in __init__
    self.agent = DDPGAgent(user_config, train_config)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\PycharmProjects\mec_drl-masterr\mec_drl-master\mec_drl-master\conftest.py"", line 24, in __init__
    self.critic = CriticNetwork(self.state_dim, self.action_dim, float(train_config['critic_lr']),
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\PycharmProjects\mec_drl-masterr\mec_drl-master\mec_drl-master\ddpg.py"", line 102, in __init__
    self.target_model = tf.keras.models.clone_model(self.model)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\models\cloning.py"", line 539, in clone_model
    return _clone_functional_model(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\models\cloning.py"", line 222, in _clone_functional_model
    model_configs, created_layers = _clone_layers_and_model_config(
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\models\cloning.py"", line 298, in _clone_layers_and_model_config
    config = functional.get_network_config(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\engine\functional.py"", line 1583, in get_network_config
    node_data = node.serialize(
                ^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\engine\node.py"", line 219, in serialize
    kwargs = tf.nest.map_structure(_serialize_keras_tensor, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\tensorflow\python\util\nest.py"", line 624, in map_structure
    return nest_util.map_structure(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\tensorflow\python\util\nest_util.py"", line 1054, in map_structure
    return _tf_core_map_structure(func, *structure, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\tensorflow\python\util\nest_util.py"", line 1094, in _tf_core_map_structure
    [func(*x) for x in entries],
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\tensorflow\python\util\nest_util.py"", line 1094, in <listcomp>
    [func(*x) for x in entries],
     ^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\engine\node.py"", line 215, in _serialize_keras_tensor
    return (_COMPOSITE_TYPE, json_utils.Encoder().encode(t))
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py"", line 55, in encode
    return super().encode(_encode_tuple(obj))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\json\encoder.py"", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\json\encoder.py"", line 258, in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py"", line 52, in default
    return get_json_type(obj)
           ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py"", line 225, in get_json_type
    ""spec"": get_json_type(spec),
            ^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\USER\anaconda3\Lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py"", line 214, in get_json_type
    raise ValueError(
ValueError: Unable to serialize VariableSpec(shape=(300,), dtype=tf.float32, trainable=True, alias_id=None) to JSON, because the TypeSpec class <class 'tensorflow.python.ops.resource_variable_ops.VariableSpec'> has not been registered.
```
",True,"[-0.3861227  -0.16432297 -0.34948832 -0.08794706  0.31241894 -0.29958394
 -0.10482213  0.16475928 -0.27606896 -0.27228332  0.15342116 -0.09660278
 -0.13507754  0.02957632 -0.19678366  0.22344375 -0.3827989  -0.0104559
  0.15953574  0.27170572 -0.25278693  0.11215156 -0.28421375  0.23283467
  0.0963439   0.14747253 -0.49792066  0.05027871 -0.08259055  0.3652919
  0.3314252   0.27798304 -0.05996959  0.14608696  0.15595785  0.06590688
 -0.35754228 -0.06028303 -0.3385395  -0.08163024  0.10894341  0.06720947
 -0.03009962 -0.27737272  0.0769996  -0.28745705 -0.12481236 -0.03054719
 -0.06762538 -0.24767098 -0.14444672  0.03745686 -0.5966681  -0.41386792
 -0.15152855 -0.21836583 -0.00372335 -0.06003032  0.13152817  0.14797404
  0.0245647   0.04962476  0.11806877 -0.21329626  0.09234505  0.20869192
  0.22087216  0.01937826  0.41777503 -0.46453696  0.07982662 -0.01905636
 -0.3898302   0.28481996  0.04041974  0.08308917 -0.11604647  0.19559646
  0.26699838 -0.20730607 -0.10767376 -0.29873258 -0.18771227 -0.2688026
  0.22754063 -0.06111563  0.43004012  0.18264768  0.53453493 -0.13073257
  0.30887336  0.42227635  0.14805825  0.11149091  0.41827455  0.14941868
  0.13606599  0.14187242 -0.01372147 -0.06428902 -0.19937477 -0.37575155
 -0.10029228  0.07575902 -0.10929753 -0.0702155   0.16338801 -0.20277134
  0.10941863 -0.20818649  0.07827719  0.20101768  0.13918489 -0.33814824
 -0.08969278  0.1921201  -0.08418968  0.01659251  0.10222963  0.674405
  0.12551463 -0.1646099  -0.1714266   0.24859808  0.5951085   0.10550509
 -0.10605031  0.06501975  0.09653089 -0.10436152  0.30470824  0.05594376
 -0.0631038   0.2587341  -0.14390251 -0.00204658 -0.09187205 -0.03005871
 -0.26960313 -0.3396786  -0.31805563  0.22469884  0.01327005 -0.52875
  0.21298593  0.07139647 -0.26617515  0.30059725 -0.19118156  0.2701724
  0.08836035 -0.16870725 -0.07365748  0.3440633  -0.00811707 -0.1438927
  0.48827344 -0.07693776  0.04422017 -0.49354923  0.07612216  0.46566355
  0.00691487 -0.21254438  0.12442762  0.21337867 -0.4536628  -0.20302773
  0.10011594  0.463028   -0.18953145 -0.09230091  0.10040427  0.01848343
  0.09099874 -0.05806728  0.29575324 -0.40841973 -0.08800235  0.42573822
  0.00913678  0.12829609  0.02198346  0.13077661  0.05808587 -0.11298591
 -0.02202952  0.01371351 -0.277296   -0.03212973 -0.3747791  -0.1846754
  0.4650163  -0.08956709 -0.19317466 -0.07459041  0.22829303 -0.26597184
  0.10388368  0.00243011 -0.25116828 -0.17378801  0.11126067 -0.05294172
  0.30037886 -0.22656229 -0.1225539  -0.4385609  -0.23851632  0.30337575
 -0.03314377 -0.4142008   0.1122264  -0.19107468 -0.3305355   0.29673356
  0.03616956  0.01058584  0.02205403  0.08643421  0.12994456 -0.24068847
  0.11082447 -0.36266375 -0.21236965  0.29776174 -0.37276563  0.18548465
 -0.03516147  0.14352241 -0.02676903  0.15495479  0.38534844  0.22192767
  0.3536344  -0.2937121  -0.06601895 -0.15887421 -0.12545884 -0.02921067
 -0.4272579  -0.14903373 -0.03182482 -0.11056714  0.22588421  0.3700874
 -0.10969962 -0.12619732 -0.40058944  0.07202399 -0.39241472  0.1736669
  0.5322294   0.11896081  0.44770348  0.2525375   0.3182264   0.19514418
  0.31084198 -0.3765903   0.4026106   0.09301144  0.07812734  0.3167548
  0.24919546  0.19459318 -0.5121595   0.44586492 -0.076658   -0.26448992
  0.23980135 -0.2070975   0.8537029  -0.33181053  0.17865312 -0.0903651
  0.2926954  -0.02162284  0.07845873  0.2170344   0.21578011  0.24731216
 -0.30852106  0.11949496 -0.05403242 -0.02529754 -0.18071313 -0.6236529
 -0.07826485  0.10539465 -0.28567526  0.24627683 -0.04519269 -0.11986534
 -0.06186643  0.13219178 -0.00912493 -0.27987495  0.01829611  0.21153595
 -0.318078   -0.06210644  0.41418564 -0.28504333 -0.07671728 -0.1566256
  0.37503237  0.45090875  0.37021106 -0.44098476  0.2282483  -0.25524217
 -0.16274235  0.61594635  0.01039586  0.1090203  -0.3412763   0.69709843
  0.08473558  0.00458956  0.12360486 -0.07045104 -0.39319012  0.22300293
  0.3356531  -0.00555196  0.01376061 -0.3832561   0.03233221  0.3862303
 -0.01029365 -0.11315121 -0.17403272 -0.10313326 -0.23704445 -0.30638647
 -0.3414808   0.14621973  0.1706851  -0.26398352 -0.13401279 -0.11858445
 -0.2269424  -0.25190115  0.1375547  -0.6177851   0.26589924  0.7001858
  0.05182307  0.3240868   0.18270901 -0.00839022 -0.3525799   0.03380552
 -0.26624137  0.24924855  0.02837054 -0.1667015   0.30385318  0.20262602
 -0.09256198  0.32783926 -0.42458537 -0.01176739  0.23908138 -0.36341158
 -0.10718022 -0.46257073  0.20598498  0.07029753  0.0356771   0.24818933
 -0.30916643  0.3061822   0.48084113 -0.31339946 -0.24170454  0.14848237
  0.2960888  -0.14837033 -0.09727051 -0.16368029  0.17583637  0.04098429]"
Check failed when running tensorflow.python.ops.gen_nn_ops.max_pool_grad_with_argmax stat:awaiting response type:bug stale comp:ops TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Specific input combination is caused check failure.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.python.ops import gen_nn_ops
try:
  try:
    with tf.device('/CPU'):
      arg_0_tensor = tf.random.uniform([2, 3, 3, 1], dtype=tf.float32)
      arg_0 = tf.identity(arg_0_tensor)
      arg_1_tensor = tf.random.uniform([2, 2, 2, 1], dtype=tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_2_tensor = tf.random.uniform([2, 2, 2, 1], minval=-256, maxval=257, dtype=tf.int64)
      arg_2 = tf.identity(arg_2_tensor)
      ksize_0 = 1
      ksize_1 = 2
      ksize_2 = 2
      ksize_3 = 1
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides_0 = 1
      strides_1 = 1
      strides_2 = 1
      strides_3 = 1
      strides = [strides_0,strides_1,strides_2,strides_3,]
      padding = ""VALID""
      include_batch_in_index = False
      out = gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      arg_0 = tf.identity(arg_0_tensor)
      arg_0 = tf.cast(arg_0, tf.float32)
      arg_1 = tf.identity(arg_1_tensor)
      arg_1 = tf.cast(arg_1, tf.float32)
      arg_2 = tf.identity(arg_2_tensor)
      arg_2 = tf.cast(arg_2, tf.int64)
      ksize = [ksize_0,ksize_1,ksize_2,ksize_3,]
      strides = [strides_0,strides_1,strides_2,strides_3,]
      gen_nn_ops.max_pool_grad_with_argmax(arg_0,arg_1,arg_2,ksize=ksize,strides=strides,padding=padding,include_batch_in_index=include_batch_in_index,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-21 00:36:14.553160: F tensorflow/core/kernels/maxpooling_op.cc:1081] Check failed: grad_out_index >= output_start && grad_out_index < output_end Invalid output gradient index: 77, 0, 18
Aborted

```
```
",True,"[-3.36921692e-01 -4.48480904e-01 -1.43799782e-01  1.30241275e-01
  2.74662256e-01 -4.18069988e-01 -9.88650471e-02  7.02349022e-02
 -3.37522745e-01 -4.41556603e-01  1.00765765e-01 -9.00120102e-03
 -2.32142895e-01  1.63828075e-01 -2.25187108e-01  1.82028428e-01
 -2.13576049e-01 -2.32602060e-01  1.99783877e-01  1.25106778e-02
 -9.73412320e-02 -1.61701500e-01 -2.37860799e-01  1.09254844e-01
  2.29219019e-01  4.00048822e-01 -2.81027973e-01 -1.60612375e-01
 -3.92418057e-02  2.85861254e-01  4.27860677e-01  9.16404575e-02
  5.92979044e-02  9.19083878e-02  3.20389234e-02  2.91018009e-01
 -3.69346321e-01 -3.07443470e-01 -1.94377899e-01  6.11656755e-02
  1.57306567e-01  1.26603320e-01  2.11391628e-01 -5.40810600e-02
  6.83329701e-02 -2.25956425e-01 -7.88206011e-02 -1.61848918e-01
 -1.82467513e-04 -1.58052102e-01 -4.45873961e-02  7.99030364e-02
 -3.29582065e-01 -3.70418847e-01 -5.53006586e-03 -1.07788399e-01
  3.39604840e-02 -8.35359171e-02 -7.68658370e-02  1.20870627e-01
  1.34666741e-01  2.83485409e-02 -1.00544572e-01 -8.99691880e-02
  1.50064260e-01  1.51515510e-02  3.45074236e-01 -3.04204486e-02
  5.64342499e-01 -1.12911709e-01  1.88386098e-01 -1.08439639e-01
 -4.63773400e-01  1.86992288e-01 -2.68570408e-02  3.19066644e-01
 -5.85063063e-02 -7.25031495e-02  2.59086668e-01 -1.36227489e-01
  7.66083449e-02 -1.73450321e-01  7.12322891e-02 -1.35375351e-01
  2.48927072e-01 -1.04875445e-01  2.85071552e-01  1.16189919e-01
  4.34766620e-01 -1.85113966e-01  6.53126121e-01  4.34718907e-01
 -8.90691131e-02  2.59821683e-01  5.11795044e-01  1.36308953e-01
  8.33484083e-02  3.55728194e-02 -2.52806723e-01 -1.87231630e-01
  6.36589453e-02 -2.40645811e-01 -6.32293969e-02  2.14251697e-01
 -1.97946757e-01 -8.58286396e-02  1.37221813e-01 -1.27925694e-01
  7.08766133e-02  4.22738418e-02  2.29806021e-01 -8.52439851e-02
  3.76163512e-01  6.75958991e-02  4.07881476e-02  9.14234072e-02
 -2.97023892e-01  2.02774674e-01 -1.10229049e-02  8.51830244e-01
 -1.20279817e-02 -2.18287995e-03 -1.41941130e-01  2.87409741e-02
  4.41072643e-01  1.72036782e-01  8.15764517e-02  1.08940795e-01
  9.72997323e-02 -1.89468831e-01  5.68699948e-02  8.32368359e-02
 -1.89933300e-01  2.55512029e-01  2.36043297e-02  2.17256069e-01
 -9.87938344e-02 -1.35358468e-01 -1.12706304e-01 -1.05256829e-02
 -1.91808999e-01  1.29099220e-01 -2.16785789e-01 -4.99497175e-01
  2.38342538e-01  1.95141539e-01 -2.03992531e-01  3.37779433e-01
 -1.23495668e-01 -9.61955637e-02 -1.43209919e-01  4.54537123e-02
 -3.48446369e-01  5.48951387e-01 -5.01947403e-02  1.61965340e-01
  3.79361510e-01  5.18068261e-02 -3.79341319e-02 -5.23645580e-01
  1.14460429e-02  4.23253357e-01 -6.85414523e-02 -5.85072786e-02
  3.02140951e-01  1.23453945e-01 -3.92622173e-01 -3.86496753e-01
 -1.04770333e-01  4.71645951e-01 -1.61979198e-01 -7.14400709e-02
  5.47289476e-03 -4.16878201e-02 -4.45556194e-02 -1.39756203e-01
  2.52059042e-01 -5.01296818e-01  2.25176588e-02  2.10378855e-01
 -5.29210865e-02  2.58468837e-01  4.73469310e-02  2.83480346e-01
  1.31136954e-01  1.50312603e-01  3.55931893e-02  1.43934503e-01
 -1.90595269e-01 -1.40652675e-02 -4.37118113e-01 -2.17712134e-01
  5.26332259e-01 -2.53470317e-02 -7.61296228e-02  8.74374956e-02
  3.12521756e-01  6.39120117e-03  3.54432315e-02  8.04158300e-03
 -1.75621748e-01  5.19639850e-02 -4.50785011e-02  3.06629203e-02
  1.34763926e-01 -2.87997186e-01 -1.71343684e-01 -4.17923510e-01
 -1.98428884e-01 -4.88323718e-03 -1.04841933e-01 -5.85506558e-01
  6.14005588e-02 -3.83542515e-02 -3.90332371e-01  2.00379133e-01
  2.32553631e-01  1.43636674e-01 -6.02989420e-02  8.34341645e-02
  5.68125211e-02 -8.13123286e-02 -8.37846249e-02 -4.35514271e-01
 -2.90875107e-01 -3.33373472e-02 -3.93162608e-01  2.78475732e-02
 -4.60421070e-02  2.38967612e-01  1.78771943e-01  2.69452333e-01
  3.11834037e-01  1.46555200e-01  3.88064533e-01 -1.30324692e-01
 -1.48170397e-01  8.80631804e-02 -1.89146832e-01  2.64999904e-02
 -3.77538770e-01 -2.33349860e-01  1.12169579e-01 -1.09374583e-01
  4.39165115e-01  3.59985232e-01 -7.54718035e-02 -9.01381820e-02
 -3.26603949e-01  3.35654259e-01 -1.26887590e-01 -8.48916620e-02
  2.24307522e-01  6.60262853e-02  5.92504859e-01  2.16722056e-01
  1.21343076e-01  3.09302211e-01  2.18830436e-01 -2.31319353e-01
  3.97252470e-01  2.43370503e-01 -1.51542034e-02  3.60937446e-01
  8.88208374e-02  1.81264132e-01 -3.62266272e-01  5.03393054e-01
  2.93801159e-01 -1.51130468e-01  2.24618271e-01 -3.97372186e-01
  6.05073094e-01 -2.41391599e-01 -1.57577157e-01 -1.66858554e-01
  3.20429027e-01  9.16424952e-03 -5.27863018e-02 -7.60361105e-02
  1.55772105e-01  2.35587239e-01 -3.11894894e-01 -1.04915418e-01
  2.46437602e-02 -2.38475069e-01 -3.84865664e-02 -6.43309355e-01
 -1.96144804e-01  1.17452018e-01 -1.23761669e-01  7.59107471e-02
  4.23447378e-02  1.39192551e-01 -2.82161385e-01  3.11126143e-01
  1.80105358e-01 -1.83737934e-01  2.29207799e-01  2.28216305e-01
 -2.50805318e-01  1.39250487e-01  2.88508177e-01 -3.63656819e-01
 -2.28051484e-01 -6.46891724e-03  3.03318560e-01  2.38011286e-01
  3.35528791e-01 -4.17693138e-01  5.75098321e-02 -5.84118925e-02
 -1.40901683e-02  4.50295419e-01 -3.06661725e-02 -1.16398651e-02
 -2.57666051e-01  4.77370113e-01  2.59743840e-01 -2.38926604e-01
  1.30474657e-01 -1.54740766e-01 -3.81670535e-01  7.27291182e-02
  2.47143313e-01 -1.34734765e-01 -1.57029122e-01 -3.12311172e-01
 -1.75405070e-02  8.18236470e-02  1.78589914e-02  4.68336344e-02
 -2.83281147e-01  4.53712232e-03 -1.23736262e-01 -1.43466145e-01
 -3.66177708e-01  2.88223594e-01 -1.76789425e-02 -4.23044205e-01
  3.74030229e-03 -5.84320724e-02  6.86097890e-03 -2.80766487e-01
 -1.08769119e-01 -3.33762974e-01  4.85054135e-01  4.59375024e-01
 -2.01688439e-01 -4.90334108e-02 -4.48056497e-04  1.94813367e-02
 -2.98021823e-01 -6.78663254e-02 -1.58168420e-01  4.61759180e-01
  2.16730908e-01 -2.06649333e-01  4.36937034e-01  1.99255005e-01
 -3.80911350e-01 -2.27661971e-02 -2.88732648e-01 -5.42252995e-02
  1.31146222e-01 -2.22725704e-01 -1.57317057e-01 -1.36730701e-01
  1.14737734e-01  1.61592096e-01 -2.41183281e-01  8.76314044e-02
 -4.04315919e-01  1.19989090e-01  5.10391951e-01 -4.15883660e-01
 -3.11181396e-01  1.53613448e-01 -4.27112915e-03 -1.55349314e-01
 -2.59265341e-02 -3.90424877e-02  4.95808423e-02 -5.53511605e-02]"
Activation function of a Dense hidden layer not getting invoked. stat:awaiting response type:bug stale subtype:macOS TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

MacOS 13.4, MacBook Pro M2 Max

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Issue:
In the given auto-encoder setup, the encoder layers activation function (relu) is not getting invoked.

1. We create a simple auto-encoder, with Input size 3, hidden size 2, and output back to 3.
2. The activation function of the encoder layer is set a relu.
3. The weights of the encoder layers are all made negative. Idea is, if input is +ve, all the neutrons will have negative value and relu will o/p zero.
4. Give input as [1, 0, 0].
5. We expect the final decoder o/p layer, which has sigmoid activation, to o/p all [0.5, 0.5, 0.5] as the input to this layer from the encoder should have been [0, 0, 0]. 
6. But we find that is not the case, which clearly shows that 'relu' activation of the hidden layer is not getting invoked.

Installation:
pip install tensorflow-macos
pip install tensorflow-metal

### Standalone code to reproduce the issue

```shell
# https://colab.research.google.com/drive/14KKrdiBg8FT2cdUC5pjqJHi5S3BkTOHi?usp=sharing
# The above colab will run fine, but the same code on Mac with the said config has issue.
# Copying the code here for quick reference.

import tensorflow as tf    
import tensorflow.keras
import tensorflow as tf
import platform
import sys
from tensorflow.keras.layers import Input, Dense, Layer
from tensorflow.keras.models import Model

# Print versions:
print(f""Python {sys.version}"")
print(f""Python Platform: {platform.platform()}"")
print(f""Tensor Flow Version: {tf.__version__}"")
gpu = len(tf.config.list_physical_devices('GPU'))>0
print(""GPU is"", ""available"" if gpu else ""NOT AVAILABLE"")

# Setup input
import numpy as np
X_check = np.array([[1, 0, 0]])

# Setup autoencoder model
input_layer = Input(shape=(X_check.shape[1]))
bottleneck = Dense(2, activation='relu', name='bottleneck')(input_layer)
output = Dense(X_check.shape[1], activation='sigmoid', name='output')(bottleneck)
autoencoder = Model(input_layer, output)

# Set encoder layer weights to all negative.
layer = autoencoder.layers[1]
weights = np.array([[-1, -1],[-1, -1], [-1, -1]])
biases = np.array([0, 0])
layer.set_weights([weights, biases])

# create encoder model.
encoder = Model(input_layer, bottleneck)

# create decoder model.
decoder_input = Input(shape=(2,), name='decoder_input')
decoder_layer = autoencoder.layers[-1]
decoder = Model(decoder_input, decoder_layer(decoder_input))

# Run auto-encoder, with [1, 0, 0], since encoder has all negative weights,
# and has 'relu' activation o/p of enocder should all be zeros. And that being
# the input of next sigmod we should get output [0.5, 0.5, 0.5]
output_data = autoencoder.predict(X_check)
print(output_data)
```


### Relevant log output

```shell
Python 3.8.17 (default, Jul  5 2023, 15:45:03) 
[Clang 14.0.6 ]
Python Platform: macOS-13.4-arm64-arm-64bit
Tensor Flow Version: 2.13.0
GPU is available
1/1 [==============================] - 0s 38ms/step
[[0.287966   0.85427195 0.28276426]]
```
",True,"[-4.11966920e-01 -3.07439059e-01 -3.53789851e-02  1.00654021e-01
  3.44140947e-01 -2.00134397e-01  2.19977766e-01  4.04844880e-02
 -4.03603792e-01 -2.32428476e-01  3.22774470e-01 -3.28673184e-01
 -7.89310783e-03 -1.33911083e-02 -2.88225692e-02  2.53078550e-01
 -2.15183929e-01  4.11057696e-02  5.12944087e-02  1.62211731e-02
 -6.87354133e-02 -1.47471309e-01 -1.81552470e-01  1.24831237e-01
  1.73649698e-01  1.24059841e-01 -2.08186746e-01  5.51409572e-02
 -1.90057948e-01  1.05249785e-01  1.95089579e-01  2.22847015e-01
  3.01870294e-02 -3.09292525e-02  1.22168615e-01  1.49072051e-01
 -1.87949210e-01 -2.48211578e-01 -4.35347259e-01 -1.03933178e-02
 -1.35094404e-01  5.81294373e-02  1.32645503e-01  1.10551700e-01
 -8.85555670e-02  3.74873988e-02 -9.58185345e-02 -1.09820321e-01
  1.20987579e-01 -4.05219734e-01  8.43841285e-02 -3.64367701e-02
 -3.29225421e-01 -3.40740979e-01 -2.75005341e-01  1.54480875e-01
 -4.28615473e-02 -1.27204601e-02  4.11049910e-02  2.75214434e-01
  1.05618328e-01 -4.18989882e-02  1.74068198e-01  3.61444242e-02
  1.58718050e-01  2.26523146e-01  1.23124227e-01 -1.25270635e-01
  5.10841787e-01 -5.57967462e-03  1.66478634e-01 -3.49288955e-02
 -5.47104001e-01  3.93480435e-02  5.92477322e-02  3.07475299e-01
 -2.84085929e-01 -1.01032615e-01  2.67592072e-01 -5.23966774e-02
 -1.27047390e-01 -2.05457866e-01  4.57784608e-02  4.08834405e-03
  1.56219259e-01  5.53493015e-03  2.92025715e-01 -9.46381316e-03
  1.34631097e-01 -1.03033662e-01  5.57458401e-01  2.42284834e-01
 -8.47183987e-02 -8.28080028e-02  5.36498427e-01 -3.86851355e-02
  3.47998813e-02  2.22782269e-01 -7.40970895e-02  2.48732716e-02
 -6.47340268e-02 -3.78205553e-02 -1.51317090e-01  1.26369923e-01
 -6.17281012e-02 -8.50959569e-02  2.71666557e-01 -2.93625653e-01
  8.10387284e-02  3.48962724e-01  2.39829421e-01 -1.37177810e-01
  9.66689885e-02  2.42781956e-02  1.28613180e-02 -9.04243812e-02
  6.14516363e-02  2.39683986e-01  1.89731661e-02  7.28796840e-01
  1.57336399e-01 -2.31487155e-01 -8.21398944e-03  2.94411868e-01
  4.44301546e-01  1.28304958e-01 -2.81492174e-01  1.64627671e-01
  3.29964235e-02 -3.62801477e-02  2.44401023e-01  1.27899721e-01
  1.45287484e-01  3.16613950e-02 -6.14302084e-02 -1.03668734e-01
 -6.94014132e-02 -2.30030529e-02 -5.80664277e-01  7.78772235e-02
 -2.44131505e-01  3.53276461e-01 -1.00656062e-01 -4.53328729e-01
  1.70465961e-01  1.49588048e-01 -1.59323394e-01  1.04702652e-01
 -5.45217395e-02 -9.95128006e-02 -3.45353708e-02 -7.41596147e-02
  3.56225669e-02  3.27463478e-01  1.62655830e-01  1.56703979e-01
  1.90095544e-01  4.35806513e-02 -1.55429453e-01 -6.07614398e-01
  7.27034211e-02  2.33990043e-01 -9.71920416e-02  4.45856014e-03
  3.40427727e-01 -5.84536232e-04 -2.62796104e-01 -2.17584923e-01
 -1.35562420e-01  1.98752403e-01 -1.72441661e-01 -1.01419628e-01
  1.71678253e-02  3.94838583e-03  2.80301273e-01 -2.37106785e-01
  7.37056807e-02 -5.44917703e-01 -3.65354307e-03 -7.47423619e-02
  8.62643719e-02 -1.41984463e-01 -7.49975890e-02  1.45629108e-01
 -3.81095931e-02  2.05793709e-01  2.84057166e-02  2.56999195e-01
 -3.18478405e-01  1.47599071e-01 -5.23920715e-01  2.46097147e-01
  3.78200889e-01  1.63036399e-03  4.97562997e-02 -1.32600188e-01
  5.75357005e-02 -1.53205246e-01 -9.64542180e-02  1.90259516e-01
 -1.62952334e-01 -1.15449227e-01 -4.26157527e-02  1.03313671e-02
  3.64373177e-02 -4.98356611e-01 -1.78552285e-01 -3.12166184e-01
 -4.21066344e-01  5.07717654e-02 -1.38498113e-01 -3.82264018e-01
  5.90321496e-02 -3.30150723e-02 -3.01429689e-01  3.74736965e-01
 -3.56477797e-02  1.67541623e-01 -2.24284217e-01  3.49371396e-02
  7.20296055e-03 -2.47149229e-01 -1.51796147e-01 -3.26194316e-01
 -2.06247330e-01 -1.22781888e-01 -2.25890771e-01  5.43378107e-02
 -2.79852711e-02  3.69495481e-01  1.56900674e-01  1.53750911e-01
  2.68059880e-01  2.68477499e-01  1.83741093e-01 -3.55135463e-02
 -9.69887972e-02 -2.09416807e-01 -7.53999129e-03  3.51725742e-02
 -6.16166830e-01  6.80635273e-02 -1.25469923e-01  8.31301659e-02
  1.72467202e-01  3.20714772e-01 -1.14579141e-01 -4.70996872e-02
 -1.90868467e-01  1.63994879e-01 -5.20391822e-01  2.04723686e-01
  5.09528518e-01  8.26010481e-02  9.47598591e-02  1.61211625e-01
  1.75299332e-01 -3.01676970e-02  2.69794494e-01 -1.28006622e-01
  3.12971115e-01 -5.43425232e-02  2.00923294e-01  3.27721030e-01
 -6.88031316e-02  3.63086462e-01 -3.36239547e-01  4.99964237e-01
  6.18347600e-02 -1.54073119e-01  2.15148240e-01 -2.77294606e-01
  5.05383253e-01 -4.97425646e-01  3.31208855e-03 -8.93636793e-02
  1.93173930e-01  1.13745362e-01 -1.06952608e-01  2.80097336e-01
 -6.72908723e-02  1.75911531e-01 -3.31414193e-01  1.10659853e-01
 -9.41438377e-02 -3.17202330e-01 -1.75142437e-01 -4.84392345e-01
 -7.64747933e-02 -1.32935867e-01 -1.68755054e-02  1.00683108e-01
  3.92255373e-02  6.64828271e-02 -2.90890522e-02  2.05883861e-01
  2.11527511e-01 -2.59333789e-01 -5.22977114e-02  1.38997659e-01
 -6.77754432e-02  1.89063296e-01  2.13459015e-01 -3.73130977e-01
 -1.63778067e-01 -1.74349938e-02  3.76999646e-01 -6.27023429e-02
  5.40947080e-01 -1.42020702e-01  3.96987379e-01 -7.43014272e-03
 -7.26474151e-02  4.89502609e-01 -1.50888443e-01 -1.04064845e-01
 -5.15874684e-01  6.56103849e-01  8.61950368e-02 -7.74427503e-03
  2.40639225e-01  2.46592462e-02 -6.17539138e-02  2.28009179e-01
  3.58442664e-01  9.43440385e-03 -1.18836872e-01 -4.10281628e-01
  1.27058670e-01  7.93653578e-02  5.45714758e-02 -1.77477330e-01
 -7.24389702e-02  1.15282133e-01  8.00237879e-02  8.81255418e-02
 -4.04010326e-01  2.90497005e-01 -7.03471377e-02 -2.56314129e-01
 -2.58892238e-01 -5.56735694e-02 -1.68507081e-02 -2.14312613e-01
  7.45788366e-02 -2.14646459e-01  1.55336171e-01  4.93813455e-01
 -1.74969286e-02  3.22186261e-01  1.08486772e-01  8.47527832e-02
 -3.54196012e-01  9.15289745e-02 -3.73871863e-01  4.61626679e-01
  6.75036013e-02 -1.95689201e-01  2.46271826e-02  1.86034143e-01
 -5.69129288e-01 -2.05125824e-01 -3.22236836e-01  1.78277418e-01
 -1.68021508e-02 -1.13663629e-01 -8.49027038e-02 -3.22945744e-01
 -1.16112165e-01  2.67489254e-01  7.47493207e-02 -5.08589149e-02
 -2.80402184e-01  4.80187237e-01  3.75925153e-01 -3.48015904e-01
 -2.16771394e-01  3.64323184e-02  6.41035736e-02  1.96130335e-01
  2.58770473e-02 -7.51731992e-02 -8.19227472e-02  1.02533866e-02]"
Cannot build tensorflow from source stat:awaiting response type:bug type:build/install stale subtype: ubuntu/linux,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

latest

### Custom code

Yes

### OS platform and distribution

Debian 12

### Mobile device

_No response_

### Python version

Python 3.8

### Bazel version

Latest

### GCC/compiler version

Clang 16

### CUDA/cuDNN version

Dont have

### GPU model and memory

Dont have

### Current behavior?

I tried several times, still same error. Searched on google found nothing

### Standalone code to reproduce the issue

```shell
I follow the guide from tensorflow.com but still faced this error. Please help
```


### Relevant log output

```shell
(myenv) root@drowsiness:~/tensorflow# bazel build -j 2 --local_ram_resources=3000 --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=189
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /root/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/root/anaconda3/envs/myenv/bin/python3 --action_env PYTHON_LIB_PATH=/root/anaconda3/envs/myenv/lib/python3.8/site-packages --python_path=/root/anaconda3/envs/myenv/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/lib/llvm-16/bin/clang --repo_env=CC=/usr/lib/llvm-16/bin/clang --repo_env=BAZEL_COMPILER=/usr/lib/llvm-16/bin/clang --copt=-Wno-gnu-offsetof-extensions
INFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file /root/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (648 packages loaded, 42257 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/sandbox
ERROR: /root/tensorflow/tensorflow/compiler/mlir/tensorflow/BUILD:475:11: Compiling tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc failed: (Killed): clang failed: error executing command (from target //tensorflow/compiler/mlir/tensorflow:tensorflow_ops) 
  (cd /root/.cache/bazel/_bazel_root/efb88f6336d9c4a18216fb94287b8d97/execroot/org_tensorflow && \
  exec env - \
    CLANG_COMPILER_PATH=/usr/lib/llvm-16/bin/clang \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-x86_64/bin:/root/anaconda3/envs/myenv/bin:/root/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/root/anaconda3/envs/myenv/bin/python3 \
    PYTHON_LIB_PATH=/root/anaconda3/envs/myenv/lib/python3.8/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib/llvm-16/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++0x' -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops/tf_ops.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops/tf_ops.pic.o' -fPIC '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' '-DLLVM_NATIVE_ARCH=""X86""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeX86Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeX86TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA' '-DLLVM_HOST_TRIPLE=""x86_64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64-unknown-linux-gnu""' '-DLLVM_VERSION_MAJOR=18' '-DLLVM_VERSION_MINOR=0' '-DLLVM_VERSION_PATCH=0' '-DLLVM_VERSION_STRING=""18.0.0git""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS '-DBLAKE3_USE_NEON=0' -DBLAKE3_NO_AVX2 -DBLAKE3_NO_AVX512 -DBLAKE3_NO_SSE2 -DBLAKE3_NO_SSE41 -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/llvm-project -iquote bazel-out/k8-opt/bin/external/llvm-project -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/ml_dtypes -iquote bazel-out/k8-opt/bin/external/ml_dtypes -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithCanonicalizationIncGen -Ibazel-out/k8-opt/bin/external/llvm-project/mlir/_virtual_includes/AsmParserTokenKinds -isystem external/llvm-project/llvm/include -isystem bazel-out/k8-opt/bin/external/llvm-project/llvm/include -isystem external/llvm-project/mlir/include -isystem bazel-out/k8-opt/bin/external/llvm-project/mlir/include -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/ml_dtypes -isystem bazel-out/k8-opt/bin/external/ml_dtypes -isystem external/ml_dtypes/ml_dtypes -isystem bazel-out/k8-opt/bin/external/ml_dtypes/ml_dtypes -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions -Wno-sign-compare '-std=c++17' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc -o bazel-out/k8-opt/bin/tensorflow/compiler/mlir/tensorflow/_objs/tensorflow_ops/tf_ops.pic.o)
# Configuration: 4332b06bceb8e99a0d8ed4f75fa26218a779c66acf683ffad0936df1e9f625df
# Execution platform: @local_execution_config_platform//:platform
In file included from tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc:16:
In file included from ./tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h:38:
In file included from ./tensorflow/compiler/mlir/tensorflow/ir/tf_attributes.h:21:
In file included from ./tensorflow/core/ir/types/dialect.h:31:
bazel-out/k8-opt/bin/tensorflow/core/ir/types/dialect.h.inc:31:19: warning: 'parseType' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
     ::mlir::Type parseType(::mlir::DialectAsmParser &parser) const;
                  ^
external/llvm-project/mlir/include/mlir/IR/Dialect.h:107:16: note: overridden virtual function is here
  virtual Type parseType(DialectAsmParser &parser) const;
               ^
In file included from tensorflow/compiler/mlir/tensorflow/ir/tf_ops.cc:16:
In file included from ./tensorflow/compiler/mlir/tensorflow/ir/tf_ops.h:38:
In file included from ./tensorflow/compiler/mlir/tensorflow/ir/tf_attributes.h:21:
In file included from ./tensorflow/core/ir/types/dialect.h:31:
bazel-out/k8-opt/bin/tensorflow/core/ir/types/dialect.h.inc:32:11: warning: 'printType' overrides a member function but is not marked 'override' [-Winconsistent-missing-override]
     void printType(::mlir::Type type, ::mlir::DialectAsmPrinter &printer) const;
          ^
external/llvm-project/mlir/include/mlir/IR/Dialect.h:110:16: note: overridden virtual function is here
  virtual void printType(Type, DialectAsmPrinter &) const {
               ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3990.011s, Critical Path: 343.16s
INFO: 991 processes: 3 internal, 988 local.
FAILED: Build did NOT complete successfully
```
",True,"[-5.00236869e-01 -4.96682137e-01 -1.76123917e-01  1.24481902e-01
  2.06069380e-01 -4.23806310e-01 -3.41141641e-01 -7.81746395e-03
 -3.00158858e-01 -3.89187902e-01 -8.18884745e-03 -1.12425089e-02
 -2.09486961e-01  1.21334478e-01 -1.30277485e-01  3.54124367e-01
 -2.45174810e-01 -4.88454625e-02  2.75230944e-01  2.29277864e-01
 -1.62842959e-01  7.67641738e-02 -2.83785939e-01  1.59386516e-01
  3.05184603e-01  1.02999546e-01 -3.66645157e-01  3.12411636e-02
  6.31928295e-02  1.42823830e-01  5.68227708e-01  1.02559373e-01
 -1.11168064e-02  9.31707546e-02  1.63055658e-02  3.88944387e-01
 -3.77421111e-01 -1.43489510e-01 -1.78168803e-01  2.19893783e-01
  2.33297814e-02  5.15150316e-02  1.20474592e-01 -1.96098253e-01
  5.48708811e-02 -1.34915248e-01  3.98162752e-02 -2.36854047e-01
  9.26147997e-02 -3.81813705e-01 -1.39331177e-01  1.92678487e-03
 -4.20111150e-01 -3.53060842e-01 -2.38666236e-01 -2.03907877e-01
  3.89713533e-02 -2.54555233e-02 -1.08231381e-01  1.50232941e-01
  3.25700417e-02  3.06569990e-02  1.88849136e-01 -1.86815947e-01
 -6.06232770e-02  8.63164142e-02  3.25023085e-01 -5.40716350e-02
  6.06166482e-01 -3.38366449e-01  1.95492715e-01 -1.73166633e-01
 -3.90238076e-01  9.00412574e-02 -7.53387809e-02  1.23601854e-01
  1.22091118e-02  9.49424580e-02  2.87126780e-01 -1.89063907e-01
 -1.35407895e-01 -2.49888450e-01  2.17168462e-02 -3.48452896e-01
  2.22130701e-01 -5.87880537e-02  4.62530494e-01  1.70833170e-01
  3.19647282e-01 -1.68585002e-01  4.97990519e-01  3.94381255e-01
 -5.82618499e-03  5.50683066e-02  5.73211193e-01  1.07166693e-01
  1.23428762e-01  3.53919685e-01 -2.69806478e-02 -2.69781291e-01
 -1.54848173e-01 -1.84677571e-01 -5.49237505e-02  4.80092466e-02
 -5.00972532e-02 -1.56527132e-01  1.15884587e-01 -2.54106075e-02
  1.42384216e-01 -2.26191789e-01  2.46385530e-01 -1.01629592e-01
  2.91866839e-01 -1.37075439e-01  1.02621615e-01 -1.14600465e-01
 -3.01027060e-01 -1.17198983e-02 -1.76766813e-01  1.01448584e+00
 -5.34688644e-02  7.26697408e-03  6.86781704e-02  2.82366276e-01
  4.29793298e-01  1.34276748e-01 -2.18815710e-02 -6.57666996e-02
  1.24434665e-01 -3.66400816e-02  1.37584686e-01  7.49914870e-02
 -9.24092382e-02  3.64619344e-01 -3.38874087e-02  4.52282503e-02
 -7.53671825e-02 -1.96975112e-01 -1.37185737e-01 -3.55516851e-01
 -2.74519801e-01  1.55637547e-01 -2.92636245e-01 -7.00471282e-01
  1.34267390e-01  5.40868789e-02 -2.40788400e-01  1.80066004e-01
 -1.47548541e-01  1.63214445e-01 -1.14248723e-01  1.32116318e-01
 -1.77982479e-01  3.16186786e-01  2.96530247e-01  2.33623728e-01
  2.84944117e-01 -1.09458178e-01 -6.83480501e-02 -7.40431726e-01
  7.42664188e-02  5.50051332e-01 -1.16832435e-01 -1.98131055e-01
  1.16923265e-01  1.08527258e-01 -4.07574475e-01 -4.09036517e-01
  1.61898702e-01  5.35397828e-01 -2.29407698e-01 -1.86191112e-01
  1.08619578e-01  2.56015152e-01  2.09753007e-01  6.34580851e-02
  2.63356864e-01 -6.88229144e-01 -3.59881483e-02  5.67548633e-01
  1.86777055e-01  2.09641814e-01  4.62955572e-02  2.85255849e-01
  1.18735999e-01 -3.37164067e-02 -4.21708897e-02  4.50213626e-02
 -2.80672073e-01  1.87485963e-01 -3.42914402e-01 -7.83178508e-02
  3.76354277e-01 -1.99885502e-01 -1.61625683e-01  1.34791210e-01
  2.14881182e-01  8.13158527e-02  9.27759856e-02 -7.53069445e-02
 -3.12179625e-01 -6.45458326e-02 -9.82089117e-02  6.90103471e-02
  1.76966935e-02 -4.57649380e-01 -2.31623966e-02 -2.53096253e-01
 -4.85666871e-01  8.63124430e-02 -4.75732535e-02 -5.03293276e-01
  2.75319844e-01 -1.00520074e-01 -2.88219035e-01  1.61740273e-01
  2.66554266e-01  8.24324712e-02 -2.38087550e-01  1.63864210e-01
  1.38215840e-01 -3.58292997e-01  1.78121552e-02 -3.51679593e-01
 -8.24209899e-02  5.04693352e-02 -4.13974166e-01  1.91888586e-01
  2.20594525e-01  3.76803368e-01  1.16273105e-01  2.30570436e-01
  3.08251768e-01  2.68517315e-01  4.43466425e-01 -2.31557041e-01
 -8.35105702e-02 -6.42457232e-02 -1.10195465e-01  5.38880825e-02
 -3.40680391e-01 -1.65481940e-01  8.14724714e-04  1.66960731e-01
  1.88820809e-01  3.17816108e-01 -1.77572876e-01 -1.17599450e-01
 -4.23714161e-01  1.80955231e-01 -1.76632345e-01  1.90300256e-01
  4.24188375e-01  1.63392603e-01  5.69506228e-01  2.45219976e-01
 -1.40439905e-02  2.69839704e-01  2.11320490e-01 -2.72101790e-01
  3.87330025e-01  1.68419361e-01 -1.29091859e-01  3.29189360e-01
  2.77586490e-01  2.06022069e-01 -5.35267532e-01  5.97541451e-01
  1.11438751e-01 -1.03751972e-01  2.20409214e-01 -2.52410710e-01
  5.78164458e-01 -4.83637959e-01  5.89816868e-02 -8.56726840e-02
  3.86582017e-01  1.56117544e-01 -6.10782998e-03 -1.64421812e-01
  6.74538910e-02  4.24647093e-01 -3.89684707e-01  2.02098452e-02
 -2.26477031e-02 -2.67551720e-01 -3.52618732e-02 -6.55999303e-01
 -3.66926432e-01  8.18452090e-02 -2.81323850e-01  2.54306376e-01
 -1.06852792e-01  2.95264013e-02 -2.50453591e-01 -1.44753689e-02
  8.69067833e-02 -1.09010190e-01  9.73043218e-03  2.83096939e-01
 -1.52737632e-01  4.16826233e-02  5.23712933e-01 -4.45766985e-01
 -1.98550582e-01 -1.77063406e-01  4.01669413e-01  2.76019275e-01
  5.09380519e-01 -4.38595831e-01  2.70000964e-01 -4.30653021e-02
 -6.18318766e-02  5.50219834e-01  2.43075844e-02  3.55990119e-02
 -4.08748448e-01  7.54605293e-01  1.26019627e-01 -2.36793041e-01
  2.09261775e-01 -8.38415623e-02 -3.49778384e-01  8.18439126e-02
  3.62407446e-01 -1.05359107e-01 -3.80420573e-02 -4.13609952e-01
 -1.20691866e-01  2.92909503e-01 -3.06930542e-02 -4.27601561e-02
 -1.16073824e-01 -2.36854050e-03 -1.37113437e-01 -1.22130394e-01
 -4.92766917e-01  2.70963311e-01  4.13557701e-03 -3.44082117e-01
 -1.19774438e-01  2.39832699e-03  4.56213951e-02 -2.74113387e-01
  4.73151393e-02 -4.75553751e-01  4.41949785e-01  5.31945229e-01
 -2.33166367e-01  1.45739555e-01  4.23362851e-02  3.17696184e-01
 -5.84071279e-01 -1.05070554e-01 -1.04646713e-01  2.55747408e-01
  3.35113071e-02 -1.40466914e-01  4.69521672e-01  3.76014233e-01
 -2.28404790e-01  1.86400026e-01 -3.40790451e-01 -2.06115730e-02
  1.71423644e-01 -3.80739152e-01 -1.36683255e-01 -2.92179793e-01
  7.96942934e-02  2.86497772e-01 -8.26296657e-02  2.33321965e-01
 -3.90262961e-01  3.02069306e-01  6.24472737e-01 -3.71259511e-01
 -6.00333989e-01  2.35139623e-01 -4.69159707e-03 -2.19498396e-01
 -3.58629301e-02 -9.15498808e-02  2.76247025e-01 -4.99410927e-02]"
Check failure when running tf.config.experimental_connect_to_host stat:awaiting response type:bug stale comp:ops TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

GTX 1660 TI

### Current behavior?

Due to feeding NaN input Argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
try:
  try:
    with tf.device('/CPU'):
      arg_0 = ""nan""
      out = tf.config.experimental_connect_to_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
  try:
    with tf.device('/GPU:0'):
      tf.config.experimental_connect_to_host(arg_0,)
  except Exception as e:
    print(""Error:""+str(e))
except Exception as e:
  print(""Error:""+str(e))
```
```


### Relevant log output

```shell
2023-08-19 19:02:09.775956: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-19 19:02:10.305057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-19 19:02:10.742608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.761041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.761185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.762359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.762491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.762611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.826971: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3389 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-19 19:02:10.829417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-08-19 19:02:10.829854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3389 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
2023-08-19 19:02:10.838878: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:600] INVALID_ARGUMENT: Could not interpret ""nan"" as a host-port pair.
E0819 19:02:10.839114974  187448 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
Aborted

```
```
",True,"[-5.30976295e-01 -3.96934420e-01  7.84180015e-02 -2.71465145e-02
  4.00241792e-01 -3.93084228e-01 -4.07936275e-02  1.68073803e-01
 -4.66545939e-01 -3.17121476e-01  1.48199290e-01 -1.93918198e-01
 -1.06246367e-01  1.00082621e-01 -1.67408839e-01  2.11955309e-01
 -2.05600947e-01 -2.72690535e-01  3.25318515e-01  2.06737697e-01
 -3.38813722e-01 -7.09783435e-02 -2.64476180e-01  1.80974022e-01
 -9.44516137e-02  2.21409261e-01 -2.57302850e-01  2.59224355e-01
 -1.76322058e-01  4.67400193e-01  3.05846989e-01 -8.74887258e-02
 -3.18707645e-01  1.47174373e-01  1.74785838e-01  1.99489027e-01
 -2.73517430e-01 -3.04629773e-01 -3.31787825e-01 -5.68399206e-03
  2.21841440e-01 -4.47681025e-02  1.26170963e-01 -2.06145167e-01
 -4.46388051e-02 -7.91670531e-02  6.64839894e-02  5.85000962e-04
 -2.05510914e-01 -4.00556564e-01  4.41898853e-02  1.54694200e-01
 -4.08832103e-01 -2.70426512e-01  1.26925796e-01  4.16050702e-02
  1.58437248e-02  4.65781875e-02 -3.93127017e-02  1.73488855e-01
  3.47303540e-01 -5.05647808e-02  1.47994608e-04 -2.23567665e-01
  2.05486357e-01  8.62142146e-02  3.26543838e-01  8.20672736e-02
  3.97433013e-01 -3.76294143e-02 -4.73138224e-03 -2.55085863e-02
 -4.26010132e-01  1.46857902e-01  9.90411714e-02  2.23261267e-01
 -2.04895630e-01 -1.17790140e-01  3.75503778e-01 -1.13431707e-01
  9.54585522e-02 -1.90510467e-01 -1.65399075e-01 -1.56948760e-01
  6.50475621e-02 -2.85886347e-01  3.71941894e-01 -9.03953426e-03
  4.95011777e-01 -2.67199278e-01  4.21034038e-01  4.66754645e-01
  8.87601525e-02  4.23227400e-02  4.03178751e-01  1.38953358e-01
  5.65972039e-03  1.48922667e-01 -8.55495259e-02  7.08048195e-02
 -6.69859499e-02 -2.53881037e-01 -9.60430801e-02  1.31721303e-01
 -2.65009761e-01 -2.76670635e-01  1.12001546e-01  1.12079144e-01
  6.72383681e-02  5.67063093e-02  2.76031531e-02  1.01335242e-01
  1.21387556e-01 -1.34220243e-01  3.50636095e-02  1.48615152e-01
 -4.36931141e-02  2.85027232e-02  1.29245177e-01  6.36880279e-01
  4.92893048e-02 -2.99844801e-01 -3.16455215e-03  5.52188382e-02
  5.01092613e-01  9.75540727e-02 -2.80773714e-02  8.26897770e-02
  1.21662557e-01 -1.58909820e-02  4.88606840e-02  1.44346356e-02
  3.32280025e-02  2.24562570e-01 -1.05074748e-01  2.29421347e-01
 -6.13789000e-02  1.74509045e-02 -1.90787464e-01 -1.39831424e-01
 -1.75979555e-01  1.72706604e-01 -2.14174882e-01 -4.78195488e-01
  3.20838004e-01  9.52305496e-02 -1.23622522e-01  3.67043257e-01
 -2.96866775e-01  2.02015061e-02  3.04230273e-01  2.19016597e-02
 -1.21050432e-01  4.56296355e-01 -1.43693149e-01  6.24467805e-02
  3.76850307e-01 -7.86057115e-02  2.50483006e-01 -5.11121273e-01
  4.55105342e-02  4.35336053e-01 -1.23511165e-01 -1.09234087e-01
  1.58858150e-01  9.75315422e-02 -5.16261935e-01 -2.92196691e-01
  2.20125318e-02  3.42482984e-01 -3.37560177e-01 -1.81527674e-01
  7.39368424e-02 -3.30264606e-02  4.87592667e-02 -3.41786116e-01
  3.47456217e-01 -2.20856518e-01 -4.56270352e-02  2.22649395e-01
 -8.15426484e-02  1.62302688e-01  1.06963083e-01  3.77222121e-01
  1.81288347e-01  8.65484402e-02  1.45489007e-01  1.59478933e-01
 -2.80374646e-01 -1.42703295e-01 -6.56861424e-01 -1.53296441e-01
  6.07853472e-01  9.53472406e-02 -2.86376327e-02 -1.53720707e-01
  1.84123158e-01  3.63323838e-03  1.88257426e-01  1.60152406e-01
 -2.07375556e-01 -1.13491341e-01 -3.03944707e-01  1.07873574e-01
  2.25143787e-02 -1.68674409e-01 -1.52894646e-01 -3.93819094e-01
 -2.18152434e-01 -9.79016498e-02 -9.49889421e-02 -4.28333908e-01
 -1.63340811e-02 -6.03957959e-02 -2.72832960e-01  2.70802021e-01
 -9.14255306e-02  8.98745656e-02  1.29701704e-01  1.99753851e-01
  1.21256262e-01 -2.50998437e-01 -4.34533656e-02 -4.29552168e-01
 -3.22971195e-01  1.71665266e-01 -5.25168002e-01  1.24633372e-01
 -1.24961138e-04  1.37484014e-01  1.84178464e-02  1.64173692e-01
  3.66042137e-01  1.90391541e-01  4.81837600e-01 -2.10403234e-01
 -9.29672718e-02 -1.98859915e-01 -2.72602886e-01 -3.50950994e-02
 -4.24798548e-01 -3.36255550e-01 -7.73912072e-02 -3.12060565e-02
  4.46108341e-01  5.59311748e-01 -1.74498290e-01  1.53699499e-02
 -3.43788356e-01  3.17383111e-01 -6.65273443e-02  5.54410256e-02
  4.38002944e-02  3.52661684e-02  5.20644307e-01  3.76153976e-01
  2.68864244e-01  2.04861417e-01  2.83636838e-01 -1.38649166e-01
  3.02772105e-01  3.84495080e-01  3.17272574e-01  2.02922404e-01
  2.67692536e-01  4.26825702e-01 -3.21099997e-01  3.29807222e-01
 -9.58981737e-02 -6.32142276e-02  1.59003168e-01 -4.16066736e-01
  6.78308010e-01 -2.21828699e-01  2.49689370e-02 -2.28494138e-01
  3.64786446e-01 -1.67232618e-01 -5.06241247e-02  2.72489607e-01
  2.74557434e-02  4.55607623e-01 -2.11298347e-01  1.11978233e-01
  1.35190859e-01 -2.65547812e-01 -2.66185313e-01 -5.23371220e-01
 -2.32305780e-01  1.02112982e-02 -3.62754643e-01  3.47033255e-02
  7.32840896e-02  1.38368130e-01 -1.73051298e-01  2.81261802e-01
  7.02500641e-02 -2.65770972e-01  8.03745985e-02  8.31549615e-02
 -2.74302542e-01  1.06447496e-01  4.83588874e-01 -2.33368903e-01
 -2.47509211e-01  2.68204752e-02  3.38444561e-01  7.62297362e-02
  4.51903224e-01 -5.66511333e-01  2.25278348e-01  2.67896298e-02
 -2.31856201e-02  3.54691952e-01  1.95327364e-02  1.17039964e-01
 -3.67551804e-01  4.74626124e-01 -1.16457716e-02 -1.82939723e-01
  4.82813418e-01 -3.03485632e-01 -3.33460003e-01  1.48691162e-02
  3.51028442e-01 -6.76973239e-02 -1.09685048e-01 -3.65981638e-01
  1.14822417e-01  2.78051078e-01 -6.19073324e-02 -3.29322517e-02
 -1.84481844e-01  1.56203844e-02 -2.02192768e-01 -1.33053899e-01
 -3.14080775e-01  2.42001712e-01 -3.98996174e-02 -2.89736986e-01
 -2.51889825e-01 -1.68665141e-01 -1.43276051e-01 -1.22720018e-01
  1.09328672e-01 -3.34650338e-01  3.20140332e-01  2.78636396e-01
 -4.70197275e-02  1.45489186e-01 -1.00293942e-01  1.15288235e-02
 -4.54049349e-01 -2.38630921e-02 -1.46640509e-01  4.57435310e-01
  7.74509460e-03 -2.50469089e-01  3.43969882e-01  2.18488336e-01
 -2.03219205e-01  1.16738752e-01 -2.53573775e-01 -1.97762549e-02
 -3.08167189e-03 -8.55023414e-02 -1.01476640e-01 -2.05621839e-01
  2.86009848e-01  1.13656171e-01 -1.32641405e-01  1.23031899e-01
 -3.13075900e-01  6.54336624e-03  4.66840595e-01 -6.11196458e-01
 -1.35430664e-01  1.52658835e-01  1.54731885e-01 -1.01053894e-01
 -8.30899552e-02  1.53170973e-02  1.76538020e-01  7.24258870e-02]"
"libtensorflowlite_jni.so (offset 0x2f3000):signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xa59fdbc0 stat:awaiting response type:bug stale comp:lite TF 2.3","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

org.tensorflow:tensorflow-lite:0.0.0-nightly org.tensorflow:tensorflow-lite-gpu:2.3.0 org.tensorflow:tensorflow-lite-support:0.1.0

### Custom code

Yes

### OS platform and distribution

Andorid 13

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Our process com.vt.tv.aipq use tensorflow-lite cause process crash:
*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
Build fingerprint: 'Hisense/songshan-FFM/songshan:11/RTT2.220118.001/00.00.00.40:user/release-keys'
Revision: '1234'
ABI: 'arm'
Timestamp: 2023-05-06 14:36:47+0800
pid: 14794, tid: 28796, name: TFService-T  >>> com.vt.tv.aipq <<<
uid: 1000
signal 11 (SIGSEGV), code 2 (SEGV_ACCERR), fault addr 0xa59fdbc0
    r0  00000004  r1  00000020  r2  00000300  r3  a59fdbc0
    r4  31a05640  r5  000002f0  r6  31a05740  r7  a59fe4c0
    r8  31a056c0  r9  a5674220  r10 a59fe1c0  r11 31a055c0
    ip  a59fdec0  sp  84db73e8  lr  832ded97  pc  832f08ac

backtrace:
      #00 pc 000258ac  /system_ext/app/HiAIPQ/HiAIPQ.apk!libtensorflowlite_jni.so (offset 0x2f3000)

### Standalone code to reproduce the issue

```shell
The Google server background statistics process crash information, unable to clarify the steps to reproduce the problem.
```


### Relevant log output

_No response_",True,"[-4.72315192e-01 -4.22165662e-01 -9.86882821e-02  1.10953748e-01
  2.94388831e-01 -3.30092877e-01  3.55004445e-02 -3.85335498e-02
 -2.84686744e-01 -2.24707335e-01 -2.20425278e-01 -2.22670615e-01
 -1.80680886e-01  7.88005218e-02 -2.39734828e-01  2.92785704e-01
 -3.00604761e-01 -2.29045928e-01  1.93966761e-01 -1.57103866e-01
 -1.49525404e-01 -6.72564954e-02 -4.09027576e-01  2.46960074e-01
  1.03013769e-01  2.41703972e-01 -1.17598407e-01 -1.06996089e-01
 -6.78039268e-02  1.59555972e-01  3.10735762e-01  9.45364684e-02
 -4.51253615e-02  1.76522285e-01 -1.14728257e-01  1.09942064e-01
 -2.09614336e-01 -5.31634018e-02 -1.71980262e-01 -8.85707587e-02
  1.46434218e-01 -1.82361510e-02  1.38629943e-01 -1.97943836e-01
  8.35904256e-02  2.71842536e-03  9.02877599e-02  9.15895216e-03
  2.32370570e-02 -2.04231188e-01 -1.42122999e-01  1.06432438e-01
 -3.65320474e-01 -6.58343434e-02 -1.15596548e-01 -6.40601069e-02
  1.92953944e-01 -7.87633285e-03  9.28585604e-03  2.44091034e-01
  1.45968437e-01 -5.52422851e-02  1.00646123e-01 -6.13212772e-02
  1.43636912e-02  1.93858311e-01  3.87745500e-01 -1.89642370e-01
  3.94826710e-01  7.62812644e-02  1.31383926e-01 -4.19402868e-02
 -2.91061521e-01  2.46853400e-02  1.54631168e-01  1.33383885e-01
 -3.37022603e-01  2.86861479e-01  3.23137105e-01 -5.44549227e-02
 -1.41283691e-01 -3.13242227e-01  6.97820634e-02 -1.45983785e-01
  2.05060139e-01 -1.80182308e-01  3.04125369e-01 -1.62348878e-02
  6.04422569e-01 -1.48557842e-01  4.93872821e-01  2.96418130e-01
 -1.16433620e-01  1.12260073e-01  5.83488643e-01 -6.04762807e-02
  1.35571480e-01  2.34105423e-01 -1.19295955e-01 -6.03897125e-02
 -2.17532843e-01 -9.21617895e-02 -1.55678898e-01  2.61861593e-01
  1.04810148e-01  9.67057049e-02  3.15149724e-01 -2.73583345e-02
  1.12232581e-01 -4.53444794e-02  2.85567671e-01 -1.24461897e-01
  3.46164286e-01 -4.50781658e-02  2.75649607e-01  1.50879920e-01
  1.02834322e-01  7.21293539e-02 -2.50883996e-02  6.86432838e-01
 -1.09495044e-01 -7.43795708e-02 -9.33395028e-02  1.73528433e-01
  2.82290339e-01  1.23074114e-01 -1.65619284e-01 -9.35730152e-03
  8.08393061e-02  4.63355854e-02 -4.62740734e-02  2.68739939e-01
 -4.88409624e-02  8.50598440e-02 -1.02300972e-01  1.70621186e-01
 -1.70471221e-01 -9.18388069e-02 -1.67308360e-01 -7.68263340e-02
 -1.04769677e-01  3.38249922e-01 -8.32027197e-02 -2.01951832e-01
  2.98358202e-01  5.65392487e-02 -2.30305731e-01  2.28946358e-01
 -1.22035936e-01 -2.48829186e-01 -8.72513205e-02  9.91427600e-02
 -4.33691852e-02  5.48004746e-01  2.59465665e-01  3.04026067e-01
  2.02733874e-01 -2.11833671e-01  2.77273878e-02 -3.86461377e-01
  1.97608590e-01  4.60696101e-01 -1.50570139e-01 -5.43526933e-02
  3.57095599e-01  1.56159267e-01 -4.70082492e-01 -9.76055488e-02
  1.22884005e-01  2.06382722e-01 -9.56266969e-02 -1.40542179e-01
  1.59286056e-02  1.63620282e-02  2.19463617e-01 -3.80943455e-02
  3.84289742e-01 -5.22790074e-01 -1.39157504e-01  1.00920707e-01
 -6.93068951e-02  2.31404781e-01 -7.37591181e-04  2.55367637e-01
 -1.07732490e-01 -5.77105582e-02  7.10125044e-02  7.19631240e-02
 -1.46500647e-01 -1.54285524e-02 -2.71579891e-01 -3.61824334e-02
  3.85819912e-01 -6.63630590e-02 -5.55219427e-02  2.53835935e-02
  4.44242284e-02 -1.41358823e-01 -7.95575157e-02  2.13029414e-01
  5.94308134e-03 -1.35269940e-01 -1.93154007e-01  5.17449453e-02
  1.84152499e-02 -3.25961292e-01 -3.38497870e-02 -2.99718201e-01
 -1.57749355e-01 -1.72760352e-01  2.38422841e-01 -4.71198916e-01
 -4.08267379e-02 -1.33599132e-01 -3.87121618e-01  1.71463281e-01
 -6.38270825e-02 -1.58128321e-01 -2.94392884e-01  3.27815533e-01
  3.47214878e-01 -5.84086031e-02 -1.84725463e-01 -3.40892881e-01
 -2.63001800e-01 -2.19037592e-01 -2.78748184e-01  2.41682068e-01
  4.60230596e-02  2.57743984e-01  2.34987885e-02  7.31633604e-02
  5.75199246e-01  5.11671975e-02  2.71947891e-01 -3.24792027e-01
 -2.86456674e-01 -1.45114899e-01 -2.33555406e-01 -1.74454063e-01
 -3.07280183e-01 -1.81125000e-01  7.82125816e-03 -5.43207526e-02
  1.75837025e-01  1.14627056e-01  8.37402195e-02 -2.95227855e-01
 -3.88012737e-01  3.38537872e-01  5.32515347e-04  5.41309044e-02
  2.04621390e-01  1.57436535e-01  2.85710961e-01  3.48650813e-01
  1.03648968e-01 -1.30968690e-01  1.94289938e-01 -8.75915587e-02
  4.23946410e-01  1.70717686e-01  1.28956288e-02  4.15331006e-01
  1.86775327e-01  4.96295035e-01 -3.85011852e-01  2.36761272e-01
 -9.49984565e-02 -2.77052298e-02 -3.63613404e-02 -1.15732282e-01
  3.52689624e-01 -2.74623513e-01  1.31750554e-01 -3.36568713e-01
  4.21000779e-01  1.70478478e-01 -1.57519192e-01  1.64810181e-01
  2.70588323e-04  4.38727587e-01 -1.30276695e-01  4.00868393e-02
 -3.35267857e-02 -6.33966029e-02 -1.28151596e-01 -3.64271969e-01
 -8.92054513e-02  1.59930304e-01 -3.00429583e-01  1.98631972e-01
  3.49836320e-01  1.71186775e-01 -9.80746299e-02 -4.87152524e-02
 -6.33332431e-02 -9.82461646e-02  1.78279504e-02  3.79723683e-02
 -1.13044828e-01  7.49512017e-02  3.27326417e-01 -3.22463870e-01
 -1.05722547e-02 -1.44753277e-01  2.58088082e-01  2.08836123e-01
  2.60001719e-01 -4.40712273e-01  2.19411299e-01  9.59141701e-02
 -8.41340795e-02  5.16236722e-01 -1.20264702e-01  3.28841172e-02
 -4.64832306e-01  5.13950169e-01  1.52180225e-01 -1.07008889e-01
  1.87487528e-02 -1.60522044e-01 -4.69367623e-01 -3.01420651e-02
  3.51249039e-01  4.99433167e-02 -1.25409812e-01 -1.76195145e-01
 -4.09560502e-01 -1.18602537e-01 -1.10940285e-01 -3.29348594e-01
 -1.64983928e-01  2.15215206e-01  3.62898484e-02 -1.46891460e-01
 -5.67033887e-01  1.15057826e-01  1.84288453e-02 -3.01565111e-01
  3.36280726e-02 -2.00515211e-01  7.53688589e-02 -4.01126504e-01
  1.08153559e-02 -4.08803523e-01  3.05051982e-01  5.51912308e-01
 -2.53383040e-01  2.21406937e-01 -3.21837723e-01  2.06727177e-01
 -4.60322127e-02 -1.43782631e-01 -4.89985794e-02  4.14127111e-01
 -2.07063481e-02 -6.21601008e-02  2.28398561e-01  4.24151450e-01
 -4.20456380e-01  5.07798344e-02 -2.60683775e-01 -1.59981489e-01
  1.44700944e-01 -2.41627887e-01 -8.17202479e-02 -1.22392982e-01
  2.03755319e-01  4.52325612e-01 -5.48249111e-04  2.71329761e-01
 -1.33624762e-01  3.74705121e-02  2.38362223e-01 -5.59789479e-01
 -1.79870605e-01  1.55931324e-01 -5.54766953e-02 -2.68376648e-01
 -1.28987223e-01  2.88362429e-02 -3.95781212e-02 -1.44191518e-01]"
Corrupt png files type:support,"### Issue type
Support

### TensorFlow version
2.14

### OS platform and distribution
Windows 11

### Python version
3.9

I am getting an error: Invalid PNG data, size 49253

Part of my classification process classifying bad images from good ones. All of my png files can be opened, so I think the images are getting corrupt with they are resized. In the graphic below are a few of the ""bad"" images. 
![image](https://github.com/tensorflow/tensorflow/assets/7605046/bd18cefb-f001-41a3-8f57-b14379cebdc4)

After importing the images into the training dataset (code below), how I can filter out the images that became corrupt in the resizing? Right now it is throwing an error on when iterating through the normalization_layer

```
batch_size = 32
img_height = 224
img_width = 224
band_count = 4

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=""training"",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size,
  color_mode=""rgba"")

normalization_layer = layers.Rescaling(1./255)
normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
#Error here: Invalid PNG data, size 49253
image_batch, labels_batch = next(iter(normalized_ds))
```


### Standalone code to reproduce the issue

```shell
Code above
```


### Relevant log output

_No response_",False,"[-0.37542093 -0.33230582 -0.2263647   0.06105768  0.5019475  -0.27729642
 -0.152598   -0.02980973 -0.2668785   0.02639655 -0.11727992  0.01592697
 -0.12177376 -0.00811801 -0.53735536  0.08825442 -0.3336382  -0.1221581
  0.1807487   0.23931652 -0.11932073  0.14821206  0.28490856 -0.09003472
  0.03846001  0.16751482 -0.05836436 -0.30560037  0.09059629 -0.28278604
  0.02385613 -0.14949632  0.12246981  0.34596476  0.23317981 -0.1507872
 -0.31924897 -0.0898557   0.02828239 -0.15693441 -0.07682008  0.14695145
  0.17834634 -0.17787822  0.15184447  0.1481404  -0.04286265  0.15660203
 -0.06967203 -0.27958223  0.0172129   0.10164148 -0.43847623  0.0721961
 -0.16729078 -0.40611088  0.16193789 -0.02177491 -0.02270753 -0.15696767
  0.21607302  0.2387597   0.4098922  -0.21378237  0.02227294  0.32384217
  0.14418994 -0.13392925  0.30358088 -0.324168    0.16141474  0.15967208
 -0.43222785  0.0038576  -0.10812203  0.13010564 -0.2551409   0.38670582
 -0.03197477 -0.24449706  0.34145516 -0.31058657  0.19930156  0.07925002
  0.16899592 -0.09830475  0.04115403  0.05921368  0.4144471   0.05629884
  0.33992758  0.21876466 -0.03357624  0.39014822  0.27761984 -0.06365679
  0.06602055  0.18157579  0.07099177  0.0762411  -0.3043549  -0.2074773
  0.10291942  0.07532106  0.3460228   0.29558018  0.5607021   0.05815578
  0.08002397  0.08452119  0.07256968 -0.0586433  -0.08692561 -0.02487409
  0.08485329  0.08195575 -0.3846868   0.09166497 -0.54666615 -0.02828549
  0.02531777 -0.14609644 -0.24511883  0.19594844  0.2861073  -0.10726725
 -0.4936748  -0.03506094  0.0180466  -0.07209663  0.00453669  0.04463644
  0.08279217  0.06986689  0.44197732  0.01755916 -0.27982718  0.0088049
 -0.42038274 -0.08749467 -0.35283154  0.13959259  0.13176574  0.0373676
 -0.00544073  0.1059394  -0.23220429  0.46357515 -0.15627885  0.14626019
 -0.00458369 -0.06413233 -0.35200524  0.7001282  -0.01636794 -0.11599639
  0.27355015 -0.14477307 -0.1580256  -0.19985607  0.23948401 -0.00612968
  0.04190055 -0.03037437 -0.07153001  0.2752554  -0.4530545  -0.2869694
 -0.14223234 -0.10856112 -0.10795026 -0.11014928  0.21213959 -0.06424186
  0.19945315 -0.01987618 -0.43549103 -0.09795097  0.12545618  0.02008358
 -0.08459777  0.1809141  -0.0631599  -0.08819848 -0.00494793 -0.10612309
  0.04788917  0.02732427  0.02818679  0.3896597  -0.12343712  0.00355006
  0.04129859 -0.21159062  0.17905173  0.35178143  0.14234596  0.3279389
 -0.05707458  0.10647052  0.07094508 -0.21413578  0.04871196 -0.02410271
  0.00138585 -0.02727359 -0.23925003 -0.11632041 -0.50272787  0.22322352
 -0.17964903 -0.39962822 -0.571329    0.02547631 -0.0828169   0.05823862
  0.34183544 -0.05752549 -0.02152887  0.24524647  0.09359223 -0.06426939
 -0.2171284  -0.17709245  0.18638334  0.24687913 -0.10385413 -0.14989296
 -0.5779542   0.0625411  -0.13162944  0.04935348  0.4331115  -0.08783286
  0.02722143  0.28948098 -0.2971874  -0.08508795  0.00520815  0.24086723
 -0.13870382 -0.19027781 -0.3923499  -0.1901596   0.11318547 -0.05405828
  0.2231066  -0.00541478 -0.31738728  0.5635562   0.13197975 -0.09354606
  0.2574451   0.2626143   0.23543827 -0.07122406  0.2442133  -0.14340535
 -0.14095883 -0.09334571  0.23634183 -0.17805122 -0.12181975  0.10722243
  0.05662999  0.29686522 -0.4536203   0.35762858 -0.10288318 -0.51075387
  0.2265948  -0.1494998   0.39994323 -0.06325482  0.27713716  0.19200101
  0.27273735 -0.06313061 -0.14879704  0.1154685   0.0121692   0.28941065
 -0.17521176  0.12833744  0.07729254  0.15080756 -0.60716665 -0.1618402
 -0.23500083 -0.12110157 -0.13698772 -0.19991547  0.28123105  0.23651242
  0.22544365  0.00875892  0.14934145  0.03576298  0.03984848 -0.11983999
 -0.35134318  0.32701358  0.05561639 -0.00372958 -0.05762009 -0.12532774
  0.2684542   0.25434625  0.29838586 -0.24913332  0.18476795 -0.25951868
 -0.08581494  0.3827078  -0.11886229  0.01868331 -0.06079054  0.53907734
  0.36185515  0.19811793 -0.21404935 -0.13459025 -0.20866793  0.09654775
  0.10537356 -0.2495942  -0.3010584   0.10081576 -0.13087538 -0.12583596
  0.15976728 -0.4358115  -0.10879782 -0.09144425  0.06325437  0.03113835
 -0.35662806  0.03911938  0.02050921 -0.27005422 -0.1863696  -0.33215997
 -0.02542875 -0.4426757   0.00463507 -0.21904707  0.2903661   0.0088187
 -0.26940107 -0.05758953  0.23988584  0.04627707  0.14333974  0.2584557
  0.27382734  0.23039487  0.21703242  0.03970668 -0.06253654  0.25101066
 -0.3273629   0.16569468 -0.08698107 -0.20473272 -0.09870628 -0.26492816
  0.0022032  -0.19480965  0.21964917  0.16730246  0.14021856  0.08609092
 -0.46204156  0.561859    0.40987012 -0.18129882 -0.0452292   0.28344595
 -0.19375467  0.18384673 -0.02811008  0.20533033 -0.02857311 -0.11212075]"
Issue created for Rollback of PR #61276: return assert_shapes for debugging.assert_shapes ,"Merged PR #61276 is rolled back in 9c492018a641ac6b5a03b65a77ff9d1ce60699eb.
    Please follow up with the reviewer and close this issue once its resolved.",False,"[-0.89410305  0.5924112  -0.33238575 -0.18283892 -0.18130445 -0.08501178
  0.26466823 -0.2832856  -0.19901848  0.00261928  0.07968491 -0.23540898
  0.36017188  0.00460248  0.46181822  0.32927182 -0.58065546  0.07368179
 -0.02541464 -0.09010601 -0.10531904 -0.1405203  -0.2589647  -0.00210557
  0.11336474  0.06097069  0.16788025  0.079347    0.06640884 -0.1138482
  0.4265182   0.4203607  -0.3018188   0.07758204 -0.31322742  0.16324018
 -0.7715131   0.07486206  0.13478428  0.02666378 -0.25986025  0.1665389
 -0.27531454 -0.25205517  0.1252156   0.10236912 -0.28170013  0.38042715
  0.24501735  0.0038402   0.08665568 -0.23503876 -0.35072526 -0.35159093
  0.17516364  0.14690283 -0.00715905 -0.0025287   0.12310472 -0.17140615
  0.13675806 -0.00570292  0.19317669  0.01497603 -0.08642501  0.11249048
  0.02417929 -0.01618974  0.00520212  0.38808104  0.40133113  0.07895856
  0.03459439 -0.425813    0.44252324  0.04593206 -0.7553119   0.06322826
  0.09424687 -0.00662507  0.20138001 -0.24838257 -0.24083333  0.31204197
  0.32064068 -0.32375768  0.05546793 -0.4616817   0.3666724   0.33251232
  0.2748106   0.05297771 -0.21074279  0.50867605 -0.07561676  0.26738745
 -0.21734987 -0.04017793  0.2973216   0.06595854 -0.3352462   0.19741903
 -0.15473638  0.06574722  0.22079185 -0.22026491 -0.10152069 -0.42810106
  0.25875092  0.30823556  0.10036317 -0.51347667 -0.09708546  0.04643428
 -0.01166253  0.13526109  0.1318407  -0.31692743 -0.42414734 -0.60560596
  0.4825203   0.27636018 -0.18906374  0.37702212  0.2045804  -0.05753763
 -0.22418208  0.10561115 -0.00357427 -0.13952157 -0.1003366   0.231429
  0.29782227 -0.07598285  0.1488471   0.0681112  -0.56224644 -0.11662966
 -0.19287942 -0.43234092  0.3391578   0.18881406 -0.03874546 -0.28718585
  0.41096714 -0.09571245 -0.6911156   0.1281259  -0.13706036  0.25248566
 -0.15907758  0.01393782 -0.54347104  0.44234702  0.42794216  0.4126454
  0.07333082 -0.05893858  0.07155249 -0.5721798   0.46595877  0.45691827
  0.7307907   0.25105262  0.43449947 -0.06423712 -0.11760361  0.1403686
  0.17608131  0.2900454   0.03574929  0.05605082 -0.13176937 -0.24161538
  0.42303696  0.11997797 -0.04942021 -0.25968346  0.24866952  0.4785567
  0.13236412  0.01357297  0.08025654 -0.29744226 -0.65107554  0.56502295
 -0.48310006  0.01414618 -0.06373893 -0.04163662 -0.25320908 -0.18765281
 -0.46963397  0.06484605  0.00453434  0.0440782   0.23247105  0.09319848
  0.4011811   0.15727583 -0.15268862  0.02387482  0.09639766 -0.31165686
 -0.009424    0.11137364  0.61556214 -0.08333069 -0.04128957  0.5015002
 -0.11184914 -0.8172225  -0.24665464 -0.3595771   0.31772006 -0.09113268
  0.01717934  0.24434417 -0.69529414  0.14987665 -0.00873826 -0.21467453
  0.14162847 -0.33765712 -0.24502516  0.2709167  -0.06229501 -0.14039984
 -0.42742407 -0.09592073  0.4909884   0.05159659  0.13213973 -0.06036728
  0.1855315  -0.01171151 -0.08340675  0.09976754  0.2420522  -0.0867225
 -0.5133528  -0.5370456  -0.01492734 -0.25181854  0.65321046  0.05362401
  0.12419842  0.3398352   0.10101314  0.35472798 -0.02736535  0.01657173
  0.6399015  -0.06841279 -0.04488617 -0.43923074  0.3362538   0.32991707
 -0.1458427  -0.08709895  0.10996196  0.057636    0.21838033 -0.08910587
  0.08361725  0.16669197 -0.06309319  0.664733    0.0323978  -0.32257602
  0.30996686  0.00305654  0.18198419 -0.28615046  0.23138733 -0.02099163
  0.44765043  0.14723696 -0.3537078   0.25190344 -0.3033595  -0.14378297
  0.03384542  0.5484592   0.30408645 -0.14608797  0.15799405 -0.36274028
  0.230244   -0.17013222 -0.09127229 -0.50540954  0.12332419 -0.46064922
 -0.17045707 -0.00415283 -0.06133193 -0.4243028   0.30782548  0.15543933
 -0.0950669   0.3438784  -0.05041311 -0.2043834  -0.29725185 -0.31724408
  0.13827795  0.35285577 -0.2368061  -0.11222757  0.19160178  0.24905749
 -0.00104249  0.3148112  -0.22291933  0.26520735  0.03615812  0.38312054
 -0.23121898  0.2691219  -0.28807813 -0.345686    0.2956398  -0.1286607
  0.20635633  0.05161918  0.09461165 -0.25021088 -0.4242579  -0.44498712
 -0.18751281 -0.17090334 -0.15733834  0.03667248  0.31529853 -0.6127373
 -0.6243824   0.25437194 -0.12781906 -0.22506377 -0.65549254 -0.02826468
 -0.22948493 -0.08255531 -0.5199034  -0.08183653 -0.03161652  0.01078606
 -0.22091553 -0.11113852  0.34090844  0.12142565  0.135827    0.00242575
 -0.08399127  0.02123053  0.08088029 -0.0350556  -0.0317476   0.4165963
 -0.03538996 -0.04397099  0.02982698  0.14304647 -0.15499547 -0.09529525
  0.28021988 -0.44101658  0.17068806 -0.00239347 -0.19082594  0.3315671
 -0.45061606  0.4112577   0.13202615 -0.21750024  0.514849   -0.09225512
 -0.03545995  0.14987633 -0.03033091  0.2129113  -0.00298059  0.18522657]"
[v2.14.0] Build failed on linux/arm64 w/ clang type:build/install subtype:bazel TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf v2.14.0

### Custom code

No

### OS platform and distribution

Linux arm64

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

6.4.0

### GCC/compiler version

Debian clang version 14.0.6 aarch64-unknown-linux-gnu

### CUDA/cuDNN version

n/a

### GPU model and memory

_No response_

### Current behavior?

When trying to build `libtensorflow` the linking step failed. Is there a way to fix this?

### Standalone code to reproduce the issue

```shell
Run `bazel build --config opt //tensorflow/tools/lib_package:libtensorflow`
```


### Relevant log output

```shell
[5,037 / 5,041] Compiling tensorflow/core/kernels/conv_grad_input_ops_3d.cc; 246s local
[5,037 / 5,041] Compiling tensorflow/core/kernels/conv_grad_input_ops_3d.cc; 276s local
[5,037 / 5,041] Compiling tensorflow/core/kernels/conv_grad_input_ops_3d.cc; 336s local
[5,038 / 5,041] [Prepa] Linking tensorflow/libtensorflow.so.2.14.0
[5,038 / 5,041] Linking tensorflow/libtensorflow.so.2.14.0; 1s local
[5,038 / 5,041] Linking tensorflow/libtensorflow.so.2.14.0; 11s local
[5,038 / 5,041] Linking tensorflow/libtensorflow.so.2.14.0; 41s local
ERROR: /mnt/sandisk/tensorflow/tensorflow/tensorflow/BUILD:1243:20: Linking tensorflow/libtensorflow.so.2.14.0 failed: (Exit 1): clang failed: error executing command (from target //tensorflow:libtensorflow.so.2.14.0) /usr/lib/llvm-14/bin/clang @bazel-out/aarch64-opt/bin/tensorflow/libtensorflow.so.2.14.0-2.params
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
/usr/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
clang: error: linker command failed with exit code 1 (use -v to see invocation)
[5,039 / 5,041] checking cached actions
Target //tensorflow/tools/lib_package:libtensorflow failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 50743.078s, Critical Path: 1006.17s
INFO: 5039 processes: 38 internal, 5001 local.
FAILED: Build did NOT complete successfully
```
",False,"[-5.77548325e-01 -1.84904948e-01 -1.92666471e-01  8.54170173e-02
  8.42699036e-02 -5.53834856e-01 -1.85264200e-01  8.59089419e-02
 -3.82945359e-01 -3.33565325e-01  2.64637433e-02  2.25342549e-02
 -3.20919007e-01  1.19479820e-01 -5.54928854e-02  4.96327460e-01
 -2.18302637e-01 -5.28168455e-02  2.59737611e-01  1.32362768e-01
 -2.90704966e-01 -5.46953455e-02 -1.81217641e-01  2.58045137e-01
  1.30225718e-01  9.73324925e-02 -3.43227893e-01  1.27836615e-01
  3.12607884e-02  1.09420866e-01  3.37887079e-01  2.22840965e-01
 -4.40092720e-02 -1.42763972e-01  1.95693523e-01  4.34720933e-01
 -1.13392055e-01 -1.79998070e-01 -3.63904297e-01 -1.67244911e-01
 -1.06122807e-01  6.20763749e-02  1.73578322e-01 -2.95302033e-01
 -1.14017725e-03 -2.93112338e-01  1.70303971e-01 -5.28693292e-03
 -6.86900318e-02 -4.77267504e-01 -1.34018451e-01 -6.29736930e-02
 -3.66525948e-01 -4.37479705e-01 -6.64785430e-02 -9.23746545e-03
  4.33194600e-02 -2.46473849e-02  8.69251974e-03  1.96696594e-01
  1.76275760e-01  1.66671470e-01  4.38654162e-02 -4.94940169e-02
  2.27505162e-01  1.86865479e-01  3.17666948e-01 -1.17242463e-01
  3.96346927e-01 -2.67442226e-01  2.29773477e-01 -1.38285995e-01
 -2.25591183e-01  2.14414567e-01  4.45637777e-02  6.83622956e-02
  3.66294324e-01  1.61107257e-01  3.04508179e-01 -1.31725729e-01
 -2.75698423e-01 -2.83575028e-01 -1.12975396e-01 -2.29997769e-01
  1.23778164e-01  5.42483926e-02  3.15283775e-01  1.40333861e-01
  5.00595272e-01 -1.54322326e-01  2.73351848e-01  4.25829291e-01
 -9.52601805e-02  1.78526547e-02  2.71773398e-01  2.13139087e-01
  3.52026939e-01  2.86358416e-01 -1.36257589e-01  6.41634166e-02
 -5.47164902e-02 -4.05064225e-01  1.19687825e-01 -2.74572410e-02
 -1.64305478e-01 -2.44452327e-01  2.94616342e-01  4.20817398e-02
  4.55636904e-02 -1.68333307e-01  4.27145250e-02  1.25154883e-01
  7.93073773e-02 -8.88818204e-02 -1.43487364e-01  4.89367507e-02
 -1.75832003e-01 -2.13997513e-01  1.96637232e-02  7.43427515e-01
 -3.04727890e-02 -1.36443198e-01  4.73647639e-02  1.63444821e-02
  5.41346908e-01  1.07949361e-01 -1.59384787e-01 -1.19931005e-01
  3.22147876e-01  1.27126295e-02  3.65626067e-03  2.54501224e-01
  5.26068509e-02  2.73115247e-01 -2.51970487e-03  9.62049663e-02
 -1.57418862e-01 -6.50007725e-02 -7.38575310e-02 -4.77820039e-01
 -1.17997512e-01  9.20487866e-02  4.74489070e-02 -7.28420734e-01
 -9.91430879e-03 -7.79099017e-02 -2.84790426e-01  6.30160868e-02
 -3.30042124e-01  5.19613512e-02 -1.73039943e-01  1.89245760e-01
 -1.97272580e-02  4.33291405e-01  2.64190674e-01 -1.83998812e-02
  3.55094969e-01 -3.16485725e-02  8.52358900e-03 -4.71561462e-01
 -8.04808177e-03  4.55766380e-01 -8.68006647e-02 -2.32570440e-01
 -1.81087136e-01  1.56332344e-01 -4.85172808e-01 -2.25070089e-01
  3.11579406e-01  5.49463034e-01 -1.35319084e-01 -1.08037084e-01
  2.49744982e-01  1.50781006e-01  2.70015001e-01 -2.27216855e-01
  3.51892114e-01 -2.48068362e-01  1.26330107e-01  5.41938663e-01
  2.19089866e-01 -3.88657190e-02  1.21113598e-01  2.52746195e-01
  1.34219334e-01  1.11856230e-01  1.78051800e-01  2.35885262e-01
 -1.67940423e-01  1.60134304e-02 -3.80439669e-01 -5.08554205e-02
  3.52495730e-01 -2.34279841e-01 -2.45188177e-01  1.51111022e-01
  2.20422834e-01 -1.46863461e-01  1.45549268e-01 -1.18709549e-01
  9.67686251e-03  1.06240613e-02 -1.71613887e-01  2.22250044e-01
  8.04665685e-02 -2.85250515e-01 -1.56800095e-02 -2.90322840e-01
 -3.50887239e-01  1.55346781e-01  1.48470476e-01 -3.90443206e-01
  1.82799488e-01 -1.62065923e-01 -1.27855450e-01  8.55680928e-03
  2.02317029e-01 -9.83453095e-02 -8.27359110e-02  2.13479161e-01
 -4.77167405e-03 -4.56949890e-01  1.02741338e-01 -3.33113551e-01
 -5.13680354e-02 -1.36801777e-02 -2.02736229e-01 -1.44014120e-01
 -1.67153060e-01 -4.21506874e-02 -2.48220190e-03 -7.63176158e-02
  4.26727712e-01  2.10759670e-01  4.90493417e-01 -1.11838996e-01
 -7.18375072e-02 -2.76260018e-01 -3.97610217e-01 -5.81264868e-02
 -3.14298481e-01 -3.78079951e-01  1.93934321e-01  1.69245135e-02
  3.03851455e-01  4.32621300e-01  1.05429497e-02 -4.85762283e-02
 -4.29796934e-01  2.53163487e-01 -3.36717844e-01  4.00869519e-01
  3.09473544e-01  9.13651064e-02  4.09892648e-01  2.29466170e-01
  2.04992190e-01  2.99995780e-01  2.92397797e-01 -2.82754093e-01
  3.14675301e-01  3.47481728e-01  1.81067258e-01  4.00528461e-02
  9.37364623e-02  2.63659120e-01 -3.87676120e-01  3.87899697e-01
  3.51834446e-02 -7.66249448e-02  3.23081911e-01 -1.92829132e-01
  5.21603882e-01 -3.86867642e-01  6.63331822e-02 -1.18660040e-01
  3.15406621e-01 -1.10326774e-01  2.80335825e-02 -1.14861503e-02
  1.97750747e-01  1.78906128e-01 -9.21115875e-02  1.06401108e-01
 -6.65161759e-04 -2.03314096e-01 -4.24777269e-02 -8.79310250e-01
 -2.54244953e-01  1.20360434e-01 -3.21767986e-01  8.21024179e-02
 -1.86975300e-01 -2.28501618e-01 -1.92837089e-01  1.59156233e-01
 -4.25093696e-02 -6.40519708e-02 -3.76039892e-02  1.84758633e-01
 -1.61766544e-01 -1.28601804e-01  3.86280775e-01 -4.21131134e-01
 -7.38876611e-02 -1.55515701e-01  1.67019710e-01  2.00211585e-01
  4.62753087e-01 -5.40554166e-01  5.04888594e-03  6.35428056e-02
 -2.34706298e-01  3.03662479e-01  1.64487492e-02  2.29074150e-01
 -3.18042159e-01  8.69541407e-01  2.44367659e-01 -1.48453057e-01
  2.28723451e-01 -1.89909935e-01 -4.19173211e-01 -4.58054915e-02
  2.07181931e-01 -7.40385801e-02 -4.47158255e-02 -4.92977053e-01
  2.45201960e-01  1.79868832e-01 -1.82865500e-01 -1.66059788e-02
 -1.58917934e-01  1.90601468e-01 -4.00103480e-01 -8.45839903e-02
 -4.02610481e-01  6.93282709e-02 -2.19315112e-01 -3.09379280e-01
 -2.02811405e-01 -2.09518269e-01 -1.80182233e-01 -1.99368685e-01
 -6.20126650e-02 -3.48480731e-01  1.56853363e-01  3.64994764e-01
  4.74191532e-02  3.35866213e-02 -3.35251689e-02  3.38792652e-01
 -3.43926847e-01  1.38428465e-01 -5.96560277e-02  2.19077155e-01
 -1.82261020e-01  8.37773308e-02  5.00004768e-01  1.76916942e-01
  5.84830120e-02  1.39647201e-02 -3.92526805e-01  1.17438957e-01
  1.47630244e-01 -2.91485041e-01 -3.20052445e-01 -4.99784434e-03
  4.95113954e-02  2.15006888e-01  1.37359276e-01  2.31398672e-01
 -2.62854785e-01  2.83778608e-01  5.67527890e-01 -4.69320416e-01
 -1.60354346e-01  1.74431592e-01  2.01269060e-01 -2.91757341e-02
  6.81699663e-02 -1.53583437e-01  3.98847818e-01  4.50196750e-02]"
Building pip package broken by new shared object in build type:build/install awaiting PR merge,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

17.0.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Auditwheel used during pip package creation fails due to unable to locate needed shared object.

Failure introduced by https://github.com/tensorflow/tensorflow/commit/96c01ac84cc5befb70d258a2ef06846a71d2bc0c

### Standalone code to reproduce the issue

```shell
$ bazel build --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --define=tf_api_version=2 --verbose_failures --jobs=100 -- //tensorflow/tools/pip_package:build_pip_package
$ bazel-bin/tensorflow/tools/pip_package/build_pip_package --cpu --project_name tensorflow_aarch64 ./tensorflow-pkg
$ auditwheel repair --plat manylinux2014_aarch64 -w /tensorflow/whl /workspace/tensorflow-pkg/tensorflow_aarch64-2.16.0-cp310-cp310-linux_aarch64.whl
```


### Relevant log output

```shell
INFO:auditwheel.main_repair:Repairing tensorflow_aarch64-2.16.0-cp310-cp310-linux_aarch64.whl
Traceback (most recent call last):
  File ""/usr/local/bin/auditwheel"", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/main.py"", line 59, in main
    rval = args.func(args, p)
           ^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/main_repair.py"", line 173, in execute
    out_wheel = repair_wheel(
                ^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.11/dist-packages/auditwheel/repair.py"", line 80, in repair_wheel
    raise ValueError(
ValueError: Cannot repair wheel, because required library ""_pywrap_tensorflow_internal.so"" could not be located
```
",False,"[-5.71946442e-01 -4.83879954e-01 -4.75826114e-03  1.01498049e-02
  8.11583996e-02 -3.15150261e-01 -2.76385516e-01 -1.11801349e-01
 -3.19118977e-01 -1.66821450e-01 -8.48120749e-02  1.59555957e-01
 -1.07261591e-01  2.48980761e-01 -6.17100298e-03  3.21448594e-01
 -2.16113016e-01 -1.25476569e-01  7.38968626e-02  1.76793575e-01
 -2.30911732e-01 -2.28597131e-02 -3.81351709e-01  1.92878857e-01
  2.86584586e-01  1.22682355e-01 -2.35853091e-01 -7.31301494e-03
  6.78033680e-02 -3.21438462e-02  5.16809404e-01  5.60686216e-02
 -8.03534985e-02  1.82644427e-01  1.82953507e-01  3.81861091e-01
 -1.00965947e-01 -3.42612088e-01 -1.92268550e-01 -1.21699974e-01
 -9.36336666e-02  8.12742636e-02 -1.64609030e-02 -2.65078723e-01
 -1.11634158e-01 -3.82616609e-01 -1.22175619e-01 -3.41331422e-01
 -7.19792992e-02 -2.53881544e-01 -2.56597161e-01  1.77806512e-01
 -5.35228491e-01 -9.48509052e-02 -1.24830678e-01 -3.15300107e-01
  1.24600440e-01  1.09352373e-01  1.25841290e-01 -1.43117562e-01
  3.41043547e-02 -2.63851900e-02  4.74171937e-02  9.95702744e-02
  2.90077645e-03  1.18391991e-01  2.10702389e-01 -3.91493589e-02
  2.10944653e-01 -3.93496662e-01  9.07258317e-02 -6.11722171e-02
 -5.08126020e-01 -1.27385676e-01  1.73401088e-01 -9.33662802e-02
 -1.20080456e-01  3.15573215e-01 -4.26176116e-02 -7.56412819e-02
  6.20244592e-02 -9.59787294e-02 -6.31892234e-02 -4.16554883e-02
 -3.35267298e-02 -2.57659368e-02  1.48677707e-01 -3.38257328e-02
  5.09746552e-01 -2.08420992e-01  4.76904511e-02  3.66265774e-01
  2.42709458e-01  2.05917507e-01  5.32636344e-01  2.82459725e-02
  1.36831954e-01  2.56848514e-01  2.08609030e-02 -7.66330510e-02
 -6.91100955e-02 -2.15717584e-01 -5.24546579e-02 -2.66991138e-01
  3.24373953e-02  8.59883949e-02  3.95539850e-01 -1.33470129e-02
 -1.72044616e-02 -1.83098078e-01  3.98574285e-02  9.60286558e-02
 -1.71659105e-02 -4.52427387e-01 -8.96831751e-02  1.90344267e-02
 -1.29274428e-01 -3.29199016e-01 -3.07425931e-02  7.54666805e-01
  1.82907343e-01 -4.90119830e-02 -9.94064584e-02  3.48331273e-01
  4.25069928e-01  2.17707485e-01  2.10753173e-01  2.58191675e-02
  7.94370752e-03 -1.61017418e-01  3.81310247e-02  2.98727244e-01
  3.43301028e-01  2.18393162e-01  4.11651611e-01  2.11388022e-01
 -3.48371565e-01 -4.12272096e-01 -8.33280310e-02 -1.76837355e-01
 -3.69514644e-01  4.47118491e-01 -2.64234185e-01 -7.42808819e-01
  1.15503035e-01  1.55152932e-01 -1.69080496e-01  4.11557913e-01
 -2.20932961e-01  4.47947979e-02 -1.37023538e-01 -8.74626338e-02
 -2.33068708e-02  4.73367125e-01  4.51261431e-01  7.14942664e-02
  7.73909166e-02  1.73382685e-02  2.13151515e-01 -3.33722681e-01
 -2.29298875e-01  4.28726196e-01 -4.54123557e-01 -1.05664283e-01
 -6.31423369e-02  1.40688807e-01 -6.31600142e-01 -2.23290861e-01
  2.45774478e-01  1.78646058e-01 -4.24251229e-01 -1.60577804e-01
  2.56247193e-01  1.01277931e-02  1.51097983e-01 -1.93751361e-02
  2.18226284e-01 -4.37085479e-01  9.21042711e-02  6.73633456e-01
  2.07231179e-01  3.93181592e-01  4.97418121e-02  5.02207801e-02
 -4.75237928e-02  8.69608223e-02  2.08227575e-01  4.32254560e-03
  3.78114805e-02  1.24969989e-01 -3.16946208e-01  1.03435248e-01
  3.56080055e-01 -2.56556511e-01 -1.30679563e-01  3.60591233e-01
  5.26876330e-01  1.41018048e-01 -1.07408371e-02 -1.54072702e-01
 -2.66082644e-01 -1.09645955e-01 -1.24803945e-01  1.07509442e-01
  3.54544446e-02 -1.94156468e-01  3.13264906e-01 -3.17450166e-01
 -5.29851794e-01  2.87339658e-01 -1.09703995e-01 -2.31884152e-01
  1.16466761e-01 -2.25408331e-01 -1.16665125e-01  2.89369702e-01
 -1.40650302e-01  1.49568930e-01 -1.34147376e-01  1.47493519e-02
  1.65908545e-01 -3.26391220e-01  2.74844974e-01 -3.18774045e-01
  6.07219972e-02  1.51858687e-01 -6.97114050e-01  5.62710091e-02
 -1.65974736e-01  4.73670810e-02 -7.04402402e-02 -1.25882193e-01
  3.82292569e-01  4.53630716e-01  5.21907508e-01 -4.71389145e-02
  1.40084282e-01 -5.58195896e-02 -1.01042822e-01  3.06182683e-01
 -1.44303754e-01 -2.89411187e-01  1.62456602e-01 -4.67843451e-02
 -5.02556190e-03  5.53237617e-01 -1.37672335e-01 -4.11155745e-02
 -4.43910122e-01  4.37515005e-02 -4.29774940e-01 -4.65841815e-02
  3.88379276e-01  1.72526985e-02  4.63481247e-01  1.91998869e-01
  1.70872677e-02  2.09892720e-01  1.00221738e-01 -1.88469216e-01
 -3.65570262e-02  2.61682630e-01  1.61917657e-01  1.23942614e-01
  2.38095641e-01  3.91049646e-02 -2.68244743e-01  4.98891413e-01
 -2.22777739e-01 -1.64494380e-01  4.00830209e-01 -2.98473865e-01
  5.25011301e-01 -5.89903295e-02  1.91356748e-01  2.16121152e-01
  2.30310634e-01 -1.57091975e-01  6.24853000e-02  1.42412335e-01
 -4.76756804e-02  4.56305474e-01 -5.64210594e-01  2.75574028e-01
  1.46278962e-01  6.78373873e-02 -5.04640490e-02 -4.40042734e-01
 -2.75578469e-01 -5.71361706e-02 -3.74579579e-01  1.11887995e-02
 -1.78482309e-01 -5.45761772e-02  3.87313031e-03  1.55556783e-01
  1.76642597e-01 -8.17179829e-02  3.71872962e-01  3.94306108e-02
 -1.07818376e-02 -1.43847615e-03  1.63592756e-01 -2.17270195e-01
 -3.73613477e-01 -9.74105448e-02  1.62443906e-01  2.63945401e-01
  3.96308064e-01 -6.10530019e-01  3.34287614e-01 -5.51420897e-02
 -2.96496749e-01  4.40264285e-01 -1.36615455e-01  1.53664693e-01
 -6.14022732e-01  4.17056888e-01  5.51944673e-02 -1.63728550e-01
  6.06265441e-02 -3.72712947e-02 -7.22789407e-01  9.88525450e-02
  9.83856469e-02 -1.64158583e-01 -2.36163929e-01 -1.66053653e-01
 -1.91100687e-01  3.75045210e-01 -8.01615417e-02 -1.20545782e-01
 -2.18239613e-04  3.53515387e-01 -2.07161203e-01 -4.21254039e-02
 -3.40262353e-01  1.21751294e-01 -1.57238930e-01 -2.64210999e-01
 -5.71207628e-02  2.08472963e-02  2.08420753e-01  7.85387121e-03
 -2.22825319e-01 -4.28528905e-01  3.28846723e-01  3.04551512e-01
 -3.86481881e-02  3.45902257e-02  9.02675763e-02  1.00952305e-01
 -7.13463187e-01  1.57103315e-01  1.80864349e-01  5.31430125e-01
  5.69610670e-02 -7.45434463e-02  2.56883740e-01  2.18438432e-01
 -8.20716321e-02  8.56926814e-02 -3.47687304e-01 -3.84776704e-02
  2.57943757e-02 -7.28204101e-03 -3.74866277e-01 -3.08165073e-01
 -2.15188175e-01  5.24521828e-01 -6.54848143e-02  2.89002389e-01
 -3.05930793e-01  4.33174789e-01  7.37670600e-01 -2.88397670e-01
 -2.44861484e-01  2.18563691e-01  2.26537183e-01 -1.39637828e-01
 -1.45883799e-01  2.79773921e-01  3.24983716e-01  3.48429531e-02]"
Status of ragged tensors in tf nightly  stat:awaiting response type:feature type:support comp:keras,"Hello,

It seems keras 3 (used in tf nightly) has dropped support for ragged tensors.

What are the plans for the future of ragged tensors in keras tf?

Thanks ",False,"[-4.37409610e-01 -2.43075162e-01  1.43003026e-02 -3.10412437e-01
 -1.22886784e-01 -1.34198815e-01 -1.44143075e-01 -4.96980641e-03
 -2.21682921e-01 -1.60152271e-01 -1.29519463e-01  6.64764196e-02
 -3.63738775e-01 -7.97914714e-02 -3.22510675e-02  1.71971902e-01
 -1.62699223e-01  1.32704526e-01  2.98630148e-02  6.41666651e-02
 -2.17138901e-01  1.31687850e-01 -9.01262686e-02  3.15800309e-01
 -4.31645699e-02 -1.34659737e-01  2.90763319e-01 -2.03783751e-01
 -7.81420469e-02 -7.22168311e-02 -2.58872509e-01  4.16458368e-01
 -1.43541798e-01  2.37357081e-03 -2.70084649e-01  3.48419279e-01
  4.74105738e-02  1.15889549e-01  8.33588764e-02  1.64812639e-01
  6.23205639e-02 -1.02151766e-01  2.76858453e-02 -1.70820862e-01
  1.30345533e-02 -3.53765748e-02  1.50193229e-01 -1.53237164e-01
 -2.42907703e-02  1.33840412e-01 -1.56645223e-01  3.58960964e-02
 -1.70720711e-01 -1.28693715e-01 -1.36980359e-02 -7.00378716e-02
  1.98471412e-01  7.84944668e-02 -1.07271068e-01  1.13491371e-01
 -2.11375225e-02  3.33088823e-02  4.74049225e-02 -6.89784437e-02
  3.88465375e-01  1.87309131e-01  2.82041967e-01 -3.28232944e-02
  2.22745255e-01  1.35648787e-01 -1.49329856e-01 -3.46729420e-02
 -6.16782725e-01 -2.27816731e-01 -6.95989504e-02  1.46602184e-01
  2.01768443e-01  1.27138853e-01  5.21128893e-01 -1.44936532e-01
 -5.23605272e-02  5.50199151e-02  1.40236653e-02 -2.82467574e-01
  4.53098640e-02  2.53501553e-02 -2.91755516e-02  5.02393320e-02
 -6.48098588e-02 -3.67547959e-01  5.23387074e-01 -1.97476685e-01
 -3.24032307e-02  2.11740062e-01  1.65670633e-01  2.41160110e-01
  1.15893163e-01  1.61908582e-01 -1.03395738e-01  5.35595939e-02
 -3.15277606e-01 -5.87998889e-02 -2.61795759e-01  2.15276435e-01
  6.02030009e-03  3.19559097e-01  1.38493299e-01 -3.58544827e-01
 -1.16786227e-01 -2.91240718e-02  3.65573823e-01  6.10188916e-02
  3.87799352e-01  2.97342055e-02  9.83120054e-02 -9.34375823e-02
  3.74169290e-01  2.36726046e-01  4.01548482e-02  5.75884521e-01
 -9.92271006e-02 -8.75616223e-02  2.61570117e-03  2.95546591e-01
  9.07212198e-02  2.60199457e-01 -1.30871966e-01  4.55600433e-02
  3.00441653e-01  1.60563409e-01  5.86512052e-02  1.65269300e-01
 -2.28223458e-01  1.67264655e-01 -7.67702758e-02 -8.83797035e-02
  1.26988322e-01 -1.74672946e-01 -2.68934250e-01 -2.84214705e-01
 -9.99586061e-02 -1.65509015e-01  1.15542561e-01 -4.90324423e-02
  2.89780080e-01 -1.79849975e-02 -2.73728758e-01 -3.42241436e-01
 -2.24072531e-01  2.08717525e-01 -1.19563378e-01 -1.86757833e-01
  1.00031793e-01 -1.89863443e-01  5.99107668e-02  9.40159932e-02
 -6.92864656e-02  2.12546326e-02 -3.29903007e-01 -3.07210058e-01
  2.57780194e-01  3.93723965e-01 -7.56482929e-02  3.00543666e-01
 -5.85277826e-02  3.08026701e-01 -1.88189879e-01 -2.11656287e-01
  1.83410212e-01  2.08294407e-01  1.98181480e-01 -1.33620918e-01
 -2.40584895e-01  1.15749769e-01  6.50837302e-01 -9.11335573e-02
  3.91095988e-02 -5.10358632e-01  2.08191231e-01  1.20453775e-01
 -5.09873070e-02  6.35896474e-02  1.81956381e-01  5.16852029e-02
  1.00231968e-01 -3.59863102e-01  1.64220139e-01  1.32646173e-01
 -3.29502791e-01 -1.43989086e-01  3.88381816e-02 -3.84036064e-01
  9.35181379e-02  8.87949765e-02  6.95398971e-02 -2.25942001e-01
  2.29843631e-01 -6.18478954e-01 -4.51968685e-02 -1.89334586e-01
  9.97987092e-02 -3.75660248e-02 -3.18559170e-01  4.04027756e-03
  8.49780664e-02 -4.79271978e-01 -6.84741214e-02 -3.02734494e-01
  1.37401349e-03 -3.48185271e-01  1.00272477e-01 -1.38954997e-01
  5.50686754e-02 -9.41538960e-02 -7.65592977e-02  5.79275787e-02
  1.04561716e-01 -2.18972519e-01 -2.94108719e-01  1.08196564e-01
  2.75765896e-01  1.24735758e-01  1.16021624e-02 -3.20449732e-02
 -1.65853783e-01 -6.09613769e-02 -2.17111275e-01  4.03093994e-01
 -1.07973419e-01  1.88362226e-01  1.52164221e-01  3.59017909e-01
 -2.78219599e-02 -1.46061167e-01  1.92233384e-01 -1.03271224e-01
 -2.04744920e-01 -8.48238356e-03  9.09164101e-02 -7.06374869e-02
 -5.11378229e-01  1.28754973e-01 -2.82396488e-02  2.18482748e-01
 -4.43665124e-02 -1.55535176e-01 -2.65123069e-01 -1.45747804e-03
  1.12387992e-01  2.43260100e-01 -1.44102901e-01 -3.72824296e-02
 -2.38293007e-01  6.43534958e-02  2.61311028e-02 -3.57962519e-01
 -9.57901627e-02  3.74581277e-01  6.86672688e-01  4.97293398e-02
  3.87391061e-01  1.74096506e-02 -1.14259608e-01  2.90677935e-01
  3.00464153e-01  3.47213089e-01  5.40693067e-02  3.34022164e-01
 -7.03492910e-02  3.10504083e-02 -4.00137424e-01 -7.45973364e-02
  9.91998464e-02 -1.73173875e-01  2.55237281e-01 -1.13579430e-01
  2.76937842e-01  5.37137806e-01 -2.60878265e-01  2.80461282e-01
 -1.36212707e-01  2.50510693e-01 -2.63208866e-01  4.79934752e-01
 -2.97154874e-01  3.97439152e-02 -6.73453659e-02 -7.60049641e-01
  4.35823083e-01 -2.08502725e-01 -2.07788557e-01 -4.12623495e-01
  3.69206160e-01 -2.81495988e-01  1.56730801e-01  2.78791428e-01
 -3.29570860e-01  4.30053659e-02  1.16608188e-01 -1.20530017e-01
 -1.18118912e-01 -3.77228186e-02  1.27728298e-01 -1.14644401e-01
  2.31256366e-01 -2.08062865e-02  1.55118421e-01  3.59944373e-01
 -8.04942772e-02  1.65060788e-01  2.77304173e-01  1.37317717e-01
  2.32910678e-01  1.52865216e-01 -6.55104443e-02  6.74307272e-02
  1.90723851e-01  5.05351305e-01  2.29690865e-01  1.41630813e-01
  1.31905034e-01 -1.54750645e-01 -4.30990100e-01 -1.87590808e-01
  1.37608498e-01 -1.85004249e-01  8.05872679e-02 -4.90680009e-01
 -1.83343500e-01  3.36032808e-01  2.86008045e-02 -2.64259398e-01
 -3.65442842e-01  7.47093037e-02  2.69440114e-02 -1.15552612e-01
 -4.00538981e-01  5.78579027e-03 -2.43052393e-01 -3.86303306e-01
 -2.45024994e-01 -1.81850523e-03  5.15250713e-02 -9.80890170e-02
  1.68537498e-01 -2.39948243e-01 -1.01445012e-01  3.79451841e-01
 -7.25455349e-04  5.39359413e-02 -1.94977522e-01  3.91285956e-01
 -1.12819284e-01 -1.62531480e-01  6.25216290e-02  7.06960214e-03
  1.33035943e-01  1.91036001e-01  2.79899269e-01  4.21923876e-01
 -4.12909120e-01 -1.41717508e-01 -1.00062229e-01 -1.29078105e-01
  2.72887856e-01 -4.13453907e-01  2.39938736e-01 -2.01930285e-01
 -1.60719037e-01  9.01912749e-02  1.82774574e-01  4.62850869e-01
 -4.61603366e-02  2.42252111e-01 -5.60167059e-02 -2.23706961e-01
 -3.22674155e-01  4.33566362e-01 -2.69387633e-01 -2.81783104e-01
 -1.54707521e-01 -6.13079928e-02 -2.49779493e-01 -1.04767829e-01]"
TF Lite in play services does not support version 1.13.0 stat:awaiting response comp:lite,"I received an error report from a user that was using TensorFlow Lite through the `mobile_scanner` library.

As this library uses the most up-to-date version of `mlkit-barcode-scanning`, which has `TensorFlow Lite` as a dependency,
I cannot resolve the root cause of this issue.

From the changelogs I do not directly see which version of TensorFlow Lite supports version `1.13.0` in the Google Play services.
Thus I don't know if I should ask the maintainers of `play-services-mlkit-barcode-scanning` to update to the latest available version with a fix. (the last version of that dependency dates back a few months)

I can ask the user for their device / Google Play services version.

**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible): N/A 
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
   - is a transitive dependency of `com.google.android.gms:play-services-mlkit-barcode-scanning` version 18.3.0
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`): N/A

**Standalone code to reproduce the issue**
```
import com.google.mlkit.vision.barcode.BarcodeScanning

class MobileScanner() {
  private var scanner = BarcodeScanning.getClient()
}
```

**Any other info / logs**
```
W/native (30072): W0000 00:00:1698456375.622782 32103 abi_method_util.h:33] not supported: TfLiteInterpreterGetTensor: TF Lite in Google Play services has stable ABI version 1.11.0 which is less than the required version 1.13.0.
```
",False,"[-2.57586241e-01 -5.58749199e-01 -4.45333980e-02 -1.56382531e-01
  1.72887683e-01  9.25309211e-02 -2.06414804e-01  5.05507961e-02
 -1.24518305e-01 -1.62037075e-01 -1.31937265e-01 -1.30220518e-01
 -2.52821863e-01  1.90140814e-01  2.44960830e-01  1.29739940e-01
 -2.01876551e-01 -2.58845925e-01  1.61858305e-01 -7.23921433e-02
 -9.36274678e-02 -1.98336780e-01 -2.01597810e-01  9.30179581e-02
 -9.50078219e-02 -8.10257420e-02 -1.80379540e-01 -2.90977150e-01
  1.53749436e-01  2.28776053e-01  3.97444516e-02  1.71451032e-01
 -1.63915828e-01  1.24686928e-02 -3.89832631e-02 -1.79686714e-02
 -1.76305667e-01 -1.01177424e-01 -3.15035999e-01 -7.13176578e-02
  1.71269491e-01  1.59238458e-01 -1.68796871e-02  4.09085825e-02
 -2.54956447e-03  1.31746277e-01 -4.12041470e-02 -3.75576355e-02
 -2.15016350e-01  1.91062510e-01 -1.30121827e-01  1.38178719e-02
 -3.19528580e-01 -1.13865048e-01  1.30916864e-01  7.28646517e-02
 -3.14332992e-02  2.47923389e-01  3.42898577e-01  2.77026683e-01
 -8.44133869e-02  4.75009456e-02 -8.13481957e-02 -8.99193585e-02
  2.08115093e-02  2.68567838e-02  1.34548157e-01 -2.62127042e-01
  3.80698919e-01 -3.10082406e-01 -6.96461797e-02 -9.10473913e-02
  2.53344942e-02 -1.04532056e-02 -1.71103001e-01  6.27150685e-02
 -2.64929295e-01  3.57737899e-01  3.17896545e-01 -1.62963107e-01
  1.23626962e-01 -2.62514073e-02  5.27869165e-02  1.06964499e-01
  3.28242362e-01 -9.94872376e-02  2.71171540e-01  4.73412126e-02
  2.07360089e-01  2.65221838e-02 -3.24954814e-03  1.89571589e-01
  9.58191380e-02  3.65677893e-01  1.34856105e-01 -1.64032038e-02
  1.08240172e-04  3.47948000e-02 -1.09596588e-01  5.61215244e-02
 -1.59971803e-01 -3.48829955e-01 -4.99285497e-02  2.79078424e-01
 -5.78294694e-03 -1.45437315e-01  7.11866617e-02  1.11467309e-01
  1.64945096e-01  2.76295841e-02  4.16513860e-01  2.93263067e-02
  1.56540692e-01 -1.05330937e-01 -5.16707525e-02  2.09786389e-02
 -1.07271805e-01 -8.56422074e-03  2.18476996e-01  5.79419971e-01
  1.39739364e-04 -2.40426973e-01  1.31026089e-01  1.69821884e-02
  1.35891169e-01  1.64145544e-01 -3.01148862e-01  5.11066690e-02
 -1.29420996e-01  7.58660398e-03  6.00926131e-02  1.57817483e-01
 -1.17669381e-01 -2.30790302e-02  1.89164430e-01  1.27543837e-01
 -3.88161242e-01 -3.17173302e-01 -1.35407478e-01 -5.81033938e-02
 -2.52389014e-01  2.03365684e-01 -4.84855250e-02 -1.01905875e-01
  2.87372675e-02  2.15465307e-01  4.34580073e-02  1.79373831e-01
 -6.87267780e-02 -2.20029593e-01  2.19256412e-02 -1.98695645e-01
 -6.82693720e-02  2.71787256e-01  6.20993413e-02  2.26632655e-02
  3.62281144e-01 -1.16995260e-01  5.58271632e-02 -4.83985037e-01
 -3.45220566e-02  1.61939338e-01  6.14215657e-02  2.59235110e-02
  1.13840751e-01  3.29495072e-02 -5.01238465e-01 -2.53714081e-02
 -1.42252058e-01  2.00876430e-01  2.08459534e-02 -1.58756316e-01
 -2.64282245e-02 -1.04404218e-01  6.89962730e-02 -2.44326115e-01
  3.44263196e-01 -3.91286552e-01 -2.99416304e-01  5.85385896e-02
  1.01943389e-01  9.43929106e-02  1.62736565e-01 -6.30149245e-02
 -6.25058934e-02  2.00201273e-02  1.68716222e-01  2.28323177e-01
  1.54457986e-04 -1.65127590e-03 -4.09675121e-01 -1.44540414e-01
  1.14748858e-01 -1.50427986e-02 -1.06390983e-01 -1.40584260e-01
  6.83571100e-02 -2.16382027e-01  1.64236277e-01  4.68123630e-02
 -9.47272703e-02  3.04969221e-01  1.31681897e-02 -1.63790122e-01
  1.69077396e-01 -7.32575506e-02 -1.97359487e-01 -2.42190272e-01
 -2.74996072e-01 -9.08221006e-02  1.61436528e-01 -2.74848521e-01
 -3.06490570e-01 -2.50164151e-01 -2.27578074e-01  1.88839227e-01
 -1.67726278e-02 -2.03287497e-01 -1.67594090e-01  3.15718770e-01
 -1.10203862e-01  2.68770810e-02 -1.64865956e-01 -1.65443569e-01
 -4.27891493e-01 -7.72728622e-02 -1.09040514e-01  2.74298370e-01
 -8.04612115e-02  2.93584168e-01 -5.61270490e-02  7.94770718e-02
  5.80645204e-01  1.05040610e-01  2.56939530e-01 -1.50534928e-01
 -1.41077667e-01 -1.34309039e-01 -3.20253313e-01 -2.17789292e-01
 -3.90602142e-01 -9.89269018e-02 -2.59463992e-02 -1.44300073e-01
 -1.92400888e-02  1.66053474e-01 -1.27426192e-01  1.73830733e-01
 -9.26546603e-02  3.61957997e-01 -1.50873009e-02  2.91174687e-02
  1.05634034e-01  1.69769600e-01  6.53444052e-01  2.25579180e-02
 -2.61016116e-02 -2.10906789e-01  3.58695686e-01 -5.13652712e-02
  4.08832371e-01  1.84288323e-01  2.24764254e-02  5.23329914e-01
  3.06341708e-01  2.03099176e-01 -9.62312296e-02 -4.20770720e-02
 -2.16277733e-01  8.43624249e-02  6.88934922e-02 -6.23757094e-02
  4.89539325e-01 -2.23140553e-01  1.52827725e-01 -9.18110311e-02
  3.83512408e-01  1.56033505e-02 -3.78345907e-01  3.11204419e-03
  2.46475041e-01  4.44002986e-01 -4.02909547e-01  4.06208374e-02
  6.16243929e-02 -2.62363553e-01 -1.96074247e-01 -5.19891560e-01
 -5.23123518e-02 -5.00726327e-03 -1.27175242e-01 -4.93395701e-02
  2.78534055e-01  7.41763189e-02 -1.94968536e-01 -1.04501583e-02
 -1.51458576e-01 -8.20418298e-02 -7.81286657e-02  9.21515897e-02
  3.55587378e-02  1.58998802e-01  4.91222620e-01 -1.94333792e-01
  2.14566946e-01 -5.39776571e-02  2.40017921e-01 -8.26024413e-02
  3.35299909e-01 -1.47781789e-01  2.08280534e-01 -4.93353345e-02
  9.76407714e-03  3.06432247e-01 -1.42984450e-01  2.25296468e-01
 -3.98126364e-01  3.07827860e-01  8.72981027e-02  3.63234952e-02
  1.94569528e-01 -1.26410618e-01 -3.37979823e-01 -7.11436104e-03
 -1.23894811e-01  1.14727288e-01 -1.27026103e-02 -2.33091116e-01
 -6.77794516e-02  1.12418979e-01 -1.34905994e-01 -7.96309114e-02
 -2.83783115e-03 -1.96406562e-02 -5.19878604e-02 -9.38063860e-02
 -2.48062223e-01  2.46446550e-01  3.29992399e-02 -1.26859337e-01
  1.12264812e-01 -8.13995749e-02 -6.24719113e-02 -3.48919868e-01
 -1.30053461e-01 -1.97746098e-01  2.32814133e-01  2.69313514e-01
 -7.23708719e-02  8.72872323e-02  7.03744441e-02  2.78608531e-01
 -2.21289635e-01 -7.93295652e-02 -6.34917915e-02  2.14244753e-01
 -2.43745539e-02 -9.78169031e-03  1.98110044e-01  6.36196613e-01
 -8.10503867e-03  2.59847213e-02 -1.51138812e-01 -2.80920684e-01
 -5.04840501e-02 -1.69196546e-01 -4.72786091e-02 -5.06888442e-02
  2.11059839e-01  5.91312468e-01  1.76439788e-02  3.43683451e-01
 -3.55021477e-01 -2.27083027e-01  2.57842571e-01 -2.76696682e-01
  4.56869751e-02  7.86173493e-02 -1.17314018e-01 -1.97056100e-01
  1.90146454e-03 -2.70755179e-02  2.41759211e-01 -1.60652455e-02]"
"`rate` must be a scalar or scalar tensor. Received: rate=ListWrapper([3, 4, 5, 6]) type:support comp:apis TF2.14","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.saved_model.save` APIcan not convert the model with `SpatialDropout2D` operator and threw a crash message: ""
ValueError: `rate` must be a scalar or scalar tensor. Received: rate=ListWrapper([3, 4, 5, 6])""

However, for models containing other operators, The script can run well.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow import keras as keras
from tensorflow.keras import layers, models
import numpy as np

layer = keras.layers.SpatialDropout2D(rate=[3, 4, 5, 6])
input_shape = [12, 10, 6, 18]
input_data = np.random.random(input_shape)
weights = layer.get_weights()
layer.set_weights(weights)

x = layers.Input(shape=input_shape[1:], dtype=""float32"") 
y = layer(x)
model = models.Model(x, y)
model.summary()
res_keras = model(input_data)
tf.saved_model.save(model, ""tf_model"")
```
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    tf.saved_model.save(model, ""tf_model"")
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1331, in save
    save_and_return_nodes(obj, export_dir, signatures, options)
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1366, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def))
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1578, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1490, in _build_meta_graph_impl
    signatures = signature_serialization.find_function_to_export(
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\saved_model\signature_serialization.py"", line 109, in find_function_to_export
    for name, child in children:
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\saved_model\save.py"", line 189, in list_children
    for name, child in super(_AugmentedGraphView, self).list_children(
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\checkpoint\graph_view.py"", line 75, in list_children
    for name, ref in super(ObjectGraphView,
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\checkpoint\trackable_view.py"", line 84, in children
    for name, ref in obj._trackable_children(save_type, **kwargs).items():
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\engine\functional.py"", line 460, in _trackable_children
    dependencies.update(super()._trackable_children(save_type, **kwargs))
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\engine\training.py"", line 4002, in _trackable_children
    children = super()._trackable_children(save_type, **kwargs)
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\engine\base_layer.py"", line 3470, in _trackable_children
    children = self._trackable_saved_model_saver.trackable_children(
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\base_serialization.py"", line 61, in trackable_children
    children = self.objects_to_serialize(serialization_cache)
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\layer_serialization.py"", line 79, in objects_to_serialize
    return self._get_serialized_attributes(
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\layer_serialization.py"", line 106, in _get_serialized_attributes
    object_dict, function_dict = self._get_serialized_attributes_internal(
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\model_serialization.py"", line 57, in _get_serialized_attributes_internal
    objects, functions = super()._get_serialized_attributes_internal(
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\layer_serialization.py"", line 117, in _get_serialized_attributes_internal
    functions = save_impl.wrap_layer_functions(
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\save_impl.py"", line 168, in wrap_layer_functions
    original_fns = _replace_child_layer_functions(layer, serialization_cache)
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\save_impl.py"", line 305, in _replace_child_layer_functions
    serialized_functions = child_layer._trackable_saved_model_saver._get_serialized_attributes(  # noqa: E501
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\layer_serialization.py"", line 106, in _get_serialized_attributes
    object_dict, function_dict = self._get_serialized_attributes_internal(
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\layer_serialization.py"", line 117, in _get_serialized_attributes_internal
    functions = save_impl.wrap_layer_functions(
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\save_impl.py"", line 223, in wrap_layer_functions
    fn.get_concrete_function()
  File ""C:\software\conda\envs\torch\lib\contextlib.py"", line 126, in __exit__
    next(self.gen)
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\save_impl.py"", line 390, in tracing_scope
    fn.get_concrete_function(*args, **kwargs)
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\eager\polymorphic_function\polymorphic_function.py"", line 1222, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\eager\polymorphic_function\polymorphic_function.py"", line 1192, in _get_concrete_function_garbage_collected
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\eager\polymorphic_function\polymorphic_function.py"", line 694, in _initialize
    self._concrete_variable_creation_fn = tracing_compilation.trace_function(
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\eager\polymorphic_function\tracing_compilation.py"", line 178, in trace_function
    concrete_function = _maybe_define_function(
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\eager\polymorphic_function\tracing_compilation.py"", line 284, in _maybe_define_function
    concrete_function = _create_concrete_function(
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\eager\polymorphic_function\tracing_compilation.py"", line 308, in _create_concrete_function
    traced_func_graph = func_graph_module.func_graph_from_py_func(
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 1059, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\eager\polymorphic_function\polymorphic_function.py"", line 597, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\save_impl.py"", line 632, in wrapper
    ret = method(*args, **kwargs)
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\utils.py"", line 190, in wrap_with_training_arg
    return control_flow_util.smart_cond(
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\utils\control_flow_util.py"", line 108, in smart_cond
    return tf.__internal__.smart_cond.smart_cond(
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\framework\smart_cond.py"", line 53, in smart_cond
    return true_fn()
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\utils.py"", line 192, in <lambda>
    lambda: replace_training_and_call(True),
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\utils.py"", line 188, in replace_training_and_call
    return wrapped_call(*new_args, **new_kwargs)
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\saving\legacy\saved_model\save_impl.py"", line 698, in call_and_return_conditional_losses
    call_output = layer_call(*args, **kwargs)
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\layers\regularization\dropout.py"", line 120, in call
    output = control_flow_util.smart_cond(
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\utils\control_flow_util.py"", line 108, in smart_cond
    return tf.__internal__.smart_cond.smart_cond(
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\framework\smart_cond.py"", line 53, in smart_cond
    return true_fn()
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\layers\regularization\dropout.py"", line 116, in dropped_inputs
    return self._random_generator.dropout(
  File ""C:\software\conda\envs\torch\lib\site-packages\keras\src\backend.py"", line 2172, in dropout
    return tf.nn.dropout(
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\util\traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""C:\software\conda\envs\torch\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 5786, in _dropout
    raise ValueError(
ValueError: `rate` must be a scalar or scalar tensor. Received: rate=ListWrapper([3, 4, 5, 6])
```
",False,"[-4.44523245e-01 -4.12010521e-01 -8.34484100e-02  2.78694719e-01
  1.55524403e-01 -5.77793837e-01 -2.58498549e-01 -8.62335488e-02
 -2.96965837e-01 -2.95537829e-01  2.39134565e-01 -2.03923196e-01
 -3.21766913e-01  3.47497046e-01 -6.76603764e-02  2.96102077e-01
  1.71471924e-01  1.21561654e-01  3.19723666e-01  3.72145846e-02
 -8.02499056e-02 -3.23475838e-01 -3.27922881e-01  2.46182501e-01
  1.03942953e-01  1.22767590e-01 -2.57120758e-01 -1.19433612e-01
  2.35206373e-02  1.78072214e-01  1.98314548e-01  1.26801223e-01
  4.30365503e-02 -5.51714860e-02 -1.83942646e-01  4.13377464e-01
 -1.51754469e-01 -3.07693183e-01 -2.75701284e-01 -1.08847246e-02
 -4.58054505e-02  7.01585878e-03  2.45275691e-01 -4.93788682e-02
  4.54484262e-02 -1.29512340e-01  1.05135944e-02 -1.16354838e-01
 -2.19756126e-01 -1.08254507e-01 -7.10141212e-02  1.21333553e-02
 -3.94691646e-01 -4.96972978e-01 -2.23696947e-01 -3.68612409e-02
  9.11998004e-02 -9.45807323e-02 -5.48963808e-02  7.01324120e-02
  8.72753561e-03 -5.37252948e-02 -1.81218714e-01 -9.56047922e-02
  1.32963449e-01  2.20391005e-01  3.29989046e-01 -1.05032668e-01
  2.95481384e-01 -1.04194820e-01  2.03879058e-01 -2.61162937e-01
 -3.51154923e-01  2.84351893e-02 -1.03595987e-01  4.81727533e-02
  2.62285769e-01  1.97030693e-01  3.18034321e-01  1.17267240e-02
  7.95133859e-02 -3.66270423e-01 -5.16089946e-02 -4.21292663e-01
  2.07879782e-01 -2.39851192e-01  6.03037417e-01  2.87055075e-01
  5.01930892e-01 -3.76016200e-01  5.91464639e-01  3.94441485e-01
 -4.33298677e-01  4.47110087e-02  4.50209379e-01  1.78007424e-01
  1.63679659e-01  4.75202315e-02 -6.78183958e-02 -1.52925998e-01
  3.01817395e-02 -2.79772639e-01 -5.76761588e-02  1.55405670e-01
 -2.30515078e-02 -6.99271411e-02  4.10289094e-02 -5.00182994e-03
  2.12956503e-01  6.42469451e-02  1.14840232e-01  5.84638268e-02
  1.88752741e-01 -7.70372450e-02  7.56212249e-02 -1.07868901e-02
 -1.05084673e-01  1.02576427e-03  3.47308695e-01  7.83266008e-01
 -3.58225293e-02 -1.38364971e-01  7.18584582e-02 -7.98348263e-02
  5.12309551e-01  2.27221131e-01 -1.29533689e-02  1.22328199e-01
  1.10965297e-02 -9.75545123e-02  1.14241064e-01 -7.44131953e-02
 -1.97768092e-01  2.87321746e-01 -1.13490641e-01  1.92759633e-01
 -2.57385895e-04 -2.03318626e-01 -2.20912442e-01 -3.51855695e-01
 -3.12756062e-01  2.40020528e-01 -1.52978867e-01 -6.33671463e-01
  3.39583129e-01  7.22489655e-02 -2.18223631e-01  1.74131170e-01
 -2.22638249e-01  1.47516757e-01 -1.88227028e-01  2.77007550e-01
  1.69767756e-02  3.22845638e-01  4.92825843e-02  2.56573141e-01
  4.21801239e-01 -2.40218371e-01  8.81194919e-02 -4.68286335e-01
  5.47876507e-02  4.54194754e-01 -4.11131866e-02 -1.57339856e-01
 -8.38825852e-02  2.01876193e-01 -3.09606224e-01 -2.86820054e-01
 -4.82439734e-02  5.71331501e-01 -1.00913040e-01 -1.29571348e-01
 -1.51137814e-01  1.33034870e-01 -1.03903022e-02 -1.27562732e-01
  1.43961906e-01 -6.55788422e-01 -1.52230978e-01  2.65875161e-01
 -2.63458341e-02  1.60268888e-01  1.37661949e-01  6.03292026e-02
  9.69610661e-02 -9.59953479e-03 -1.05310772e-02  1.46840066e-01
 -3.36517096e-01 -3.93926166e-03 -3.11287075e-01 -5.88104844e-01
  3.86913747e-01 -1.09306529e-01 -1.64826125e-01  5.35695329e-02
  2.03089342e-01 -5.21102771e-02  1.56241786e-02 -2.47674119e-02
 -4.40062508e-02 -2.43080314e-02 -8.54033977e-03  9.61782783e-02
  1.07569471e-01 -2.86335498e-01 -9.83504876e-02 -5.18471241e-01
 -3.62575531e-01  1.80406764e-01  2.25808024e-01 -5.10171890e-01
  1.15520418e-01  1.14085630e-01 -2.77516276e-01  2.13049144e-01
  2.57486880e-01  1.01161547e-01 -1.93118632e-01  2.90144265e-01
  4.45495062e-02 -1.17880724e-01 -4.50290591e-02 -4.26317394e-01
 -3.06927592e-01  1.70971259e-01 -1.97775140e-01  1.36698276e-01
 -1.47517741e-01  3.98874134e-02 -3.53743210e-02  1.84044465e-02
  5.22274852e-01  2.74031937e-01  1.24831922e-01 -9.66565907e-02
 -2.58332819e-01 -8.86291265e-02 -1.40287027e-01  2.98587680e-01
 -4.13065821e-01 -1.94319725e-01  1.17545500e-01 -3.75507586e-02
  2.89792836e-01  5.48341870e-01 -7.02012479e-02 -1.39621317e-01
 -4.01817679e-01  2.79056847e-01 -8.93427506e-02  1.47196904e-01
  1.78023607e-01  6.75259084e-02  3.56267571e-01  7.64788389e-02
  3.31066072e-01  2.45176136e-01  2.95052171e-01 -2.66974390e-01
  4.52248394e-01  2.02192113e-01  5.66324405e-02  6.07075512e-01
  3.54439020e-01  3.60890090e-01 -2.69543290e-01  6.21685982e-01
  1.44645527e-01 -2.61539310e-01  5.52666746e-02 -4.42420244e-01
  4.11157250e-01 -4.60211605e-01  1.46093994e-01 -2.35561579e-01
  2.95840681e-01  1.54705554e-01 -2.87549734e-01 -7.80526698e-02
  1.73409253e-01  1.82915986e-01 -2.07483321e-01  1.24486396e-03
 -2.42510885e-01 -2.07403868e-01 -8.83218646e-02 -5.69339752e-01
 -1.08879313e-01  1.89029574e-01 -3.18957984e-01  4.81274724e-02
  1.79642603e-01 -1.40825719e-01 -3.24447632e-01  1.64211690e-01
  1.60605118e-01 -9.89148021e-03  2.22302042e-02  1.13342315e-01
 -2.41861850e-01  1.12922587e-01  2.51991808e-01 -5.37010550e-01
 -1.97882950e-01 -1.51232816e-02  5.28532267e-01  2.02184349e-01
  2.54581839e-01 -5.32508492e-01  1.94068998e-03 -1.61101162e-01
 -3.32007557e-02  1.90797538e-01  7.25543797e-02  2.15322644e-01
 -2.21236154e-01  8.20660055e-01  2.89729387e-01 -3.39928344e-02
  1.40680969e-01 -1.63114905e-01 -4.61521804e-01  1.39702737e-01
  1.84108421e-01  5.18559515e-02 -3.79068777e-03 -5.32307386e-01
  1.28569394e-01  7.02440739e-02 -2.86423992e-02 -5.75305298e-02
 -1.70911193e-01 -3.10463756e-02 -8.31186920e-02 -1.74313933e-01
 -4.93937254e-01  1.80628419e-01 -7.08097517e-02 -1.67521507e-01
 -2.27293707e-02 -1.03711121e-01  4.46724333e-03 -2.80231148e-01
  9.42257494e-02 -2.36184806e-01  3.77057046e-01  6.83630407e-01
 -1.58421084e-01  5.07829636e-02 -4.01023310e-03  2.40265042e-01
 -2.26021558e-01 -7.80031690e-03  6.49782345e-02  4.25664425e-01
 -3.25540751e-02 -1.93564549e-01  3.98170888e-01  5.60746789e-01
 -1.14644974e-01  4.68371511e-02 -3.46282244e-01  1.05819292e-01
  2.34986365e-01 -3.49863708e-01 -2.18997493e-01 -3.38028580e-01
  2.60693789e-01  3.50298882e-01 -1.53287247e-01  3.53505790e-01
 -2.13872224e-01  2.60087490e-01  5.24067879e-01 -5.47567487e-01
 -1.87697023e-01  5.60892373e-02  1.55371308e-01 -1.60906225e-01
  2.77171195e-01 -3.47444654e-01  1.46115541e-01 -6.19940907e-02]"
Shape problems in LSTM; Configuring LSTM properly in Keras stat:awaiting response type:support stale comp:keras TF 2.8,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

So I wanted to make a neural network that predicts weather on python. It should take data on previous 30 days as input and make a prediction on how many % (+ or -) will the air temperature change. Neural network will compute 2 inputs, so data on a single day looks like this:

1-st input:
[20, 72, 1]

First entry is temperature, second is humidity and third is atmospheric pressure. 


2-nd input:
[*Weather observations*]

[*Weather observations*] array contains long strings of weather observations. In this array there may be one observation, may be more than one or none.


One complete unit of training data for 30-ty days looks like this:

1-st input:
[ [20, 72, 1], #1-st day
  [15, 89, 1.03], #2-nd day
  [19, 79, 1.01], #3-d day
  [24, 67, 0.98], #...
  ...
  ... ]

2-nd input:

[ [""Many cloudy spots..."", ""Today we had experienced...""], #1-st day
  [""Rain has been going for...""], #2-nd day
  [""""], #3-d day
  [""In following regions..."", ""Cause of geomagnetic..."", ""Today is one of the sunniest...], #...
  ...
  ... ]

So both of those input units contains thirty entries like this (becuase of mentioned 30-ty days), and the label for this unit is air temperature on the day that is a week ahead of the last entry.


Summarizing, the inputs will be lists of lists, where each list contains device readings/weather observations for 30 days with daily readings/observations placed in another list inside. So, first element in the second group of 30 days will be a set of observations which is second element in the first group of 30 days (aka 3-rank list, I hope you got it).

I wrote the following model:

### Standalone code to reproduce the issue

```shell
embedding_dim = 128  
vocab_size = len(words_to_int)  # number of unique words

num_input = keras.Input(shape= (30,3), name=""nums"")
nmlstm = layers.LSTM(64)(num_input) #num input goes to num lstm
nmdense = layers.Dense(64)(nmlstm) #num lstm goes to num dense

text_input = keras.Input(shape= (30, max_sequence_length), name=""text"") #basically length of the longest string observation sequence 

text_vec = layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(text_input) #text input goes to text embedding
txlstm = layers.LSTM(64)(text_vec) #text embedding goes to text lstm


united = layers.concatenate([nmdense, txlstm]) #concatenating text lstm and num dense
almostlast = layers.Dense(64)(united) #computing united input 
last = layers.Dense(2, name='prediction')(almostlast) #output dense layer

model = keras.Model(inputs=[num_input, text_input], outputs=last)
model.compile(optimizer='adam', loss=keras.losses.CategoricalCrossentropy)
model.fit(
    {""nums"": X1_train, ""text"": X2_train},
    {""prediction"": y_train},
    epochs=8,
    batch_size=10)
```
```


### Relevant log output

```shell
When I try to run the code, I get this error:

ValueError: Input 0 of layer ""lstm_1"" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 30, 2, 128)


I tried changing the input shape of text_input, but I kept on getting the same error.
To mention, I didn't have any problems on encoding data, padding it etc. Also it's better if there's a way of fixing it without changing the shape of 2-nd input data because the main goal is to make the neural network treat 30-day data as one input and providing only one output (how many % (+ or -) the air temperature will change) while learning on text observations to make predictions also based on them. Thank you in advance.
```
",False,"[-0.33807042 -0.13021596 -0.20880748  0.0880456   0.08307063 -0.3108806
 -0.06756672 -0.13721311 -0.4057343  -0.11081593  0.05730973 -0.11923458
 -0.1925059  -0.20985249 -0.12938297  0.16918069 -0.43983766  0.20728159
  0.12583068  0.19717047 -0.03195965  0.06229058 -0.10247844  0.25985548
 -0.01811081  0.20632102 -0.11289125 -0.02754782 -0.13856778 -0.07643735
  0.17910334  0.10339084 -0.16259594  0.1357005  -0.06214355  0.22092988
 -0.08711204 -0.16555    -0.25066146  0.04005364  0.10576653  0.14685419
  0.0632234  -0.4053655   0.28829736 -0.10576497  0.05547233 -0.22158879
 -0.10315053 -0.3108173  -0.23603624 -0.1922442  -0.30370525 -0.5364053
 -0.2315747  -0.16214734  0.13697585  0.00977119  0.10203781 -0.17449617
  0.01808363 -0.00403843  0.23939762 -0.13174304  0.15289609 -0.11298124
  0.09088325  0.0881983   0.40279353 -0.10205822  0.03871495 -0.00280856
 -0.47512564  0.18771103  0.19345081  0.2186926  -0.00580036  0.12914947
  0.40820783 -0.25265533 -0.07473637 -0.2423149  -0.24238256 -0.2591288
  0.17660838 -0.21122748  0.3772578   0.04125379  0.48453197 -0.16408455
  0.49669248  0.3077699  -0.19712679  0.3735897   0.640648    0.18046466
 -0.04652452  0.20218547 -0.02044839 -0.23723175 -0.13394023 -0.14702883
  0.10316464  0.29732627 -0.05145183 -0.2799955   0.19392556 -0.09443294
  0.16963601 -0.14803398  0.26383454 -0.14163646  0.11971519 -0.0987867
 -0.07780181 -0.01682962  0.08071202 -0.01179023 -0.24821553  0.6250677
 -0.09547717 -0.26311257 -0.04126565  0.35900572  0.39779398  0.03046877
 -0.37503052  0.07251546  0.06501924  0.03716577 -0.02989764  0.10945815
  0.16914971  0.09861091 -0.14660187  0.02536084 -0.08394608 -0.15179586
 -0.27848303 -0.26415682 -0.19630826  0.00106569 -0.17201656 -0.50345105
  0.31223556  0.06499162 -0.39162958 -0.16219181 -0.31467268  0.25248998
 -0.0078211  -0.07977457 -0.04902113  0.25182933  0.2581647  -0.04525993
  0.02806443 -0.23793341  0.00755082 -0.45131135  0.1254716   0.44888005
 -0.12723847 -0.22621416  0.36862564  0.14825754 -0.28755713 -0.4600179
  0.12400278  0.3126974  -0.03331471  0.20180589  0.05149962  0.12438208
  0.4094404   0.14692944  0.12483327 -0.51259315 -0.01618501  0.5653975
  0.17490102  0.16086489  0.1388486   0.2937228   0.0220126  -0.05602577
  0.06472267  0.05752103 -0.07622925  0.20479544 -0.17924726 -0.04435071
  0.38414034 -0.16761532 -0.02613242  0.08959214  0.28230384 -0.1328183
  0.02457005 -0.14820626 -0.20633748 -0.11113356 -0.27069798 -0.06697846
  0.11440749 -0.4881935  -0.14867899 -0.09042895 -0.27895847  0.38019186
  0.18967195 -0.73471254  0.10550898 -0.17855611 -0.27333027  0.18791528
  0.2807951   0.03299133 -0.17755216  0.35447916 -0.00194178 -0.24307042
 -0.16182894 -0.33087885 -0.16379553  0.23239636 -0.221651    0.31362927
  0.06764492  0.33520034  0.07904049  0.07037082  0.4684859   0.22311479
  0.5085638  -0.4724368  -0.14886639 -0.11336599  0.05876545 -0.3086668
 -0.6872025  -0.24844132 -0.03601812 -0.09265245  0.18560302  0.332323
 -0.292729    0.04538836 -0.57449424  0.01728424 -0.2269518   0.25099173
  0.25310862  0.14632964  0.44873494  0.04835741  0.21866477  0.02835082
  0.30406684 -0.18452875  0.3281482   0.14578073  0.06644743  0.35714668
  0.24614377  0.3183075  -0.27794644  0.50938785 -0.1886099  -0.30250955
  0.27015522 -0.37807003  0.509489   -0.31768018  0.30121195 -0.16685122
  0.21101403  0.04559975  0.0987608   0.08504387 -0.05562311  0.41520822
 -0.5575069   0.03191486  0.09984162 -0.00160646 -0.03823298 -0.8229423
  0.01710924  0.03330823 -0.06021687  0.00248482  0.18036312 -0.00704711
 -0.16734685  0.10985938  0.06578653 -0.00352962  0.14787908  0.08399102
 -0.19536954  0.13406327  0.48972303 -0.3537178  -0.1466595  -0.18798172
  0.41905352  0.36648583  0.52193105 -0.22996923  0.02789463  0.1719673
 -0.01118137  0.71965456  0.05745126 -0.17553301 -0.07235192  0.67979014
 -0.13501525  0.01564699 -0.00129735 -0.1369282  -0.17459443  0.10994693
  0.26591796 -0.13765052  0.10107149 -0.49295118 -0.0482478   0.16215184
 -0.22466415 -0.21708946 -0.15864626  0.07826284 -0.12204617 -0.1034316
 -0.4688248   0.4606608  -0.10106519 -0.3487278  -0.35915962  0.10481779
 -0.14012927 -0.00776701  0.3322751  -0.5144984  -0.07445897  0.47255915
 -0.10584033  0.1378947   0.01259213  0.2365374  -0.2366696  -0.1196356
 -0.2641363   0.21131957 -0.0832437  -0.09089999  0.39671016  0.5736811
 -0.15193844  0.0854014  -0.10901076  0.08124485 -0.12271079 -0.06239127
 -0.00913332 -0.28949955 -0.01754424  0.24768999 -0.02387735  0.30981487
 -0.23408073  0.31349546  0.32374924 -0.18909913 -0.18383187  0.3614835
  0.28755504 -0.10699037 -0.14912032 -0.08220494 -0.06478967  0.07191601]"
Installation of Tensorflow type:build/install,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Windows10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have installed tensorflow. While importing tensorflow with following command it failed to import: 

C:\Program Files\Python39\Lib\site-packages>python3 -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""



### Standalone code to reproduce the issue

```shell
C:\Program Files\Python39\Lib\site-packages>python3 -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""C:\Program Files\Python39\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Program Files\Python39\Lib\site-packages\tensorflow\__init__.py"", line 38, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Program Files\Python39\Lib\site-packages\tensorflow\python\__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Program Files\Python39\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 77, in <module>
    raise ImportError(
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python39\Lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors for some common causes and solutions.
If you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.
```
",False,"[-5.78479648e-01 -3.73829395e-01  5.10148853e-02  2.21311778e-01
  2.65531272e-01 -6.01280570e-01 -4.79425192e-01 -6.40077665e-02
 -1.63525432e-01 -2.00810373e-01  2.34556552e-02  5.91597483e-02
 -2.47935295e-01  1.60670578e-01 -1.48821265e-01  5.37601829e-01
 -2.26060122e-01 -1.18696824e-01  2.73483008e-01  3.13931853e-01
 -1.76197976e-01 -3.06548327e-01 -3.27001721e-01 -8.35363939e-02
  2.13267177e-01  3.23416471e-01 -2.36209333e-01 -8.16893727e-02
  8.17736834e-02  8.22544172e-02  6.75816298e-01 -7.20219836e-02
  1.31053224e-01  2.35611826e-01  2.69619972e-01  3.46457541e-01
 -1.95632011e-01 -2.83765078e-01 -2.25230455e-01 -1.78674936e-01
  7.48842731e-02  1.42109528e-01  1.69575125e-01 -2.44717091e-01
  1.59959905e-02 -3.54896665e-01  6.45718426e-02 -2.37883225e-01
  3.67163420e-02 -4.34068978e-01  4.19509690e-03 -1.82108134e-01
 -6.02096558e-01 -2.15897545e-01 -4.21585441e-01 -4.66927439e-02
  9.94533449e-02 -1.46151781e-01 -4.93569206e-03  3.73644471e-01
  1.41745269e-01  7.89389014e-04  8.94337818e-02 -8.35278779e-02
  9.04955640e-02  1.28786549e-01  3.68385948e-03 -1.28676370e-01
  5.37930429e-01 -5.29318154e-01  1.31028444e-01 -5.21951877e-02
 -3.31835121e-01  1.73283204e-01  4.47864383e-02  1.69350848e-01
  1.40961200e-01  1.64766416e-01  2.20571429e-01 -6.77600130e-02
 -3.48438323e-03 -2.54738897e-01 -2.33497657e-02 -3.16273898e-01
  8.48243907e-02 -7.35527202e-02  3.15292478e-01  3.17608953e-01
  4.56851482e-01 -3.87190968e-01  4.16872203e-01  3.04047585e-01
  1.27579663e-02  4.10138592e-02  4.63551760e-01  8.72504711e-03
  1.79222524e-01  9.50589031e-02 -9.39475447e-02 -2.74121284e-01
  5.61249442e-02 -3.01689535e-01  1.04026064e-01  6.75244406e-02
 -1.90885454e-01 -3.44412252e-02  1.40414253e-01 -1.49174318e-01
  9.64708403e-02 -3.30426455e-01  6.20311126e-02 -3.66386659e-02
  3.35545510e-01 -1.89918980e-01 -1.02540463e-01  3.33011039e-02
 -6.84754014e-01  1.94777139e-02 -6.48052841e-02  9.37440574e-01
 -8.66779834e-02 -6.35186769e-03  1.80991828e-01 -9.47261229e-03
  5.13215899e-01  1.49623632e-01 -2.90832072e-01 -4.61600870e-02
 -4.05593142e-02  7.47440010e-02 -1.51094690e-01  1.79277509e-01
  3.15473557e-01  3.13826978e-01  1.49763236e-02  3.19872439e-01
 -7.81340301e-02 -3.81221890e-01  1.87198818e-02 -3.39209527e-01
 -1.43139213e-01  2.64283359e-01 -4.71029505e-02 -8.76136661e-01
  5.87983057e-02  1.14744063e-02 -1.26740456e-01  3.93511921e-01
 -1.65794045e-01  1.24597289e-01 -1.05152003e-01  2.85083711e-01
 -1.55890197e-01  2.57897139e-01  9.53171030e-02  1.29745647e-01
  5.07323503e-01 -4.06332836e-02 -2.57746220e-01 -5.97377181e-01
 -2.39382274e-02  4.91493344e-01 -2.97747076e-01 -8.57974514e-02
 -4.56165411e-02  1.10688508e-02 -4.28228170e-01 -4.57967460e-01
  6.03928044e-02  4.50886965e-01 -1.58381581e-01 -1.56710759e-01
  1.59853980e-01  1.29685104e-01  8.05106014e-02 -3.70572358e-02
  5.63297033e-01 -7.09502697e-01 -8.69844705e-02  5.75061440e-01
  1.45751953e-01  1.17883489e-01  2.71682322e-01  1.53260201e-01
  3.03059757e-01  5.20280972e-02  2.24810932e-02  1.15745723e-01
 -1.86379910e-01  7.77657628e-02 -4.10231590e-01 -4.40855026e-02
  4.84952748e-01 -2.29493111e-01 -1.52882174e-01  2.83994615e-01
  1.95730910e-01 -7.02952296e-02 -1.07012250e-01 -1.84144393e-01
 -1.45923585e-01 -1.15703352e-01 -3.09736520e-01  5.78625835e-02
 -7.85240531e-02 -2.50795960e-01 -1.33274838e-01 -2.64221609e-01
 -4.88303691e-01 -1.34135634e-01  2.18418483e-02 -5.14581919e-01
  1.98626846e-01  3.44836377e-02 -4.75416541e-01  3.12083662e-01
  2.77238429e-01  1.34305373e-01 -2.92888552e-01  2.95056701e-01
  2.18039960e-01 -1.72959507e-01 -1.04678553e-02 -3.49442810e-01
 -1.72612131e-01  7.89904594e-03 -2.76978016e-01  3.79492640e-02
  6.63389862e-02  2.27825955e-01 -7.81085342e-03 -7.95074403e-02
  4.43586648e-01  2.23274618e-01  4.67017978e-01  5.04950806e-02
 -1.01021469e-01 -1.01861268e-01 -2.66717672e-01  1.02555901e-01
 -3.22740912e-01 -1.50800765e-01  4.83853184e-02  6.10718690e-02
  1.10882483e-01  4.76280689e-01 -3.61362994e-01 -2.11864069e-01
 -4.17425215e-01  1.60674557e-01 -1.22517459e-02  2.74501055e-01
  4.86267179e-01  3.27681750e-01  4.96255577e-01  3.28683943e-01
  2.17728354e-02  2.88496435e-01  3.33158731e-01 -3.78878452e-02
  5.64717233e-01  2.07307160e-01 -5.17820865e-02  1.82893366e-01
  3.30292284e-01  2.46797636e-01 -6.27681732e-01  5.17740846e-01
  2.77080178e-01 -2.38521010e-01  4.87993285e-02 -3.84947062e-01
  5.96564531e-01 -4.97124672e-01 -2.27143526e-01  8.89400169e-02
  4.01449621e-01  5.41437604e-02 -3.10011813e-03 -5.60695231e-02
  1.34581804e-01  2.81433463e-01 -2.60598481e-01 -1.63941026e-01
  1.00152560e-01 -1.66836008e-01  1.13169938e-01 -8.02610815e-01
 -4.26921606e-01  1.99728161e-01 -2.33548880e-01  2.43758917e-01
 -1.28549725e-01  1.91251725e-01 -3.58129829e-01 -6.05284572e-02
  1.54428750e-01 -1.23510640e-02  3.53521228e-01  4.02806312e-01
 -2.02174321e-01 -5.87141216e-02  3.64050686e-01 -7.52315104e-01
 -1.47417367e-01 -1.47260323e-01  1.95993543e-01  3.26530546e-01
  5.08093357e-01 -5.65725982e-01  1.89002126e-01 -1.80593550e-01
 -2.47977115e-02  4.49402213e-01 -3.41080129e-04  1.07709825e-01
 -3.10327351e-01  6.16698265e-01  3.53723198e-01 -2.61611938e-01
  2.75214672e-01 -1.43751800e-01 -2.48173118e-01 -1.53280258e-01
  3.64185274e-01 -3.22053969e-01 -2.04938114e-01 -5.13728380e-01
 -1.14558041e-02  1.75532341e-01 -1.49121001e-01  2.18390435e-01
  4.34391089e-02  2.92965978e-01 -3.25992137e-01  8.29451233e-02
 -5.00023007e-01  2.25828379e-01  2.92101391e-02 -4.28282499e-01
 -4.66236770e-02 -1.07919782e-01  1.75862551e-01 -2.12628797e-01
  1.67913273e-01 -3.02994609e-01  5.91852427e-01  5.68859816e-01
 -2.57050097e-01  1.42176270e-01 -8.24919119e-02  3.16670299e-01
 -5.85687816e-01 -2.73014121e-02  1.74341276e-01  1.82325706e-01
  2.77002215e-01 -2.07555369e-01  6.21804714e-01  1.65088028e-01
 -1.07381493e-01  4.17055309e-01 -3.04531455e-01  5.49248829e-02
  1.95214346e-01 -1.64848804e-01 -2.52000272e-01 -1.92422122e-01
 -6.00454137e-02  4.95155424e-01 -2.59879827e-01  2.73625821e-01
 -4.51203912e-01  2.76371896e-01  6.10060215e-01 -4.62887704e-01
 -5.17213583e-01  2.17437163e-01  1.85549557e-01 -1.52742416e-01
  6.01888411e-02 -6.76392764e-02  2.18578994e-01 -1.17061853e-01]"
Tensorflow requires Python<3.8 for Docker stat:awaiting response type:build/install stale subtype: ubuntu/linux TF2.14,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.18-slim-bullseye

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I tried create a container which based on python:3.9.18-slim-bullseye by docker compose, tensorflow didn't let the build complete as it required python >2.6 and python<3.8. 


### Standalone code to reproduce the issue

```shell
FROM python:3.9.18-slim-bullseye

RUN pip install --upgrade pip && pip install pip-tools

COPY ./requirements.txt .

RUN pip install -r requirements.txt

WORKDIR /app

## add app
COPY . /app

RUN apt-get update && apt-get install -y netcat

ADD https://github.com/krallin/tini/releases/download/v0.19.0/tini /tini
RUN chmod +x /tini
# Set Tini as the ENTRYPOINT and specify Uvicorn CMD
ENTRYPOINT [""/tini"", ""--""]
CMD [""uvicorn"", ""main:app"", ""--host"", ""0.0.0.0"", ""--port"", ""30022""]



requirements.txt

tensorboard==2.14.0
tensorboard-data-server==0.7.2
tensorflow==v2.15.0-rc0
tensorflow-estimator==v2.15.0-rc0
tensorflow-intel==v2.15.0-rc0
tensorflow-io-gcs-filesystem==0.31.0
```


### Relevant log output

```shell
ERROR: Ignored the following versions that require a different python version: 0.2.1 Requires-Python >2.6, !=3.3.*, <3.8
ERROR: Could not find a version that satisfies the requirement tensorflow-intel==2.14.0 (from versions: 0.0.1)
ERROR: No matching distribution found for tensorflow-intel==2.14.0
```
",False,"[-0.3721611  -0.44180197 -0.0623802   0.22331503  0.37979856 -0.47996733
 -0.20550334 -0.04314189 -0.2668672  -0.38798583 -0.17730457  0.20582293
 -0.14683607  0.21487726 -0.25748444  0.3102541  -0.25975096 -0.24439701
  0.27584454  0.42751113 -0.14998505  0.03386673 -0.21034299  0.1349721
  0.18930721  0.1555634  -0.39221454  0.01945004  0.06705867  0.16167222
  0.5412489   0.2882323  -0.06207556  0.05537146  0.05460434  0.1779998
 -0.23864621 -0.03762674 -0.4189701   0.05509487  0.21443689  0.17454982
  0.09179203  0.01933885 -0.13005486 -0.30784333  0.01316451 -0.30561864
  0.00253419 -0.14943999 -0.24931836 -0.01228157 -0.2448133  -0.4363634
 -0.14373547 -0.14741752  0.10091437  0.09711598 -0.04906385  0.2370469
 -0.04095276  0.06535535  0.1687068  -0.0305769   0.1894533   0.40240723
  0.08739515 -0.14673242  0.46370497 -0.21323414  0.13261732 -0.20597416
 -0.37855303  0.01215186 -0.08035652  0.0163238   0.03151059  0.0535861
  0.20782688 -0.01707282 -0.20656927 -0.29499596 -0.07912161 -0.07697905
  0.113097   -0.07371931  0.2845964   0.22194472  0.33463654 -0.28104913
  0.50718576  0.51202106 -0.06113914  0.06196292  0.51752305  0.11981212
  0.2765664   0.31237277 -0.18026136 -0.1303902  -0.02358237 -0.22137469
  0.05642534  0.00531535 -0.11136679 -0.18646775  0.18746316 -0.26345918
  0.05070601 -0.06949173  0.373163    0.01723687  0.3115862   0.01949618
 -0.17198853 -0.19077834 -0.49952045  0.03775933  0.02976632  0.775526
 -0.17816865  0.04785589 -0.02250647  0.13873237  0.23230232  0.14104822
 -0.13665833 -0.11357279  0.0845305  -0.04566175  0.05603167  0.03060978
 -0.06901363  0.22388305  0.11051486 -0.01139432 -0.02661884 -0.24970828
 -0.28615862 -0.138636   -0.12595533  0.07537787 -0.011858   -0.5476531
 -0.03249588  0.19189727 -0.1910554   0.09430534 -0.25876164  0.35236037
 -0.1320492   0.07549702 -0.13639872  0.34083053  0.16351601  0.12027179
  0.31298894 -0.05296085 -0.07442937 -0.45049986 -0.18792236  0.59537166
 -0.08489988 -0.20210215 -0.03933896  0.15237439 -0.43182445 -0.37984473
  0.08827725  0.3849147  -0.15931588 -0.14121613  0.04743144 -0.00769736
  0.00815886 -0.04196772  0.28742117 -0.5957634   0.10752907  0.29099798
  0.13640335  0.32426453  0.00431151  0.08856142  0.12127782  0.09782122
 -0.00623285  0.0459147  -0.26031724 -0.1349486  -0.17928398 -0.13219345
  0.5646138  -0.1079868  -0.2516511   0.29719478  0.30470437  0.01745944
  0.21080437 -0.06955008 -0.07152551  0.09795561  0.0390061  -0.01310427
  0.06099744 -0.31771684  0.00667376 -0.25272995 -0.24694306  0.00926507
 -0.00436368 -0.36214352  0.07155206 -0.25558484 -0.22651972  0.3112291
  0.11436021  0.05533282 -0.10964112  0.12354541  0.07399    -0.18147063
 -0.17275158 -0.33284014 -0.18997401  0.06205667 -0.32435194  0.04984425
 -0.07356346  0.2225676   0.16214654  0.10482204  0.25894755  0.06926964
  0.33596218 -0.12177006  0.1132695  -0.06468569 -0.22486794  0.33279818
 -0.31372452 -0.18048373 -0.0065168  -0.0181285   0.26398835  0.14911988
 -0.07715116 -0.04979312 -0.41484445  0.08567934  0.02310861  0.03538503
  0.2975083   0.16491134  0.569433    0.31504643 -0.02311515  0.26559597
  0.14972252 -0.21323453  0.3870595   0.10837683 -0.04979514  0.30712122
  0.3090358   0.07504269 -0.32494658  0.660154    0.23936239 -0.20251612
  0.12661827 -0.21472944  0.5592616  -0.44671872 -0.13072723  0.08572107
  0.4967891   0.05241889 -0.00711255 -0.06911419  0.08471437  0.35458952
 -0.40332848  0.06768174  0.17422709 -0.17347182 -0.04597455 -0.52163666
 -0.32339454  0.3461964  -0.37447354  0.04366634 -0.37716648  0.20970304
 -0.30422688 -0.14669448  0.17140698  0.009989    0.23015332  0.3568164
 -0.21860757  0.002398    0.34842697 -0.40318033 -0.21526451 -0.04189378
  0.10181776  0.26708412  0.33565205 -0.42074108  0.23868856 -0.05216892
 -0.0743153   0.35960042  0.23262446 -0.15788823 -0.59510815  0.820762
  0.33822066 -0.27547228  0.13602182 -0.11713865 -0.33750075  0.07434449
  0.2985513   0.01511503 -0.0748934  -0.43217498 -0.13911559 -0.00311423
 -0.02797199 -0.13749456 -0.1817248   0.12303834 -0.03667577 -0.11924808
 -0.2874566   0.04373424 -0.04559749 -0.49802762  0.0639747  -0.15384668
  0.21616225 -0.33521235 -0.12617704 -0.20592555  0.36182883  0.49263573
 -0.17004648  0.00881006  0.11359186  0.19279394 -0.56690943 -0.06332276
 -0.17276356  0.34790862  0.05391076 -0.20177042  0.48840404  0.2855351
 -0.25257286  0.08896959 -0.42700744 -0.00754448  0.28266776 -0.11786964
 -0.30720472  0.04622778  0.03911492  0.40869197 -0.07694485  0.16995029
 -0.15964615  0.24310927  0.38234457 -0.2250697  -0.4253682   0.22043598
  0.00115404 -0.27543345  0.05602606  0.07657897  0.2106352   0.00773234]"
how to self-hosting docs of old version type:support TF 1.12 type:docs-feature,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

1.12

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

need to host old version doc as a website

### Standalone code to reproduce the issue

```shell
no code
```


### Relevant log output

_No response_",False,"[-4.64916766e-01 -1.54442787e-01  5.56759052e-02  4.81326580e-02
  3.94619882e-01 -2.69552767e-01 -3.84500027e-01 -1.32422112e-02
 -2.90399432e-01  1.74093351e-01  2.45759398e-01 -1.12572797e-02
 -1.89444631e-01  1.99104205e-01 -1.32454142e-01  2.58294314e-01
 -2.83104360e-01 -9.53064263e-02  4.87000644e-01  4.20680344e-01
 -3.89483392e-01  1.42671853e-01 -2.84968376e-01  2.35534027e-01
  1.26729548e-01 -6.44363612e-02 -3.61379564e-01 -1.19554728e-01
  1.71404444e-02  1.48490727e-01  4.77396429e-01  1.12851769e-01
  4.01967466e-02  2.15778381e-01  4.03645992e-01  3.06936979e-01
 -4.82236519e-02 -1.18854955e-01 -4.90794480e-01 -9.89973396e-02
 -2.00420860e-02  1.80823114e-02  1.54732630e-01 -1.18577234e-01
  1.32944137e-01 -3.08008999e-01 -9.00030434e-02 -1.64785802e-01
  2.27027759e-03 -1.39681250e-01  7.77464360e-04 -1.75995559e-01
 -5.48639655e-01 -7.25317672e-02 -1.08233869e-01 -6.47082776e-02
 -3.89873087e-02  3.42405319e-01  1.29802525e-01  3.44068348e-01
  1.73515007e-01 -3.22409086e-02 -3.54396924e-02 -1.72702163e-01
  7.24991709e-02  1.89626738e-01  2.00781971e-01  2.93489303e-02
  4.49711651e-01 -2.85696775e-01 -1.56543136e-01 -1.05271012e-01
 -3.63647223e-01  2.10531950e-02 -5.49665000e-03 -3.31861861e-02
  5.24994954e-02 -4.78247181e-02  1.26252353e-01 -1.29913211e-01
  7.50957876e-02 -2.48713046e-01 -1.47345394e-01 -2.54122585e-01
 -1.25302315e-01 -3.14207941e-01  4.15873796e-01  1.20828263e-01
  5.29334307e-01 -3.43729377e-01  2.86288708e-01  4.26130295e-01
  8.74693692e-02  1.44140244e-01  1.56595618e-01 -1.79082796e-01
  2.00669244e-01  1.49405062e-01  1.42114937e-01  1.89985454e-01
  2.98753176e-02 -3.37123066e-01  1.09629743e-01 -2.51869243e-02
 -1.21067747e-01 -2.32528567e-01  3.85565937e-01  1.00058056e-02
 -4.69344184e-02  3.14488150e-02 -8.14089365e-03  9.76191834e-02
  1.03607245e-01 -3.53218079e-01  4.34821770e-02 -7.38492012e-02
 -1.81709543e-01 -7.02121034e-02  3.84473726e-02  4.46593285e-01
  1.92907006e-01 -2.42903039e-01  1.68070674e-01  1.04732715e-01
  4.58965123e-01  2.61384904e-01 -1.72963753e-01  6.45242445e-03
  5.22111654e-02 -6.19894937e-02 -1.96568444e-02  1.56809360e-01
  2.41400540e-01  3.49630266e-01  4.79197316e-03  2.58132875e-01
 -2.78086573e-01 -2.92409301e-01 -2.23987341e-01 -1.03554651e-01
 -3.84209663e-01 -2.57152393e-02 -1.97504386e-01 -6.59653425e-01
 -2.79753599e-02  1.66846231e-01 -4.18172628e-02  3.52756768e-01
 -3.25748771e-01  2.66052157e-01  1.12310350e-01 -4.98005450e-02
  1.45843655e-01  2.56301075e-01  2.19268054e-01 -4.41105030e-02
  4.70623791e-01 -2.04421595e-01 -1.37641691e-02 -6.43120289e-01
 -1.48768052e-01  1.37212351e-01 -9.91666317e-02 -1.62473604e-01
 -1.13930598e-01 -9.06942692e-03 -5.39991558e-01 -2.23009437e-01
 -1.02105737e-01  5.22877574e-01 -4.52979691e-02 -1.41013250e-01
  1.20358132e-01  4.23755497e-01  1.26789376e-01 -8.16607699e-02
  4.72808301e-01 -5.28183043e-01 -1.56517670e-01  3.90868455e-01
 -2.37699956e-01  1.47946566e-01  8.56518894e-02  8.19298625e-02
  5.78112118e-02  7.15760887e-02  2.54981399e-01  5.89614436e-02
  9.79660451e-02 -6.68293163e-02 -3.60277027e-01 -2.43211240e-01
  2.98225641e-01 -1.94545567e-01 -9.25631970e-02  3.40845823e-01
  1.63518071e-01 -6.66637346e-02 -2.23912243e-02 -1.77798029e-02
 -4.22867775e-01 -1.38509095e-01 -4.08692658e-02  1.52834892e-01
  2.64732361e-01 -2.06764504e-01 -1.37219042e-01 -3.03511798e-01
 -3.88499677e-01 -1.15825772e-01  6.07934594e-02 -2.60295331e-01
 -6.56648576e-02 -1.10845685e-01 -3.74322474e-01  4.34813380e-01
  2.82636106e-01  6.92611933e-02 -1.68663293e-01  2.50376731e-01
  1.58929139e-01 -2.91903019e-01  1.77665018e-02 -5.02763987e-01
 -1.13013849e-01  4.15754318e-02 -5.91511488e-01  1.71638280e-01
 -1.30378976e-01  1.26001865e-01 -1.95805326e-01  6.40931726e-02
  4.60561097e-01  3.83626074e-02  3.93950790e-01 -1.12274066e-01
 -8.36332813e-02 -2.22974867e-01 -3.25857729e-01  3.69790196e-02
 -4.87073272e-01 -1.85088158e-01 -1.55466229e-01  1.13756470e-01
  1.71954319e-01  6.57851875e-01 -3.83746326e-01  1.64495744e-02
 -2.75181353e-01  2.86949605e-01 -1.99745983e-01  4.72591758e-01
  3.52257073e-01  5.25368080e-02  5.34175992e-01 -8.09319466e-02
  2.20435873e-01  2.41832078e-01  2.61631846e-01 -5.51454797e-02
  2.54549712e-01  1.59968168e-01 -4.12009582e-02  4.70769584e-01
  3.49383026e-01  4.30488169e-01 -3.88942033e-01  5.83723187e-01
 -5.05157709e-02 -1.06450781e-01  2.80339923e-02 -4.09753889e-01
  8.21683168e-01 -1.17576569e-01 -2.35053733e-01 -1.01861596e-01
  4.16933239e-01 -8.90156925e-02  8.86193365e-02  6.21325150e-03
 -1.07081383e-01  5.01852930e-01 -4.96096194e-01 -1.06146736e-02
 -1.10451147e-01 -2.89412498e-01 -9.17261392e-02 -6.43853784e-01
 -2.97025889e-01  5.71128204e-02 -3.14199805e-01  2.17117667e-01
 -1.09577626e-01 -9.30139720e-02 -1.32668748e-01 -1.56735063e-01
 -8.36270750e-02 -1.29368782e-01  2.44201645e-01  2.21096575e-01
  3.42240632e-02  7.56381527e-02  3.77783477e-01 -3.91339481e-01
  6.70061037e-02 -2.79749222e-02  2.83599734e-01  2.16411799e-01
  4.73248690e-01 -3.92966509e-01  2.62420714e-01 -2.28374109e-01
 -1.50447816e-01  5.01154542e-01  1.19922921e-01  1.91447958e-02
 -5.29981971e-01  5.54545343e-01  3.17860804e-02 -2.09159985e-01
  1.57464474e-01 -1.09047771e-01 -3.54617923e-01  1.24200240e-01
  1.83773577e-01 -1.89934641e-01  1.60356283e-01 -3.55079174e-01
  1.72359750e-01  1.53496429e-01 -1.89986914e-01 -4.53626588e-02
 -1.35819465e-01 -1.76239312e-01 -2.15487763e-01 -3.95821407e-04
 -3.32265794e-01  1.71208948e-01 -4.88546602e-02 -1.55398220e-01
 -3.09356689e-01 -2.04476833e-01 -4.19923961e-02 -2.06755817e-01
  5.63553125e-02 -2.02752918e-01  3.26451272e-01  3.55953932e-01
 -6.33193105e-02 -1.77257136e-02  4.92831133e-02  8.57944489e-02
 -6.18079185e-01 -5.38787208e-02  1.55281484e-01  4.87058550e-01
 -1.19178787e-01  4.96700406e-02  3.39789867e-01  4.01382864e-01
 -8.93560238e-03  1.62771404e-01 -1.54624358e-01 -9.90881324e-02
  3.19121443e-02 -1.54626042e-01 -8.95103738e-02 -1.54863268e-01
  1.50768250e-01  2.93417513e-01  2.13368908e-02  2.25924522e-01
 -6.67330548e-02  3.42467487e-01  3.90806079e-01 -3.13763559e-01
 -4.83062752e-02  1.73819631e-01  2.25264370e-01 -1.45706803e-01
  1.78220421e-01 -2.05028236e-01  3.21191132e-01  6.40847459e-02]"
Error when building from source: AddKernelActivityEvent<TF_CUPTI_HAS_CHANNEL_ID> TF_CUPTI_HAS_CHANNEL_ID was not declared in this scope type:build/install,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

6.1.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

Cuda=11.4.152, cuDNN=8.9.5

### GPU model and memory

GeForce RTX 2070 Rev. A

### Current behavior?

I am trying to install tensorflow from source. I am following this [tutorial](https://www.tensorflow.org/install/source). I have the cuDNN and the Cuda Toolkit installed, and the following configuration: 
```
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 

Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.8/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: N
No TensorRT support will be enabled for TensorFlow.

Found CUDA 11.4 in:
    /usr/local/cuda-11.4/targets/x86_64-linux/lib
    /usr/local/cuda-11.4/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/local/cuda-11.4/targets/x86_64-linux/lib
    /usr/local/cuda-11.4/targets/x86_64-linux/include

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 7.5]: 

Do you want to use clang as CUDA compiler? [Y/n]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.
```

Then doing: `bazel build --config=opt --action_env=PATH -c opt //tensorflow/tools/pip_package:build_pip_package`

```
ERROR: /home/master/.cache/bazel/_bazel_master/7d15ddcbf9badca106464f95f49bc497/external/local_xla/xla/backends/profiler/gpu/BUILD:172:16: Compiling xla/backends/profiler/gpu/cupti_tracer.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target @local_xla//xla/backends/profiler/gpu:cupti_tracer) external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/external/local_xla/xla/backends/profiler/gpu/_objs/cupti_tracer/cupti_tracer.pic.d ... (remaining 180 arguments skipped)
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc: In member function tsl::Status xla::profiler::CuptiTracer::ProcessActivityBuffer(CUcontext, uint32_t, uint8_t*, size_t):
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:2177:34: error: TF_CUPTI_HAS_CHANNEL_ID was not declared in this scope
 2177 |           AddKernelActivityEvent<TF_CUPTI_HAS_CHANNEL_ID>(
      |                                  ^~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:2178:76: error: no matching function for call to AddKernelActivityEvent<<expression error> >(xla::profiler::CuptiTraceCollector*&, xla::profiler::{anonymous}::CuptiActivityKernelTy*)
 2178 |               collector_, reinterpret_cast<CuptiActivityKernelTy *>(record));
      |                                                                            ^
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:747:6: note: candidate: template<bool cupti_has_channel_id, class CuptiActivityKernel> void xla::profiler::{anonymous}::AddKernelActivityEvent(xla::profiler::CuptiTraceCollector*, const CuptiActivityKernel*)
  747 | void AddKernelActivityEvent(CuptiTraceCollector *collector,
      |      ^~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:747:6: note:   template argument deduction/substitution failed:
external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:2178:76: error: template argument 1 is invalid
 2178 |               collector_, reinterpret_cast<CuptiActivityKernelTy *>(record));
      |                                                                            ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2354.699s, Critical Path: 306.56s
INFO: 11360 processes: 84 internal, 11276 local.
FAILED: Build did NOT complete successfully

```

### Standalone code to reproduce the issue

```shell
bazel build --config=opt --action_env=PATH -c opt //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/master/Downloads/tensorflow/tensorflow/core/kernels/image/BUILD:325:18: Compiling tensorflow/core/kernels/image/image_ops_gpu.cu.cc failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command (from target //tensorflow/core/kernels/image:image_ops_gpu) 
  (cd /home/master/.cache/bazel/_bazel_master/7d15ddcbf9badca106464f95f49bc497/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.4 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 \
    LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64:/home/master/TensorRT-8.6.1.6/lib:/usr/local/cuda-11.4/lib64:/home/master/TensorRT-8.6.1.6/lib \
    PATH=/home/master/.cache/bazelisk/downloads/sha256/6c25a6d716545d6b672ec46f770521cd9ebb63d73617b8f4e6747825d1db1839/bin:/usr/local/cuda-11.4/bin:/home/master/.local/bin:/usr/local/cuda-11.4/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=7.5 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/image_ops_gpu/image_ops_gpu.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/image_ops_gpu/image_ops_gpu.cu.pic.o' '-DEIGEN_MAX_ALIGN_BYTES=64' -DEIGEN_ALLOW_UNALIGNED_SCALARS '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL '-DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0' '-DEIGEN_NEON_GEBP_NR=4' '-DBAZEL_CURRENT_REPOSITORY=""""' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/local_tsl -iquote bazel-out/k8-opt/bin/external/local_tsl -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/ml_dtypes -iquote bazel-out/k8-opt/bin/external/ml_dtypes -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/snappy -iquote bazel-out/k8-opt/bin/external/snappy -iquote external/double_conversion -iquote bazel-out/k8-opt/bin/external/double_conversion -iquote external/nccl_archive -iquote bazel-out/k8-opt/bin/external/nccl_archive -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/png -iquote bazel-out/k8-opt/bin/external/png -iquote external/onednn -iquote bazel-out/k8-opt/bin/external/onednn -iquote external/local_xla -iquote bazel-out/k8-opt/bin/external/local_xla -Ibazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8 -Ibazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/int4 -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/nccl_config -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/eigen_archive/mkl_include -isystem bazel-out/k8-opt/bin/external/eigen_archive/mkl_include -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/ml_dtypes -isystem bazel-out/k8-opt/bin/external/ml_dtypes -isystem external/ml_dtypes/ml_dtypes -isystem bazel-out/k8-opt/bin/external/ml_dtypes/ml_dtypes -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/png -isystem bazel-out/k8-opt/bin/external/png -isystem external/onednn/include -isystem bazel-out/k8-opt/bin/external/onednn/include -isystem external/onednn/src -isystem bazel-out/k8-opt/bin/external/onednn/src -isystem external/onednn/src/common -isystem bazel-out/k8-opt/bin/external/onednn/src/common -isystem external/onednn/src/common/ittnotify -isystem bazel-out/k8-opt/bin/external/onednn/src/common/ittnotify -isystem external/onednn/src/cpu -isystem bazel-out/k8-opt/bin/external/onednn/src/cpu -isystem external/onednn/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/onednn/src/cpu/gemm -isystem external/onednn/src/cpu/x64/xbyak -isystem bazel-out/k8-opt/bin/external/onednn/src/cpu/x64/xbyak -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++17' -x cuda '-DGOOGLE_CUDA=1' '--cuda-include-ptx=sm_75' '--cuda-gpu-arch=sm_75' '-Xcuda-fatbinary=--compress-all' '-nvcc_options=expt-relaxed-constexpr' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -DENABLE_ONEDNN_V3 -DAMD_ZENDNN -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/image/image_ops_gpu.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/image_ops_gpu/image_ops_gpu.cu.pic.o)
# Configuration: a0763b48cf05909e46ed8ba9df7ecc2eca8eed6449f35b44f6b17dbe52c77dbe
# Execution platform: @local_execution_config_platform//:platform
bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e4m3b11fnuz, To=ml_dtypes::float8_internal::float8_e4m3fn, kSaturate=false, kTruncate=false]"" 
(1255): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::ConvertFrom(const From &) [with Derived=ml_dtypes::float8_internal::float8_e4m3fn, kSaturate=false, kTruncate=false, From=ml_dtypes::float8_internal::float8_e4m3b11fnuz]"" 
(258): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e4m3b11fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(1261): here
            instantiation of ""To ml_dtypes::float8_internal::float8_base<Derived>::ConvertTo<To,kSaturate,kTruncate>(const Derived &) [with Derived=ml_dtypes::float8_internal::float8_e4m3b11fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(87): here
            instantiation of ""ml_dtypes::float8_internal::float8_base<Derived>::operator float() const [with Derived=ml_dtypes::float8_internal::float8_e4m3b11fnuz]"" 
(128): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::operator-(const Derived &) const [with Derived=ml_dtypes::float8_internal::float8_e4m3b11fnuz]"" 
(287): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e4m3fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(1261): here
            instantiation of ""To ml_dtypes::float8_internal::float8_base<Derived>::ConvertTo<To,kSaturate,kTruncate>(const Derived &) [with Derived=ml_dtypes::float8_internal::float8_e4m3fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(87): here
            instantiation of ""ml_dtypes::float8_internal::float8_base<Derived>::operator float() const [with Derived=ml_dtypes::float8_internal::float8_e4m3fnuz]"" 
(128): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::operator-(const Derived &) const [with Derived=ml_dtypes::float8_internal::float8_e4m3fnuz]"" 
(335): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e4m3b11fnuz, To=ml_dtypes::float8_internal::float8_e4m3fnuz, kSaturate=false, kTruncate=false]"" 
(1255): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::ConvertFrom(const From &) [with Derived=ml_dtypes::float8_internal::float8_e4m3fnuz, kSaturate=false, kTruncate=false, From=ml_dtypes::float8_internal::float8_e4m3b11fnuz]"" 
(323): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e5m2fnuz, To=ml_dtypes::float8_internal::float8_e5m2, kSaturate=false, kTruncate=false]"" 
(1255): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::ConvertFrom(const From &) [with Derived=ml_dtypes::float8_internal::float8_e5m2, kSaturate=false, kTruncate=false, From=ml_dtypes::float8_internal::float8_e5m2fnuz]"" 
(357): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e5m2fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(1261): here
            instantiation of ""To ml_dtypes::float8_internal::float8_base<Derived>::ConvertTo<To,kSaturate,kTruncate>(const Derived &) [with Derived=ml_dtypes::float8_internal::float8_e5m2fnuz, To=float, kSaturate=false, kTruncate=false]"" 
(87): here
            instantiation of ""ml_dtypes::float8_internal::float8_base<Derived>::operator float() const [with Derived=ml_dtypes::float8_internal::float8_e5m2fnuz]"" 
(128): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::operator-(const Derived &) const [with Derived=ml_dtypes::float8_internal::float8_e5m2fnuz]"" 
(399): here

bazel-out/k8-opt/bin/external/ml_dtypes/_virtual_includes/float8/ml_dtypes/include/float8.h(1164): warning: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To ml_dtypes::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=ml_dtypes::float8_internal::float8_e5m2, To=ml_dtypes::float8_internal::float8_e5m2fnuz, kSaturate=false, kTruncate=false]"" 
(1255): here
            instantiation of ""Derived ml_dtypes::float8_internal::float8_base<Derived>::ConvertFrom(const From &) [with Derived=ml_dtypes::float8_internal::float8_e5m2fnuz, kSaturate=false, kTruncate=false, From=ml_dtypes::float8_internal::float8_e5m2]"" 
(383): here

external/com_google_absl/absl/status/internal/statusor_internal.h(252): warning: ignoring return value type with ""nodiscard"" attribute

external/com_google_absl/absl/status/internal/statusor_internal.h(259): warning: ignoring return value type with ""nodiscard"" attribute

external/com_google_absl/absl/strings/internal/str_format/bind.h: In constructor absl::lts_20230125::str_format_internal::FormatSpecTemplate<Args>::FormatSpecTemplate(const absl::lts_20230125::str_format_internal::ExtendedParsedFormat<absl::lts_20230125::FormatConversionCharSet(C)...>&):
external/com_google_absl/absl/strings/internal/str_format/bind.h:171:1: error: parse error in template argument list
  171 |     CheckArity<sizeof...(C), sizeof...(Args)>();
      | ^   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~              
external/com_google_absl/absl/strings/internal/str_format/bind.h:171:63: error: expected ; before ) token
  171 |     CheckArity<sizeof...(C), sizeof...(Args)>();
      |                                                               ^
      |                                                               ;
external/com_google_absl/absl/strings/internal/str_format/bind.h:172:147: error: template argument 1 is invalid
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                   ^
external/com_google_absl/absl/strings/internal/str_format/bind.h:172:151: error: expected primary-expression before { token
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                       ^
external/com_google_absl/absl/strings/internal/str_format/bind.h:172:150: error: expected ; before { token
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                      ^ 
      |                                                                                                                                                      ;
external/com_google_absl/absl/strings/internal/str_format/bind.h:172:153: error: expected primary-expression before ) token
  172 |     CheckMatches<C...>(absl::make_index_sequence<sizeof...(C)>{});
      |                                                                                                                                                         ^
external/com_google_absl/absl/strings/internal/str_format/arg.h: In instantiation of constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ArgumentToConv() [with Arg = long int]:
external/com_google_absl/absl/strings/str_format.h:268:156:   required by substitution of template<class ... Args> using FormatSpec = absl::lts_20230125::str_format_internal::FormatSpecTemplate<absl::lts_20230125::FormatConversionCharSet((ArgumentToConv<Args>)())...> [with Args = {long int, const tensorflow::ResourceBase*}]
external/com_google_absl/absl/strings/str_format.h:351:1:   required by substitution of template<class ... Args> std::string absl::lts_20230125::StrFormat(absl::lts_20230125::FormatSpec<Args ...>&, const Args& ...) [with Args = {long int, const tensorflow::ResourceBase*}]
./tensorflow/core/framework/resource_base.h:44:62:   required from here
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: error: no matching function for call to ExtractCharSet(ConvResult)
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note: candidate: template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::FormatConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)
  196 | constexpr FormatConversionCharSet ExtractCharSet(FormatConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnt deduce template parameter C
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note: candidate: template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::str_format_internal::ArgConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)
  201 | constexpr FormatConversionCharSet ExtractCharSet(ArgConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnt deduce template parameter C
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h: In instantiation of constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ArgumentToConv() [with Arg = const tensorflow::ResourceBase*]:
external/com_google_absl/absl/strings/str_format.h:268:156:   required by substitution of template<class ... Args> using FormatSpec = absl::lts_20230125::str_format_internal::FormatSpecTemplate<absl::lts_20230125::FormatConversionCharSet((ArgumentToConv<Args>)())...> [with Args = {long int, const tensorflow::ResourceBase*}]
external/com_google_absl/absl/strings/str_format.h:351:1:   required by substitution of template<class ... Args> std::string absl::lts_20230125::StrFormat(absl::lts_20230125::FormatSpec<Args ...>&, const Args& ...) [with Args = {long int, const tensorflow::ResourceBase*}]
./tensorflow/core/framework/resource_base.h:44:62:   required from here
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: error: no matching function for call to ExtractCharSet(ConvResult)
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note: candidate: template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::FormatConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)
  196 | constexpr FormatConversionCharSet ExtractCharSet(FormatConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnt deduce template parameter C
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note: candidate: template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::str_format_internal::ArgConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)
  201 | constexpr FormatConversionCharSet ExtractCharSet(ArgConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnt deduce template parameter C
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
./tensorflow/core/framework/resource_base.h: In member function virtual std::string tensorflow::ResourceBase::MakeRefCountingHandleName(int64_t) const:
./tensorflow/core/framework/resource_base.h:44:62: error: no matching function for call to StrFormat(const char [18], int64_t&, const tensorflow::ResourceBase*)
   44 |     return absl::StrFormat(""Resource-%d-at-%p"", resource_id, this);
      |                                                              ^
external/com_google_absl/absl/strings/str_format.h:351:1: note: candidate: template<class ... Args> std::string absl::lts_20230125::StrFormat(absl::lts_20230125::FormatSpec<Args ...>&, const Args& ...)
  351 | ABSL_MUST_USE_RESULT std::string StrFormat(const FormatSpec<Args...>& format,
      | ^~~~~~~~~
external/com_google_absl/absl/strings/str_format.h:351:1: note:   substitution of deduced template arguments resulted in errors seen above
external/com_google_absl/absl/strings/internal/str_format/arg.h: In instantiation of constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ArgumentToConv() [with Arg = int]:
external/com_google_absl/absl/strings/internal/str_format/arg.h:570:61:   required from static bool absl::lts_20230125::str_format_internal::FormatArgImpl::Dispatch(absl::lts_20230125::str_format_internal::FormatArgImpl::Data, absl::lts_20230125::str_format_internal::FormatConversionSpecImpl, void*) [with T = int]
external/com_google_absl/absl/strings/internal/str_format/arg.h:524:15:   required from void absl::lts_20230125::str_format_internal::FormatArgImpl::Init(const T&) [with T = int]
external/com_google_absl/absl/strings/internal/str_format/arg.h:474:1:   required from absl::lts_20230125::str_format_internal::FormatArgImpl::FormatArgImpl(const T&) [with T = int]
external/com_google_absl/absl/strings/internal/str_format/bind.h:202:49:   required from here
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: error: no matching function for call to ExtractCharSet(ConvResult)
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note: candidate: template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::FormatConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)
  196 | constexpr FormatConversionCharSet ExtractCharSet(FormatConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:196:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnt deduce template parameter C
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note: candidate: template<absl::lts_20230125::FormatConversionCharSet C> constexpr absl::lts_20230125::FormatConversionCharSet absl::lts_20230125::str_format_internal::ExtractCharSet(absl::lts_20230125::str_format_internal::ArgConvertResult<(absl::lts_20230125::FormatConversionCharSet)(C)>)
  201 | constexpr FormatConversionCharSet ExtractCharSet(ArgConvertResult<C>) {
      | ^~~~~~~~~~~~~~
external/com_google_absl/absl/strings/internal/str_format/arg.h:201:1: note:   template argument deduction/substitution failed:
external/com_google_absl/absl/strings/internal/str_format/arg.h:403:43: note:   couldnt deduce template parameter C
  403 |   return absl::str_format_internal::ExtractCharSet(ConvResult{});
      |        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 35.257s, Critical Path: 32.01s
INFO: 238 processes: 30 internal, 208 local.
FAILED: Build did NOT complete successfully
```
",False,"[-1.92351401e-01 -4.53009844e-01 -8.58886540e-02  1.74715251e-01
  2.58149147e-01 -3.90287220e-01 -3.06196541e-01 -6.68229833e-02
 -3.83491099e-01 -1.28957331e-01 -3.16235125e-02 -2.30051577e-04
 -3.00964773e-01  2.16460064e-01 -4.11447920e-02  4.91458297e-01
 -1.88407555e-01 -1.90006152e-01  3.09599340e-01  1.57104716e-01
  8.19856822e-02 -2.34572351e-01 -2.51149893e-01  2.45659798e-02
  3.10554683e-01  7.62056187e-02 -3.59160602e-02  1.19227618e-01
  7.30497539e-02  2.09518254e-01  3.12578321e-01  1.17375217e-01
 -5.34613654e-02 -2.38945335e-02  1.00508966e-01  2.36269206e-01
 -2.22514540e-01 -2.88794398e-01 -3.25056195e-01 -7.09963357e-03
 -4.49548662e-03  1.07888050e-01  1.49886627e-02 -2.23467350e-02
 -9.61347297e-02 -2.83454120e-01  1.57436114e-02 -9.97244865e-02
 -7.34522790e-02 -2.11346298e-01 -7.61309043e-02  2.33983286e-02
 -4.50595051e-01 -2.01471776e-01 -1.64089248e-01 -1.39240831e-01
 -6.47788644e-02  1.81442648e-01  2.09569603e-01  1.90907970e-01
  3.75400633e-01 -5.85247949e-02 -6.18682876e-02 -7.78635368e-02
  3.44154909e-02  1.57078966e-01 -2.03279257e-02 -4.56952117e-03
  2.83795178e-01 -6.18368566e-01  1.44155130e-01 -1.27796769e-01
 -1.40955463e-01 -3.10001895e-04  1.56045482e-01  4.77265120e-02
  4.73381802e-02  3.03578377e-01  1.27429843e-01 -6.87602162e-02
 -1.74223613e-02 -1.95953220e-01 -1.20333955e-01 -9.45910364e-02
  1.58594817e-01 -9.01128948e-02  1.86636150e-01  4.40810099e-02
  3.31069618e-01 -2.91346014e-01  3.78550231e-01  3.74470115e-01
 -1.25258505e-01  8.82538483e-02  5.05543113e-01 -3.35054174e-02
  1.32092223e-01  8.56531337e-02 -3.15789208e-02 -7.86630139e-02
  1.75301760e-01 -3.93788755e-01 -1.74296603e-01  1.40520856e-01
 -1.50680751e-01 -5.54566681e-02  2.71141052e-01  6.37038574e-02
  9.90698412e-02 -2.19373554e-01  1.20669387e-01  1.26675725e-01
  1.31023049e-01 -2.18928352e-01  6.85134679e-02  1.90572977e-01
 -4.47066724e-01  3.78383026e-02  3.12949754e-02  8.24750721e-01
 -3.07604838e-02 -4.43122275e-02  8.04147795e-02  1.02542520e-01
  2.63422877e-01  1.49019837e-01 -1.17769115e-01 -6.42668083e-02
 -1.75757334e-04  1.36228621e-01 -1.87880341e-02  2.95540571e-01
  8.55435058e-02  1.78263336e-01  1.55396104e-01  8.62435848e-02
 -2.17726126e-01 -2.27602750e-01 -2.39821911e-01 -2.76434958e-01
 -2.61184454e-01  3.95390034e-01 -7.85351992e-02 -6.26281381e-01
  6.55640066e-02  7.99668729e-02 -1.16105720e-01  4.12832320e-01
 -1.28207505e-01  6.18431866e-02 -1.08658776e-01  1.13045640e-01
  1.55606372e-02  5.15574276e-01  1.76537275e-01  1.47745371e-01
  3.41165364e-01 -7.05800802e-02  2.03891750e-03 -5.61344147e-01
 -2.19931342e-02  3.96973610e-01 -1.71900138e-01 -2.33528450e-01
 -4.73179966e-02 -4.71217558e-02 -5.74714720e-01 -3.74558955e-01
  7.36021474e-02  2.30606526e-01 -3.21831495e-01 -1.25031948e-01
  1.06477529e-01  2.13308230e-01 -5.61333671e-02 -1.85549468e-01
  6.61296487e-01 -6.14476919e-01 -1.62750125e-01  4.87247348e-01
  2.21862257e-01  1.62591323e-01  1.31810695e-01  1.51516974e-01
  9.07174796e-02 -3.71540971e-02  1.68341041e-01 -1.16507255e-01
  6.36918992e-02 -1.05791017e-02 -4.01803911e-01 -1.04713023e-01
  4.09282357e-01 -1.53884180e-02 -1.14709601e-01  1.57431841e-01
  3.37265253e-01 -2.56397873e-01  4.64204922e-02 -5.70997186e-02
 -1.09034088e-02  7.35260174e-02 -4.69822809e-02  1.62830800e-01
 -1.15381166e-01 -1.48207650e-01  4.22982872e-02 -2.53200948e-01
 -5.67734480e-01 -8.07923824e-02  1.96031947e-02 -4.49546337e-01
  2.08165318e-01 -1.91973612e-01 -2.63126552e-01  5.18878475e-02
  9.90109146e-02  2.67218590e-01  2.06611026e-02  4.03743446e-01
 -5.90464398e-02 -1.54334098e-01 -4.45236824e-02 -4.01515573e-01
 -6.53182045e-02 -1.12488046e-02 -4.45681453e-01 -8.32848698e-02
 -8.69537294e-02  1.60312891e-01 -6.32970631e-02 -1.03577375e-01
  6.57489955e-01  1.89933062e-01  4.53613102e-01  3.95630375e-02
  4.98520918e-02 -7.35120773e-02 -3.20446223e-01  1.29718989e-01
 -3.25605333e-01 -1.59518480e-01  7.70996511e-02 -1.00219950e-01
  9.35378149e-02  1.60998642e-01 -1.32723734e-01 -2.63720632e-01
 -3.16662759e-01  4.64083314e-01 -5.11553437e-02  8.22647810e-02
  2.69616008e-01  1.43347293e-01  5.36290288e-01  2.23294586e-01
 -6.41464517e-02  2.61468887e-01  2.97453940e-01  5.45222908e-02
  3.36838186e-01  1.97700918e-01  1.12250205e-02  2.38784283e-01
  2.06905156e-01  3.36192042e-01 -2.62072653e-01  5.51989675e-01
 -4.13605757e-02 -2.34646812e-01  2.97502041e-01 -3.15843195e-01
  5.96261978e-01 -3.24075997e-01  8.18860531e-02 -2.13122964e-01
  4.38787818e-01 -1.92982197e-01  1.04992688e-01  1.42971203e-02
  1.77237064e-01  3.97832930e-01 -1.63655192e-01  9.70884115e-02
  9.41565633e-02 -4.92672250e-02  1.35364821e-02 -7.90389180e-01
 -2.98230618e-01  1.45119131e-01 -3.19446743e-01  3.03909123e-01
  1.27432942e-01  6.21667355e-02 -1.82356030e-01  1.77716509e-01
 -1.24908336e-01 -1.10944565e-02  1.35745704e-01  3.12317133e-01
 -1.16249375e-01  2.08861232e-01  3.43800962e-01 -3.59512061e-01
 -7.72752464e-02  4.33711335e-04  1.74701422e-01  1.39371276e-01
  6.84394598e-01 -6.68160200e-01  2.20871165e-01  8.75358135e-02
 -3.10553815e-02  4.56652433e-01  1.34118930e-01  1.67602897e-01
 -4.61206794e-01  4.70023632e-01  1.92799211e-01 -2.92681336e-01
  1.12241969e-01 -8.50726217e-02 -7.13281631e-01 -1.39273942e-01
  2.15578586e-01 -2.56991357e-01 -7.84459561e-02 -4.51243281e-01
  1.24346009e-02  4.59997654e-02 -1.67206183e-01  1.52159691e-01
 -1.05588719e-01  2.16536641e-01 -2.42879391e-01 -2.17882827e-01
 -4.02660489e-01  2.13890076e-01 -6.00622892e-02 -3.57639432e-01
  5.35701662e-02 -8.52122083e-02 -8.02735835e-02 -1.87221318e-01
 -3.56977805e-02 -4.91728067e-01  3.93069655e-01  3.28674406e-01
 -1.33930206e-01  3.01160924e-02 -4.11042944e-02  1.87809974e-01
 -5.68035364e-01  4.85204868e-02 -2.11977154e-01  2.32077658e-01
 -5.40243350e-02 -1.04525961e-01  4.83491123e-01  4.21141893e-01
 -1.66655153e-01  2.36186251e-01 -2.80302525e-01 -1.28594249e-01
  2.96288431e-02 -3.98735404e-02 -4.53069925e-01  1.81501023e-02
 -1.56069482e-02  3.53149235e-01 -2.38935947e-01  5.17333508e-01
 -3.78557622e-01  1.19995937e-01  5.28033137e-01 -3.34939003e-01
 -2.09897816e-01  3.46364647e-01  1.92625999e-01 -3.40721965e-01
 -8.30894113e-02  4.48415615e-02  1.98915839e-01 -1.58333778e-01]"
How to use tf.repeat and another buil-in highlevel funcs on Dataset? stat:awaiting response type:support stale,Can you help me in [stackoverflow](https://stackoverflow.com/questions/77332989/how-to-use-tf-repeat-and-another-buil-in-highlevel-funcs-on-dataset)? ,False,"[-0.49928758 -0.04156073 -0.22664756 -0.1764724  -0.08822756 -0.04247359
  0.14144634  0.18927777 -0.2271117  -0.21837036 -0.17843458 -0.02489682
  0.39356175  0.03049457  0.02769737  0.40964845 -0.14344841  0.11617016
 -0.03523294 -0.11729287 -0.17486396 -0.09551068  0.07938276  0.1585921
 -0.11079482  0.18794474 -0.24057367  0.04950115 -0.08074091  0.31083253
  0.01999352  0.16887537 -0.43504164 -0.03215441 -0.12148266  0.02920671
 -0.29063234 -0.02909017 -0.14747435  0.01864422  0.12564775 -0.3628574
 -0.11673842 -0.12684734 -0.06364581 -0.07147203 -0.05545197  0.1904345
  0.10609704  0.29671198 -0.03068002  0.12092823 -0.26538232  0.08469087
  0.25799456  0.10981277 -0.04136935 -0.13384202 -0.07778478  0.03878589
  0.0546079  -0.01055981  0.4019536   0.1449147  -0.26140407  0.21533345
  0.0519075   0.13991275  0.17306988 -0.28093818  0.13124263  0.21166594
 -0.22343685 -0.12279546 -0.14478746 -0.10133561 -0.41682926 -0.23062105
 -0.17362988  0.13849461 -0.10388752 -0.20630658  0.03411912 -0.36676887
  0.06861924 -0.14695644  0.19071814 -0.19666457 -0.03023299  0.19552527
  0.22238946  0.10324692  0.1062501   0.2418921   0.081274    0.22686897
 -0.15098363  0.1087038   0.01677675  0.12773587 -0.25553375 -0.17312796
 -0.05840557  0.15470465  0.06578486 -0.1442626  -0.17390728  0.2240788
  0.00685603 -0.07206455  0.15825525 -0.07961748  0.3332165   0.01596651
 -0.03172183  0.05674962 -0.00918587  0.04617745 -0.20436214  0.10508756
  0.23810248 -0.09781276  0.1295061   0.14261132  0.42985982  0.06852662
  0.13288216  0.06031088 -0.05610977 -0.18007985  0.21344075  0.11535858
 -0.14024921  0.1251197  -0.23865752  0.00539058 -0.31923574 -0.06187842
 -0.1579503   0.18570027  0.04566646  0.03921989  0.0140669  -0.42895517
  0.10309172  0.12876274 -0.14696735 -0.02533413  0.02396046  0.12365653
  0.2810338   0.10731884 -0.19932136  0.18232974 -0.04762384  0.17308795
  0.52318525  0.1792314   0.20102137 -0.38910487  0.21970306  0.18910137
  0.03219318  0.00388059  0.12299286 -0.04642896 -0.23454261 -0.0999684
  0.2205317   0.20805722 -0.06017328  0.05685357 -0.33832973 -0.12287844
  0.20518234 -0.00375762  0.03721388 -0.2821738   0.14593628  0.2627265
 -0.05119671  0.09950828 -0.04074765 -0.0742074   0.01147002  0.08649034
 -0.04133993  0.30387476 -0.20760185 -0.22477524 -0.03585618 -0.26929203
  0.25552344  0.20200881 -0.02626299 -0.2092553   0.08588544 -0.22869134
  0.4464596   0.0504631  -0.07387431 -0.0815458   0.09468746  0.04981147
  0.07456978  0.01911429 -0.27773294 -0.5344619   0.15883082  0.0233502
 -0.04854331 -0.03721118 -0.11349285 -0.05843782 -0.44594988 -0.12719035
 -0.03347182 -0.11475547 -0.22933595  0.2148257   0.1972412  -0.114733
  0.17497395 -0.2272465  -0.15438809  0.25008625 -0.10137469  0.13118869
  0.23969086  0.1262559   0.13118182  0.03436481 -0.15105443 -0.0196565
  0.08944882 -0.12560257 -0.01499346 -0.13118184 -0.10533332 -0.23735002
 -0.19101214 -0.17379095 -0.24483043 -0.24043533  0.10696016 -0.1533303
 -0.18984227 -0.17249432 -0.02968279  0.2805751  -0.3599562  -0.323775
  0.31588697  0.01798431  0.37586495  0.2825077   0.28179845 -0.101532
  0.25441492  0.12805425  0.23332669  0.18885869  0.22469713  0.17336123
  0.15165827 -0.0487183  -0.30397204 -0.18050823 -0.08366746  0.08660631
 -0.42590514 -0.0470154   0.21492855 -0.06443734  0.01906753 -0.16584665
  0.14667508 -0.21628678  0.05620348  0.32328612  0.10728273  0.27221352
  0.09614358  0.00450659 -0.05109124 -0.1382278  -0.15898299 -0.19895984
  0.02552035 -0.02663383 -0.06932011 -0.33152378 -0.18251643 -0.1512
 -0.18590485  0.13623087  0.14274402 -0.12526482  0.05507446  0.05265608
 -0.26367465  0.19974524  0.3251283   0.11788502 -0.08248093  0.03903368
  0.21633025  0.24367853 -0.18774296  0.02995706  0.25796348  0.06135583
  0.04683056  0.25800043 -0.20061749  0.10903328 -0.03564087  0.23731354
 -0.00943206 -0.12579729 -0.00393083 -0.30743837 -0.14362156  0.19044477
  0.16077693  0.23488097 -0.06076738 -0.01940992  0.13439386  0.0189333
  0.13084005  0.05932443  0.02933909 -0.1298806  -0.10058101 -0.2297779
 -0.05389781 -0.1367173   0.13300774 -0.09242416 -0.12896323 -0.06151704
  0.00219182 -0.12267666  0.00357856 -0.17155379 -0.15547013  0.4550196
  0.19327815  0.26824045  0.13809502  0.06616916 -0.00284498 -0.19342646
  0.08753131  0.03022907 -0.00157975  0.21426874 -0.15088111  0.21600789
  0.06881993  0.11678853  0.1000157  -0.07994497 -0.08117671 -0.24595053
 -0.29048452  0.00324953  0.22785226  0.12374905 -0.35416472  0.02287236
 -0.21355608 -0.00633914  0.2396543  -0.0446915   0.18483923 -0.05619257
  0.2275151  -0.06925347 -0.03790396 -0.10050549 -0.01632753  0.14154242]"
New invalid,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
",False,"[-3.85169774e-01 -3.38949859e-01 -8.23499188e-02 -2.49498546e-01
 -1.25067651e-01 -1.99878708e-01 -2.26185262e-01 -8.44543576e-02
 -4.42114651e-01 -2.53209352e-01  1.85767710e-01 -7.78118074e-02
 -4.87217903e-02  7.81770498e-02  2.91715920e-01  1.04920045e-02
  4.92743701e-02 -2.18964279e-01  1.24131411e-01  1.34310901e-01
  1.02997102e-01 -2.30068848e-01 -2.55171955e-01  3.01780971e-03
 -3.21918011e-01  1.77109852e-01 -9.28643644e-02 -2.08132267e-01
  2.30768383e-01 -1.76896062e-02  5.27334988e-01  1.91327736e-01
 -1.10876383e-02  3.86613980e-02  3.23645771e-02  1.45185709e-01
 -2.47580528e-01 -2.85876721e-01 -2.35207409e-01 -1.38252169e-01
  1.43768355e-01  1.38948664e-01 -1.98307395e-01  9.46189910e-02
 -1.15512371e-01  1.61158949e-01 -1.79730505e-01  5.03269210e-02
 -2.38667458e-01 -3.76941562e-02 -1.03867188e-01  6.39190450e-02
 -5.20822048e-01 -2.16659278e-01 -1.84051827e-01 -3.19752358e-02
 -1.98669374e-01  1.88230366e-01  9.30083096e-02  2.06664085e-01
  3.77019256e-01 -6.07216768e-02 -1.31782353e-01 -2.74449550e-02
  6.04820773e-02  6.12063035e-02  1.67526808e-02 -1.42885521e-01
  2.63085544e-01  2.36679912e-01 -2.43601024e-01  1.21509075e-01
 -1.62540436e-01  3.53720300e-02  1.60051882e-01  3.16773951e-01
 -3.96850675e-01  1.56825691e-01 -7.20675439e-02 -9.90173444e-02
 -4.10915092e-02 -1.49118662e-01 -1.33401919e-02  2.65697092e-01
  1.17082149e-01  1.10505432e-01 -2.81363688e-02  2.69096419e-02
  9.99267958e-03  2.24983066e-01 -1.68379188e-01 -9.31283385e-02
 -3.28270271e-02  2.11769491e-01  1.79123729e-01  7.49046728e-02
 -1.51427627e-01 -1.23171788e-02 -4.70377225e-03 -9.55410451e-02
 -5.78103587e-03 -1.40751123e-01  1.76959723e-01  1.40373558e-01
  1.62680030e-01 -3.93448174e-01 -1.73960686e-01  5.97137734e-02
  1.11945130e-01 -6.61283955e-02  7.65628144e-02 -2.36926153e-01
  1.16809331e-01 -4.71852385e-02  4.65535596e-02  4.63572778e-02
 -6.62980527e-02 -1.64063424e-01 -7.55389109e-02  3.51790428e-01
  8.86144340e-02 -1.89312965e-01  1.27379373e-01 -5.77830561e-02
  2.62701631e-01  1.49293482e-01  2.07988862e-02  1.02229401e-01
  8.32309108e-03  5.84531054e-02  1.33794188e-01  5.29707894e-02
 -8.68406333e-03 -1.25379875e-01  9.71566811e-02  1.64984658e-01
 -3.58152330e-01 -2.96398737e-02  4.83910292e-02  5.62401339e-02
  1.45243645e-01  2.69320399e-01  2.67039418e-01 -6.23508133e-02
 -1.73557699e-02  6.51034340e-03  5.83718792e-02  3.58675569e-01
  5.92260994e-02 -2.97796071e-01 -8.57412219e-02 -4.28867757e-01
 -2.06780918e-02  4.19828594e-01  1.00332022e-01  3.72801460e-02
  2.83792287e-01 -4.66154702e-02  2.12981299e-01 -5.77477694e-01
 -7.86257088e-02  2.94668287e-01 -1.22167636e-02  8.63518789e-02
  8.74082893e-02 -8.54198914e-03 -2.37460807e-01 -2.77147949e-01
 -8.63531977e-02 -2.78342888e-03 -8.07186961e-02 -1.40243638e-02
 -2.83014961e-04  1.05892435e-01  6.40956908e-02 -2.45839000e-01
  1.51296496e-01 -2.77776390e-01 -4.34178337e-02  1.53757319e-01
  2.10157722e-01 -1.09196931e-01 -5.94674796e-02 -1.64353922e-01
 -1.64279670e-01  1.40792221e-01 -7.91114792e-02  2.47001290e-01
  2.47066066e-01  3.65058258e-02 -3.63078713e-02  5.54258972e-02
 -2.91261464e-01 -2.22287416e-01 -2.70753086e-01 -1.94322601e-01
  1.75311908e-01 -1.25033110e-01  2.49433815e-01 -1.12558566e-02
 -4.31609415e-02  2.12663729e-02 -2.44976342e-01 -9.68972687e-03
  3.26398253e-01 -1.66825622e-01 -4.48311329e-01 -1.45247415e-01
 -2.57757872e-01  1.63783818e-01  4.42039073e-02 -4.13361490e-01
 -3.02197933e-01  1.29784286e-01  1.28719151e-01 -1.27444953e-01
 -1.61600947e-01  2.44434271e-02 -2.12936997e-01  2.46548563e-01
 -2.36429632e-01  2.46267505e-02  2.61166245e-02 -1.29119575e-01
 -9.71263945e-02 -1.20183565e-01 -2.26146638e-01 -3.25529993e-01
 -2.78855823e-02  1.69047952e-01  3.92643400e-02  6.99575022e-02
  1.55886382e-01  9.36649367e-02 -6.66731149e-02 -1.91832274e-01
  3.74298960e-01 -1.68317422e-01 -3.93636167e-01  3.38437632e-02
 -5.02054632e-01 -3.20150182e-02  1.93934873e-01 -7.29130208e-02
 -3.82881425e-02  2.20936202e-02  2.12994844e-01  4.29118723e-01
  4.13955702e-03  8.14730003e-02 -1.05747417e-01 -1.70190126e-01
  2.85835385e-01  8.24806541e-02  3.45274776e-01  9.67975259e-02
  1.54414773e-03 -2.10724056e-01  2.85255872e-02 -1.72006726e-01
  3.17533612e-01  3.22205156e-01  2.38057315e-01 -3.98263335e-02
  2.79448837e-01  5.81646562e-02 -3.35319340e-01 -1.94882110e-01
  3.04125041e-01  8.31519216e-02  4.14052367e-01 -2.69359797e-01
  9.38529298e-02 -1.16702817e-01  1.99272767e-01  3.12893689e-02
  3.00299585e-01 -9.81981605e-02 -1.32970005e-01  3.85066569e-01
  1.20001666e-01 -9.67021659e-03 -1.35282427e-01 -2.80705810e-01
  1.33569464e-01 -4.44631815e-01 -1.07696272e-01 -1.90136164e-01
 -1.74265057e-01 -1.67729408e-01  2.04309687e-01  2.21744210e-01
  1.03295140e-01 -2.95349080e-02 -3.05037081e-01  3.70093524e-01
 -1.45741165e-01  1.48010954e-01  1.20183080e-01  3.56320560e-01
 -1.50046200e-01 -5.98318800e-02 -2.83210650e-02  1.32800668e-01
 -2.80851007e-01 -4.42544278e-03  2.07203820e-01 -3.09800692e-02
  4.42745507e-01 -1.20219640e-01  2.81471729e-01  1.87953755e-01
 -2.09691703e-01  3.84120077e-01 -2.57534474e-01  4.42207038e-01
 -2.56573439e-01  7.21715093e-01  1.16257936e-01  7.02895895e-02
  1.74579501e-01 -3.83868009e-01 -4.77435365e-02 -1.09663317e-02
 -2.09942579e-01  2.09915126e-03 -2.02732578e-01 -1.74836397e-01
 -9.00194943e-02  1.32546246e-01 -2.60751843e-01 -1.53400470e-02
 -9.95694101e-02  2.94033051e-01 -5.62170520e-02 -3.28379571e-02
 -2.25189090e-01  5.05584836e-01 -4.59770486e-02  1.25002623e-01
  1.43172190e-01  4.84775119e-02  1.56579375e-01 -2.58321464e-01
 -1.54752731e-01 -1.36057094e-01  4.44876075e-01  2.56805062e-01
 -7.60180056e-02  8.79262090e-02  9.97811332e-02 -1.56645384e-02
 -9.80765820e-02 -2.57137775e-01 -1.34724349e-01  1.82647794e-01
  2.74189502e-01 -1.68684453e-01 -5.44349514e-02  9.04906750e-01
 -1.87492281e-01 -1.65728033e-01 -1.93639815e-01 -2.68117189e-01
 -3.00120022e-02 -7.26736039e-02 -6.45000637e-02 -2.70902812e-01
  4.14904475e-01  5.20545959e-01 -2.19104439e-01  2.50816822e-01
 -3.72465521e-01 -1.99342161e-01  5.13284683e-01  3.65717709e-03
 -1.14768207e-01 -1.36767417e-01  6.22354597e-02  2.14792877e-01
  1.02980584e-01  2.82927394e-01  2.86067367e-01  9.13923085e-02]"
can't install @tensorflow/tf-node on npm ,"please help. i can't install @tensorflow/tf-node on my project node.js

i have python 3.12, node.js 18 and npm ver. 10.2.1

but i can install @tensorflow/tfjs

this is the error:

npm WARN cleanup Failed to remove some directories [
npm WARN cleanup   [
npm WARN cleanup     'D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@mapbox',
npm WARN cleanup     [Error: EPERM: operation not permitted, rmdir 'D:\xampp8\htdocs\testnodejs\node_modules\@mapbox\node-pre-gyp\node_modules\agent-base'] {
npm WARN cleanup       errno: -4048,
npm WARN cleanup       code: 'EPERM',
npm WARN cleanup       syscall: 'rmdir',
npm WARN cleanup       path: 'D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@mapbox\\node-pre-gyp\\node_modules\\agent-base'
npm WARN cleanup     }
npm WARN cleanup   ]
npm WARN cleanup ]
npm ERR! code 1
npm ERR! path D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node
npm ERR! command failed
npm ERR! command C:\WINDOWS\system32\cmd.exe /d /s /c node scripts/install.js
npm ERR! CPU-windows-4.12.0.zip
npm ERR! https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-2.9.1.zip
npm ERR! node-pre-gyp install failed with error: Error: Command failed: node-pre-gyp install --fallback-to-build
npm ERR! node-pre-gyp info it worked if it ends with ok
npm ERR! node-pre-gyp info using node-pre-gyp@1.0.9
npm ERR! node-pre-gyp info using node@18.18.2 | win32 | x64
npm ERR! node-pre-gyp info check checked for ""D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node\lib\napi-v8\tfjs_binding.node"" (not found)
npm ERR! node-pre-gyp http GET https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/4.12.0/CPU-windows-4.12.0.zip
npm ERR! node-pre-gyp ERR! install response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/4.12.0/CPU-windows-4.12.0.zip
npm ERR! node-pre-gyp WARN Pre-built binaries not installable for @tensorflow/tfjs-node@4.12.0 and node@18.18.2 (node-v108 ABI, unknown) (falling back to source compile with node-gyp)
npm ERR! node-pre-gyp WARN Hit error response status 404 Not Found on https://storage.googleapis.com/tf-builds/pre-built-binary/napi-v8/4.12.0/CPU-windows-4.12.0.zip
npm ERR! gyp info it worked if it ends with ok
npm ERR! gyp info using node-gyp@9.4.0
npm ERR! gyp info using node@18.18.2 | win32 | x64
npm ERR! gyp info ok
npm ERR! gyp info it worked if it ends with ok
npm ERR! gyp info using node-gyp@9.4.0
npm ERR! gyp info using node@18.18.2 | win32 | x64
npm ERR! gyp info find Python using Python version 3.12.0 found at ""C:\Program Files\Python312\python.exe""
npm ERR! gyp http GET https://nodejs.org/download/release/v18.18.2/node-v18.18.2-headers.tar.gz
npm ERR! gyp http 200 https://nodejs.org/download/release/v18.18.2/node-v18.18.2-headers.tar.gz
npm ERR! gyp http GET https://nodejs.org/download/release/v18.18.2/SHASUMS256.txt
npm ERR! gyp http GET https://nodejs.org/download/release/v18.18.2/win-x64/node.lib
npm ERR! gyp http 200 https://nodejs.org/download/release/v18.18.2/SHASUMS256.txt
npm ERR! gyp http 200 https://nodejs.org/download/release/v18.18.2/win-x64/node.lib
npm ERR! gyp info find VS using VS2022 (17.7.34202.233) found at:
npm ERR! gyp info find VS ""C:\Program Files\Microsoft Visual Studio\2022\Enterprise""
npm ERR! gyp info find VS run with --verbose for detailed information
npm ERR! gyp info spawn C:\Program Files\Python312\python.exe
npm ERR! gyp info spawn args [
npm ERR! gyp info spawn args   'C:\\Users\\User\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\gyp\\gyp_main.py',
npm ERR! gyp info spawn args   'binding.gyp',
npm ERR! gyp info spawn args   '-f',
npm ERR! gyp info spawn args   'msvs',
npm ERR! gyp info spawn args   '-I',
npm ERR! gyp info spawn args   'D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@tensorflow\\tfjs-node\\build\\config.gypi',
npm ERR! gyp info spawn args   '-I',
npm ERR! gyp info spawn args   'C:\\Users\\User\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\addon.gypi',
npm ERR! gyp info spawn args   '-I',
npm ERR! gyp info spawn args   'C:\\Users\\User\\AppData\\Local\\node-gyp\\Cache\\18.18.2\\include\\node\\common.gypi',
npm ERR! gyp info spawn args   '-Dlibrary=shared_library',
npm ERR! gyp info spawn args   '-Dvisibility=default',
npm ERR! gyp info spawn args   '-Dnode_root_dir=C:\\Users\\User\\AppData\\Local\\node-gyp\\Cache\\18.18.2',
npm ERR! gyp info spawn args   '-Dnode_gyp_dir=C:\\Users\\User\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp',
npm ERR! gyp info spawn args   '-Dnode_lib_file=C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\node-gyp\\\\Cache\\\\18.18.2\\\\<(target_arch)\\\\node.lib',
npm ERR! gyp info spawn args   '-Dmodule_root_dir=D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@tensorflow\\tfjs-node',npm ERR! gyp info spawn args   '-Dnode_engine=v8',
npm ERR! gyp info spawn args   '--depth=.',
npm ERR! gyp info spawn args   '--no-parallel',
npm ERR! gyp info spawn args   '--generator-output',
npm ERR! gyp info spawn args   'D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@tensorflow\\tfjs-node\\build',
npm ERR! gyp info spawn args   '-Goutput_dir=.'
npm ERR! gyp info spawn args ]
npm ERR! Traceback (most recent call last):
npm ERR!   File ""C:\Users\User\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\gyp\gyp_main.py"", line 42, in <module>
npm ERR!     import gyp  # noqa: E402
npm ERR!     ^^^^^^^^^^
npm ERR!   File ""C:\Users\User\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\__init__.py"", line 9, in <module>
npm ERR!     import gyp.input
npm ERR!   File ""C:\Users\User\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\gyp\pylib\gyp\input.py"", line 19, in <module>
npm ERR!     from distutils.version import StrictVersion
npm ERR! ModuleNotFoundError: No module named 'distutils'
npm ERR! gyp ERR! configure error
npm ERR! gyp ERR! stack Error: `gyp` failed with exit code: 1
npm ERR! gyp ERR! stack     at ChildProcess.onCpExit (C:\Users\User\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\lib\configure.js:325:16)
npm ERR! gyp ERR! stack     at ChildProcess.emit (node:events:517:28)
npm ERR! gyp ERR! stack     at ChildProcess._handle.onexit (node:internal/child_process:292:12)
npm ERR! gyp ERR! System Windows_NT 10.0.19045
npm ERR! gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""C:\\Users\\User\\AppData\\Roaming\\npm\\node_modules\\npm\\node_modules\\node-gyp\\bin\\node-gyp.js"" ""configure"" ""--fallback-to-build"" ""--module=D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8\\tfjs_binding.node"" ""--module_name=tfjs_binding"" ""--module_path=D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@tensorflow\\tfjs-node\\lib\\napi-v8"" ""--napi_version=9"" ""--node_abi_napi=napi"" ""--napi_build_version=8"" ""--node_napi_label=napi-v8""
npm ERR! gyp ERR! cwd D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node
npm ERR! gyp ERR! node -v v18.18.2
npm ERR! gyp ERR! node-gyp -v v9.4.0
npm ERR! gyp ERR! not ok
npm ERR! node-pre-gyp ERR! build error
npm ERR! node-pre-gyp ERR! stack Error: Failed to execute 'C:\Program Files\nodejs\node.exe C:\Users\User\AppData\Roaming\npm\node_modules\npm\node_modules\node-gyp\bin\node-gyp.js configure --fallback-to-build --module=D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node\lib\napi-v8\tfjs_binding.node --module_name=tfjs_binding --module_path=D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node\lib\napi-v8 --napi_version=9 --node_abi_napi=napi --napi_build_version=8 --node_napi_label=napi-v8' (1)
npm ERR! node-pre-gyp ERR! stack     at ChildProcess.<anonymous> (D:\xampp8\htdocs\testnodejs\node_modules\@mapbox\node-pre-gyp\lib\util\compile.js:89:23)
npm ERR! node-pre-gyp ERR! stack     at ChildProcess.emit (node:events:517:28)
npm ERR! node-pre-gyp ERR! stack     at maybeClose (node:internal/child_process:1098:16)
npm ERR! node-pre-gyp ERR! stack     at ChildProcess._handle.onexit (node:internal/child_process:303:5)
npm ERR! node-pre-gyp ERR! System Windows_NT 10.0.19045
npm ERR! node-pre-gyp ERR! command ""C:\\Program Files\\nodejs\\node.exe"" ""D:\\xampp8\\htdocs\\testnodejs\\node_modules\\@mapbox\\node-pre-gyp\\bin\\node-pre-gyp"" ""install"" ""--fallback-to-build""
npm ERR! node-pre-gyp ERR! cwd D:\xampp8\htdocs\testnodejs\node_modules\@tensorflow\tfjs-node
npm ERR! node-pre-gyp ERR! node -v v18.18.2
npm ERR! node-pre-gyp ERR! node-pre-gyp -v v1.0.9
npm ERR! node-pre-gyp ERR! not ok
npm ERR! * Downloading libtensorflow
npm ERR!
npm ERR! * Building TensorFlow Node.js bindings

![Screenshot_1](https://github.com/tensorflow/tensorflow/assets/85969639/8f834004-73e9-415c-9436-8a006693074e)
![Screenshot_2](https://github.com/tensorflow/tensorflow/assets/85969639/bea6901a-3557-4f09-b6d0-368ae0852487)
![Screenshot_3](https://github.com/tensorflow/tensorflow/assets/85969639/d4caa528-fbdb-4be4-a0d1-c7c9e03662f4)
![Screenshot_4](https://github.com/tensorflow/tensorflow/assets/85969639/42b7d62e-47af-4571-8d8c-18ab4c256157)
![Screenshot_5](https://github.com/tensorflow/tensorflow/assets/85969639/018b3162-a010-4939-9853-b4735f2fc360)
![Screenshot_6](https://github.com/tensorflow/tensorflow/assets/85969639/0f5fc8a0-57fd-4f6e-9699-573f82b7de04)


what I missed?",False,"[-2.51577824e-01 -4.64185178e-01  2.30806708e-01 -3.89366075e-02
  6.06679380e-01 -4.30117667e-01 -2.07117219e-02 -2.59583712e-01
 -3.40404868e-01  1.53790742e-01  9.23842788e-02  4.92457189e-02
 -1.36583939e-01  2.77384043e-01 -9.70194116e-02  4.02514040e-01
 -3.67150784e-01 -6.00466989e-02  5.72877765e-01  3.46299767e-01
  7.89930820e-02  8.13699886e-03 -8.62013027e-02  1.33638114e-01
  1.49010599e-01  8.78752023e-03  7.66704828e-02 -2.21830070e-01
 -1.56580105e-01  7.91262612e-02  2.81718612e-01 -2.02014133e-01
 -3.43435049e-01 -6.99282289e-02  3.77502292e-01  3.72471601e-01
 -5.70487201e-01 -8.17980021e-02 -7.58508891e-02 -2.51153320e-01
  4.74609196e-01 -6.09253757e-02  1.44962490e-01 -6.12383336e-02
 -2.42315412e-01 -2.15161562e-01 -1.03871070e-01 -2.21487358e-02
 -2.02829286e-01 -9.46694016e-02  7.73737058e-02 -1.78513378e-01
 -6.33024871e-01 -2.29580924e-01 -6.57142177e-02  2.04105284e-02
  5.15544042e-02  2.39813700e-01  1.04225948e-01  4.88329649e-01
  3.32550377e-01  4.80996817e-02 -1.68018699e-01  1.55992880e-01
  6.38101816e-01  3.18740368e-01  2.01487154e-01 -3.08583915e-01
  2.87511826e-01 -2.04166308e-01  3.54826689e-01 -1.55261725e-01
 -2.33039588e-01 -3.57710779e-01 -1.61949843e-01  1.81588098e-01
  4.79557216e-02  3.06896448e-01  1.54539198e-01  7.57514760e-02
 -4.31269377e-01  3.47336531e-02  3.67987931e-01 -1.63372040e-01
 -2.68866867e-03  1.39224544e-01 -1.69075076e-02  2.09858879e-01
  4.39354539e-01 -2.60794938e-01  5.28801024e-01  2.06540763e-01
  3.05548143e-02  2.38162518e-01  2.33527839e-01 -2.43507698e-02
  1.20338261e-01  2.05161735e-01 -3.14560086e-01 -3.82488251e-01
  1.56210378e-01 -3.37480217e-01 -7.93357864e-02  1.20844811e-01
 -6.32504076e-02  2.55181044e-01  2.21095383e-01 -2.26082638e-01
 -8.25315937e-02  9.75110233e-02  9.12754238e-02  1.43430218e-01
  2.97266603e-01 -1.21174112e-01  5.90565130e-02  3.12699318e-01
 -1.89099640e-01  3.90083253e-01  2.95198429e-03  6.34021521e-01
 -1.69803172e-01 -2.74795324e-01  9.76654440e-02  3.43848653e-02
  1.71217769e-01  4.58399385e-01 -3.57233256e-01 -3.50915119e-02
  2.47833252e-01  2.30133414e-01 -2.28761099e-02  2.56286770e-01
 -2.53690690e-01  3.31054665e-02  7.00738952e-02  1.23094052e-01
 -2.99874783e-01 -2.54912764e-01 -6.40688092e-02 -9.21563059e-03
  4.13905978e-02  5.82538247e-02 -1.19436830e-01 -4.52954322e-01
  3.40974510e-01  4.88801766e-03 -1.27781093e-01  1.87072843e-01
 -2.83663213e-01  1.15524188e-01 -2.92058349e-01  1.88693196e-01
 -1.24408402e-01  5.46941817e-01 -2.03957751e-01  4.47479665e-01
  1.91487268e-01 -9.14808363e-02 -5.48605993e-03 -3.90287399e-01
 -1.19912803e-01  6.77439630e-01 -3.93670984e-02  1.14730150e-02
 -9.90675464e-02 -1.05212271e-01 -3.93538803e-01 -4.54438597e-01
 -1.11838542e-01  1.45866796e-01 -1.83759555e-02 -6.47218078e-02
 -1.27485067e-01  1.50643349e-01  3.03266943e-01 -2.00181365e-01
  7.48001099e-01 -5.10039508e-01  9.15027782e-03  1.72115237e-01
 -4.32931185e-02 -1.67316198e-01  7.81125054e-02 -4.35064360e-02
  1.97752908e-01 -5.85398562e-02 -9.65169147e-02 -1.48118995e-02
  1.20833047e-01 -7.53344297e-02 -8.75867605e-02 -1.10431448e-01
  5.20730555e-01 -6.30465820e-02 -2.40451351e-01  9.16471332e-02
  4.49617535e-01 -9.03531462e-02  1.76238537e-01 -1.75827574e-02
  4.99434412e-01  2.15290740e-01 -1.73947334e-01  1.07891440e-01
  6.77200034e-02 -4.49301824e-02 -1.08837709e-01 -2.03452945e-01
 -3.52340996e-01 -3.10792625e-01 -1.37167156e-01 -2.07230687e-01
  1.05442137e-01 -2.67263591e-01 -3.49262089e-01  1.67570800e-01
  1.20377131e-01  1.75562575e-01  1.06914505e-01  2.51091987e-01
  1.50725707e-01 -1.15682781e-02 -2.65885115e-01 -1.35605738e-01
 -1.04142115e-01 -1.31072283e-01  2.19647754e-02 -8.54935572e-02
 -3.98565233e-01  4.26851064e-02 -2.60201544e-01 -1.32455379e-01
  3.17414552e-01 -5.59016466e-02 -4.57404777e-02  6.16498142e-02
 -3.85185964e-02 -6.80090711e-02 -3.55889529e-01  1.44717425e-01
 -2.07391381e-01 -3.08002025e-01  6.52785897e-02 -4.35005724e-02
  2.56998800e-02  5.05181029e-02 -6.37511462e-02 -2.08571047e-01
 -3.68206859e-01  5.71457088e-01 -1.58098370e-01 -2.50812650e-01
  1.14159822e-01  3.63161713e-02  6.99708462e-02  2.62941092e-01
 -1.55309767e-01  1.06458709e-01  1.58745140e-01 -8.81871283e-02
  3.92637312e-01  1.74599111e-01 -3.39067876e-02  3.59628916e-01
  1.25029728e-01  3.11359406e-01 -2.53188848e-01  1.18631035e-01
  6.45429641e-02 -1.78612739e-01  3.40520114e-01 -4.26388025e-01
  5.66265509e-02 -5.09936735e-02 -2.82951415e-01 -1.44845054e-01
  4.80832845e-01 -3.77104819e-01 -7.76215047e-02  2.45617092e-01
  1.09596007e-01  4.04571384e-01 -2.46917307e-02  1.54812336e-01
  3.76485914e-01 -1.32007033e-01 -2.70155258e-02 -5.67954063e-01
 -2.70851046e-01  3.18789124e-01 -3.02241206e-01  2.84898281e-03
  1.28302630e-02 -9.16797370e-02 -2.84032702e-01  1.11582920e-01
  3.26004922e-02 -1.70409083e-01 -5.85541576e-02  4.64951918e-02
 -1.27656266e-01 -1.36407703e-01  2.03205645e-01 -5.11768579e-01
  2.08800554e-01 -9.80072767e-02 -7.44409636e-02 -9.02435556e-02
  2.50928819e-01 -3.86665165e-01  3.85459006e-01 -1.03111796e-01
  1.91743970e-02  1.46848649e-01 -2.77419239e-02  1.82899937e-01
 -3.47671993e-02  5.47610521e-01  4.10070419e-01 -1.34080350e-01
  1.47692531e-01 -2.43831664e-01 -6.23157740e-01 -2.64071912e-01
  3.01409543e-01 -4.73557711e-02 -2.26553932e-01 -5.85296601e-02
 -2.18210071e-02  2.46204197e-01  3.04599553e-02  1.87985133e-03
 -2.08967999e-01  2.91892529e-01 -8.66043866e-02 -2.86450922e-01
 -2.97422767e-01  2.28744715e-01 -1.31933421e-01 -4.80821341e-01
  8.35225508e-02 -1.09653965e-01  4.02950719e-02 -2.70402998e-01
 -1.92657411e-01 -1.79401487e-01  5.46678245e-01  5.41053295e-01
 -1.12003990e-01  8.38327706e-02 -7.78911188e-02  1.41238391e-01
 -3.15787792e-01 -5.09244055e-02 -1.55675471e-01  4.18582797e-01
 -1.29268050e-01 -4.37569339e-04  3.94868135e-01  5.95037401e-01
 -3.10842514e-01  3.39816362e-02 -2.24278480e-01 -2.65635811e-02
  6.62759990e-02 -2.71082342e-01 -2.18588188e-01  2.05184519e-01
  1.11638710e-01  3.16347539e-01 -2.65641987e-01  3.59974325e-01
 -3.71910155e-01 -8.35484341e-02  3.28265995e-01 -1.87694341e-01
 -7.60413259e-02  1.68706805e-01 -7.64333457e-02 -1.15003429e-01
 -1.51430368e-01 -8.96257460e-02 -8.98192450e-02 -5.31188369e-01]"
Set reshuffle_each_iteration option of shuffle as True by default instead of None stat:awaiting response type:feature stale type:docs-feature TF2.14,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When shuffling a dataset, the option reshuffle_each_iteration is None by default. But, when we look closer at the documentation, the default behavior is to reshuffle each iteration.
It would be better to set the default value of reshuffle_each_iteration to 'True' instead of None which is ambiguous.
Please find attached the code of the function at : https://github.com/tensorflow/tensorflow/blame/8c25f8252175c6a4f9614797e953fcbfb93c3aed/tensorflow/python/data/ops/shuffle_op.py#L49C12-L49C12

### Standalone code to reproduce the issue

```shell
# Code in tensorflow/tensorflow/python/data/ops/shuffle_op.py
class _ShuffleDataset(dataset_ops.UnaryUnchangedStructureDataset):
  """"""A `Dataset` that randomly shuffles the elements of its input.""""""

  def __init__(self,
               input_dataset,
               buffer_size,
               seed=None,
               reshuffle_each_iteration=None,
               name=None):
    """"""See `Dataset.shuffle()` for details.""""""
    self._input_dataset = input_dataset
    self._buffer_size = ops.convert_to_tensor(
        buffer_size, dtype=dtypes.int64, name=""buffer_size"")
    self._seed, self._seed2 = random_seed.get_seed(seed)
    if reshuffle_each_iteration is None:
      reshuffle_each_iteration = True
    self._reshuffle_each_iteration = reshuffle_each_iteration
    self._name = name
```


### Relevant log output

_No response_",False,"[-6.73335910e-01 -2.99351722e-01  5.28365150e-02  4.72514890e-03
  3.30524236e-01 -2.32844383e-01  7.50606656e-02 -1.85930077e-02
 -1.58204496e-01 -3.41667563e-01 -8.20035338e-02  7.35938475e-02
 -2.70718694e-01  2.45215237e-01 -7.27182925e-02  3.13368857e-01
 -5.41798651e-01  6.67106733e-02  2.29478240e-01  1.96358711e-01
 -2.47382432e-01 -1.28482953e-01 -3.28825831e-01  2.34152317e-01
  2.10224688e-01  2.20738828e-01 -1.24726854e-01 -8.63183141e-02
 -2.39051059e-02  3.08859758e-02  3.00062180e-01  5.09144902e-01
  1.16613522e-01  4.71348166e-02 -5.93257248e-02  2.31326699e-01
 -3.35168749e-01 -2.14401454e-01 -2.04267204e-01  9.65671688e-02
 -9.30167884e-02  1.22119769e-01  2.72758931e-01 -2.72349447e-01
  1.85945839e-01 -1.19751610e-01 -1.30533010e-01  1.36360973e-01
 -4.63635437e-02 -4.94437739e-02 -1.08473003e-01 -1.62501216e-01
 -3.61061007e-01 -3.28085631e-01 -7.55476952e-02 -1.22029588e-01
  2.15941459e-01 -1.38415515e-01  1.91647150e-02  2.51206279e-01
 -3.94710675e-02  5.90002723e-02  1.27062947e-01 -1.53368801e-01
 -2.63313055e-02  1.66213438e-01  2.56364822e-01 -2.37144694e-01
  5.35753250e-01 -7.35906363e-02 -3.59539539e-02 -3.14759463e-02
 -3.53800476e-01  1.38570666e-01  2.05044635e-04 -1.48292512e-01
 -1.02202818e-01  5.22873700e-02  2.82198250e-01 -2.51875017e-02
 -2.75305986e-01 -1.82960019e-01 -7.11852610e-02 -4.30534482e-01
  1.30143344e-01 -2.06535012e-01  4.21410084e-01  6.88936561e-02
  5.41287422e-01 -1.83477372e-01  2.77090788e-01  4.58799243e-01
  7.34305158e-02  6.08776808e-02  6.56158090e-01  3.34214807e-01
 -1.82624999e-02  3.91412079e-02  2.92360902e-01 -2.75953114e-02
  4.16955091e-02 -2.11371601e-01  2.59516649e-02 -3.95085514e-02
  7.33409226e-02 -2.94834465e-01  3.99112180e-02 -2.38929436e-01
  2.19432022e-02 -7.18315840e-02  3.18300068e-01  6.97173327e-02
  4.11449611e-01 -1.64107159e-01 -1.44341975e-01 -1.85066327e-01
  3.23380157e-03 -1.05756581e-01 -2.62056012e-02  8.40060771e-01
  4.95823845e-02 -8.71923566e-02  8.21053535e-02  3.84396195e-01
  4.19266760e-01  1.72351211e-01 -9.50791240e-02 -2.06581485e-02
 -2.38414202e-03 -1.79630697e-01  3.52917053e-02  2.45929193e-02
 -4.82693687e-03  1.41065955e-01  2.05586273e-02 -5.90944923e-02
 -2.15265751e-01 -1.93263799e-01 -1.96096748e-01 -6.99823573e-02
 -2.72432387e-01 -4.07939479e-02 -2.02840537e-01 -5.46442568e-01
  1.09022632e-01  2.50703335e-01 -3.16021413e-01  1.37469068e-01
 -3.01275492e-01  3.51579994e-01  6.31738361e-03  5.87034151e-02
 -7.31256008e-02  2.38423571e-01  1.61401421e-01  1.73136532e-01
  2.77411163e-01 -9.65905637e-02 -1.66874994e-02 -5.15720129e-01
 -1.45822376e-01  4.56661850e-01 -1.25083417e-01 -3.41400206e-01
  2.90560946e-02  2.45678440e-01 -3.42522323e-01 -1.92293048e-01
  3.40719447e-02  5.11357725e-01 -1.91944018e-01 -2.62108624e-01
 -1.90736111e-02  1.00837409e-01  2.33905122e-01  2.21168041e-01
  2.72862345e-01 -8.32076907e-01  8.99258479e-02  3.02359730e-01
  7.80803710e-02  1.89794973e-01 -6.43202290e-02  1.32486105e-01
  1.80142283e-01  7.20464587e-02  1.59247845e-01  2.04540521e-01
 -2.39316091e-01  9.03990418e-02 -3.53055656e-01 -1.88714743e-01
  3.92962098e-01 -2.71198869e-01  2.94973273e-02  1.88812345e-01
  1.78662121e-01  3.02931722e-02  1.75117120e-01  7.87524283e-02
 -3.51868033e-01 -9.32509154e-02 -2.05242559e-01 -2.07474783e-01
  1.19918987e-01 -2.49483898e-01 -3.16209882e-01 -3.17333460e-01
 -1.19997792e-01  2.52609491e-01  7.89215416e-02 -6.80059314e-01
  7.66594857e-02 -1.66790545e-01 -3.89458030e-01  3.06392252e-01
  8.92802030e-02  1.71041749e-02 -1.96865439e-01  5.17239831e-02
  1.84356079e-01 -1.26424149e-01  1.68676659e-01 -3.55743319e-01
 -2.06943780e-01 -4.82086502e-02 -2.46680945e-01  2.76659638e-01
  1.55978929e-03  9.04124081e-02  2.24991202e-01  2.04545125e-01
  3.00018191e-01  1.35836065e-01  3.07701081e-01 -2.63294667e-01
 -1.88695937e-01  2.06235647e-02 -5.12155667e-02  2.37670869e-01
 -3.65725040e-01 -2.61694968e-01  9.23743993e-02  1.47270977e-01
  3.06027353e-01  4.45157528e-01 -3.90374839e-01 -4.67700474e-02
 -4.25178647e-01 -5.90130687e-04 -3.21514308e-01  3.03296089e-01
  3.63275170e-01  4.58111614e-02  3.30066025e-01 -2.16069706e-02
  1.42154545e-01  1.83057293e-01  4.38790292e-01 -5.03858805e-01
  2.47998267e-01  1.63122386e-01  1.49879485e-01  7.15934813e-01
  3.37031662e-01  1.35188878e-01 -4.46389318e-01  6.86396122e-01
  1.26059085e-01 -1.72261000e-01  3.19842100e-02 -2.03756362e-01
  4.72383142e-01 -4.15431440e-01 -5.34370914e-03  3.85310724e-02
  4.28653687e-01 -1.35171294e-01 -7.48361573e-02  1.99067920e-01
 -1.50800049e-01  2.60149091e-01 -3.87372583e-01  2.14861065e-01
 -2.20202267e-01 -2.35695004e-01 -6.49145339e-03 -7.12898612e-01
 -1.39995627e-02  8.87583792e-02 -5.01427352e-01  1.31631657e-01
 -3.69601995e-02 -2.03504533e-01 -6.98846504e-02  1.80879429e-01
 -1.10381007e-01 -3.84908974e-01  9.75511372e-02  1.51539087e-01
  1.96634173e-01  1.20870955e-01  4.51063871e-01 -3.69888932e-01
 -9.01307762e-02 -1.72752067e-01  4.84322727e-01  3.68308872e-01
  6.96314499e-02 -9.03145820e-02  6.42438605e-03 -2.92884968e-02
  1.36364579e-01  4.70335335e-01  1.51434354e-02 -1.16973281e-01
 -1.13868758e-01  6.56364799e-01  1.63762882e-01  8.92142951e-03
  1.57920599e-01 -3.06050003e-01 -4.04437184e-02  2.61219800e-01
  4.01211321e-01 -2.05485523e-01  3.86468507e-02 -2.42541339e-02
 -2.90986240e-01  1.36571065e-01 -4.11621407e-02 -9.42847580e-02
 -7.74235800e-02 -7.72457123e-02  2.01182161e-02 -2.10659295e-01
 -4.08439130e-01  2.88049042e-01  1.30305784e-02 -1.16617680e-01
 -1.40570402e-01 -1.94615707e-01  8.67494866e-02 -3.86448920e-01
  2.07663774e-02 -3.09607923e-01  3.70327592e-01  5.00430346e-01
 -1.17128892e-02  5.14871255e-02 -1.46151826e-01  1.74032114e-02
 -6.59783423e-01 -7.07935467e-02 -8.10684934e-02  3.51351053e-01
 -2.22656317e-02 -3.04401577e-01  3.83597761e-01  1.20468892e-01
 -2.73843467e-01 -1.36842169e-02 -3.74930859e-01  9.84766111e-02
 -1.98282003e-01 -5.18718481e-01 -2.33686060e-01 -3.98152500e-01
  1.66209310e-01  2.52434313e-01 -2.02250496e-01  2.31180191e-01
 -2.65975416e-01  6.45830214e-01  5.94574094e-01 -3.60789657e-01
 -1.75104886e-01 -8.86576623e-03  1.37235403e-01 -8.09188262e-02
  4.77363542e-02  3.57112959e-02  7.08963498e-02  5.61473183e-02]"
Building failure of libtensorflowlite_gpu_delegate.so stat:awaiting response type:build/install comp:lite subtype: ubuntu/linux,"System information

OS Platform and Distribution: Ubuntu 20.04
TensorFlow installed from (source or binary): source
TensorFlow version: 2.12.0
Bazel version (if compiling from source): 5.3.0
GCC/Compiler version (if compiling from source): 9.4.0


Describe the problem
Building libtensorflowlite_gpu_delegate.so fails
------------------------

Provide the exact sequence of commands / steps that you executed before running into the problem

bazel build -c opt tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so --copt -DEGL_NO_X11=1

Any other info / logs

ERROR: /home/sstc/tensorflow/tensorflow/lite/delegates/gpu/BUILD:134:10: Linking tensorflow/lite/delegates/gpu/libtensorflowlite_gpu_delegate.so failed: (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/lite/delegates/gpu/libtensorflowlite_gpu_delegate.so-2.params
/usr/bin/ld: cannot find -lnativewindow
/usr/bin/ld: cannot find -lnativewindow
collect2: error: ld returned 1 exit status
Target //tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 123.625s, Critical Path: 46.21s
INFO: 804 processes: 190 internal, 614 local.
FAILED: Build did NOT complete successfully",False,"[-3.92355382e-01 -3.81194055e-01 -4.49931562e-01  1.01622686e-01
  1.69262558e-01 -9.10721198e-02 -5.86177036e-02 -5.57579659e-03
 -1.30848408e-01 -2.32041836e-01 -1.66964114e-01 -1.27505839e-01
  1.13371506e-01  1.00049734e-01 -1.48461521e-01  2.99052685e-01
 -2.31553882e-01 -1.88691020e-01  5.12041330e-01 -1.01382164e-02
 -1.34711295e-01 -1.02696031e-01 -1.42242491e-01  3.55515122e-01
  4.26684588e-01  1.37925804e-01 -7.19977766e-02  6.66212812e-02
  9.95735005e-02  1.04589745e-01  6.10457599e-01  1.15808062e-02
 -1.71344608e-01  6.40422292e-03 -5.03372699e-02  1.87282383e-01
 -2.81669140e-01 -2.45678961e-01 -9.35060829e-02  4.49097753e-02
  1.83340698e-01  2.40556866e-01  1.55348733e-01  3.72906253e-02
 -1.41570121e-02  7.46678710e-02  4.32102755e-02  8.15243572e-02
 -2.69477367e-01 -2.13713259e-01 -1.21203564e-01  1.19919091e-01
 -2.07126051e-01 -5.49459085e-02  2.15833396e-01 -3.43896821e-02
  2.00394064e-01  1.19262077e-01  4.34950460e-03  2.90064335e-01
  2.55131692e-01  1.26118762e-02 -1.83108732e-01 -4.55270708e-03
 -3.70166481e-01  2.18846798e-01  2.39469498e-01 -2.81963348e-01
  4.11684930e-01 -1.75655678e-01 -1.84296034e-02 -6.98192120e-02
 -1.58651829e-01 -1.78959370e-01  6.22699633e-02  6.06395528e-02
 -2.22749054e-01  4.39052492e-01  2.51348615e-01 -1.60840243e-01
 -6.05349690e-02 -2.41629519e-02  1.58690423e-01  7.81754255e-02
  2.61808217e-01 -1.21426985e-01  1.69986546e-01  2.78696883e-02
  3.49791169e-01 -1.91477071e-02  4.48716909e-01  3.73195887e-01
  3.49351391e-03  1.33285105e-01  1.87982127e-01  1.59441352e-01
  1.08522311e-01  1.50288582e-01  1.72400951e-01 -5.19025661e-02
 -2.71691859e-01 -1.28632579e-02 -9.10928845e-02  6.52454123e-02
 -1.37799427e-01 -2.65946656e-01  3.36110502e-01  2.30848268e-01
 -1.08063474e-01 -5.54968789e-02  1.72840744e-01 -5.45006711e-03
 -1.52554773e-02  5.53870276e-02  1.56777084e-01  3.09808731e-01
 -2.35438392e-01  1.19624987e-01 -2.54741728e-01  4.81113404e-01
 -7.89065957e-02  1.44860327e-01  9.06942263e-02  1.44718856e-01
  3.39094877e-01 -9.77681205e-03 -1.62205011e-01 -1.46405354e-01
  1.68624848e-01  5.53118624e-03  2.17816010e-02  4.05654788e-01
 -3.43355626e-01  1.88202113e-02  1.94153279e-01  5.36954626e-02
 -2.30702251e-01 -1.69198856e-01  7.84604698e-02  4.89155613e-02
 -5.05834669e-02  3.63121808e-01 -3.34048085e-02 -1.89060450e-01
  4.56622243e-03  1.53297335e-01 -6.59229159e-02  6.82508796e-02
  1.28141701e-01 -4.02642414e-04 -1.54195353e-01  1.66348983e-02
 -2.00800866e-01  3.12139809e-01  1.06882274e-01  1.95996821e-01
  3.01250219e-01 -8.66867825e-02 -3.40236202e-02 -3.29435229e-01
 -2.81125493e-03  3.52271616e-01  6.09381869e-02 -1.45132514e-02
 -1.51003689e-01  1.22151300e-01 -4.71003592e-01 -2.41630033e-01
  2.13438481e-01  2.12327167e-01 -1.18784972e-01 -6.87442869e-02
  1.73802480e-01  9.29115415e-02  3.18235308e-01 -2.35453501e-01
  4.99447495e-01 -2.83499897e-01 -5.68772964e-02  2.19119996e-01
  1.62545741e-01  2.61796743e-01  5.14451787e-02 -4.52741235e-02
 -2.07784802e-01  5.40032834e-02  1.06114037e-01 -1.94811419e-01
 -3.83777246e-02  1.26859784e-01 -7.11687922e-01  1.34049356e-01
  2.78347015e-01 -2.02195957e-01 -3.92903149e-01 -5.70688546e-02
  2.06571490e-01  8.84677619e-02  3.06700349e-01  2.26272658e-01
 -2.98674941e-01  5.41056283e-02  1.46510094e-01 -1.64769460e-02
 -9.67497230e-02  1.74854286e-02 -2.61422992e-01 -2.72970349e-01
 -3.11360389e-01 -9.70362425e-02 -7.37977102e-02 -3.49131972e-01
 -3.88321728e-02 -8.93252417e-02 -2.12804705e-01  1.11585096e-01
 -1.09130964e-01 -1.08851552e-01 -5.85666224e-02  2.01569766e-01
 -4.05831076e-03 -2.36177519e-01 -1.02352081e-02 -2.19107613e-01
 -1.74806029e-01 -8.21089074e-02  5.63016534e-02  8.36187005e-02
  1.46111041e-01  2.57470727e-01 -1.73295692e-01 -3.27746749e-01
  5.00512898e-01  1.75568998e-01  1.13048151e-01 -2.53203541e-01
 -1.99925005e-02  2.39353925e-02 -1.52522370e-01 -5.88393658e-02
 -3.03338647e-01 -4.28608745e-01  4.87787165e-02 -8.88832659e-02
  1.09039627e-01  8.44330117e-02 -1.06485918e-01 -2.15020239e-01
 -5.35155907e-02  4.11374003e-01  1.05473489e-01 -1.03995234e-01
  1.64098874e-01  2.95732021e-01  3.05557251e-01  1.98965251e-01
 -2.74906576e-01 -8.49031508e-02  7.80680776e-02 -1.24042407e-01
 -2.11874038e-01  3.56816471e-01 -3.31937432e-01  2.67733157e-01
  2.13839903e-01  1.09280363e-01 -2.70409495e-01  4.74901982e-02
  5.67369722e-02 -2.60242939e-01  4.13538903e-01 -2.46386752e-01
  3.41654330e-01 -2.07472801e-01  1.01345234e-01 -8.45783651e-02
  2.90191472e-01  1.21243671e-01 -1.17839240e-01 -1.88949015e-02
  7.87752420e-02  4.68437254e-01 -5.88844270e-02 -1.50707081e-01
  3.34380753e-02 -2.45951384e-01  9.53667015e-02 -5.38504660e-01
 -3.53072524e-01  1.39520049e-01 -1.42202377e-01  8.47871229e-02
  2.08423316e-01  6.29322529e-02 -1.80993170e-01 -7.87233412e-02
  7.65249878e-02 -1.08550355e-01  2.54648514e-02  9.92973596e-02
 -1.38098106e-01  1.71455249e-01  1.39213115e-01 -1.17021143e-01
  1.63544238e-01  3.10038403e-02  2.00790465e-01  2.30047889e-02
  5.00262022e-01 -4.35125381e-01  4.51197326e-01 -1.09422490e-01
 -1.00603290e-01  3.87844861e-01 -1.06217667e-01  5.99907339e-02
 -1.89092517e-01  2.16188103e-01  9.80431437e-02 -7.23481774e-02
  2.15947717e-01 -3.31737921e-02 -5.06349802e-01  1.08516499e-01
  1.38540745e-01 -9.46451500e-02 -2.79528499e-01 -3.70424718e-01
 -3.70255709e-01  3.14666927e-02  1.14425212e-01 -1.52985066e-01
 -6.65111374e-03  5.41443899e-02 -1.93205744e-01 -2.02640042e-01
 -3.10993254e-01  2.70614982e-01 -1.00668006e-01 -2.45484069e-01
  5.64960688e-02 -3.18303764e-01 -4.11239937e-02 -3.53822410e-01
 -4.40676749e-01 -1.13269515e-01  2.12657899e-01  1.43753454e-01
 -1.57264620e-01  5.48321307e-02  5.22486269e-02  2.19232708e-01
 -2.29289740e-01 -8.02686252e-03  3.24863717e-02  6.39023706e-02
 -2.58880377e-01  1.21239588e-01  2.32793689e-01  4.45373595e-01
 -2.01665044e-01  1.32176697e-01 -2.43469447e-01 -1.98842764e-01
  4.53270897e-02 -2.33509406e-01 -2.61369914e-01 -2.66486198e-01
  2.12641191e-02  5.28493881e-01 -2.66796127e-02  3.20023894e-01
 -4.01778251e-01  2.63423532e-01  3.73120099e-01 -7.48649836e-02
 -9.39203799e-02 -1.04799144e-01 -2.11294174e-01 -3.64319026e-01
 -2.31123254e-01 -5.02797142e-02  2.10840851e-01  1.06045604e-03]"
Cross compilation TensorFlow Lite with CMake mat error stat:awaiting response type:build/install stale comp:lite TF 2.8,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf.2.8

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

gcc-arm-10.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to get a libtensorflow.so for my Arm Linux.

### Standalone code to reproduce the issue

```shell
ARMCC_FLAGS=""-march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -mfp16-format=ieee""
cmake -DCMAKE_C_COMPILER=gcc-arm-10.3-2021.07-x86_64-arm-none-linux-gnueabihf/bin/arm-none-linux-gnueabihf-gcc \
  -DCMAKE_CXX_COMPILER=gcc-arm-10.3-2021.07-x86_64-arm-none-linux-gnueabihf/bin/arm-none-linux-gnueabihf-g++ \
  -DCMAKE_C_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_CXX_FLAGS=""${ARMCC_FLAGS}"" \
  -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \
  -DCMAKE_SYSTEM_NAME=Linux \
  -DCMAKE_SYSTEM_PROCESSOR=armv7 \
  ../tensorflow/lite/

make -j 16

then I get the Error:
w/tensorflow/build_sdk/xnnpack/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-1x16c4-minmax-neondot.c
[ 79%] Building C object _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o
cd /mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/_deps/xnnpack-build && /mnt/fileroot/guanghui.wan/nanoQ/Algorithm/tools/gcc-arm-10.3-2021.07-x86_64-arm-none-linux-gnueabihf/bin/arm-none-linux-gnueabihf-gcc -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ARM_BF16=0 -DXNN_ENABLE_ARM_DOTPROD=1 -DXNN_ENABLE_ARM_FP16_SCALAR=1 -DXNN_ENABLE_ARM_FP16_VECTOR=1 -DXNN_ENABLE_ARM_I8MM=0 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_CPUINFO=1 -DXNN_ENABLE_DWCONV_MULTIPASS=0 -DXNN_ENABLE_GEMM_M_SPECIALIZATION=1 -DXNN_ENABLE_JIT=0 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_RISCV_VECTOR=1 -DXNN_ENABLE_SPARSE=1 -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/third_party/xla/third_party/tsl -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/xnnpack/src -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/pthreadpool-source/include -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/FXdiv-source/include -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/FP16-source/include -march=armv8-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -mfp16-format=ieee -O3 -DNDEBUG -std=c99 -fPIC -Wno-psabi -O2 -pthread  -fno-math-errno  -marm  -march=armv8.2-a+dotprod -mfpu=neon-fp-armv8  -MD -MT _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o -MF CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o.d -o CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o -c /mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/xnnpack/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c
/tmp/ccrZ5z20.s: Assembler messages:
/tmp/ccrZ5z20.s:75: Error: selected processor does not support `vsdot.s8 q14,q12,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:76: Error: selected processor does not support `vsdot.s8 q10,q9,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:77: Error: selected processor does not support `vsdot.s8 q14,q11,d7[1]' in ARM mode
/tmp/ccrZ5z20.s:78: Error: selected processor does not support `vsdot.s8 q10,q8,d7[1]' in ARM mode
/tmp/ccrZ5z20.s:134: Error: selected processor does not support `vsdot.s8 q14,q9,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:135: Error: selected processor does not support `vsdot.s8 q10,q8,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49877: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-1x8c4-minmax-neondot.c.o] Error 1
make[2]: *** Waiting for unfinished jobs....
/tmp/ccWEkUHN.s: Assembler messages:
/tmp/ccWEkUHN.s:81: Error: selected processor does not support `vsdot.s8 q12,q9,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:82: Error: selected processor does not support `vsdot.s8 q13,q2,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:83: Error: selected processor does not support `vsdot.s8 q15,q8,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:85: Error: selected processor does not support `vsdot.s8 q10,q14,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:91: Error: selected processor does not support `vsdot.s8 q12,q8,d7[1]' in ARM mode
/tmp/ccWEkUHN.s:93: Error: selected processor does not support `vsdot.s8 q13,q14,d7[1]' in ARM mode
/tmp/ccWEkUHN.s:99: Error: selected processor does not support `vsdot.s8 q10,q14,d7[1]' in ARM mode
/tmp/ccWEkUHN.s:100: Error: selected processor does not support `vsdot.s8 q15,q8,d7[1]' in ARM mode
/tmp/ccWEkUHN.s:184: Error: selected processor does not support `vsdot.s8 q13,q2,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:185: Error: selected processor does not support `vsdot.s8 q12,q9,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:186: Error: selected processor does not support `vsdot.s8 q10,q14,d7[0]' in ARM mode
/tmp/ccWEkUHN.s:187: Error: selected processor does not support `vsdot.s8 q15,q8,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49905: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-1x16c4-minmax-neondot.c.o] Error 1
/tmp/ccEQXTmv.s: Assembler messages:
/tmp/ccEQXTmv.s:88: Error: selected processor does not support `vsdot.s8 q15,q11,d7[0]' in ARM mode
/tmp/ccEQXTmv.s:89: Error: selected processor does not support `vsdot.s8 q14,q9,d7[0]' in ARM mode
/tmp/ccEQXTmv.s:90: Error: selected processor does not support `vsdot.s8 q15,q10,d7[1]' in ARM mode
/tmp/ccEQXTmv.s:91: Error: selected processor does not support `vsdot.s8 q14,q8,d7[1]' in ARM mode
/tmp/ccEQXTmv.s:93: Error: selected processor does not support `vsdot.s8 q13,q11,d7[0]' in ARM mode
/tmp/ccEQXTmv.s:94: Error: selected processor does not support `vsdot.s8 q12,q9,d7[0]' in ARM mode
/tmp/ccEQXTmv.s:95: Error: selected processor does not support `vsdot.s8 q13,q10,d7[1]' in ARM mode
/tmp/ccEQXTmv.s:96: Error: selected processor does not support `vsdot.s8 q12,q8,d7[1]' in ARM mode
/tmp/ccEQXTmv.s:189: Error: selected processor does not support `vsdot.s8 q15,q9,d6[0]' in ARM mode
/tmp/ccEQXTmv.s:190: Error: selected processor does not support `vsdot.s8 q14,q8,d6[0]' in ARM mode
/tmp/ccEQXTmv.s:191: Error: selected processor does not support `vsdot.s8 q13,q9,d7[0]' in ARM mode
/tmp/ccEQXTmv.s:192: Error: selected processor does not support `vsdot.s8 q12,q8,d7[0]' in ARM mode
/tmp/ccmAlACc.s: make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49933: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-2x8c4-minmax-neondot.c.o] Error 1
Assembler messages:
/tmp/ccmAlACc.s:87: Error: selected processor does not support `vsdot.s8 q1,q10,q8' in ARM mode
/tmp/ccmAlACc.s:90: Error: selected processor does not support `vsdot.s8 q0,q5,q8' in ARM mode
/tmp/ccmAlACc.s:91: Error: selected processor does not support `vsdot.s8 q2,q9,q8' in ARM mode
/tmp/ccmAlACc.s:92: Error: selected processor does not support `vsdot.s8 q14,q4,q8' in ARM mode
/tmp/ccmAlACc.s:101: Error: selected processor does not support `vsdot.s8 q13,q5,q8' in ARM mode
/tmp/ccmAlACc.s:102: Error: selected processor does not support `vsdot.s8 q3,q10,q8' in ARM mode
/tmp/ccmAlACc.s:103: Error: selected processor does not support `vsdot.s8 q12,q4,q8' in ARM mode
/tmp/ccmAlACc.s:104: Error: selected processor does not support `vsdot.s8 q15,q9,q8' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49919: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-1x16c8-minmax-neondot-ld64.c.o] Error 1
/tmp/ccBfIgBU.s: Assembler messages:
/tmp/ccBfIgBU.s:109: Error: selected processor does not support `vsdot.s8 q14,q10,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:110: Error: selected processor does not support `vsdot.s8 q13,q9,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:111: Error: selected processor does not support `vsdot.s8 q12,q8,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:112: Error: selected processor does not support `vsdot.s8 q1,q11,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:114: Error: selected processor does not support `vsdot.s8 q0,q10,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:115: Error: selected processor does not support `vsdot.s8 q4,q9,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:116: Error: selected processor does not support `vsdot.s8 q5,q8,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:117: Error: selected processor does not support `vsdot.s8 q6,q11,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:126: Error: selected processor does not support `vsdot.s8 q14,q11,d6[1]' in ARM mode
/tmp/ccBfIgBU.s:127: Error: selected processor does not support `vsdot.s8 q0,q11,d7[1]' in ARM mode
/tmp/ccBfIgBU.s:128: Error: selected processor does not support `vsdot.s8 q13,q10,d6[1]' in ARM mode
/tmp/ccBfIgBU.s:129: Error: selected processor does not support `vsdot.s8 q4,q10,d7[1]' in ARM mode
/tmp/ccBfIgBU.s:130: Error: selected processor does not support `vsdot.s8 q12,q9,d6[1]' in ARM mode
/tmp/ccBfIgBU.s:131: Error: selected processor does not support `vsdot.s8 q1,q8,d6[1]' in ARM mode
/tmp/ccBfIgBU.s:132: Error: selected processor does not support `vsdot.s8 q5,q9,d7[1]' in ARM mode
/tmp/ccBfIgBU.s:133: Error: selected processor does not support `vsdot.s8 q6,q8,d7[1]' in ARM mode
/tmp/ccBfIgBU.s:255: Error: selected processor does not support `vsdot.s8 q13,q9,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:256: Error: selected processor does not support `vsdot.s8 q4,q9,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:258: Error: selected processor does not support `vsdot.s8 q14,q10,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:260: Error: selected processor does not support `vsdot.s8 q0,q10,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:262: Error: selected processor does not support `vsdot.s8 q12,q9,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:263: Error: selected processor does not support `vsdot.s8 q5,q9,d7[0]' in ARM mode
/tmp/ccBfIgBU.s:264: Error: selected processor does not support `vsdot.s8 q1,q8,d6[0]' in ARM mode
/tmp/ccBfIgBU.s:265: Error: selected processor does not support `vsdot.s8 q6,q8,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49947: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-2x16c4-minmax-neondot.c.o] Error 1
/tmp/ccDJj1kS.s: Assembler messages:
/tmp/ccDJj1kS.s:75: Error: selected processor does not support `vsdot.s8 q15,q12,q10' in ARM mode
/tmp/ccDJj1kS.s:76: Error: selected processor does not support `vsdot.s8 q2,q9,q10' in ARM mode
/tmp/ccDJj1kS.s:77: Error: selected processor does not support `vsdot.s8 q13,q11,q10' in ARM mode
/tmp/ccDJj1kS.s:78: Error: selected processor does not support `vsdot.s8 q3,q8,q10' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49891: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-1x8c8-minmax-neondot-ld64.c.o] Error 1
/tmp/ccRqcaMJ.s: Assembler messages:
/tmp/ccRqcaMJ.s:143: Error: selected processor does not support `vsdot.s8 q1,q14,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:146: Error: selected processor does not support `vsdot.s8 q5,q14,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:147: Error: selected processor does not support `vsdot.s8 q15,q14,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:149: Error: selected processor does not support `vsdot.s8 q6,q7,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:151: Error: selected processor does not support `vsdot.s8 q4,q7,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:152: Error: selected processor does not support `vsdot.s8 q0,q7,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:158: Error: selected processor does not support `vsdot.s8 q7,q14,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:159: Error: selected processor does not support `vsdot.s8 q13,q10,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:160: Error: selected processor does not support `vsdot.s8 q12,q14,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:162: Error: selected processor does not support `vsdot.s8 q11,q14,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:163: Error: selected processor does not support `vsdot.s8 q10,q1,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:166: Error: selected processor does not support `vsdot.s8 q14,q1,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:170: Error: selected processor does not support `vsdot.s8 q6,q1,d5[1]' in ARM mode
/tmp/ccRqcaMJ.s:171: Error: selected processor does not support `vsdot.s8 q5,q9,d5[1]' in ARM mode
/tmp/ccRqcaMJ.s:172: Error: selected processor does not support `vsdot.s8 q4,q1,d6[1]' in ARM mode
/tmp/ccRqcaMJ.s:173: Error: selected processor does not support `vsdot.s8 q0,q1,d7[1]' in ARM mode
/tmp/ccRqcaMJ.s:174: Error: selected processor does not support `vsdot.s8 q15,q9,d7[1]' in ARM mode
/tmp/ccRqcaMJ.s:177: Error: selected processor does not support `vsdot.s8 q1,q9,d6[1]' in ARM mode
/tmp/ccRqcaMJ.s:186: Error: selected processor does not support `vsdot.s8 q7,q9,d5[1]' in ARM mode
/tmp/ccRqcaMJ.s:187: Error: selected processor does not support `vsdot.s8 q10,q8,d6[1]' in ARM mode
/tmp/ccRqcaMJ.s:188: Error: selected processor does not support `vsdot.s8 q14,q8,d7[1]' in ARM mode
/tmp/ccRqcaMJ.s:191: Error: selected processor does not support `vsdot.s8 q13,q8,d5[1]' in ARM mode
/tmp/ccRqcaMJ.s:192: Error: selected processor does not support `vsdot.s8 q12,q9,d6[1]' in ARM mode
/tmp/ccRqcaMJ.s:193: Error: selected processor does not support `vsdot.s8 q11,q9,d7[1]' in ARM mode
/tmp/ccRqcaMJ.s:453: Error: selected processor does not support `vsdot.s8 q6,q9,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:454: Error: selected processor does not support `vsdot.s8 q5,q8,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:456: Error: selected processor does not support `vsdot.s8 q4,q9,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:457: Error: selected processor does not support `vsdot.s8 q0,q9,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:458: Error: selected processor does not support `vsdot.s8 q1,q8,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:460: Error: selected processor does not support `vsdot.s8 q15,q8,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:467: Error: selected processor does not support `vsdot.s8 q10,q9,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:468: Error: selected processor does not support `vsdot.s8 q12,q9,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:469: Error: selected processor does not support `vsdot.s8 q11,q9,d5[0]' in ARM mode
/tmp/ccRqcaMJ.s:472: Error: selected processor does not support `vsdot.s8 q13,q8,d7[0]' in ARM mode
/tmp/ccRqcaMJ.s:474: Error: selected processor does not support `vsdot.s8 q9,q8,d6[0]' in ARM mode
/tmp/ccRqcaMJ.s:478: Error: selected processor does not support `vsdot.s8 q9,q8,d5[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49961: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-3x16c4-minmax-neondot.c.o] Error 1
/tmp/ccuyJ4D9.s: Assembler messages:
/tmp/ccuyJ4D9.s:64: Error: selected processor does not support `vsdot.s8 q9,q11,d7[0]' in ARM mode
/tmp/ccuyJ4D9.s:66: Error: selected processor does not support `vsdot.s8 q8,q10,d7[0]' in ARM mode
/tmp/ccuyJ4D9.s:69: Error: selected processor does not support `vsdot.s8 q9,q10,d7[1]' in ARM mode
/tmp/ccuyJ4D9.s:73: Error: selected processor does not support `vsdot.s8 q8,q10,d7[1]' in ARM mode
/tmp/ccuyJ4D9.s:130: Error: selected processor does not support `vsdot.s8 q9,q11,d7[0]' in ARM mode
/tmp/ccuyJ4D9.s:132: Error: selected processor does not support `vsdot.s8 q8,q10,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50031: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-1x8c4-minmax-neondot.c.o] Error 1
/tmp/ccRNmUcC.s: Assembler messages:
/tmp/ccRNmUcC.s:110: Error: selected processor does not support `vsdot.s8 q4,q11,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:111: Error: selected processor does not support `vsdot.s8 q2,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:112: Error: selected processor does not support `vsdot.s8 q4,q10,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:113: Error: selected processor does not support `vsdot.s8 q2,q8,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:116: Error: selected processor does not support `vsdot.s8 q0,q11,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:117: Error: selected processor does not support `vsdot.s8 q15,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:118: Error: selected processor does not support `vsdot.s8 q0,q10,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:119: Error: selected processor does not support `vsdot.s8 q15,q8,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:123: Error: selected processor does not support `vsdot.s8 q1,q11,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:124: Error: selected processor does not support `vsdot.s8 q14,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:125: Error: selected processor does not support `vsdot.s8 q1,q10,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:126: Error: selected processor does not support `vsdot.s8 q14,q8,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:129: Error: selected processor does not support `vsdot.s8 q13,q11,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:130: Error: selected processor does not support `vsdot.s8 q12,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:131: Error: selected processor does not support `vsdot.s8 q13,q10,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:132: Error: selected processor does not support `vsdot.s8 q12,q8,d7[1]' in ARM mode
/tmp/ccRNmUcC.s:300: Error: selected processor does not support `vsdot.s8 q4,q10,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:301: Error: selected processor does not support `vsdot.s8 q2,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:303: Error: selected processor does not support `vsdot.s8 q0,q10,d6[0]' in ARM mode
/tmp/ccRNmUcC.s:304: Error: selected processor does not support `vsdot.s8 q15,q9,d6[0]' in ARM mode
/tmp/ccRNmUcC.s:305: Error: selected processor does not support `vsdot.s8 q1,q10,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:307: Error: selected processor does not support `vsdot.s8 q13,q10,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:309: Error: selected processor does not support `vsdot.s8 q14,q9,d7[0]' in ARM mode
/tmp/ccRNmUcC.s:311: Error: selected processor does not support `vsdot.s8 q12,q9,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49975: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-4x8c4-minmax-neondot.c.o] Error 1
/tmp/ccNllqu0.s: Assembler messages:
/tmp/ccNllqu0.s:176: Error: selected processor does not support `vsdot.s8 q6,q11,d4[0]' in ARM mode
/tmp/ccNllqu0.s:177: Error: selected processor does not support `vsdot.s8 q4,q11,d5[0]' in ARM mode
/tmp/ccNllqu0.s:178: Error: selected processor does not support `vsdot.s8 q15,q11,d6[0]' in ARM mode
/tmp/ccNllqu0.s:179: Error: selected processor does not support `vsdot.s8 q14,q11,d7[0]' in ARM mode
/tmp/ccNllqu0.s:183: Error: selected processor does not support `vsdot.s8 q7,q12,d4[0]' in ARM mode
/tmp/ccNllqu0.s:184: Error: selected processor does not support `vsdot.s8 q5,q12,d5[0]' in ARM mode
/tmp/ccNllqu0.s:186: Error: selected processor does not support `vsdot.s8 q0,q12,d6[0]' in ARM mode
/tmp/ccNllqu0.s:189: Error: selected processor does not support `vsdot.s8 q1,q12,d7[0]' in ARM mode
/tmp/ccNllqu0.s:195: Error: selected processor does not support `vsdot.s8 q9,q11,d5[0]' in ARM mode
/tmp/ccNllqu0.s:196: Error: selected processor does not support `vsdot.s8 q8,q11,d7[0]' in ARM mode
/tmp/ccNllqu0.s:199: Error: selected processor does not support `vsdot.s8 q13,q11,d4[0]' in ARM mode
/tmp/ccNllqu0.s:200: Error: selected processor does not support `vsdot.s8 q12,q10,d4[0]' in ARM mode
/tmp/ccNllqu0.s:203: Error: selected processor does not support `vsdot.s8 q9,q10,d5[0]' in ARM mode
/tmp/ccNllqu0.s:211: Error: selected processor does not support `vsdot.s8 q9,q11,d6[0]' in ARM mode
/tmp/ccNllqu0.s:215: Error: selected processor does not support `vsdot.s8 q9,q10,d7[0]' in ARM mode
/tmp/ccNllqu0.s:221: Error: selected processor does not support `vsdot.s8 q11,q10,d6[0]' in ARM mode
/tmp/ccNllqu0.s:225: Error: selected processor does not support `vsdot.s8 q7,q10,d4[1]' in ARM mode
/tmp/ccNllqu0.s:226: Error: selected processor does not support `vsdot.s8 q5,q10,d5[1]' in ARM mode
/tmp/ccNllqu0.s:227: Error: selected processor does not support `vsdot.s8 q0,q10,d6[1]' in ARM mode
/tmp/ccNllqu0.s:228: Error: selected processor does not support `vsdot.s8 q1,q10,d7[1]' in ARM mode
/tmp/ccNllqu0.s:231: Error: selected processor does not support `vsdot.s8 q6,q9,d4[1]' in ARM mode
/tmp/ccNllqu0.s:232: Error: selected processor does not support `vsdot.s8 q4,q9,d5[1]' in ARM mode
/tmp/ccNllqu0.s:233: Error: selected processor does not support `vsdot.s8 q15,q9,d6[1]' in ARM mode
/tmp/ccNllqu0.s:234: Error: selected processor does not support `vsdot.s8 q14,q9,d7[1]' in ARM mode
/tmp/ccNllqu0.s:240: Error: selected processor does not support `vsdot.s8 q10,q9,d5[1]' in ARM mode
/tmp/ccNllqu0.s:241: Error: selected processor does not support `vsdot.s8 q13,q9,d4[1]' in ARM mode
/tmp/ccNllqu0.s:244: Error: selected processor does not support `vsdot.s8 q12,q8,d4[1]' in ARM mode
/tmp/ccNllqu0.s:245: Error: selected processor does not support `vsdot.s8 q11,q8,d6[1]' in ARM mode
/tmp/ccNllqu0.s:248: Error: selected processor does not support `vsdot.s8 q10,q8,d5[1]' in ARM mode
/tmp/ccNllqu0.s:255: Error: selected processor does not support `vsdot.s8 q10,q9,d6[1]' in ARM mode
/tmp/ccNllqu0.s:260: Error: selected processor does not support `vsdot.s8 q10,q9,d7[1]' in ARM mode
/tmp/ccNllqu0.s:264: Error: selected processor does not support `vsdot.s8 q9,q8,d7[1]' in ARM mode
/tmp/ccNllqu0.s:658: Error: selected processor does not support `vsdot.s8 q7,q9,d7[0]' in ARM mode
/tmp/ccNllqu0.s:659: Error: selected processor does not support `vsdot.s8 q6,q8,d7[0]' in ARM mode
/tmp/ccNllqu0.s:660: Error: selected processor does not support `vsdot.s8 q5,q9,d6[0]' in ARM mode
/tmp/ccNllqu0.s:661: Error: selected processor does not support `vsdot.s8 q4,q8,d6[0]' in ARM mode
/tmp/ccNllqu0.s:665: Error: selected processor does not support `vsdot.s8 q0,q9,d5[0]' in ARM mode
/tmp/ccNllqu0.s:667: Error: selected processor does not support `vsdot.s8 q15,q8,d5[0]' in ARM mode
/tmp/ccNllqu0.s:669: Error: selected processor does not support `vsdot.s8 q1,q9,d4[0]' in ARM mode
/tmp/ccNllqu0.s:670: Error: selected processor does not support `vsdot.s8 q14,q8,d4[0]' in ARM mode
/tmp/ccNllqu0.s:678: Error: selected processor does not support `vsdot.s8 q10,q8,d7[0]' in ARM mode
/tmp/ccNllqu0.s:679: Error: selected processor does not support `vsdot.s8 q13,q9,d7[0]' in ARM mode
/tmp/ccNllqu0.s:684: Error: selected processor does not support `vsdot.s8 q10,q9,d6[0]' in ARM mode
/tmp/ccNllqu0.s:689: Error: selected processor does not support `vsdot.s8 q10,q8,d6[0]' in ARM mode
/tmp/ccNllqu0.s:694: Error: selected processor does not support `vsdot.s8 q10,q9,d5[0]' in ARM mode
/tmp/ccNllqu0.s:699: Error: selected processor does not support `vsdot.s8 q10,q9,d4[0]' in ARM mode
/tmp/ccNllqu0.s:704: Error: selected processor does not support `vsdot.s8 q9,q8,d5[0]' in ARM mode
/tmp/ccNllqu0.s:708: Error: selected processor does not support `vsdot.s8 q9,q8,d4[0]' in ARM mode
/tmp/ccWKfZvp.s: Assembler messages:
/tmp/ccWKfZvp.s:65: Error: selected processor does not support `vsdot.s8 q10,q14,q8' in ARM mode
/tmp/ccWKfZvp.s:68: Error: selected processor does not support `vsdot.s8 q12,q13,q8' in ARM mode
/tmp/ccWKfZvp.s:71: Error: selected processor does not support `vsdot.s8 q9,q13,q8' in ARM mode
/tmp/ccWKfZvp.s:75: Error: selected processor does not support `vsdot.s8 q11,q13,q8' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49989: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-4x16c4-minmax-neondot.c.o] Error 1
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50045: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-1x8c8-minmax-neondot-ld64.c.o] Error 1
/tmp/ccw5qRTa.s: Assembler messages:
/tmp/ccw5qRTa.s:183: Error: selected processor does not support `vsdot.s8 q6,q7,d2[0]' in ARM mode
/tmp/ccw5qRTa.s:184: Error: selected processor does not support `vsdot.s8 q4,q7,d3[0]' in ARM mode
/tmp/ccw5qRTa.s:185: Error: selected processor does not support `vsdot.s8 q10,q7,d4[0]' in ARM mode
/tmp/ccw5qRTa.s:186: Error: selected processor does not support `vsdot.s8 q15,q7,d5[0]' in ARM mode
/tmp/ccw5qRTa.s:188: Error: selected processor does not support `vsdot.s8 q11,q7,d6[0]' in ARM mode
/tmp/ccw5qRTa.s:189: Error: selected processor does not support `vsdot.s8 q5,q9,d2[0]' in ARM mode
/tmp/ccw5qRTa.s:190: Error: selected processor does not support `vsdot.s8 q0,q9,d3[0]' in ARM mode
/tmp/ccw5qRTa.s:193: Error: selected processor does not support `vsdot.s8 q10,q9,d4[0]' in ARM mode
/tmp/ccw5qRTa.s:196: Error: selected processor does not support `vsdot.s8 q12,q9,d5[0]' in ARM mode
/tmp/ccw5qRTa.s:197: Error: selected processor does not support `vsdot.s8 q14,q9,d6[0]' in ARM mode
/tmp/ccw5qRTa.s:198: Error: selected processor does not support `vsdot.s8 q13,q9,d7[0]' in ARM mode
/tmp/ccw5qRTa.s:201: Error: selected processor does not support `vsdot.s8 q10,q7,d7[0]' in ARM mode
/tmp/ccw5qRTa.s:206: Error: selected processor does not support `vsdot.s8 q6,q9,d2[1]' in ARM mode
/tmp/ccw5qRTa.s:207: Error: selected processor does not support `vsdot.s8 q5,q8,d2[1]' in ARM mode
/tmp/ccw5qRTa.s:208: Error: selected processor does not support `vsdot.s8 q4,q9,d3[1]' in ARM mode
/tmp/ccw5qRTa.s:209: Error: selected processor does not support `vsdot.s8 q0,q8,d3[1]' in ARM mode
/tmp/ccw5qRTa.s:210: Error: selected processor does not support `vsdot.s8 q10,q9,d7[1]' in ARM mode
/tmp/ccw5qRTa.s:212: Error: selected processor does not support `vsdot.s8 q15,q9,d5[1]' in ARM mode
/tmp/ccw5qRTa.s:213: Error: selected processor does not support `vsdot.s8 q1,q9,d4[1]' in ARM mode
/tmp/ccw5qRTa.s:214: Error: selected processor does not support `vsdot.s8 q12,q8,d5[1]' in ARM mode
/tmp/ccw5qRTa.s:216: Error: selected processor does not support `vsdot.s8 q11,q9,d6[1]' in ARM mode
/tmp/ccw5qRTa.s:217: Error: selected processor does not support `vsdot.s8 q14,q8,d6[1]' in ARM mode
/tmp/ccw5qRTa.s:220: Error: selected processor does not support `vsdot.s8 q13,q8,d7[1]' in ARM mode
/tmp/ccw5qRTa.s:221: Error: selected processor does not support `vsdot.s8 q1,q8,d4[1]' in ARM mode
/tmp/ccw5qRTa.s:513: Error: selected processor does not support `vsdot.s8 q6,q9,d7[0]' in ARM mode
/tmp/ccw5qRTa.s:514: Error: selected processor does not support `vsdot.s8 q5,q8,d7[0]' in ARM mode
/tmp/ccw5qRTa.s:515: Error: selected processor does not support `vsdot.s8 q4,q9,d6[0]' in ARM mode
/tmp/ccw5qRTa.s:516: Error: selected processor does not support `vsdot.s8 q0,q8,d6[0]' in ARM mode
/tmp/ccw5qRTa.s:517: Error: selected processor does not support `vsdot.s8 q15,q9,d4[0]' in ARM mode
/tmp/ccw5qRTa.s:519: Error: selected processor does not support `vsdot.s8 q12,q8,d4[0]' in ARM mode
/tmp/ccw5qRTa.s:520: Error: selected processor does not support `vsdot.s8 q3,q9,d5[0]' in ARM mode
/tmp/ccw5qRTa.s:521: Error: selected processor does not support `vsdot.s8 q11,q9,d3[0]' in ARM mode
/tmp/ccw5qRTa.s:523: Error: selected processor does not support `vsdot.s8 q10,q9,d2[0]' in ARM mode
/tmp/ccw5qRTa.s:524: Error: selected processor does not support `vsdot.s8 q14,q8,d3[0]' in ARM mode
/tmp/ccw5qRTa.s:527: Error: selected processor does not support `vsdot.s8 q13,q8,d2[0]' in ARM mode
/tmp/ccw5qRTa.s:528: Error: selected processor does not support `vsdot.s8 q3,q8,d5[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50003: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-6x8c4-minmax-neondot.c.o] Error 1
/tmp/cczsG4jB.s: Assembler messages:
/tmp/cczsG4jB.s:76: Error: selected processor does not support `vsdot.s8 q12,q2,q8' in ARM mode
/tmp/cczsG4jB.s:79: Error: selected processor does not support `vsdot.s8 q3,q2,q8' in ARM mode
/tmp/cczsG4jB.s:82: Error: selected processor does not support `vsdot.s8 q11,q2,q8' in ARM mode
/tmp/cczsG4jB.s:85: Error: selected processor does not support `vsdot.s8 q15,q2,q8' in ARM mode
/tmp/cczsG4jB.s:88: Error: selected processor does not support `vsdot.s8 q10,q2,q8' in ARM mode
/tmp/cczsG4jB.s:91: Error: selected processor does not support `vsdot.s8 q14,q2,q8' in ARM mode
/tmp/cczsG4jB.s:94: Error: selected processor does not support `vsdot.s8 q9,q2,q8' in ARM mode
/tmp/cczsG4jB.s:98: Error: selected processor does not support `vsdot.s8 q13,q2,q8' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50073: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-1x16c8-minmax-neondot-ld64.c.o] Error 1
/tmp/cchtznlO.s: Assembler messages:
/tmp/cchtznlO.s:236: Error: selected processor does not support `vsdot.s8 q7,q12,d2[0]' in ARM mode
/tmp/cchtznlO.s:238: Error: selected processor does not support `vsdot.s8 q6,q11,d2[0]' in ARM mode
/tmp/cchtznlO.s:240: Error: selected processor does not support `vsdot.s8 q5,q12,d3[0]' in ARM mode
/tmp/cchtznlO.s:242: Error: selected processor does not support `vsdot.s8 q4,q11,d3[0]' in ARM mode
/tmp/cchtznlO.s:245: Error: selected processor does not support `vsdot.s8 q0,q12,d4[0]' in ARM mode
/tmp/cchtznlO.s:246: Error: selected processor does not support `vsdot.s8 q9,q11,d4[0]' in ARM mode
/tmp/cchtznlO.s:247: Error: selected processor does not support `vsdot.s8 q14,q12,d5[0]' in ARM mode
/tmp/cchtznlO.s:250: Error: selected processor does not support `vsdot.s8 q13,q11,d5[0]' in ARM mode
/tmp/cchtznlO.s:251: Error: selected processor does not support `vsdot.s8 q15,q11,d7[0]' in ARM mode
/tmp/cchtznlO.s:255: Error: selected processor does not support `vsdot.s8 q9,q12,d6[0]' in ARM mode
/tmp/cchtznlO.s:261: Error: selected processor does not support `vsdot.s8 q9,q12,d7[0]' in ARM mode
/tmp/cchtznlO.s:267: Error: selected processor does not support `vsdot.s8 q12,q11,d6[0]' in ARM mode
/tmp/cchtznlO.s:274: Error: selected processor does not support `vsdot.s8 q9,q11,d2[0]' in ARM mode
/tmp/cchtznlO.s:275: Error: selected processor does not support `vsdot.s8 q8,q10,d3[0]' in ARM mode
/tmp/cchtznlO.s:281: Error: selected processor does not support `vsdot.s8 q9,q10,d2[0]' in ARM mode
/tmp/cchtznlO.s:286: Error: selected processor does not support `vsdot.s8 q9,q11,d3[0]' in ARM mode
/tmp/cchtznlO.s:291: Error: selected processor does not support `vsdot.s8 q9,q11,d4[0]' in ARM mode
/tmp/cchtznlO.s:296: Error: selected processor does not support `vsdot.s8 q8,q10,d4[0]' in ARM mode
/tmp/cchtznlO.s:301: Error: selected processor does not support `vsdot.s8 q9,q11,d5[0]' in ARM mode
/tmp/cchtznlO.s:306: Error: selected processor does not support `vsdot.s8 q8,q10,d5[0]' in ARM mode
/tmp/cchtznlO.s:311: Error: selected processor does not support `vsdot.s8 q9,q11,d6[0]' in ARM mode
/tmp/cchtznlO.s:316: Error: selected processor does not support `vsdot.s8 q8,q11,d7[0]' in ARM mode
/tmp/cchtznlO.s:321: Error: selected processor does not support `vsdot.s8 q9,q10,d7[0]' in ARM mode
/tmp/cchtznlO.s:329: Error: selected processor does not support `vsdot.s8 q11,q10,d6[0]' in ARM mode
/tmp/cchtznlO.s:335: Error: selected processor does not support `vsdot.s8 q8,q9,d4[1]' in ARM mode
/tmp/cchtznlO.s:336: Error: selected processor does not support `vsdot.s8 q7,q10,d2[1]' in ARM mode
/tmp/cchtznlO.s:339: Error: selected processor does not support `vsdot.s8 q5,q10,d3[1]' in ARM mode
/tmp/cchtznlO.s:340: Error: selected processor does not support `vsdot.s8 q0,q10,d4[1]' in ARM mode
/tmp/cchtznlO.s:341: Error: selected processor does not support `vsdot.s8 q14,q10,d5[1]' in ARM mode
/tmp/cchtznlO.s:344: Error: selected processor does not support `vsdot.s8 q8,q10,d6[1]' in ARM mode
/tmp/cchtznlO.s:347: Error: selected processor does not support `vsdot.s8 q12,q9,d6[1]' in ARM mode
/tmp/cchtznlO.s:348: Error: selected processor does not support `vsdot.s8 q6,q9,d2[1]' in ARM mode
/tmp/cchtznlO.s:349: Error: selected processor does not support `vsdot.s8 q4,q9,d3[1]' in ARM mode
/tmp/cchtznlO.s:352: Error: selected processor does not support `vsdot.s8 q8,q10,d7[1]' in ARM mode
/tmp/cchtznlO.s:353: Error: selected processor does not support `vsdot.s8 q13,q9,d5[1]' in ARM mode
/tmp/cchtznlO.s:356: Error: selected processor does not support `vsdot.s8 q15,q9,d7[1]' in ARM mode
/tmp/cchtznlO.s:367: Error: selected processor does not support `vsdot.s8 q10,q12,d2[1]' in ARM mode
/tmp/cchtznlO.s:368: Error: selected processor does not support `vsdot.s8 q11,q8,d6[1]' in ARM mode
/tmp/cchtznlO.s:372: Error: selected processor does not support `vsdot.s8 q10,q8,d2[1]' in ARM mode
/tmp/cchtznlO.s:377: Error: selected processor does not support `vsdot.s8 q10,q12,d3[1]' in ARM mode
/tmp/cchtznlO.s:382: Error: selected processor does not support `vsdot.s8 q10,q8,d3[1]' in ARM mode
/tmp/cchtznlO.s:387: Error: selected processor does not support `vsdot.s8 q10,q12,d4[1]' in ARM mode
/tmp/cchtznlO.s:392: Error: selected processor does not support `vsdot.s8 q10,q8,d4[1]' in ARM mode
/tmp/cchtznlO.s:397: Error: selected processor does not support `vsdot.s8 q9,q8,d7[1]' in ARM mode
/tmp/cchtznlO.s:404: Error: selected processor does not support `vsdot.s8 q10,q12,d5[1]' in ARM mode
/tmp/cchtznlO.s:409: Error: selected processor does not support `vsdot.s8 q10,q8,d5[1]' in ARM mode
/tmp/cchtznlO.s:414: Error: selected processor does not support `vsdot.s8 q10,q12,d6[1]' in ARM mode
/tmp/cchtznlO.s:419: Error: selected processor does not support `vsdot.s8 q10,q12,d7[1]' in ARM mode
/tmp/cchtznlO.s:1117: Error: selected processor does not support `vsdot.s8 q7,q9,d7[0]' in ARM mode
/tmp/cchtznlO.s:1118: Error: selected processor does not support `vsdot.s8 q6,q8,d7[0]' in ARM mode
/tmp/cchtznlO.s:1119: Error: selected processor does not support `vsdot.s8 q5,q9,d6[0]' in ARM mode
/tmp/cchtznlO.s:1120: Error: selected processor does not support `vsdot.s8 q4,q8,d6[0]' in ARM mode
/tmp/cchtznlO.s:1124: Error: selected processor does not support `vsdot.s8 q12,q8,d5[0]' in ARM mode
/tmp/cchtznlO.s:1126: Error: selected processor does not support `vsdot.s8 q0,q9,d5[0]' in ARM mode
/tmp/cchtznlO.s:1134: Error: selected processor does not support `vsdot.s8 q14,q9,d4[0]' in ARM mode
/tmp/cchtznlO.s:1136: Error: selected processor does not support `vsdot.s8 q13,q8,d4[0]' in ARM mode
/tmp/cchtznlO.s:1140: Error: selected processor does not support `vsdot.s8 q12,q9,d3[0]' in ARM mode
/tmp/cchtznlO.s:1147: Error: selected processor does not support `vsdot.s8 q12,q9,d2[0]' in ARM mode
/tmp/cchtznlO.s:1148: Error: selected processor does not support `vsdot.s8 q15,q8,d2[0]' in ARM mode
/tmp/cchtznlO.s:1151: Error: selected processor does not support `vsdot.s8 q9,q8,d3[0]' in ARM mode
/tmp/cchtznlO.s:1162: Error: selected processor does not support `vsdot.s8 q10,q9,d7[0]' in ARM mode
/tmp/cchtznlO.s:1166: Error: selected processor does not support `vsdot.s8 q10,q8,d7[0]' in ARM mode
/tmp/cchtznlO.s:1171: Error: selected processor does not support `vsdot.s8 q10,q9,d6[0]' in ARM mode
/tmp/cchtznlO.s:1176: Error: selected processor does not support `vsdot.s8 q10,q8,d6[0]' in ARM mode
/tmp/cchtznlO.s:1181: Error: selected processor does not support `vsdot.s8 q10,q9,d5[0]' in ARM mode
/tmp/cchtznlO.s:1186: Error: selected processor does not support `vsdot.s8 q10,q8,d5[0]' in ARM mode
/tmp/cchtznlO.s:1191: Error: selected processor does not support `vsdot.s8 q10,q9,d4[0]' in ARM mode
/tmp/cchtznlO.s:1196: Error: selected processor does not support `vsdot.s8 q10,q8,d4[0]' in ARM mode
/tmp/cchtznlO.s:1201: Error: selected processor does not support `vsdot.s8 q10,q9,d3[0]' in ARM mode
/tmp/cchtznlO.s:1206: Error: selected processor does not support `vsdot.s8 q10,q9,d2[0]' in ARM mode
/tmp/cchtznlO.s:1211: Error: selected processor does not support `vsdot.s8 q9,q8,d3[0]' in ARM mode
/tmp/cchtznlO.s:1216: Error: selected processor does not support `vsdot.s8 q9,q8,d2[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50017: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-6x16c4-minmax-neondot.c.o] Error 1
/tmp/ccJrNOda.s: Assembler messages:
/tmp/ccJrNOda.s:72: Error: selected processor does not support `vsdot.s8 q8,q13,d7[0]' in ARM mode
/tmp/ccJrNOda.s:74: Error: selected processor does not support `vsdot.s8 q11,q12,d7[0]' in ARM mode
/tmp/ccJrNOda.s:77: Error: selected processor does not support `vsdot.s8 q10,q12,d7[0]' in ARM mode
/tmp/ccJrNOda.s:80: Error: selected processor does not support `vsdot.s8 q9,q12,d7[0]' in ARM mode
/tmp/ccJrNOda.s:83: Error: selected processor does not support `vsdot.s8 q8,q12,d7[1]' in ARM mode
/tmp/ccJrNOda.s:86: Error: selected processor does not support `vsdot.s8 q11,q12,d7[1]' in ARM mode
/tmp/ccJrNOda.s:89: Error: selected processor does not support `vsdot.s8 q10,q12,d7[1]' in ARM mode
/tmp/ccJrNOda.s:93: Error: selected processor does not support `vsdot.s8 q9,q12,d7[1]' in ARM mode
/tmp/ccJrNOda.s:171: Error: selected processor does not support `vsdot.s8 q8,q13,d7[0]' in ARM mode
/tmp/ccJrNOda.s:173: Error: selected processor does not support `vsdot.s8 q11,q12,d7[0]' in ARM mode
/tmp/ccJrNOda.s:176: Error: selected processor does not support `vsdot.s8 q10,q12,d7[0]' in ARM mode
/tmp/ccJrNOda.s:180: Error: selected processor does not support `vsdot.s8 q9,q12,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50059: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-1x16c4-minmax-neondot.c.o] Error 1
/tmp/cceW7UTH.s: Assembler messages:
/tmp/cceW7UTH.s:78: Error: selected processor does not support `vsdot.s8 q10,q11,d6[0]' in ARM mode
/tmp/cceW7UTH.s:80: Error: selected processor does not support `vsdot.s8 q13,q12,d6[0]' in ARM mode
/tmp/cceW7UTH.s:84: Error: selected processor does not support `vsdot.s8 q9,q11,d7[0]' in ARM mode
/tmp/cceW7UTH.s:85: Error: selected processor does not support `vsdot.s8 q8,q12,d7[0]' in ARM mode
/tmp/cceW7UTH.s:87: Error: selected processor does not support `vsdot.s8 q13,q11,d6[1]' in ARM mode
/tmp/cceW7UTH.s:89: Error: selected processor does not support `vsdot.s8 q8,q11,d7[1]' in ARM mode
/tmp/cceW7UTH.s:92: Error: selected processor does not support `vsdot.s8 q10,q11,d6[1]' in ARM mode
/tmp/cceW7UTH.s:93: Error: selected processor does not support `vsdot.s8 q9,q11,d7[1]' in ARM mode
/tmp/cceW7UTH.s:168: Error: selected processor does not support `vsdot.s8 q13,q12,d6[0]' in ARM mode
/tmp/cceW7UTH.s:169: Error: selected processor does not support `vsdot.s8 q8,q12,d7[0]' in ARM mode
/tmp/cceW7UTH.s:171: Error: selected processor does not support `vsdot.s8 q10,q11,d6[0]' in ARM mode
/tmp/cceW7UTH.s:172: Error: selected processor does not support `vsdot.s8 q9,q11,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:50087: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o] Error 1
make[2]: Leaving directory '/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk'
make[1]: *** [CMakeFiles/Makefile2:6654: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/all] Error 2
make[1]: Leaving directory '/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk'
make: *** [Makefile:139: all] Error 2
guanghui.wan@rd01:/mnt/fileroot/guanghui.wan/tensorflow/tensorflow$
```


### Relevant log output

```shell
[ 79%] Building C object _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o
cd /mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/_deps/xnnpack-build && /mnt/fileroot/guanghui.wan/nanoQ/Algorithm/tools/gcc-arm-10.3-2021.07-x86_64-arm-none-linux-gnueabihf/bin/arm-none-linux-gnueabihf-gcc -DEIGEN_MPL2_ONLY -DFXDIV_USE_INLINE_ASSEMBLY=0 -DNOMINMAX=1 -DPTHREADPOOL_NO_DEPRECATED_API=1 -DXNN_ENABLE_ARM_BF16=0 -DXNN_ENABLE_ARM_DOTPROD=1 -DXNN_ENABLE_ARM_FP16_SCALAR=1 -DXNN_ENABLE_ARM_FP16_VECTOR=1 -DXNN_ENABLE_ARM_I8MM=0 -DXNN_ENABLE_ASSEMBLY=1 -DXNN_ENABLE_CPUINFO=1 -DXNN_ENABLE_DWCONV_MULTIPASS=0 -DXNN_ENABLE_GEMM_M_SPECIALIZATION=1 -DXNN_ENABLE_JIT=0 -DXNN_ENABLE_MEMOPT=1 -DXNN_ENABLE_RISCV_VECTOR=1 -DXNN_ENABLE_SPARSE=1 -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/third_party/xla/third_party/tsl -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/xnnpack/src -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/pthreadpool-source/include -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/FXdiv-source/include -I/mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/FP16-source/include -march=armv8-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -mfp16-format=ieee -O3 -DNDEBUG -std=c99 -fPIC -Wno-psabi -O2 -pthread  -fno-math-errno  -marm  -march=armv8.2-a+dotprod -mfpu=neon-fp-armv8  -MD -MT _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o -MF CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o.d -o CMakeFiles/microkernels-all.dir/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c.o -c /mnt/fileroot/guanghui.wan/tensorflow/tensorflow/build_sdk/xnnpack/src/qd8-f32-qc8w-gemm/gen/qd8-f32-qc8w-gemm-2x8c4-minmax-neondot.c
/tmp/ccrZ5z20.s: Assembler messages:
/tmp/ccrZ5z20.s:75: Error: selected processor does not support `vsdot.s8 q14,q12,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:76: Error: selected processor does not support `vsdot.s8 q10,q9,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:77: Error: selected processor does not support `vsdot.s8 q14,q11,d7[1]' in ARM mode
/tmp/ccrZ5z20.s:78: Error: selected processor does not support `vsdot.s8 q10,q8,d7[1]' in ARM mode
/tmp/ccrZ5z20.s:134: Error: selected processor does not support `vsdot.s8 q14,q9,d7[0]' in ARM mode
/tmp/ccrZ5z20.s:135: Error: selected processor does not support `vsdot.s8 q10,q8,d7[0]' in ARM mode
make[2]: *** [_deps/xnnpack-build/CMakeFiles/microkernels-all.dir/build.make:49877: _deps/xnnpack-build/CMakeFiles/microkernels-all.dir/src/qd8-f32-qc4w-gemm/gen/qd8-f32-qc4w-gemm-1x8c4-minmax-neondot.c.o] Error 1
```
",False,"[-5.65917850e-01 -3.31433713e-01 -2.04035878e-01 -6.54604658e-02
  2.58224726e-01 -3.78277361e-01 -2.59254485e-01  1.34027526e-01
 -4.38888431e-01 -5.03073990e-01 -3.55730951e-02 -1.66355938e-01
 -3.00757110e-01 -1.02353871e-01  3.30778807e-02  2.34932274e-01
 -3.09524596e-01 -2.78661847e-02  2.20670015e-01 -2.83183418e-02
 -1.70671999e-01  2.98140571e-02 -3.11277449e-01  2.21289039e-01
  1.47492349e-01  3.36121231e-01 -3.21239471e-01  4.09134431e-03
 -4.61615510e-02  1.60261109e-01  6.86708331e-01  6.87857717e-02
 -2.14699805e-01  1.86167926e-01  1.20621495e-01  2.75014490e-01
 -4.02556777e-01 -1.36546478e-01 -1.38372943e-01 -1.13501504e-01
  4.90258411e-02  2.48745710e-01  1.79421663e-01 -1.27336830e-01
 -1.65842846e-02 -1.51430577e-01 -1.70191541e-01 -7.82050192e-02
 -1.05485152e-02 -3.74315709e-01  1.81447715e-04 -2.48019192e-02
 -2.82737017e-01 -1.16853029e-01 -2.39596561e-01 -7.23683611e-02
  1.85570866e-01 -1.36540204e-01  1.72818139e-01  3.27801704e-01
  1.32383883e-01 -4.76650335e-02 -2.46794447e-02 -1.43131703e-01
  8.45160559e-02  1.54782444e-01  3.56130183e-01 -1.04636066e-01
  3.94431293e-01 -2.30243474e-01  2.31832787e-02 -1.88174501e-01
 -3.66923273e-01 -1.81600362e-01 -9.55281705e-02  1.22232996e-01
 -1.73888981e-01  3.60081136e-01  1.93508267e-01 -2.10056588e-01
 -1.71161778e-02 -2.99496084e-01 -5.71390912e-02 -3.58793914e-01
  1.79307729e-01 -1.19878545e-01  3.51535022e-01  1.67966545e-01
  5.51564634e-01 -4.51004475e-01  3.41550797e-01  5.40441215e-01
  1.68078355e-02  4.00685132e-01  5.18528700e-01  8.21999162e-02
  1.15844749e-01  8.31967071e-02 -8.31108838e-02 -5.36873937e-02
 -3.09474021e-01 -7.73997605e-02  8.47778320e-02 -3.42370942e-04
 -1.82656899e-01 -2.19547749e-01  3.11879277e-01  9.11046267e-02
  1.54919952e-01 -2.38169804e-01  1.41234472e-01  6.31493032e-02
  3.08595300e-01 -1.61255628e-01  5.09216264e-02  1.48360729e-01
 -1.49069410e-02 -6.73499107e-02  5.47262207e-02  6.68737352e-01
 -6.16871491e-02  5.09272814e-02 -8.70763808e-02  2.33110696e-01
  4.22285587e-01  3.16043459e-02 -2.04166442e-01 -9.29270219e-03
  8.49024951e-02  9.88302082e-02 -1.97085068e-01  2.43653506e-01
  1.14503793e-01  3.88263524e-01  2.39971038e-02  1.36604100e-01
 -1.72214165e-01 -1.48184434e-01  8.39458257e-02 -2.91436195e-01
 -3.41880590e-01  1.85381308e-01 -2.26932377e-01 -6.95567608e-01
  8.38324279e-02  1.54863652e-02 -3.20032239e-01  1.68529391e-01
 -3.05071175e-01 -1.20794624e-01 -5.69096915e-02  1.91934317e-01
 -1.17541432e-01  4.87097174e-01  4.09281313e-01  2.64877230e-02
  6.12476349e-01 -8.86822045e-02  3.89336757e-02 -5.22171736e-01
  1.32721718e-02  6.29957080e-01 -3.21931541e-02  4.78314459e-02
 -1.41414925e-02  1.04886338e-01 -4.71597433e-01 -4.15170230e-02
 -2.34001353e-02  2.87745953e-01  1.85072869e-02 -2.35589981e-01
  2.71493345e-02  5.32613546e-02  1.41568765e-01 -1.25963330e-01
  5.19658327e-01 -6.93176031e-01 -4.45239767e-02  4.33647901e-01
 -7.80979693e-02  2.38183782e-01  2.99758852e-01  1.06038675e-01
  1.39285386e-01  1.75039709e-01  1.36854500e-01  2.43467227e-01
 -3.88658762e-01 -6.85769543e-02 -6.49202764e-01 -2.22301573e-01
  4.09799039e-01 -7.43387789e-02 -1.91800594e-01  1.36456534e-01
  2.59234130e-01 -7.80841522e-03 -3.70421186e-02 -6.94315135e-03
 -2.01882850e-02  2.86086109e-02 -6.48339540e-02 -1.33350223e-01
  1.20763574e-02 -2.93159068e-01 -2.17017941e-02 -3.90757442e-01
 -3.87812376e-01 -4.86615151e-02 -3.40654254e-02 -4.68544424e-01
  6.89495653e-02 -2.41233259e-01 -5.93971014e-01  3.76389235e-01
  1.51987270e-01 -1.51052251e-01 -2.30744869e-01  2.77535558e-01
  2.36891329e-01 -1.98274583e-01  2.25618742e-02 -3.42397302e-01
 -2.13512599e-01  9.68299285e-02 -3.57253075e-01  3.31742704e-01
  4.04963568e-02  1.64639324e-01  4.69115078e-02  2.76885275e-02
  4.94925648e-01  1.07388169e-01  3.56029838e-01 -1.92116678e-01
 -5.12650833e-02 -1.11677714e-01 -2.52286792e-01  4.42651249e-02
 -1.54408455e-01 -3.60722691e-01  1.53230071e-01 -3.03182229e-02
  5.04004657e-01  2.98803270e-01 -3.04086387e-01 -8.70072246e-02
 -2.16375470e-01  2.10394502e-01 -4.85290289e-02  2.55205154e-01
  3.30032408e-01  9.81707573e-02  7.59336889e-01  1.96199477e-01
  9.16812271e-02  3.50767851e-01  2.11117059e-01 -1.17385082e-01
  4.75425750e-01  4.35680836e-01 -1.72587223e-02  4.49603528e-01
  1.31199032e-01  2.74139941e-01 -5.12378573e-01  2.89208591e-01
 -1.55806933e-02 -1.23967454e-02  4.73428965e-02 -2.72988260e-01
  5.72932780e-01 -2.68764734e-01  3.80840451e-02 -1.14330702e-01
  3.13075602e-01 -1.13747977e-01 -1.38297141e-01  1.22455224e-01
 -1.08635753e-01  2.76025683e-01 -3.05495620e-01  5.04029021e-02
  1.39688969e-01  1.82030164e-02  1.73852056e-01 -7.58706808e-01
 -3.19912553e-01  3.11588734e-01 -3.32000017e-01  9.61753726e-02
 -1.27575286e-02  3.20022970e-01 -9.06151906e-02  4.89687249e-02
 -1.09003723e-01 -1.12361267e-01  2.39054024e-01  2.85974979e-01
 -1.67283844e-02  1.09806970e-01  4.20236886e-01 -5.62240601e-01
 -1.04519948e-01 -7.36731589e-02  3.56853008e-01  2.25501478e-01
  4.70292568e-01 -4.30160820e-01  1.39374420e-01 -2.00805962e-01
 -1.03752173e-01  3.73052835e-01 -1.11117452e-01  3.37118655e-02
 -3.39347512e-01  7.69379199e-01  3.19456637e-01 -1.03808559e-01
  3.72701675e-01 -9.06212032e-02 -5.00330746e-01 -1.54822648e-01
  4.07720685e-01 -4.75989357e-02 -5.03505841e-02 -5.04706264e-01
 -1.57521576e-01  1.38624921e-01 -1.47382230e-01 -2.08586320e-01
 -8.80252123e-02  2.15429127e-01 -3.14983070e-01 -1.90687314e-01
 -4.61942166e-01  1.68431044e-01 -1.16118804e-01 -5.38545072e-01
 -2.41133898e-01 -2.38125756e-01  1.85872048e-01 -1.67013094e-01
 -3.88095900e-02 -2.83123553e-01  3.68157089e-01  4.86048579e-01
 -2.38098815e-01  1.21916361e-01 -6.95695281e-02  2.28797778e-01
 -3.80316854e-01 -7.46273398e-02  1.42659143e-01  2.60979027e-01
  1.94443595e-02 -1.31805122e-01  5.78359127e-01  3.77874553e-01
 -1.13663882e-01  1.84853882e-01 -3.66807580e-01  5.33602946e-02
  2.29142725e-01 -3.20734739e-01 -3.39546740e-01 -2.68093228e-01
  1.04871199e-01  3.90178442e-01  4.27962765e-02  2.74049193e-01
 -3.49008620e-01  3.20704699e-01  5.00626087e-01 -5.31062126e-01
 -3.52476120e-01  2.69235671e-01 -7.66657591e-02 -3.56830180e-01
 -2.33559042e-01 -1.59873843e-01  2.98348337e-01  5.77520393e-02]"
Incorrect calculation of cropping indices stat:awaiting response stale,"https://github.com/tensorflow/tensorflow/blob/b2a7a25d102fec8fc7e3690218b627738d8a6fc2/tensorflow/compiler/mlir/tosa/transforms/legalize_common.cc#L1234

Should be:

```
int crops_lo = crops_const[i * 2 + 0];
int crops_hi = crops_const[i * 2 + 1];
```

The tests happen to cover only the case in which `crops_dims` is 2, so the bug is hidden.",False,"[-4.25555147e-02  1.25362933e-01 -2.14632720e-01 -2.55580153e-02
  1.97104722e-01 -4.00278747e-01  8.04118216e-02  3.63455154e-04
 -4.99590725e-01  1.90823391e-01  3.50914478e-01 -2.05938682e-01
 -4.99946289e-02 -1.68688316e-02 -2.20066592e-01  1.57608375e-01
 -3.42145503e-01  1.64813660e-02 -4.12540436e-01  1.52357310e-01
 -1.99580193e-03  1.82226539e-01 -4.33293253e-01  1.03092894e-01
  2.67688155e-01  2.44231731e-01 -1.66574895e-01  2.30093505e-02
  1.44729331e-01  1.01518005e-01  2.15508193e-01  5.82062542e-01
  2.85911635e-02  4.98547077e-01  7.04948604e-02  1.35763045e-02
 -2.59933770e-01 -2.49365419e-01  2.91485339e-01  4.67274860e-02
  6.66797906e-02 -2.26713642e-01  1.13352410e-01 -1.88499659e-01
  3.04935537e-02 -6.73408732e-02 -2.46624917e-01  5.01014814e-02
  1.34072363e-01 -3.37132156e-01  4.71678525e-02  2.00244069e-01
 -3.15604329e-01 -7.23306537e-01 -2.78199553e-01 -9.44303721e-02
 -2.23293472e-02 -1.33343235e-01  2.28952244e-01 -1.39714226e-01
  2.78168023e-01 -3.60824447e-03  1.84567660e-01  3.23753208e-02
  1.43778902e-02  2.83692051e-02 -1.00042425e-01 -2.47153997e-01
  2.12763697e-01  1.32009611e-01  2.59624958e-01  1.47019625e-01
 -3.16459358e-01 -1.76429003e-01 -1.02202356e-01  6.20153472e-02
 -1.04603179e-01  2.93075860e-01  1.65494055e-01 -5.37440896e-01
 -2.22892672e-01 -2.61952043e-01 -2.02827021e-01 -2.76846707e-01
  1.54532552e-01 -3.29878688e-01  3.13354492e-01 -5.11852130e-02
  8.88336778e-01 -2.23622367e-01  3.83737087e-01 -6.83471709e-02
 -4.38570559e-01 -1.28612682e-01  2.57054150e-01  1.93016440e-01
  2.34154820e-01 -2.04545349e-01 -2.63795972e-01 -1.36400864e-01
 -5.45316696e-01 -1.50920495e-01 -1.22829303e-01  1.56281050e-03
 -3.01988065e-01  7.87861180e-03 -5.16941734e-02 -7.20538385e-03
  2.86958396e-01  1.90397441e-01  1.87383682e-01 -6.85793012e-02
 -1.68192293e-02  2.07643315e-01 -1.24127772e-02  8.77287686e-02
  2.29800880e-01 -3.31302464e-01  7.89235681e-02  1.18159369e-01
 -2.84967035e-01  1.16624258e-01  1.62317187e-01  4.11960065e-01
  1.01798363e-01 -5.91366738e-02 -2.07040220e-01  2.41597593e-02
 -3.93527985e-01 -4.03423533e-02  2.85677552e-01 -1.89230725e-01
 -1.25033349e-01 -1.08891189e-01 -6.89634606e-02 -1.88090056e-01
  1.05119914e-01 -1.82536289e-01 -2.51660407e-01  4.09217030e-02
 -1.00312799e-01  3.17035198e-01  1.18891224e-01 -1.25377387e-01
  4.31926131e-01 -4.48378697e-02 -3.55168164e-01  8.59533548e-02
 -4.19388503e-01  2.26353556e-02 -2.42053911e-01 -9.47590470e-02
 -2.62071460e-01 -4.11264077e-02  5.70407987e-01 -9.49666649e-02
  2.46940285e-01 -1.55640364e-01  3.33368540e-01 -1.93212301e-01
  4.91594225e-02  5.68864226e-01  2.41361819e-02  1.95346490e-01
  9.23719555e-02  1.84942126e-01 -1.24756038e-01 -1.38572693e-01
 -3.32122669e-02  4.13868606e-01  3.46566021e-01  9.05373394e-02
 -2.29100227e-01  3.85551266e-02  5.31906724e-01  6.01403005e-02
  3.18554677e-02 -9.32800919e-02 -1.62472188e-01  4.62658137e-01
  4.31961954e-01 -1.46897614e-01  1.94065318e-01  1.22895256e-01
  3.10315311e-01  2.93508433e-02 -1.89220101e-01 -5.52000627e-02
 -4.23427910e-01 -1.50570616e-01 -4.24296647e-01  2.05763102e-01
 -3.53691369e-01  1.04648471e-01 -9.34406966e-02  5.61247841e-02
  4.30229425e-01  4.05935347e-01  2.08088785e-01  1.53451547e-01
 -2.33707041e-01  1.19522855e-01  1.81909427e-01 -1.13308653e-01
  3.41691554e-01 -1.91927344e-01 -2.55717874e-01 -2.97757387e-01
 -9.40048546e-02  5.09508371e-01 -2.02460721e-01 -4.39622164e-01
 -1.99287146e-01 -2.91115820e-01 -2.66237110e-01  4.00346994e-01
  5.61659396e-01  4.87120412e-02 -1.49388939e-01  1.51823238e-01
  1.51046822e-02 -2.88956225e-01  4.00809757e-03 -2.08278596e-01
 -3.34126472e-01  2.51125008e-01 -1.13497101e-01  9.73756164e-02
 -5.16867414e-02 -2.05857277e-01  9.04595852e-02 -1.70036733e-01
  3.43874454e-01 -9.27312300e-03  1.28219455e-01  7.01481849e-02
  5.11737075e-03 -2.21425325e-01  2.06939086e-01 -3.61959822e-02
 -3.76527786e-01 -5.30743226e-02  1.95045903e-01  8.90315324e-02
 -6.27800524e-02  2.87999898e-01 -1.09237563e-02  4.99485992e-02
 -5.28930128e-01 -2.87636556e-02 -3.76972079e-01  6.76465258e-02
  1.45785987e-01 -2.67453223e-01 -1.05255377e-02 -2.47494802e-01
  7.05538467e-02  1.69034332e-01 -5.49643785e-02 -3.45518142e-01
  3.48522246e-01 -1.74923256e-01 -1.46391362e-01  6.04450345e-01
 -1.17188960e-01 -3.44933122e-02 -2.81081378e-01  4.29554552e-01
 -9.51237977e-02 -1.42084986e-01  5.62587976e-01  1.98681235e-01
  2.73661494e-01 -2.92724073e-01  2.44548336e-01 -2.16710910e-01
  1.25916451e-01 -1.56945363e-01 -1.23417854e-01 -1.65027715e-02
  1.10588998e-01  3.57309282e-01 -4.55358118e-01  3.92564297e-01
  2.35160828e-01  1.13426326e-02  1.18420318e-01  5.61451316e-02
 -3.79599094e-01 -1.69592261e-01 -7.90447593e-02 -1.35754555e-01
  7.93213099e-02  9.76117551e-02 -1.06814459e-01 -1.47261843e-01
  2.35120773e-01 -1.37611881e-01 -3.48018073e-02 -3.23358476e-02
 -8.21123831e-03 -3.13560739e-02  3.60771686e-01 -4.20442522e-01
 -3.54805112e-01  1.28729455e-02  3.59313414e-02  2.81318486e-01
  3.79316539e-01 -4.25326884e-01 -2.53875591e-02 -1.56958140e-02
 -1.44681275e-01  6.43100142e-01  9.51254219e-02  5.79202846e-02
 -2.94082463e-01  6.93926573e-01  8.90896842e-02  2.15771765e-01
 -3.24156553e-01 -1.51147008e-01 -2.90469170e-01  4.69956815e-01
  3.13970685e-01 -9.23101306e-02 -9.18015540e-02 -7.94345438e-02
  8.78408849e-02  7.10383505e-02 -9.94830132e-02  1.94002688e-02
 -3.05591464e-01  1.49141073e-01 -1.16699547e-01 -7.58256987e-02
 -2.48302549e-01  2.23263144e-01  4.99047004e-02  6.39364822e-03
 -9.96861458e-02 -4.82608452e-02 -1.11893788e-01 -1.83258131e-01
 -7.52470046e-02 -3.24330747e-01  7.08207339e-02  5.60244501e-01
 -2.49146521e-02  3.61786664e-01  3.28601003e-01  2.62953997e-01
  9.76824611e-02  1.13276660e-01 -2.12527826e-01  8.27292204e-02
  2.84583326e-02  1.86407566e-01  3.47258329e-01  3.57497543e-01
 -2.84306347e-01  4.63484079e-02 -3.47420931e-01  8.58066231e-02
 -2.34718293e-01 -1.00347832e-01 -1.84880346e-02  2.50479635e-02
  1.70333162e-01 -1.71012823e-02 -1.11528307e-01 -5.23729995e-03
 -5.78987561e-02 -2.79826462e-01  2.53615081e-01 -5.45850277e-01
 -3.21304679e-01 -3.71168032e-02  3.46600533e-01 -1.30858839e-01
 -9.51566845e-02 -1.74050182e-01 -9.88155305e-02  1.39691830e-01]"
Build broken for AARCH64 by XLA PJRT type:build/install comp:xla subtype: ubuntu/linux,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

17.0.0

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Build fails on AARCH64 since https://github.com/tensorflow/tensorflow/commit/7b81b8c94a3fcf067ce50c31ce031a9ea449a59b

### Standalone code to reproduce the issue

```shell
bazel build --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --define=tf_api_version=2 --verbose_failures --jobs=75 -- //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/external/local_xla/xla/pjrt/BUILD:754:11: Compiling xla/pjrt/transpose.cc failed: (Exit 1): clang failed: error executing command (from target @local_xla//xla/pjrt:transpose) 
  (cd /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    CACHEBUSTER=20220325 \
    CLANG_COMPILER_PATH=/usr/lib/llvm-17/bin/clang \
    PATH=/home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazelisk/downloads/bazelbuild/bazel-6.1.0-linux-arm64/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3.10 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib/llvm-17/bin/clang -MD -MF bazel-out/aarch64-opt/bin/external/local_xla/xla/pjrt/_objs/transpose/transpose.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/local_xla/xla/pjrt/_objs/transpose/transpose.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-DBAZEL_CURRENT_REPOSITORY=""local_xla""' -iquote external/local_xla -iquote bazel-out/aarch64-opt/bin/external/local_xla -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/local_tsl -iquote bazel-out/aarch64-opt/bin/external/local_tsl -iquote . -iquote bazel-out/aarch64-opt/bin -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/ml_dtypes -iquote bazel-out/aarch64-opt/bin/external/ml_dtypes -iquote external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -iquote external/snappy -iquote bazel-out/aarch64-opt/bin/external/snappy -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -Ibazel-out/aarch64-opt/bin/external/ml_dtypes/_virtual_includes/float8 -Ibazel-out/aarch64-opt/bin/external/ml_dtypes/_virtual_includes/int4 -isystem third_party/eigen3/mkl_include -isystem bazel-out/aarch64-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/ml_dtypes -isystem bazel-out/aarch64-opt/bin/external/ml_dtypes -isystem external/ml_dtypes/ml_dtypes -isystem bazel-out/aarch64-opt/bin/external/ml_dtypes/ml_dtypes -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions '-mtune=generic' '-march=armv8-a' -O3 -flax-vector-conversions '-std=c++17' '--sysroot=/dt10' -c external/local_xla/xla/pjrt/transpose.cc -o bazel-out/aarch64-opt/bin/external/local_xla/xla/pjrt/_objs/transpose/transpose.pic.o)
# Configuration: 05cc03ffdfad7b565350ad2b01d7497c783d20d25ca4abeda724de457d2afb5e
# Execution platform: @local_execution_config_platform//:platform
In file included from external/local_xla/xla/pjrt/transpose.cc:94:
external/local_xla/xla/pjrt/transpose_kernels.h:710:46: error: use of undeclared identifier '__m128i'
  710 |       if constexpr (sizeof(T) * bs <= sizeof(__m128i)) {
      |                                              ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned short, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned short, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned short, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:459:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned short, xla::TransposePlan::Transformation::kNone>' requested here
  459 |         ExecuteTyped<uint16_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '512 <= 256'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:471:17: error: static assertion failed due to requirement '32UL * (0 + 1) <= sizeof(__attribute__((neon_vector_type(2))) unsigned long)'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:551:4: note: in instantiation of function template specialization 'xla::StoreElementFromVec128<32UL, 0>' requested here
  551 |   (StoreElementFromVec128</*bytes=*/bytes, lane>(b + ldb * (i + lane), x), ...);
      |    ^
external/local_xla/xla/pjrt/transpose_kernels.h:594:7: note: in instantiation of function template specialization 'xla::StoreElementsFromVec128<32UL, 0UL>' requested here
  594 |       StoreElementsFromVec128<element_size * bs>(
      |       ^
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned short, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned short, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned short, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:459:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned short, xla::TransposePlan::Transformation::kNone>' requested here
  459 |         ExecuteTyped<uint16_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:471:36: note: expression evaluates to '32 <= 16'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:498:21: error: static assertion failed due to requirement '32UL == 0'
  498 |       static_assert(bytes == 0);
      |                     ^~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned int, 8>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned int, 8>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned int, 8, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:463:11: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned int, xla::TransposePlan::Transformation::kNone>' requested here
  463 |           ExecuteTyped<uint32_t, Transformation::kNone>(ac, bc, nodes);
      |           ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '256 <= 128'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned int, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned int, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned int, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:463:11: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned int, xla::TransposePlan::Transformation::kNone>' requested here
  463 |           ExecuteTyped<uint32_t, Transformation::kNone>(ac, bc, nodes);
      |           ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '1024 <= 256'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:471:17: error: static assertion failed due to requirement '64UL * (0 + 1) <= sizeof(__attribute__((neon_vector_type(2))) unsigned long)'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:551:4: note: in instantiation of function template specialization 'xla::StoreElementFromVec128<64UL, 0>' requested here
  551 |   (StoreElementFromVec128</*bytes=*/bytes, lane>(b + ldb * (i + lane), x), ...);
      |    ^
external/local_xla/xla/pjrt/transpose_kernels.h:594:7: note: in instantiation of function template specialization 'xla::StoreElementsFromVec128<64UL, 0UL>' requested here
  594 |       StoreElementsFromVec128<element_size * bs>(
      |       ^
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned int, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned int, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned int, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:463:11: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned int, xla::TransposePlan::Transformation::kNone>' requested here
  463 |           ExecuteTyped<uint32_t, Transformation::kNone>(ac, bc, nodes);
      |           ^
external/local_xla/xla/pjrt/transpose_kernels.h:471:36: note: expression evaluates to '64 <= 16'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:498:21: error: static assertion failed due to requirement '64UL == 0'
  498 |       static_assert(bytes == 0);
      |                     ^~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned long, 4>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned long, 4>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned long, 4, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:470:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned long, xla::TransposePlan::Transformation::kNone>' requested here
  470 |         ExecuteTyped<uint64_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '128 <= 64'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned long, 8>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned long, 8>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned long, 8, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:470:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned long, xla::TransposePlan::Transformation::kNone>' requested here
  470 |         ExecuteTyped<uint64_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '512 <= 128'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned long, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned long, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned long, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:470:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned long, xla::TransposePlan::Transformation::kNone>' requested here
  470 |         ExecuteTyped<uint64_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '2048 <= 256'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:471:17: error: static assertion failed due to requirement '128UL * (0 + 1) <= sizeof(__attribute__((neon_vector_type(2))) unsigned long)'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:551:4: note: in instantiation of function template specialization 'xla::StoreElementFromVec128<128UL, 0>' requested here
  551 |   (StoreElementFromVec128</*bytes=*/bytes, lane>(b + ldb * (i + lane), x), ...);
      |    ^
external/local_xla/xla/pjrt/transpose_kernels.h:594:7: note: in instantiation of function template specialization 'xla::StoreElementsFromVec128<128UL, 0UL>' requested here
  594 |       StoreElementsFromVec128<element_size * bs>(
      |       ^
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<unsigned long, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<unsigned long, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<unsigned long, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:470:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<unsigned long, xla::TransposePlan::Transformation::kNone>' requested here
  470 |         ExecuteTyped<uint64_t, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:471:36: note: expression evaluates to '128 <= 16'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:498:21: error: static assertion failed due to requirement '128UL == 0'
  498 |       static_assert(bytes == 0);
      |                     ^~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<xla::uint128, 2>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<xla::uint128, 2>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<xla::uint128, 2, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:473:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<xla::uint128, xla::TransposePlan::Transformation::kNone>' requested here
  473 |         ExecuteTyped<uint128, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '64 <= 32'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<xla::uint128, 4>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<xla::uint128, 4>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<xla::uint128, 4, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:473:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<xla::uint128, xla::TransposePlan::Transformation::kNone>' requested here
  473 |         ExecuteTyped<uint128, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '256 <= 64'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<xla::uint128, 8>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<xla::uint128, 8>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<xla::uint128, 8, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:473:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<xla::uint128, xla::TransposePlan::Transformation::kNone>' requested here
  473 |         ExecuteTyped<uint128, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '1024 <= 128'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:576:19: error: static assertion failed due to requirement 'kBytesInMatrix <= sizeof(__attribute__((neon_vector_type(2))) unsigned long) * last_transpose.size()'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<xla::uint128, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<xla::uint128, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<xla::uint128, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:473:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<xla::uint128, xla::TransposePlan::Transformation::kNone>' requested here
  473 |         ExecuteTyped<uint128, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:576:34: note: expression evaluates to '4096 <= 256'
  576 |     static_assert(kBytesInMatrix <= sizeof(Vec128) * last_transpose.size());
      |                   ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:471:17: error: static assertion failed due to requirement '256UL * (0 + 1) <= sizeof(__attribute__((neon_vector_type(2))) unsigned long)'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:551:4: note: in instantiation of function template specialization 'xla::StoreElementFromVec128<256UL, 0>' requested here
  551 |   (StoreElementFromVec128</*bytes=*/bytes, lane>(b + ldb * (i + lane), x), ...);
      |    ^
external/local_xla/xla/pjrt/transpose_kernels.h:594:7: note: in instantiation of function template specialization 'xla::StoreElementsFromVec128<256UL, 0UL>' requested here
  594 |       StoreElementsFromVec128<element_size * bs>(
      |       ^
external/local_xla/xla/pjrt/transpose_kernels.h:711:66: note: in instantiation of member function 'xla::Vec128RectangularTransposeMicroKernelImpl<xla::uint128, 16>::Apply' requested here
  711 |         return Vec128RectangularTransposeMicroKernelImpl<T, bs>::Apply(a, lda,
      |                                                                  ^
external/local_xla/xla/pjrt/transpose.cc:170:42: note: in instantiation of member function 'xla::TransposeMicroKernel<xla::uint128, 16>::Apply' requested here
  170 |       TransposeMicroKernel<T, inner_bs>::Apply(
      |                                          ^
external/local_xla/xla/pjrt/transpose.cc:410:9: note: in instantiation of function template specialization 'xla::MacroKernel<xla::uint128, 16, xla::TransposePlan::Transformation::kNone>' requested here
  410 |         MacroKernel<T, const_inner_block_elems, transformation>(
      |         ^
external/local_xla/xla/pjrt/transpose.cc:473:9: note: in instantiation of function template specialization 'xla::TransposePlan::ExecuteTyped<xla::uint128, xla::TransposePlan::Transformation::kNone>' requested here
  473 |         ExecuteTyped<uint128, Transformation::kNone>(ac, bc, nodes);
      |         ^
external/local_xla/xla/pjrt/transpose_kernels.h:471:36: note: expression evaluates to '256 <= 16'
  471 |   static_assert(bytes * (lane + 1) <= sizeof(uint64x2_t));
      |                 ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/pjrt/transpose_kernels.h:498:21: error: static assertion failed due to requirement '256UL == 0'
  498 |       static_assert(bytes == 0);
      |                     ^~~~~~~~~~
19 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
",False,"[-4.83865559e-01 -3.96463215e-01  1.21655345e-01 -1.94587335e-01
  8.85092393e-02 -2.80417472e-01 -1.73144475e-01 -2.55281106e-02
 -3.49018812e-01 -2.15921074e-01 -3.66147570e-02  2.43303925e-01
 -8.37975442e-02  2.19734795e-02 -1.00160064e-02  4.37695116e-01
 -2.74508864e-01 -2.60441542e-01  2.06227198e-01  5.93989789e-02
 -6.47475794e-02 -7.61667788e-02 -3.62441480e-01  1.39391959e-01
  1.77904129e-01  2.38930970e-01 -1.97428763e-01  1.01554304e-01
  3.14027853e-02  1.09329201e-01  5.37757099e-01  9.89564359e-02
  1.15874261e-01  1.22611172e-01  4.58689183e-02  4.54414278e-01
 -9.49309170e-02 -3.31533045e-01 -1.06320091e-01  1.77665472e-01
 -1.76003039e-01  1.57819614e-01  1.59681976e-01 -3.15296322e-01
  1.99910641e-01 -2.21192703e-01 -8.70148465e-02 -1.31808162e-01
  1.17817208e-01 -9.42734033e-02  7.51325488e-02  1.00252815e-01
 -2.86284000e-01 -2.81207263e-01 -9.15804654e-02  1.06835961e-01
 -7.28000887e-03  8.37652106e-03  7.76926577e-02  2.39988685e-01
  2.73104846e-01 -4.97276522e-03 -1.13261513e-01 -5.91396727e-02
  6.30523339e-02  1.14837319e-01  3.81402433e-01 -4.14160080e-03
  4.12608862e-01  4.63448688e-02  2.85557471e-02 -1.54079080e-01
 -3.45937908e-01 -1.10487103e-01 -9.43219513e-02 -1.55856907e-01
  5.27579188e-02  2.19571944e-02  1.76188350e-01 -1.14733800e-01
  5.86513653e-02 -8.66476446e-02  1.40213341e-01 -1.56825840e-01
  1.91416070e-01 -2.27831393e-01  2.92510331e-01  1.91518530e-01
  5.45668662e-01 -1.64131254e-01  3.48758698e-01  3.90735567e-01
  7.27728456e-02  2.06419319e-01  5.29873848e-01  1.09078035e-01
  1.92483246e-01  2.06975698e-01 -9.00135338e-02 -1.70050830e-01
 -2.19232619e-01 -1.54130623e-01  2.98012614e-01 -1.35585248e-01
 -2.62684882e-01 -1.78847730e-01  3.69376361e-01  1.15260229e-01
 -9.64947641e-02 -1.89563707e-01  1.94799677e-02 -8.88074338e-02
  5.02897650e-02 -4.86654639e-02 -3.46576683e-02 -2.78746411e-02
 -9.86523628e-02 -3.29493940e-01 -8.10741484e-02  5.34274995e-01
  8.95354152e-02 -2.57565856e-01  1.45870417e-01  5.32850623e-01
  3.50721180e-01  8.74073505e-02 -1.46994531e-01 -9.27205980e-02
  3.03869750e-02  8.75067152e-03  2.76597179e-02  1.43414497e-01
  6.75698668e-02  3.67545784e-01  1.20668598e-01  1.34705395e-01
 -3.72589171e-01 -2.24078804e-01  7.29172751e-02 -3.07963192e-01
 -3.68358463e-01  4.28618133e-01 -1.71019793e-01 -5.44269681e-01
  1.57062769e-01  9.32649896e-02 -4.01609570e-01  2.33539522e-01
 -1.62441686e-01 -1.79970473e-01 -6.76130950e-02  1.63142130e-01
 -8.29964802e-02  5.65546036e-01  2.36762732e-01  2.34773666e-01
  2.83917069e-01 -1.28876477e-01  4.57818806e-02 -5.20674586e-01
 -1.02110291e-02  3.80365491e-01 -1.03010811e-01 -1.08505890e-01
 -6.75045848e-02  1.95550367e-01 -5.56476772e-01 -1.58625126e-01
  1.90692142e-01  3.82980406e-01 -6.92064017e-02 -1.83239877e-01
  3.86338264e-01  1.27492636e-01  1.57407701e-01 -8.49155486e-02
  3.08821321e-01 -4.14981455e-01 -1.04279213e-01  6.54079378e-01
  8.45047385e-02  1.28675714e-01 -5.69508523e-02  2.68936455e-01
  1.48419011e-02  1.60202906e-01  1.88960463e-01  1.71775147e-01
 -9.84906405e-02  3.21594030e-02 -5.49208403e-01  2.43836120e-02
  2.37185836e-01 -2.80771762e-01 -1.58437103e-01  1.86306894e-01
  1.77067384e-01  1.91920385e-01 -8.50221887e-02  8.79068971e-02
 -1.88568443e-01  8.94808620e-02 -9.45563763e-02 -2.11161934e-02
  1.30846098e-01 -1.54157579e-01 -1.52774617e-01 -2.61545241e-01
 -4.56342369e-01  9.43510383e-02  1.44912094e-01 -3.14748973e-01
  2.25418106e-01  4.23263386e-02 -8.39054137e-02  1.59614533e-01
  5.25828451e-02  1.92549706e-01 -1.73711568e-01  1.67896643e-01
  2.28304043e-02 -2.98330724e-01  1.18279532e-01 -4.00345504e-01
  7.93052763e-02  4.39674687e-03 -4.11516398e-01 -1.09810010e-01
  1.45120369e-02  1.54001623e-01 -1.53572597e-02  1.06653064e-01
  3.91322494e-01  1.34274930e-01  5.75120449e-01 -7.02445805e-02
 -5.68672828e-02 -1.56507969e-01 -7.28125870e-02  2.94469953e-01
 -2.83247173e-01 -3.21735859e-01  1.64668635e-02 -9.23640653e-02
  4.66016889e-01  3.63091469e-01  6.39182925e-02 -2.75545381e-02
 -2.13909030e-01  2.41265699e-01 -2.27441698e-01  1.50670782e-01
  2.85498083e-01  1.37775764e-01  2.04067528e-01  3.26215923e-01
  1.13124311e-01  2.87803710e-01  1.02248579e-01 -7.73043856e-02
  6.52536154e-02  1.83878124e-01 -5.39454594e-02  2.87449419e-01
  2.55238593e-01  2.84058034e-01 -3.32582772e-01  2.63920248e-01
  1.57028347e-01 -7.99002275e-02  4.38679338e-01 -2.67574191e-01
  5.11999607e-01 -4.33060616e-01  1.06894940e-01 -2.51905248e-03
  2.32819080e-01 -1.30369172e-01  1.05572656e-01 -1.19335160e-01
 -5.76694459e-02  2.46571153e-01 -4.33428288e-01  2.81025879e-02
  3.59599963e-02 -2.33604684e-01  7.26939067e-02 -7.38122582e-01
 -3.71065021e-01 -7.49114230e-02 -3.30481887e-01 -4.89721447e-02
 -3.68248299e-02  7.68289715e-02 -3.21710676e-01  3.97918336e-02
 -3.27003449e-02 -1.52473636e-02  2.65906099e-02  6.95707053e-02
 -2.41366655e-01 -9.01504382e-02  1.92810684e-01 -4.79457259e-01
 -4.38156016e-02 -2.17202514e-01  3.44375193e-01  8.67507011e-02
  5.43321252e-01 -5.96410334e-01  1.46432817e-01 -1.16856489e-02
 -3.50100785e-01  5.03882051e-01 -1.62939020e-02  4.32791114e-02
 -4.93546873e-01  6.88453317e-01 -4.25297767e-05 -1.73140109e-01
  4.53061089e-02 -1.29754186e-01 -5.15847325e-01 -1.03432328e-01
  1.26891106e-01 -8.07000250e-02 -1.56713426e-01 -2.59278238e-01
 -5.18653616e-02  5.58476523e-02 -1.96679398e-01 -1.02935888e-01
 -2.24512994e-01  2.35219344e-01 -3.88174623e-01 -1.33386865e-01
 -5.21333218e-01  1.96421221e-01 -1.63083375e-01 -2.59049118e-01
 -1.53660774e-01  2.69089192e-02 -9.34827179e-02 -8.09104294e-02
 -8.98764133e-02 -2.91975468e-01  4.49244350e-01  2.60452271e-01
 -6.41240925e-02  3.78832892e-02 -1.24656279e-02  2.90481597e-01
 -4.16753381e-01 -7.42386468e-03  5.54290786e-02  3.08057785e-01
  1.69574395e-02 -8.84405337e-03  1.81627899e-01  1.11527011e-01
  1.57854091e-02  1.06642544e-01 -5.33784866e-01  4.25354764e-02
  1.23694763e-01 -2.27107763e-01 -2.13892832e-01 -2.70068467e-01
  2.57478245e-02  3.36712360e-01 -6.21622056e-02  2.64484078e-01
 -3.99401009e-01  3.34202409e-01  5.83467960e-01 -4.21809256e-01
 -3.36681992e-01  1.61378235e-01  1.89540297e-01 -1.80240124e-01
 -1.21024303e-01  4.73527238e-02  3.58238578e-01 -3.63656133e-02]"
static_assert failure with gcc type:build/install subtype:bazel,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

Build failure

### Standalone code to reproduce the issue

```shell
bazel build --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --define=tf_api_version=2 --verbose_failures --jobs=75 -- //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo.cc: In member function 'mlir::LogicalResult mlir::odml::{anonymous}::ConvertReduceOpToTfArgMinMax<TfReduce, TfArgReduce>::matchAndRewrite(mlir::mhlo::ReduceOp, mlir::OpConversionPattern<mlir::mhlo::ReduceOp>::OpAdaptor, mlir::ConversionPatternRewriter&) const':
tensorflow/compiler/mlir/lite/stablehlo/transforms/legalize_hlo.cc:2186: error: static assertion failed: Only TF::MaxOp and TF::MinOp are supported.
 2186 |         static_assert(false, ""Only TF::MaxOp and TF::MinOp are supported."");
      | 
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 502.712s, Critical Path: 197.25s
INFO: 1603 processes: 79 internal, 1524 local.
FAILED: Build did NOT complete successfully
```
",False,"[-7.04064369e-01 -9.14499462e-02 -1.48579717e-01  1.77175641e-01
  8.83432925e-02 -3.31571907e-01 -4.39268351e-02  7.31637403e-02
 -3.30294043e-01 -2.87798434e-01  1.18656345e-01 -6.82668835e-02
 -1.84351765e-02  5.09828180e-02 -4.73517887e-02  4.52338815e-01
 -1.44068286e-01  8.68605524e-02  3.06347907e-01  5.94409220e-02
 -5.82632273e-02 -2.41152328e-02 -1.66297227e-01  1.56186044e-01
 -5.77458739e-02 -4.60560918e-02 -2.95871079e-01  6.60653412e-02
  1.11179404e-01 -9.52155292e-02  5.26142955e-01  9.79572982e-02
 -6.53885603e-02  7.40491673e-02 -5.87344579e-02  3.60009104e-01
 -2.13439673e-01 -4.85724151e-01 -2.50478983e-01 -7.60653056e-04
 -1.42359495e-01  7.82595128e-02  2.46454105e-01 -3.94786209e-01
  2.16507912e-01 -2.63350070e-01  8.27022865e-02  8.89788866e-02
 -8.07947665e-03 -3.80118191e-01 -8.05687830e-02 -1.28341466e-01
 -4.04267997e-01 -4.26976442e-01 -2.58940365e-02 -8.57085735e-02
  1.14626810e-01 -1.85801595e-01 -7.27542639e-02  2.09438235e-01
  3.27217072e-01 -1.13162994e-01 -1.76126391e-01 -9.60917175e-02
  4.15394232e-02  1.75000176e-01  3.87760639e-01 -3.46890613e-02
  4.78342235e-01 -1.54682264e-01  1.55754715e-01  9.72346812e-02
 -3.68520021e-01  9.03142169e-02  1.31408474e-03 -1.38555309e-02
  6.07622899e-02  2.41192341e-01  3.29224408e-01 -2.09883630e-01
  9.64928567e-02 -6.12808466e-02 -4.16533202e-02 -3.39698255e-01
  1.24199733e-01 -5.12878634e-02  4.53765005e-01  2.71190941e-01
  4.66951489e-01 -1.75722837e-02  4.74927351e-02  2.72534758e-01
 -1.03457130e-01  3.26751219e-03  2.76935816e-01  2.04179678e-02
  3.25308502e-01  1.15990639e-03  1.76516905e-01 -6.28567860e-02
 -1.13677382e-01 -3.04115951e-01  1.42179295e-01 -7.95963481e-02
 -1.27087265e-01 -2.39684433e-01  3.72557789e-01  1.37111872e-01
 -2.04508901e-01 -3.02361101e-01  2.01810058e-02  1.27145618e-01
 -5.26583567e-02 -4.03539568e-01 -1.33964807e-01  1.42795533e-01
 -1.00613564e-01 -3.57986718e-01 -3.37088481e-02  5.79715848e-01
  8.53343755e-02  2.61255372e-02  1.00039259e-01  2.63975292e-01
  5.81877887e-01  1.03037599e-02 -1.06416762e-01 -1.42949998e-01
  1.27157629e-01 -3.17651443e-02  1.18412964e-01  2.42271975e-01
  1.34581223e-01  4.33717489e-01  3.31624523e-02  2.67924339e-01
 -2.51878411e-01 -1.33974135e-01  5.12836203e-02 -2.52884150e-01
 -1.21082142e-01  1.56948306e-02  2.42504776e-02 -6.93517625e-01
  1.56749338e-01 -1.35240495e-01 -2.42788360e-01  9.20728743e-02
 -1.36852622e-01 -5.48478402e-02 -6.89384714e-02  1.08147576e-01
 -1.46483094e-01  5.31004369e-01  6.89394549e-02  2.50262730e-02
  4.64011490e-01 -9.39868242e-02  3.98470014e-01 -4.89585757e-01
  9.66636240e-02  5.07817626e-01  1.32026702e-01 -1.38579488e-01
 -1.36830702e-01  1.76147878e-01 -3.35910529e-01 -2.54688025e-01
  3.40997837e-02  6.03382766e-01 -3.54844689e-01 -1.39372140e-01
  8.60154778e-02  2.50800811e-02 -2.00973898e-02 -1.61097854e-01
  1.31316662e-01 -3.49074125e-01  6.86385930e-02  6.17354751e-01
 -9.23328549e-02  1.25725985e-01 -1.29042789e-02  1.60843611e-01
 -1.99260563e-01  2.80740261e-01  2.68414244e-02  1.03355646e-01
  2.70294119e-02  2.74848156e-02 -6.41096830e-01 -8.24272931e-02
  1.72133818e-01 -3.06565672e-01 -1.09983034e-01  1.38023615e-01
  1.50629401e-01 -1.01339296e-01 -3.39234527e-03 -1.07624322e-01
 -2.32343316e-01 -1.10808134e-01 -2.48209536e-01  1.54754311e-01
  2.08238155e-01 -4.59786281e-02 -1.06064724e-02 -3.20482552e-01
 -2.38916188e-01  1.48029834e-01 -7.67878257e-03 -5.78425407e-01
  4.02371399e-02  1.06524415e-01 -3.06112498e-01  9.17021334e-02
  3.17428231e-01  1.88694179e-01  2.77195498e-02  2.50249416e-01
  2.02378988e-01 -4.53534871e-01  2.70323843e-01 -2.89181292e-01
  1.68959886e-01  1.98751897e-01 -3.19349587e-01 -2.97797658e-02
 -5.46158850e-02  3.60251628e-02 -1.12091377e-03 -1.61748081e-01
  2.36567587e-01  1.35164753e-01  3.86080295e-01 -7.21642002e-03
 -2.80063301e-02  2.46696249e-02 -2.53063053e-01  1.68544427e-02
 -4.93103445e-01 -4.17584240e-01  6.64400309e-02  4.61454764e-02
  2.07866088e-01  3.26213181e-01 -2.04368550e-02 -5.07026985e-02
 -3.14477950e-01  2.85413325e-01 -4.42262471e-01  2.80433059e-01
  4.35398698e-01  1.16298504e-01  3.02617550e-01  4.07530963e-01
 -2.90423632e-04  2.23125499e-02  2.11418867e-01 -3.73176813e-01
  2.03079432e-02  4.23266768e-01  1.84987023e-01  1.92364663e-01
  9.61916745e-02  2.14921042e-01 -3.12676787e-01  2.33100325e-01
  2.86873639e-01 -1.03084423e-01  3.34174097e-01 -3.94082189e-01
  4.94871676e-01 -3.59084666e-01  3.06933559e-02 -2.18317121e-01
  2.26095960e-01 -3.09591651e-01  1.03961363e-01 -5.65867126e-03
  1.00563124e-01  7.93217793e-02 -1.85579836e-01  1.32737607e-01
  1.18956968e-01 -2.10897014e-01  1.68101370e-01 -6.35790825e-01
 -2.42213964e-01 -3.38057727e-02 -4.16929901e-01  2.17647463e-01
  2.52177306e-02 -2.07682788e-01 -1.05719738e-01  8.15803707e-02
 -3.81208956e-04 -2.12027520e-01 -5.57105541e-02  1.47510469e-01
 -1.75992280e-01 -1.66858912e-01  1.94364354e-01 -4.19628769e-01
 -1.15649596e-01 -8.74424130e-02  4.47644413e-01  3.02348107e-01
  4.21800733e-01 -4.57742870e-01  2.73010194e-01 -2.24083364e-01
 -2.94327229e-01  3.78090769e-01 -3.64142299e-01  1.88002020e-01
 -3.89995217e-01  7.32829630e-01  1.81447625e-01 -4.67924029e-03
 -5.47849387e-02 -3.65141273e-01 -2.23902196e-01  3.13708410e-02
  2.43221059e-01 -1.62725598e-01 -1.72429115e-01 -5.56281149e-01
  4.11695242e-02  8.58568549e-02 -9.12310630e-02 -1.19510302e-02
 -2.55626030e-02 -4.76585738e-02 -2.46483400e-01  1.68600693e-01
 -3.61888260e-01  8.96920040e-02 -8.57270434e-02 -1.93029195e-02
 -2.92532265e-01 -3.05565000e-02 -1.32556632e-01  3.35417241e-02
 -1.20647019e-02 -3.38909686e-01  3.67888361e-01  3.52815896e-01
 -9.19631943e-02 -9.31627676e-03  9.90991518e-02  2.65076697e-01
 -4.28018123e-01  6.57923147e-02  3.33625749e-02  4.08259451e-01
 -1.64636709e-02 -5.02006039e-02  3.07393938e-01  2.38485277e-01
  1.33470118e-01  2.90096045e-01 -2.50655144e-01  9.91159379e-02
  8.08211416e-02 -2.39050090e-01 -6.90763593e-02 -4.10260379e-01
  2.95055639e-02  2.98547328e-01 -1.15795285e-01  6.12947904e-02
 -4.06716347e-01  6.92006886e-01  6.18167877e-01 -2.66907990e-01
 -1.82304252e-02  5.68080321e-02  1.51823357e-01 -1.84574388e-02
 -2.79902905e-01 -1.48775458e-01  3.89966071e-01  1.18055575e-01]"
Different results interpreter python and Android stat:awaiting response type:support stale TFLiteConverter TF 2.13,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13


I get different results when running my model with the TF Lite interpreter in Python and on Android. I do the same normalization in both of those.


The Python code:

```
def read_image(file_path):
    mean = 255*np.array([0.485, 0.456, 0.406])
    std = 255*np.array([0.229, 0.224, 0.225])
    img = Image.open(file_path).convert('RGB') 
    img = img.resize((224,224), resample=PIL.Image.BILINEAR) 
    img = np.array(img)
    img = (img - mean[None, None, :]) / std[None, None, :]
    img = np.float32(img)
    return img

img = read_image_normal(path)

interpreter = tf.lite.Interpreter(model_path=""./model.tflite"") 
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], processed_image)
interpreter.invoke()
predictions = interpreter.get_tensor(output_details[0]['index'])
```

Snippets of the relevant Android code:
For preprocessing:

```
  private fun preprocessImage(image1: Bitmap) = with(TensorImage(DataType.FLOAT32)) {
      load(image1)
      val imageProcessor: ImageProcessor = ImageProcessor.Builder()
              .add(
              ResizeOp(
                  224,
                  224,
                  ResizeOp.ResizeMethod.BILINEAR
              )
          )
          .add(
              NormalizeOp(
                  floatArrayOf(
                          0.485f*255f,
                          0.456f*255f,
                          0.406f*255f,
                  ),
                  floatArrayOf(
                          0.229f*255f,
                          0.224f*255f,
                          0.225f*255f,
                  ),
              )
          )

          .build()
      imageProcessor.process(this)
  }
```

For inference:

  ```
        val options = Interpreter.Options()
        options.setNumThreads(1)
        interpreter = Interpreter(model, options)
        val timeStart = System.currentTimeMillis()
        val image = preprocessImage(bitmap)
        val probabilityBuffer = TensorBuffer.createFixedSize(intArrayOf(1, 1), DataType.FLOAT32)
        interpreter.run(image.buffer, probabilityBuffer.buffer)
        val score = probabilityBuffer.floatArray[0]
```


Both results are different by quite a lot - around 0.1 score. The images that enter are exactly the same.

Model: https://drive.google.com/file/d/1Pr3mCZ7kocEPw_tAQuokrZBpqqXC0gek/view?usp=sharing
",False,"[-3.73203963e-01 -5.83833456e-01 -2.84994483e-01 -1.43294111e-01
  1.21545628e-01 -1.00963101e-01 -1.11704558e-01  8.70474502e-02
 -2.65850723e-01 -1.42622918e-01 -8.45023319e-02 -3.12602296e-02
  5.98364808e-02  2.74610877e-01 -1.30328462e-01  1.48925334e-01
 -3.38346660e-02 -3.05381000e-01  1.72799408e-01 -9.83187854e-02
  4.12459821e-02 -2.59466060e-02  3.45144905e-02  2.42163092e-01
  3.41349006e-01  1.35524958e-01 -1.12758569e-01 -1.71277344e-01
  7.05200136e-02  2.34678179e-01 -3.65398675e-02  1.18331723e-01
 -3.12925220e-01 -3.71186854e-03 -1.10871330e-01  1.83317184e-01
 -2.43808866e-01 -1.05279043e-01 -3.39137197e-01  6.40386418e-02
  2.71975398e-02  5.18739298e-02 -1.44682620e-02  3.70633304e-02
 -3.32629710e-01  9.32073370e-02  2.09247530e-01 -1.61875971e-02
 -2.84181178e-01  6.33504540e-02 -3.72942202e-02  8.93583521e-02
 -2.76561797e-01 -3.39541882e-01  1.88970760e-01  6.28525857e-03
 -1.20275363e-01  3.01712096e-01  9.03541446e-02 -9.97474417e-05
 -9.70782116e-02  2.70074327e-02 -7.72169679e-02  2.28259489e-01
  2.23986477e-01  2.99961656e-01  5.00052646e-02 -2.43125409e-01
  3.52597237e-01 -5.18848419e-01 -2.23571226e-01 -9.49407369e-02
 -8.08163136e-02  7.00544566e-02 -1.32603914e-01  6.51920289e-02
  1.37074068e-02  2.82392681e-01  4.12441343e-02 -1.20933861e-01
 -4.05624211e-02 -1.61873370e-01 -2.24995557e-02  2.33012468e-01
  1.23463467e-01 -9.58974883e-02  8.98203105e-02  9.56602246e-02
  1.63996875e-01  2.35736549e-01  4.19137836e-01  3.58519077e-01
  5.26255853e-02  2.82866538e-01  9.15442258e-02  1.28970265e-01
  9.20973644e-02  3.88214141e-01  8.73003304e-02  7.29408115e-02
  8.78953934e-02 -3.79019678e-01 -1.62789822e-01  6.44873828e-02
  2.17136785e-01 -2.31420651e-01 -4.74490374e-02 -1.42439872e-01
  1.16999626e-01  6.82556480e-02  1.38984993e-01  2.89386101e-02
  1.74250737e-01 -9.29027945e-02  8.25705472e-03  8.31517130e-02
 -7.20614716e-02  2.37124190e-01  2.85924494e-01  5.25834322e-01
 -7.42283762e-02 -2.71074533e-01  2.73140110e-02 -1.82814151e-02
  2.80142844e-01  2.05262452e-01 -1.20526172e-01 -9.57207009e-02
  6.69382066e-02  4.18438166e-02  2.83885747e-01  5.76630719e-02
 -5.80789685e-01  3.18712741e-02  1.66438639e-01  1.40880436e-01
 -1.25801325e-01 -2.17429444e-01 -2.92287767e-01  1.69847280e-01
 -2.97753423e-01  7.32745081e-02 -6.86627477e-02 -1.41143665e-01
 -9.60276369e-03  1.41160518e-01 -7.20341280e-02  2.66875684e-01
 -7.96582103e-02  1.35552809e-01 -6.47961050e-02 -6.44526035e-02
 -5.05994819e-02  1.60568565e-01  3.08395982e-01 -3.13502550e-02
  1.01212844e-01  3.81016955e-02  2.21739393e-02 -2.48463228e-01
 -1.08154230e-01  1.88106835e-01 -3.06180418e-01 -1.16784260e-01
 -6.43184781e-02  1.81457520e-01 -4.92813528e-01 -1.63886756e-01
 -6.03295900e-02  1.75347075e-01  2.37624452e-01 -3.45463902e-02
  2.80093163e-01 -8.17999542e-02  7.69574642e-02 -1.97669148e-01
  2.05699682e-01 -3.37948859e-01  1.55185774e-01 -1.01977535e-01
  4.08551931e-01  2.68273234e-01  4.20836769e-02  1.01138940e-02
 -2.31908709e-01  1.38412351e-02  6.83597177e-02  1.58457294e-01
 -2.23359495e-01  2.21617535e-01 -2.09341973e-01  3.80622037e-03
  2.96410441e-01  8.32269266e-02 -2.09288836e-01 -2.27167651e-01
  2.24733934e-01  2.06830055e-02  1.92900389e-01  1.56392843e-01
  1.54467762e-01  9.84555334e-02  1.56563044e-01 -1.95664614e-02
 -8.06896314e-02 -2.20885754e-01 -6.56863302e-02 -3.36547315e-01
 -2.06738591e-01  1.62380010e-01  1.14779413e-01 -1.82002276e-01
 -1.69132113e-01 -1.31994814e-01 -2.11515278e-01  8.46594572e-03
 -6.46242797e-02  9.78780463e-02 -3.20826232e-01  6.03388585e-02
 -4.32392284e-02 -1.99803580e-02  1.23936735e-01 -2.48534173e-01
 -1.39699176e-01 -1.48640256e-02 -8.32618624e-02  1.52267411e-01
 -1.30315319e-01  2.74674028e-01 -6.55633435e-02  1.20447636e-01
  3.05731803e-01  2.01878682e-01  1.06711030e-01 -3.32036912e-01
  1.89235300e-01 -2.67021090e-01 -2.79690564e-01 -1.53034091e-01
 -7.35216774e-03  4.92133833e-02 -9.92437080e-03 -2.03740865e-01
 -7.53937662e-02  6.08101636e-02 -6.87418133e-03 -7.99448118e-02
 -1.64773643e-01  1.76757097e-01 -1.07203677e-01 -5.98582737e-02
  1.01249382e-01  1.61754906e-01  3.25819194e-01  1.52985871e-01
 -1.52749747e-01 -1.81421995e-01  1.72189832e-01 -7.84892961e-02
  1.80556113e-03  2.51040220e-01  9.89049748e-02  5.75185359e-01
  1.39900476e-01  4.33599651e-02 -7.09419250e-02  2.39982218e-01
 -1.77970037e-01 -1.02411292e-01 -8.55985731e-02 -1.77849069e-01
  3.11082363e-01 -2.93372393e-01  8.84517282e-02  2.75009032e-02
  3.62654090e-01 -3.64135578e-02 -6.12105206e-02  1.02198139e-01
  5.06836921e-02  3.85391355e-01 -4.94505167e-01  3.37601528e-02
  1.51950583e-01 -2.84370124e-01 -2.39047036e-02 -2.89926946e-01
 -1.67237103e-01  1.20094121e-01 -7.52981082e-02 -6.75758943e-02
  1.68204270e-02  6.18926659e-02  2.26124730e-02 -7.58415610e-02
  2.29902178e-01 -5.54148108e-02  3.33017856e-01 -2.15438660e-03
  1.26595516e-03  1.69646904e-01  7.05155209e-02  7.36275166e-02
 -1.26453973e-02 -6.09093979e-02  1.49604812e-01 -2.48472225e-02
  2.03094110e-01 -3.76961768e-01  2.19900370e-01  4.21574861e-02
 -1.03734462e-02  2.97673881e-01  6.43122643e-02  1.66346639e-01
 -3.31179887e-01  2.07368106e-01  1.72917515e-01 -2.20111161e-02
  4.39289436e-02 -2.07772210e-01 -4.76202726e-01 -9.25044119e-02
 -1.85427512e-03  2.78842181e-01 -8.92497450e-02 -1.13612235e-01
 -4.54798117e-02 -8.91391560e-03 -1.07072458e-01 -2.65443511e-02
 -2.76122361e-01  1.54629260e-01 -9.90808755e-02 -4.35286686e-02
 -1.76585227e-01  1.82567194e-01 -1.04952306e-01 -1.08951733e-01
  4.66270223e-02 -3.05963337e-01 -1.92552149e-01 -3.16922963e-01
 -7.34704435e-02 -8.13263059e-02  1.58769429e-01  2.27108598e-01
  1.08950257e-01 -5.14994338e-02 -7.51552265e-03 -9.38142538e-02
 -1.44855604e-01 -1.15214169e-01 -5.09179868e-02  2.76969552e-01
  1.84392817e-02 -1.35982364e-01  1.52315229e-01  4.62469161e-01
 -4.76342551e-02 -1.68905869e-01 -3.15951556e-01 -2.49888703e-01
  3.72676514e-02 -1.81629270e-01 -2.40476564e-01  1.71589479e-01
  1.34606451e-01  5.84378183e-01 -6.84923679e-02  3.12124133e-01
 -3.24498355e-01 -1.40323207e-01  3.20299476e-01 -1.48270443e-01
  8.43669474e-02  1.63194072e-03  2.82915728e-03 -3.98329124e-02
 -9.31557342e-02  4.72479090e-02  1.97333887e-01 -2.77787000e-02]"
Tensorflow Importation Error type:build/install,"Good day! 
I have a project and i need TensorFlow module (this is the first time using the library)
I have installed the library and try to force-download different lower versions and they got installed successfully but to import now is the issue
Below is the error message it keep bringing up:

ImportError                               Traceback (most recent call last)
~\anaconda3\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     61   try:
---> 62     from tensorflow.python._pywrap_tensorflow_internal import *
     63   # This try catch logic is because there is no bazel equivalent for py_extension.

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>
----> 1 import tensorflow as tf

~\anaconda3\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     35 import typing as _typing
     36 
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     39 

~\anaconda3\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     34 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     35 
---> 36 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     37 from tensorflow.python.eager import context
     38 

~\anaconda3\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     76 except ImportError:
     77   raise ImportError(
---> 78       f'{traceback.format_exc()}'
     79       f'\n\nFailed to load the native TensorFlow runtime.\n'
     80       f'See https://www.tensorflow.org/install/errors '

ImportError: Traceback (most recent call last):
  File ""C:\Users\DAMMYICAN\anaconda3\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 62, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.

I would be glad if you could be of help
Thanks in advance.
",False,"[-4.13611174e-01 -8.09877753e-01 -1.98982894e-01  1.90219909e-01
  2.95064449e-01 -2.81881571e-01 -1.18384399e-01 -2.41884246e-01
 -3.16822410e-01 -1.10796601e-01  5.96891567e-02 -2.47819647e-02
 -2.13128597e-01  3.17196339e-01 -1.42442495e-01  3.95403504e-01
 -1.03088945e-01 -1.73431188e-01  1.63286924e-03  5.18642291e-02
 -7.18419626e-03 -1.12608306e-01 -2.10782558e-01  1.54778138e-01
  2.68089563e-01 -1.28304930e-02 -1.05305731e-01 -3.80655348e-01
  2.77522862e-01 -6.78914338e-02  3.95722985e-01 -9.40454230e-02
 -1.65806338e-02  9.98117700e-02  4.37143371e-02  4.64710861e-01
 -2.31599838e-01 -2.41252899e-01  1.14437811e-01 -5.02036735e-02
  2.51044959e-01  7.35916644e-02  4.35409322e-02  6.19410872e-02
 -1.69898570e-02 -1.01881355e-01  2.28887439e-01  8.04770291e-02
  1.72882691e-01  7.52861649e-02 -2.54712433e-01 -1.70375884e-01
 -5.51459193e-01 -1.56089336e-01 -1.79100156e-01  8.88189003e-02
  3.79609704e-01  1.75194263e-01 -9.31121856e-02  1.74887210e-01
 -1.83789790e-01 -1.13782629e-01  6.02190681e-02  1.10977143e-02
 -1.11510962e-01 -6.72183931e-02  6.57859743e-02 -2.42477626e-01
  5.37525892e-01 -3.17971557e-01 -1.29183590e-01  1.22621864e-01
 -3.92724365e-01  1.38114532e-03  1.66132972e-02  2.08427876e-01
  1.75037473e-01  1.86069727e-01  2.00041547e-01  3.94579098e-02
 -1.82440102e-01 -1.01452664e-01  3.29241753e-01 -2.00133130e-01
  9.90853161e-02  8.20060000e-02 -3.80024165e-02  1.96379423e-01
  1.55903444e-01 -1.70934692e-01  7.50519454e-01  1.53173357e-01
  6.28306996e-03  2.08381817e-01  4.92315739e-01  1.37563303e-01
 -2.73762643e-03  3.94354224e-01  1.01510733e-01 -2.12155804e-01
 -2.84431279e-02 -3.74548405e-01 -1.20626166e-01  2.29443446e-01
  4.48093861e-02  1.94771573e-01 -1.99636538e-02 -1.14349678e-01
  2.61408687e-01 -1.38512880e-01  1.48624301e-01 -1.43738419e-01
  4.69846785e-01  1.00009724e-01  1.23279057e-02  4.32977974e-01
 -2.92206824e-01  2.17295945e-01 -1.15029931e-01  8.88028800e-01
 -3.61792564e-01 -1.10506848e-01  5.99651992e-01 -4.04587612e-02
  7.15573505e-03  5.85076958e-02  4.56564873e-03 -1.26220211e-01
  1.91595834e-02 -1.90199405e-01  1.66301876e-01  2.50889421e-01
 -5.10004997e-01  3.10690701e-02  5.39634451e-02 -2.42656156e-01
 -2.88288057e-01 -4.50142920e-01 -3.52754258e-02 -2.21792132e-01
 -2.53936321e-01  2.89955914e-01 -1.77938581e-01 -3.73592257e-01
  5.10695726e-02 -4.48333248e-02  1.20297551e-01  3.58295083e-01
 -8.74032006e-02 -1.45072520e-01 -6.00905046e-02  1.03453398e-01
 -1.36012286e-01  4.55318183e-01  1.45826995e-01  5.38577557e-01
  4.75265622e-01  3.45090330e-02 -1.13548554e-01 -7.94027328e-01
  1.44327655e-02  2.94278949e-01 -2.07584202e-01  1.89814731e-01
 -1.56907998e-02  2.75221653e-02 -4.48299885e-01 -2.29786858e-01
 -1.11667544e-01  5.30814528e-02 -2.49045014e-01  4.20279913e-02
 -2.22298373e-02  7.41525888e-02  4.06051278e-01  1.45485058e-01
  3.90629917e-01 -7.23306417e-01 -3.22865307e-01  1.28171399e-01
  3.49060297e-01 -3.44563625e-04 -7.48565793e-02 -2.40949709e-02
  8.56563002e-02  2.34948359e-02  4.16190997e-02 -1.54843837e-01
  2.92610470e-02  2.84014881e-01 -2.15471685e-02  1.47169888e-01
  4.43803102e-01 -1.10849142e-01 -1.29700661e-01  2.31630147e-01
  2.20859051e-01  3.53425778e-02  1.27254277e-01  4.63485382e-02
 -4.73759472e-02  9.91146117e-02  1.29822627e-01  2.19896078e-01
  1.68182999e-02 -4.15914029e-01  1.33165181e-01 -2.96013594e-01
 -4.37086940e-01  2.09241756e-03 -7.18141496e-02 -3.59582990e-01
  2.44166497e-02  2.45289579e-02 -4.40654755e-01  1.46464020e-01
  1.88261032e-01 -8.90992135e-02 -1.00737736e-01 -2.17358880e-02
  2.05031745e-02 -3.04899924e-02  4.37595919e-02 -1.75230801e-01
  7.50560835e-02 -1.75563723e-01 -1.20109960e-01  5.55521771e-02
 -1.38185937e-02  2.15402514e-01 -7.54470974e-02  2.35351250e-01
  4.92849261e-01  2.99304593e-02  1.36994362e-01  5.00571057e-02
 -1.39005393e-01 -5.65405600e-02  1.34961247e-01  1.55923739e-01
 -1.61868066e-01 -5.48670851e-02  6.21355213e-02 -4.25625443e-02
 -6.47718608e-02 -3.11677754e-01 -1.94357410e-01 -2.92246819e-01
 -4.49666679e-01  2.73390204e-01 -7.92527944e-02 -4.21159446e-01
  3.17644536e-01  2.64270365e-01  2.48974264e-01  2.36854687e-01
 -1.93077043e-01  5.88774681e-02  1.71253562e-01  4.04562429e-02
  1.49521083e-01  1.14513308e-01 -5.85562736e-03  5.38903356e-01
  6.69105127e-02  2.15542719e-01 -3.79913926e-01 -3.82894874e-02
  4.80503067e-02  1.04119837e-01 -4.98040244e-02 -1.29343584e-01
  1.03324875e-01 -3.83582115e-01  3.54832932e-02  1.56135827e-01
  5.20977139e-01  9.71656442e-02 -2.78814253e-03  1.99828744e-01
  9.47410241e-02  3.93862754e-01 -2.28756472e-01  6.64417073e-02
 -2.08986118e-01 -1.59819424e-01 -6.72720224e-02 -4.41467524e-01
 -2.48924315e-01  1.73507720e-01 -1.28448635e-01  4.35228087e-02
  2.25093171e-01  4.75773588e-04 -4.22346592e-02  1.45171806e-01
 -8.17380846e-02 -1.20721310e-02 -1.30934909e-01  3.56053412e-01
  3.00716814e-02  3.67044270e-01  2.38681078e-01 -3.13249290e-01
 -4.18774039e-02  2.24729441e-02  2.45206103e-01  2.33382508e-02
  2.39722192e-01 -3.73681545e-01  3.51463407e-01 -1.58307374e-01
 -1.02555901e-01  2.96452582e-01 -2.90022586e-02 -8.34289119e-02
  1.22601584e-01  5.56847215e-01  3.56467128e-01 -1.51117325e-01
 -3.77016775e-02 -2.10103631e-01 -3.41486007e-01 -1.26909912e-01
  1.32559806e-01 -1.19872697e-01 -2.29000986e-01 -1.06298238e-01
 -2.53722519e-01  8.00596476e-02 -6.40594959e-02 -5.70725799e-02
 -7.11589530e-02  2.25368887e-01 -2.58409798e-01  1.56180948e-01
 -3.83780092e-01  2.73999423e-01  3.30572352e-02 -5.79884589e-01
  3.73144567e-01 -2.34370083e-01  2.96011388e-01 -3.80371034e-01
 -1.41135097e-01 -2.80776501e-01  4.58216965e-01  3.43673319e-01
 -1.33795127e-01  1.12836827e-02 -3.56242359e-02  1.75346822e-01
 -5.08586586e-01 -5.86772785e-02 -3.03262342e-02  1.29697621e-01
  1.11612201e-01 -2.78261155e-01  1.37739712e-02  6.92760050e-01
 -2.40169913e-01 -1.43973663e-01 -1.78180002e-02 -4.23434019e-01
  2.82309413e-01 -2.54071265e-01 -3.13541889e-01 -1.19817838e-01
  1.64318457e-02  3.71196032e-01 -1.72334522e-01  2.71732956e-01
 -4.52590764e-01  1.26185566e-01  4.66211855e-01 -3.15977752e-01
 -5.66479027e-01  2.45097339e-01  7.43568242e-02 -2.22214550e-01
 -8.59971493e-02  1.32529706e-01  7.96860382e-02 -3.09443504e-01]"
Tensorflow Im type:support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",False,"[-3.70544761e-01 -7.09184647e-01 -1.47585839e-01  5.87240607e-02
  5.67126833e-03 -3.92902255e-01 -2.35594332e-01 -2.18460917e-01
 -1.94321901e-01 -1.52209938e-01 -3.51014957e-02  1.39198273e-01
 -1.68132275e-01  1.10170566e-01 -6.98803589e-02  3.49107683e-01
 -1.03872851e-01 -7.55293444e-02 -7.92351663e-02  8.16360414e-02
  2.69147512e-02  4.81682941e-02 -1.26968566e-02  8.76541287e-02
  1.24357454e-02  3.09410058e-02 -1.26703352e-01 -2.50035405e-01
  6.93569109e-02  2.90150829e-02  5.26098371e-01 -1.08505422e-02
 -9.30635184e-02 -8.50898027e-02  4.26177755e-02  2.13783816e-01
 -6.46315962e-02 -3.33778620e-01 -3.23681980e-02  4.45651915e-03
  2.97019631e-01  8.01871046e-02  1.71501175e-01  1.98411465e-01
  1.13521308e-01  3.98523621e-02  8.78980570e-03 -1.68832496e-01
  8.28693211e-02 -1.96490623e-02 -1.64057791e-01 -4.40427423e-01
 -3.23680699e-01 -1.04264408e-01 -3.40295359e-02 -1.53653413e-01
  5.08406609e-02 -8.76535662e-04  4.29944741e-03  1.33933634e-01
  1.89634822e-02 -1.84425395e-02  2.22431049e-01  1.74656510e-01
  2.09753215e-02  8.91462564e-02  1.04001999e-01 -1.21735185e-01
  3.97260994e-01  4.32299674e-02  1.54220432e-01  3.13004330e-02
 -2.09468603e-01  2.07655728e-01  8.86282623e-02 -9.45763439e-02
 -2.86159307e-01  3.08277220e-01  4.01544511e-01  7.80930966e-02
  3.04022450e-02  3.40519287e-03  3.87691438e-01 -6.30104020e-02
  9.43133533e-02  1.18616767e-01  1.65482819e-01  1.26311183e-01
  1.37604490e-01 -4.02665809e-02  3.08786184e-01  2.16155186e-01
  1.48277432e-02  5.00105396e-02  4.94891018e-01 -1.39480513e-02
 -2.13941447e-02  1.49600804e-01 -2.44999200e-01 -2.69319862e-01
 -2.65410721e-01 -2.18332782e-01 -1.40726883e-02  1.45480275e-01
  2.65869172e-03  1.78659800e-02  1.87612474e-01  1.72338784e-01
 -6.54870644e-04 -7.88841695e-02  2.96592385e-01 -2.01617569e-01
  2.13656008e-01  1.51127586e-02 -6.60863891e-02  1.02973366e-02
  3.61716636e-02  1.44261912e-01 -1.22253887e-01  5.02843857e-01
 -1.91980660e-01 -1.36039495e-01  3.42656597e-02  1.17630690e-01
  3.23294580e-01 -5.90035506e-03 -4.26774681e-01  1.62253693e-01
 -1.08996071e-01  1.52115300e-01  1.43302202e-01  2.10713446e-02
 -1.68218762e-01  7.71927983e-02 -1.56383336e-01 -1.17586344e-01
 -3.16756845e-01 -9.56649780e-02 -6.27528727e-02 -5.11238202e-02
  3.10610421e-02  2.85075098e-01 -7.58255422e-02 -3.67957413e-01
  1.36015378e-02  7.52538443e-02 -1.90718770e-01 -5.00885397e-02
 -3.51906847e-03 -1.64559007e-01 -2.29649961e-01  9.94140133e-02
 -5.20542189e-02  3.95923942e-01  1.07924223e-01  1.17496461e-01
  2.43846968e-01 -1.33649021e-01 -1.10278629e-01 -4.87296164e-01
 -3.64854112e-02  2.15024799e-01 -2.55396217e-01  5.08622825e-02
  2.53917038e-01 -5.77566586e-03 -3.70065331e-01 -1.91162914e-01
 -7.95087814e-02  7.86035582e-02 -4.54273894e-02  7.84049481e-02
 -2.35286839e-02  1.04635835e-01  4.56180781e-01  1.50212765e-01
  4.33844864e-01 -5.95195532e-01 -2.12693453e-01  1.20780736e-01
  2.94348210e-01 -7.35848099e-02  2.13437062e-02  6.99092150e-02
  1.18717868e-02  1.19311415e-01 -3.09605777e-01 -1.61896646e-01
 -1.37071460e-01 -1.79629326e-01 -2.20261142e-01 -4.41354364e-02
  5.28798662e-02  6.49294928e-02 -1.21771961e-01  1.54260457e-01
  1.83818951e-01 -9.03164521e-02 -1.56079829e-01  3.76520269e-02
 -1.74499750e-01  9.55798030e-02 -3.12106550e-01 -1.57389596e-01
 -4.20177914e-02 -1.49637103e-01 -2.38145404e-02 -1.56578183e-01
 -3.69160205e-01 -2.04825372e-01  1.10758170e-01 -9.72198248e-02
 -2.09518045e-01  1.17096841e-01 -3.39177191e-01  1.32571999e-03
  1.70027435e-01  2.56312251e-01 -2.97287941e-01  2.62359798e-01
  9.97222215e-03 -9.74535942e-02  1.41036734e-02 -2.33578101e-01
 -5.01414955e-01 -2.05520093e-02  6.63111359e-02  3.63258779e-01
 -1.02472603e-01  3.24784160e-01  1.06246054e-01 -1.58112541e-01
  5.21645308e-01  4.20293845e-02 -9.26215388e-03  1.25440896e-01
 -2.77191103e-01 -8.95847827e-02 -4.86906841e-02 -1.61519684e-02
 -3.54342461e-01 -6.91674054e-02 -3.51686291e-02 -9.88916904e-02
 -5.62561192e-02  6.29497170e-02  2.61024870e-02  5.36826402e-02
 -1.22441523e-01  4.55021739e-01  1.04217842e-01 -4.27754879e-01
  4.79131997e-01  1.87343806e-01  2.22457975e-01  9.29202363e-02
  8.89664441e-02 -7.60665257e-03  6.82466105e-02  2.50482470e-01
  2.62181520e-01  2.93887965e-02  1.49992943e-01  6.47061706e-01
  3.05549562e-01  2.57526338e-01 -2.69123673e-01  1.04720719e-01
 -8.87134150e-02 -1.49415463e-01  1.41606685e-02  7.42298812e-02
  1.83391482e-01 -2.65356123e-01 -2.26681352e-01 -7.93565214e-02
  3.47417623e-01 -1.89278126e-02 -1.14124924e-01 -6.72942698e-02
  2.10126013e-01  1.69855118e-01 -8.09259862e-02 -7.99080953e-02
  1.04307845e-01 -8.65821987e-02  2.03729123e-01 -4.34263021e-01
 -3.44690308e-02 -1.00524515e-01 -1.22198239e-01  5.97070456e-02
  2.47560799e-01  3.24819326e-01 -4.59887311e-02 -1.59905657e-01
  9.88968685e-02 -1.40238211e-01  2.18719780e-01  5.14244556e-01
  1.13638036e-01  8.50035697e-02  8.40944797e-02 -2.64827132e-01
 -2.54778892e-01  2.72524178e-01  5.89884780e-02  4.08355258e-02
  5.23448884e-01 -2.89322995e-03  2.72321612e-01  1.51069611e-01
 -2.77335167e-01  4.71779943e-01  6.37012273e-02 -1.79795399e-02
 -2.08035842e-01  3.68250698e-01 -1.26454383e-01 -1.83576941e-01
 -3.19046043e-02 -1.72151312e-01 -1.71019927e-01  6.69025164e-03
  7.56183118e-02  6.26282394e-02 -3.30727935e-01 -1.06394932e-01
 -3.43815178e-01  1.89724222e-01 -9.53109264e-02 -1.00444749e-01
 -1.20049492e-02  7.20483661e-02 -1.74072757e-01 -6.52334690e-02
 -1.81414127e-01  3.83437097e-01 -9.83170420e-02 -3.01876247e-01
  1.39619987e-02 -1.88114181e-01 -1.72994994e-02 -3.07476759e-01
 -2.10049778e-01 -2.46513143e-01  4.05643344e-01  5.59154809e-01
 -2.43789643e-01  5.93319163e-02 -7.01353252e-02  3.31866682e-01
 -4.99054849e-01  8.51840712e-03 -1.05377279e-01  3.46273839e-01
 -9.19844806e-02 -1.54486550e-02  2.55694389e-02  3.34900200e-01
 -2.12657332e-01  9.93024483e-02 -1.29045561e-01 -8.17691684e-02
  1.67053908e-01  1.46219760e-01  4.89483662e-02 -2.95550786e-02
  2.29549706e-01  4.41525966e-01 -1.63150102e-01  2.39957962e-02
 -2.35029891e-01  1.87253773e-01  2.44755700e-01 -1.33961052e-01
 -1.70367807e-01  1.49160638e-01 -2.35897042e-02  4.56572045e-03
 -2.07721338e-01 -2.00419813e-01 -1.40294939e-01 -1.75491050e-01]"
How to use Tensorflow or TFLite for Renesas Boards. There are almost no tutorials on official tf website also. stat:awaiting response type:support stale comp:lite,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

Renesas Board

### Python version

3.8

### Bazel version

6.1.0

### GCC/compiler version

9

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

AXM-8-256

### Current behavior?

I want to run a resnet50 model on Renesas v4h board using tensorflow or tflite. Does tflite/tensorflow support Renesas v-series boards . If so , then do we need to cross-compile tflite for renesas board  and how to run the model on board?

### Standalone code to reproduce the issue

```shell
How to run tensorflow/tflite models on renesas v-series boards?
```


### Relevant log output

_No response_",False,"[-0.6266103  -0.36389038 -0.1253293  -0.0586081   0.10150865 -0.38557142
 -0.09398982  0.0707069  -0.21694298 -0.32885072 -0.17177825  0.1070343
 -0.31964338  0.0854323  -0.18634272  0.2618707  -0.14056915 -0.0080485
  0.19914968  0.08005572 -0.28859204  0.02923133 -0.34014702  0.28558964
  0.13363616  0.2649783  -0.18287227  0.03208932  0.05220746  0.20303167
  0.09773801  0.1640005  -0.1856328   0.00733402 -0.08797306  0.26017118
 -0.22585253 -0.10418171 -0.34570938 -0.24730831  0.03319918  0.16665736
  0.08711694 -0.07653564  0.03010328 -0.08704647  0.09437241 -0.02398329
  0.05659965 -0.09530188  0.2438744  -0.10956673 -0.36669892 -0.22342141
 -0.0920959   0.18985698  0.24058068  0.04440093  0.03923579  0.08673898
  0.14066723  0.06070036 -0.05813371 -0.02291926  0.18792495  0.26029956
  0.17221689  0.1712131   0.3328526  -0.1223855  -0.00524286 -0.21173982
 -0.24117841  0.0759545   0.22293988 -0.02595116  0.00790661  0.08625565
  0.25909734 -0.12323309  0.01309694 -0.25860193 -0.07039897 -0.12918639
  0.12600018 -0.0978574   0.29211515  0.09920244  0.525984   -0.33832425
  0.5544014   0.4314695   0.1625065  -0.04113924  0.12177265  0.08562239
  0.10826854  0.30330253  0.17668313 -0.0357292  -0.07624152 -0.13083668
 -0.022669    0.04210266 -0.116097   -0.3225573   0.2994174   0.00474832
  0.01283888 -0.13417244  0.20076221  0.07281369  0.13482438 -0.07637837
  0.04112155 -0.03596956 -0.08122733  0.00848221  0.19305873  0.67193943
  0.17232212 -0.05506398 -0.00412853  0.28647935  0.5178174   0.13608155
 -0.17095448  0.06091388  0.15674675  0.19150083  0.09768269  0.1055202
 -0.05849559  0.09370504 -0.10355215  0.16862163 -0.3193293  -0.00256878
 -0.14505482 -0.051683   -0.28419462  0.22542077 -0.01786796 -0.5211517
  0.10530849 -0.16512364 -0.14654651  0.28446734 -0.20779158  0.2251103
  0.09186812  0.2315277  -0.0523695   0.2561322   0.08979196  0.09645246
  0.27968317 -0.10860245 -0.04986702 -0.6203709   0.06253927  0.39721662
 -0.08955797 -0.1077956   0.26762903  0.15432724 -0.46425086 -0.19357656
  0.17352442  0.43423817 -0.06386514 -0.23496568  0.21463683  0.06150875
  0.3282327  -0.07724951  0.4953929  -0.43431303 -0.24756543  0.21384352
 -0.1134487   0.11135884 -0.04332937  0.07678974 -0.08707987  0.11437517
  0.17925952  0.20451957 -0.3624157  -0.05418168 -0.55696034 -0.0688723
  0.28644714 -0.099475   -0.10543714 -0.03210647  0.10967326 -0.30622053
 -0.29104084  0.07641082 -0.06614012  0.03325653 -0.1411724  -0.00608572
  0.17125896 -0.18628046  0.03574083 -0.5107504  -0.38554913 -0.05130236
  0.3293671  -0.22858031  0.11958907 -0.21267316 -0.21407369 -0.06888924
  0.07834226 -0.05830054 -0.32553178  0.3477389   0.12417329 -0.3078256
  0.1958153  -0.3153782  -0.21507643 -0.03163582 -0.32934406  0.23719577
 -0.14818273  0.08548155 -0.08284885  0.22269374  0.43917388  0.09893861
  0.506409   -0.15701705 -0.2285955  -0.24584016 -0.15279287  0.05191001
 -0.37013555 -0.14297518  0.00197743 -0.14866795  0.21588817  0.5703037
  0.06821366 -0.17319635 -0.2823338   0.12132799 -0.13548833  0.05454097
  0.3266108   0.22553763  0.36715412  0.23505208  0.13042614  0.06366038
  0.07640751 -0.0319581   0.37349606  0.38290554  0.12615076  0.5180373
  0.29734915  0.43814847 -0.31796333  0.18795708 -0.10411296 -0.21575817
 -0.4061225  -0.19732507  0.44630027 -0.3568429   0.18849127 -0.21434301
  0.44499534 -0.24719118 -0.03071     0.15200284  0.36635917  0.19184911
 -0.19979408  0.15057084  0.08179405 -0.2035552  -0.16014272 -0.6135003
 -0.07648797  0.02849099 -0.35543427  0.11621346 -0.11640269  0.0127657
 -0.4392535   0.08527186  0.00505409 -0.30496785  0.02820776  0.12999865
 -0.05503455  0.06517746  0.3383051  -0.33756667 -0.17210314 -0.01334531
  0.2921435   0.13599649  0.3950277  -0.45966202 -0.14203405  0.05457046
  0.05122412  0.20920381  0.11516684 -0.00467661 -0.31188938  0.52541715
  0.08770365 -0.0410004   0.2268982  -0.1908557  -0.2167585   0.09793228
  0.18336695 -0.00533921  0.07739785 -0.37379056  0.03225716  0.24234186
 -0.27872512 -0.08259057 -0.12306073 -0.14320837 -0.27189827 -0.00619974
 -0.00372615  0.12006585 -0.31323528 -0.30229202 -0.26699996 -0.00230346
 -0.17271245 -0.19540453  0.06181979 -0.33447737  0.19359878  0.34846354
 -0.03378436  0.11058186 -0.20529619  0.14078565 -0.31755906  0.03732661
  0.06279387  0.28334796 -0.05579959 -0.15480459  0.1994524   0.30266753
 -0.06368092  0.22972256 -0.3624938   0.04181464  0.22259483 -0.27056718
 -0.20719908 -0.06064072  0.087861    0.11438949 -0.07048542  0.4275382
 -0.18693015  0.1647464   0.47682017 -0.4369033  -0.08844802  0.1564694
  0.00286482 -0.13416228  0.02970928 -0.1316539   0.04311593 -0.11970192]"
Command `pip install tensorflow[and-cuda]` need quotation mark stat:awaiting response type:support stale,"Running this `pip install tensorflow[and-cuda]` command would be wrong as failed to install, but running `pip install ""tensorflow[and-cuda]""` is okay. It just required the quotation mark. Please fix the documentation. ",False,"[-0.5020727  -0.62169844 -0.16100918  0.05966307  0.18679672 -0.25711963
 -0.06310039 -0.01927937  0.0109638  -0.06905496 -0.03973487 -0.28640458
 -0.24703097  0.26668802 -0.27102593  0.11579518 -0.06288192 -0.10238627
  0.3145088  -0.12367529 -0.25909302  0.18352018 -0.35444382  0.18060459
  0.24635333  0.14217228 -0.23263541 -0.33614546  0.5204331   0.1552498
  0.4760218  -0.06210585 -0.03987395  0.10163512  0.17315714  0.36127076
 -0.42477217  0.15180235  0.1235617  -0.28725773  0.45254615 -0.34653458
 -0.18186429 -0.08376808 -0.09397264 -0.20190895  0.22974494 -0.02013688
 -0.08508551  0.21279629 -0.48125812  0.22828723 -0.4212788  -0.11549309
  0.0045377  -0.20143534  0.1407097   0.37281668 -0.08757813  0.060526
 -0.31141067  0.16999581 -0.00198194  0.10114633  0.02296173  0.16540901
 -0.04238424 -0.04172577  0.3364486  -0.14160685  0.02582739 -0.02486086
 -0.3184045   0.04126152 -0.21224652  0.23396845 -0.05902247  0.3462968
  0.00927848 -0.05422789 -0.06479945  0.12047689  0.40740073  0.0094608
  0.13144124 -0.05287882  0.16215068  0.01576806  0.11036603 -0.2624464
  0.15685873  0.30964422  0.22692871  0.21674769  0.27016738 -0.09076522
 -0.15086277  0.45029664 -0.36121872 -0.34754038  0.04224582 -0.04901045
 -0.33482227  0.1430689   0.07086948  0.01762002  0.03362219  0.01993661
 -0.02283255 -0.23582305  0.2327107  -0.02176984  0.2833169  -0.09994038
  0.00526427  0.22782424 -0.4924028  -0.09590378 -0.14392008  0.59235525
 -0.26042098 -0.02142994 -0.15531534  0.04231995  0.13091387 -0.11149564
  0.38931513 -0.09418783  0.21301855 -0.12002262  0.21713378  0.00272324
 -0.11447009  0.4112707   0.21580024  0.08674178 -0.07648464 -0.39333293
 -0.09936979 -0.2934114  -0.06860188  0.30831555 -0.25077862 -0.21762262
  0.07919376  0.12282175 -0.04173011  0.01674674  0.07413293  0.15250267
 -0.12632196  0.09068244 -0.36440566  0.4467941   0.08585008  0.13933873
  0.16852874  0.20332208 -0.26439184 -0.55401844  0.08158806  0.29315075
 -0.331388    0.07481401 -0.2627068   0.26550764 -0.18350795 -0.29857048
 -0.08459935  0.3150181  -0.18666492 -0.02840202 -0.07914277 -0.03968139
  0.00826437 -0.01924234  0.2507987  -0.09696502  0.06258924  0.18363139
  0.21582459  0.34062788 -0.0479081  -0.16850671 -0.02661427  0.14074177
  0.07364505  0.00773627 -0.23460947  0.33027235 -0.03846935  0.1348779
  0.26542714  0.12141758 -0.2198532   0.12697196  0.5839719   0.10901585
  0.0437835  -0.04799706 -0.19064136  0.07640129  0.08375982  0.07923973
 -0.11350739 -0.46270305  0.01027918 -0.11345857 -0.55620456 -0.08311227
 -0.08656224 -0.07697894  0.03323015 -0.2703147   0.09950364 -0.00534733
 -0.06812216 -0.04289945 -0.07877756  0.066286   -0.16277243  0.01139924
  0.30997607 -0.05506282  0.2394391   0.07984284 -0.00442189  0.25889865
 -0.17557591  0.18703474  0.11182149 -0.0915126   0.21860354  0.30558974
 -0.1017001  -0.06784961 -0.00301635  0.14590685 -0.00523331  0.24304414
 -0.34248495 -0.01924383  0.03153925  0.20724627 -0.1160951  -0.05421056
  0.02308446  0.13905483 -0.25427768 -0.0265544   0.1266011  -0.48800567
  0.3669873   0.22037245  0.1204264   0.31117827 -0.32353842  0.04114307
 -0.02438721 -0.32024375  0.4385443   0.45347562  0.01403884  0.09544996
  0.295436   -0.02430059 -0.19936092  0.39316648 -0.09182645 -0.06032219
  0.00435633 -0.08763712 -0.01502484 -0.12354597 -0.10549715  0.03105042
  0.30409437  0.39043203  0.05718211  0.01501145  0.07862727  0.5090106
 -0.73023206  0.0523742   0.01718523 -0.23387039 -0.19910775 -0.40799782
 -0.14735092 -0.09238268 -0.06570061  0.03209191  0.15316226  0.01934626
 -0.19968182 -0.1671464   0.47131523  0.12656145  0.08108863  0.1582759
 -0.37906843 -0.06159545  0.30032292  0.08685371  0.07334348  0.02140316
  0.08870434  0.08295996  0.2347463  -0.12997378  0.35051063  0.1281138
 -0.18495372  0.2622422  -0.00218647  0.00374721 -0.11262916  0.20966181
  0.20565215 -0.02484054 -0.08643091 -0.2271936  -0.45963976  0.06141223
 -0.13584293 -0.06394482 -0.34466127 -0.00130104 -0.19667538  0.40441588
  0.40024284 -0.08346469 -0.22806159  0.09982654 -0.43284857 -0.0678883
 -0.10378968 -0.00494078  0.09584422 -0.34870303 -0.02197428 -0.31377798
  0.11873127 -0.3060808  -0.09301469 -0.37299466  0.3788722   0.08831567
 -0.1414809  -0.15669948  0.03146892  0.0495618  -0.19720197 -0.19520867
  0.27303785  0.08822147  0.17006388 -0.01784572  0.26590157 -0.2145316
 -0.31335923 -0.12545668 -0.18741934 -0.5040166   0.21517126 -0.13634782
 -0.09313463  0.117938    0.03094482  0.1349609  -0.4778434   0.09133177
 -0.12374455 -0.03494583  0.6284593  -0.1365885  -0.37310714 -0.00304832
  0.17872031 -0.21355224  0.3010975   0.19479136  0.33777013 -0.21924244]"
Questions about the interpreters operational flow for quantized neural networks type:support comp:lite TF 2.13,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi. TensorFlow
This is not a post related to issues or bugs, but a post related to questions.

I am curious about how the interpreter operates when a quantized neural network is given to it.
I looked for documentation related to this, but couldn't find it.

Let me give you an example of what I am looking for in a document.
Let's say that the data type of the convolution's input activation is float32, and the data type of the weight is int8.

At this time, the interpreter
operates after dequantizing the weight of the convolution. 
OR,
operates after converting the activation of Convolution to int8.

I'm looking for an explanation like this.

Is there a link with an explanation like this?

### Standalone code to reproduce the issue

```shell
Not required.
```


### Relevant log output

```shell
Not required.
```
",False,"[-0.5058421  -0.32877845 -0.20780393 -0.04181135  0.09571145 -0.38263276
 -0.07875387 -0.05554156 -0.33701697 -0.2874826   0.00257928 -0.1633499
 -0.0889992   0.17341934 -0.03235435  0.32611254 -0.18191105 -0.02135966
  0.05710621 -0.14784537 -0.08134662 -0.0487824  -0.17572866  0.15937844
  0.3384893   0.18606418 -0.19714975 -0.19185486  0.04605801  0.24033523
  0.2845988   0.01208182 -0.04017772  0.15527895 -0.11519238  0.35463804
 -0.10649911 -0.12158236 -0.32882258  0.12287331  0.03823889  0.06587282
  0.05288035 -0.19918296  0.26320708 -0.18495846 -0.1509805  -0.19663152
 -0.25819    -0.24352075 -0.06114792  0.02627867 -0.36874044  0.022151
 -0.05381323 -0.10650767  0.2488853  -0.11214733 -0.11199264  0.1836614
 -0.0391245  -0.05809324  0.02689035 -0.01384384  0.02305339  0.12834734
  0.32218093  0.07738413  0.56141925 -0.19534527  0.04985647 -0.21125743
 -0.34542185  0.12273045  0.12357323  0.01070244  0.03634675  0.25119263
  0.41484082 -0.06981248  0.16903166 -0.2244766   0.05470349 -0.27858377
  0.07658161 -0.062585    0.16930562  0.03743598  0.347581   -0.20402688
  0.35976082  0.23875606 -0.23422241  0.19376555  0.63596666  0.1213313
  0.07970759  0.22638744  0.19348177 -0.23636214 -0.23439535 -0.17663196
 -0.07016323 -0.03171936 -0.14932749 -0.13139918  0.14704761 -0.02215232
  0.20023154 -0.19445696  0.17992777  0.02788337  0.1972546  -0.24192023
  0.05656136 -0.099889   -0.05808372 -0.03023602 -0.01387558  0.7460073
  0.06278026 -0.0131391   0.05238124  0.3403243   0.56722254  0.1436007
 -0.15933606 -0.08617982  0.16234721 -0.05152081  0.10551118  0.13207334
  0.01799062  0.31461012  0.01623377 -0.07607448  0.02972473 -0.14262919
 -0.3296511  -0.19134954 -0.4148772   0.2063196  -0.11479021 -0.70293725
  0.20509571  0.00646166 -0.17170703  0.22796506 -0.00664156  0.03440199
 -0.12332545  0.1339838  -0.04146589  0.21407156  0.22191031  0.20066655
  0.38283    -0.13363609 -0.14753845 -0.5898657   0.15315354  0.33466178
 -0.09573925 -0.23629637  0.16595036  0.24679601 -0.35337836 -0.34785366
  0.07416057  0.40884423 -0.1152362  -0.19282556  0.12605178  0.16465688
  0.0981651  -0.13925278  0.22268513 -0.7256365   0.04244495  0.480555
  0.10898302  0.13554908  0.44028002  0.18790403  0.09435223  0.06582756
  0.12920779  0.33780155 -0.3884768   0.15578222 -0.5222068  -0.21447672
  0.5135181  -0.35592455 -0.20809183  0.08300674  0.16872692 -0.13880017
 -0.29127365  0.24427114 -0.16449055 -0.32561573 -0.1000911  -0.00447297
  0.06937945 -0.18076158 -0.19297235 -0.4898466  -0.44102058  0.14474937
 -0.01956605 -0.36338896  0.20688276  0.04256895 -0.40327013  0.11544058
 -0.05297371 -0.06203389 -0.13302015  0.40156877  0.24774313 -0.15426154
 -0.03823797 -0.4012585  -0.26498666  0.25217667 -0.480493    0.25204182
 -0.12322508  0.16037619  0.23059098 -0.02271337  0.48818848  0.17780286
  0.525258   -0.0735655  -0.13365032 -0.15242738 -0.37917244 -0.00125619
 -0.53614783 -0.21900375 -0.00181059  0.08618898  0.34842676  0.47135517
 -0.36124492 -0.16034175 -0.21492325  0.33694705 -0.171888    0.19172183
  0.17770764  0.13317922  0.3913365   0.1295918   0.26109695  0.15231957
  0.22825292 -0.22264987  0.4420156   0.23436615  0.121736    0.2998643
  0.3804289   0.24498698 -0.48123232  0.5207137   0.09385958  0.04197532
  0.08426933 -0.39614666  0.3996     -0.47926155  0.09407315 -0.07250383
  0.3604133  -0.04994367 -0.21436095 -0.00512093 -0.0124576   0.22670668
 -0.23171511  0.0632319   0.02752316 -0.29538074  0.08148788 -0.67488015
 -0.2067256   0.06486166 -0.22883114  0.12504211  0.01164016 -0.02632435
 -0.15311906  0.18952692  0.13601276 -0.0895658   0.17248261  0.37469542
 -0.19869442 -0.07330325  0.18779914 -0.28898507 -0.1418068   0.01506252
  0.44943517  0.16320479  0.67234397 -0.43276915  0.12387501 -0.17672186
 -0.06584644  0.5077274  -0.05727539  0.09070793 -0.41953197  0.67244554
  0.24606477 -0.12887967  0.08449007 -0.31696182 -0.38045698 -0.2993594
  0.36826256 -0.1581275  -0.07062592 -0.49973345 -0.07841066  0.12955213
 -0.10066248  0.08067346 -0.33417508  0.20078702 -0.08369464 -0.09695639
 -0.39435035  0.2663036   0.05128887 -0.24730799 -0.15137829  0.006807
 -0.33323815  0.00391018 -0.01866263 -0.36992875  0.37185407  0.54456574
 -0.1242753   0.07285576 -0.06326474  0.3404665  -0.30153042  0.07764143
 -0.1657467   0.44792235 -0.11978285 -0.09004635  0.20666908  0.25848794
 -0.23613217  0.20580526 -0.2752456   0.12194319  0.13392392 -0.1875825
 -0.02392673 -0.3636275   0.06718168  0.49847376 -0.04537025  0.26937398
 -0.38202518  0.42916268  0.5736467  -0.40801227 -0.3005588  -0.06974371
 -0.03716705  0.00484941 -0.1997599   0.09604499  0.29661426 -0.06035092]"
"After compilation, only 'libtensorflow_cc.so' was generated, and 'liblibtensorflow_cc.so.ifso' is missing stat:awaiting response type:build/install stale subtype:windows 2.6.0","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.6.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.7.16

### Bazel version

3.7.2

### GCC/compiler version

2019

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

3090

### Current behavior?


""libtensorflow_cc.so""""liblibtensorflow_cc.so.ifso"" were both generated.

### Standalone code to reproduce the issue

```shell
only 'libtensorflow_cc.so' was generated, and 'liblibtensorflow_cc.so.ifso' is missing
```


### Relevant log output

_No response_",False,"[-0.32905233 -0.23881786 -0.14687338  0.02745825  0.4180366  -0.45538533
 -0.32884085  0.15615776 -0.35617292 -0.42711973  0.06482604 -0.05940252
 -0.03624049 -0.00147492 -0.22930989  0.23165637 -0.20861015 -0.24294545
  0.17930137  0.13124713 -0.18017548 -0.01520529 -0.22019252  0.14678007
  0.38250995  0.13572638 -0.13820338  0.04156099 -0.01573479  0.15795408
  0.3902498   0.12121151 -0.03900683 -0.0236917   0.19332777  0.1539179
 -0.01148057 -0.15138458 -0.322002   -0.04319281 -0.17611405  0.12572652
  0.05665768  0.02069154 -0.13343865 -0.10138378 -0.03979253 -0.20915845
 -0.09609755 -0.38201678 -0.04761674 -0.0436305  -0.15956128 -0.2314626
 -0.15368912 -0.11481861 -0.07569467 -0.0844349   0.03596644  0.30183613
  0.26989028 -0.07282221  0.02389051 -0.00508656 -0.02414367  0.29516855
  0.2629681  -0.04331987  0.40221918 -0.19340545  0.12386385 -0.0326106
 -0.2784767   0.04018807  0.1564918   0.22340263 -0.15570155  0.14810269
  0.24481007 -0.2739396  -0.2654158  -0.3011871   0.04673447  0.22857492
  0.20710073 -0.09593819  0.07437134 -0.01134629  0.37404823 -0.10505797
  0.21560822  0.4546459   0.06127454  0.1501917   0.20794705  0.13341255
  0.17861563  0.2644066   0.03121565 -0.09495011 -0.13209929  0.00324748
 -0.01192313 -0.06071223 -0.0439802  -0.30285633  0.35772914  0.06486514
  0.16699126 -0.1930048   0.12600335  0.05855905  0.06477568 -0.03107626
  0.05411816  0.1656707  -0.17733908  0.064083   -0.09912595  0.73648787
  0.07742161  0.09814355 -0.1714367  -0.05782631  0.23016578  0.16575688
 -0.12876001  0.06779187  0.15896899  0.08972755 -0.12952304  0.29909876
  0.17458051  0.06250142  0.18267797 -0.03776472 -0.05871749 -0.06921667
 -0.01103163 -0.04030387 -0.3062216   0.2966932  -0.03563199 -0.5443786
 -0.00948612  0.05679004 -0.19609025  0.3243162  -0.02300108 -0.26807427
 -0.16814506  0.13961461 -0.08770376  0.47588834  0.25122967 -0.02205055
  0.46987933  0.07658753  0.1029745  -0.4017782  -0.07838479  0.4799232
 -0.12941715 -0.18103445  0.2741596   0.11595074 -0.45789915 -0.0776108
  0.20879036  0.22360018 -0.17750517 -0.23146933  0.20760351 -0.08901788
  0.10292553 -0.09189039  0.44462642 -0.4402002   0.09347787  0.23425218
  0.2821529   0.24232538  0.11697555  0.15040338 -0.20120999  0.11275065
  0.01237129  0.18163234 -0.14742547  0.132427   -0.43246114  0.07361582
  0.30183673 -0.10804511 -0.17191796 -0.11523367 -0.06943697  0.0515876
 -0.0617037   0.06405385 -0.17205657  0.12605828 -0.06108216 -0.15067226
  0.07829762 -0.14091794 -0.0544045  -0.4435542  -0.2528103  -0.10492206
 -0.17373851 -0.406642    0.12032468 -0.05016311 -0.20054984  0.15409008
 -0.10238348 -0.10691717 -0.10559002  0.01260318 -0.19118702 -0.38221616
 -0.1664942  -0.29527724  0.01068607  0.07190195 -0.21764615 -0.0381242
 -0.01816503  0.23604709  0.05936787 -0.14412329  0.37355623 -0.0593017
  0.49917912 -0.24052665 -0.10096767  0.11566047 -0.3937008   0.33792663
 -0.4179877  -0.21165223  0.02776533 -0.13390246  0.13671745  0.05465554
 -0.04299541 -0.20999877 -0.32019308  0.14307314 -0.30201674  0.22199848
  0.2846988   0.16327368  0.54246914  0.34054878 -0.06825189  0.1397762
 -0.10195165 -0.1785263   0.14707029  0.17071071  0.11151864  0.10505038
  0.06360815  0.07186598 -0.3516889   0.3568985   0.05106633 -0.12564567
  0.3301381  -0.4903038   0.49723354 -0.26918077  0.06678253  0.12604202
  0.2768978   0.03955342 -0.10923864  0.03357078 -0.02127131  0.27734995
 -0.24498199  0.0517207   0.02374712 -0.02951968 -0.0090807  -0.4098699
 -0.24706131  0.24427454 -0.22176859  0.17535035 -0.061106    0.194747
 -0.15784155  0.0237141  -0.09782416 -0.13628483  0.20242864  0.29205093
 -0.16726688  0.04815854  0.32645905 -0.24549751 -0.21059641 -0.020504
  0.26294228  0.15284732  0.3569472  -0.51124156  0.36837     0.13424915
 -0.06071338  0.45344138 -0.06053994  0.03616861 -0.62585676  0.56095994
  0.19141753  0.06575601  0.3392553   0.00785562 -0.38602018 -0.05744665
  0.37448466  0.08771299 -0.05230581 -0.42575207 -0.17144725 -0.00520406
 -0.00886054 -0.12647837 -0.16687806  0.16343248 -0.13801694 -0.10460122
 -0.30501056  0.33099073 -0.14027879 -0.1736002  -0.18341273 -0.13813248
  0.1412948  -0.2742352  -0.07649823 -0.03783708  0.39971173  0.10727343
 -0.22481957  0.23479393 -0.01680485  0.25877374 -0.4903065  -0.09114404
 -0.05610132  0.237979   -0.13883445 -0.1531109   0.30977035  0.20700517
 -0.35897332  0.2318978  -0.4635222   0.10115469  0.22373447 -0.11846115
 -0.3821975  -0.2167184  -0.08346818  0.33450818 -0.0218787   0.2110847
 -0.35639173  0.47918516  0.29047137 -0.08748849 -0.2526253   0.07744873
 -0.08583874 -0.1974368  -0.03582628 -0.05235092  0.19399518 -0.01086434]"
[Q] ConvertPrimitiveTypeToMLIRType and mlir::IntegerType::Signless stat:awaiting response comp:xla,"I have a question about the code block below https://github.com/tensorflow/tensorflow/blob/master/third_party/xla/xla/translate/hlo_to_mhlo/hlo_utils.cc#L286-L288
hlo_utils.cc - ConvertPrimitiveTypeToMLIRType
```
    default:
      if (primitive_util::IsIntegralType(element_type)) {
        return mlir::IntegerType::get(
            builder.getContext(),
            /*width=*/primitive_util::BitWidth(element_type),
            /*signed=*/
            primitive_util::IsUnsignedIntegralType(element_type)
                ? mlir::IntegerType::Unsigned
                : mlir::IntegerType::Signless);
      }
```
Should we return `mlir::IntegerType::Signed` instead of `mlir::IntegerType::Signless` ?
",False,"[-3.49006712e-01 -1.29998341e-01 -3.44127357e-01 -9.60098282e-02
 -4.41961773e-02 -1.12907942e-02  2.23531500e-01  7.75942355e-02
 -4.39445347e-01 -1.75489560e-01  1.58929229e-01 -2.10649148e-01
  1.52022451e-01 -1.05585694e-01 -1.63937360e-01  1.58295870e-01
  1.55531690e-01  2.10696861e-01 -2.63357162e-01  4.88184951e-02
 -8.29294473e-02  1.55097902e-01 -8.57393965e-02  2.72722334e-01
  1.14283070e-01  1.38520569e-01 -8.15501213e-02  1.14839464e-01
  1.14879869e-01  5.49702384e-02  8.70403126e-02  1.38500005e-01
  2.46707201e-01  9.75751877e-02  8.91952515e-02  7.45607838e-02
 -6.13803081e-02 -9.82940663e-04 -5.98807111e-02  2.02258319e-01
  4.71712276e-02 -5.27724251e-02  7.55693465e-02  8.77156388e-03
  5.44552356e-02 -1.42014667e-01 -1.54544741e-01  1.68089032e-01
 -6.03690185e-03  7.61523619e-02  1.67066418e-02 -1.71031564e-01
 -3.86831105e-01 -3.05328906e-01  1.31623819e-01 -1.73003435e-01
 -2.51676738e-01 -1.03279173e-01  3.35875005e-02  1.54692143e-01
  4.03726920e-02  3.09687182e-02  4.26235318e-01  4.33736518e-02
 -2.91139651e-02  3.09718966e-01  3.86389829e-02 -1.07616119e-01
  4.60209966e-01 -1.01509437e-01  6.32691607e-02  7.98434317e-02
 -4.05315250e-01 -1.84798194e-03  6.25851527e-02 -8.61566365e-02
 -1.85817301e-01  1.35878280e-01  1.50626063e-01 -3.90008152e-01
 -2.13840185e-03 -2.02450842e-01 -5.71565554e-02 -1.01028286e-01
  1.74107909e-01 -1.05178699e-01  2.78492153e-01  8.99370313e-02
  2.89699107e-01  1.48542672e-01  1.86802164e-01  1.29892871e-01
 -2.86314338e-01 -4.31355052e-02  3.07071000e-01 -1.97078362e-02
  5.41900732e-02 -8.72142613e-02  4.97398823e-02  3.41014862e-02
 -3.46367627e-01 -4.07317817e-01 -6.18660487e-02 -3.02829631e-02
  4.36880300e-03 -3.43426973e-01 -2.59901769e-03  8.55597258e-02
  1.25513747e-01 -1.37156457e-01  8.53521526e-02 -2.50014544e-01
  9.27999169e-02  1.74127221e-02 -3.42950076e-02 -2.91326612e-01
  3.76384676e-01  9.58285555e-02 -2.26077884e-01  2.78266877e-01
  7.08499104e-02 -1.79444909e-01  1.85329378e-01  3.24153304e-01
  3.70018244e-01 -1.45307228e-01 -3.83901834e-01  4.85545024e-03
  4.45067324e-02  8.42791200e-02  3.88259649e-01 -7.61707127e-02
 -3.17390025e-01  1.94409236e-01 -5.69843315e-02 -2.89438784e-01
 -2.22661704e-01 -1.26467779e-01 -1.75023913e-01 -4.52280901e-02
 -2.61627883e-01  1.72101319e-01 -9.48430039e-03 -9.37778056e-02
 -1.70324408e-02  1.45899296e-01 -2.73997486e-01  1.15494858e-02
  6.16015755e-02  1.16513908e-01 -1.12620212e-01 -2.57388391e-02
 -7.43956044e-02  1.92841470e-01  1.65571615e-01 -6.79936334e-02
  3.98825437e-01  1.33362353e-01  2.03340173e-01 -4.01904315e-01
 -1.99182168e-01  2.48123735e-01  7.51564056e-02 -2.66160071e-01
  2.61950254e-01  2.33813673e-01 -3.03669333e-01 -4.03061043e-03
 -1.79532990e-02  2.64073431e-01  2.50323042e-02  1.97088160e-02
 -7.20502734e-02  1.60776883e-01  2.13500306e-01 -1.06015079e-01
  3.42338681e-01 -2.77137697e-01 -1.47980601e-01  1.22214004e-01
  2.49376386e-01 -1.03056014e-01  1.52963456e-02  4.59809005e-02
 -1.80562317e-01  5.93242645e-02 -7.89396465e-02  9.86694396e-02
 -8.64277333e-02  5.90617061e-02 -1.82754472e-01 -8.38425942e-03
 -8.05532336e-02 -3.64190713e-02 -1.11624599e-01 -2.16217607e-01
  1.20201305e-01 -3.81903201e-02  5.13496637e-01  1.73255190e-01
 -3.70894283e-01 -7.33784586e-03 -4.42259479e-03 -2.32308388e-01
  3.19167435e-01 -4.31161746e-02 -6.26942515e-02 -4.35610890e-01
 -4.75791395e-02  2.85821795e-01 -8.50837827e-02 -6.80282488e-02
 -2.61137728e-04 -3.48694995e-02 -1.16238639e-01  2.13678017e-01
 -1.50418296e-01 -7.06938580e-02 -5.23839109e-02  3.06961611e-02
 -1.81252807e-01 -4.93182689e-02  3.10431421e-03 -2.53747523e-01
 -1.75041884e-01  6.62692860e-02 -1.16286889e-01 -8.24632868e-03
 -1.72295153e-01  1.64847314e-01  3.27752292e-01  1.58261023e-02
  2.98427284e-01  1.00184202e-01  2.38130838e-01 -1.25001550e-01
 -8.20487961e-02 -1.44005001e-01 -1.62670508e-01 -1.38469338e-01
 -1.58578485e-01  9.12217796e-03 -2.65779465e-01 -1.35390088e-02
  2.40003876e-02  7.14730173e-02 -2.25688294e-01 -6.18657768e-02
 -6.30004704e-02  2.15801015e-01 -3.12337816e-01  1.42476678e-01
  3.54269594e-01 -7.90760815e-02  1.33323986e-02  5.50307930e-01
 -1.18638910e-01 -4.24626470e-02  2.45181434e-02 -1.34147078e-01
  8.19219798e-02  1.71756912e-02  1.72818661e-01  3.51781696e-01
 -3.81207606e-03  1.26711667e-01 -3.85118544e-01 -4.08869330e-03
 -3.33270580e-02 -5.93932942e-02  3.97896111e-01 -3.86986136e-02
  3.86220932e-01 -9.45491195e-02  1.50683429e-02  1.54279247e-02
  3.04428160e-01 -2.17962116e-02 -9.58222002e-02 -8.46658498e-02
 -1.88858777e-01  9.22574177e-02 -1.80328429e-01  6.83894157e-02
  8.62047523e-02 -2.83686250e-01  1.53348729e-01  1.73736870e-01
 -3.71281803e-01  9.22700912e-02 -3.94584760e-02 -5.48046157e-02
  2.18584403e-01 -5.07384613e-02 -9.93741006e-02  5.22521734e-02
  1.24352604e-01 -5.68519160e-02  2.03903258e-01  3.32455486e-02
 -1.26017794e-01  1.93180293e-01  4.59737062e-01  2.28720009e-02
 -6.17983267e-02 -6.99965581e-02  1.87178433e-01  3.10289919e-01
  4.75693524e-01 -2.64575839e-01 -3.06845531e-02 -1.18882246e-02
 -1.96988583e-01  3.86540473e-01 -9.15827826e-02  5.83044626e-02
 -3.76842409e-01  4.03496206e-01 -1.70830637e-01 -1.82822719e-02
 -7.50973970e-02 -1.86214775e-01 -1.79637045e-01  1.72612742e-01
  2.01076955e-01  2.60811508e-01 -4.06924844e-01 -4.48369652e-01
 -4.28005122e-03 -1.82532355e-01 -8.46751034e-02  1.51220441e-01
 -1.13203302e-01 -1.58220083e-01 -2.00343713e-01 -1.85094237e-01
 -2.25523397e-01  1.13093883e-01  3.64415616e-01 -5.78492694e-02
 -2.19267458e-01 -1.50989860e-01 -7.81613886e-02 -3.10486436e-01
 -4.13246267e-02 -9.93222445e-02  3.58755067e-02  2.57346451e-01
  9.49630663e-02  4.03971165e-01  2.90680677e-01  8.96331966e-02
 -1.90444261e-01  1.24444785e-02 -2.17551306e-01  8.62738267e-02
  1.62396636e-02  3.53331923e-01 -2.98165809e-03  4.53248501e-01
 -4.79951277e-02 -1.78806439e-01 -4.41274703e-01  1.03604943e-02
  1.48109704e-01 -2.82871902e-01  1.31594360e-01 -2.54613757e-01
  1.45823345e-01  1.09117307e-01  1.72520243e-02  1.03191294e-01
 -1.09077647e-01  1.21828988e-01 -1.91388614e-02 -5.50477952e-02
  1.91975564e-01 -2.96350569e-02  6.30203485e-02 -2.29025129e-02
  1.60199583e-01 -8.48591626e-02 -2.10636556e-01  1.27936825e-01]"
liblibtensorflow_cc.so.ifso libtensorflow_cc.so type:build/install invalid,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.6.0

### Custom code

Yes

### OS platform and distribution

Windows

### Mobile device

_No response_

### Python version

3.7.16

### Bazel version

3.7.2

### GCC/compiler version

2019

### CUDA/cuDNN version

11.2/8.1

### GPU model and memory

3090

### Current behavior?

1

### Standalone code to reproduce the issue

```shell
INFO: Analyzed target //tensorflow:libtensorflow_cc.so (227 packages loaded, 22492 targets configured).
INFO: Found 1 target...
Target //tensorflow:libtensorflow_cc.so up-to-date:
  bazel-bin/tensorflow/libtensorflow_cc.so
INFO: Elapsed time: 5307.398s, Critical Path: 942.31s
INFO: 14402 processes: 3952 internal, 10450 local.
INFO: Build completed successfully, 14402 total actions
```


### Relevant log output

_No response_",False,"[-4.79638904e-01 -2.94714391e-01 -1.19911134e-01  1.94191009e-01
  3.32128942e-01 -6.15116775e-01 -2.32242852e-01  2.72756666e-02
 -3.51106167e-01 -3.72119695e-01 -5.47842979e-02 -9.88508761e-03
 -2.39403695e-01  1.06311426e-01 -1.19788989e-01  5.73611557e-01
 -2.75873363e-01 -7.31345713e-02  1.50274232e-01  2.41047040e-01
 -9.37463343e-02 -1.77047193e-01 -2.94788510e-01  6.99973702e-02
  2.31252700e-01  1.79919481e-01 -1.54632658e-01  4.14764360e-02
  7.15843067e-02 -5.82369557e-03  6.07385635e-01  2.28110030e-02
  3.21102813e-02  1.67950362e-01  2.75719762e-01  3.63464534e-01
 -8.67688656e-02 -3.56328577e-01 -2.77017266e-01 -9.45975110e-02
 -1.40305087e-01  6.38778210e-02  1.78435564e-01 -1.34538367e-01
  7.44719058e-02 -2.69682348e-01  6.62710220e-02 -1.74121499e-01
  2.30914596e-02 -2.10540473e-01 -1.27285689e-01 -1.39640450e-01
 -4.76218104e-01 -1.31681129e-01 -1.66006297e-01  3.18586826e-04
  1.10203385e-01 -1.72095597e-01  4.71426174e-02  2.75847614e-01
  3.90373290e-01 -5.87363355e-02 -3.52284983e-02  1.55621432e-02
 -8.75321105e-02  3.88839066e-01  2.01683089e-01 -2.14033127e-01
  3.51399213e-01 -1.84144229e-01  1.35862961e-01 -5.73194548e-02
 -4.40916717e-01  4.75237295e-02  8.19700807e-02  8.53651986e-02
  9.37054008e-02  2.38035247e-01  1.28906906e-01 -4.21770774e-02
 -1.74663037e-01 -2.86773086e-01  1.58497654e-02 -5.30145541e-02
  1.90861493e-01 -1.06853500e-01  2.10193098e-01  1.21998698e-01
  5.19621670e-01 -3.75636101e-01  3.80979747e-01  3.89931321e-01
 -1.45075053e-01  1.22993432e-01  3.81065607e-01  1.44981537e-02
  2.89771557e-01  1.57435149e-01  9.34928358e-02 -1.25072539e-01
 -8.79157931e-02 -1.15760960e-01  8.53330344e-02 -1.97103880e-02
 -2.46005803e-02 -1.44019067e-01  2.56468266e-01 -2.96969041e-02
  1.21826790e-02 -4.19752181e-01  2.35695839e-01 -4.03355919e-02
  1.74454898e-01 -1.63904846e-01  8.22876841e-02  1.05890036e-01
 -4.48702276e-01 -8.29769373e-02 -3.67059968e-02  8.95780623e-01
  1.16578005e-01  1.78677112e-01 -3.60321328e-02  1.65139973e-01
  4.53589737e-01  1.36986673e-01 -2.73539890e-02  2.59328075e-02
  4.17170823e-02  8.88993219e-03 -7.74172246e-02  3.64681125e-01
  2.11122036e-01  2.84082383e-01  1.14043012e-01  1.80291280e-01
 -1.66068584e-01 -2.39855975e-01  1.52261257e-01 -1.72212631e-01
 -2.48670399e-01  4.80610967e-01 -1.63215339e-01 -7.11157799e-01
  1.57435000e-01 -7.72975460e-02 -1.41732767e-01  3.41587901e-01
 -6.96942136e-02 -1.98717386e-01 -1.59526408e-01  1.18834212e-01
  2.54699085e-02  4.29612696e-01  2.90861428e-01  1.08757071e-01
  5.75474977e-01  4.35620435e-02 -5.25264721e-03 -6.60226405e-01
 -2.15576470e-01  5.50430775e-01 -2.35715866e-01 -7.83450305e-02
 -1.40429974e-01  1.21850185e-01 -5.44124246e-01 -2.40126327e-01
  1.80110320e-01  4.85892236e-01 -1.28538221e-01 -9.18042064e-02
  2.46410444e-01  1.13118798e-01 -5.60344085e-02 -1.41455987e-02
  6.60144389e-01 -6.70229197e-01 -1.58436932e-02  4.69744027e-01
  2.87060171e-01  2.20305622e-01  1.71430707e-01 -5.43580949e-03
  7.80310854e-02  6.81065395e-02 -3.34732421e-02  1.20486647e-01
 -7.53777847e-02  9.61646996e-03 -4.73364621e-01 -4.68776375e-03
  3.48109722e-01 -2.46240020e-01 -2.67243654e-01  2.29868144e-01
  4.77379039e-02 -1.85117364e-01 -9.93097126e-02 -4.18679602e-03
 -1.50593743e-01 -9.39175636e-02 -3.17377388e-01  5.86898327e-02
 -4.44273874e-02 -1.56426877e-01 -2.68007424e-02 -4.49157119e-01
 -6.16116047e-01  5.21511538e-03 -3.13174985e-02 -5.36971450e-01
  1.89001083e-01 -6.39672875e-02 -2.34107882e-01  2.18956187e-01
  2.61027813e-01 -2.16927566e-03 -1.93846285e-01  3.31725389e-01
 -2.01617740e-02 -3.50929260e-01 -4.69471589e-02 -3.59753996e-01
 -1.52146116e-01  3.23262960e-02 -3.32890749e-01  9.56340507e-03
 -3.23881879e-02  1.92674533e-01 -6.12191074e-02 -3.45777214e-01
  3.93388987e-01  5.22583909e-02  5.09033740e-01 -4.06819489e-03
 -1.38769478e-01 -3.36259604e-02 -3.41188550e-01  2.30159953e-01
 -3.75837386e-01 -1.38956964e-01  1.25918537e-01 -1.61406528e-02
  2.28166267e-01  8.21542963e-02 -1.17825434e-01 -2.27287769e-01
 -2.63793886e-01  2.50790685e-01 -1.91935450e-01  1.47194773e-01
  4.07337844e-01  2.39853784e-01  4.01155740e-01  3.40875745e-01
 -3.55506726e-02  2.58288741e-01  5.11314832e-02 -1.52830988e-01
  1.47349074e-01  1.87341362e-01  6.19072691e-02  2.90002227e-01
  1.78039283e-01  2.18434393e-01 -4.86390471e-01  3.56511533e-01
  9.69445556e-02 -1.76486284e-01  3.67320955e-01 -3.40896457e-01
  6.36842251e-01 -3.85509729e-01 -3.87845822e-02  3.46506760e-02
  3.30830693e-01  2.93779224e-02 -1.11665940e-02  1.33878253e-02
 -3.98114249e-02  2.38332421e-01 -2.23050609e-01 -8.26295242e-02
 -4.15626019e-02 -1.33801669e-01  1.97441846e-01 -6.71809971e-01
 -4.45213377e-01  2.26761147e-01 -3.66345286e-01  2.28236735e-01
 -1.15677737e-01 -1.00240279e-02 -2.12026998e-01  1.02485493e-01
 -1.51533969e-02 -9.52056125e-02  1.22316487e-01  3.22582483e-01
  5.02858730e-03  1.15393832e-01  2.65958399e-01 -4.31193918e-01
 -2.15783510e-02 -4.51210141e-02  4.49859321e-01  2.67584503e-01
  4.31488156e-01 -6.17940187e-01  2.94274151e-01 -1.42513558e-01
 -1.95873097e-01  3.92268002e-01 -1.07467264e-01  1.84570745e-01
 -3.91328812e-01  6.81264758e-01  3.82249117e-01 -1.01235777e-01
  1.12724692e-01 -3.17271829e-01 -5.87744057e-01 -2.21075863e-01
  3.35750878e-01 -3.65276754e-01 -2.02699795e-01 -5.00438273e-01
 -1.97375864e-01 -5.33379130e-02 -3.38262096e-02 -4.28018682e-02
 -1.50844201e-01  2.18643785e-01 -3.47508758e-01  8.24033767e-02
 -3.77876878e-01  9.93495584e-02 -1.21277489e-01 -1.30553231e-01
 -1.05684042e-01 -1.18640535e-01 -4.57233936e-02 -1.51016563e-01
  6.70655444e-02 -2.52904832e-01  4.13734913e-01  2.79975414e-01
 -1.83566511e-01  2.10773259e-01  9.93538201e-02  4.02232468e-01
 -5.94609141e-01 -4.89186458e-02  1.43626601e-01  8.69213641e-02
  1.36966228e-01 -8.44174027e-02  4.93212521e-01  3.07669222e-01
 -1.74593627e-01  2.58668274e-01 -4.63610828e-01  1.55419603e-01
  3.09492558e-01 -3.05546880e-01 -3.16771477e-01 -1.75094217e-01
 -1.47241503e-01  4.56990659e-01 -1.39525354e-01  2.27084756e-01
 -3.54218900e-01  5.61822534e-01  5.95464885e-01 -3.82614553e-01
 -4.77002501e-01  1.94418848e-01 -1.27727181e-01 -3.02651405e-01
  5.07825166e-02  4.91991378e-02  3.59009713e-01  4.80685011e-03]"
Problemas com instalacao do tensorflow no docker stat:awaiting response type:build/install stale subtype: ubuntu/linux,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

Atual

### Custom code

Yes

### OS platform and distribution

Torizon-Linux arm64

### Mobile device

Linux arm64

### Python version

3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Estou tentando construir um docker, mas somente a lib do tensorflow no funciona. Coisa que no acontecia antes, h pouco tempo atrs, eu no tinha nenhum problema. J tentei diversas coisas como especificar verso do tensorflow, e do tensorflow-cpu-aws no requirements, atualizo o setuptools, construo a wheel, construo o build.

### Standalone code to reproduce the issue

```shell
Antes usando mesma imagem eu no tinha nenhum problema com instalao das libs, agora eu tenho problema somente com a lib do tensorflow.
```


### Relevant log output

```shell
FROM python:3.10

RUN apt-get update && apt-get install -y libgl1-mesa-glx

RUN pip install --upgrade pip

RUN pip install --upgrade setuptools

RUN pip install -r requirements.txt



Meu arquivo requirements.txt:
pytorch
keras
pytesseract
joblib
facenet-pytorch
flask-cors
flask
pandas
plotly
opencv-python
folium
socketio
regex
paramiko
joblib
numpy
psycopg2
tensorflow



ERRO:
ERROR: Could not find a version that satisfies the requirement tensorflow-cpu-aws==2.11.1; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"") (from tensorflow) (from versions: 2.9.1, 2.10.0rc0, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0)
ERROR: No matching distribution found for tensorflow-cpu-aws==2.11.1; platform_system == ""Linux"" and (platform_machine == ""arm64"" or platform_machine == ""aarch64"")
```
",False,"[-3.03755522e-01 -3.97524595e-01 -1.19750276e-01  4.90848869e-02
  3.71422976e-01 -5.50009310e-01 -1.10796206e-01 -9.17430818e-02
 -3.26313019e-01 -2.85066813e-01  9.05201361e-02  1.26292959e-01
 -2.81810701e-01  2.05971628e-01 -2.29569167e-01  2.60044128e-01
 -4.22283351e-01 -2.34122172e-01  2.17321813e-01  3.46583217e-01
 -1.58777967e-01  1.46796824e-02 -3.02551866e-01  2.02281982e-01
  1.09197974e-01  3.13392788e-01 -4.25556421e-01  1.55852791e-02
  6.90491050e-02  1.20646454e-01  6.16640449e-01  3.08743179e-01
  6.79181144e-02  5.70759028e-02  3.05018455e-01  2.58878648e-01
 -2.79764831e-01 -1.19211450e-01 -2.87450254e-01 -5.35562523e-02
  1.99027389e-01  8.65849108e-02  6.48002625e-02 -3.48187163e-02
  1.78854857e-02 -3.02564979e-01 -6.58534281e-03 -3.39245319e-01
  1.82785660e-01 -3.17234457e-01 -1.98718607e-01  2.45026704e-02
 -3.62260252e-01 -3.45730901e-01 -2.17751637e-01 -1.11994863e-01
  9.61139053e-02  2.06203908e-02 -3.74932773e-03  2.89521754e-01
  3.40327136e-02  6.85235038e-02 -3.29687595e-02 -9.05223712e-02
  4.83158082e-02  1.97945490e-01  2.88477927e-01 -1.04032584e-01
  4.93492424e-01 -1.56225279e-01  1.39677018e-01 -1.18746519e-01
 -3.62282187e-01  4.54332680e-02  1.94253772e-02  1.65801480e-01
  5.34828193e-02  1.54617772e-01  2.16061056e-01  2.84092501e-03
 -2.07814693e-01 -3.24901164e-01 -2.72280797e-02 -2.55271584e-01
  2.09321640e-02 -8.29969198e-02  2.47481823e-01  1.68191314e-01
  4.84196454e-01 -4.09073442e-01  4.22888249e-01  4.45593238e-01
 -1.11478411e-01 -4.08386812e-04  6.84592009e-01  5.53251952e-02
  1.68856174e-01  3.31260532e-01 -2.75861025e-01 -1.56333849e-01
 -1.67896271e-01 -7.65007883e-02  4.79361266e-02 -8.19957033e-02
 -2.34337002e-01 -9.59120095e-02  2.12480277e-01 -1.54032797e-01
  7.06314817e-02 -7.57546127e-02  2.34493837e-01 -7.86962882e-02
  2.26608336e-01 -2.05678791e-02  2.52412595e-02  1.01494910e-02
 -2.57582307e-01 -1.23951659e-01 -2.62334980e-02  6.83624089e-01
 -2.78096318e-01  1.85120180e-02  4.03579026e-02  2.55757511e-01
  2.00316131e-01  9.34357494e-02 -4.83883321e-02 -1.23305805e-02
  1.26974687e-01 -4.74208519e-02  2.06489246e-02 -6.02355041e-02
  4.26880419e-02  1.72317386e-01  9.71447006e-02  5.30524217e-02
  3.85019854e-02 -2.39727706e-01 -2.51467377e-01 -2.73628294e-01
 -1.34647012e-01  1.79217070e-01 -1.11179531e-01 -5.49359798e-01
  4.69817445e-02  9.01151299e-02 -3.04853588e-01  1.25054076e-01
 -1.59398347e-01  3.73699784e-01 -1.94986910e-01  1.28782332e-01
 -4.36375216e-02  3.71901661e-01  1.23158470e-01  2.20777303e-01
  3.68728906e-01 -1.15999922e-01 -1.97867036e-01 -5.02800226e-01
 -9.48249251e-02  5.37356198e-01 -4.77498248e-02 -5.76649792e-03
  1.10242531e-01  1.53615028e-01 -4.78339434e-01 -2.89194077e-01
  4.73088697e-02  5.22783697e-01 -1.47138968e-01 -5.40541150e-02
  7.85583183e-02  1.48886032e-02  1.59300357e-01 -8.14611241e-02
  5.22635937e-01 -5.63290298e-01 -1.01797193e-01  4.43908036e-01
  5.90101853e-02  1.88381910e-01  8.75542872e-04  1.66749045e-01
  1.09618858e-01  1.21314958e-01  1.61065497e-02 -7.42969662e-02
 -2.68427134e-01 -9.74131972e-02 -3.13279212e-01 -4.10946235e-02
  4.73985732e-01 -1.96077023e-02 -2.70699024e-01  3.07511032e-01
  3.41439217e-01 -6.16395753e-03  1.15515716e-01 -1.14695385e-01
 -5.41378111e-02 -9.84208509e-02 -3.78790461e-02  5.40972203e-02
 -4.27859351e-02 -3.23755801e-01 -8.53858441e-02 -1.94950894e-01
 -3.48144352e-01 -5.82050085e-02  1.18143037e-01 -3.65208864e-01
  1.83838487e-01 -2.51417160e-01 -3.41953158e-01  3.86351705e-01
  8.18679705e-02 -4.84380871e-03 -7.53582865e-02  3.61875832e-01
  1.13154799e-01 -2.90217161e-01 -8.93305615e-02 -2.91200191e-01
 -8.47261101e-02  4.32530046e-02 -3.20629716e-01  8.27295035e-02
 -2.60816179e-02  2.83870548e-01  9.39488411e-02  1.67724624e-01
  2.93593287e-01 -3.06516998e-02  4.56098139e-01 -3.92585285e-02
  1.21475495e-02 -1.11779213e-01 -2.73686320e-01  2.28868991e-01
 -3.63451600e-01 -3.54153633e-01  1.56751364e-01  2.11846218e-01
  3.39593112e-01  1.31440893e-01 -1.17270552e-01 -1.72689576e-02
 -4.35166955e-01  1.03000864e-01  1.71675980e-01  1.22221731e-01
  3.52239728e-01  1.32430851e-01  5.31014740e-01  1.91382438e-01
 -6.67914450e-02  3.49360287e-01  2.43775472e-01 -1.08634926e-01
  2.86772937e-01  2.56827950e-01 -7.41247982e-02  3.83636892e-01
  3.07313025e-01  1.77105337e-01 -4.29604977e-01  5.40412188e-01
  2.29449451e-01 -2.46543214e-01  1.85808927e-01 -3.34676474e-01
  4.33553606e-01 -4.58606631e-01 -3.07883006e-02 -6.71205819e-02
  4.08723772e-01 -4.36302274e-03  4.58165668e-02 -2.14253068e-02
  9.73862261e-02  3.34169298e-01 -5.12749791e-01  6.13730326e-02
  2.17477620e-01 -1.70187742e-01  2.90173292e-02 -6.85363650e-01
 -3.96532655e-01  2.45577157e-01 -3.19998652e-01  6.34278636e-03
 -2.49547243e-01  2.37310499e-01 -1.82658136e-01 -1.38074011e-01
  6.91588819e-02  3.90398614e-02  1.50798663e-01  2.94750571e-01
 -2.43085399e-01 -8.94588307e-02  4.54021811e-01 -6.06700003e-01
 -2.86690086e-01 -1.41439497e-01  2.01445907e-01  2.18109488e-01
  4.59618926e-01 -4.59250242e-01  2.38025740e-01 -4.38284986e-02
 -9.06095356e-02  4.28062171e-01  1.88746989e-01 -1.67119175e-01
 -4.72246557e-01  7.60469377e-01  1.16905563e-01 -2.04189137e-01
  2.62964249e-01 -1.02279186e-01 -3.83637369e-01  1.96576431e-01
  2.93094605e-01 -9.06180739e-02 -9.82211307e-02 -3.76501530e-01
 -8.97715241e-02  5.48922792e-02 -4.64177728e-02 -8.61685425e-02
 -1.46033764e-01  2.04370543e-01 -1.93132833e-01 -1.13109998e-01
 -4.25490856e-01  1.96050823e-01 -1.19758092e-01 -5.61775267e-01
  1.12573244e-02 -1.59984574e-01  1.20166637e-01 -3.71407986e-01
  6.00021966e-02 -3.18660855e-01  3.53948623e-01  4.47451890e-01
 -2.24761993e-01  1.98939174e-01 -2.70955674e-02  2.43080944e-01
 -4.83886003e-01 -6.37611002e-02  2.90515274e-03  3.94947588e-01
  1.16846718e-01 -1.10928565e-01  3.88769090e-01  1.31040663e-01
 -1.83069393e-01  1.72475994e-01 -4.56494123e-01 -4.87231985e-02
  1.59456134e-01 -1.34620622e-01 -2.75527358e-01 -1.59747750e-02
  9.76218879e-02  3.49403620e-01 -7.95199946e-02  2.60854840e-01
 -3.11264873e-01  2.01701999e-01  4.20009375e-01 -3.55119348e-01
 -5.04172742e-01  1.98196679e-01 -2.41873227e-02 -2.65860200e-01
 -5.36361560e-02  2.39162501e-02  1.96916953e-01  1.50750920e-01]"
The configuration file of newer tensorflow versions doesn't provide building with sycl/opencl option. Does newer versions of Tensorflow doesn't provide sycl support? stat:awaiting response type:support stale TF2.14,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

9

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

NVIDIA RTX 3090 24 GB

### Current behavior?

I am trying to build Tensorflow version 2.14 from source with sycl/opencl support but when I am running the configuration file it isn't showing any sycl support option. So does the newer versions of tensorflow doesn't provide opencl/sycl support? 
If they do provide support, how to build newer versions of TF with sycl/opencl support?

### Standalone code to reproduce the issue

```shell
download and unzip the TF 2.14 
and run configure file
```


### Relevant log output

_No response_",False,"[-4.90226209e-01 -4.91845250e-01  3.48412804e-03 -4.67769131e-02
  2.03685910e-01 -4.53645170e-01 -4.66578305e-01 -1.07750438e-01
 -2.06848264e-01 -4.43702042e-01  1.02113180e-01  1.38037175e-01
 -1.53514817e-01  1.63419798e-01 -1.74932610e-02  3.22592169e-01
 -9.32793841e-02 -3.59632075e-02  1.77024856e-01  1.56456321e-01
 -3.32348764e-01 -1.07545540e-01 -1.07877932e-01  1.19752318e-01
  1.47624433e-01  2.00043559e-01 -1.71838671e-01 -1.19959921e-01
  1.34022739e-02  2.61120141e-01  4.49751019e-01  1.41672254e-01
 -1.89681500e-01  1.82686076e-02 -1.13817185e-01  2.52699733e-01
 -3.54463100e-01 -1.52047664e-01 -3.33667636e-01  9.70361978e-02
  1.85557865e-02  1.01786509e-01  8.59193802e-02 -1.65252492e-01
  4.13234942e-02 -2.95745790e-01  1.83517992e-01 -7.01353475e-02
 -5.14596477e-02 -1.80012047e-01  1.01969823e-01 -1.34230524e-01
 -4.46472168e-01 -2.27431744e-01 -2.35114247e-01  4.20664027e-02
 -1.38161793e-01 -5.09831980e-02  3.65490951e-02  7.55765289e-03
  1.06406078e-01  1.34113625e-01 -1.36210889e-01 -4.63048443e-02
  1.31413400e-01  1.11745246e-01  4.36232030e-01 -1.99827969e-01
  6.76620483e-01 -2.35258326e-01 -6.03338778e-02 -1.63283885e-01
 -1.70414880e-01  5.60047776e-02 -5.38200773e-02  1.33913696e-01
  1.43798053e-01  2.34356821e-01  1.61329776e-01 -2.13717461e-01
  1.14819296e-01 -2.91063488e-01  8.05202127e-02 -2.15475842e-01
  2.13989720e-01  1.28002763e-02  4.32689548e-01  1.31333753e-01
  1.40598491e-01 -2.84053445e-01  4.59060162e-01  3.28781307e-01
 -3.12676020e-02 -4.95500341e-02  3.81905913e-01  1.59639210e-01
  1.08574130e-01  4.60179180e-01  1.22600362e-01 -2.19590634e-01
 -1.40220523e-01 -1.95467576e-01  8.70392993e-02  5.56580797e-02
 -1.22680068e-01 -1.56735182e-01  1.78455681e-01 -1.35894239e-01
  2.69448087e-02 -2.64521241e-01  1.65531695e-01 -2.18371078e-02
  3.42095405e-01 -1.82010800e-01  8.08250904e-02 -3.09924763e-02
 -3.21951151e-01  1.71513706e-01  9.79181007e-02  9.34287071e-01
  8.57150853e-02 -2.26685941e-01  4.62367013e-02  5.66554591e-02
  6.44471049e-01  1.51722968e-01 -3.92427564e-01 -4.88177538e-02
  9.43353921e-02  3.52453105e-02  1.02298975e-01  8.33666325e-02
 -1.47530790e-02  3.56402159e-01 -7.98970312e-02  7.69677982e-02
 -2.91032553e-01 -1.24479458e-01 -1.67626262e-01 -2.07667947e-01
 -2.78258920e-01  8.89571533e-02 -1.00192554e-01 -6.24037862e-01
  1.04960009e-01  1.85008287e-01 -1.55244008e-01  1.80880144e-01
 -2.49516577e-01  5.74296489e-02 -1.99344397e-01  5.26026711e-02
  1.66300759e-02  3.15302610e-01  3.90056759e-01  2.57452011e-01
  2.36533865e-01 -1.66946515e-01 -5.83434887e-02 -5.29504180e-01
  1.36122316e-01  5.37029028e-01 -3.54763746e-01 -1.57954097e-01
 -1.66657835e-01  1.69601768e-01 -2.90877163e-01 -3.59653592e-01
  1.52880833e-01  4.76665199e-01 -9.03809667e-02 -1.13281846e-01
  1.59006372e-01 -3.71595025e-02  6.23914078e-02  7.51736686e-02
  5.46959281e-01 -8.10818791e-01 -4.58371341e-02  3.73890251e-01
 -1.89495146e-01  5.14275022e-02  5.24109229e-04  9.36238766e-02
  3.54680270e-02 -5.66877685e-02 -8.46188292e-02  7.97376186e-02
 -2.63740599e-01  2.75595393e-02 -4.18540478e-01 -2.97460079e-01
  4.06964839e-01 -1.17008001e-01 -6.24337569e-02  5.63990995e-02
  2.33937830e-01 -1.51894301e-01 -1.65994734e-01 -2.48256959e-02
 -1.81608111e-01  3.81993353e-02  9.59062651e-02 -8.79014656e-03
  1.59989357e-01 -3.34491700e-01 -4.36273962e-02 -3.49243015e-01
 -5.72175860e-01 -1.52303323e-01  2.11290449e-01 -2.97911823e-01
  1.92717880e-01 -4.91352845e-03 -4.26835209e-01  2.66283929e-01
  1.86679557e-01 -1.54925466e-01 -2.40439937e-01  3.33290875e-01
  1.35689497e-01 -2.00322568e-01 -9.29430500e-03 -3.51508796e-01
 -1.93581671e-01  7.45580643e-02 -3.03019583e-01  1.32878616e-01
  5.41069284e-02  3.65837634e-01  3.68142426e-02  1.48331717e-01
  4.45429027e-01  1.42300993e-01  4.44505304e-01 -1.20222874e-01
 -9.70669687e-02 -1.26346678e-01 -2.10305721e-01  1.32110506e-01
 -4.75962520e-01 -2.53798693e-01  4.72402014e-02 -7.40731880e-02
  2.02760771e-01  4.23801810e-01 -1.62047837e-02 -2.02362210e-01
 -2.18083084e-01  1.81334183e-01 -1.25413418e-01  1.92622080e-01
  3.74709785e-01  3.31914961e-01  4.88318533e-01  1.86291277e-01
  6.10963479e-02  1.53661013e-01  1.54154763e-01  1.84163079e-02
  5.40314376e-01  1.55247211e-01 -5.86627759e-02  5.72599411e-01
  2.36501589e-01  2.72293150e-01 -3.60636204e-01  3.97755474e-01
  1.58317938e-01 -1.47037864e-01  4.75255661e-02 -3.53215098e-01
  4.35387075e-01 -4.74348217e-01  3.33578624e-02  1.03419587e-01
  4.36796546e-01 -3.68126808e-03 -6.33729771e-02  2.90850639e-01
  1.91393733e-01  1.83977813e-01 -3.25487673e-01 -1.67138025e-01
  7.20773488e-02 -1.23798013e-01  1.77457090e-03 -6.50634587e-01
 -1.61629394e-01  1.04355723e-01 -3.21568310e-01  1.12789541e-01
 -1.12560689e-01 -5.33017144e-03 -4.68634605e-01  8.49440843e-02
  1.17379196e-01 -9.53966230e-02  2.10587949e-01  3.30083370e-01
  2.47565284e-03 -1.28224567e-01  2.82619864e-01 -4.81076777e-01
 -1.98681384e-01 -8.42259228e-02  2.54040062e-01  2.35249966e-01
  4.00102854e-01 -4.00597334e-01  7.34369308e-02  1.69327445e-02
 -1.62465740e-02  5.02985418e-01  3.11855581e-02  1.22025549e-01
 -1.74531743e-01  7.65857458e-01  2.28167683e-01 -1.05776235e-01
  1.43011838e-01 -4.26255576e-02 -3.63068283e-01  2.54324507e-02
  2.56397009e-01  5.62528335e-02 -1.36971831e-01 -4.03989315e-01
  6.24137372e-02  3.46888900e-01 -3.90183516e-02  1.00367703e-02
 -5.56197986e-02  2.08612140e-02 -3.74234438e-01 -9.69753321e-03
 -3.77429754e-01  1.09377213e-01 -1.36472046e-01 -4.52786565e-01
 -9.74114537e-02  1.41098425e-01 -6.66742697e-02 -1.75143808e-01
  1.50237888e-01 -4.18608218e-01  2.51623809e-01  6.07037902e-01
 -3.82896900e-01  2.12078989e-02 -6.16136566e-02  2.96897918e-01
 -4.27074969e-01 -1.06181920e-01  5.36165647e-02  3.05053890e-01
 -1.47576690e-01 -2.28818357e-01  3.94009173e-01  4.16363418e-01
 -3.90408151e-02  1.53154537e-01 -3.57825369e-01  9.55719873e-02
  2.85764486e-01 -3.80769104e-01 -1.42093241e-01 -2.53282636e-01
  3.85668203e-02  4.19464111e-01 -2.03220379e-02  2.51977950e-01
 -2.72915870e-01  1.34961918e-01  5.56009650e-01 -3.43586087e-01
 -4.76792723e-01  1.51447475e-01  9.78028551e-02 -1.26095355e-01
 -1.26176506e-01  2.48771794e-02  1.47277713e-01 -2.28090405e-01]"
"Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. stat:awaiting response stale comp:lite comp:lite-flex","
ERROR: Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding ""org.tensorflow:tensorflow-lite-select-tf-ops"" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select
ERROR: Node number 13 (FlexTensorArrayV3) failed to prepare.

Problems with calling tesonflowlite in C/C++Has anyone encountered this problem",False,"[-2.74149179e-01 -4.33622122e-01 -1.28498584e-01  3.14605758e-02
  1.98365644e-01  1.29166454e-01 -3.06967534e-02 -2.08695352e-01
 -7.38648623e-02 -2.26746246e-01  1.34630680e-01 -8.50102678e-02
 -4.07458022e-02  3.33204806e-01  2.23772258e-01  2.05741614e-01
 -1.12710826e-01 -1.84024751e-01  3.91844884e-02  3.75433415e-02
  4.69557382e-02 -1.58971861e-01 -4.95287366e-02 -9.26066041e-02
  5.19273542e-02  1.50154680e-01 -1.06711417e-01 -2.82508433e-01
  2.20560521e-01  6.04206622e-02  2.91223019e-01  3.39466147e-02
 -1.92762002e-01 -4.18239050e-02 -3.69223446e-01  2.19123840e-01
 -3.28394353e-01 -1.12765610e-01 -1.29979908e-01  9.73145813e-02
  3.44632864e-02  1.91358060e-01 -1.50747020e-02  1.12738818e-01
 -1.13262057e-01 -3.90047468e-02  1.12762965e-01 -4.92049679e-02
 -2.60536373e-01  1.59142446e-02 -1.07291661e-01 -7.02778921e-02
 -3.63970250e-01 -2.40868270e-01  3.98217663e-02  1.18265346e-01
  9.75533500e-02  2.19782852e-02  3.56169231e-03  1.06329523e-01
 -7.72550236e-03  4.68081906e-02 -2.65599430e-01 -1.18650690e-01
 -4.97466177e-02  8.70568752e-02  2.76404619e-01 -2.91512847e-01
  5.40597677e-01 -3.23784202e-01  1.42780274e-01  4.58847694e-02
  7.95913674e-03 -3.22856084e-02 -5.87294325e-02  3.02153856e-01
 -8.58737230e-02  2.46029854e-01  4.36095655e-01 -1.67145848e-01
  1.92015767e-02 -1.42163262e-01  5.42612746e-02  1.85715690e-01
  4.31503385e-01 -3.57283279e-02  9.87203717e-02  1.52500227e-01
  2.25728706e-01  1.83412611e-01  1.96183205e-01  1.17692009e-01
  5.28451912e-02  5.02613634e-02  2.90976286e-01  3.61788332e-01
 -3.78466919e-02  4.00227875e-01 -2.07636744e-01 -1.19641297e-01
 -2.70474494e-01 -1.40561581e-01 -3.02555084e-01  1.78807870e-01
  3.74037772e-04 -3.11414778e-01 -7.05261454e-02  1.08609781e-01
  6.95635974e-02  1.18284576e-01  2.71034509e-01 -1.02432646e-01
  3.71793151e-01  1.56299606e-01 -2.09235430e-01 -2.46024970e-02
  1.66431218e-02  3.16849984e-02 -3.80714051e-02  3.09742600e-01
 -1.53602093e-01 -4.38281037e-02  3.14764380e-01  1.58707917e-01
  2.45255306e-01  4.32027038e-03 -3.71583968e-01 -6.41692653e-02
 -3.70757170e-02  4.56863716e-02  2.66644299e-01  7.68915713e-02
 -2.25505918e-01  2.90791877e-03 -2.18083501e-01 -2.25976944e-01
 -2.29602590e-01 -4.71674442e-01 -4.19438362e-01 -1.48879290e-02
 -6.12698570e-02 -3.28327268e-02 -1.95808619e-01 -1.05018660e-01
  2.09508300e-01  3.34854394e-01  2.96167824e-02  3.99917131e-03
 -7.05259889e-02  2.16867737e-02 -2.50155717e-01  5.03906608e-02
 -2.47514501e-01  2.04359636e-01  1.49083897e-01  1.88311532e-01
  2.83424586e-01  1.49453338e-02 -1.77387387e-01 -4.31361079e-01
 -8.73321667e-02  2.61691570e-01  4.56723869e-02  2.10627485e-02
  1.79112926e-01  1.44557238e-01 -3.27079713e-01 -3.89786288e-02
  9.28537846e-02  1.54015586e-01  6.48687035e-02 -2.03077812e-02
 -1.14833653e-01  1.12095261e-02 -5.45561984e-02  2.08468456e-03
  2.09233314e-01 -4.67383742e-01 -2.56627314e-02 -2.56859027e-02
  1.95057660e-01 -8.50368589e-02  1.25439614e-01 -6.38545156e-02
 -3.25709999e-01  1.97696030e-01  1.43151626e-01  1.12185493e-01
 -3.57512146e-01  8.91142786e-02 -4.03805435e-01  4.20926325e-03
  5.05153127e-02  1.05664268e-01 -2.55238056e-01 -1.76421136e-01
  2.29670033e-01 -1.01652108e-02  1.11920722e-02  1.83967546e-01
  7.39167556e-02  1.50044352e-01 -1.44379869e-01 -2.26059705e-01
  1.62916452e-01 -1.56857342e-01 -6.82493672e-02 -2.08222881e-01
 -2.96259254e-01 -2.17329375e-02  2.25560844e-01 -2.74987996e-01
 -1.40665710e-01 -8.07528645e-02 -2.70946562e-01 -6.25528470e-02
  1.34332757e-02  4.74310257e-02 -3.00950587e-01  6.87293410e-02
  7.27418661e-02 -1.57236814e-01 -5.48831671e-02 -1.40608668e-01
 -4.18808371e-01 -1.78642243e-01  1.89055264e-01 -1.16783880e-01
  8.78897309e-02  2.71176696e-01 -2.70071775e-02 -1.88625008e-01
  4.03299540e-01 -1.80251881e-01  1.75155044e-01 -2.34331504e-01
  1.54874381e-02 -1.21082038e-01 -1.92365974e-01 -9.49362516e-02
 -2.91603506e-01 -2.54361331e-01 -1.51756614e-01 -6.27175793e-02
  5.74454516e-02  1.00365274e-01  9.89292115e-02  3.44801366e-01
 -4.61274832e-02  2.35894635e-01 -4.65089604e-02 -9.02614444e-02
  3.76610339e-01  2.74481267e-01  3.76620024e-01  2.83397622e-02
 -2.18938574e-01  4.02501822e-02  1.26109138e-01  2.19218731e-01
  2.06316844e-01  4.39178228e-01 -8.87716487e-02  6.78900480e-01
  2.43724898e-01 -1.42527372e-03 -2.42407441e-01  2.44173706e-01
 -3.37703899e-02 -1.73098966e-02  8.73998925e-02 -9.95939746e-02
  1.89195871e-01 -3.77288431e-01 -1.82032213e-02  9.88052338e-02
  1.90095395e-01 -9.10283774e-02 -3.47390413e-01 -7.15956166e-02
  3.73976260e-01  9.18883383e-02 -2.10893556e-01 -1.16254412e-01
  6.38723969e-02 -4.45364714e-01  1.90467894e-01 -5.19524336e-01
 -7.27958977e-02 -4.61282358e-02 -1.10596731e-01  1.24425016e-01
  1.92138940e-01 -5.97309545e-02  3.73039432e-02  2.25972058e-03
  6.58175498e-02 -4.40451540e-02  2.84972936e-01  1.47210330e-01
  4.42275405e-02  1.89120173e-01  2.87969798e-01 -1.36690140e-01
  8.28508660e-02 -1.22291893e-01  3.87172520e-01 -1.32383779e-02
  2.32580662e-01 -1.43594608e-01  1.02695823e-01  8.71931016e-02
  1.74711570e-02  3.00221950e-01 -2.64388829e-01 -2.32578125e-02
  5.10581285e-02  5.27911305e-01  1.65198296e-01  2.52365768e-02
  1.21527880e-01 -1.36071950e-01 -7.47608393e-02 -5.78284152e-02
 -3.80903631e-02  2.31736884e-01 -1.44128054e-01 -3.24894547e-01
 -7.50365406e-02  9.92077291e-02 -6.24357238e-02 -1.23180926e-01
 -2.82644685e-02  1.28821090e-01 -5.68010882e-02 -1.09576145e-02
 -2.54226446e-01  3.65306705e-01 -9.10265148e-02 -3.11862350e-01
 -1.13283889e-02 -1.43282175e-01  4.13542911e-02 -3.02016795e-01
 -6.14751875e-02 -2.83018291e-01  1.50637962e-02  2.99846381e-01
 -6.14650249e-02  4.32946563e-01 -1.68990403e-01  2.39669055e-01
  6.28408194e-02 -1.36491805e-01 -4.88869101e-02  2.31272802e-01
 -1.83072463e-02 -6.22083955e-02  2.02591121e-01  5.93573391e-01
 -1.80866346e-01  3.05167586e-03 -1.51888028e-01 -9.68605354e-02
  1.49563149e-01 -1.87678039e-01  2.69472413e-02 -1.93661168e-01
  2.20140114e-01  4.88792509e-01 -1.77131683e-01  2.01700598e-01
 -2.63645351e-01  8.19415227e-03  1.26928151e-01 -2.04808861e-01
 -2.29144879e-02 -5.77581637e-02  3.57042067e-03 -1.61699265e-01
 -1.97392792e-01 -2.58865595e-01  5.79598993e-02 -2.02539153e-02]"
WSL2 tensorflow install not working type:build/install wsl2 TF2.14,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.14.0-rc1-21-g4dacf3f368e 2.14.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.1 on WSL2

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA: 12.2 cudnn: ??

### GPU model and memory

rtx 3090 24gb 

### Current behavior?

Im trying to use tensorflow 2 on wsl2. I did the installation as in the tf docs.
After Installing tensorflow on wsl2 and running a test script as stated in the relevant code.

I get this some errors.

I have tried training a simple model with resnet which, at first, didnt work because cudnn was missing. After manually installing cudnn it worked but it feels so sketchy given that it should work without having to install it manually.

Also when I train my model (i dont know if this is normal behaviour) only my vram is used, the compute power of my gpu seems to not be used at all when i look at my taskmanager.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.test.is_built_with_cuda()
```


### Relevant log output

```shell
2023-10-11 22:19:14.589975: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-11 22:19:14.590010: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-11 22:19:14.590032: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-11 22:19:14.593224: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0
  warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}""
```
",False,"[-4.89236921e-01 -4.08354670e-01 -1.11248940e-01  1.97607309e-01
  2.74545580e-01 -3.14277977e-01 -4.10944849e-01 -1.60577744e-02
 -3.07193935e-01 -1.95257038e-01 -9.68969315e-02  1.24213509e-01
 -1.12127006e-01  3.20594668e-01 -3.39044705e-02  5.56239367e-01
 -1.10643432e-01 -2.91454136e-01  1.46550387e-01  5.06444834e-02
 -3.77844065e-01 -2.76354730e-01 -1.81544989e-01 -3.39084081e-02
  1.43350095e-01  1.84886843e-01 -4.91388142e-01  6.00304082e-02
  1.59816042e-01  4.07358408e-01  6.79595888e-01 -1.17871597e-01
 -1.36898398e-01  2.39703000e-01  6.32493049e-02  3.22304726e-01
 -1.27531737e-01 -2.70474553e-01 -1.56954557e-01 -4.96334210e-02
 -1.28682911e-01  2.10272968e-02 -3.42257619e-02 -8.43808278e-02
 -1.60695821e-01 -1.69362962e-01  3.22517753e-01 -4.88545336e-02
 -1.17784977e-01 -2.30601594e-01  3.77686992e-02 -7.96275958e-02
 -3.72434348e-01 -1.12060569e-02 -6.05711415e-02  3.27143848e-01
 -5.13552427e-02  3.30991983e-01  4.31758352e-03  1.27211884e-01
  1.46232247e-01  1.14247501e-01  1.65982842e-01 -5.52878790e-02
 -9.68471728e-03  1.96982831e-01  2.47049153e-01 -2.09306210e-01
  5.53769827e-01 -3.96580160e-01 -3.98674794e-02  7.11598247e-02
 -2.78279126e-01  7.04926625e-02  1.43140793e-01  7.33498260e-02
 -2.66561173e-02  1.13994293e-01  3.27793270e-01  2.56558061e-02
 -2.06544429e-01 -2.87038207e-01 -2.38553345e-01  3.06993015e-02
  6.24583103e-04 -2.23087549e-01 -8.83247051e-03  1.18009597e-01
  3.63447607e-01 -2.73577750e-01  5.67979038e-01  2.89425224e-01
 -3.44955586e-02  2.05619991e-01  4.91917491e-01  1.74267054e-01
 -1.44313937e-02  5.45503020e-01  5.45265675e-02 -9.86588746e-02
 -1.33475363e-01 -6.73050880e-01 -1.57853276e-01  1.16063714e-01
 -1.37261394e-02 -2.62721062e-01  2.16271579e-01  1.46279767e-01
 -2.12500319e-02 -1.65991962e-01  1.38111129e-01  2.92597711e-02
  1.57500818e-01 -2.05235220e-02 -1.49806052e-01 -3.86670269e-02
 -5.19999683e-01  1.00241415e-02  3.75603177e-02  5.79476476e-01
  2.01463178e-02 -3.26581337e-02  1.23794645e-01 -1.91815913e-01
  4.26318705e-01 -1.04043789e-01 -4.20978628e-02 -1.16598055e-01
  8.51343796e-02  2.27560386e-01 -2.15366155e-01  2.55312979e-01
  8.21573883e-02  1.44710064e-01  5.07742614e-02  1.65798157e-01
 -1.55188620e-01 -2.09373415e-01 -2.91483235e-02 -3.53861153e-01
 -3.25577140e-01  3.28592926e-01  1.22160904e-01 -7.43800998e-01
  1.03708118e-01  1.85624305e-02  3.19197923e-02  6.19955897e-01
 -2.17365712e-01  2.97726858e-02 -3.98024917e-02  6.10063039e-02
 -1.14553832e-01  3.79794955e-01  6.07406870e-02  1.57779515e-01
  5.75805783e-01  3.13024968e-02  6.41003028e-02 -3.08483005e-01
 -7.18576089e-02  4.75860178e-01 -2.19171137e-01 -1.68333143e-01
 -1.90897062e-01  8.49072933e-02 -4.96232957e-01 -3.32874358e-01
  5.18189780e-02  3.40265840e-01  6.30422831e-02 -4.02611122e-03
  2.95729220e-01  2.04871461e-01  7.34243020e-02 -7.52147883e-02
  7.30805755e-01 -5.89202762e-01  3.55487168e-01  4.32678938e-01
  1.30545706e-01  3.13000351e-01  3.70466590e-01  1.02871969e-01
  1.60285175e-01  1.05863750e-01  1.81263834e-01  2.00466841e-01
 -2.75832236e-01 -2.99620014e-02 -2.31578127e-01 -4.05933075e-02
  4.80727792e-01 -1.98323101e-01 -2.48894989e-01  1.70669984e-02
  2.22397387e-01  2.95166224e-02  1.45225570e-01 -1.17023543e-01
  8.42600614e-02  1.70163646e-01 -2.19551444e-01  1.39324695e-01
  2.16788188e-01 -1.28173470e-01  1.48124009e-01 -3.65823328e-01
 -4.21883374e-01 -3.05098861e-01  2.12556735e-01 -2.14809209e-01
 -1.74230393e-02  7.38304704e-02 -2.67453134e-01  7.63437003e-02
  1.32129118e-02  1.06525943e-01 -3.26698214e-01  2.22203374e-01
 -1.66028857e-01 -2.79059410e-01 -6.63016140e-02 -3.65870386e-01
 -2.71068692e-01  3.76960784e-02 -2.75035083e-01 -1.64548740e-01
 -7.65412487e-03  2.69063175e-01 -1.80764526e-01 -1.97301030e-01
  3.76878351e-01  1.33142591e-01  5.90775490e-01 -2.96707034e-01
 -2.53664762e-01 -5.15195802e-02 -3.15726727e-01  6.93185925e-02
 -2.90689588e-01 -1.78155243e-01 -6.25377595e-02  3.47652435e-02
  2.89444447e-01  4.18819785e-01 -2.49773130e-01 -9.98082384e-02
 -6.65875673e-02  3.10381234e-01  3.80908176e-02  1.89664677e-01
  1.52729645e-01  1.40849888e-01  5.26704431e-01  2.77019560e-01
  9.59016904e-02  2.60795951e-01  2.28173584e-01  3.21362801e-02
  3.52290012e-02  2.39073932e-01  2.17039332e-01  2.21090823e-01
  3.90569329e-01  2.61136204e-01 -3.15450013e-01  3.02100837e-01
 -1.72865182e-01  3.97193246e-02 -4.73684911e-03 -3.48033667e-01
  4.76815164e-01 -3.64374936e-01 -1.29659772e-01  1.91645518e-01
  4.12265152e-01 -1.56167284e-01 -8.00529644e-02  6.74400702e-02
 -2.65141781e-02  3.24858308e-01 -1.88131452e-01 -3.48742664e-01
  1.08858265e-01  4.85137813e-02  1.65230095e-01 -6.39768958e-01
 -3.18622410e-01  3.10107827e-01 -1.83984846e-01 -5.50971143e-02
 -3.58387649e-01 -8.03757384e-02 -4.09725249e-01 -2.61855900e-01
  1.28158584e-01 -8.27693269e-02  4.33915406e-01  1.84692204e-01
 -3.57102513e-01  3.42418969e-01  3.57651442e-01 -3.92508984e-01
 -1.59473363e-02 -3.07223126e-02  3.39114934e-01  1.71224564e-01
  4.51787055e-01 -5.83209932e-01  1.72169015e-01  1.94661990e-02
 -7.45156854e-02  4.44595158e-01  1.74199462e-01  1.05073899e-01
 -3.45232904e-01  2.88938284e-01  2.41957903e-01 -2.45508879e-01
  2.02978134e-01 -2.08713830e-01 -5.01404643e-01 -5.13393641e-01
  2.40991130e-01  4.54148091e-02 -2.63637513e-01 -3.83133829e-01
  2.76370719e-02  1.68947086e-01 -3.06475729e-01 -6.91004656e-03
 -2.08308846e-01  2.70129770e-01 -5.41936040e-01 -4.49376740e-02
 -6.19916618e-01 -3.11142057e-02 -2.04757266e-02 -4.60182816e-01
 -9.10444856e-02 -3.05229813e-01  1.91931687e-02 -2.00121492e-01
  6.92193806e-02  3.75741497e-02  3.44704747e-01  2.15628654e-01
 -1.76621616e-01  3.55188176e-02 -1.02693401e-01  8.72496739e-02
 -2.78972894e-01 -6.60520494e-02  8.77632201e-02  1.13598444e-01
 -1.22157499e-01 -2.28959709e-01  4.48952883e-01  3.39769781e-01
 -2.68357158e-01  3.35343868e-01 -1.22885704e-01 -1.53548736e-03
  1.52540118e-01 -6.67200238e-02 -3.34716976e-01  2.24751964e-01
  5.30694500e-02  6.54934764e-01 -1.43053085e-01  4.00818378e-01
 -5.61866462e-01  1.02299705e-01  3.65845978e-01 -3.82433951e-01
 -3.24827105e-01  1.30277902e-01  3.13709080e-02 -2.72491217e-01
 -1.84377462e-01 -8.48681331e-02  3.28551680e-01 -8.10739547e-02]"
Is there a way to run predict() for the same batch of data on two different sets of weights (same model architecture) stat:awaiting response type:support stale,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

n/a

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi all, sorry If not the write forum to ask such a question, but I am wondering about functionality of TensorFlow.

Basically I want to run the same batch of data, through X different models. These models will have a different set of weights, but are architecturally the same model.

From a technical perspective, I don't see why it is not possible to do in a single set of computations. In practice, it not different than two submodels that share inputs nodes, but the nodes after are just not connected at all, you would have two output nodes (one of each sub-model).

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

_No response_",False,"[-5.10489821e-01 -3.30889791e-01 -2.23680064e-01  1.12967290e-01
  3.50362897e-01 -1.79721922e-01 -8.69307518e-02 -1.26801640e-01
 -2.97671229e-01 -2.31881231e-01  1.28493756e-01 -2.59686969e-02
 -1.61486566e-01 -9.41330567e-03 -2.60985345e-01  1.31524220e-01
 -2.44434252e-01  7.73451291e-04  1.44331872e-01  2.15097189e-01
  7.66634643e-02 -1.26775324e-01 -1.24845244e-01  2.58529723e-01
 -1.28421374e-02  2.96435356e-01 -3.46441567e-01  1.54185206e-01
 -9.79518071e-02  2.04333305e-01  2.71621674e-01  1.96435258e-01
  2.92867254e-02 -6.95047602e-02 -1.67512968e-01  1.74250633e-01
 -2.31810674e-01 -2.07091182e-01 -4.52583969e-01  2.40285456e-01
  8.88978411e-03  1.48454040e-01  2.87520085e-02 -1.73540026e-01
  1.17426785e-02 -1.23370886e-01 -1.49885537e-02 -8.31376240e-02
 -1.52732804e-03 -2.02282429e-01 -5.88934682e-03 -1.91441208e-01
 -4.17262852e-01 -4.51958954e-01 -2.09274307e-01 -1.81448311e-01
  2.14306474e-01 -2.29586095e-01 -1.30376607e-01  5.70577979e-02
 -4.00000513e-02  8.65383446e-02  2.56481290e-01 -1.64507613e-01
  8.49587694e-02  2.74987698e-01  2.09900409e-01  2.24748105e-02
  5.48906147e-01 -6.91357404e-02  2.58084178e-01 -7.56276250e-02
 -4.66603398e-01 -1.36933168e-02 -1.31158620e-01  3.02717030e-01
  1.64850950e-01  1.75988048e-01  4.08792019e-01 -1.29857838e-01
 -6.82168528e-02 -3.52787316e-01 -2.77246088e-01 -3.12881887e-01
  6.42532781e-02 -1.21534139e-01  4.78999466e-01  2.89187670e-01
  6.10933006e-01 -2.35356651e-02  3.69361877e-01  3.31207663e-01
 -1.68529361e-01  8.93706530e-02  5.45906782e-01  5.83537996e-01
 -1.11636013e-01  5.73622063e-02  1.43408686e-01 -8.36284831e-02
 -9.85985100e-02 -2.25517064e-01  1.50478691e-01 -3.32556665e-02
 -6.50963262e-02 -2.26965025e-01 -4.74004671e-02 -2.55474150e-01
  2.03444153e-01 -2.88296670e-01  1.14285558e-01  1.12871453e-02
  3.49919140e-01  3.44901532e-02  4.31679189e-04 -1.69854403e-01
  1.83249891e-01  1.24581791e-02 -1.10919014e-01  7.13441551e-01
  1.85063034e-01 -1.99593142e-01  2.61091620e-01  2.93151796e-01
  6.10576749e-01  2.19908103e-01 -3.35483968e-01  1.25329524e-01
  1.17005572e-01 -1.33008018e-01  3.70044053e-01  9.45077688e-02
 -9.09612626e-02  2.85829782e-01 -3.88083696e-01 -4.77672853e-02
 -2.72604991e-02 -5.64092919e-02 -3.58733982e-01 -6.49463236e-02
 -2.25279048e-01  7.39734471e-02 -2.84595907e-01 -6.18764162e-01
  1.02768198e-01  1.81114838e-01 -8.30364674e-02 -3.61298118e-03
 -2.18366444e-01 -2.29585543e-01 -6.22536205e-02 -8.62665325e-02
 -3.23994383e-02  2.98270941e-01  3.74235898e-01  3.52031253e-02
  2.78795570e-01 -1.02841213e-01  1.52041674e-01 -5.29605031e-01
 -1.25508592e-01  2.09365726e-01 -3.12593967e-01 -3.14314067e-01
  1.34443954e-01  2.68246204e-01 -2.49239728e-01 -1.79945737e-01
  2.93067098e-01  5.18207788e-01 -1.12350583e-02 -2.32050464e-01
  2.40357667e-02 -4.91412953e-02 -7.34059364e-02 -1.33629486e-01
  5.59524000e-02 -6.92685902e-01 -9.50889289e-02  1.68118879e-01
  1.20988779e-01  1.61002856e-03  3.11026126e-02  2.78969258e-01
 -3.39320526e-02 -6.69247657e-02 -8.13391358e-02  1.11924239e-01
 -2.99539804e-01  9.39049758e-03 -3.76241326e-01 -9.54824239e-02
  4.09560949e-01  1.76370084e-01 -1.13438815e-03  1.08919805e-02
  1.56187937e-01 -6.68737963e-02  9.78684276e-02 -1.47154063e-01
 -2.18048885e-01 -7.71994516e-02 -3.91969979e-01 -4.59109247e-02
  1.10891841e-01 -1.86144471e-01 -1.70336485e-01 -3.27608019e-01
 -2.70877600e-01  2.60992467e-01  7.47924205e-03 -3.83761615e-01
  2.58383483e-01  1.50819719e-01 -2.20967993e-01  2.10547894e-02
 -5.57909906e-02  1.89695477e-01 -1.71246022e-01  1.24297515e-01
  9.20985863e-02 -2.29657322e-01  4.08630297e-02 -2.00595960e-01
 -3.49693030e-01  2.11803943e-01 -1.15817472e-01 -1.23294726e-01
  7.04622716e-02  1.99488521e-01  1.05593562e-01 -6.98932633e-03
  2.12552816e-01  2.62958676e-01  4.20924902e-01 -3.38720024e-01
 -6.58330321e-02 -2.05792204e-01 -7.48234093e-02  1.05649240e-01
 -5.72692394e-01 -2.69893646e-01  1.59215964e-02  5.28445467e-03
  1.53058350e-01  4.79718506e-01 -4.71474051e-01  7.97951967e-03
 -3.83690715e-01 -3.04166615e-01 -2.47666582e-01  2.37562299e-01
  3.41833979e-01 -3.03389691e-02  1.95910290e-01  1.58940643e-01
  2.99816519e-01  4.79292989e-01  3.44839662e-01 -2.78956383e-01
  5.70396781e-01  1.20050080e-01  1.66979253e-01  5.12357533e-01
  4.44996506e-01  2.54337788e-01 -4.33355242e-01  3.57368052e-01
  3.78432833e-02 -3.00632060e-01  1.23755671e-01 -3.54189157e-01
  6.21282220e-01 -2.54065096e-01  1.25961155e-01  8.86782482e-02
  2.41037205e-01  5.97347878e-02 -2.25173578e-01  1.07408389e-02
  1.16387531e-01  1.03504241e-01 -4.14608777e-01  1.29292876e-01
 -1.46840036e-01 -3.14123899e-01 -4.15150635e-02 -5.15274644e-01
 -4.34878647e-01  1.26849324e-01 -1.96211129e-01  2.56304555e-02
  7.01397061e-02 -1.30425274e-01  7.02869520e-02  4.39594612e-02
  2.66483784e-01 -8.41711164e-02  2.59340286e-01  2.93765217e-01
 -1.54594243e-01 -1.51218832e-01  4.95228112e-01 -4.54527110e-01
 -3.62163842e-01 -1.50177017e-01  5.05368292e-01  1.79891467e-01
  4.86650795e-01 -3.49490285e-01  1.14594452e-01 -9.90025178e-02
 -1.07233092e-01  5.80456734e-01 -1.06608279e-01 -1.42514035e-01
 -3.77679229e-01  7.35191226e-01  6.42511323e-02  2.06145316e-01
  1.80190235e-01 -1.53840169e-01 -2.32970826e-02  2.92629302e-01
  1.32533133e-01 -6.63415790e-02  1.06921002e-01 -5.02661645e-01
 -5.52528948e-02  1.94304794e-01 -8.46031383e-02 -4.48165461e-02
  8.89023244e-02  5.32253906e-02 -2.17129290e-01 -1.16072118e-01
 -5.13972521e-01  2.49789327e-01  5.18253446e-02 -3.50315392e-01
 -1.76602617e-01 -1.36482134e-01  4.57186066e-03 -2.35775486e-01
  1.56161278e-01 -3.91471267e-01  2.83589482e-01  5.80131531e-01
 -2.87477970e-01  3.26662749e-01  1.41152740e-01  1.51944533e-01
 -4.90178049e-01 -2.02935979e-01  3.90020534e-02  3.82352412e-01
  1.42977118e-01 -5.57684563e-02  2.44165897e-01  4.44028646e-01
 -2.60972738e-01  8.83749947e-02 -3.58763754e-01 -8.95084813e-03
  4.13300037e-01 -3.55257809e-01 -1.89936832e-01 -6.08482540e-01
  1.56166911e-01  8.31782743e-02 -9.55395028e-02  7.60428095e-03
 -3.80091593e-02  3.20166707e-01  4.93176103e-01 -3.28525603e-01
 -2.12258518e-01  1.37652263e-01  3.21692467e-01 -8.19782391e-02
  3.10628444e-01 -3.07543665e-01 -1.49615675e-01 -3.18185650e-02]"
Conda-forge doesn't have tensorflow 1.15 anymore? stat:awaiting response type:build/install type:support TF 1.15,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

1.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have an old project that uses tensorflow 1.15, but recently I can't install the conda environment for this anymore. The version no longer exists on conda-forge, for some reason?
There are versions 1.14 and lower, and 2.*, but no longer 1.15? Why is that?

### Standalone code to reproduce the issue

```shell
conda create --name tf1 python=3.7
conda activate tf1
conda install -c conda-forge tensorflow=1.15
```


### Relevant log output

```shell
PackagesNotFoundError: The following packages are not available from current channels:                                                                                                                                                                                 
                                                                                                                                                                                                                                                                       
  - tensorflow=1.15   

Current channels:

  - https://conda.anaconda.org/conda-forge/win-64
  - https://conda.anaconda.org/conda-forge/noarch
```
",False,"[-0.7134143  -0.34400904 -0.2431834   0.1393539   0.21166685 -0.58919585
 -0.14325583  0.09099171 -0.45247573 -0.07769867  0.06883113 -0.23168138
 -0.32930875  0.26745224 -0.23574248  0.39978743 -0.35778406 -0.2639177
  0.38290703  0.25271374 -0.26638094  0.01639267 -0.42402878  0.3680678
  0.21310481  0.3974849  -0.34714723 -0.01003622 -0.01619635  0.1475283
  0.35454023  0.17476645 -0.1575202   0.15117034  0.14316109  0.11087449
 -0.3098083  -0.07694133 -0.15469813 -0.14167391 -0.10723233 -0.04351681
  0.06680527 -0.13612166  0.07166219 -0.20047592  0.05385786 -0.03575247
 -0.01943133 -0.34724423 -0.2322941  -0.05016252 -0.3530612  -0.3869351
 -0.08306822 -0.122223    0.02793512  0.2237016   0.1652352   0.29020298
  0.09410076  0.12706155  0.07598054 -0.2041939   0.01012074  0.07394984
  0.2886145  -0.08680981  0.40065315 -0.2926279   0.24099891 -0.13722488
 -0.41415706 -0.14267251 -0.10376094  0.11358224  0.02314905  0.08644894
  0.24947125 -0.12303568 -0.15688339 -0.09827581 -0.03874182 -0.28850174
  0.25786257 -0.10913911  0.40675598  0.30393016  0.52817726 -0.4068225
  0.47801718  0.40026262 -0.03473671  0.25467545  0.4282287   0.01786748
 -0.03107448  0.37492478 -0.07092904 -0.16947116 -0.12091134 -0.2767228
 -0.15061185  0.00442085 -0.17771828 -0.05215799  0.27720588 -0.02365443
  0.02680675 -0.09705009  0.3776364   0.05578201  0.18406653 -0.13096443
 -0.0912478   0.04672289 -0.47943518 -0.23604137  0.10599809  0.6075861
  0.05524518 -0.1323609  -0.01528591  0.11337062  0.49798983  0.04484518
  0.01922675 -0.06262236  0.19590273  0.24418974 -0.02615941  0.22339085
  0.07102873  0.38783818 -0.12594292  0.266378   -0.4096647  -0.29081786
 -0.37118107 -0.13368852 -0.22153948  0.30329156 -0.16669756 -0.5899854
  0.12343575  0.03983601 -0.1310539   0.35026667  0.05855094  0.24685468
 -0.05818674  0.08355734 -0.12196304  0.4378295   0.05618877  0.13412167
  0.37345582 -0.03714725 -0.21343559 -0.91395783  0.04198863  0.4922143
 -0.17255083  0.03189557  0.06575812  0.11244816 -0.48594403 -0.41721666
  0.03898891  0.5712585  -0.10581704 -0.25651821  0.09618692  0.22056557
  0.18875365 -0.01636902  0.30132645 -0.4589635  -0.1210437   0.55786407
  0.11948849  0.2141523   0.18753366  0.10711457  0.28068     0.16362901
  0.19625846  0.2098463  -0.34634846  0.02552298 -0.5479174  -0.10257293
  0.3376975  -0.0413914  -0.21478838  0.16351853  0.36994547 -0.21968405
  0.16252023 -0.05355576 -0.33718896 -0.05768184 -0.12626326 -0.01931478
  0.05743437 -0.35873026 -0.16691597 -0.3333547  -0.47153223 -0.04077669
  0.032801   -0.50011396  0.14016216 -0.1783142  -0.33460468  0.26148823
  0.05226243 -0.17862898 -0.24687958  0.08292639  0.374981   -0.2240114
  0.23096952 -0.40630728 -0.1447809   0.02065332 -0.04431425  0.06598017
  0.11461459  0.1735183   0.25401676 -0.01410596  0.36013305  0.19216229
  0.35384876 -0.19839329 -0.12789145 -0.16687077 -0.15082026 -0.07276564
 -0.65397817 -0.12189146  0.12701555  0.08888803  0.4350437   0.56878334
 -0.18892941 -0.02877048 -0.34403417  0.24375378 -0.15032217  0.07355393
  0.35400987  0.25471184  0.38345423  0.11393937  0.2256801   0.19756117
  0.2588721  -0.25988752  0.32606745  0.30554992  0.11983538  0.22432524
  0.18454951  0.5050666  -0.43704438  0.6826452  -0.3677058  -0.00886006
  0.073089   -0.25900623  0.69553787 -0.36580628 -0.04191992  0.02642444
  0.47794098 -0.07147323 -0.02994635  0.04160375  0.1192061   0.39404917
 -0.44630757  0.22955641  0.09119084 -0.36132443 -0.16712272 -0.712706
 -0.11431691 -0.03509822 -0.34463453  0.27326405 -0.05899923 -0.0662124
 -0.16051583 -0.0090403   0.01131381 -0.22167012  0.25943375  0.22593655
 -0.3026967   0.0625694   0.47770703 -0.47319296  0.04649714 -0.23091769
  0.35159433  0.4897126   0.46641812 -0.31138897  0.3090521  -0.31679577
 -0.03835293  0.64011246 -0.02859947  0.07195962 -0.1812771   0.70143825
  0.06499399 -0.17030324  0.28764173 -0.18157199 -0.3165574  -0.05718859
  0.1742453  -0.03933654 -0.11181792 -0.229428    0.02796219  0.36058104
 -0.07761045 -0.09128366 -0.03654668  0.07882712 -0.27085677 -0.1746467
 -0.3749186   0.02589789  0.03748332 -0.3566593  -0.10433243 -0.28777882
 -0.15841962 -0.32768255 -0.02891256 -0.35322493  0.3870261   0.552908
 -0.2731093   0.23069337 -0.01155974  0.1939489  -0.31124476  0.08361801
 -0.06514225  0.13368051  0.01907692  0.02016866  0.33763826  0.05557337
 -0.3684024   0.26607    -0.33929467 -0.08423358  0.04421839 -0.28233546
 -0.25190985 -0.21805523  0.1083083   0.27979633 -0.10041498  0.35298392
 -0.27063665  0.19284594  0.6211885  -0.3454815  -0.36161745  0.32841384
  0.31806034 -0.21819875  0.2991398  -0.15565099  0.40330756 -0.11737645]"
"Tensorflow build with sycl and ComputeCPP throwing error ""missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule"" stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.3","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.3.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

3.1.0

### GCC/compiler version

9

### CUDA/cuDNN version

11.8 / 8.6

### GPU model and memory

RTX 3090- 24 GB

### Current behavior?

I'm trying to build Tensorflow from source with sycl and ComputeCPP backend but when I'm trying to build Tensorflow, I'm getting following errors:
@local_config_sycl//crosstool:cc-compiler-local: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule
Target '@local_config_sycl//crosstool:empty' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:cc-compiler-local'
Target '@local_config_sycl//crosstool:cc-compiler-local' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:toolchain'
/home/vpy2kor/tensorflow-2.3.0/tensorflow/python/BUILD:2998:1: every rule of type cc_binary implicitly depends upon the target '@local_config_sycl//crosstool:toolchain', but this target could not be found because of: Target '@local_config_sycl//crosstool:toolchain' contains an error and its package is in error

### Standalone code to reproduce the issue

```shell
Move to tensorflow folder 
Run ./configure and make the necessary checks for sycl and computecpp
Start building tensorflow : bazel build --verbose_failures --jobs=16 //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_",False,"[-0.38752496 -0.44187284 -0.03109615 -0.01885387  0.18406475 -0.52962697
 -0.17637941 -0.13260189 -0.43647444 -0.3292306   0.11716553  0.00785116
 -0.03257069  0.22646272 -0.01548175  0.35327905 -0.12466136 -0.23872253
  0.33624268  0.18743461 -0.13841757  0.03498415 -0.10553642  0.1852643
  0.14687584  0.0707841  -0.29434997  0.08485281  0.08566336  0.3023656
  0.55177754 -0.05646865 -0.18028553  0.10401276  0.01533113  0.36923587
 -0.29950735 -0.1306546  -0.2111332  -0.05075045  0.02402747  0.16213815
  0.05602148 -0.05828612  0.00776195 -0.17690864 -0.01205678 -0.09226874
 -0.01597465 -0.3708915   0.00740582 -0.07856382 -0.36777186 -0.24285227
 -0.2496659   0.00792537  0.18474177  0.01159149 -0.0132136   0.14675564
  0.21297824  0.03120101  0.22310738  0.01552191  0.16090798  0.2847389
  0.2826588  -0.00795044  0.5236507  -0.05566649  0.24728903 -0.16719648
 -0.2913135   0.14960563  0.17379874  0.10879746 -0.13758472  0.12276337
  0.21147422 -0.09847605 -0.03039648 -0.01985946  0.15007013  0.04157741
  0.2677412  -0.06091416  0.32431328  0.17555165  0.27506277 -0.26882786
  0.4566149   0.37990656 -0.19155435 -0.04790988  0.4483712   0.05105171
  0.06958416  0.4818647  -0.13172628 -0.22434986 -0.247066   -0.2646436
 -0.02561469 -0.08221073 -0.05220737  0.04134546  0.28611702 -0.0848161
 -0.02255846 -0.13089761  0.11811747 -0.05417234  0.14659649 -0.13927594
  0.03538953  0.06425473 -0.2828074  -0.07205815  0.05405339  0.7222419
 -0.11542421 -0.09367447  0.07131357  0.19293065  0.4689344   0.144635
 -0.18714142 -0.12481475  0.07190139 -0.02275217 -0.00343575  0.12965554
  0.24732502  0.18323886  0.2032033  -0.03138197 -0.34013036 -0.16225237
 -0.02031158 -0.2239351  -0.23909628  0.32383808 -0.14464676 -0.50397027
  0.14290798  0.10845379 -0.1460389   0.15379733 -0.0388514  -0.11261534
 -0.03127963  0.10946971  0.02918403  0.47250545  0.22051972  0.36434674
  0.30094755 -0.19567904 -0.03751601 -0.69576156  0.11265025  0.4814732
 -0.19536415 -0.10787186  0.15182006  0.10827249 -0.3452828  -0.23994306
  0.12966369  0.33597946 -0.29636315 -0.21057436  0.04486484  0.15585354
  0.15733778 -0.09047738  0.37548944 -0.7105605   0.02114675  0.34245926
  0.07614806  0.01441049 -0.06711271  0.2244397   0.00792515  0.03952488
  0.07318088  0.08094261 -0.21196783  0.06460735 -0.4511991   0.15121345
  0.1882132  -0.14206281 -0.13219154  0.23077781  0.3715376  -0.02298121
 -0.03431944 -0.04094542 -0.26487702 -0.04269727 -0.08887099  0.02843425
  0.06244017 -0.45301408  0.04176533 -0.3311209  -0.5681134   0.01760357
 -0.05743559 -0.13224474  0.27632725 -0.09690309 -0.2652467   0.08738481
  0.03917147  0.10746202 -0.15404578  0.25622672 -0.06586599 -0.32192075
 -0.1460517  -0.32177854 -0.11288468  0.08192705 -0.3022046   0.18118003
  0.09240443  0.18385722 -0.0372225  -0.04362839  0.43065402  0.13440984
  0.37921464 -0.07568686 -0.13910052 -0.15290412 -0.13066947  0.1862708
 -0.60607445 -0.19415215  0.08987415 -0.04927858  0.35538307  0.26543424
  0.09909026 -0.09242108 -0.273646    0.25226724 -0.32523066 -0.04733368
  0.5155997   0.10847352  0.25160635  0.32799608  0.02676593  0.24038044
  0.04359515 -0.14756611  0.20216897  0.15028402  0.04310539  0.35825545
  0.21329007  0.3694333  -0.4628816   0.41952246 -0.00124157 -0.1868057
  0.18758571 -0.24817742  0.50548357 -0.48218676  0.1210308  -0.01990672
  0.47876382 -0.12520203 -0.09692562  0.21965832  0.03135345  0.29644793
 -0.2705993   0.11313866  0.09567653 -0.32502955 -0.18165484 -0.40774497
 -0.36364156  0.09810597 -0.2903952   0.04662234 -0.23686709  0.05540756
 -0.28193742  0.19799937  0.01294552 -0.15600812  0.04621705  0.31095812
 -0.09263799  0.04307858  0.34049022 -0.5107638  -0.13864112 -0.06885113
  0.20442718  0.1776214   0.41536924 -0.639549    0.18183795  0.06070102
 -0.1337032   0.500265    0.06095447  0.12014525 -0.4252348   0.7738187
  0.09744599 -0.18341269  0.1620444   0.13310337 -0.3187229  -0.03183837
  0.22201374 -0.00188844 -0.01836779 -0.37231785 -0.04317532  0.36079055
 -0.04640043 -0.15277039 -0.18501624  0.08048394 -0.25206554 -0.16480462
 -0.3273852   0.17622353 -0.17244208 -0.44864258 -0.03294548  0.04691152
 -0.09237196 -0.08983035 -0.14810453 -0.29495448  0.39608458  0.5319786
 -0.18424305  0.23139451 -0.043878    0.16904566 -0.6572538  -0.01678511
 -0.0824926   0.28968543 -0.07872701 -0.26819986  0.22152859  0.31181595
 -0.2733785   0.15141319 -0.27757734 -0.11413829  0.24728315 -0.1637977
 -0.2797764  -0.1175398   0.01522434  0.35540068 -0.05465752  0.11313161
 -0.4515528   0.16832873  0.5110234  -0.33508083 -0.3550849   0.19879232
  0.1209343  -0.24630648 -0.09509934 -0.01823783  0.23075378 -0.03761034]"
"GPU not detected even after installing CUDA, cuDNN correct versions type:support","### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14.0

### Custom code

No

### OS platform and distribution

Windows 10 22H2 19045.3448

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA v11.2, cudnn-11.2-windows-x64-v8.1.0.77

### GPU model and memory

GeForce GTX 960M, 4Gb

### Current behavior?

Unable to detect the GPU. TF says: 

`print(tf.test.is_built_with_cuda())
False`

### Standalone code to reproduce the issue

```shell
It is an installation issue. 

C:\Users\user>python
Python 3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> import tensorflow as tf
>>> print(tf.test.is_built_with_cuda())
False
>>> print(tf.config.list_physical_devices('GPU'))
[]
>>>
>>> print(tf.config.list_physical_devices())
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
>>>
```


### Relevant log output

```shell
C:\Windows\system32>
python -V 
Python 3.10.10 (tags/v3.10.10:aad5f6a, Feb  7 2023, 17:20:36) [MSC v.1929 64 bit (AMD64)] on win32
---------------------------------------------------------------------------------
pip list
Package                      Version
---------------------------- ------------
absl-py                      2.0.0
anyio                        4.0.0
argon2-cffi                  23.1.0
argon2-cffi-bindings         21.2.0
arrow                        1.3.0
asttokens                    2.4.0
astunparse                   1.6.3
async-lru                    2.0.4
attrs                        23.1.0
Babel                        2.13.0
backcall                     0.2.0
beautifulsoup4               4.12.2
bleach                       6.1.0
cachetools                   5.3.1
certifi                      2023.7.22
cffi                         1.16.0
charset-normalizer           3.3.0
colorama                     0.4.6
comm                         0.1.4
contourpy                    1.1.1
cycler                       0.12.1
debugpy                      1.8.0
decorator                    5.1.1
defusedxml                   0.7.1
exceptiongroup               1.1.3
executing                    2.0.0
fastjsonschema               2.18.1
flatbuffers                  23.5.26
fonttools                    4.43.1
fqdn                         1.5.1
gast                         0.5.4
google-auth                  2.23.3
google-auth-oauthlib         1.0.0
google-pasta                 0.2.0
grpcio                       1.59.0
h5py                         3.10.0
idna                         3.4
ipykernel                    6.25.2
ipython                      8.16.1
ipython-genutils             0.2.0
ipywidgets                   8.1.1
isoduration                  20.11.0
jedi                         0.19.1
Jinja2                       3.1.2
joblib                       1.3.2
json5                        0.9.14
jsonpointer                  2.4
jsonschema                   4.19.1
jsonschema-specifications    2023.7.1
jupyter                      1.0.0
jupyter_client               8.3.1
jupyter-console              6.6.3
jupyter_core                 5.4.0
jupyter-events               0.7.0
jupyter-lsp                  2.2.0
jupyter_server               2.7.3
jupyter_server_terminals     0.4.4
jupyterlab                   4.0.6
jupyterlab-pygments          0.2.2
jupyterlab_server            2.25.0
jupyterlab-widgets           3.0.9
keras                        2.14.0
kiwisolver                   1.4.5
libclang                     16.0.6
Markdown                     3.5
MarkupSafe                   2.1.3
matplotlib                   3.8.0
matplotlib-inline            0.1.6
mistune                      3.0.2
ml-dtypes                    0.2.0
mpmath                       1.3.0
nbclient                     0.8.0
nbconvert                    7.9.2
nbformat                     5.9.2
nest-asyncio                 1.5.8
notebook                     7.0.4
notebook_shim                0.2.3
numpy                        1.26.0
oauthlib                     3.2.2
opt-einsum                   3.3.0
overrides                    7.4.0
packaging                    23.2
pandas                       2.1.1
pandocfilters                1.5.0
parso                        0.8.3
pickleshare                  0.7.5
Pillow                       10.0.1
pip                          23.2.1
platformdirs                 3.11.0
plotly                       5.17.0
prometheus-client            0.17.1
prompt-toolkit               3.0.39
protobuf                     4.24.4
psutil                       5.9.5
pure-eval                    0.2.2
pyasn1                       0.5.0
pyasn1-modules               0.3.0
pycparser                    2.21
Pygments                     2.16.1
pyparsing                    3.1.1
python-dateutil              2.8.2
python-json-logger           2.0.7
pytz                         2023.3.post1
pywin32                      306
pywinpty                     2.0.12
PyYAML                       6.0.1
pyzmq                        25.1.1
qtconsole                    5.4.4
QtPy                         2.4.0
referencing                  0.30.2
requests                     2.31.0
requests-oauthlib            1.3.1
rfc3339-validator            0.1.4
rfc3986-validator            0.1.1
rpds-py                      0.10.4
rsa                          4.9
scikit-learn                 1.3.1
scipy                        1.11.3
Send2Trash                   1.8.2
setuptools                   65.5.0
six                          1.16.0
sniffio                      1.3.0
soupsieve                    2.5
stack-data                   0.6.3
sympy                        1.12
tenacity                     8.2.3
tensorboard                  2.14.1
tensorboard-data-server      0.7.1
tensorflow                   2.14.0
tensorflow-estimator         2.14.0
tensorflow-intel             2.14.0
tensorflow-io-gcs-filesystem 0.31.0
termcolor                    2.3.0
terminado                    0.17.1
threadpoolctl                3.2.0
tinycss2                     1.2.1
tomli                        2.0.1
tornado                      6.3.3
tqdm                         4.66.1
traitlets                    5.11.2
types-python-dateutil        2.8.19.14
typing_extensions            4.8.0
tzdata                       2023.3
uri-template                 1.3.0
urllib3                      2.0.6
wcwidth                      0.2.8
webcolors                    1.13
webencodings                 0.5.1
websocket-client             1.6.4
Werkzeug                     3.0.0
wheel                        0.41.2
widgetNVIDIA snbextension           4.0.9
wrapt                        1.14.1
---------------------------------------------------------------------------------
Hardware: GeForce GTX 960M
https://www.nvidia.com/en-us/geforce/gaming-laptops/geforce-gtx-960m/specifications/
https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html

CUDA Cores
1096 + Boost Base Clock (MHz)
GTX 960M Memory Specs:
2500 MHz Memory Clock
GDDR5 Memory Interface
128-bit Memory Interface Width
80Memory Bandwidth (GB/sec)
GTX 960M Technology Support: Yes 
NVIDIA Optimus Support 1 Yes 
NVIDIA Battery Boost Support 2
2.0NVIDIA GPU Boost Yes
NVIDIA GameStream-Ready Yes
GeForce ShadowPlay Yes
NVIDIA GameWorks
12 API Microsoft DirectX
4.5 OpenGL 
CUDA Yes
PCI Express 3.0 Bus Support
Windows 8 and 8.1
Windows 7OS Certification

Processor	Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz   2.60 GHz
Installed RAM	32.0 GB (31.8 GB usable)
System type	64-bit operating system, x64-based processor

---------------------------------------------------------------------------------

>>> import tensorflow as tf
>>> print(tf.__version__)
2.14.0

---------------------------------------------------------------------------------
Used table from here: https://www.tensorflow.org/install/source_windows

tensorflow 2.14.0,	Python 3.7-3.10,	cuDNN 8.1	CUDA 11.2

CUDA v11.2, cudnn-11.2-windows-x64-v8.1.0.77, Python 3.10.10
---------------------------------------------------------------------------------

>>> print(tf.test.is_built_with_cuda())
False
>>> print(tf.config.list_physical_devices('GPU'))
[]
>>>
>>> print(tf.config.list_physical_devices())
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
>>>

---------------------------------------------------------------------------------
OS

Edition	Windows 10 Home
Version	22H2
Installed on	2/28/2023
OS build	19045.3448
Experience	Windows Feature Experience Pack 1000.19044.1000.0
```
",False,"[-5.51127076e-01 -4.80247855e-01 -3.23556244e-01  1.67654723e-01
  1.23518869e-01 -3.99978489e-01 -5.65780520e-01  4.93546203e-02
 -1.57034606e-01 -3.66986156e-01  1.07945643e-01 -2.04727262e-01
 -1.11541562e-01 -2.11372465e-01 -4.70777392e-01  6.06518239e-02
 -5.91121316e-02  7.62222335e-02  2.34483302e-01  2.79191285e-01
 -4.96974826e-01 -7.44632185e-02 -5.28033137e-01  2.96973605e-02
  5.22931457e-01  1.22294933e-01  1.19709574e-01 -2.31127828e-01
  1.81338474e-01 -8.53600949e-02  7.41671503e-01  7.28080124e-02
  3.31164658e-01  4.63351309e-01  2.73648411e-01 -1.56047896e-01
 -2.71472812e-01 -5.34573078e-01 -2.27848932e-01 -3.11350048e-01
 -9.14969146e-02 -1.32703066e-01  1.46201015e-01  3.35130915e-02
  1.25693351e-01 -3.75425339e-01  7.98356384e-02 -1.67903960e-01
 -2.07152501e-01 -3.70262980e-01  2.73693681e-01 -6.52957559e-02
 -3.30602467e-01 -1.69102952e-01 -1.65829062e-01 -9.65774879e-02
  1.40687943e-01 -7.26265684e-02 -1.54087525e-02  2.03343123e-01
  2.62038112e-01 -2.78644562e-02  1.45924479e-01 -2.83320159e-01
  5.64283393e-02  3.91046196e-01  1.35654449e-01 -2.72183776e-01
  8.26655090e-01 -2.95690119e-01  2.57074654e-01  2.30233997e-01
 -2.42470473e-01 -3.54483634e-01  9.28913876e-02  2.62467057e-01
  4.68068123e-01  2.40999654e-01 -1.11212209e-02 -1.73782170e-01
  3.26297134e-01  9.63546149e-03  3.48888338e-02 -2.62331158e-01
  2.42883533e-01 -1.69050440e-01  3.39726150e-01  9.48098004e-02
  3.07155758e-01 -5.66409349e-01  1.01192378e-01  5.61434746e-01
 -1.28958374e-01  7.60574490e-02  1.89140528e-01 -4.02520560e-02
  1.33608714e-01  6.31929412e-02 -6.67212531e-02 -2.18600273e-01
 -2.82091916e-01 -5.69723845e-02  5.61749786e-02  3.54899228e-01
 -3.78321201e-01 -1.52447522e-01  2.15474039e-01  1.05252400e-01
 -3.53225172e-01 -2.14933068e-01  2.60376483e-01  4.98762690e-02
  1.99161977e-01 -8.13560747e-03  2.14293227e-01  1.24234952e-01
 -3.77021372e-01 -1.28142357e-01  4.63599674e-02  3.70731950e-01
 -3.56313020e-01  1.75287068e-01  1.22490376e-01 -3.18117082e-01
  2.88010895e-01  1.63843855e-01 -1.38427019e-01 -7.85029083e-02
 -1.19136050e-01 -2.62503922e-02 -1.63968965e-01  2.30285712e-02
  6.03398383e-01  1.99788183e-01 -2.10185170e-01  1.76160425e-01
 -3.23538780e-01 -2.30618313e-01 -2.89758205e-01 -2.26003140e-01
 -5.82876429e-02  2.86350846e-01 -1.28216684e-01 -4.03533548e-01
  9.42310616e-02  1.56595305e-01 -1.38344958e-01  4.21159863e-01
  3.75700966e-02  2.45827407e-01 -2.17747748e-01  2.53175408e-01
 -2.11629122e-01  4.56553519e-01 -2.68278457e-03  5.46571575e-02
  3.81641328e-01 -2.32371092e-01 -5.14953211e-03 -4.41435397e-01
  3.70115697e-01  3.50492060e-01 -1.45845667e-01  2.96371374e-02
 -1.23949513e-01  2.82543778e-01 -2.05234632e-01 -1.41138524e-01
  3.97560775e-01  5.60181797e-01 -2.53640801e-01 -1.29063636e-01
  3.52700830e-01 -3.90157439e-02 -9.53902677e-02 -3.82797241e-01
  2.16431543e-02 -3.48061949e-01 -1.17046326e-01  4.58758593e-01
 -2.55575418e-01  8.87189507e-02  3.22516590e-01 -1.69552475e-01
  2.76360333e-01  2.29246199e-01  1.26377970e-01  1.59200341e-01
 -3.38929862e-01 -1.26257241e-01 -4.12964106e-01 -2.00287938e-01
 -1.30628854e-01 -4.85109180e-01 -2.04972118e-01  4.11417544e-01
  8.95242244e-02  2.21823990e-01 -1.66459799e-01  2.04197526e-01
 -4.34695333e-01 -2.94493616e-01  2.88179237e-02  3.12693894e-01
 -1.73776984e-01 -2.07517356e-01 -2.57108361e-01 -1.41035169e-01
 -4.14294004e-01  4.11192961e-02 -3.66771594e-02 -3.29435647e-01
  6.06486499e-02  1.61913186e-01 -3.11972976e-01  4.17428434e-01
  2.20559150e-01 -2.71700881e-02 -1.85937285e-01  4.18169558e-01
  5.28523028e-01 -2.33908534e-01  1.96991548e-01 -1.94644868e-01
  7.09738433e-02  1.21751338e-01  6.66602887e-03 -5.37089631e-02
 -2.02864125e-01 -2.64615566e-02  7.39390701e-02  6.06474988e-02
  1.79177999e-01  1.81175955e-03  5.47919810e-01  2.51206160e-01
 -5.66220954e-02 -1.71124727e-01 -3.55746299e-02  2.11850077e-01
 -4.46006328e-01  2.38928013e-04 -1.43585177e-02  1.72311470e-01
  3.84529889e-01  6.14145577e-01 -1.02358028e-01 -1.16606422e-01
 -6.22703195e-01 -1.38962984e-01  2.22770169e-01 -1.15200669e-01
  2.11288244e-01 -3.54426727e-02  5.37089884e-01  4.01112214e-02
  1.24179095e-01  3.32872152e-01  1.63887590e-02  1.48529604e-01
  4.50987756e-01  2.05625176e-01 -4.25647438e-01  2.34024838e-01
  2.66821563e-01  5.27552187e-01 -4.17842120e-01  6.53318524e-01
 -8.39318484e-02  7.81145319e-03  1.93193614e-01 -2.50194579e-01
  4.92724180e-01 -4.09745216e-01  1.12425104e-01  2.67169885e-02
  5.11985064e-01  7.83748627e-02  6.64377362e-02  9.47141647e-02
  2.83684760e-01 -3.06582800e-03 -1.65841281e-01 -8.41706917e-02
  2.70567656e-01 -3.32595468e-01 -3.70453745e-01 -6.72810674e-01
 -3.07590991e-01 -4.79112752e-03 -3.41262341e-01  3.57791722e-01
  1.97188199e-01 -9.11250263e-02 -9.14620310e-02 -1.67946279e-01
  8.31272453e-03  1.48793131e-01  7.98755437e-02  3.80697325e-02
 -6.53185129e-01 -1.64380699e-01  2.38008991e-01 -6.54290915e-01
 -3.81615549e-01 -3.17870021e-01  1.42871052e-01  2.41124645e-01
  4.65952665e-01 -2.64117181e-01 -8.92749727e-02 -3.49842399e-01
  6.50600344e-02  2.26095259e-01 -2.39555001e-01  9.83353257e-02
 -3.41002285e-01  4.63835776e-01  3.36448252e-01  1.03004314e-01
  1.20659500e-01 -2.55311608e-01 -4.95992422e-01  1.34293348e-01
  2.53395736e-01 -3.49473357e-01 -2.47506559e-01 -1.23261459e-01
  9.51834545e-02  4.11187351e-01 -1.56394795e-01  2.09060870e-02
  1.04858600e-01 -4.78051826e-02 -2.51256526e-01  4.72538352e-01
 -1.44887656e-01 -6.60045892e-02  2.80482233e-01 -3.64289939e-01
 -5.61625898e-01 -1.24921054e-02 -6.66691065e-02 -3.86035085e-01
 -1.47662237e-01 -8.13335367e-03  6.22754693e-01  5.80755353e-01
 -7.34170601e-02  1.73545182e-01 -2.46394854e-02  6.96467310e-02
 -4.31746125e-01 -1.28889829e-01  4.85919490e-02  4.94720519e-01
  3.76961112e-01 -1.87803656e-01  7.83438504e-01 -4.90242764e-02
  1.10436514e-01  2.40544379e-02 -1.34262264e-01 -3.29593793e-02
 -9.05631483e-02 -2.53135234e-01 -2.72828072e-01 -6.74980581e-02
  1.38088346e-01  3.09920102e-01 -3.35988551e-01  1.07915655e-01
 -2.41370171e-01  3.48250449e-01  6.03063762e-01 -4.09711242e-01
 -4.34584141e-01 -1.12875916e-01  2.45751560e-01 -8.16225260e-02
  1.78315967e-01 -1.75197065e-01  3.34539831e-01 -9.34607908e-02]"
Code to make TFQ Demos run in Colab stat:awaiting response type:build/install type:support stale type:others TF 2.7,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tensorflow==2.7.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9, Also tried current version in Colab

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Does not correctly install tensorflow or tensorflow_quantum with current version of Python in Colab or Python 3.9 for multiple demos. 

### Standalone code to reproduce the issue

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow==2.7.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0)
ERROR: No matching distribution found for tensorflow==2.7.0

ERROR: Could not find a version that satisfies the requirement tensorflow-quantum==0.7.2 (from versions: none)
ERROR: No matching distribution found for tensorflow-quantum==0.7.2

ModuleNotFoundError: No module named 'tensorflow_quantum'
```


### Relevant log output

_No response_",False,"[-5.79863429e-01 -3.29833567e-01 -3.41122150e-01 -1.18778385e-02
  3.13305408e-01 -2.88425267e-01 -5.77391163e-02  5.09404726e-02
 -4.63382959e-01 -2.04851657e-01 -1.13378063e-01 -1.15056224e-01
 -2.89730638e-01  3.27055633e-01 -1.17604420e-01  3.42105389e-01
 -2.08322763e-01 -1.95020542e-01  3.79256576e-01  1.51463538e-01
 -1.27089843e-01 -1.78582743e-01 -3.05035591e-01  3.43842626e-01
 -3.18006128e-02  1.34529933e-01 -2.24337623e-01 -4.03275974e-02
  1.22091390e-01  2.04345033e-01  1.97357863e-01  2.12291092e-01
  1.12007812e-01  1.82139352e-02  1.53951392e-01  1.33520812e-01
 -2.79184520e-01 -1.12921469e-01 -4.26569790e-01 -2.31257558e-01
  2.08023768e-02 -7.43293613e-02 -7.50403702e-02 -1.10431500e-01
  1.45272491e-02 -1.12492546e-01  2.32512444e-01  9.37700719e-02
  3.12816761e-02 -2.25709677e-01 -3.68803367e-02 -9.31627154e-02
 -3.51217389e-01 -5.35452068e-01  8.74457136e-03 -2.50842154e-01
  1.65242493e-01  9.38767418e-02  1.08785823e-01  1.99566901e-01
  8.64504129e-02  2.23230377e-01 -4.53948863e-02 -1.35250628e-01
  1.93309709e-01  3.25369895e-01  1.51267797e-01  2.74524894e-02
  4.07349825e-01 -1.87212110e-01  1.33795410e-01 -1.22827798e-01
 -3.55171353e-01 -7.43646845e-02 -2.73155980e-02  1.66813489e-02
  1.68052644e-01  1.56234866e-02  8.14788938e-02  8.94796997e-02
 -2.38002002e-01 -3.54033560e-01 -2.15500712e-01 -2.36021325e-01
 -7.87439793e-02 -2.09044546e-01  2.72702307e-01  1.38165355e-01
  5.44041812e-01 -4.06523973e-01  4.50283617e-01  5.02472878e-01
 -9.51436013e-02  1.15476400e-02  3.72072875e-01  8.37452486e-02
  9.64524299e-02  2.81511962e-01  4.01246101e-02 -4.68312651e-02
 -3.03354710e-02 -3.61787498e-01 -1.10595450e-01  4.30494100e-02
 -1.90126710e-02 -2.79479057e-01  1.82145268e-01 -1.45363035e-02
  9.63106528e-02 -3.29646878e-02  9.79717225e-02  2.25325435e-01
  3.56953263e-01 -1.55328512e-01  4.35203388e-02  4.96352538e-02
 -4.05675262e-01  6.28937781e-03  1.50546312e-01  7.68816769e-01
  2.23200709e-01 -2.47270595e-02  2.32057557e-01  5.14911264e-02
  5.79095602e-01  1.24450065e-01 -2.54442066e-01  5.60762323e-02
  1.80843905e-01  1.45188674e-01  2.34555662e-01  4.13156867e-01
  8.97182301e-02  1.88608155e-01 -2.32259706e-01  7.99314007e-02
 -2.65931368e-01  5.13233989e-03 -7.89547563e-02 -2.96105862e-01
 -2.60874897e-01  3.28598499e-01  3.50330248e-02 -5.17552018e-01
 -1.23269014e-01 -1.10568337e-01 -1.81465805e-01  5.17377079e-01
 -2.40887403e-01  4.04869840e-02  1.18551821e-01  1.50138855e-01
  9.51315761e-02  4.15058434e-01  1.85941160e-01  1.06090046e-01
  3.80587965e-01 -6.52066320e-02 -5.22641949e-02 -5.12564659e-01
 -1.08840421e-01  3.47412646e-01 -2.24743694e-01 -1.08798593e-01
 -1.22204907e-01  2.65265018e-01 -5.16940832e-01 -2.46697739e-01
  3.75593722e-01  5.78374505e-01  1.20043922e-02 -1.77427560e-01
  5.57493903e-02 -2.49411035e-02  1.76652879e-01 -1.90139234e-01
  4.28837687e-01 -5.69288135e-01 -8.18830580e-02  2.04013914e-01
  2.82305032e-02  5.83439432e-02  1.35859430e-01  5.04252501e-02
  9.60876048e-02 -1.03918202e-02  1.22963302e-01  1.31709445e-02
 -1.00129293e-02 -7.35328868e-02 -4.49756563e-01 -1.54646277e-01
  3.15520227e-01  6.85326383e-02 -3.75403523e-01  1.88837312e-02
  2.53084719e-01 -3.66804332e-01  5.16253263e-02  1.55233055e-01
 -6.38373867e-02 -1.76344648e-01 -1.52448088e-01  1.03239313e-01
  2.66037941e-01 -1.69059604e-01 -1.11995660e-01 -4.72292483e-01
 -3.24558258e-01 -1.71754062e-01  9.87035409e-02 -3.02658170e-01
  1.16445310e-01 -2.62026966e-01 -3.18841815e-01  7.99124017e-02
 -4.04937379e-02 -1.13807127e-01 -2.80955404e-01  2.77319014e-01
  1.07460380e-01 -3.35486293e-01  1.37128055e-01 -4.54528272e-01
 -2.09164008e-01  5.49714267e-02 -1.94797933e-01  2.52075195e-02
  2.37984844e-02  5.52362874e-02  7.06894994e-02  1.44884884e-02
  4.00948316e-01  3.48477513e-02  3.93264264e-01 -1.89386964e-01
 -1.75867349e-01 -1.36587203e-01 -2.53806412e-01  3.08940187e-02
 -3.91058147e-01 -7.30206817e-02  9.48612299e-03 -6.21708594e-02
  1.70750678e-01  4.32873905e-01 -1.22001156e-01 -1.31470561e-01
 -2.92330444e-01  1.57973185e-01 -3.36912066e-01  2.09855497e-01
  5.19797087e-01  1.53485596e-01  4.23978478e-01  1.55498356e-01
  1.76913366e-01  3.17405462e-01  3.12348366e-01 -1.50935531e-01
  5.08109331e-01  1.94857851e-01  2.85033822e-01  3.07963133e-01
  3.25163007e-01  4.18309838e-01 -4.47314441e-01  3.18473518e-01
 -7.10626468e-02  6.50561973e-02 -7.80743584e-02 -4.52480197e-01
  4.94140238e-01 -4.01287794e-01 -7.39037991e-02 -1.72044083e-01
  3.75203460e-01  5.42088896e-02 -2.89551187e-02  7.04712942e-02
  2.88979888e-01  4.16675925e-01 -1.87998325e-01  1.89752758e-01
 -9.82114822e-02 -1.48586184e-01 -1.25376493e-01 -7.28620946e-01
 -2.08992824e-01  1.48798466e-01 -3.53302419e-01  3.70840788e-01
  1.42457299e-02 -1.55537650e-01 -2.16802210e-01  8.31582919e-02
 -2.81789377e-02 -1.75184026e-01 -1.03362858e-01  3.60776246e-01
 -4.17351454e-01  1.08727805e-01  4.53028858e-01 -2.70898700e-01
 -1.96348339e-01 -5.21375574e-02  2.85530150e-01  3.39187741e-01
  5.23585558e-01 -2.73864925e-01  2.81125486e-01 -1.00313053e-01
 -2.20305659e-02  4.75542128e-01  1.03226282e-01  5.97212799e-02
 -3.08253109e-01  7.40904272e-01  1.96248934e-01 -7.28175715e-02
  2.29391694e-01 -3.20627511e-01 -4.50347155e-01  7.93488882e-03
  2.41715178e-01  3.91190797e-02 -9.48429108e-03 -4.88701463e-01
  1.53791100e-01  1.44709095e-01 -2.26414144e-01 -4.37924042e-02
 -1.79098681e-01  1.73991099e-01 -2.94447452e-01 -1.53136030e-01
 -4.13948834e-01  1.18164532e-01 -7.24918693e-02 -3.09322029e-01
 -2.99935371e-01 -1.66977450e-01 -1.99792117e-01 -2.60102034e-01
 -6.68881088e-03 -4.24800336e-01  2.87003040e-01  4.43022937e-01
 -1.15628392e-01  1.32033050e-01 -6.46613613e-02  1.63510263e-01
 -4.36722577e-01 -7.75223821e-02  5.10709845e-02  1.73800871e-01
  6.33557290e-02 -9.17768925e-02  3.46356213e-01  2.29241714e-01
 -1.17591873e-01 -6.39740527e-02 -3.18849653e-01 -8.19323286e-02
  2.64458358e-01 -2.25292265e-01 -4.00667071e-01 -2.06503287e-01
  1.39577478e-01  3.39500785e-01  2.59097703e-02  4.83854353e-01
 -1.80910856e-01  2.96518922e-01  6.30334020e-01 -3.07408750e-01
 -2.17520848e-01  2.94131994e-01  4.40485597e-01 -2.06283495e-01
 -3.72270122e-04 -8.81093293e-02  4.57550526e-01 -9.45713967e-02]"
Question about ptxas warning output of tensorflow-gpu 2.11 stat:awaiting response type:others comp:ops TF 2.11,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.11

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The running environment is cuda 11.7 cuDNN 8.5.0 python 3.10 tensorflow-gpu 2.11
When I run the code, it show the warning message which seems weird. Since my cuda version is 11.7, not older than 11.1 and the log says Loaded cuDNN version 8904 while my cuDNN version is 8500. Althought the code can run successfully, i have doubts about this log, and I hope I can figure out why. Thanks in advance.

The warning log is as follows
 I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8904
2023-10-07 11:43:16.647615: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.
2023-10-07 11:43:16.651012: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6
2023-10-07 11:43:16.651030: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:237] Used ptxas at ptxas
2023-10-07 11:43:16.651094: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.

### Standalone code to reproduce the issue

```shell
any code that use tensorflow-gpu
concretely, the code i run is https://github.com/acctouhou/Prediction_of_battery/blob/main/1_Predicting/predict.py
```


### Relevant log output

_No response_",False,"[-4.37755287e-01 -3.61868799e-01 -8.75678509e-02  1.38297021e-01
  2.79425859e-01 -6.04209065e-01 -1.71464793e-02  1.74448743e-01
 -2.95859396e-01 -3.83531779e-01 -5.77148721e-02 -3.50367762e-02
 -1.70376763e-01  3.73421237e-02 -2.14420840e-01  2.72160411e-01
 -1.84404373e-01 -7.44377524e-02  2.03035086e-01  1.06475875e-01
 -3.62114236e-03 -1.63985759e-01 -3.50505084e-01  3.52358580e-01
  2.50074089e-01  3.99427861e-01 -1.36898160e-01  8.15561637e-02
 -1.87534302e-01  4.20386404e-01  3.31450939e-01  5.13921864e-02
 -1.59794539e-01 -3.39615084e-02 -1.24039881e-01  2.84453064e-01
 -2.37949267e-01 -2.49089122e-01 -3.74945343e-01  1.38085596e-02
  6.59242794e-02 -1.91766359e-02  1.65658742e-02 -4.53208946e-03
 -5.13242185e-03 -2.87593305e-01  6.62283897e-02 -1.46454096e-01
 -7.72433579e-02 -1.28740937e-01 -1.06422059e-01  3.12234722e-02
 -4.68461484e-01 -2.98091054e-01  3.47283594e-02 -2.53680199e-01
 -1.08564571e-02 -2.15344671e-02  1.17895991e-01  1.69072106e-01
  1.05052933e-01 -6.97021782e-02  1.37467742e-01 -5.94512820e-02
  2.05660298e-01  1.26664668e-01  4.52003539e-01 -6.78782463e-02
  3.90008837e-01 -3.55884522e-01  1.97020054e-01 -1.18966535e-01
 -1.70509771e-01 -4.30142879e-02  8.68951976e-02  1.23185538e-01
 -4.89951596e-02  1.40423149e-01  1.79457039e-01 -3.84110957e-01
  1.53119028e-01 -1.62565142e-01  8.63158554e-02 -2.80403197e-01
  1.98133871e-01  6.37260824e-02  4.12234128e-01  1.73944011e-01
  4.80760068e-01 -3.03322941e-01  3.47304463e-01  2.82748103e-01
 -3.40557322e-02  5.58488145e-02  5.34508407e-01  6.23494834e-02
  1.26267835e-01  1.32207036e-01 -1.13637492e-01 -1.27546743e-01
  1.73749357e-01 -2.17671052e-01 -1.88976079e-01  4.11022604e-02
 -1.68044269e-01 -6.54174984e-02  1.45199478e-01  1.16923064e-01
 -1.73561350e-01 -2.10493039e-02  1.29838496e-01  1.20462507e-01
  9.92233157e-02 -8.11382756e-02 -2.74535269e-03 -6.03886042e-03
  2.12243125e-02  2.73544062e-03  1.06548920e-01  6.97750568e-01
  6.32994100e-02 -2.43358221e-03 -1.43700480e-01  1.22321039e-01
  4.47549760e-01  2.83011235e-04 -8.51372816e-03  1.47327065e-01
  2.91237682e-01  1.64093345e-01  1.55112684e-01  4.28313762e-03
 -2.61282027e-01  3.42992723e-01 -1.42509803e-01  1.65136918e-01
 -6.85904026e-02 -3.03666800e-01 -2.74764001e-01 -1.90950245e-01
 -1.52418315e-01  6.17149509e-02 -2.73317486e-01 -6.97438002e-01
  2.47645169e-01  1.52919605e-01 -2.29738832e-01  3.56622994e-01
 -2.16754645e-01  1.50397956e-01 -2.86501199e-01  2.02631220e-01
 -2.40171343e-01  6.93219841e-01 -4.12108973e-02  9.02415067e-02
  2.31333911e-01 -5.54676205e-02  2.27101892e-03 -4.63348776e-01
  1.26837254e-01  4.03294802e-01 -7.61450082e-02 -8.43407810e-02
  6.15317784e-02  1.55685753e-01 -4.72205818e-01 -3.89579296e-01
 -6.25938550e-02  3.38473350e-01 -1.58979103e-01 -1.06276795e-01
 -9.74307954e-02  1.21330924e-01 -2.01177783e-03 -2.13526368e-01
  2.60305256e-01 -4.01412010e-01  1.65666286e-02  2.79726386e-01
 -8.30149204e-02  6.96923211e-03 -7.73196220e-02  1.00619763e-01
 -1.18758589e-01 -4.73002233e-02 -1.19056925e-01  5.61349839e-02
 -2.42450386e-01  2.14495286e-02 -5.12956381e-01 -2.36681029e-01
  3.50324810e-01 -9.00882632e-02 -1.46311015e-01  1.45374820e-01
  3.12067568e-01 -8.09632242e-04 -1.26260314e-02  2.74593741e-01
 -9.52823237e-02 -9.55354422e-03 -3.69347259e-03  1.00815669e-01
  5.35001121e-02 -2.64408976e-01 -1.30559012e-01 -4.57030028e-01
 -4.36492831e-01 -4.49724570e-02  1.80680543e-01 -5.33788681e-01
  1.82661161e-01 -1.12230748e-01 -3.12356949e-01  2.70545542e-01
 -1.62521392e-01  8.06129649e-02  3.86811793e-04  2.08191350e-01
  1.69354528e-01 -1.88992679e-01 -1.52500257e-01 -2.62765408e-01
 -2.08637297e-01  2.38456234e-01 -4.06777024e-01  4.05921303e-02
 -2.40955412e-01  2.65779734e-01  6.81831688e-02  2.40109667e-01
  4.43984509e-01  2.40316391e-01  4.49432909e-01 -4.05780971e-04
 -2.05950290e-01 -1.95848763e-01 -3.17952454e-01  7.95448795e-02
 -4.29141998e-01 -2.54141033e-01  8.62273946e-03 -2.97891553e-02
  3.91152322e-01  4.54649597e-01 -5.86708896e-02 -1.93205476e-01
 -3.82836342e-01  2.74040699e-01  1.69392359e-02 -5.35875112e-02
  2.43902266e-01  1.87632054e-01  6.15509391e-01  2.71928728e-01
  1.35875627e-01  1.96427941e-01  2.74780929e-01 -3.43187869e-01
  2.98292279e-01  8.42625797e-02  5.84258437e-02  4.38401878e-01
  1.56396106e-01  2.76779324e-01 -2.23187894e-01  5.26246905e-01
 -3.93761508e-02 -7.55397379e-02  2.53744692e-01 -2.64127761e-01
  6.55673027e-01 -3.32764924e-01  1.35037333e-01 -2.34317660e-01
  3.92832994e-01 -2.53699012e-02 -7.07667470e-02  8.81706774e-02
  1.54825956e-01  3.28504741e-01 -2.79709399e-01  1.62557274e-01
  6.20852076e-02 -1.26949519e-01 -1.28225327e-01 -6.82639182e-01
 -1.76610261e-01  7.80870989e-02 -2.07142800e-01 -6.35216460e-02
  1.89786494e-01  6.27585277e-02  6.38080910e-02  1.84439063e-01
 -4.91674989e-04 -2.34500900e-01  8.84963423e-02  2.08322197e-01
 -1.74961209e-01 -4.82487492e-02  1.29518539e-01 -3.34701777e-01
 -2.05922708e-01 -2.87164226e-02  3.38189781e-01  1.96436584e-01
  3.21043611e-01 -4.53826308e-01  1.54253304e-01 -1.38948262e-01
 -1.50781363e-01  2.41094813e-01 -4.55726609e-02  1.55023634e-01
 -3.20748627e-01  6.65960550e-01  1.77974790e-01  1.48399100e-02
  2.76256263e-01 -1.90256625e-01 -4.62193966e-01  6.38591051e-02
  9.41731930e-02 -1.25747547e-01 -1.24122426e-01 -5.36430717e-01
  1.56676590e-01  1.33045524e-01 -5.10179512e-02 -3.79729494e-02
 -1.74193099e-01 -9.54504535e-02 -7.97944739e-02 -1.43621877e-01
 -2.95209706e-01  1.85944214e-02 -2.89531052e-02 -3.58227521e-01
 -3.11641693e-01 -1.95161462e-01  1.89288147e-02 -1.44900799e-01
 -5.32720610e-02 -2.95345515e-01  4.26024944e-01  5.35206556e-01
  7.76960514e-03  1.24534048e-01  5.54784574e-02  1.84658855e-01
 -4.05269444e-01 -1.37150772e-02 -1.21259063e-01  4.58060682e-01
 -4.69687954e-02  1.74049335e-03  4.14813340e-01  3.21554422e-01
 -2.50495762e-01 -2.14669835e-02 -3.44651490e-01  1.93420038e-01
  2.53607541e-01 -3.00874770e-01 -1.85081631e-01 -2.19889626e-01
  1.00527048e-01  7.12174177e-02 -2.07214486e-02  2.94139743e-01
 -3.16306055e-01  2.96065331e-01  5.98782539e-01 -4.08586800e-01
 -2.00898379e-01  4.03623804e-02  1.15834735e-01 -3.54343504e-01
  1.70875609e-01 -2.15778686e-02 -3.70761901e-02  8.30890238e-03]"
Issue Installing TensorFlow on Windows 11 with Python 3.12.0 ,"Hello,

I am encountering an issue while trying to install TensorFlow on my Windows 11 machine with Python 3.12.0 and pip 23.2.1 (64-bit). Despite several attempts, I keep receiving the following error messages:

```
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
```

I have tried creating virtual environments using both Python 3.7.0 and Python 3.6.4
cleared the pip cache using the command `pip cache purge`
Install TensorFlow with various specific versions
`tensorflow==2.1.0`
`tensorflow==2.5.0`
`tensorflow==2.2.0rc4`

I hoping anyone know what the issue here",False,"[-1.50859356e-01 -6.76925302e-01 -5.82393222e-02  2.15832978e-01
  1.33978963e-01 -3.03735703e-01 -3.86937410e-01 -2.84632683e-01
 -2.58066535e-01 -2.03914955e-01 -8.31745639e-02 -2.30151191e-01
 -4.28777993e-01  1.44836307e-01 -1.25333011e-01  2.38255858e-01
 -7.51500875e-02 -3.31609875e-01  2.48894125e-01  1.12821631e-01
 -1.67210937e-01 -9.36868414e-02 -2.70991206e-01  1.95474979e-02
  4.38493371e-01 -1.58805296e-01 -9.45386961e-02 -2.79609412e-01
  3.79461408e-01  9.29823816e-02  5.22981882e-01 -2.51059741e-01
  2.92872824e-02  3.46227288e-02  1.96966261e-01  1.48756564e-01
 -2.30574369e-01 -1.81806937e-01 -1.13570683e-01 -9.26429182e-02
  1.60244972e-01  1.59971312e-01  9.89825651e-02 -9.69962180e-02
  6.99591115e-02 -1.34499341e-01  2.12817371e-01 -2.15354532e-01
 -1.81047112e-01 -1.26313090e-01 -1.38600662e-01  5.34993634e-02
 -2.38778338e-01 -1.51095152e-01 -2.35417038e-01 -1.53724462e-01
  1.37969553e-01  4.00824845e-01 -2.16712594e-01 -3.75768468e-02
  1.61675550e-03 -3.13323848e-02  1.88309699e-01  5.72360940e-02
 -1.53048813e-01  2.51957774e-01  7.44516402e-02 -4.12285447e-01
  6.32679522e-01 -2.62507707e-01  1.59602523e-01 -5.85860983e-02
 -4.21897113e-01 -6.12587929e-02  4.16257903e-02  2.69903660e-01
  1.34948611e-01  1.83690041e-01  2.33709723e-01 -9.14965644e-02
 -2.01878361e-02  1.23254485e-01  3.69793475e-01  1.15378693e-01
  2.28919774e-01 -5.26775457e-02  4.80300672e-02  1.05981663e-01
  9.67856795e-02 -1.27486721e-01  4.65466261e-01  1.29587412e-01
  4.05078121e-02  2.46260166e-01  3.30458939e-01 -3.70758325e-02
  1.68659277e-02  3.84904891e-01 -1.08685225e-01 -2.63325393e-01
 -5.31480350e-02 -1.76488727e-01 -3.52344096e-01  2.40628272e-01
  1.25352383e-01  2.04774708e-01  3.10191780e-01 -2.84589589e-01
  2.00055212e-01 -1.32867932e-01  2.25330174e-01 -1.28480613e-01
  2.57045507e-01 -5.29951900e-02  9.48062688e-02  2.20168769e-01
 -5.47084808e-01  2.85890341e-01 -1.77349281e-02  9.40232158e-01
 -3.28830540e-01 -2.07201973e-01  3.41755778e-01 -1.73029751e-01
  1.03379555e-01  1.10131726e-01 -1.29767314e-01 -1.87453806e-01
 -1.22684769e-01 -9.28760022e-02  2.83905715e-01  1.18962958e-01
 -4.51905057e-02  9.54789966e-02  4.10550714e-01 -1.08992057e-02
 -2.00964242e-01 -4.72582340e-01 -2.45324448e-01 -4.62347940e-02
 -2.36045718e-02  1.93842292e-01 -2.51530707e-01 -3.05351257e-01
  5.15278131e-02 -9.12918989e-03  3.00837636e-01  2.04382092e-01
 -3.87273058e-02 -2.33067185e-01 -2.03795984e-01  1.82978004e-01
 -3.16101551e-01  1.76459700e-01  2.20595211e-01  3.75544220e-01
  2.46699899e-01 -7.86431357e-02 -3.75147909e-01 -5.54436445e-01
 -6.11123964e-02  2.82275826e-01 -4.53793854e-01  4.64171842e-02
  4.72163036e-03  2.16779679e-01 -3.54590356e-01 -2.59251654e-01
  1.07497796e-01  6.17486387e-02  8.79193656e-04 -8.53505135e-02
  4.91270572e-02 -5.27685434e-02  2.85437107e-01  5.03399931e-02
  3.71920764e-01 -4.22507942e-01 -1.24199297e-02  8.19067508e-02
  3.66618395e-01  8.60911310e-02  1.15946651e-01 -1.36960700e-01
  9.12262872e-02  1.25227749e-01  1.02262147e-01  9.20947492e-02
  1.77255243e-01  2.92893827e-01 -3.43807302e-02  9.11988765e-02
  1.29536748e-01 -2.58620102e-02 -1.03297919e-01  1.13307655e-01
  2.68826127e-01 -1.34530604e-01 -1.13284469e-01  5.97909614e-02
  5.99034801e-02  2.06093162e-01  1.28188923e-01  6.96545318e-02
 -1.93752587e-01 -5.09198666e-01 -3.53856087e-02 -2.71653924e-02
 -4.86979663e-01  2.76333541e-02 -4.40645851e-02 -1.40480980e-01
 -1.86921149e-01 -1.94398910e-01 -2.94954360e-01  1.01149440e-01
  5.24319053e-01 -1.40393138e-01 -6.59702569e-02 -6.69359565e-02
 -1.11663572e-01  4.17963192e-02 -9.19651687e-02 -1.11488856e-01
  5.19738682e-02 -7.82782137e-02 -8.24413449e-02 -2.99090166e-02
 -1.87340558e-01  1.03604071e-01  3.98451649e-02 -1.73240602e-01
  4.04209405e-01 -7.97638297e-02  2.57103652e-01 -5.71050234e-02
  1.88968897e-01  2.38608271e-02 -1.17835701e-01  2.27584511e-01
 -5.22350669e-01 -1.19654179e-01  7.69537408e-03 -7.71747902e-04
 -1.94469437e-01 -7.22498894e-02 -2.21223861e-01 -2.92210162e-01
 -3.37236851e-01  3.54721636e-01  2.70370301e-02 -4.61933732e-01
  3.08503211e-01  2.14397892e-01  3.94578516e-01  2.18526214e-01
 -1.94099411e-01 -3.61955911e-02  1.20300665e-01 -9.61303115e-02
  1.60351560e-01  4.39106189e-02 -2.18904048e-01  1.47192031e-01
  1.12618431e-01  2.20271081e-01 -1.97155997e-01  5.94820797e-01
  2.08763480e-01  8.30715075e-02  2.59927869e-01 -2.16678947e-01
  1.80167705e-01 -2.76601166e-01 -1.55707091e-01  8.50292370e-02
  2.75992095e-01  3.98823678e-01 -8.51451606e-02 -1.29963517e-01
  6.10983968e-02  5.68605483e-01 -4.83741134e-01  7.67018944e-02
 -1.31225497e-01 -6.28895462e-02  2.47346014e-02 -3.61122370e-01
 -1.24654785e-01  2.75330245e-01 -1.77306622e-01 -3.72002944e-02
  1.13876686e-01  1.70110419e-01 -1.50558770e-01 -1.21227711e-01
  2.64334172e-01 -1.15959667e-01  1.77820325e-01  3.60720813e-01
 -2.29826868e-01  1.32538378e-01  6.61684573e-02  1.02754049e-02
  8.78945738e-03 -2.87322462e-01  1.56389266e-01 -1.26314051e-02
  3.29137802e-01 -2.84897625e-01  3.64317626e-01 -1.35204569e-01
 -1.94383860e-01  4.96463358e-01  5.93914241e-02 -6.14381358e-02
 -2.83134192e-01  5.87886512e-01  4.97100890e-01 -1.02433741e-01
  7.83066750e-02 -1.79993853e-01 -4.32680428e-01 -2.86672294e-01
  6.56217933e-02 -1.59474872e-02 -1.26869693e-01 -2.64257461e-01
 -3.02752733e-01  9.33528990e-02 -1.83670800e-02 -1.30047217e-01
 -5.95486276e-02  1.39693022e-01 -1.79095030e-01 -6.21214807e-02
 -3.42625260e-01  4.20312166e-01  1.15746692e-01 -2.54896671e-01
  2.53626227e-01 -7.55775124e-02  1.55480891e-01 -4.05118167e-01
 -1.63460866e-01 -7.64622316e-02  4.73039001e-01  1.42175913e-01
 -3.46730441e-01 -1.77850306e-01  5.70983291e-02  1.06460586e-01
 -4.66385007e-01 -1.24376267e-01 -2.54267305e-02  2.09021479e-01
  1.00009277e-01 -6.17171153e-02  3.05612773e-01  2.54205704e-01
 -2.06219643e-01 -3.48721407e-02 -1.63611144e-01 -4.18940127e-01
  3.36066127e-01 -2.46140566e-02 -5.10793805e-01  7.80044049e-02
  7.60615915e-02  5.01188934e-01 -1.40516251e-01  2.62972176e-01
 -3.01262140e-01  1.48465768e-01  5.94345152e-01 -1.20316893e-01
 -4.94831741e-01  3.37666184e-01 -1.68105394e-01 -7.94340074e-02
  1.11417182e-01  2.82830834e-01  3.73557746e-01 -3.15976888e-01]"
Compatibility with Python 3.12.0 type:build/install,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tensorflow 2.14.0

### Custom code

No

### OS platform and distribution

Mac OS

### Mobile device

_No response_

### Python version

3.12.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Python 3.12.0 is now available. I tried to pip install TensorFlow 2.14.0. The attempt failed as TF is only compatible with Python 3.11 at most at the moment.

When will a version of TensorFlow be available for the latest Python?  

Thanks.

### Standalone code to reproduce the issue

```shell
Terminal output:

(venv) ## my user ## % pip install tensorflow==2.14.0
ERROR: Could not find a version that satisfies the requirement tensorflow==2.14.0 (from versions: none)
```


### Relevant log output

_No response_",False,"[-0.4558642  -0.61255395 -0.18247208  0.15263787  0.2253825  -0.5750714
 -0.40934068 -0.21261021 -0.61722875 -0.26459295  0.12198336  0.00653547
 -0.43319854  0.18524083 -0.01228901  0.25131553 -0.10104181 -0.16728343
  0.17333367  0.157182   -0.1633107  -0.21812792 -0.35511863 -0.02318982
  0.15733539  0.14335465 -0.25610405 -0.10189812  0.06371947 -0.0148443
  0.6013853   0.11646751  0.12702233  0.06902246  0.20680708  0.3617903
  0.03516106 -0.50483906 -0.40452033 -0.11643115 -0.29714525  0.17121464
  0.12426297 -0.11502891  0.14279574 -0.16964501  0.05584166 -0.2485472
  0.00653315 -0.25092757  0.0199391  -0.16081697 -0.35430342 -0.4665051
 -0.19618055  0.00233267  0.06176549  0.1125872   0.01323132  0.04564637
  0.16243742  0.06646723 -0.02030977 -0.08588305  0.1480237   0.22461808
  0.10803384 -0.16470183  0.41463625 -0.0735487   0.13959783  0.00414896
 -0.53514695  0.03120923  0.05800881  0.28296378  0.2892268   0.05363469
  0.21812269 -0.10861704 -0.04658585 -0.3121382  -0.14256069 -0.25721586
  0.11978859 -0.22525905  0.39933848  0.08226116  0.25752476 -0.19343185
  0.5264324   0.3951511  -0.04295056 -0.07958204  0.5078662   0.06256597
  0.14407983  0.19937295 -0.04276446 -0.12739421 -0.03717528 -0.28735194
  0.00152014  0.01699665 -0.02335477  0.00553894  0.23899171 -0.26005253
  0.26044652 -0.15505102  0.19581428 -0.06731097  0.24310859 -0.17802139
  0.08401188  0.04564675 -0.4494659  -0.04977784  0.10142688  1.0161983
 -0.00801228  0.04932021  0.2166815   0.05892738  0.49386942  0.10745594
 -0.2807586  -0.01007221 -0.21360517 -0.09525591  0.07126193  0.12558532
  0.16993259  0.43139318 -0.12867242  0.29060018 -0.14045691 -0.43217874
 -0.12832376 -0.44883972 -0.28685006  0.36207524 -0.08425317 -0.78383803
  0.17693248 -0.0567905  -0.25320363  0.45343745 -0.23348239 -0.23532483
 -0.07313111  0.32426754 -0.10181464  0.31635627  0.39302665  0.23377842
  0.44544828 -0.11051609 -0.11660584 -0.6382203  -0.09433241  0.5071543
 -0.55493546 -0.10486984 -0.11096006  0.1370148  -0.48952365 -0.25431558
 -0.00875686  0.43428794 -0.22884081 -0.09493701  0.37561288  0.19919226
  0.19809192 -0.06664644  0.32692924 -0.85236514 -0.0688566   0.5293251
  0.12248662  0.13567661  0.08632109  0.19290666  0.07082212  0.06626377
  0.0406884   0.03149685  0.06830885  0.14154226 -0.37812552 -0.18519877
  0.3578108  -0.14073291  0.01883407  0.20063666  0.1504336  -0.06867214
 -0.2511834  -0.10001577 -0.2874237  -0.14991304 -0.35951322  0.18059142
 -0.00965112 -0.35867387  0.01211178 -0.11492027 -0.49506092  0.05361551
  0.08659679 -0.6345494   0.15139425 -0.13116142 -0.3710838   0.12047847
  0.28971058  0.18209487 -0.09178914  0.17693767  0.01481242 -0.16519983
  0.1075922  -0.4041545  -0.06543393  0.10141672 -0.3686993   0.03149656
 -0.22217712  0.31035107  0.0487402  -0.03657578  0.5364701   0.17308772
  0.51944196 -0.10287866 -0.11180993 -0.19681686 -0.09025449  0.15386921
 -0.5221442   0.0882154   0.05109276 -0.03373042  0.31367022  0.4682313
 -0.21210892 -0.01119711 -0.47049534  0.18247704 -0.3207618   0.00674197
  0.4811499   0.13822897  0.61075366  0.3466962   0.1396246   0.3722773
  0.27279568 -0.00384369  0.4123829   0.09309746  0.00152587  0.3595131
  0.37952483  0.47274852 -0.45739084  0.594563    0.2416856  -0.1461806
  0.16603786 -0.35214752  0.719267   -0.45629597 -0.02999174  0.11417217
  0.4949035  -0.0416055   0.15710154 -0.05969836  0.08805203  0.0940523
 -0.55478895  0.12132595 -0.11484724 -0.19063     0.22850817 -0.6597473
 -0.25768685  0.23329654 -0.3073564   0.360709    0.2868155   0.07259677
 -0.23169532  0.05544753  0.14284249 -0.12766552  0.13687584  0.39607748
 -0.24103397  0.19427665  0.17743966 -0.51162946 -0.0636646  -0.09747729
  0.3151674   0.42866504  0.4090805  -0.533059    0.24909714 -0.25186023
 -0.20063786  0.53534174  0.02567392  0.11804093 -0.56136274  0.7448996
  0.29706058 -0.17345193  0.13426648 -0.11734854 -0.38061073 -0.18731722
  0.24447472 -0.1221406  -0.12099689 -0.591246   -0.1061651   0.20203933
 -0.20682698  0.06113704 -0.14181572  0.282781   -0.27539116  0.16825879
 -0.5987954   0.20743254 -0.02626317 -0.25355124 -0.12427711 -0.02339342
 -0.00997832 -0.03628025  0.08586304 -0.30092564  0.65219486  0.54456425
 -0.4152858  -0.01721842 -0.02332854  0.24074835 -0.8348018  -0.06644031
  0.17932016  0.09000273  0.07841662 -0.21771738  0.44596982  0.25060338
 -0.2664666   0.24281769 -0.4216993   0.06306797  0.23433349 -0.19825885
 -0.18628135 -0.20888504 -0.07602677  0.43370742 -0.15283513  0.35786435
 -0.28439695  0.60183275  0.7160182  -0.46640173 -0.5770879   0.3734895
  0.26647717 -0.13663822  0.18452258 -0.09388223  0.44009367 -0.07942703]"
Building TF 2.14 using bazel with later NDK versions failing stat:awaiting response type:build/install stale subtype: ubuntu/linux TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Building without CUDA support

### GPU model and memory

_No response_

### Current behavior?

In order to build Tensorflow Delegate Performance Benchmark tool, I am building latest TF (2.14) from source (using ./configure) using bazel 6.1.0 and Android NDK version 25.0.8775105. But this fails

WARNING: The NDK version in /home/<username>/AndroidSDK/ndk/25.0.8775105 is 25, which is not supported by Bazel (officially supported versions: [19, 20, 21]). Please use another version. Compiling Android targets may result in confusing errors.

Traceback (most recent call last):
  File ""./configure.py"", line 1466, in <module>
    main()
  File ""./configure.py"", line 1439, in main
    create_android_ndk_rule(environ_cp)
  File ""./configure.py"", line 658, in create_android_ndk_rule
    get_ndk_api_level(environ_cp, android_ndk_home_path))
  File ""./configure.py"", line 752, in get_ndk_api_level
    api_levels = sorted(os.listdir(platforms))
FileNotFoundError: [Errno 2] No such file or directory: '/home/<username>/AndroidSDK/ndk/25.0.8775105/platforms'

As far as I understand later NDK versions don't have platforms/

As a user I should be able to build tensorflow using later NDK versions. Could someone have a look into this please.
 

### Standalone code to reproduce the issue

```shell
Steps followed:
1. Clone TF : git clone --recurse-submodules https://github.com/tensorflow/tensorflow.git
2. Clone bazel: https://bazel.build/install/ubuntu
3. Install AndroidSDK: sudo snap install androidsdk
4. Install Android NDK : androidsdk --install ""ndk;25.0.8775105""
5. Install android sources: androidsdk --install ""sources;android-30""
6. Install platforms: androidsdk --install ""platforms;android-30""
7. Install platform-tools: androidsdk --install ""platform-tools""
8. Run ./configure
These are the options we provided:

Please specify the location of python. [Default is /usr/bin/python3]:
 
 Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.8/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]
 
Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.
 
Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.
 
Do you wish to download a fresh release of clang? (Experimental) [y/N]:
Clang will not be downloaded.
 
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: --config=opt
 
 
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: y
Searching for NDK and SDK installations.
 
Please specify the home path of the Android NDK to use. [Default is /home/<username>/Android/Sdk/ndk-bundle]: <provide path here>
```


### Relevant log output

_No response_",False,"[-7.04591751e-01 -1.57587677e-01 -1.03989288e-01  8.91164094e-02
  1.99175119e-01 -4.38560873e-01 -1.38017267e-01  2.91864544e-01
 -2.79245734e-01 -3.95963132e-01 -7.85008352e-03 -7.90248960e-02
 -1.40383005e-01 -7.33272955e-02 -2.19345748e-01  3.58731389e-01
 -2.63358861e-01 -1.15966298e-01  3.03916395e-01  7.79053718e-02
 -4.12897646e-01 -1.22549057e-01 -1.96493298e-01  3.71705204e-01
  2.24725604e-01  3.97445783e-02 -2.12859675e-01  2.53119141e-01
  2.33981088e-02  1.96535051e-01  9.55197960e-02  3.08047295e-01
 -1.78435192e-01  9.93150622e-02  1.40158623e-01  1.56022534e-01
 -2.38022685e-01 -3.50147635e-02 -4.95200217e-01 -1.19639829e-01
 -6.91651553e-02 -9.92040038e-02  2.26207599e-01 -2.76953399e-01
  1.75599813e-01 -1.03346676e-01 -5.30126169e-02  4.54723798e-02
 -1.68571740e-01 -1.94217235e-01  4.17133346e-02  5.26267588e-02
 -3.15336287e-01 -2.75880665e-01 -5.83305508e-02  1.23964604e-02
  6.06692135e-02  4.83580455e-02 -3.64616886e-02  3.46459091e-01
  1.59199446e-01  6.70733005e-02 -4.64513637e-02 -6.73215315e-02
  2.19615638e-01  2.83566356e-01  2.04272330e-01 -1.08040713e-01
  3.29432666e-01 -1.88270360e-01  2.29266390e-01 -2.30922058e-01
 -1.86751187e-01  7.59776756e-02 -5.91527671e-02  1.01341337e-01
  1.12198606e-01  2.34461665e-01  2.61737257e-01 -2.43212849e-01
 -1.16144225e-01 -3.73509347e-01 -2.14035064e-01 -2.20658675e-01
  1.38809919e-01 -1.19457230e-01  3.42279464e-01 -4.52659354e-02
  4.64850187e-01 -1.27555043e-01  5.02712846e-01  6.34407461e-01
 -6.78547798e-03  1.75390199e-01  2.55338222e-01  2.72345096e-01
  7.82314092e-02  2.85023957e-01 -4.89376485e-04  8.37155432e-02
 -1.28713787e-01 -2.86759675e-01  3.22613679e-02  3.47124413e-02
 -8.27906728e-02 -3.08991075e-01  2.29967147e-01 -1.12705410e-01
 -3.13939974e-02 -8.20328817e-02  2.12422132e-01  3.26053575e-02
  1.20284535e-01  6.84161410e-02 -1.02930013e-02 -5.69347814e-02
  1.06899794e-02  9.34021920e-02  6.14216253e-02  5.95064878e-01
  1.11468680e-01 -1.51226014e-01 -1.72551885e-01 -2.48850100e-02
  5.10476112e-01  1.00210890e-01 -2.19930053e-01 -1.85923055e-02
  3.18908989e-01  1.64480776e-01  1.05300494e-01  2.11032927e-01
 -2.67717093e-01  2.11847708e-01 -1.81852072e-01  4.49052043e-02
 -1.93701535e-01  7.94988684e-03 -2.55182028e-01 -2.88072705e-01
 -2.41028517e-01  5.09916283e-02  2.62547135e-01 -5.46154201e-01
  5.75791933e-02 -7.19743036e-03 -2.11183935e-01  2.30854958e-01
 -3.38333935e-01  2.89041668e-01 -6.87279105e-02 -1.85494989e-01
 -9.33413357e-02  4.83155131e-01  1.49251193e-01 -2.02999473e-01
  1.85356945e-01  5.23046330e-02  8.80059451e-02 -3.95590276e-01
  5.78675196e-02  4.17000473e-01 -2.54771188e-02 -4.16184038e-01
 -1.03590891e-01  3.00915062e-01 -4.65616405e-01 -2.40055263e-01
  3.48962307e-01  4.42036808e-01 -1.36830866e-01 -1.64904624e-01
  2.20668375e-01  1.06613576e-01  2.12854028e-01 -2.87004352e-01
  3.33125889e-01 -3.38867933e-01 -7.83247873e-02  3.20081353e-01
  1.42463595e-02  9.81471837e-02  3.64823602e-02 -6.63025491e-03
  1.31994367e-01 -1.01966858e-01  8.64364654e-02  1.52889788e-01
 -1.60602480e-01 -4.88042831e-04 -4.52686220e-01 -1.71413064e-01
  4.42557156e-01 -1.04732946e-01 -2.01383770e-01 -7.08375452e-03
  1.27637208e-01 -2.04599246e-01  2.47475356e-01 -8.26020688e-02
 -7.07698017e-02  1.15819648e-01  1.26215547e-01 -3.37012112e-02
  1.63259417e-01 -1.25561848e-01 -1.76693536e-02 -3.34165990e-01
 -1.38874829e-01  1.03463531e-02 -6.39021620e-02 -3.52234840e-01
  2.60274947e-01 -1.38359010e-01 -2.24358931e-01  2.01439336e-01
  6.96210340e-02 -6.49686754e-02 -4.94237505e-02  3.47380310e-01
  2.12814897e-01 -1.98931158e-01  9.62942988e-02 -4.69093740e-01
 -7.84724653e-02  2.71286249e-01 -1.30315304e-01  6.90931529e-02
  1.55884586e-02 -1.32092714e-01  4.90849726e-02  8.97499025e-02
  3.89567494e-01  4.58222151e-01  3.28574270e-01 -1.73436940e-01
 -9.75856110e-02 -2.36513138e-01 -2.15627149e-01 -6.97546173e-03
 -3.25547367e-01 -3.19065154e-01  2.39029564e-02 -1.64051473e-01
  3.16378117e-01  3.29480529e-01 -1.97562695e-01 -3.00171580e-02
 -3.11124384e-01  4.24896687e-01 -2.90304184e-01  3.01415861e-01
  1.02077454e-01  8.87682587e-02  4.14820373e-01  1.33771509e-01
  2.15337783e-01  2.34583676e-01  3.32691163e-01 -3.24684799e-01
  3.84473383e-01  2.16749087e-01  1.77753657e-01  1.52766034e-01
  6.09296747e-02  2.11103767e-01 -3.19412649e-01  4.08322334e-01
  4.19658795e-02 -1.99243024e-01  2.28163809e-01 -3.28909159e-01
  5.34022450e-01 -3.09363663e-01  1.85835302e-01 -1.44399166e-01
  2.09254488e-01  4.75620702e-02 -8.33086967e-02  1.07816689e-01
  1.03640288e-01  2.78217912e-01 -3.11320871e-01  1.46283358e-01
  1.12691104e-01 -1.01978034e-01  4.56126109e-02 -6.87082648e-01
 -2.64017820e-01  7.05742538e-02 -1.73697487e-01  2.35945344e-01
 -3.52816761e-01 -1.11545473e-01 -3.48046958e-01  1.12505563e-01
  2.96894461e-04 -2.09372371e-01  1.23335803e-02  1.34022683e-01
 -2.43414342e-01  1.42227277e-01  4.81598854e-01 -1.63233802e-01
 -1.88072070e-01 -1.40938014e-01  1.68036833e-01  4.04297918e-01
  4.41248417e-01 -3.56018126e-01  1.22039266e-01 -4.62963618e-02
 -1.04635153e-02  4.17416632e-01  9.15447176e-02  2.35750407e-01
 -4.45842087e-01  7.13362455e-01  1.68681175e-01 -1.37470543e-01
  2.89191753e-01 -2.21223012e-01 -3.65493149e-01  1.63914442e-01
  3.58049452e-01 -6.96676075e-02 -2.25002952e-02 -3.72055352e-01
  2.22018793e-01  1.98937953e-01 -1.88412309e-01  7.12415352e-02
 -1.51836336e-01  8.65468830e-02 -3.09687704e-01 -1.15071803e-01
 -3.29790592e-01 -1.70411840e-02 -1.61271900e-01 -2.73527205e-01
 -1.95323378e-01 -2.97807753e-01 -1.73728943e-01 -2.32338816e-01
 -1.37450784e-01 -2.96794236e-01  1.81861147e-01  3.96632671e-01
 -4.73490208e-02  1.46896765e-01  1.64786428e-01  4.89896946e-02
 -2.90116996e-01  8.20948854e-02 -4.29194830e-02  9.80990157e-02
 -1.04851864e-01  2.61160880e-02  4.36812103e-01  2.16266885e-01
 -9.75946784e-02  7.58848339e-02 -3.46524507e-01  1.46070927e-01
  1.85546219e-01 -4.91509885e-01 -3.74232590e-01 -1.58484012e-01
 -1.19332418e-01  2.20858842e-01 -7.94578530e-03  3.19607407e-01
 -2.70901233e-01  2.25236222e-01  5.30265689e-01 -5.93357861e-01
 -1.14514023e-01  3.05484891e-01 -8.56690854e-03  1.28029473e-03
 -5.94346412e-03 -2.79184073e-01  1.49213001e-01  7.47813797e-03]"
Can TensorFlow be used on other microcontrollers other than listed by TensorFlow?  stat:awaiting response type:support stale comp:lite comp:micro TFLiteConverter,"I have created a TensorFlow model that I wish to run on STM32F407VGT6. 
Is it possible to run TensorFlow model on microcontrollers other than listed ones. 
Also, is there any documentation to create and explore new models other than Hello World, Miro Speech and Person detection to understand TensorFlow Lite Micro more?
Is TensorFlow expandable to other models apart from listed above?",False,"[-1.65577292e-01 -6.35979176e-01 -2.30039418e-01 -8.30329210e-02
  4.11797762e-02 -3.88719440e-02 -3.09313565e-01 -2.49345258e-01
 -1.68797404e-01 -2.96738863e-01 -1.18858159e-01  1.52528048e-01
 -3.27307463e-01  4.14828420e-01  9.64069888e-02  1.52763635e-01
 -6.09414130e-02 -1.73711389e-01 -2.12102383e-02  6.30767345e-02
  9.76812690e-02 -1.13571793e-01 -1.01973690e-01  1.15101248e-01
  7.98018128e-02  1.49888411e-01 -8.69423449e-02 -2.23716319e-01
  1.31950647e-01  2.55588382e-01  3.98524255e-01  2.38380402e-01
  8.83768201e-02  4.53002825e-02 -4.81676817e-01  1.41400293e-01
 -4.74044308e-02 -5.14008775e-02 -2.94766307e-01  1.32333502e-01
  4.28651646e-02  1.71695739e-01  1.05122291e-02  4.99523804e-02
  1.46040708e-01 -5.31338304e-02  2.16391385e-01  3.51864733e-02
 -5.47121130e-02 -1.98420525e-01 -2.36815616e-01 -3.30814719e-01
 -2.11456180e-01  2.79441830e-02 -2.03308128e-02  1.04587026e-01
  4.47399467e-01 -4.72991765e-02 -2.02567041e-01 -7.76326656e-02
  1.69214159e-02 -1.09607756e-01  4.16235365e-02 -7.62617290e-02
 -1.61034897e-01  1.10177636e-01  2.86512196e-01 -1.38920471e-01
  4.56103444e-01  5.78914881e-02  1.32342324e-01 -2.17359528e-01
 -3.07006717e-01 -5.39368130e-02 -1.03504330e-01  2.80601799e-01
  1.13947108e-01  3.18425633e-02  4.88879174e-01  1.01090416e-01
  1.69713944e-01 -2.90595181e-02  2.28865921e-01 -1.13490850e-01
  1.99751407e-01  7.51137957e-02  5.53879291e-02  5.72335087e-02
  9.95409489e-02 -1.41754061e-01  4.54768054e-02  2.00668871e-01
 -1.44020682e-02 -4.21397425e-02  5.87256849e-01  1.62166134e-01
 -5.19609116e-02  1.03717729e-01  2.90933430e-01 -1.66944399e-01
 -2.35921592e-01 -1.47994936e-01  3.63706648e-02  1.77830338e-01
  1.81071460e-01 -1.84926778e-01  1.87002450e-01  2.57782817e-01
  8.17492232e-02 -2.13078186e-01  2.40850836e-01  1.37079015e-01
  3.74938518e-01 -4.52954210e-02  1.36335492e-01 -2.37988442e-01
  4.56639826e-02  2.13612884e-01 -4.53383699e-02  7.76587784e-01
 -2.54295349e-01 -2.30244339e-01  2.05856726e-01 -1.09246597e-01
  2.71796495e-01  1.90245062e-01 -3.68403643e-01 -1.44550920e-01
  9.24083591e-02 -1.45584375e-01  1.38906375e-01  2.26808861e-01
 -5.59449345e-02  2.48194233e-01  1.71340257e-02 -5.12266792e-02
  3.66486460e-02 -1.58021271e-01 -1.03978209e-01 -7.62763172e-02
 -2.76347697e-01  1.96942434e-01 -2.68995941e-01 -2.48975068e-01
  3.00142989e-02  6.46487772e-02  6.43667579e-02 -7.83661902e-02
  3.12974192e-02 -2.54547864e-01 -1.89992696e-01 -3.71436626e-02
  2.37246320e-01  2.99339354e-01  4.84688222e-01  2.38146871e-01
  2.73816258e-01 -1.06289968e-01 -1.09287731e-01 -2.30422154e-01
 -2.40783632e-01  1.54200494e-01 -2.15364888e-01 -2.10220478e-02
  1.77943408e-01  5.57342693e-02 -4.12109166e-01 -1.09658346e-01
  1.45894721e-01  1.91596299e-01 -1.33996636e-01 -1.44607097e-01
 -2.18166009e-01 -2.46815197e-02  1.86440758e-02 -2.58254111e-02
  2.58519888e-01 -8.32604408e-01 -3.29236984e-02 -1.96731851e-01
 -3.81479822e-02  8.61935541e-02  3.46766651e-01 -1.90115184e-01
 -2.37926245e-01  9.14169401e-02  2.44381368e-01 -2.88429502e-02
 -2.37250552e-01  8.82382020e-02 -2.25123823e-01 -3.09145115e-02
  3.94872665e-01  2.21235678e-02 -8.81775394e-02  3.32349092e-02
  2.83455774e-02  2.23913435e-02 -1.36009663e-01  8.74620825e-02
 -2.16654643e-01 -4.97736521e-02 -1.17573917e-01  3.55702192e-02
  1.43815190e-01 -2.79603094e-01 -1.92787834e-02 -1.21114943e-02
 -5.13719022e-01  1.07187718e-01  3.37947816e-01 -7.01427087e-02
 -1.53801471e-01  1.90970987e-01 -4.09411013e-01 -1.22980244e-01
 -7.16537535e-02  1.61778275e-02 -6.42645836e-01  6.51781261e-02
 -3.37273180e-02 -1.10632248e-01 -3.00724387e-01 -1.33602604e-01
 -7.27558196e-01 -5.97450696e-02 -3.27176899e-01  1.38767749e-01
 -1.40319139e-01  5.06648958e-01 -8.95725489e-02  1.98144883e-01
  1.85834169e-01 -2.72674978e-01  1.30172014e-01 -8.09190124e-02
  1.09590985e-01 -1.83354676e-01 -5.09649932e-01  6.14532828e-02
 -3.06993634e-01 -2.31164858e-01 -1.22219309e-01  2.23809201e-02
 -8.86056721e-02  1.51195601e-01 -3.42737362e-02 -2.64983587e-02
  3.01795769e-02  3.15883160e-01 -7.81976581e-02 -3.42887975e-02
  4.04866010e-01  1.76960796e-01  4.85748738e-01  7.20246285e-02
 -1.19198076e-01  5.35085052e-02  1.45270750e-01  1.12656444e-01
  2.26723179e-01  2.09817380e-01  6.57338277e-02  7.07556844e-01
  1.99878111e-01  2.82640755e-01 -1.52978510e-01  2.30725616e-01
 -7.45451972e-02  7.63354599e-02 -3.62778902e-01 -1.75321609e-01
  1.17742360e-01 -4.75790203e-01 -1.38528243e-01  2.40820460e-03
  1.01217307e-01  1.12146534e-01 -4.03518707e-01 -6.46693632e-03
  3.50278974e-01  1.05161816e-02 -2.30748802e-01 -5.47874495e-02
 -2.71423031e-02 -3.27582538e-01 -3.97167355e-03 -2.52131820e-01
 -8.03316087e-02  2.24157989e-01 -2.65766203e-01  2.51026273e-01
  1.03537679e-01  2.03004889e-02 -8.69602710e-02  1.49062946e-01
  4.12365705e-01  1.84769392e-01  1.39551550e-01 -2.72620879e-02
 -1.07383735e-01 -4.80934456e-02  1.65775776e-01 -2.28274390e-01
 -6.24983236e-02  3.81419659e-02  3.24515015e-01 -1.63690031e-01
  3.09762478e-01 -1.48796260e-01  2.83172309e-01  9.39489529e-02
  8.34069401e-02  3.20684671e-01 -2.45899945e-01 -3.02396957e-02
 -7.66509771e-02  4.00114894e-01  1.93194315e-01 -5.08528501e-02
 -3.73951122e-02  1.81017257e-03  1.31060094e-01 -1.72892541e-01
 -4.09863424e-03  8.10783952e-02 -5.91369271e-02 -1.97919860e-01
 -1.37748450e-01  2.18442939e-02 -2.13397712e-01 -3.22290629e-01
 -1.48653522e-01  2.50719011e-01  8.30906630e-02  5.83322123e-02
 -1.01061970e-01  1.21442303e-01 -4.66461852e-03 -1.15685686e-01
  1.21700369e-01  1.54666305e-01 -1.11243978e-01 -2.73181796e-02
  9.03378427e-02 -3.68529528e-01 -6.15609065e-02  5.12513757e-01
 -1.19856328e-01 -1.40408687e-02 -2.38603309e-01  2.31399402e-01
 -1.47004187e-01 -1.74693599e-01 -3.99538159e-01  3.93275559e-01
  5.58027253e-03 -9.15673524e-02  5.87161034e-02  5.84533215e-01
 -4.77389768e-02 -2.92066395e-01 -2.03908831e-01 -1.39311492e-01
  2.52657264e-01 -1.07849658e-01  2.59051416e-02 -3.85307282e-01
  1.97363168e-01  8.90970111e-01 -3.96071002e-04  3.02150011e-01
 -7.57889077e-02  1.80165425e-01  3.41238230e-01 -1.32159978e-01
 -2.17836991e-01  9.64849442e-03  4.73288111e-02  4.02690917e-02
 -3.06101203e-01 -1.46215275e-01 -3.27184170e-01 -1.59066707e-01]"
2.14 Docker image errors stat:awaiting response type:build/install stale subtype: ubuntu/linux TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14

### Custom code

Yes

### OS platform and distribution

Linux 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Updating docker file to latest version should allow for building image

### Standalone code to reproduce the issue

```shell
FROM tensorflow/tensorflow:latest-gpu

RUN dpkg --configure -a
```


### Relevant log output

```shell
When building this I get the error 

dpkg: error: error executing hook 'if { test ""$DPKG_HOOK_ACTION"" = add-architecture || test ""$DPKG_HOOK_ACTION"" = remove-architecture; } && test -x /usr/share/pkg-config-dpkghook; then /usr/share/pkg-config-dpkghook update; fi', exit code 32512
```
",False,"[-3.32693398e-01 -3.43916148e-01  1.79977745e-01  1.02441095e-01
  5.89740753e-01 -4.82926667e-01 -1.50359377e-01  7.38097355e-02
 -2.88709968e-01 -3.82259429e-01  4.59391177e-02  8.02360550e-02
 -7.71124959e-02  4.82643545e-02 -5.11769712e-01  3.26380074e-01
 -3.63076836e-01 -2.21674263e-01  3.63761246e-01  5.49397349e-01
 -3.38911444e-01  1.11903660e-02 -2.53708869e-01  1.71771377e-01
  2.02504233e-01  1.78513944e-01 -5.75558007e-01  2.22923711e-01
  1.41894706e-02  1.57761246e-01  5.72036982e-01  3.10138762e-01
 -2.61175156e-01  3.90073508e-02  4.52290475e-01  3.98770235e-02
 -2.78212965e-01 -1.01183519e-01 -5.52287936e-01 -5.28595373e-02
 -1.01515288e-02 -4.31331843e-02  8.67167264e-02 -2.14602590e-01
 -6.53909892e-02 -2.59860069e-01 -4.59191352e-02 -3.42071682e-01
  1.40782036e-02 -2.48768434e-01 -3.27283442e-01  7.20248893e-02
 -3.78579438e-01 -5.09141326e-01 -9.32214409e-02 -8.17790255e-02
 -1.35633677e-01  1.18641041e-01  1.44493580e-01  4.26296979e-01
  1.13928892e-01  4.66019027e-02  8.55266079e-02 -1.11810125e-01
  3.40208739e-01  3.04568499e-01  1.85238838e-01 -1.91040128e-01
  4.09315795e-01 -3.50770116e-01  1.32307544e-01 -1.10000730e-01
 -3.48164082e-01  4.90970165e-02  7.04110935e-02 -6.47014305e-02
 -6.48739189e-02  1.55823603e-01  1.67987555e-01 -2.17723832e-01
 -1.56014055e-01 -4.32383090e-01 -2.56806701e-01 -2.24702686e-01
 -5.59924692e-02 -1.61812708e-01  2.77529001e-01  1.62112445e-01
  4.39225495e-01 -3.07785511e-01  3.69323581e-01  5.52645206e-01
 -1.15054380e-02  1.09819509e-01  5.91262281e-01  5.37375361e-02
  1.45632014e-01  3.32409531e-01 -1.33943677e-01  9.70596969e-02
 -4.85153608e-02 -1.80712149e-01  6.62600398e-02 -2.44911127e-02
 -3.10463548e-01 -2.34400094e-01  2.61209518e-01 -1.86266571e-01
 -4.43939380e-02  3.01038027e-02  2.43088081e-01  7.65417442e-02
  1.84949338e-02 -1.38773620e-01 -2.86600918e-01 -1.11987278e-01
 -2.10051790e-01 -2.80105293e-01 -3.11306119e-02  3.19039643e-01
 -8.87990892e-02 -9.43425149e-02 -1.79900527e-01  2.27605820e-01
  2.21092403e-01  6.49534911e-02 -2.39217192e-01 -8.01200122e-02
  1.64540887e-01 -2.81861015e-02  9.50170308e-02 -7.43380785e-02
  2.53877193e-01  1.44064546e-01  6.23621568e-02  7.00639337e-02
  1.97082199e-02 -1.03331178e-01 -3.51718068e-01 -3.32182229e-01
 -2.05808192e-01  4.84872721e-02  7.70109706e-03 -5.22703052e-01
  3.15498840e-03  1.33111477e-01 -3.45612824e-01  2.21470475e-01
 -2.98331857e-01  4.20381755e-01 -4.73871231e-02  1.15661815e-01
 -1.33778855e-01  5.14043272e-01  7.12079331e-02 -4.41663107e-03
  3.77906263e-01 -9.65653732e-02 -9.19146016e-02 -3.18473637e-01
 -1.08590052e-01  4.27653670e-01 -1.35407271e-02 -2.47705355e-01
  1.39655620e-02  1.82479620e-01 -5.88377714e-01 -2.96186626e-01
  1.61190405e-01  5.65951824e-01 -2.41155207e-01 -1.84549600e-01
  2.37408489e-01 -2.93715559e-02  3.11914921e-01 -8.76807868e-02
  1.89769834e-01 -2.64188737e-01 -2.90261731e-02  5.23000240e-01
  1.53842360e-01  2.76457131e-01  1.41897993e-02  2.85494387e-01
  1.81020826e-01  3.14614177e-02  2.39585154e-02  7.18980804e-02
 -3.35766196e-01 -7.30129033e-02 -3.44729602e-01 -8.13021660e-02
  4.91659909e-01  1.65016264e-01 -1.96840256e-01  4.71124023e-01
  3.23518991e-01  2.86821797e-02  2.75830507e-01 -5.81876282e-03
 -6.24232553e-02 -1.29145846e-01  1.04839787e-01  4.02860017e-03
 -3.54511067e-02 -2.76123017e-01  8.44990388e-02 -1.61389887e-01
 -3.05732340e-01  2.30904594e-01  7.32552931e-02 -5.72650492e-01
 -1.42487064e-02 -3.62774670e-01 -2.41570666e-01  4.72249299e-01
 -1.88616216e-01 -2.17976142e-02 -3.03987265e-02  3.82084280e-01
  8.27739015e-02 -2.78866708e-01 -1.41522735e-02 -3.05292070e-01
 -5.56434691e-02  2.93154120e-01 -4.65087175e-01  3.30618769e-03
 -1.59850299e-01  1.98027417e-01  1.59520939e-01  1.56733423e-01
  3.63937974e-01  4.84325066e-02  3.84972870e-01 -1.09284446e-01
 -7.69591779e-02 -2.25357890e-01 -3.79878908e-01  2.43828356e-01
 -2.78622061e-01 -3.65815938e-01 -8.68796334e-02  3.68002243e-02
  4.51104343e-01  3.24021429e-01 -3.06855887e-04  1.96954936e-01
 -4.69481975e-01  1.16891876e-01  6.70009106e-02  1.69099361e-01
  2.66983211e-01  8.91461000e-02  5.73723733e-01  3.11596334e-01
  1.93728119e-01  4.03979123e-01  2.27509767e-01 -2.67871529e-01
  3.31812948e-01  1.97955728e-01 -6.62369132e-02  3.26151609e-01
  2.41821587e-01  1.74051836e-01 -2.73220241e-01  5.34877896e-01
  6.47611767e-02 -3.70256603e-01  3.53520244e-01 -3.91143948e-01
  7.24163473e-01 -4.79085028e-01 -1.69580728e-01 -2.63602827e-02
  3.64225566e-01 -8.40711668e-02  2.64826734e-02  2.89906655e-03
  5.01826685e-03  4.20408189e-01 -3.04794699e-01  2.50698566e-01
  2.45060980e-01 -6.99337795e-02 -2.09751531e-01 -7.52016306e-01
 -3.41609240e-01  2.03424960e-01 -3.78864646e-01 -2.62992382e-02
 -4.34815466e-01  3.09239328e-01 -1.66505128e-01 -1.52914226e-01
  7.57446885e-02  8.57300609e-02  1.60509259e-01  1.40737265e-01
 -3.36462140e-01  7.69646615e-02  4.59509909e-01 -4.53492999e-01
 -2.88178921e-01 -2.71201935e-02  2.15736181e-02  3.87327194e-01
  4.61029589e-01 -4.15214181e-01  1.50655180e-01 -7.67656937e-02
 -1.96657300e-01  4.67444301e-01  1.81954294e-01 -7.25501925e-02
 -6.23226523e-01  7.84300745e-01  8.08016881e-02 -1.39877439e-01
  3.01610351e-01 -7.60550946e-02 -3.49677444e-01  1.36332244e-01
  2.58968651e-01  2.36723572e-02 -7.65417144e-02 -2.79497892e-01
 -1.52879655e-02  2.06316859e-02  5.27384728e-02 -1.39869466e-01
 -5.16471788e-02  1.73041373e-01 -1.75998926e-01 -2.57472903e-01
 -2.94854373e-01  1.40376851e-01 -2.26282030e-02 -5.05022645e-01
 -1.60120457e-01 -3.75973105e-01  1.68236509e-01 -2.85460055e-01
 -7.05297738e-02 -2.29043201e-01  2.65658051e-01  3.80235940e-01
 -1.73986405e-01  1.10615700e-01  2.24945426e-01  1.30411655e-01
 -4.79236186e-01  1.52327061e-01 -1.02052152e-01  2.57583112e-01
  6.23493306e-02  1.26454327e-02  3.79680693e-01  1.10188395e-01
 -3.03078920e-01  1.55675590e-01 -4.35657352e-01  7.11632371e-02
  1.37943223e-01  2.20928565e-02 -2.61541367e-01 -3.02136131e-02
  1.66513264e-01  3.04037631e-01  6.72701672e-02  7.87907541e-02
 -3.98750395e-01  2.21322685e-01  4.44265246e-01 -3.19556534e-01
 -1.43811375e-01  2.07868695e-01  1.51812553e-01 -1.62710652e-01
  3.97601351e-02 -2.03721169e-02  1.36846125e-01  2.03627273e-01]"
AttributeError: module 'object_detection.protos.input_reader_pb2' has no attribute 'NUMERICAL_MASKS'  stat:awaiting response type:support stale comp:model TF 2.13,"python3 train.py --logtostderr --train_dir=CAPTCHA_training_dir/ --pipeline_config_path=CAPTCHA_training/faster_rcnn_inception_v2_coco.config

While running this command I am getting error like this :

Traceback (most recent call last):
  File ""train.py"", line 51, in <module>
    from object_detection.builders import dataset_builder
  File ""/home/Desktop/test/test_env/lib/python3.8/site-packages/object_detection/builders/dataset_builder.py"", line 32, in <module>
    from object_detection.builders import decoder_builder
  File ""/home/Desktop/test/test_env/lib/python3.8/site-packages/object_detection/builders/decoder_builder.py"", line 24, in <module>
    from object_detection.data_decoders import tf_example_decoder
  File ""/home/Desktop/test/test_env/lib/python3.8/site-packages/object_detection/data_decoders/tf_example_decoder.py"", line 131, in <module>
    class TfExampleDecoder(data_decoder.DataDecoder):
  File ""/home/Desktop/test/test_env/lib/python3.8/site-packages/object_detection/data_decoders/tf_example_decoder.py"", line 136, in TfExampleDecoder
    instance_mask_type=input_reader_pb2.NUMERICAL_MASKS,
AttributeError: module 'object_detection.protos.input_reader_pb2' has no attribute 'NUMERICAL_MASKS'

Please do help me to solve this issue.

",False,"[-2.48433232e-01 -3.01234692e-01 -1.34038836e-01  9.15162079e-03
  2.83577323e-01 -6.80239499e-02  5.83630614e-02  2.84725845e-01
 -4.17494446e-01 -9.20423120e-02 -5.18995970e-02 -3.42148244e-01
  1.47971243e-01  1.87598065e-01 -8.15839320e-02  1.22852847e-01
  3.83335026e-03 -1.64457947e-01  3.71229172e-01 -1.08756870e-01
 -6.85057640e-02  4.77115773e-02  1.05052907e-02  1.40014231e-01
 -6.53188899e-02  2.12877437e-01 -5.79624809e-02 -1.48657382e-01
 -4.62087467e-02  4.55111787e-02 -4.12877612e-02 -1.85910791e-01
 -4.33304310e-01  4.56661917e-02  2.58999079e-01  1.66029692e-01
 -9.87735242e-02 -2.01583244e-02 -1.12425558e-01 -2.45894603e-02
 -1.97964702e-02  3.38852033e-02  1.90451264e-01  1.36005968e-01
  1.32424861e-01 -5.06726131e-02 -3.07862490e-01  1.07267290e-01
 -1.84434250e-01 -1.11782745e-01 -2.71696925e-01  1.07508972e-01
 -1.28001958e-01 -5.06932139e-01  2.96984445e-02 -5.27210198e-02
  1.70366138e-01  3.83562416e-01  9.67536569e-02  1.39527410e-01
 -6.44102693e-02  3.49931680e-02  1.68907583e-01 -1.00379407e-01
  1.65945947e-01  6.88036531e-02 -2.45135296e-02  3.07556670e-02
  5.23020506e-01 -4.33031097e-03 -6.05695099e-02 -7.06733614e-02
 -1.35944426e-01 -1.42903745e-01 -6.36143088e-02 -9.57025290e-02
 -2.68380463e-01  2.25007653e-01  5.86666204e-02 -2.08318725e-01
 -3.38248253e-01  3.29308324e-02 -6.71217963e-02 -8.10444262e-03
  1.39775455e-01 -3.77343297e-02  3.45232844e-01 -5.99336699e-02
  4.22749609e-01  1.11333415e-01  4.12059814e-01  1.27736598e-01
  3.43279094e-02  4.91159439e-01 -4.57311347e-02  1.82546467e-01
  1.02271311e-01  2.84747183e-01 -7.75730237e-02 -2.01511830e-01
 -8.53134617e-02 -2.81005800e-01 -1.00280214e-02  1.96901590e-01
  1.15300100e-02 -9.94417220e-02  2.22804576e-01 -2.37427466e-02
  3.43354225e-01 -7.81992525e-02  9.62961167e-02 -1.23409048e-01
 -2.10130692e-01  1.31863296e-01  6.58534616e-02  1.79538310e-01
 -2.16503710e-01  1.59998015e-01 -1.18143290e-01  4.33071554e-01
  1.99528575e-01 -1.99704438e-01  2.31134921e-01  2.61443872e-02
  1.56626493e-01 -3.19720179e-01  9.41985026e-02 -6.41104355e-02
  5.10330983e-02 -1.14287801e-01  1.44861251e-01  2.46479481e-01
 -1.93855941e-01 -1.56154826e-01  9.31648761e-02  9.21978056e-02
 -7.67542869e-02 -9.62283462e-02 -1.40796572e-01 -2.15797618e-01
 -3.93486500e-01  9.43149179e-02 -3.69440764e-03 -1.75827235e-01
 -9.38039944e-02  2.54587568e-02 -3.13611925e-01  3.17716122e-01
  3.88605371e-02 -1.64268762e-01  9.62213427e-02 -2.90985286e-01
 -1.73572183e-01  6.27365112e-01  1.67763785e-01  2.74871588e-01
  2.58492172e-01  1.58933699e-01  2.61355907e-01 -4.21243519e-01
  6.16121478e-02  4.76721883e-01 -6.08211569e-03 -9.65947062e-02
  2.54026234e-01  1.21130764e-01 -2.68158376e-01 -1.17831007e-01
 -1.40550092e-01 -1.25587001e-01 -1.00619338e-01 -1.37911230e-01
 -1.92140222e-01 -3.56692195e-01  2.23908767e-01 -2.01858163e-01
 -2.88186371e-01 -2.42795050e-01 -8.90711546e-02  1.85400337e-01
  1.76538408e-01  2.02094108e-01 -3.25837359e-02  1.13850914e-01
 -1.16908662e-01  2.02806085e-01  2.05444902e-01 -1.15840778e-01
 -3.48262526e-02  9.91038010e-02 -1.04993045e-01  2.08838787e-02
  2.99419045e-01  2.78647505e-02 -8.17848742e-02 -1.98382810e-01
  4.50421810e-01  1.29625723e-01  1.54350176e-01 -1.29064750e-02
 -1.11349605e-01 -4.10041772e-04  2.08748937e-01 -1.77617460e-01
  1.49721950e-01  6.90456778e-02 -1.90427512e-01 -3.58207107e-01
 -1.01714909e-01  9.81851220e-02 -3.51553082e-01 -2.86564499e-01
  2.10134998e-01 -1.60888657e-01 -3.48682180e-02  2.17884928e-01
 -2.06668079e-01  1.38811767e-01  5.44449873e-02  9.50540416e-03
 -7.72750229e-02  5.71064949e-02 -1.68126717e-01 -2.97552705e-01
  1.39115602e-01  1.68970570e-01  7.22050518e-02 -1.16764374e-01
 -1.65630132e-01 -1.53566837e-01  1.87936753e-01  1.21894497e-02
  3.64425600e-01  2.13664442e-01 -4.63741180e-03 -3.22197229e-02
  3.40292826e-02 -2.35439986e-02 -1.78796938e-03  1.68406665e-01
 -2.80373275e-01  3.36558037e-02  2.30195206e-02 -2.46404693e-01
  2.52980113e-01 -2.49591604e-01 -1.42928720e-01 -1.98863178e-01
 -2.07448542e-01  1.52830720e-01  1.34005547e-02 -1.26377672e-01
 -3.16080078e-02 -1.15700446e-01  3.80830705e-01  4.29792970e-01
 -1.14715979e-01 -1.37438886e-02 -1.07438587e-01 -2.75274813e-01
  5.63352704e-02  2.98036903e-01  1.62666559e-01 -2.04916541e-02
  8.11228156e-03  1.37130469e-01 -4.21363831e-01  2.87673414e-01
 -7.23177344e-02 -8.69697779e-02  2.38882840e-01 -5.71712613e-01
  2.15745747e-01  7.64922947e-02  4.03529376e-01 -1.53247207e-01
  3.12130153e-01 -1.62298098e-01 -1.08212516e-01  1.81255996e-01
 -2.37492830e-01  2.80757934e-01 -1.51671499e-01  1.38918608e-01
  4.18865979e-01 -5.17251901e-02 -2.02424437e-01 -2.26330101e-01
 -2.48885095e-01 -9.73101556e-02 -1.70082003e-01  2.11029679e-01
  1.07940041e-01 -1.41230166e-01 -6.54609650e-02  2.39366353e-01
  1.10958099e-01  9.12119895e-02  4.58183698e-03 -5.13386950e-02
 -3.50171238e-01  1.04620047e-01  2.38843948e-01  5.80933988e-02
  5.39178886e-02  3.22807953e-03  1.84703559e-01 -4.77070808e-02
  5.59899926e-01 -3.53104055e-01  2.20988944e-01  3.16006809e-01
 -1.06640548e-01  2.53880858e-01  2.30320450e-03  1.43208534e-01
 -1.73744321e-01  4.50778931e-01 -1.13836825e-01 -2.00875029e-01
 -1.63538665e-01 -2.81510264e-01 -2.63920039e-01  1.19868353e-01
  2.00740024e-01  1.28078371e-01 -5.18337488e-01  8.87083821e-04
 -2.30152339e-01 -4.70770858e-02  1.99630260e-01 -1.86752733e-02
 -2.10348040e-01  1.62626863e-01 -2.82681733e-02  6.07562363e-02
 -3.30980957e-01  2.99362779e-01 -2.10723318e-02 -3.43282849e-01
  1.06088825e-01 -1.86769783e-01  6.72141239e-02 -5.71841896e-02
 -1.38110906e-01 -1.24568991e-01  2.20907301e-01  5.94395876e-01
 -1.03591913e-02 -2.20151335e-01  3.16918604e-02  1.74125507e-02
 -8.64904672e-02 -1.80494115e-02 -8.86826590e-03  5.19959271e-01
  1.61310792e-01  4.57511880e-02  1.70134589e-01  4.55065221e-01
 -1.73746794e-01 -4.53904420e-02 -3.15631121e-01 -2.72821605e-01
  2.97133066e-02  1.73117146e-02 -3.12770605e-01 -2.50032395e-01
 -1.46897152e-01 -2.37770174e-02 -2.16836154e-01 -8.12639296e-02
 -3.84449005e-01  1.02977112e-01  3.04503977e-01  2.99217813e-02
  5.47268875e-02  7.76291713e-02  5.58689833e-02  2.60784477e-01
  3.09318304e-02  2.07472861e-01  1.17750637e-01 -2.65247207e-02]"
AttributeError: module 'object_detection.protos.input_reader_pb2' has no attribute 'NUMERICAL_MASKS'  stat:awaiting response type:support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",False,"[-0.26626927 -0.3733266  -0.34034806  0.01930273  0.0905654  -0.2434419
 -0.18877414 -0.01718141 -0.31859308 -0.16146114  0.04511485 -0.1684996
  0.17018132  0.11504669  0.0136838   0.16341212 -0.1579334   0.05354963
  0.14303416  0.03688841  0.01198617  0.18289787  0.08583238  0.09686644
  0.06462849 -0.05221817 -0.16625544 -0.137891   -0.02945801  0.08124394
  0.25515777  0.00394929 -0.23729655 -0.10160036  0.23197497  0.07789691
  0.07484832 -0.2099622  -0.02430428  0.12149654  0.12117855  0.20028417
  0.13046163  0.2297443   0.01678442  0.0241412  -0.06709366  0.02672785
 -0.0846092  -0.1413717  -0.15424584 -0.16883582 -0.39955422 -0.15954891
 -0.1429175  -0.07506341  0.15284404  0.09311644 -0.05653816  0.04930674
 -0.00958206 -0.00595534  0.26732546  0.07706809  0.01547115  0.19949746
 -0.07695878 -0.16916615  0.35523152 -0.11789862  0.12182044  0.04774624
 -0.25760338 -0.05369113 -0.10985892 -0.10957006 -0.5509456   0.23566517
  0.35335854 -0.02687261 -0.10915418  0.11644087  0.19867063  0.11537308
  0.17827147  0.0987549   0.13668163  0.17615491  0.09711027  0.17891875
  0.38645098  0.1901146   0.01857369  0.29038736  0.1871725   0.11816324
  0.06358413  0.06422213 -0.0552553  -0.1536118  -0.14843312 -0.43577698
 -0.20715253  0.11073347  0.07380699  0.11311571  0.12994231  0.04275837
  0.20524646 -0.09083477  0.20984612 -0.03585663 -0.05053844 -0.04454439
 -0.08356479  0.06572679  0.15418118  0.08456057 -0.2939155   0.3439919
  0.19412132  0.02598848  0.14071889  0.1439576   0.45766592 -0.11568042
 -0.26344788  0.18802449 -0.1055402   0.09279655  0.30635673  0.08677731
 -0.1923243  -0.09858478  0.06211352 -0.10703768 -0.1891761  -0.08443463
 -0.03599587 -0.0981847   0.01694417  0.08724196  0.00070126 -0.25232095
  0.04713041  0.27283365 -0.1671609   0.10206799  0.08110056 -0.20219816
 -0.03203411  0.14934751 -0.23131937  0.5803783   0.02806646  0.2135089
  0.17278543 -0.00930284  0.16837633 -0.42704934  0.01262019  0.21982193
 -0.05981931 -0.09603407  0.22869931 -0.04374399 -0.32364988 -0.22417504
 -0.13895153 -0.08721047 -0.14799736  0.15552261 -0.13263077 -0.2762349
  0.38737828 -0.02932047  0.07099412 -0.32630196 -0.09996773  0.21931675
  0.26316726  0.12036759 -0.04053697  0.1249581   0.00172075  0.08017372
 -0.18082055 -0.24577785 -0.03583239 -0.06985681 -0.23327754  0.01956635
  0.04652205 -0.0604786  -0.05691943 -0.08261424  0.10801072 -0.12214828
  0.10092569  0.0650046  -0.16690132  0.05754352 -0.1640546  -0.10072631
  0.02437854 -0.04732939 -0.05473305 -0.2973212  -0.3503946   0.07391686
 -0.19866323 -0.29373896 -0.0638331  -0.003671   -0.17018749  0.18913892
  0.03606744  0.18399553  0.02828727 -0.0094031  -0.18091848  0.00117696
 -0.07980303 -0.25755575 -0.3219207   0.21730547 -0.04245841  0.10667767
 -0.09845628 -0.02946377  0.03064341 -0.07578353  0.5753511   0.13355936
 -0.12996173  0.20833822 -0.27468944 -0.05304178  0.02791377 -0.05237798
 -0.42641193  0.01682855 -0.00259718 -0.1163718   0.00390037 -0.07828013
  0.06152064 -0.23738793 -0.22260274  0.27585685 -0.12242138 -0.5017556
  0.45417196 -0.02236722  0.16539535  0.2718153   0.07967457 -0.0324904
  0.00315091 -0.05255417  0.12201286 -0.12557337  0.21049216  0.35885972
  0.12384868  0.11300089 -0.23815516  0.20932738 -0.14148447 -0.22583227
  0.2576689  -0.07646617  0.40130988 -0.07761234  0.03468083 -0.05297802
  0.20893082 -0.08750395 -0.11148368  0.05290725 -0.10182113  0.13864635
  0.00276307  0.16579989  0.21261683 -0.23420437 -0.2040965  -0.20210077
 -0.14381137  0.03630686 -0.1197529   0.22844389  0.21984775  0.10746397
  0.01096123  0.03344298  0.18465042 -0.3195794   0.05943845  0.33822304
 -0.08043656  0.18425718  0.22702749 -0.05626504 -0.10285407  0.20938778
  0.15592541 -0.02153102  0.5831983  -0.28333336  0.2304024   0.22130123
 -0.23570876  0.47926405 -0.09526393 -0.0138372   0.03744905  0.3124139
 -0.05790498 -0.05457121 -0.12223961 -0.04698472 -0.20628819  0.10191541
  0.14647591  0.11130305 -0.37944543 -0.06171758 -0.17905563  0.06669344
 -0.00912515 -0.09261267 -0.02385793  0.06576183  0.02043531 -0.15751539
 -0.23337486  0.21454494 -0.10271329 -0.18732     0.03206624 -0.15857844
  0.04783364 -0.28098893 -0.27695268 -0.03227979  0.3163063   0.618433
 -0.09769444  0.12309272  0.12683846  0.11792624 -0.42690563 -0.02202548
 -0.23692966  0.2999971  -0.00221008  0.0358588   0.06918865  0.42089728
 -0.37241322 -0.00589912 -0.18455966 -0.23189585  0.12380784  0.17676675
 -0.1677832  -0.1580992   0.15298523  0.17544803 -0.2594408  -0.2184375
 -0.44457328  0.21865392  0.14028263  0.1142504   0.2398699   0.03477609
  0.12174884  0.14354602  0.00162318 -0.0552559  -0.08150215 -0.2259166 ]"
In TF-v2.12 there are no methods called `_set_hyper` and `_get_hyper` stat:awaiting response type:support stale comp:keras TF 2.12,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

Yes

### OS platform and distribution

Kaggle kernel

### Mobile device

_No response_

### Python version

3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0]

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

There are no methods called `_set_hyper` and `_get_hyper`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

class MyAdamOptimizer(tf.keras.optimizers.Optimizer):
    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, name=""MyAdamOptimizer"", **kwargs):
        super(MyAdamOptimizer, self).__init__(name, **kwargs)
        
        self._set_hyper(""learning_rate"", kwargs.get(""lr"", learning_rate))
        self._set_hyper(""beta_1"", beta_1)
        self._set_hyper(""beta_2"", beta_2)
        self._set_hyper(""epsilon"", epsilon)
        
    def _create_slots(self, var_list):
        for var in var_list:
            self.add_slot(var, ""m"")
            self.add_slot(var, ""v"")
            
    def _resource_apply_dense(self, grad, var):
        lr = self._get_hyper(""learning_rate"", var_dtype=var.dtype.base_dtype)
        beta_1 = self._get_hyper(""beta_1"", var_dtype=var.dtype.base_dtype)
        beta_2 = self._get_hyper(""beta_2"", var_dtype=var.dtype.base_dtype)
        epsilon = self._get_hyper(""epsilon"", var_dtype=var.dtype.base_dtype)
        
        m = self.get_slot(var, ""m"")
        v = self.get_slot(var, ""v"")
        
        m.assign_add((1 - beta_1) * (grad - m))
        v.assign_add((1 - beta_2) * (tf.square(grad) - v))
        
        m_hat = m / (1 - tf.math.pow(beta_1, tf.cast(self.iterations + 1, tf.float32)))
        v_hat = v / (1 - tf.math.pow(beta_2, tf.cast(self.iterations + 1, tf.float32)))
        
        var_update = lr * m_hat / (tf.sqrt(v_hat) + epsilon)
        
        var.assign_sub(var_update)
        
        return var_update
        
    def _resource_apply_sparse(self, grad, var):
        raise NotImplementedError(""Sparse gradient updates are not supported."")

    
optimizer = MyAdamOptimizer(learning_rate=0.001)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[5], line 42
     38     def _resource_apply_sparse(self, grad, var):
     39         raise NotImplementedError(""Sparse gradient updates are not supported."")
---> 42 optimizer = MyAdamOptimizer(learning_rate=0.001)

Cell In[5], line 7, in MyAdamOptimizer.__init__(self, learning_rate, beta_1, beta_2, epsilon, name, **kwargs)
      4 def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, name=""MyAdamOptimizer"", **kwargs):
      5     super(MyAdamOptimizer, self).__init__(name, **kwargs)
----> 7     self._set_hyper(""learning_rate"", kwargs.get(""lr"", learning_rate))
      8     self._set_hyper(""beta_1"", beta_1)
      9     self._set_hyper(""beta_2"", beta_2)

AttributeError: 'MyAdamOptimizer' object has no attribute '_set_hyper'
```
",False,"[-0.43270063 -0.27909395 -0.10797803  0.02229845  0.2703279  -0.45419505
 -0.05038683 -0.10952726 -0.32200563 -0.21585035  0.28447813 -0.00352252
 -0.06584071  0.10726741 -0.1377697   0.31929702 -0.22375563 -0.07363789
  0.05944388  0.14026439  0.15538292 -0.08847786 -0.25006276  0.1912333
  0.07229225  0.22208706 -0.3047616  -0.01449245 -0.01800587  0.2900899
  0.36984557  0.16093296 -0.31846023 -0.03629132 -0.04366009  0.15102796
 -0.22887254  0.06421876 -0.27616206 -0.06831405 -0.0526657  -0.17472523
  0.02051197 -0.11357135 -0.03158451 -0.17495567  0.13308696  0.00820349
  0.0692542  -0.32262784 -0.08619401  0.01111011 -0.35119402 -0.3785885
 -0.1000077   0.12286527  0.07409829 -0.06754856  0.05895353  0.09632581
  0.13912508 -0.00412074 -0.07987206  0.11090314  0.15326557  0.10589541
  0.18222435  0.07856639  0.47667527 -0.11352384  0.01117768 -0.01543012
 -0.12653467  0.28238425  0.18859932  0.2632672  -0.24491623 -0.0058725
  0.38248843  0.08626503  0.13283738 -0.16455834 -0.0101751  -0.12372285
  0.05720926 -0.0457924   0.37932545  0.23789579  0.4142643  -0.17956086
  0.41012654  0.28650573 -0.05082074  0.03626749  0.46517664  0.05376463
  0.00590946  0.1381688  -0.05740642 -0.10553636 -0.18006846 -0.28566015
 -0.06659007  0.10202651 -0.07798575 -0.14748877  0.08211024 -0.06515278
  0.14759819  0.00314888  0.07667996  0.03534395  0.02613764 -0.07816882
 -0.07555953 -0.03157457  0.00638588  0.0077281   0.13702759  0.60374194
  0.07336712 -0.26158226 -0.06219428  0.13525128  0.48124754  0.20957811
 -0.20402427  0.1057993   0.19894782  0.03171188  0.10608183  0.00447816
  0.18269119  0.09274687 -0.01610957 -0.10028907  0.06719326 -0.00140843
 -0.17860621 -0.0013365  -0.30048528  0.11223868 -0.11303879 -0.5919324
  0.17802839  0.10596389 -0.3092392   0.04666739  0.01710062  0.1547732
 -0.05300319 -0.08909279 -0.05571395  0.29936212  0.04690175  0.11310321
  0.24640346 -0.10725029  0.01489466 -0.4586782   0.11996952  0.32806385
 -0.14888105 -0.12338025  0.33684808  0.11060081 -0.28435868 -0.25217915
 -0.0050918   0.30378634 -0.19067475 -0.01029093 -0.14637189  0.07172884
  0.15727758 -0.03775991  0.09216619 -0.6678866  -0.14638291  0.25374258
  0.02010146 -0.07445503  0.03946429  0.2277104   0.07385503  0.02965627
  0.11406541  0.06151976 -0.3173706  -0.02605931 -0.356772   -0.15542911
  0.29331845 -0.08068047 -0.07317478 -0.0604438   0.20905972 -0.17946196
 -0.00619356 -0.01946123 -0.14040247 -0.10853577 -0.14055684 -0.06588671
  0.13517025 -0.32575926 -0.21578652 -0.47590715 -0.3120939  -0.05962044
  0.15977827 -0.21624023  0.20063831 -0.03801456 -0.3873645   0.19975908
 -0.17945898 -0.10603125 -0.22993752  0.4071411   0.01118174 -0.15341346
 -0.09789096 -0.3657828  -0.30642846  0.01595808 -0.312723    0.20589668
  0.08719319  0.3171336   0.08276229 -0.07929208  0.42192614  0.21972775
  0.19392154 -0.28466892 -0.15565051 -0.22085923 -0.03180387  0.04189416
 -0.6243032  -0.09336223  0.01312064 -0.02993849  0.12313664  0.33578402
 -0.28379285  0.04153835 -0.35206744  0.17560105 -0.12945864  0.07247858
  0.33216435  0.2136544   0.28933555  0.1192691   0.10960436  0.0913938
  0.34560585 -0.1706872   0.31066492  0.19227563  0.19159225  0.42364055
  0.27593204  0.2696912  -0.19551146  0.44803122 -0.13322876 -0.2026244
  0.08447727 -0.50943506  0.56172115 -0.3813445   0.19879395 -0.24265495
  0.3278383   0.02841881 -0.02448837  0.11073089  0.08110884  0.20707607
 -0.47942513  0.00614026 -0.07288809 -0.22788376 -0.20356992 -0.34992176
 -0.08850787 -0.01312458 -0.18115777 -0.06514399 -0.01011612  0.00756814
 -0.2608732   0.05539407  0.1509921  -0.17493454  0.22191781  0.19578394
 -0.27774024 -0.02856766  0.40941158 -0.3210577  -0.19838291  0.02202193
  0.3330515   0.09812096  0.37431082 -0.3661297   0.23092948  0.29643252
 -0.00600945  0.44996446  0.02166674  0.10718857 -0.39461675  0.62724113
  0.00073736 -0.06427471  0.22477019 -0.08699054 -0.03164208  0.14093907
  0.1475494   0.11517669  0.1345671  -0.22850627  0.15540603  0.02008519
 -0.15944457  0.0229102  -0.05754749  0.09207561 -0.04936541 -0.14603624
 -0.44771302  0.31138223 -0.04706046 -0.27699035 -0.3591621   0.06011409
 -0.15110148 -0.07976393  0.20821896 -0.22989887  0.24213564  0.44992736
  0.16928536  0.27420473 -0.04108707  0.10029165 -0.3501804  -0.02927944
 -0.23353821  0.46102688  0.09132525 -0.1785343   0.13905835  0.32563865
 -0.25239855  0.042418   -0.26314592  0.07999718  0.099781   -0.1067415
 -0.2318617  -0.21831416  0.1077721   0.17266063  0.05609088  0.17956668
 -0.3566883   0.18937948  0.37615845 -0.28593305 -0.27132344  0.08901191
  0.22976735 -0.23593128 -0.02214032 -0.1028348  -0.02909133  0.085304  ]"
Tensorflow debug build reports the error: relocation R_X86_64_PC32 against undefined symbol `_ZN15stream_executor3gpu12_GLOBAL__N_120CUDABlasLtMatmulPlan14kMaxBatchCountE' can not be used when making a shared object stat:awaiting response type:build/install stale TF 2.0 subtype: ubuntu/linux,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

1.16.0

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

12.2

### GPU model and memory

GTX 1080 & RTX 2080

### Current behavior?

Reported the following error:
```
WARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /root/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /root/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/usr/bin/python3 --config=xla --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.7 --action_env TF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,6.2,7.0,7.2,7.5 --action_env LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-9 --config=cuda --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /root/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /root/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /root/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file /root/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /root/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:cuda in file /root/tensorflow/.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file /root/tensorflow/.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:dbg in file /root/tensorflow/.bazelrc: --config=opt -c dbg --cxxopt -DTF_LITE_DISABLE_X86_NEON --copt -DDEBUG_BUILD
INFO: Found applicable config definition build:opt in file /root/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare --define with_default_optimizations=true
INFO: Found applicable config definition build:linux in file /root/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /root/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Build options --copt and --cxxopt have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 32967 targets configured).
INFO: Found 1 target...
ERROR: /root/tensorflow/tensorflow/BUILD:724:1: Linking of rule '//tensorflow:libtensorflow_framework.so.2.4.4' failed (Exit 1)
/usr/bin/ld: bazel-out/k8-dbg/bin/tensorflow/stream_executor/cuda/libcublas_plugin.lo(cuda_blas.pic.o): relocation R_X86_64_PC32 against undefined symbol `_ZN15stream_executor3gpu12_GLOBAL__N_120CUDABlasLtMatmulPlan14kMaxBatchCountE' can not be used when making a shared object; recompile with -fPIC
/usr/bin/ld: final link failed: bad value
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 7241.142s, Critical Path: 2298.04s
INFO: 6729 processes: 6729 local.
FAILED: Build did NOT complete successfully
```

### Standalone code to reproduce the issue

```shell
git clone -b r2.0 https://github.com/tensorflow/tensorflow.git
bazel build --config=cuda --config=dbg --copt=-fPIC --cxxopt=-fPIC //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_",False,"[-0.443695   -0.38070935 -0.4763907   0.1008372   0.29143274 -0.3715442
 -0.10486747 -0.17089194 -0.17258823 -0.2660311  -0.00322962  0.00525207
 -0.3453048   0.11347401 -0.2842582   0.3342442  -0.27512175 -0.12320272
  0.05592167  0.20783283 -0.22957806 -0.05784284 -0.38275596  0.28231013
  0.28832912  0.31435105 -0.28269726 -0.00204336  0.21685833  0.33188087
  0.5146879  -0.01434108 -0.15110913  0.04315127  0.1840966   0.25489983
 -0.45203748 -0.19530322 -0.18901923  0.05828789  0.21991916  0.08320118
  0.03679334 -0.16969651 -0.03767425 -0.02635392 -0.10913894 -0.0556445
 -0.04924949 -0.03632868 -0.15740827 -0.09145607 -0.3802075  -0.3274557
 -0.0966889  -0.01092714  0.15417269  0.06693339  0.12872204  0.15536235
  0.19486392 -0.11112559  0.131333    0.02969781 -0.19926253  0.00763468
  0.05298197 -0.05142575  0.4601645  -0.12912196  0.2388311  -0.07477799
 -0.24765131  0.02430142  0.16036794  0.06128887 -0.00164632  0.29951972
  0.19132033 -0.07512213 -0.12653653 -0.17715903 -0.0872753  -0.07273398
  0.18578294 -0.19709665  0.16621155  0.10680219  0.38278228 -0.34691477
  0.652905    0.33252272  0.05518211  0.09715439  0.5395318   0.07472461
 -0.06492132  0.406502    0.03764588 -0.19758311 -0.31997678 -0.19389895
  0.05957416  0.2572909  -0.13471276  0.0376776   0.05744287 -0.08227676
  0.24058731 -0.21165751  0.09574364 -0.02632163  0.15359543  0.02892144
  0.00598217  0.01829014 -0.19140626  0.11484452 -0.16257453  0.57120776
 -0.15927824 -0.35245842  0.02558629  0.02409872  0.357037    0.07941779
 -0.44012433 -0.14066586  0.06086121  0.03810134  0.10528159  0.19429281
 -0.21658799 -0.0106912   0.08130241 -0.05428876 -0.3951893  -0.2506352
 -0.2008633  -0.05683452 -0.02182917  0.28341827 -0.22774306 -0.531689
  0.25445706  0.10138807 -0.12528342  0.17528859 -0.07116908  0.16967225
 -0.00885306 -0.14901167 -0.12018524  0.39040577  0.2118121   0.13170958
  0.32348716  0.02261255 -0.10720835 -0.6924804  -0.08439387  0.3915301
  0.08592457  0.02391697  0.30726355  0.10762456 -0.44641298 -0.20736498
  0.33293638  0.2071509  -0.28980917 -0.08526923  0.09448737  0.3838497
  0.46070075 -0.11333224  0.54477835 -0.6593329  -0.23685974  0.23774467
  0.34609616  0.03139877  0.28176758  0.02375227  0.01930243  0.18478876
  0.14588098 -0.04704447 -0.06007008  0.15638122 -0.2961585   0.1816578
  0.40718138 -0.00605503 -0.11432917  0.09100212  0.20830484 -0.2556389
  0.2050806   0.20631948 -0.07293577  0.07680538 -0.11649627 -0.06342538
 -0.1224208  -0.30764115  0.10343111 -0.32282752 -0.33387208  0.1263771
  0.048974   -0.27444595  0.01762248 -0.18048382 -0.32882223  0.05976468
 -0.18368825  0.01458423 -0.19970772  0.16940638  0.06081943 -0.20408969
  0.00978122 -0.2834804  -0.2658149  -0.128564   -0.3503096   0.18375896
 -0.08666387  0.26781172  0.05313305 -0.08295561  0.40914235  0.23803222
  0.3638969  -0.29427668 -0.03171954 -0.23538275 -0.11311616  0.08039729
 -0.49189273 -0.22913246 -0.19513804 -0.21822602  0.19778815  0.03912896
 -0.13983321 -0.11037895 -0.2821299   0.48236582  0.05311695 -0.10399227
  0.26182875 -0.01492205  0.13107018  0.2880332  -0.051552    0.15843087
  0.13622075 -0.08253077  0.04017682  0.26548702  0.07586782  0.37209818
  0.2646684   0.39434963 -0.26990852  0.39221007 -0.13856593 -0.02182057
  0.17263961 -0.2430268   0.50236976 -0.48367095  0.18990847 -0.14456949
  0.567054    0.15109369  0.01764986  0.0968367   0.18617965  0.58166826
 -0.18330742  0.2751755   0.2493187  -0.17918304 -0.12372404 -0.42717806
 -0.19914843  0.17390692 -0.2133072   0.01572896 -0.08841176 -0.03827526
 -0.3022027   0.0664844   0.08532656 -0.16007227  0.12821864  0.26822636
 -0.10829034  0.00881034  0.3960796  -0.29375398  0.02336225 -0.22534654
  0.28656125  0.08597171  0.47989425 -0.6228286   0.39367706 -0.10872161
 -0.1488691   0.42595157  0.06518915  0.15926927 -0.373278    0.28553566
  0.08985838 -0.10915603  0.2314682  -0.09141162 -0.22506145 -0.12015183
  0.2230568  -0.06636722 -0.02246185 -0.2979516  -0.13236909  0.38312316
 -0.14077745 -0.35752904 -0.05986071  0.15618262 -0.29218975 -0.3530601
 -0.42149642  0.21260741  0.05625539 -0.25891212 -0.05144928 -0.09265829
 -0.08428388 -0.1441888  -0.04303532 -0.53891516  0.24498998  0.5690154
  0.13855538  0.05553301 -0.04335466  0.15013283 -0.28599256  0.06259127
 -0.13254794  0.25334898  0.02059169  0.06251407  0.09849019  0.25293714
 -0.01521892  0.31778917 -0.3210619  -0.0091619   0.19511944 -0.3287934
 -0.41971943 -0.12352856 -0.01765043  0.3304308  -0.11585045  0.31790823
 -0.34125227  0.1947715   0.53512335 -0.48687223 -0.3302592   0.19979143
 -0.00333103 -0.17567617 -0.01228919 -0.01750851  0.03269203  0.13429601]"
implement llama 2 using Tensorflow type:feature,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.8

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

-

### Standalone code to reproduce the issue

```shell
-
```


### Relevant log output

```shell
-
```
",False,"[-5.80936313e-01 -8.31716284e-02  5.38714975e-02  1.18629128e-01
  1.77771568e-01 -4.65258569e-01 -2.67491519e-01 -1.69530421e-01
 -1.30523041e-01 -1.86156496e-01  1.11234337e-01 -8.01779479e-02
 -1.96113154e-01  2.35336781e-01  1.17253140e-01  3.61531079e-01
 -1.06110752e-01  1.73500970e-01  1.67845130e-01  1.90468848e-01
  8.47436190e-02 -2.37568900e-01 -2.67461210e-01 -2.05598716e-02
  1.92024499e-01  2.27225557e-01 -3.13501745e-01 -1.67428911e-01
  6.73657581e-02  2.23274514e-01  4.11097407e-01  2.11210907e-01
 -1.48709521e-01 -5.69461212e-02 -1.06433585e-01  1.77510381e-01
  3.45263481e-02 -1.07223004e-01 -3.41460228e-01  2.71877879e-03
 -2.35790461e-01  2.52032101e-01  1.84334785e-01 -5.30326247e-01
 -8.17125738e-02 -3.18789512e-01 -6.44836053e-02 -9.28899348e-02
 -3.71216005e-03 -2.61058152e-01 -1.08654544e-01 -6.11086860e-02
 -5.28365493e-01 -5.48543692e-01 -2.37432837e-01 -1.90238595e-01
  3.16547044e-02  2.79459935e-02 -4.45771739e-02  1.08959988e-01
 -3.04580890e-02  1.03615120e-01  2.17506215e-01 -1.33862272e-01
  2.34874189e-01  1.55146169e-02  7.76626244e-02  9.10806358e-02
  4.28549945e-01 -2.46699661e-01 -2.60208696e-02 -1.18308976e-01
 -2.58038402e-01 -5.04059643e-02  9.76766497e-02 -1.31378040e-01
  2.13025630e-01  1.44640952e-01  4.19359386e-01 -2.56367475e-02
  9.83373672e-02 -2.92250246e-01 -2.82474995e-01 -3.06655377e-01
  3.81961614e-02 -7.53259584e-02  1.81822926e-01  1.91419855e-01
  4.60824668e-01 -3.06690812e-01  4.72040445e-01  1.16950184e-01
  1.14299059e-01 -6.04372025e-02  4.18711543e-01  9.46553648e-02
  5.18985614e-02 -8.30758661e-02 -1.19195566e-01 -1.52245805e-01
  1.68611512e-01 -4.66854095e-01 -4.42107171e-02 -1.05478927e-01
  1.38308525e-01 -7.00175539e-02  1.45874992e-01 -1.09290946e-02
  2.00325638e-01 -2.59877611e-02  6.30471557e-02  1.25223830e-01
  2.59151369e-01 -1.97926357e-01 -1.72611754e-02  1.12582900e-01
 -3.73751104e-01 -5.82170561e-02 -7.70717561e-02  6.67271376e-01
  2.12378189e-01 -1.65003821e-01  1.43917084e-01  1.70812368e-01
  8.35858583e-01  1.23952948e-01 -1.93332478e-01  9.82712209e-02
  1.24063648e-01 -6.01068363e-02  9.57745537e-02  1.23600349e-01
  2.19496697e-01  2.43484795e-01  3.66125144e-02  3.34916949e-01
 -8.27774107e-02 -3.23616922e-01 -1.65784255e-01 -2.44542569e-01
 -3.98114085e-01  2.96029270e-01  4.91039455e-03 -8.29918325e-01
  2.92513277e-02  1.56120330e-01  2.22526025e-03  3.33651364e-01
 -1.87375382e-01  1.24065235e-01  1.93383515e-01  1.19984739e-01
 -2.96666427e-03  3.31265926e-01  3.23952079e-01 -5.28558940e-02
  3.50386322e-01  4.65627797e-02 -6.44204617e-02 -4.37611729e-01
 -4.12541181e-02  4.00892854e-01 -1.24574624e-01 -1.87059015e-01
  5.24084941e-02  1.51846260e-01 -5.20923078e-01 -2.96989858e-01
 -2.53580287e-02  5.72173595e-01 -3.43268156e-01 -1.63484603e-01
  1.74180910e-01  3.41694318e-02  3.39065313e-01  9.24738571e-02
  1.16150454e-01 -7.72184253e-01 -7.26639628e-02  5.37797928e-01
 -9.65562761e-02  1.54364407e-01  2.35196114e-01  5.46769463e-02
  1.86642855e-01  1.39250368e-01  2.35746935e-01  3.30962777e-01
 -1.90628022e-01 -1.06554374e-01 -4.26306129e-01 -7.78479278e-02
  3.79838407e-01 -3.47924471e-01 -5.95962070e-03  3.88372466e-02
 -2.66321935e-02 -3.65501530e-02 -3.66972536e-02 -1.03506766e-01
  6.76477775e-02 -1.20843895e-01 -2.95526266e-01  1.48273334e-02
  2.57835299e-01 -1.13329716e-01  2.10594237e-02 -3.23696911e-01
 -2.73752749e-01  1.40960112e-01  1.46685354e-03 -4.93297964e-01
  7.46371448e-02 -7.08434880e-02 -2.88735151e-01  1.72473580e-01
  1.57353580e-01  1.39474750e-01 -3.64953399e-01  1.41774677e-02
  1.27717689e-01 -2.01604560e-01  1.59490317e-01 -6.03811502e-01
 -4.14768189e-01  2.36513972e-01 -4.32699353e-01  4.34729196e-02
  2.71996949e-04  1.44572541e-01  2.25021064e-01  1.96421649e-02
  4.21538085e-01  3.06553692e-01  2.99607158e-01 -2.84098983e-01
 -3.30833972e-01 -2.02108115e-01 -1.07770413e-01 -1.75511777e-01
 -3.02517116e-01 -1.11497410e-01 -1.75083075e-02 -7.82548413e-02
  2.15275913e-01  5.53135157e-01 -5.57262376e-02 -1.83984026e-01
 -2.86989689e-01  1.74568906e-01 -2.32488722e-01  2.02991426e-01
  4.98904169e-01  2.24369932e-02  4.15427685e-01  3.10875386e-01
  3.11271667e-01  7.80100450e-02  4.11654711e-01 -2.89340496e-01
  4.18361783e-01  2.78359175e-01  4.11819667e-04  3.77379119e-01
  2.26516306e-01  2.52534300e-01 -3.89071107e-01  7.47920871e-01
  9.98521596e-02  4.00430895e-02 -1.58205241e-01 -5.45741796e-01
  7.51652360e-01 -2.77973175e-01 -5.77088222e-02 -2.86632389e-01
  4.24441814e-01 -1.88907593e-01 -3.93855348e-02  8.44243020e-02
  1.37311861e-01  1.87657341e-01 -1.37571603e-01 -2.72266157e-02
 -6.81400448e-02 -3.16511899e-01  7.34447539e-02 -5.50957799e-01
 -2.06061795e-01  6.71484023e-02 -4.02901709e-01  4.76161450e-01
 -1.64895982e-01 -8.66361931e-02 -4.72241879e-01 -7.29608685e-02
  2.97364265e-01 -1.05770782e-01  2.63123721e-01  1.11363240e-01
 -3.11147362e-01 -1.10440761e-01  4.33867425e-01 -5.50183058e-01
 -2.99660414e-01 -4.13154699e-02  4.02305245e-01  3.00961196e-01
  3.74386907e-01 -5.29619217e-01  1.91669971e-01  1.21644586e-02
  1.31396381e-02  3.94444108e-01 -1.11338921e-01  2.46799052e-01
 -3.95022392e-01  5.44204712e-01  3.04042548e-01 -1.37184754e-01
  4.07643989e-02 -2.19647735e-01 -1.14376776e-01  5.07645383e-02
  2.61080980e-01 -2.32199490e-01 -2.14312598e-01 -8.69863927e-02
  5.58582731e-02  9.43358392e-02 -2.97955215e-01  3.97136033e-01
 -3.49039555e-01  1.24600239e-01 -2.25612104e-01  2.93075591e-02
 -2.88293898e-01  2.63416171e-01 -5.82814291e-02 -3.74700308e-01
  3.19751799e-02 -1.40208289e-01 -4.44704629e-02 -1.97099335e-02
  2.17865407e-03 -3.38559836e-01  4.74835157e-01  6.91624403e-01
 -2.06766933e-01  1.45327479e-01  5.17623387e-02  5.25322594e-02
 -5.31806231e-01 -4.43728967e-03  3.89391817e-02  1.50459379e-01
  2.11129665e-01 -3.42276990e-01  4.06653941e-01  1.41617268e-01
 -1.32986352e-01  3.14745903e-01 -2.26247609e-01  1.07134998e-01
  1.40467957e-01 -1.38811737e-01 -1.13794729e-01 -3.06259871e-01
  1.71925813e-01  3.62591743e-01 -4.24466550e-01  1.86733201e-01
 -4.70725358e-01  2.96608210e-01  6.43807769e-01 -4.27991986e-01
 -1.65850490e-01 -6.49308935e-02  4.32688683e-01  1.27606988e-01
  2.10711494e-01  4.86555062e-02  3.89761508e-01 -9.08663869e-03]"
Why support for Python 3.8 has been removed in Tensorflow 2.14 type:build/install type:support TF2.14,"Not an issue, but a question. Just out of curiosity, Why has support for Python 3.8 been removed in Tensorflow 2.14?",False,"[-2.03603283e-01 -5.52794516e-01 -4.20878738e-01  1.14364885e-01
  1.62765473e-01 -1.82680696e-01 -1.49471283e-01 -1.70514300e-01
 -3.19821507e-01 -5.99380657e-02  5.98516092e-02  3.50898981e-01
 -3.73950571e-01  2.75058866e-01 -5.18871658e-02  2.87032276e-01
 -7.91110769e-02 -4.04591620e-01 -2.24347815e-01 -3.17689441e-02
 -3.43311399e-01 -2.72520095e-01 -1.56380937e-01  2.30145827e-01
  2.76704639e-01 -1.93811327e-01 -1.44467592e-01 -1.13317467e-01
  1.90634400e-01  3.00397128e-01  1.36527628e-01  7.17136636e-02
 -1.05472617e-02 -1.22926287e-01 -1.30126281e-02  1.02197133e-01
 -2.19168887e-01 -2.50205785e-01 -1.63498074e-01 -6.03489438e-03
 -4.28164266e-02  1.96615346e-02 -9.47729349e-02 -3.37204970e-02
 -1.66458413e-01  2.18645066e-01  8.36351961e-02  5.35040833e-02
  3.94948348e-02 -1.65690243e-01  2.66461879e-01 -8.63858163e-02
 -7.54432827e-02 -4.14294899e-01  7.22681284e-02  7.06093982e-02
 -9.26512480e-02  3.24426919e-01 -8.08751136e-02  3.15591097e-02
  6.12773038e-02 -8.93987715e-02 -1.25668496e-02  3.57055031e-02
  3.70032862e-02  1.18669413e-01 -1.90674543e-01 -5.10447383e-01
  3.74960631e-01 -1.15398951e-01  2.47564569e-01 -1.71053186e-01
 -1.23042561e-01  9.00990423e-03 -2.40685970e-01  2.18566209e-02
  4.24428582e-01  1.09399296e-01  1.78964421e-01 -2.47186795e-01
 -8.57031420e-02 -2.35205755e-01  9.66683403e-02  5.27826324e-02
  8.73329192e-02 -9.97270122e-02 -8.55715573e-02 -2.23766640e-01
  3.75998653e-02  3.10121942e-02  5.80578744e-01  1.41314358e-01
  4.55048978e-02  2.09086910e-01  1.43160492e-01 -1.96871072e-01
 -8.01508352e-02  4.61444467e-01 -2.93345600e-01  6.87345043e-02
  2.19220340e-01 -1.94082931e-01  7.82553926e-02  2.85167992e-02
 -1.71547636e-01  7.08688572e-02  3.50228220e-01 -2.46548951e-01
  1.58309028e-01 -4.59549688e-02  3.82747278e-02 -3.02585632e-01
  2.88462579e-01  1.75442591e-01 -1.40767857e-01  9.18780863e-02
 -1.46190956e-01  1.65579185e-01  1.91013068e-01  7.55855739e-01
 -9.26155895e-02 -3.43498975e-01  3.59536529e-01 -2.58599162e-01
  9.43726525e-02 -2.58643895e-01 -1.77537799e-01 -1.73509881e-01
 -3.42552632e-01  1.00021489e-01  2.21633852e-01  2.88988948e-01
 -4.76550251e-01  1.60012916e-01  3.82250808e-02  1.82381608e-02
 -2.98795640e-01 -6.50043368e-01  1.28514960e-01 -3.92064273e-01
  1.45115867e-01  3.70422602e-01 -7.73766264e-02 -1.24347121e-01
  4.61815819e-02  4.08884846e-02 -9.38311294e-02  3.17250043e-01
 -8.18767250e-02  2.46269643e-01 -2.40321159e-01  1.41293079e-01
 -4.00783956e-01  1.24793006e-02  1.32471010e-01  1.94598153e-01
  3.03684682e-01 -3.19537520e-02 -3.10888171e-01 -3.84892076e-01
 -3.28039117e-02  1.36769533e-01 -4.39296544e-01 -8.53367522e-02
 -2.74914026e-01 -1.68734804e-01 -2.58985877e-01 -2.10445523e-01
  1.51073858e-01  2.65038908e-01  8.64382759e-02 -2.77529269e-01
  2.42032483e-01 -9.25514847e-02  2.13325530e-01 -1.59180328e-01
  1.53469995e-01 -4.61284757e-01  4.19449359e-01  2.16864079e-01
  1.31536409e-01 -1.87708810e-02 -2.26291530e-02 -7.62574794e-03
  5.57795353e-02  2.73729563e-01  4.01139446e-02 -5.58155822e-03
  1.72856852e-01  3.49310547e-01  8.82577971e-02 -2.16027170e-01
 -7.97343627e-02  1.87381983e-01  7.67041510e-03  4.89776060e-02
  1.15651019e-01  7.04014227e-02 -6.08477443e-02  8.00229311e-02
  6.46029552e-03  1.30285382e-01  8.68340656e-02  4.00961675e-02
 -1.64052725e-01 -3.83717239e-01  3.30113292e-01 -9.28453803e-02
 -3.25251967e-01 -2.12402746e-01  2.95518011e-01 -2.02397890e-02
 -1.59246564e-01 -4.16023552e-01 -3.77091676e-01 -6.97480366e-02
  4.73720789e-01  3.25502525e-03  3.24948221e-01  2.02282682e-01
 -1.77226767e-01 -1.12568513e-01 -1.35712609e-01  1.45149939e-02
 -1.82010561e-01 -3.23576815e-02 -1.59337744e-01 -1.39197055e-02
 -5.28449297e-01  2.27194130e-01  1.96242798e-02 -9.05856937e-02
  3.30145359e-01  7.81566650e-02  5.25726199e-01 -1.73179179e-01
  1.06602043e-01 -4.46955906e-03  6.89017177e-02  8.09339583e-02
 -5.62577426e-01  4.64959145e-02 -1.83547288e-01 -1.65073365e-01
  2.03543708e-01 -1.33329839e-01  1.45577802e-03 -1.39084607e-01
 -3.07920247e-01  3.02793533e-01 -3.03035229e-01  4.57207263e-02
  4.54798222e-01  2.16375783e-01  4.13779408e-01  8.98820683e-02
 -2.14986861e-01 -7.77828600e-03  1.78029478e-01  1.36070952e-01
  1.57073200e-01  2.23476559e-01  5.18907756e-02  3.59926015e-01
 -4.43725213e-02  2.06211299e-01  2.33680382e-01  5.79426706e-01
 -2.40966573e-01  2.14869931e-01  2.41241902e-01 -8.96241236e-03
  2.95977652e-01 -2.88921058e-01  4.17915210e-02  1.53803617e-01
  3.18513334e-01  3.44561726e-01 -1.50049701e-01 -3.04750919e-01
  1.87027887e-01  4.23211366e-01 -3.33533376e-01  1.54944360e-01
 -2.84427237e-02  8.84719566e-02 -1.86323479e-01 -2.68791705e-01
  1.61861330e-01  2.37931669e-01 -4.57646400e-02  9.17242542e-02
  2.15475917e-01 -1.51921511e-01 -1.63109794e-01 -1.81325719e-01
  5.09206206e-02 -7.76419863e-02  7.55911469e-02  5.41135907e-01
 -3.18292439e-01  8.30546170e-02  1.57405823e-01  6.53580502e-02
  1.75624967e-01 -3.49907309e-01 -3.75736915e-02 -1.68497220e-01
  8.16834792e-02 -3.97229373e-01  5.37768763e-04 -1.14319837e-02
  5.85762933e-02  1.25055194e-01  5.58703005e-01 -2.16494827e-03
 -1.79133028e-01  3.74418646e-01  2.83985287e-01 -2.15329975e-01
  6.73504621e-02 -3.04776616e-03 -3.10188353e-01 -7.04789385e-02
  2.36555189e-01 -1.68130875e-01  9.32953805e-02  4.50554751e-02
  1.48670506e-02 -7.26290271e-02 -1.24004021e-01 -1.06679760e-01
 -1.83934093e-01 -1.52642084e-02  2.51261145e-01 -1.63042173e-02
 -3.09704423e-01 -7.86869228e-03  3.06373741e-02 -1.48901507e-01
 -8.14048108e-03  1.23587981e-01 -7.95595050e-02 -1.31747574e-01
 -2.20186502e-01  2.05383733e-01  4.83641386e-01  2.43430182e-01
 -3.06118816e-01 -1.66596219e-01  6.39309287e-02  2.65762448e-01
 -2.82511532e-01 -1.97090462e-01  2.71177858e-01  1.31442308e-01
 -7.60073811e-02  6.70273528e-02  8.91716406e-02  2.27502465e-01
 -6.36267662e-02 -2.06395909e-02 -2.36819401e-01 -2.16207460e-01
  1.61703423e-01 -2.05014288e-01 -3.32779348e-01  2.21978858e-01
 -1.49444357e-01  2.22024217e-01 -6.55693263e-02  1.44064173e-01
 -1.87561616e-01 -5.59894776e-04  4.04431462e-01 -3.31688762e-01
 -4.16617274e-01  8.39903876e-02 -2.01123208e-01 -2.91617960e-01
  9.39418450e-02  1.88017353e-01  5.25879383e-01 -1.86140731e-01]"
can't convert keras model to tflite comp:lite TFLiteConverter TF2.14,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22
- TensorFlow installation (pip package or built from source): pip 
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.14.0 or tf-nightly

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```
import tensorflow as tf

keras_model = tf.keras.models.load_model(keras_model_filename)
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
tflite_model = converter.convert()

file = open(tflite_model_filename, 'wb
file.write(tflite_model)

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
![image](https://github.com/tensorflow/tensorflow/assets/68658008/dc88001c-02c2-42c6-8c74-8de447c5a776)
",False,"[-4.09181952e-01 -4.67791378e-01 -9.89502445e-02 -1.61683857e-02
  5.52944839e-04 -5.50760925e-02 -1.64762080e-01 -2.01945662e-01
 -1.83812812e-01 -9.28597450e-02 -7.77366161e-02  1.83095232e-01
 -2.77397990e-01  2.15069771e-01 -1.81977391e-01  1.28209606e-01
 -1.30887538e-01 -2.62780070e-01  2.59419680e-01  5.63898422e-02
  8.91789645e-02 -5.14397770e-02 -2.57648766e-01  2.89893091e-01
  1.75471634e-01  1.81694999e-01 -8.09118748e-02  3.06092724e-02
  1.69503503e-02  6.39061108e-02  1.13655373e-01  1.39506578e-01
 -2.94114769e-01 -4.59221490e-02  2.06727237e-02  9.87424850e-02
 -1.22537084e-01 -1.00964725e-01 -2.69283772e-01 -2.58732259e-01
  2.69478023e-01  7.86121264e-02 -6.93493709e-02 -1.17682919e-01
 -1.05083838e-01 -1.11741096e-01  2.27754891e-01 -1.56901315e-01
 -2.99411654e-01 -1.20449938e-01 -4.20739017e-02 -6.23082072e-02
 -1.79232836e-01 -2.04942048e-01  1.08684674e-01 -1.32699251e-01
  2.34333739e-01  4.12366539e-02  2.48131854e-03 -1.08031869e-01
 -8.69159102e-02  2.24530697e-02 -1.21984243e-01  9.44030136e-02
  2.15397447e-01  2.55117059e-01 -5.23848925e-03  1.73357967e-03
  1.71557128e-01 -1.67953044e-01 -2.22506642e-01 -1.73071891e-01
 -6.38793334e-02  2.68096849e-03 -5.85868955e-02  4.60581183e-02
  4.29969653e-02  2.99803495e-01  1.96089506e-01  3.84060815e-02
  1.69897407e-01 -2.23275810e-01 -1.25808984e-01 -5.70148043e-03
 -8.32406729e-02  9.19026975e-03  8.98611695e-02  7.90716037e-02
  3.67089629e-01 -2.98598289e-01  2.22410023e-01  4.91486520e-01
  1.50872141e-01  1.50233805e-01  2.98574984e-01  1.56459540e-01
  1.47099495e-01  1.32519156e-01  1.15264386e-01 -3.57378833e-03
 -9.40512270e-02 -2.76175380e-01 -1.56696558e-01 -1.25627428e-01
  3.81463673e-03 -8.91925171e-02  2.43986964e-01  1.76884122e-02
  1.15181193e-01 -1.00738436e-01  1.80871144e-01 -9.07787308e-03
  1.10440478e-01 -1.11053795e-01  4.82612811e-02 -1.42825954e-02
  1.17062196e-01  1.57034814e-01  1.62965447e-01  4.18585539e-01
 -1.15530476e-01 -3.26713324e-01  1.79078192e-01  1.80242151e-01
  3.04183006e-01  2.41818756e-01 -2.31487989e-01  8.42938293e-03
  4.04106341e-02  2.72279680e-01 -5.01628965e-02  2.84223676e-01
 -8.29435736e-02  1.07564628e-01  2.03139007e-01  1.47086922e-02
 -2.10652307e-01 -1.00509867e-01 -2.03828305e-01 -8.55334997e-02
 -2.20316842e-01  1.34570539e-01 -7.94603229e-02 -1.63943142e-01
 -4.07650694e-02  8.26456677e-03 -1.01663396e-01  1.01700038e-01
 -6.70380369e-02  1.88556924e-01 -1.42659768e-01  7.18534365e-02
  1.11510009e-01  2.36106604e-01  1.57841980e-01 -5.19469976e-02
  1.36838183e-01 -8.93136039e-02  1.35807008e-01 -1.61729291e-01
 -1.56051472e-01  1.71363026e-01 -1.42148450e-01 -1.75460488e-01
  2.41661277e-02  1.12475097e-01 -4.96727616e-01 -2.70436823e-01
  2.74058372e-01  6.84604123e-02  5.36075234e-03 -1.84498280e-01
  1.62789837e-01  1.15390085e-02  3.18115205e-01 -4.86578941e-02
  4.65854645e-01 -3.71532708e-01 -1.13569915e-01  1.34995103e-01
  1.62545055e-01  8.63486528e-02  9.88606215e-02 -3.10324803e-02
 -4.07924019e-02 -7.59465843e-02  4.30102289e-01  2.60114186e-02
 -2.83149779e-01 -9.81756449e-02 -4.09471333e-01 -2.23946676e-01
  3.59115392e-01 -7.00067207e-02 -2.51414895e-01  2.19594501e-02
  1.39992505e-01  6.49348646e-02 -1.37634620e-01 -2.45642383e-02
  4.91298214e-02  4.01924960e-02 -2.90144291e-02 -7.93270618e-02
 -5.37593924e-02 -1.30400836e-01 -1.52683109e-02 -1.93752959e-01
 -5.55097938e-01  1.15875043e-01  2.97509462e-01 -2.96089917e-01
  9.15637761e-02 -1.57848001e-01 -1.11289397e-01  1.14720628e-01
 -9.54144299e-02 -1.55898184e-01 -5.08192703e-02  2.35090464e-01
  1.02059498e-01 -2.30673812e-02  1.59518242e-01 -1.80247709e-01
 -3.33924472e-01 -1.22279692e-02 -3.86834741e-01  3.48923057e-01
 -4.51361015e-02  9.40759256e-02 -2.11386517e-01 -1.18570998e-02
  5.75650096e-01  2.03082144e-01  2.40802437e-01 -1.88917905e-01
  1.78461730e-01 -1.96470976e-01 -2.64867395e-01 -7.67954066e-02
 -2.34737992e-01 -1.70485318e-01  1.26634106e-01 -2.50940621e-01
 -1.27846345e-01  2.47531384e-01 -1.09118119e-01 -1.10054709e-01
 -1.75232381e-01  1.81595281e-01 -9.93627310e-03 -1.99594349e-01
  1.44941121e-01  2.87485719e-01  2.61612475e-01  1.39367372e-01
 -3.51647064e-02 -1.29413009e-02  1.19407557e-01  3.79180349e-02
  1.59129798e-01  3.63369852e-01 -4.25005052e-03  5.76152205e-01
  2.87769735e-01  1.99801371e-01 -1.22384176e-01  1.71648681e-01
 -3.44057113e-01 -2.08465815e-01  1.26184493e-01 -2.75400758e-01
  3.23341697e-01 -1.69689700e-01  3.05813819e-01 -6.56864643e-02
  2.23744422e-01 -1.81762367e-01  7.52419978e-02  1.20320991e-01
  2.36673176e-01  3.26732397e-01 -5.36227584e-01  4.02604751e-02
  1.32952645e-01 -8.20342526e-02 -1.71366669e-02 -5.80891848e-01
 -5.67614101e-02  6.59269989e-02 -2.44232655e-01  1.03892997e-01
  4.08489406e-02  3.65191102e-02 -1.05912998e-01 -1.40880555e-01
  1.81631446e-01  2.46507917e-02  1.80348396e-01 -2.12541178e-01
 -1.67904735e-01  2.25397900e-01  2.65210509e-01 -3.06918830e-01
 -4.49168533e-02 -1.26772895e-02  2.52761960e-01  1.88230537e-02
  4.55844253e-01 -3.81138653e-01  1.37850210e-01  8.06860179e-02
  1.13617674e-01  2.96751738e-01  7.17515200e-02  5.33650406e-02
 -2.76187301e-01  2.56347239e-01  6.21297881e-02  2.17392296e-02
  2.01210767e-01  1.25374598e-02 -4.33454871e-01  7.52882585e-02
 -4.41125259e-02  1.63365185e-01  8.03140551e-02 -1.54340833e-01
 -7.50077665e-02  1.07140377e-01 -1.41186357e-01 -2.01028675e-01
 -8.39802921e-02  3.06522787e-01 -1.81458712e-01 -1.15286885e-02
 -1.03456110e-01  2.90544957e-01 -1.66556746e-01 -3.13150167e-01
 -8.12169760e-02 -1.78359255e-01 -7.44131655e-02 -2.04159319e-01
 -1.99906360e-02 -2.23031148e-01  4.84110154e-02  9.95088369e-02
  1.25797510e-01  6.30203560e-02 -1.89493805e-01 -4.30865884e-02
 -2.36930430e-01  1.69975430e-01 -6.39400072e-03  3.02352369e-01
 -2.76202351e-01 -7.63400644e-02  1.23102829e-01  4.88819063e-01
  4.30165157e-02 -2.07880512e-03 -3.30781877e-01 -1.74894631e-01
  4.50775400e-03  3.92171293e-02 -2.18067661e-01 -2.35576779e-01
  1.04742497e-03  6.68677330e-01  5.30962497e-02  3.82496774e-01
 -1.10956594e-01  7.64116496e-02  3.33743334e-01 -3.15055847e-01
  4.38966788e-03  1.54623806e-01  8.11657757e-02 -1.89242974e-01
 -7.97000676e-02  1.28677309e-01  1.38021242e-02 -2.35549212e-02]"
con't build_pip_package stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 1.14,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf1.14.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.6.9

### Bazel version

bazel 0.24.1

### GCC/compiler version

gcc 4.8

### CUDA/cuDNN version

_No response_

### GPU model and memory

 intel i7-9750H   12g

### Current behavior?

tensorflow/python/lib/core/bfloat16.cc:608:60: note: no known conversion for argument 2 from '<unresolved overloaded function type>' to 'PyUFuncGenericFunction {aka void ()(char**, const long int, const long int*, void*)}'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 4163.570s, Critical Path: 161.91s
INFO: 5035 processes: 5035 local.
FAILED: Build did NOT complete successfully

### Standalone code to reproduce the issue

```shell
bazel build --jobs=6 //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_",False,"[-6.33235276e-01 -3.34880829e-01 -8.98073241e-02  1.51807427e-01
  2.21773565e-01 -3.27260673e-01 -2.76194453e-01  1.74748838e-01
 -3.01840395e-01 -2.56366253e-01 -5.54594025e-02  1.22568244e-02
 -1.91728711e-01  1.38470232e-01 -3.80101889e-01  4.00773317e-01
 -2.43838266e-01 -2.52518326e-01  2.78464496e-01  2.12372333e-01
 -2.60636032e-01  1.22228593e-01 -3.39003623e-01  3.91028762e-01
  3.35758150e-01  1.11978471e-01 -3.43488932e-01  2.43183255e-01
 -3.88026610e-03  7.60215819e-02  3.89381886e-01  1.77881882e-01
 -1.08526766e-01  1.65741831e-01  2.45513245e-01  2.52475500e-01
 -3.99776623e-02 -1.90912932e-01 -3.31665039e-01 -1.50390625e-01
  5.56060448e-02  1.62717439e-02  5.36656454e-02 -1.60026565e-01
  7.96364807e-03 -3.32760721e-01  1.12217236e-02 -2.85983503e-01
 -1.31577462e-01 -1.64291054e-01 -3.35855305e-01  1.49639696e-01
 -3.96392107e-01 -3.10402662e-01  8.60924739e-03 -3.19006324e-01
  1.62563205e-01  1.48862600e-01 -3.80477384e-02  1.99853946e-02
  5.42649440e-02  1.48970157e-01  1.25970826e-01  1.51620835e-01
  6.34699687e-02  3.58060271e-01  2.31583357e-01 -8.91162455e-02
  3.55237305e-01 -2.11358368e-01  1.09614834e-01 -1.50745064e-01
 -4.46154714e-01  1.20346688e-01  2.61373837e-02  1.03391841e-01
 -8.97017568e-02  2.54795820e-01  1.53274417e-01 -7.59060830e-02
 -1.52072757e-01 -3.54589760e-01 -1.02715604e-01 -1.04536906e-01
 -6.29024059e-02  3.37862782e-02  2.72000670e-01  1.82007119e-01
  5.28726220e-01 -3.43415797e-01  2.97699600e-01  5.86688399e-01
  9.16999280e-02  1.20414428e-01  3.88537765e-01  2.18139105e-02
  2.22249284e-01  3.60903412e-01 -1.13244191e-01 -5.81064448e-02
  6.31840155e-03 -2.49232531e-01 -1.38325140e-01 -2.00404018e-01
  4.01548520e-02 -9.37238485e-02  3.82934034e-01 -1.15916677e-01
  1.45919891e-02 -1.23790383e-01  1.65054113e-01  5.96342608e-02
  1.69488236e-01 -1.81811765e-01 -5.40188551e-02 -1.11541279e-01
 -1.67224184e-01 -1.18208919e-02 -1.03261396e-02  8.33658576e-01
  1.06115356e-01 -1.26348576e-02 -1.38786405e-01  1.02037281e-01
  4.50197220e-01  1.20872200e-01  1.92725569e-01  2.25586258e-02
  2.35240594e-01  9.37603191e-02  1.65010527e-01  2.16903478e-01
 -1.17647313e-02  1.99459389e-01  2.43688852e-01  1.06967986e-01
 -1.46852091e-01 -8.30870494e-02 -1.61983877e-01 -1.73474491e-01
 -3.31092507e-01  3.63695860e-01 -1.98627114e-01 -5.57575345e-01
 -1.52410157e-02  1.00147799e-01 -1.44330710e-01  2.43430868e-01
 -2.85390705e-01  1.75929010e-01 -7.33944252e-02 -4.73326258e-03
 -1.98345304e-01  4.03909713e-01  2.01845676e-01  6.97056800e-02
  8.05982873e-02  4.31743637e-02  4.57132682e-02 -4.08431321e-01
 -9.34736952e-02  4.45437968e-01 -2.64978945e-01 -2.24558756e-01
 -9.16685760e-02  2.81485498e-01 -6.15438879e-01 -3.00937653e-01
  3.35863590e-01  3.74435812e-01 -1.83700904e-01 -1.56386182e-01
  2.24136636e-01  6.26395792e-02  2.96507478e-01 -1.35396153e-01
  3.95076036e-01 -2.82656252e-01  3.52192596e-02  4.66595262e-01
  2.74759144e-01  2.80405700e-01 -1.10078216e-01  9.39951837e-02
  7.30880722e-03 -1.87822655e-02  4.09893468e-02 -5.63142821e-03
 -2.20463708e-01  6.52217492e-02 -4.10570502e-01 -2.74233520e-04
  5.28799176e-01 -1.79407299e-01 -2.67231792e-01  1.85209215e-01
  4.40418124e-01  3.05140615e-02  1.91604823e-01 -8.07681903e-02
 -1.02302358e-01  1.97363868e-02 -1.34596407e-01  2.34502871e-02
  7.47540295e-02 -2.82346725e-01  1.75597444e-01 -3.73598874e-01
 -4.62105244e-01  5.19183576e-02 -4.15192880e-02 -3.01957130e-01
  2.34429285e-01 -3.80437672e-01 -2.04895325e-02  4.16604429e-01
 -4.15325575e-02 -4.90471534e-02 -5.88154830e-02  7.88588747e-02
  2.16306187e-02 -3.20268989e-01  1.69239163e-01 -4.31501210e-01
 -4.65399772e-02  1.15857273e-01 -3.70403469e-01  1.35885894e-01
 -1.07273608e-01  8.96023437e-02  6.93436787e-02  4.55275476e-02
  6.02917433e-01  4.49104339e-01  4.64385837e-01 -1.62287027e-01
  2.84965783e-02 -1.45716980e-01 -8.13035965e-02  1.89441845e-01
 -3.22085589e-01 -1.63576961e-01  1.64290071e-01 -1.72371976e-02
  1.12110272e-01  3.16892326e-01 -2.15905905e-01 -1.06756732e-01
 -5.26603103e-01  3.92213672e-01 -2.84687340e-01  6.03940040e-02
  1.31504297e-01  1.03105180e-01  5.28081775e-01  3.00291479e-01
 -2.15156339e-02  1.79805040e-01  1.34831101e-01 -2.82980770e-01
  1.38203025e-01  4.02261585e-01  1.24555752e-01  1.17875844e-01
  1.56121790e-01  1.42055988e-01 -4.03743476e-01  5.48401773e-01
 -6.09305352e-02 -3.13228756e-01  2.90348560e-01 -4.28320408e-01
  6.24038279e-01 -2.41749316e-01 -2.97248177e-03  1.09411150e-01
  3.23602855e-01  2.99220048e-02  1.52219729e-02  1.71486229e-01
 -1.27301574e-01  4.21837926e-01 -5.98246753e-01  1.84206098e-01
  8.56726766e-02 -1.81835033e-02 -1.81900501e-01 -6.43732250e-01
 -2.66325176e-01  1.23522229e-01 -3.19509178e-01  1.01405524e-01
 -2.56737679e-01 -7.34294057e-02 -7.27336854e-02  2.31636688e-04
  3.48406509e-02 -7.15723187e-02  2.13342145e-01  7.79382810e-02
 -1.91953272e-01  8.33966956e-02  3.66897494e-01 -1.49641484e-01
 -2.38708317e-01 -6.31194338e-02  7.54486099e-02  3.24028462e-01
  5.36456466e-01 -4.74951923e-01  1.83858156e-01  4.82656993e-02
 -4.18605432e-02  4.82535422e-01  1.10754386e-01  1.48939773e-01
 -6.07551277e-01  4.65906292e-01  1.33705765e-01 -2.22039565e-01
  3.67131054e-01 -2.40765452e-01 -6.06160760e-01  1.53027713e-01
  1.25236571e-01 -5.75661249e-02 -1.83984116e-01 -3.54882032e-01
 -1.67796358e-01  3.20630848e-01  5.54082543e-02 -2.77628750e-03
 -1.24809682e-01  1.30084231e-01 -3.54337245e-01 -1.26781031e-01
 -3.92422318e-01  1.36337280e-01 -1.68819547e-01 -3.89150381e-01
 -1.03978857e-01 -2.77883291e-01  3.01268529e-02 -4.33176279e-01
 -1.30568534e-01 -4.35581386e-01  3.53022635e-01  2.03680396e-01
 -1.18391767e-01  1.13760822e-01  8.16497877e-02  1.11729711e-01
 -6.25043988e-01  1.98874041e-01  5.53437360e-02  2.70058662e-01
  1.43530220e-01 -3.22661996e-02  4.52179134e-01  2.35213488e-01
 -2.29058772e-01  4.89821322e-02 -4.57849681e-01  9.36868694e-03
  1.15925655e-01 -1.73765764e-01 -3.74063283e-01 -1.01657897e-01
 -1.38010621e-01  3.08744282e-01  4.24484834e-02  2.66435236e-01
 -3.78528565e-01  2.97119051e-01  5.27465045e-01 -4.83622909e-01
 -2.79085904e-01  2.78110802e-01  8.18521827e-02 -2.35171199e-01
  6.21808767e-02  6.86173365e-02  3.85899603e-01 -3.70987616e-02]"
About int16 stat:awaiting response type:support stale TFLiteConverter,"### 1. System information

- Linux Ubuntu 16.04
- TensorFlow lite
### 2. Issue

I would like to inquire if my input and output data can only be int16. Can I write ""converter. reference_input_type=tf. int16"" when converting tflite or is there any other good method recommended?

Thanks!


",False,"[-3.32172155e-01 -3.32193434e-01 -3.12704533e-01 -1.92645460e-01
 -1.58200916e-02 -1.82052001e-01 -5.22009470e-02  2.90904343e-01
 -2.20329806e-01 -2.76466131e-01 -1.27184130e-02  6.20372891e-02
 -2.40160450e-02  1.52943218e-02 -1.39870137e-01  2.21150503e-01
 -1.86846361e-01  1.57326292e-02  3.09326261e-01 -8.11943039e-02
 -3.00582290e-01  1.97287455e-01 -1.21643141e-01  2.59973705e-01
  2.61067182e-01  1.64678708e-01  1.80705339e-01  1.20680347e-01
 -1.27556652e-01  3.74525845e-01 -6.00028746e-02  1.57546595e-01
 -9.30915326e-02  8.94255340e-02  1.31639600e-01 -1.69360995e-01
 -1.94150209e-01 -1.04607761e-01 -2.84266114e-01 -9.76308435e-02
  9.80116501e-02 -4.89881635e-02 -5.08171506e-04  9.38834250e-03
 -5.74259227e-03  5.58816046e-02  6.26600981e-02  1.31854504e-01
 -2.29952365e-01 -1.70180663e-01  2.54945844e-01 -2.68345512e-02
 -2.16338247e-01  2.23456859e-03 -8.30284730e-02  1.27082109e-01
  9.46981758e-02 -6.12326227e-02  5.31922616e-02 -1.14141867e-01
  7.31773749e-02 -2.65547425e-01  2.39501908e-01 -4.30086643e-01
 -1.36455268e-01  1.93047091e-01  9.99037474e-02  8.38631913e-02
  2.75663078e-01 -4.11699206e-01 -1.87690467e-01 -4.03787680e-02
 -2.50862509e-01 -2.83982027e-02 -9.37114358e-02 -8.15215334e-02
 -2.16802388e-01  2.27723747e-01  2.33995214e-01 -4.24072593e-02
 -7.40727559e-02 -4.26609442e-02 -7.48277828e-02 -9.53662489e-03
  4.68475342e-01  4.21945825e-02  3.35317016e-01  8.19445252e-02
  6.05476573e-02  9.57164764e-02  2.73884118e-01  4.51870412e-01
  1.27494246e-01  1.66806892e-01  2.36403123e-01 -1.31351808e-02
  1.23102002e-01 -2.08602380e-02 -2.84968857e-02 -2.28641182e-01
 -1.07843161e-01  2.04293919e-03 -1.89354733e-01  2.03544237e-02
 -5.03050536e-02 -1.50219366e-01  1.56316489e-01  2.79977828e-01
 -9.70034450e-02 -2.01140299e-01  3.04269969e-01 -1.18589953e-01
  2.20632002e-01 -5.34730381e-04  3.78451720e-02 -4.32384424e-02
  3.25556323e-02  9.64455828e-02  9.98394340e-02  4.47912544e-01
  1.14430778e-01 -5.52185662e-02 -1.20232128e-01  4.36685860e-01
  1.97237492e-01 -2.93188244e-02 -4.07525003e-01 -1.09490920e-02
  8.47964436e-02  3.58702511e-01  2.74937093e-01  1.15429694e-02
 -3.61168236e-01  1.54534176e-01 -7.13568479e-02  1.29530817e-01
 -5.08561321e-02 -1.60816237e-01 -4.65082675e-01 -3.86675410e-02
  8.12565908e-04 -1.73361674e-01 -5.43336570e-03 -3.29234630e-01
 -3.84732522e-02  4.49544117e-02 -3.11648160e-01  2.53874898e-01
  3.89107838e-02  1.80806264e-01  9.51494947e-02 -7.38624036e-02
 -1.56416390e-02  3.30540121e-01 -5.26690371e-02 -2.70077199e-01
  3.65178525e-01 -2.17840061e-01  2.00415209e-01 -4.26435798e-01
 -6.18747212e-02  3.18313599e-01  2.00345784e-01 -3.59998792e-01
  4.19016927e-01  8.41145664e-02 -4.27284926e-01 -1.71634749e-01
  9.31461900e-02  3.14080387e-01  9.65752527e-02  1.44477785e-02
  1.76456526e-01  4.12711781e-03  3.90444547e-01  1.11057080e-01
  5.69570422e-01 -3.27517927e-01 -2.13265568e-01  1.07058147e-02
 -3.48314494e-02  8.28545466e-02  2.64508784e-01 -1.62514433e-01
 -5.62938564e-02  5.34380823e-02 -6.20091036e-02 -1.07812218e-01
 -5.38765788e-01  3.30828577e-02 -1.33386001e-01 -1.54096976e-01
 -7.79862702e-02  1.23642117e-01 -1.46779150e-01 -3.04328829e-01
 -1.40790358e-01 -9.53006521e-02  4.91053499e-02  5.98635189e-02
 -4.59084153e-01 -7.71431625e-03  1.38020635e-01 -6.91580921e-02
  2.49656811e-01 -1.28327891e-01  5.52031095e-04 -4.71555352e-01
 -1.84342712e-01 -5.52892983e-02  1.32505342e-01 -3.00644189e-01
  9.23629627e-02 -1.57567680e-01 -3.13967913e-01 -3.43114212e-02
  1.02378711e-01 -1.73045173e-01 -3.30482200e-02 -8.41885954e-02
 -6.33610561e-02  4.56387177e-02  1.58811852e-01 -6.49740696e-02
 -3.42291832e-01  2.14901775e-01 -2.42802560e-01  2.90312946e-01
 -4.47537787e-02  1.34473041e-01 -5.31123243e-02 -1.66821241e-01
  2.91069180e-01 -1.42113836e-02  1.98163256e-01  5.12130149e-02
 -6.59146830e-02 -1.87617958e-01 -2.10798457e-01 -3.36868167e-02
 -3.34120750e-01 -1.30524755e-01 -1.63260892e-01 -2.85070181e-01
 -9.01852995e-02  9.67505202e-02  1.85832262e-01 -3.50385636e-01
 -2.19417125e-01  1.44055247e-01 -1.38691097e-01 -1.37728043e-02
  2.45317400e-01  1.48373544e-01  3.19868535e-01  2.04704344e-01
  4.05632317e-01  1.21888414e-01 -2.39648491e-01 -1.06459148e-01
  2.84233630e-01  1.90955475e-01  2.16550142e-01  4.54654276e-01
  2.68660784e-01  1.22970186e-01  6.05243491e-03  2.11744070e-01
 -3.93400222e-01 -1.13102868e-01 -6.61019981e-02 -4.98587918e-03
  3.06896240e-01 -1.76872820e-01  3.84043276e-01 -1.72061235e-01
  4.03704643e-01  1.33996472e-01  1.79186523e-01  1.38988495e-01
  2.07385644e-01  2.54643232e-01 -3.52946222e-01  1.03764847e-01
  3.04934353e-01 -1.99479848e-01 -1.44415267e-03 -3.52397233e-01
  4.99106050e-02 -1.48719355e-01 -2.74788350e-01 -1.68088917e-02
  9.72775370e-02 -2.33935844e-02 -2.31497794e-01  3.48347351e-02
  2.71140575e-01 -4.61801887e-02 -1.48195326e-02 -2.64560163e-01
 -3.82156968e-01  9.82815996e-02  2.62503982e-01 -2.67388850e-01
  2.87243631e-02 -7.24885007e-03  1.27411395e-01  6.04001321e-02
  2.61112303e-01 -3.41540813e-01  1.76319316e-01  6.15743175e-02
 -6.84284717e-02  3.25417936e-01 -1.04615483e-02 -3.19483853e-03
 -2.01775208e-01  2.08243221e-01 -1.01193592e-01  8.36528018e-02
  1.26852587e-01  4.50939313e-02 -9.60839465e-02  7.55730048e-02
  1.10412560e-01  1.41455065e-02  6.00175485e-02 -1.51280701e-01
  1.46593958e-01  1.33086920e-01 -8.21639299e-02 -8.54059011e-02
  1.37697935e-01 -2.16996491e-01  3.17786857e-02 -3.03002298e-01
  8.59734192e-02  4.02175123e-03 -3.14842947e-02 -1.09321242e-02
 -9.56435427e-02  3.50162573e-02 -4.08514202e-01 -1.22361019e-01
  2.14861572e-01 -3.02821159e-01  2.27541745e-01  6.43034101e-01
  1.48433316e-02 -8.80500078e-02 -1.93392843e-01  2.13372689e-02
  1.49275154e-01 -1.53791472e-01 -4.86137658e-01  5.00824332e-01
  7.67482221e-02 -8.55650567e-03  7.38731697e-02  9.18918774e-02
 -4.59944345e-02  1.24321297e-01 -6.32277131e-01 -1.74184710e-01
  1.63621232e-02 -1.01092719e-01 -3.75603177e-02 -2.34807491e-01
  7.69604221e-02  3.95705730e-01 -1.23448230e-01  2.95115530e-01
 -9.07570869e-02  3.06254644e-02  1.70424163e-01 -2.06233263e-01
 -5.37304170e-02  1.18861094e-01  4.47331145e-02 -5.90269119e-02
 -8.60360786e-02 -5.06517775e-02 -7.18897134e-02  3.73137817e-02]"
Could not find a version that satisfies the requirement tensorflow-macos==2.14.0; stat:awaiting response type:build/install subtype:macOS TF2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

No

### OS platform and distribution

mac os 13.5.1

### Mobile device

Apple silicon

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Collecting tensorflow==2.14.0
  Obtaining dependency information for tensorflow==2.14.0 from https://files.pythonhosted.org/packages/de/ea/90267db2c02fb61f4d03b9645c7446d3cbca6d5c08522e889535c88edfcd/tensorflow-2.14.0-cp311-cp311-macosx_12_0_arm64.whl.metadata
  Using cached tensorflow-2.14.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)
INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.
ERROR: Could not find a version that satisfies the requirement tensorflow-macos==2.14.0; platform_system == ""Darwin"" and platform_machine == ""arm64"" (from tensorflow) (from versions: 2.12.0, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.14.0rc0, 2.14.0rc1)
ERROR: No matching distribution found for tensorflow-macos==2.14.0; platform_system == ""Darwin"" and platform_machine == ""arm64""

### Standalone code to reproduce the issue

```shell
pip install tensorflow==2.14.0
```


### Relevant log output

_No response_",False,"[-3.51052850e-01 -5.48369646e-01  1.94678321e-01 -1.75202228e-02
  2.72426724e-01 -1.78520769e-01 -2.45754927e-01 -2.77587809e-02
 -4.20700967e-01 -3.96705538e-01  1.68310612e-01 -1.19020119e-01
 -3.04370731e-01  1.26012504e-01 -2.01089397e-01  2.51220286e-01
 -1.35099173e-01 -2.23487720e-01  2.99316555e-01  1.92340761e-01
 -1.20809197e-01 -1.98295861e-01 -1.81298360e-01  3.86726670e-02
  1.12164140e-01  1.36506990e-01 -7.78757334e-02  6.65295869e-02
  4.82200906e-02  1.86565593e-01  4.61557508e-01  1.87618628e-01
  2.07078252e-02  2.39202045e-02  2.20440537e-01  1.46972418e-01
 -6.29112124e-03 -2.87605822e-01 -4.38331246e-01  8.88193399e-03
 -1.37470171e-01  9.57490578e-02  3.16145629e-01 -5.24662398e-02
  4.78656553e-02 -2.18830079e-01  1.99343730e-02 -1.83878824e-01
  9.85748470e-02 -1.70290276e-01  1.20921738e-01 -1.42338544e-01
 -1.62835881e-01 -3.10990840e-01 -9.16282088e-02  2.52406389e-01
 -3.73193659e-02  5.72262257e-02  2.13927299e-01  2.51385003e-01
  1.60889804e-01  9.49030668e-02 -1.05757341e-02 -2.58151591e-02
  3.04884672e-01  5.27691729e-02  2.38478333e-01 -2.74638236e-01
  5.94148755e-01  3.88418045e-03  1.26439586e-01 -8.46861675e-02
 -2.17638850e-01  1.84584349e-01 -7.13107437e-02  2.05075175e-01
  9.53203365e-02 -1.15804821e-02  1.74171060e-01  2.12595705e-02
 -1.42789513e-01 -1.20557450e-01  1.55595332e-01 -7.37385405e-03
  2.01226622e-01 -2.10136130e-01  2.55453140e-01  8.87796134e-02
  1.57533005e-01 -1.72411129e-01  4.48747933e-01  2.58824557e-01
  2.20770240e-02 -2.49016523e-01  4.01727289e-01 -1.11294761e-01
  9.54357162e-02  3.80541533e-01  6.18703216e-02  7.18729272e-02
 -1.31320983e-01 -1.13526613e-01  2.01989457e-01  1.08610205e-02
 -1.31174266e-01 -2.16041207e-01  2.40538582e-01 -3.73184800e-01
  1.25676319e-01  3.16112004e-02  2.06826329e-01  6.32562339e-02
  9.68927816e-02  1.61920607e-01  3.83533537e-04 -2.02452511e-01
 -3.78808856e-01  3.43195274e-02  1.80364653e-01  8.05581331e-01
  1.26168177e-01 -4.04423416e-01 -8.96742754e-03  1.35416672e-01
  3.27921420e-01  1.42180055e-01 -3.61540973e-01 -1.84209719e-02
 -5.03929593e-02 -1.99225605e-01  1.04758650e-01 -4.01243418e-02
  2.26012021e-01  1.54487237e-01 -8.83045048e-02  2.20159754e-01
 -8.97150487e-02 -2.25599706e-01 -5.12479901e-01 -1.43456504e-01
 -2.24604145e-01  2.74524420e-01  3.51326307e-03 -4.92374301e-01
  2.32510462e-01 -5.73146604e-02 -1.67439803e-01  2.28795618e-01
 -1.49780229e-01 -3.36052597e-01 -2.96417717e-02 -9.31695849e-02
  9.31718200e-02  2.59762436e-01  3.17061305e-01  2.07879990e-01
  4.76820141e-01 -8.59849229e-02 -3.55188251e-01 -5.56123018e-01
  1.34008780e-01  2.93834656e-01 -1.63880736e-01 -1.17608927e-01
  1.56218112e-01  1.50124207e-02 -2.97749519e-01 -2.06857905e-01
  5.77951968e-02  4.34607148e-01 -4.78427261e-02 -1.85436308e-01
  1.01670690e-01  1.13654837e-01 -7.68450797e-02 -2.78567106e-01
  3.92591000e-01 -5.88113308e-01 -1.91371948e-01  1.59470588e-01
  2.90508661e-03 -2.12475374e-01  1.65974945e-01  4.47807908e-02
 -8.94236285e-03  1.01835668e-01  6.85123801e-02  2.65254736e-01
 -2.74638891e-01  1.91339970e-01 -4.01432991e-01 -9.82356817e-03
  3.74766111e-01  3.82193998e-02  5.75553589e-02 -1.79719441e-02
  6.10332154e-02 -9.84656811e-02 -2.98709959e-01  2.80682296e-01
 -2.62371570e-01 -6.03166297e-02 -8.12757537e-02 -8.47871900e-02
  3.13054658e-02 -3.32656562e-01 -1.41007304e-01 -2.05694422e-01
 -5.08367896e-01 -1.99664645e-02  2.45081544e-01 -2.97984451e-01
  2.25534797e-01  1.84149683e-01 -2.30893850e-01  2.09410220e-01
  1.90807357e-01  8.02489817e-02 -2.98539817e-01  2.49885052e-01
 -2.58043587e-01 -2.80901790e-01 -2.44319022e-01 -1.87403798e-01
 -3.22080910e-01  6.13216646e-02 -2.32866466e-01  4.14569676e-03
 -1.53559968e-01  3.27564180e-01  2.40662061e-02  1.05474345e-01
  3.81258249e-01 -2.32453439e-02  3.55918050e-01 -1.43146053e-01
  3.43577266e-02 -2.96813667e-01 -2.02682108e-01  1.59119949e-01
 -7.00950444e-01  5.60997725e-02  3.01267952e-03  2.13051829e-02
  2.85767823e-01  3.29570353e-01  2.46679448e-02 -8.98539573e-02
 -2.22602442e-01  2.14679956e-01 -2.28979886e-01  2.50272870e-01
  2.24754766e-01  2.97869623e-01  3.23297501e-01  1.72383085e-01
  3.35764959e-02  3.99923101e-02  1.31637648e-01 -2.71688104e-02
  3.49282682e-01  7.10597783e-02  1.91827416e-01  5.30280292e-01
  2.90880561e-01  3.94361377e-01 -2.65204906e-01  2.30644286e-01
  1.89680815e-01 -1.04448125e-01  6.28013089e-02 -1.87474474e-01
  6.28064871e-01 -6.47175848e-01 -1.24969624e-01  1.10437520e-01
  2.00089857e-01  2.49523893e-02 -1.47127122e-01  2.14350447e-02
 -1.11859389e-01  3.29103172e-01 -4.55954552e-01 -5.39684594e-02
 -3.43732089e-02 -3.38153541e-01 -2.34761406e-02 -6.71099424e-01
 -1.25376463e-01  9.80783850e-02 -1.74129486e-01  3.51615772e-02
 -9.73802432e-02  1.33464947e-01 -2.50726700e-01  8.18259045e-02
  1.82888731e-01  8.52240473e-02 -7.38400817e-02  2.06288055e-01
 -6.35286272e-02  6.68027773e-02  3.07273269e-01 -3.79987270e-01
 -8.91037285e-04  5.57584837e-02  3.53430718e-01 -5.53254969e-02
  5.20983756e-01 -2.15390980e-01  2.17469811e-01 -7.40473866e-02
 -9.19691026e-02  4.92672622e-01  2.00422585e-01  6.31056800e-02
 -6.18445814e-01  5.52550316e-01  9.19946805e-02 -8.11237693e-02
  2.12951258e-01 -4.67590839e-02 -3.78695935e-01 -2.82211527e-02
  2.86426008e-01 -6.46099523e-02 -1.37396395e-01 -5.74859917e-01
  2.33402938e-01  1.69517219e-01  2.39620693e-02 -3.48727591e-02
 -1.09833188e-01  1.74934328e-01 -2.83126652e-01  1.35335162e-01
 -3.76435310e-01  2.18594074e-01 -1.21836200e-01 -3.39560032e-01
  7.04627857e-03 -7.49170482e-02 -5.76666370e-02 -9.83168259e-02
  7.63152018e-02 -1.48919612e-01  1.97034314e-01  4.32374954e-01
 -1.55441314e-01  1.73608750e-01 -2.74372734e-02  2.12322325e-01
 -5.27004540e-01 -1.14670493e-01  6.81171566e-03  2.12141782e-01
 -1.98060572e-01 -2.17364281e-01  3.15195888e-01  2.01778889e-01
 -2.06620067e-01  2.46357936e-02 -4.28141057e-01  1.36795655e-01
  1.59704179e-01 -1.66490868e-01 -2.99735785e-01 -1.75604448e-01
 -1.04087099e-01  3.67969155e-01 -2.39161029e-02  9.28112939e-02
 -1.76430672e-01  1.85022056e-01  5.07412553e-01 -4.33211088e-01
 -2.56127119e-01  5.25332280e-02 -1.65412456e-01 -1.12694465e-01
  1.42821029e-01 -1.03523493e-01  1.13539621e-01 -4.47968133e-02]"
TfLiteGpuDelegate Prepare: delegate is not initialized comp:lite TFLiteGpuDelegate TF 2.13,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 12
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): v2.13.0


**Provide the text output from tflite_convert**

```
# Copy and paste here
2023-09-26 13:31:37.492599: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-26 13:31:37.500521: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2168003d970 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2023-09-26 13:31:37.500975: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2023-09-26 13:31:37.599984: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2023-09-26 13:31:37.600138: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.

```

**Standalone code to reproduce the issue** 
**java code**
Interpreter.Options options = new Interpreter.Options();
options.addDelegate(new GpuDelegate());
Interpreter  interpreter = new Interpreter(model, options);

**Convert code**
output_name = ['add_1']

input_shape = [288, 288, 3]

converter = tf.lite.TFLiteConverter.from_frozen_graph('model.pb', input_arrays=[""input_image""], output_arrays=output_name, input_shapes={""input_image"": input_shape})

tflite_model = converter.convert()
open('model.tflite', ""wb"").write(tflite_model)

GraphDef  model:
[GraphDef.zip](https://github.com/tensorflow/tensorflow/files/12723201/GraphDef.zip)

TFlite model:
[tflite.zip](https://github.com/tensorflow/tensorflow/files/12723204/tflite.zip)


**Any other info / logs**
**Java issue**
13:40:05.480  W  java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: TfLiteGpuDelegate Init: DEPTHWISE_CONV_2D: ReadNonConstantTensor: value is a constant tensor: 4
13:40:05.480  W  TfLiteGpuDelegate Prepare: delegate is not initialized
13:40:05.480  W  Node number 69 (TfLiteGpuDelegateV2) failed to prepare.
13:40:05.480  W  Restored original execution plan after delegate application failure.
13:40:05.480  W  	at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)
13:40:05.480  W  	at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:110)
13:40:05.480  W  	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:58)
13:40:05.480  W  	at org.tensorflow.lite.NativeInterpreterWrapperExperimental.<init>(NativeInterpreterWrapperExperimental.java:32)
13:40:05.480  W  	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:202)
13:40:05.480  W  	at com.unt.studio.pumpkin.core.style.DCTNet.init(DCTNet.java:33)
13:40:05.480  W  	at com.unt.studio.pumpkin.ui.fragment.StyleTransferFragment$1.run(StyleTransferFragment.java:184)
13:40:05.480  W  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167)
13:40:05.480  W  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641)
13:40:05.480  W  	at java.lang.Thread.run(Thread.java:920)",False,"[-4.18567240e-01 -5.62639356e-01 -3.72605503e-01 -1.49663895e-01
  1.14831924e-01  4.68028076e-02 -2.54125521e-03 -5.82707338e-02
 -1.65787295e-01 -1.65278576e-02  1.20991297e-01 -9.25802290e-02
 -5.75879067e-02  3.67561936e-01 -2.52474904e-01  1.26242042e-01
 -2.55412310e-02 -7.60008469e-02  3.18355501e-01 -1.07114241e-01
  4.70561720e-02  1.68161131e-02 -2.45863214e-01  3.76130760e-01
  2.68451065e-01  4.26129460e-01 -8.47326368e-02 -1.63595125e-01
  6.62426427e-02 -7.49756582e-04  1.35747995e-02  1.75600916e-01
 -2.11843073e-01 -3.02037932e-02 -2.16115564e-01  2.90681601e-01
 -3.50875378e-01  1.12441834e-02 -2.07848698e-01 -5.14293276e-02
  1.28942415e-01  2.23238003e-02  2.04141568e-02  1.31488845e-01
 -3.30011845e-01  1.39243603e-01  1.62707269e-01  2.86036432e-01
 -2.62263209e-01  3.81228290e-02  2.27704048e-02  1.03508122e-03
 -3.90640497e-01 -1.88051462e-01  7.09700137e-02  9.87349302e-02
  1.89263418e-01  1.16986200e-01  9.49406475e-02  1.72931701e-01
 -3.03632170e-02 -1.10485964e-02 -3.55716348e-01  1.40490487e-01
 -2.62470171e-02  2.58749396e-01 -3.97559479e-02 -1.79912254e-01
  2.63326585e-01 -2.99710512e-01 -5.30688390e-02 -6.73338631e-03
  5.28552458e-02  2.37325486e-03  4.16289680e-02  5.90441003e-03
  9.01791081e-02  4.32012856e-01  1.48086846e-01 -6.40593916e-02
  1.25189960e-01  8.63565281e-02  1.04537949e-01  1.43998682e-01
  4.18030322e-02 -5.75553775e-02  6.02569580e-02  4.61734027e-01
  2.81894088e-01  2.65132263e-03  4.83904600e-01  4.96618807e-01
  7.80055299e-04  4.17509526e-02  3.38999629e-02  2.40385041e-01
 -2.61055939e-02  2.58616060e-01  2.29787275e-01  4.03257087e-02
  4.77075726e-02 -2.28643477e-01 -1.62645072e-01  1.17633536e-01
  3.87783051e-02 -3.67812276e-01  9.67212468e-02 -1.34297550e-01
  4.69296575e-02  2.26626366e-01  7.04448223e-02 -1.62121505e-02
  7.86645189e-02 -1.01051264e-01  4.10887226e-02  1.98466927e-01
 -1.45796880e-01  1.71470389e-01 -1.38740949e-02  1.09063625e-01
  9.54576954e-02 -2.61471957e-01  1.41993999e-01  1.19524837e-01
  4.16610569e-01  4.79676276e-02 -2.30393112e-01 -1.31699905e-01
  1.69034258e-01  1.46034539e-01  1.74254879e-01  2.47254550e-01
 -5.43789744e-01  3.64144333e-02  2.56843343e-02  6.47067130e-02
 -2.16085255e-01 -1.13619044e-01 -4.05457467e-01  2.05426924e-02
 -1.71677023e-01  5.55139482e-02 -1.46108478e-01 -2.06694290e-01
 -5.73320985e-02  1.77133426e-01 -4.81165573e-03  2.32102782e-01
  4.09635045e-02  1.71348691e-01 -2.45754689e-01  3.47111523e-02
  5.10652922e-02  2.68779933e-01  1.78642217e-02  7.62939602e-02
  2.41105050e-01 -5.89980781e-02  1.44881696e-01 -5.27765572e-01
 -1.84441000e-01  6.90034330e-02 -5.21646105e-02  7.88940117e-03
 -2.38160975e-03  9.11840647e-02 -3.95770490e-01 -3.19021553e-01
  3.06716233e-01  2.64734030e-01 -1.43424556e-01 -1.60022423e-01
 -5.52706756e-02  1.10479772e-01  1.77688092e-01 -1.82478458e-01
  3.64437938e-01 -4.70456898e-01  4.91006486e-02 -1.17017232e-01
  2.24438518e-01  2.81027593e-02 -1.92719027e-02 -2.62486711e-02
 -2.21513808e-01 -3.56207564e-02  5.39101064e-02  1.06013477e-01
 -2.87474453e-01 -1.64735317e-02 -3.45079154e-01 -4.98917215e-02
  1.66770518e-01  1.34728730e-01 -2.48579472e-01 -9.11426544e-02
  2.85834312e-01 -3.75865921e-02  3.71443555e-02  8.81663784e-02
 -3.72184217e-02  5.15685380e-02 -1.89837664e-02 -2.59845480e-02
  1.59039825e-01 -1.08045027e-01 -1.29091531e-01 -3.34198654e-01
 -3.93278360e-01  7.98215531e-03  3.13815385e-01 -2.33738005e-01
 -9.26821455e-02 -1.12153351e-01 -1.32048815e-01 -1.53639659e-01
 -2.02700585e-01  1.98638625e-02 -3.79733801e-01  1.27801791e-01
  7.62657374e-02 -1.04107708e-01  2.37080976e-01 -2.39784151e-01
 -2.79763430e-01 -1.38447940e-01 -1.34814575e-01  1.56137317e-01
  6.70692045e-03  3.29814523e-01 -5.61109222e-02  5.67649789e-02
  3.11604172e-01  1.08390048e-01  3.48254405e-02 -2.17011988e-01
  1.89922437e-01 -2.66111076e-01 -1.29672468e-01  1.62963457e-02
 -5.07585667e-02 -1.15860272e-02 -1.01061136e-01 -2.97424376e-01
  3.20355073e-02  9.78358164e-02  4.25446182e-02  4.63971421e-02
 -1.21949743e-02  4.39994931e-01  1.53546333e-01 -1.17091879e-01
  3.68966877e-01  2.19292909e-01  2.34737203e-01 -2.52981614e-02
 -1.34203255e-01 -8.94431919e-02  2.92452313e-02 -8.82119313e-02
  8.52351189e-02  4.60459411e-01 -4.12519556e-03  7.25792408e-01
  3.69867265e-01  2.76785940e-01 -1.54702216e-01  1.88400954e-01
 -2.24920958e-01 -3.86233330e-01  4.78806123e-02 -1.19871974e-01
  2.66587257e-01 -2.65586674e-01 -1.92450717e-01 -2.68179148e-01
  3.83337796e-01 -1.78167790e-01  5.14090918e-02  2.89722264e-01
  1.79123789e-01  2.18743175e-01  8.33344981e-02 -1.43895313e-01
  7.67651573e-02 -4.14466172e-01 -2.60128021e-01 -5.66229463e-01
 -2.24188983e-01  1.41148418e-02 -4.33908105e-02  9.69604999e-02
  1.42055050e-01  1.27939522e-01 -6.16049133e-02 -4.43769014e-03
  1.71561748e-01 -2.56624222e-02  2.85039216e-01 -9.68137756e-03
  1.90833751e-02  1.94610253e-01  8.59394521e-02 -1.83743507e-01
  1.61908969e-01  4.51693200e-02  1.44182444e-01 -7.07972273e-02
  4.61967766e-01 -3.11118245e-01  3.06966186e-01 -7.77039081e-02
  1.20315410e-01  3.65883380e-01 -2.17491418e-01  1.16349697e-01
 -7.94563070e-02  2.97444612e-01  5.29151745e-02 -1.95528157e-02
  1.83558300e-01 -2.72580713e-01 -2.09263086e-01  2.73668487e-02
 -7.59021519e-03  2.17972070e-01  6.15523756e-02 -2.53551573e-01
 -2.56733056e-02 -8.91960040e-02 -1.84013069e-01 -3.27427864e-01
  1.02070253e-03 -2.22061425e-02 -1.77123666e-01 -5.51735051e-04
 -1.67938560e-01  1.82179719e-01 -1.25323176e-01 -1.98431045e-01
 -1.24105506e-01 -2.58368164e-01 -2.16750443e-01 -2.10899472e-01
 -6.74268231e-02 -7.45242313e-02  7.36873746e-02  2.65022814e-01
  2.66489923e-01  9.42333937e-02 -1.86113030e-01  1.43880785e-01
 -5.91525286e-02 -8.80634412e-02 -2.34537438e-01  1.61862314e-01
 -1.83283910e-01 -1.34838387e-01  1.21877506e-01  3.98465514e-01
 -3.87141667e-02  6.22097030e-02 -3.16959560e-01 -1.19962826e-01
 -1.43492401e-01 -1.40209839e-01 -3.48214984e-01 -9.92267877e-02
  9.31626633e-02  5.12299240e-01 -6.18751533e-02  3.37150812e-01
 -1.50800556e-01 -5.55269532e-02  3.53240073e-01 -7.77738094e-02
  1.74601391e-01 -1.09743439e-01 -1.59205105e-02 -1.59925133e-01
 -2.16692477e-01 -9.49540287e-02 -1.53653547e-02 -3.19899097e-02]"
Build from source 2.13.0 fails on Rocky Linux 8.8 type:build/install subtype: ubuntu/linux TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Rocky Linux 8.8

### Mobile device

_No response_

### Python version

3.10.2

### Bazel version

5.3.0

### GCC/compiler version

clang 16.0.1 or gcc 11.2.0

### CUDA/cuDNN version

cuda 11.7.1 /  cuDNN 8.5.0.96

### GPU model and memory

_No response_

### Current behavior?

1. load modules (all built locally on the host  and provide an access to the specified software) 
 **module load  bazel/5.3.0    gcc/11.2.0    llvm/16.0.1   clang/16.0.1  python/3.10.2   cuda/11.7.1   tensorRT/8.4.2.4**  

2.  Configure tensroflow:
**./configure**

Resulting  .tf_configure.bazelrc is:
build --action_env PYTHON_BIN_PATH=""/opt/apps/python/3.10.2/bin/python3""
build --action_env PYTHON_LIB_PATH=""/opt/apps/python/3.10.2/lib/python3.10/site-packages""
build --python_path=""/opt/apps/python/3.10.2/bin/python3""
build --config=tensorrt
build --action_env TF_CUDA_VERSION=""11""
build --action_env TF_CUDNN_VERSION=""8""
build --action_env TF_TENSORRT_VERSION=""8""
build --action_env TF_NCCL_VERSION=""""
build --action_env TF_CUDA_PATHS=""/opt/apps/cuda/11.7.1,/opt/apps/tensorRT/8.4.2.4,/usr""
build --action_env CUDA_TOOLKIT_PATH=""/opt/apps/cuda/11.7.1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""7.0""
build --action_env LD_LIBRARY_PATH=""/opt/apps/tensorRT/8.4.2.4/lib:/opt/apps/cuda/11.7.1/lib64:/opt/apps/cuda/11.7.1/nvvm/lib64:/opt/apps/cuda/11.7.1/cublas/lib64:/opt/apps/cuda/11.7.1/extras/CUPTI/lib64:/opt/apps/cuda/11.7.1
/extras/Debugger/lib64:/opt/apps/python/3.10.2/lib:/opt/apps/clang/16.0.1/lib:/opt/apps/llvm/16.0.1/lib:/opt/apps/gcc/11.2.0/lib64:/opt/apps/gcc/11.2.0/lib:/opt/apps/gcc/11.2.0/lib/gcc/x86_64-pc-linux-gnu/11.2.0""
build --config=cuda_clang
build --action_env CLANG_CUDA_COMPILER_PATH=""/opt/apps/clang/16.0.1/bin/clang""
build --config=cuda_clang
build:opt --copt=-mavx2
build:opt --host_copt=-mavx2
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_env=LD_LIBRARY_PATH
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-no_gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-no_gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-no_gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-oss_excluded,-no_gpu,-v1only

3 Run bazel build
**nohup bazel build --config=opt --jobs=8 --verbose_failures --verbose_explanations \
            --explain=/tmp/explain.txt //tensorflow/tools/pip_package:build_pip_package > build-out.txt &**

The build fails with an error. Attaching output log  files build-out.txt (collect errors) and explain.txt ( collect verbose output)
per above command:
[build-out.txt](https://github.com/tensorflow/tensorflow/files/12719249/build-out.txt)
[explain.txt](https://github.com/tensorflow/tensorflow/files/12719259/explain.txt)

If i compile without cuda/tensorRT, the clang compiler is not chosen and there is no way to choose is per current configure.py
as clang choose seem to be used only for gpu-enabled builds. In this case, gcc 11.2.0 is used and the error happens in a different area.

I tried to use different options adding **--config=cuda** or **--cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0""** and every 
time i get  a failure on a different file yet always  the culprit seem to be these lines from the  respective build output:

In file included from /opt/rh/gcc-toolset-12/root/usr/lib/gcc/x86_64-redhat-linux/12/../../../../include/c++/12/memory:77:
In file included from /opt/rh/gcc-toolset-12/root/usr/lib/gcc/x86_64-redhat-linux/12/../../../../include/c++/12/bits/shared_ptr.h:53:
/opt/rh/gcc-toolset-12/root/usr/lib/gcc/x86_64-redhat-linux/12/../../../../include/c++/12/bits/shared_ptr_base.h:196:22: error: type name does not allow function specifier to be specified
bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include/crt/host_defines.h:83:24: note: expanded from macro '__noinline__'
        __attribute__((noinline))

Note, i dont have **/opt/rh...** path on this host so this must  be coming during compiling some external  dependencies 
and looking at my .tf_configure.bazelrc file i dont  really have control over how external dependencies are handled. 
Can reproduce with nightly builds as it uses different  (newer) version of tensorflow and  bazel.

I have previously compiled tensorflow 2.8.0 (with  its corresponding bazel version)  on the same host with the same cuda, python, and gcc using identical build  steps and the builds went perfectly file. I am confident that all prerequisite software is installed correctly (using it for many other packages builds outside tensorflow).

I would really appreciate if someone can point to what am  i doing wrong or what else i can try. 

No conda or docker please. Neither one will work in our environment where i need home built RPMS 
for installation on the HPC cluster. 

Thanks!

### Standalone code to reproduce the issue

```shell
No test case
```


### Relevant log output

_No response_",False,"[-0.63346326 -0.39650786  0.00580201  0.20605573  0.18063292 -0.4341308
 -0.20314467  0.14142144 -0.17614993 -0.26467597  0.02734859  0.0779374
 -0.1977466   0.13849787  0.07174778  0.44858885 -0.15593949 -0.14620405
  0.2732353   0.09195587 -0.37984067  0.00460676 -0.20508166  0.2547083
  0.01067571  0.10435804 -0.37887317  0.19116801  0.06400292  0.07480767
  0.3813948   0.1588249  -0.05056238  0.03546023  0.3527873   0.26177162
 -0.02085628 -0.1527732  -0.30353916 -0.11047068 -0.25668272 -0.10551748
  0.25151664 -0.29758772 -0.04308241 -0.35962582  0.02950782 -0.15748939
 -0.02309901 -0.33511442  0.01030636 -0.02948764 -0.20665562 -0.40429032
 -0.15852198  0.01446105  0.26265162  0.03435297 -0.10010236  0.07070693
  0.29302955  0.2001081   0.0668008  -0.0998069   0.09347957  0.10021061
  0.1338979  -0.18203427  0.5162919  -0.33020598  0.17637362 -0.07517472
 -0.25408256 -0.13252707  0.08738305  0.02960107  0.2034759   0.04033012
 -0.00348623 -0.04134229 -0.06231396 -0.15391465 -0.18343017 -0.31845188
 -0.07729342  0.05121572  0.28290194  0.21326444  0.41964084 -0.16887805
  0.44832927  0.26495722 -0.09641609  0.04868396  0.32018217  0.2076067
 -0.08228388  0.11785383 -0.02923138 -0.05685404 -0.08712462 -0.37731525
 -0.05500809 -0.06101641 -0.1678566  -0.14954409  0.3079294  -0.01198286
  0.12192868 -0.225755    0.20828384 -0.11793423  0.10634705  0.06672602
 -0.08698109 -0.20288262 -0.44144726 -0.08641183 -0.13812041  0.6556617
  0.25470495 -0.20603716  0.20912962  0.17876585  0.48662785 -0.1164625
 -0.13728717 -0.12721583  0.24413793  0.2885279  -0.04033761  0.20452732
  0.19103546  0.3271293  -0.00178564  0.06886011 -0.15745904  0.10260218
 -0.1689381  -0.28990257 -0.14670464  0.3484948  -0.20772348 -0.68483055
  0.00849985 -0.06612805 -0.23742218  0.16699328 -0.11245122  0.09691921
 -0.06203537  0.05790476 -0.16486207  0.4490986  -0.02255768  0.13429086
  0.30698466 -0.06781968  0.01091271 -0.3408021   0.04563743  0.44973505
  0.22459042 -0.21029934 -0.22006276  0.10587652 -0.39574546 -0.20527117
  0.3215123   0.53048253 -0.13002102 -0.01241016  0.43343198  0.20965037
  0.26435655 -0.09807905  0.33265963 -0.31580696 -0.03702574  0.7960207
  0.10483247  0.17784071  0.07781872  0.23680179  0.29921213 -0.0256082
  0.209434    0.3415445  -0.28115684  0.12038933 -0.3809691  -0.23170955
  0.23666841 -0.22111326 -0.24917987  0.24972579  0.28937632 -0.04467367
  0.02274556 -0.14088532 -0.21438003  0.15522338 -0.18599078 -0.03506306
 -0.02334743 -0.13568091  0.17637914 -0.38841313 -0.45744962  0.02527345
  0.07124748 -0.40086475  0.15256318 -0.09626606 -0.21416757  0.17236549
  0.1707454  -0.18069443 -0.1524711   0.155801    0.04846886 -0.44829202
  0.07965793 -0.27609047  0.06026395  0.02423194 -0.2850489  -0.24135111
  0.0337764   0.13792998 -0.01561465 -0.09379834  0.40193662  0.43050066
  0.43100974  0.04673464  0.0418567  -0.24336272  0.05423081  0.07133918
 -0.39733842 -0.08025327  0.08544505  0.3037259   0.21138243  0.46843952
 -0.33967996 -0.08813508 -0.2636044   0.16360793 -0.17916664  0.07684369
  0.11844644  0.14527002  0.40776885  0.31332627  0.01360238  0.12309526
  0.23463202  0.03173253  0.22422896  0.14414519 -0.08986206  0.00527757
  0.250848    0.11031565 -0.27282158  0.27558088 -0.07893025 -0.17175403
  0.23468837 -0.22376682  0.4864808  -0.505756    0.22698842 -0.12862071
  0.35393482 -0.30588797  0.08941157 -0.01916632  0.0228862   0.31431448
 -0.274809   -0.05022441  0.14446798  0.00967763  0.02313454 -0.91291296
 -0.43929613 -0.08970789 -0.09222284  0.06516748 -0.42902493 -0.27114302
 -0.17633444  0.02225206  0.04383714 -0.09064813  0.05113156  0.10589122
 -0.25109446  0.07366765  0.4081385  -0.39563152 -0.13298094 -0.0255616
  0.14947629  0.1336528   0.53355765 -0.41759604  0.07238871 -0.01348855
 -0.01044493  0.36660424  0.17810291  0.14851704 -0.3840199   0.712043
  0.05491637 -0.02298325  0.3590155  -0.18224771 -0.39773464 -0.05358323
  0.14197914 -0.11122485 -0.01647333 -0.2845149   0.17630562  0.2777738
 -0.24904785  0.03563141  0.02785958  0.01876958 -0.3515281   0.1896415
 -0.16820076  0.0130135  -0.16813831 -0.21731684 -0.2338967  -0.1404207
 -0.09164548 -0.42626405  0.08783199 -0.18328118  0.46913362  0.30439043
 -0.16870737  0.11778976  0.00877675  0.15267138 -0.67407584  0.02484545
  0.3401279   0.1515185  -0.11897294  0.16999137  0.55367637  0.06636808
 -0.03223642  0.3655798  -0.3904712   0.05136301 -0.05464604 -0.20327063
 -0.6075181  -0.15493894 -0.19482808  0.15925646 -0.20474058  0.17407537
 -0.46514273  0.3429779   0.7877581  -0.39348894 -0.3078959   0.09406842
  0.08729764 -0.15598193 -0.06504081 -0.08489241  0.45325863 -0.06653105]"
Windows 10 Source build failure if python installed in a directory with spaces stat:awaiting response type:build/install stale subtype:windows TF 2.10,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.10

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

5.1.1

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1.1

### GPU model and memory

Quadro P2000 / 5 Gb

### Current behavior?

Building 2.10 tensorflow with cuda support from sources using Bazel 5.1.1 would result in an error if the python is downloaded in a directory with spaces. In my case I had python installed under `C:/Program Files/python39`. The error showed that: `'c:/program' is not recognized as an internal or external command building tensorflow'`. I traced this error into the `tensorflow\third_party\gpus\cuda_configure.bzl` file. In this file there is a function called `_check_cuda_libs` (line 492). This function checks the precense of cuda libraries by compling a cmd. However, the cmd won't work if the python_bin points to a python executive which is located in a directory with spaces. This error was simply fixed by moving python to a directory without spaces (No need to change the scripts inside `.bzl` file).

### Standalone code to reproduce the issue

```shell
You should be able to reproduce the issue by following the ""Build from source on Windows"" instructions and downloading python for example under ""Program Files"" folder. Build should be configured to include CUDA support. I used the following build command: bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

_No response_",False,"[-0.24193878 -0.18304849 -0.01979484  0.25659484  0.08749081 -0.35695672
 -0.42711878  0.1273864  -0.32441306 -0.23011945 -0.26819444 -0.11620479
 -0.17895448  0.09916258 -0.4149773   0.47166285 -0.279744    0.02057248
  0.17876163  0.30812907 -0.19569945 -0.04774879 -0.19530688  0.14370471
  0.26660994 -0.05295153 -0.24623007  0.15351327  0.04411905  0.49608347
  0.26801568 -0.11896487 -0.06007523  0.10764919  0.28848645  0.18220694
 -0.08872298 -0.22836696 -0.35133588 -0.16020095 -0.02919195  0.12983963
 -0.026522   -0.17165734 -0.06129993 -0.08902494  0.24046339 -0.1889604
 -0.31644332 -0.24282283 -0.260193    0.22730166 -0.2908407  -0.6101908
 -0.14020874 -0.35216233  0.03844882  0.15699068 -0.12539788 -0.12971395
  0.09083118 -0.06269003  0.39533508 -0.2371196   0.06552643  0.36656013
  0.03155501  0.06559286  0.3374869  -0.5005162   0.17188475  0.05022278
 -0.43576467  0.00115857  0.08910222  0.25582814  0.06583425 -0.02394657
  0.35408503 -0.39229542  0.09979632 -0.34861657 -0.2181671   0.05817919
  0.24223875 -0.06990119  0.3770912  -0.01023738  0.4818868  -0.20586425
  0.5001972   0.03088841  0.08155236  0.15321064  0.41624647  0.08962115
  0.12664996  0.40644488 -0.03383175 -0.03954932  0.03500536 -0.25969058
 -0.13189626 -0.11709376  0.08465681 -0.29993805  0.4973089   0.02668419
 -0.0181854  -0.25616825  0.2129722   0.01394446  0.1287881  -0.26914346
 -0.1728975   0.02866891 -0.2467877   0.17688277  0.04754998  0.6104516
 -0.0173387  -0.15579575 -0.00806777 -0.15005302  0.20036864  0.07034776
 -0.25986814 -0.02430728  0.04044624 -0.03833177  0.23038486  0.32829228
  0.26451564  0.22087584  0.03587666  0.04912489 -0.04871276 -0.2469273
 -0.2723798  -0.32020038 -0.09461279  0.12091608 -0.15245202 -0.5662464
 -0.03922615 -0.11116977 -0.25163567  0.33516344 -0.18030825  0.18958549
  0.08846724  0.3025425  -0.34348708  0.25943542  0.15315671 -0.16016302
  0.19207948 -0.06950941 -0.17915654 -0.43757167 -0.06973571  0.40332508
 -0.18757564 -0.04046699  0.03450031  0.2691893  -0.48538986 -0.29557344
  0.25744653  0.3250852  -0.03669138 -0.2130909   0.27391255 -0.03388996
  0.33479044 -0.02390545  0.07198908 -0.20596254  0.16333833  0.27037764
  0.17980185  0.11972346 -0.10518022  0.03222981  0.09041519  0.13390046
  0.18686846  0.26141605  0.02969237  0.25139526 -0.30171934  0.04783474
  0.398518   -0.16640204 -0.16186312  0.07853441  0.15042011  0.10358044
  0.15217602  0.1038585  -0.08453631  0.08079314 -0.13254237  0.12728809
  0.18017831 -0.3401888  -0.06368244 -0.3455212  -0.17152041 -0.06199277
 -0.09725561 -0.5053344   0.03018952 -0.06916797 -0.22483931  0.22019209
  0.24311393 -0.04673872  0.06384769 -0.18056974 -0.1089149  -0.051388
 -0.08554499 -0.37787282  0.14309263  0.0798644  -0.2795329  -0.10172278
 -0.13421053  0.173493    0.06929727 -0.14938645  0.34736443  0.08955036
  0.53252256 -0.25245875  0.08869641 -0.15694612 -0.19808961  0.17552668
 -0.44655034 -0.1775626  -0.09992556 -0.02698993  0.17627706  0.4842654
 -0.20747897  0.00258429 -0.48973402  0.12185947 -0.05067439  0.13641037
  0.38772452  0.13424991  0.4219463   0.3772512   0.11611694  0.15988426
  0.13077998 -0.2817704   0.44699317  0.03283611  0.19477473  0.0123007
  0.1768922   0.27081677 -0.2717414   0.7201042   0.16226557  0.013387
  0.26057673 -0.3846984   0.5487427  -0.53440106  0.0072259   0.02558303
  0.314434   -0.0667685   0.00204059  0.14109299 -0.05036806  0.43552336
 -0.34706306  0.3096867   0.05809683 -0.02684271 -0.00323401 -0.55027115
 -0.27362594  0.29695714 -0.34336466  0.06387712 -0.11692216 -0.07383946
 -0.09318624  0.00969939 -0.11147384 -0.42710775 -0.02425855  0.15964513
 -0.32952726  0.15107095  0.22088969 -0.18505082 -0.28286374 -0.29838195
  0.21847887  0.2752162   0.34927812 -0.48985463  0.12516189 -0.07330643
 -0.24247794  0.49751246 -0.03648583 -0.01749201 -0.49332157  0.7350179
  0.1692316  -0.08652895  0.12944202 -0.15327287 -0.2675807   0.14896618
  0.24997859  0.08827545 -0.10302702 -0.21618235  0.00604175  0.20333663
 -0.14187053 -0.2069425  -0.36839294  0.02386834 -0.07244439  0.03199299
 -0.33615932  0.26684925  0.01084129 -0.05426354 -0.03054477 -0.29491496
  0.25846136 -0.16278401  0.07959766 -0.38155872  0.36625004  0.24403399
 -0.06384203 -0.07704517  0.14401567  0.01446685 -0.55592906  0.05594631
 -0.15967637  0.22245571  0.05363294 -0.00914006  0.17374185  0.18906416
 -0.05849646  0.07936431 -0.42716172 -0.04054794  0.14228311 -0.06769623
 -0.54841644 -0.10460004  0.10127421  0.0988404   0.02221497  0.27734762
 -0.21879537  0.48773137  0.5583496  -0.08294319 -0.30351603  0.4032897
  0.1658513  -0.04942872  0.13843703  0.10817806  0.14544827  0.06812786]"
Use github.com/apssouza22/chatflow as a conversational layer. It would enable actual API requests to be carried out from natural language inputs. stat:awaiting response type:feature stale comp:apis,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

all of em

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Fine as hell

### Standalone code to reproduce the issue

```shell
n/a
```


### Relevant log output

```shell
i <3 tensorflow
```
",False,"[-0.59774524 -0.34232527  0.00970644  0.0310336   0.09577248 -0.42628878
 -0.203313   -0.18836856 -0.18340257 -0.13270494  0.14899296 -0.15994674
  0.00126786  0.18855229  0.35475838  0.34032834 -0.11752135  0.0289877
  0.31740227  0.00312063  0.02273683  0.01501775 -0.0466199   0.15859315
  0.07090689  0.146923   -0.23744613 -0.05826052  0.06282324  0.23261064
  0.388669    0.0993658   0.14026919 -0.06773865  0.05573916  0.2522909
 -0.17801088 -0.32392615 -0.22143392 -0.00671982 -0.12247047  0.2481991
  0.11547884 -0.26585162  0.02306088 -0.16479784 -0.23003247 -0.09052863
 -0.21818933 -0.2091982  -0.03311823 -0.05637726 -0.5915733  -0.10644336
 -0.15831092 -0.214571   -0.09165236  0.04983857 -0.13164467  0.21383786
 -0.1457149  -0.07294445  0.04327167  0.02184677 -0.01406956  0.32535794
  0.1145622   0.05286426  0.4137521  -0.15705433 -0.04874676 -0.21074142
 -0.4458193   0.11039297  0.06627944 -0.05923985 -0.12139057  0.1318134
  0.3228684  -0.01979168  0.14402705 -0.10051955  0.14029856 -0.07455043
  0.1152013  -0.1709142   0.2839488   0.1846606   0.16298185 -0.17678894
  0.18323176  0.35414797  0.07508349  0.17079839  0.5720008   0.14348285
  0.04726003  0.07321793  0.04966925 -0.22343531 -0.13100773 -0.13407841
 -0.11081086 -0.13901389 -0.23271452 -0.18599655  0.1877162  -0.01416305
  0.298532    0.10650148  0.06741737  0.06908309  0.08225308 -0.28677958
 -0.13574307 -0.10764964  0.09689312 -0.25669876  0.03484455  0.7846571
  0.14329188 -0.22011341 -0.10869648  0.46755108  0.5348621   0.15670457
 -0.02258743  0.00879409  0.17335683 -0.02678964  0.15823641  0.15303248
 -0.04129974  0.16605839 -0.09137145 -0.19145066 -0.23305193 -0.30982727
 -0.3280425  -0.17187634 -0.14945516  0.19156943  0.00524402 -0.5254909
  0.05351661  0.22736982  0.00128171  0.2335681  -0.1407705  -0.14272869
  0.15972331  0.3138297  -0.07849997  0.4305217   0.3408975   0.00998111
  0.24276045 -0.01012637  0.12465578 -0.5622275  -0.12930116  0.24605365
 -0.08350588 -0.31752118  0.09154035  0.01882192 -0.59266067 -0.2623915
  0.04305082  0.36461383 -0.34753555 -0.04977997  0.191962   -0.14079359
  0.1338602   0.16637126  0.25397006 -0.32272983 -0.03618176  0.45322773
 -0.05858037  0.3952813  -0.01766103  0.19120187 -0.04130097  0.04148263
  0.07694409  0.1994796  -0.01592636  0.06267539 -0.4621471  -0.03420943
  0.5476182  -0.30285585 -0.1822088   0.21012354  0.18312214 -0.13931489
 -0.14013018  0.05368497 -0.20970872 -0.01892078 -0.33588445 -0.05496399
  0.17010245 -0.10444471  0.07590013 -0.20281717 -0.11316957  0.008321
 -0.11191976 -0.49212557  0.07470448  0.06368007 -0.25996673 -0.00491688
  0.01547248  0.45679128 -0.02216323  0.22938247 -0.157042   -0.09900473
  0.0059815  -0.3911542  -0.16902308  0.29197413 -0.29640394  0.23133573
 -0.13687184  0.12456128  0.22045512  0.3004313   0.56810427  0.35678458
  0.4168837  -0.01861491 -0.09461775 -0.03908278  0.09505055  0.17701721
 -0.42414764 -0.1925414  -0.06087649  0.10456708  0.07664868  0.4410294
 -0.06018901 -0.19361043 -0.36125016  0.26131564 -0.34937152  0.074007
  0.50133485  0.04929591  0.357095    0.37605152 -0.1212151   0.16079935
  0.26663405 -0.12893765  0.30668288  0.1112792   0.22876415  0.331873
  0.44237515  0.1775516  -0.5331798   0.4341157   0.1434867  -0.12425467
 -0.09815161 -0.2823129   0.2169208  -0.27341074  0.2452199   0.10735641
  0.27821457 -0.2541582   0.13243464  0.04746069 -0.00556033  0.06233966
 -0.2987756   0.1637688   0.20893475 -0.41168082  0.07499909 -0.5919304
 -0.1237762   0.05020127 -0.2933616   0.06126888 -0.06205353  0.15571472
 -0.21279386  0.05832452  0.1627779  -0.33050606  0.23457006  0.36797455
 -0.30349693 -0.1130392   0.05098712 -0.32187253 -0.31763792  0.20220038
  0.30115533 -0.07340418  0.5789936  -0.3748039  -0.11787009  0.13216195
 -0.08713922  0.48975563 -0.10904134 -0.12523234 -0.3663835   0.74360716
  0.23800269 -0.06697591  0.04488973 -0.13400638 -0.16540961  0.13133994
  0.23231846  0.0942719  -0.11097915 -0.32618487  0.03306021  0.13839519
 -0.1236283   0.04524315 -0.20217317  0.18937284 -0.19633755 -0.10735932
 -0.2609266   0.14767624 -0.12630881 -0.17121103 -0.14783034 -0.03302114
 -0.12224743 -0.01563777 -0.11700686 -0.25388956  0.49866906  0.6109565
 -0.2920609  -0.0615834  -0.07578236  0.18159354 -0.88625646 -0.0917986
 -0.40250015  0.2992636  -0.22315124 -0.4538045   0.1797036   0.18021001
 -0.12788166 -0.06014467 -0.30726302  0.04611252  0.23797357 -0.01144956
 -0.10481638 -0.36533436 -0.07686093  0.29859686 -0.27630985  0.09853569
 -0.4711976   0.53929704  0.41818    -0.31953186  0.03832918 -0.06021212
  0.16561067  0.16650923 -0.1075821   0.09400542  0.15238705 -0.14005353]"
Color prediction result  ,"Fastwin color prediction game results 
",False,"[-1.17733598e-01  3.02300811e-01 -4.40457910e-01 -4.05497551e-01
  3.80142599e-01  5.61882913e-01 -1.57609120e-01 -9.74023119e-02
 -2.36926422e-01 -1.75705329e-01 -6.34172857e-01 -3.43293339e-01
  1.37222543e-01 -4.55866605e-01 -3.75086099e-01  1.45143643e-01
  2.83104390e-01  5.44510424e-01 -4.94810611e-01 -5.51766634e-01
 -7.82934606e-01  1.08093563e-02  2.21455574e-01 -1.39135003e-01
  6.44569173e-02 -5.51344097e-01  3.96961011e-02  2.88788021e-01
  2.60759532e-01 -1.37287593e-02 -2.00792655e-01 -8.71586949e-02
  6.18029177e-01  3.19118500e-01 -9.13521230e-01 -5.11676252e-01
 -1.76311493e-01  2.15814605e-01 -4.13059592e-01 -3.53356227e-02
 -2.61514246e-01  5.05087435e-01 -7.66128972e-02  4.15415287e-01
 -2.23956540e-01  2.88338542e-01 -4.95128036e-01 -2.81219214e-01
 -2.72990495e-01  7.00243354e-01 -2.49683429e-02  5.73558062e-02
 -5.58956742e-01 -7.30203271e-01 -2.74706315e-02  5.86795509e-01
 -1.19002968e-01 -3.31839383e-01  5.07451952e-01 -2.67000318e-01
  5.08684516e-02 -5.17142415e-01  5.49124599e-01 -3.72433633e-01
  2.30388254e-01 -5.50674856e-01 -2.68250674e-01  1.40132770e-01
 -3.64779472e-01 -4.74724084e-01  2.06552781e-02 -1.24896169e-01
 -1.66279390e-01 -2.29562715e-01 -4.59211439e-01  6.90330327e-01
 -2.42644414e-01  4.66810027e-03 -1.84492394e-02 -3.54998469e-01
  2.04302654e-01 -9.50814188e-01 -1.15133852e-01  9.93572399e-02
  1.04056120e+00  3.23873162e-01  1.25586495e-01 -8.40374380e-02
  3.31681758e-01  3.48771811e-01  9.35910717e-02  4.76863757e-02
 -2.61161149e-01  1.75845087e-01 -5.52602746e-02  4.05034333e-01
 -1.77714765e-01 -1.32251740e+00  2.08761215e-01  6.54677331e-01
 -4.12504554e-01 -3.90431792e-01  1.59418121e-01 -3.14337254e-01
  4.58330363e-01 -3.07811081e-01 -1.71514377e-01  3.44838619e-01
  3.06103945e-01  2.60399282e-01  3.84884745e-01 -4.76506576e-02
 -1.93380401e-01  4.53076333e-01 -1.13706939e-01  1.06247686e-01
  2.51130193e-01  2.45702401e-01  2.35134467e-01  2.51915872e-01
  7.52968252e-01  1.64440975e-01 -6.59990534e-02  2.95187354e-01
  3.71035308e-01  3.56672071e-02 -8.11932236e-02  3.32508355e-01
  3.12105231e-02 -1.21089607e-01  4.01957780e-01  3.91979426e-01
  2.19258875e-01 -2.05836013e-01 -1.42480373e-01 -2.58139104e-01
 -2.56675869e-01 -3.53128910e-01 -7.05227181e-02 -9.84679386e-02
 -6.61864817e-01  2.73830026e-01  1.69064373e-01  3.51301938e-01
  2.35860065e-01 -2.78564930e-01 -7.45048285e-01  4.73060131e-01
  2.55299836e-01 -5.67643940e-01  7.59464204e-02 -4.72065568e-01
 -4.69037741e-01  1.16367027e-01  7.37198830e-01  1.79917827e-01
 -2.30745316e-01  1.59591734e-01  2.57190615e-01 -4.14938241e-01
  5.07759988e-01 -6.72608688e-02  5.58064520e-01 -5.42790353e-01
 -4.76508349e-01  2.39141181e-01  1.02165848e-01  7.17897654e-01
 -5.47365904e-01 -1.32268965e-01 -1.84721693e-01 -3.66107076e-01
 -3.39694500e-01  1.45360216e-01 -1.40710743e-02 -1.65115818e-01
 -5.59649825e-01  3.34598213e-01 -3.09865922e-01  8.76078531e-02
  1.80572987e-01  2.61876553e-01 -1.32484615e-01  1.86060816e-01
  2.56233037e-01 -2.63668984e-01  7.20810220e-02  4.25979346e-01
 -1.03652680e+00  2.32069254e-01  5.03142416e-01 -6.29127979e-01
 -3.91756654e-01  2.84416527e-01  2.69446760e-01 -2.69722849e-01
 -2.75604397e-01 -3.90965253e-01  8.47425640e-01 -9.54301804e-02
 -5.17122634e-02 -9.78140756e-02 -7.23295212e-02 -6.60520315e-01
 -2.22900733e-01  1.27250776e-01 -3.39237422e-01 -9.26198959e-02
 -7.94849638e-03  6.67810798e-01 -3.68520051e-01 -5.61371267e-01
 -6.60009205e-01 -2.37400383e-01  5.37343025e-01  3.97704571e-01
 -4.85777169e-01 -3.69709969e-01  2.06454843e-01  3.46652895e-01
  1.24683544e-01 -3.67642850e-01 -2.25855052e-01  7.61069134e-02
 -4.17951137e-01  1.40848875e-01 -3.92743886e-01  6.29701555e-01
 -2.26365522e-01  5.76440729e-02 -1.86113298e-01  3.44733387e-01
 -2.44440753e-02  1.52900010e-01  1.58174932e-01  4.95808095e-01
 -4.17957783e-01  1.39384702e-01 -6.14398085e-02  1.21937186e-01
  4.52167064e-01  8.75047505e-01 -4.67030965e-02 -4.01462168e-02
 -2.91615456e-01 -1.13134738e-02 -8.26395392e-01 -1.62204087e-01
 -3.73491019e-01 -1.30563334e-01  2.65130758e-01  2.08251953e-01
 -1.77193940e-01  1.81501076e-01 -4.80811507e-01 -3.27693582e-01
 -3.63849521e-01  1.47948861e-01  5.58231413e-01  2.64616489e-01
  9.45885837e-01 -7.96845436e-01 -4.88325804e-02  5.28088987e-01
  3.67990613e-01 -3.70111912e-01 -1.41325399e-01  2.10539430e-01
 -2.31227160e-01  5.70370257e-01 -6.09175824e-02 -1.89638004e-01
  1.07177451e-01  2.97565132e-01  5.69465280e-01  5.36214232e-01
 -4.06537531e-03  8.01920712e-01 -1.71566442e-01 -1.26593322e-01
 -1.26398802e-01  4.88877684e-01 -5.41526258e-01  1.19122767e+00
 -1.53829962e-01  7.71461800e-02 -1.03999814e-02  1.13600947e-01
 -1.53536797e-02 -3.88418585e-01 -4.91166227e-02 -3.15033168e-01
  5.21844327e-01  8.24311748e-02 -2.91749835e-01  3.00809383e-01
  2.27611303e-01  5.13435125e-01 -1.60617620e-01  1.45793542e-01
  2.05108583e-01  1.06667709e+00  1.20268337e-01  3.45289022e-01
 -7.39865899e-01  6.71276972e-02  1.98597133e-01  8.14469755e-01
  1.03820629e-01 -3.04150462e-01  1.38312325e-01 -2.09006760e-02
 -1.18350342e-01  1.54787242e-01 -2.53972262e-01  5.29106081e-01
  5.64560890e-02 -2.39386320e-01 -3.30931038e-01  2.25810528e-01
  7.46104866e-02 -5.12397289e-01  1.03113241e-01  6.76041901e-01
 -9.59345177e-02  6.21958971e-01  2.11444318e-01 -4.85789299e-01
 -2.30172649e-02 -1.06943004e-01  1.18386596e-01  3.38491797e-01
 -3.18468779e-01 -3.85522097e-01  2.78950095e-01 -1.07593862e-02
 -1.08671926e-01  3.37064981e-01  1.76746920e-01 -1.12197883e-01
 -1.04628898e-01  4.52490240e-01  6.61983013e-01 -3.14178258e-01
  1.11947037e-01  1.30695313e-01 -7.11884677e-01  2.20515728e-01
 -3.34508806e-01  3.60863924e-01  3.33966613e-01  2.70462215e-01
 -3.57550472e-01 -1.26269966e-01 -5.73913194e-02  2.41075560e-01
  1.76192462e-01  2.92484641e-01  1.83475405e-01  1.12415306e-01
 -6.89450264e-01 -3.97860348e-01 -3.05794448e-01 -2.60971516e-01
  4.15578485e-01 -3.27435851e-01  5.25348365e-01 -8.61503243e-01
 -5.03853798e-01 -4.38713193e-01 -1.15456082e-01 -1.72962010e-01
  7.99745321e-04 -1.15245124e-02  1.16626710e-01 -4.92153615e-01
 -1.38436094e-01 -1.12096101e-01  3.21900725e-01 -2.17222884e-01
 -4.45762545e-01  4.82162684e-01 -4.71280009e-01 -1.38244674e-01]"
"also missing from the include ""patch"" is file ""tensorflow/tsl/c/tsl_status.h"" (there might be others) stat:awaiting tensorflower subtype:windows subtype:cpu-intel TF2.14","              also missing from the include ""patch"" is file ""tensorflow/tsl/c/tsl_status.h"" (there might be others)

_Originally posted by @esohns in https://github.com/tensorflow/tensorflow/issues/59762#issuecomment-1732376736_
            ",False,"[-1.96080640e-01 -5.35916924e-01  2.14838237e-01  1.61060750e-01
  2.49420792e-01 -9.60505605e-02 -1.71036467e-01 -2.14807391e-01
 -4.31376755e-01 -3.22729528e-01  5.52318804e-03 -8.18613991e-02
 -1.59380883e-01 -2.94799149e-01 -3.05212021e-01  7.59175867e-02
  4.92490903e-02 -2.17638090e-01  7.91864842e-02 -2.30816826e-01
 -4.01933074e-01 -2.38922864e-01 -3.07580233e-01  2.71571249e-01
  1.99411660e-01 -2.83706784e-02 -1.84077322e-02  5.62023148e-02
 -1.51865706e-02  2.63789833e-01  2.71332443e-01  1.57901093e-01
 -2.63918817e-01  9.60434750e-02  8.32402259e-02  1.10563770e-01
 -1.71056278e-02 -8.14958066e-02 -1.65724859e-01 -1.06531382e-01
  9.84692648e-02 -6.94320202e-02  1.75197840e-01  1.62058413e-01
  1.54415984e-02  5.60879111e-02  2.94522606e-02 -2.70093143e-01
 -2.13661686e-01 -1.20901994e-01  8.72545987e-02 -1.15924910e-01
 -2.88738072e-01 -2.79511154e-01 -5.99355400e-02 -1.84857756e-01
  1.12361178e-01  1.87292323e-01  8.22085291e-02  5.05028129e-01
 -4.84669767e-02 -2.46231616e-01  1.67844087e-01  4.24092710e-02
 -1.42877363e-03  1.35649107e-02  2.57376313e-01 -2.42708981e-01
  6.43373847e-01 -1.66874111e-01  3.09025973e-01  1.30267918e-01
 -1.85906410e-01 -6.37350082e-02  1.41030714e-01  2.73927927e-01
  1.06721804e-01  2.55484700e-01  3.20609748e-01 -4.41662878e-01
 -8.89376253e-02 -7.36258551e-02  2.79233426e-01  2.09811836e-01
  2.56694615e-01 -5.08448742e-02  6.24482185e-02  8.65772441e-02
  4.14177030e-02 -1.45168304e-01  3.64822358e-01 -1.27884418e-01
  4.46866937e-02 -1.08461529e-01  2.18225360e-01  1.87572077e-01
  1.44411415e-01  3.72181058e-01 -2.77876616e-01 -1.94120228e-01
 -3.25878233e-01 -2.41820708e-01 -1.96855515e-01  1.54499888e-01
 -6.89837933e-02 -2.75015794e-02  2.66531616e-01 -6.54755682e-02
 -3.12536620e-02 -1.30006954e-01  2.87313044e-01  1.17357820e-02
  1.01883993e-01  8.13273117e-02  3.34988311e-02 -4.59352769e-02
  1.96222290e-02  4.83077392e-02  1.31194592e-01  4.87967253e-01
 -3.32430661e-01 -1.30373403e-01  4.57401667e-03 -1.39741659e-01
 -1.28617063e-01  8.80742967e-02 -3.26203406e-01  1.10305421e-01
  2.29687020e-01  3.93725820e-02  1.94198713e-02  9.40854326e-02
  2.65687585e-01 -8.90091062e-02  1.38411194e-01 -3.21165860e-01
 -1.79616660e-01 -2.32346326e-01 -1.62092686e-01 -2.12329388e-01
  1.61962993e-02  2.32543387e-02 -2.84418046e-01 -1.94730282e-01
  9.30855982e-04  1.55266643e-01 -2.40976289e-01  1.23800114e-01
 -6.95068464e-02 -1.69156119e-03 -2.68920511e-01  2.44326755e-01
 -7.12310523e-02  4.59343195e-01  2.05960982e-02  2.79282361e-01
  1.98503226e-01 -1.44558042e-01 -4.58228812e-02 -3.84581119e-01
  1.84326082e-01  3.14397156e-01 -1.05823219e-01 -5.92087135e-02
  2.39872426e-01  1.34349450e-01 -3.68006229e-01  5.55941164e-02
  4.65024084e-01  2.01464638e-01 -4.25615907e-02 -2.85280664e-02
 -1.48542345e-01  1.90694824e-01  4.36169386e-01 -1.21705428e-01
  4.21299964e-01 -5.24287820e-01 -9.72649902e-02  2.04963699e-01
  2.93755829e-01 -6.25018358e-01  5.24149612e-02  1.38017684e-01
 -9.35065448e-02  8.95274878e-02  3.73046935e-01  1.63337082e-01
 -1.00176990e-01  6.99087977e-02 -2.58006871e-01  1.76056638e-01
  2.98324347e-01  2.50903755e-01 -6.48069531e-02 -6.65299147e-02
  5.91811016e-02  1.87921509e-01  3.30260061e-02  5.97467721e-02
  7.28312209e-02  1.37050211e-01  3.46621685e-03  1.22404546e-01
  8.90741050e-02 -4.06757593e-01 -1.40043378e-01 -3.39019835e-01
 -2.96397001e-01 -1.65937826e-01  1.21298015e-01  3.68616097e-02
 -2.23702509e-02 -2.98164841e-02 -1.14546612e-01  1.05647258e-01
 -6.13594279e-02 -6.13527000e-02 -6.28597662e-02 -5.40956743e-02
 -5.47147468e-02 -2.41857558e-01 -1.98876917e-01 -1.77076757e-01
 -3.01926345e-01 -5.90271950e-02 -3.28646153e-01 -2.04554543e-01
  1.89296491e-02  3.88036191e-01 -9.01037678e-02  2.29233518e-01
  4.47903246e-01  5.67079335e-02  3.63848716e-01 -1.95778579e-01
 -2.28778392e-01 -1.69060409e-01 -3.01572680e-01  3.89553696e-01
 -3.57130885e-01 -1.76859647e-01 -3.20941478e-01 -8.93175602e-03
 -3.02861556e-02 -2.17609797e-02 -3.92113551e-02  1.44234672e-01
 -2.32010946e-01  2.88668573e-01 -1.16467416e-01 -4.64563742e-02
  1.30758256e-01  1.69619918e-04  9.89637524e-02  7.89811164e-02
 -3.13191414e-02  2.34256268e-01  1.96338117e-01 -6.46228343e-02
  3.04531217e-01 -5.03194481e-02 -1.75531402e-01  4.76837367e-01
  8.84160176e-02  2.91749656e-01 -7.19521195e-02  4.01809871e-01
 -2.54021406e-01  1.77908763e-01  2.29288518e-01 -1.45231217e-01
  5.21089695e-02 -8.34295973e-02 -1.68164194e-01 -1.01520039e-01
  3.96681011e-01  1.37368172e-01 -3.08569312e-01  5.47302403e-02
  1.46363825e-01  2.43716717e-01 -2.05745757e-01  1.29637063e-01
  1.59661602e-02 -1.38366207e-01 -6.32211491e-02 -3.57214332e-01
 -1.32772088e-01  7.82043952e-03 -2.17348292e-01 -2.27618456e-01
  5.98393232e-02 -2.72839442e-02 -5.89188375e-02  1.98804401e-02
 -1.94582254e-01 -1.33331895e-01 -9.50522274e-02  2.93922722e-02
  5.50187789e-02  4.32450585e-02  1.89396143e-01 -1.35315657e-02
 -9.42914784e-02 -7.35312849e-02  2.71845236e-02  5.06465733e-02
  5.05961895e-01 -4.26073939e-01  2.81002931e-02  2.97184587e-01
  4.11716290e-04  1.56614929e-01 -8.21179897e-02  4.25489843e-02
 -3.42428654e-01  5.56748927e-01  5.01341335e-02 -1.62457943e-01
  3.75281572e-01 -1.48775876e-02 -4.24483299e-01  1.76481098e-01
  6.90646768e-02  1.98995009e-01 -2.77470231e-01 -3.18314970e-01
 -2.56293360e-03  2.48156920e-01 -1.15238957e-01 -2.21389651e-01
 -2.71657228e-01 -3.47447932e-01 -4.70898822e-02 -2.46063277e-01
 -2.27765530e-01  3.20406646e-01  1.54658690e-01 -1.57434300e-01
 -5.87577000e-02  1.36355050e-02  3.06412935e-01 -3.93618375e-01
  2.37326641e-02  1.02025613e-01  2.80940592e-01  2.49131262e-01
 -1.35272712e-01  3.04148704e-01  3.69200483e-02  7.86818936e-03
 -1.89525887e-01 -1.42623512e-02  1.67446285e-01  2.56927073e-01
 -2.51793303e-02  2.41089925e-01  1.23854026e-01  4.72099125e-01
 -1.59684271e-01  1.49054974e-01 -3.25844288e-02  6.30277023e-03
  3.39106619e-01 -2.16391444e-01 -2.61178523e-01 -7.09967762e-02
  2.34818429e-01  7.44316876e-02  8.08814391e-02 -1.21072106e-01
 -1.88528001e-01  1.48358196e-01  1.16509475e-01  4.14926820e-02
 -9.50531885e-02  4.72027771e-02  8.23018998e-02 -2.25694776e-01
  1.05721131e-01 -5.75897172e-02  2.10308507e-02  4.02157009e-03]"
Error with protobuf during installation type:build/install subtype: ubuntu/linux TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 12.3

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.1.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

/home/user1/.cache/bazel/_bazel_user1/2ed8e7afdea3ff827d1d2c14869018ce/external/com_google_protobuf/BUILD.bazel:459:10: Compiling src/google/protobuf/compiler/main.cc [for tool] failed: (Exit 1): clang failed: error executing command (from target @com_google_protobuf//:protoc) 
  (cd /home/user1/.cache/bazel/_bazel_user1/2ed8e7afdea3ff827d1d2c14869018ce/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64 \
    PATH=/home/user1/.cache/bazelisk/downloads/sha256/6c25a6d716545d6b672ec46f770521cd9ebb63d73617b8f4e6747825d1db1839/bin:/home/user1/bin:/usr/local/cuda-12.2/bin:/home/user1/anaconda3/bin:/home/user1/anaconda3/condabin:/home/user1/.local/bin:/home/user1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin \
    PWD=/proc/self/cwd \
  /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.d '-frandom-seed=bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.o' '-DBAZEL_CURRENT_REPOSITORY=""com_google_protobuf""' -iquote external/com_google_protobuf -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -iquote external/bazel_tools -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/bazel_tools -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-12.2' -g0 -w -g0 '-std=c++17' -c external/com_google_protobuf/src/google/protobuf/compiler/main.cc -o bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.o)
# Configuration: be1b6d30ad6b895c5d0cb4e11b08ee9fdfd7ce804f01eb0679a5e177ceb7e39b
# Execution platform: @local_execution_config_platform//:platform
In file included from external/com_google_protobuf/src/google/protobuf/compiler/main.cc:31:
external/com_google_protobuf/src/google/protobuf/compiler/cpp/generator.h:40:10: fatal error: 'string' file not found
#include <string>
         ^~~~~~~~
1 error generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/user1/tensorflow/tensorflow/tools/pip_package/BUILD:252:10 Middleman _middlemen/tensorflow_Stools_Spip_Upackage_Sbuild_Upip_Upackage-runfiles failed: (Exit 1): clang failed: error executing command (from target @com_google_protobuf//:protoc) 
  (cd /home/user1/.cache/bazel/_bazel_user1/2ed8e7afdea3ff827d1d2c14869018ce/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64 \
    PATH=/home/user1/.cache/bazelisk/downloads/sha256/6c25a6d716545d6b672ec46f770521cd9ebb63d73617b8f4e6747825d1db1839/bin:/home/user1/bin:/usr/local/cuda-12.2/bin:/home/user1/anaconda3/bin:/home/user1/anaconda3/condabin:/home/user1/.local/bin:/home/user1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin \
    PWD=/proc/self/cwd \
  /usr/lib/llvm-16/bin/clang -MD -MF bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.d '-frandom-seed=bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.o' '-DBAZEL_CURRENT_REPOSITORY=""com_google_protobuf""' -iquote external/com_google_protobuf -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -iquote external/bazel_tools -iquote bazel-out/k8-opt-exec-50AE0418/bin/external/bazel_tools -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/k8-opt-exec-50AE0418/bin/external/zlib -fmerge-all-constants -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wno-invalid-partial-specialization -fno-omit-frame-pointer -no-canonical-prefixes -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections '--cuda-path=/usr/local/cuda-12.2' -g0 -w -g0 '-std=c++17' -c external/com_google_protobuf/src/google/protobuf/compiler/main.cc -o bazel-out/k8-opt-exec-50AE0418/bin/external/com_google_protobuf/_objs/protoc/main.o)
# Configuration: be1b6d30ad6b895c5d0cb4e11b08ee9fdfd7ce804f01eb0679a5e177ceb7e39b
# Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 0.200s, Critical Path: 0.08s
INFO: 44 processes: 42 internal, 2 local.
FAILED: Build did NOT complete successfully


### Standalone code to reproduce the issue

```shell
bazel build //tensorflow/tools/pip_package:build_pip_package --verbose_failures
```


### Relevant log output

_No response_",False,"[-5.53065777e-01 -4.12844658e-01  1.26093805e-01  7.48067200e-02
  2.99863338e-01 -4.85668421e-01 -3.28892171e-01  6.33186996e-02
 -3.73369217e-01 -3.63737583e-01  8.65151063e-02 -1.36164188e-01
 -4.05467987e-01  1.43038005e-01 -6.61326498e-02  5.59161782e-01
 -2.02698112e-01 -2.48883069e-01  4.21735317e-01  1.72927529e-01
 -2.03512326e-01 -1.33117720e-01 -4.50414479e-01  1.50903538e-01
  2.03592435e-01  2.06757516e-01 -3.58279586e-01  1.32247388e-01
  1.03873402e-01  1.48676872e-01  6.87473059e-01  1.27547354e-01
  3.87231335e-02  9.66263115e-02  4.00407225e-01  2.75923550e-01
 -1.06482975e-01 -3.44260424e-01 -2.56738991e-01 -1.20887756e-01
 -7.53455609e-02 -4.87383120e-02  3.56603265e-02 -1.33242503e-01
 -7.08876699e-02 -1.91551968e-01  2.63591051e-01 -1.19296044e-01
 -5.14715761e-02 -5.18921852e-01  8.57507437e-03 -1.29759401e-01
 -4.62319970e-01 -2.29722410e-01  3.83974006e-03 -7.28718787e-02
  1.99227482e-01  4.45285663e-02  2.00575069e-01  2.84534305e-01
  1.54664055e-01  7.49121159e-02 -6.81066886e-02 -1.23940207e-01
  1.78476274e-01  1.37365997e-01  2.20633343e-01 -1.73468888e-01
  5.38808286e-01 -3.53010058e-01  1.89786270e-01 -5.16231209e-02
 -1.78041518e-01  1.69200093e-01  7.94591084e-02  1.20649815e-01
  1.28505334e-01  1.88750669e-01  2.54631370e-01 -2.13393897e-01
 -1.20347872e-01 -1.74727395e-01 -1.93082035e-01 -3.20949137e-01
  1.44051500e-02 -2.17323929e-01  2.79074043e-01  2.96319008e-01
  3.44577730e-01 -3.26879770e-01  4.10417497e-01  3.15271765e-01
 -5.90755492e-02  1.25991702e-01  3.92250240e-01  7.45230466e-02
  9.32246521e-02  1.59684047e-01  4.56322730e-02 -1.58482224e-01
 -7.19910413e-02 -4.10980046e-01  6.73244894e-02  1.10241875e-01
 -3.42868268e-01 -1.22269496e-01  8.62672031e-02 -1.47391791e-02
  1.47311047e-01 -1.48593962e-01  1.85068045e-03  2.25116834e-02
  1.41669288e-01 -7.76351988e-02 -5.29435538e-02  6.60340637e-02
 -3.59558195e-01 -5.10517582e-02 -1.20799560e-02  6.53456211e-01
 -6.00096658e-02 -1.18061438e-01  2.57085323e-01  8.09163377e-02
  4.76457179e-01  9.06987861e-02 -2.32497126e-01 -1.19321607e-01
  1.91672668e-01  1.85176373e-01  1.21580958e-02  2.57815540e-01
  2.52372362e-02  2.21138835e-01  1.51935086e-01  1.60454601e-01
 -1.83699161e-01 -2.31133312e-01  2.22708844e-03 -4.25924003e-01
 -2.19008356e-01  2.75818974e-01 -1.78289503e-01 -6.99148059e-01
  5.45635894e-02 -5.02992608e-02 -1.91618606e-01  3.90211672e-01
 -2.17247829e-01  1.60914630e-01 -1.22961104e-01  2.30345890e-01
 -6.09248169e-02  2.44886249e-01  1.28183272e-02  1.39877737e-01
  5.30278027e-01 -1.42718837e-01 -2.50081867e-01 -4.28357601e-01
  3.73015776e-02  5.52321553e-01 -8.91606584e-02 -1.54909804e-01
  4.66129892e-02  9.05250460e-02 -6.17997766e-01 -3.90727162e-01
  3.30585927e-01  5.09889066e-01 -4.20149006e-02 -4.95897084e-02
  3.22118938e-01  1.64014071e-01  3.84625584e-01 -1.53953239e-01
  7.17835784e-01 -4.46507365e-01 -2.01772213e-01  6.19127274e-01
  2.85762846e-01 -7.44678006e-02  1.87208593e-01  2.76323497e-01
  3.18558693e-01 -7.28957430e-02  1.28724873e-01 -3.22354734e-02
 -2.11755395e-01  1.23388261e-01 -5.68957746e-01  5.55254053e-03
  3.27916056e-01 -2.57834375e-01 -3.33815902e-01  2.46071726e-01
  2.90458590e-01  2.74881460e-02  6.03332445e-02 -3.12429257e-02
 -1.65975109e-01 -1.24541581e-01 -2.19398588e-01  1.68265283e-01
 -4.68251631e-02 -2.31335863e-01 -1.00289091e-01 -2.98677742e-01
 -5.54640055e-01  4.36357222e-03  9.87594873e-02 -4.97134030e-01
  2.50001013e-01 -2.90106423e-02 -3.88816595e-01  3.26965839e-01
  1.34569645e-01 -8.97067636e-02 -2.11399794e-02  2.38183260e-01
  1.25760078e-01 -2.82866538e-01 -1.40656754e-02 -3.58426332e-01
 -1.17182076e-01 -4.15769182e-02 -3.20301771e-01  7.14086555e-03
  4.79987152e-02  1.69189587e-01 -4.05805968e-02 -7.45566711e-02
  3.92731369e-01  1.20897584e-01  4.33152318e-01 -1.07017092e-01
 -1.21180743e-01 -2.22194135e-01 -4.77339834e-01  4.97569367e-02
 -3.56970608e-01 -1.96242899e-01  1.80026576e-01  8.62110704e-02
  3.51019919e-01  4.03398871e-01 -1.87404662e-01 -9.28556770e-02
 -4.09108102e-01  3.67564440e-01  2.63367482e-02  2.27161318e-01
  1.87371880e-01  2.12356985e-01  4.98570800e-01  3.22182894e-01
  1.31789401e-01  2.62508273e-01  3.53072852e-01 -2.58520782e-01
  1.80996627e-01  2.51464456e-01 -5.50643131e-02  1.93836823e-01
  1.52942032e-01  4.72921491e-01 -4.09038961e-01  4.56410438e-01
  5.28916717e-02 -1.58268854e-01  2.26705134e-01 -3.08340609e-01
  7.36327887e-01 -5.00644803e-01 -1.58829704e-01 -8.80584419e-02
  4.52326983e-01 -1.06621362e-01  8.58139917e-02 -5.12497909e-02
  8.81799757e-02  4.72969592e-01 -1.82342857e-01  1.04102708e-01
  1.55658811e-01  3.92274279e-03  9.68510285e-02 -1.09086084e+00
 -3.95537883e-01  1.25135764e-01 -3.32770705e-01  1.99235231e-01
 -7.26813078e-02 -2.75654253e-02 -2.63778806e-01 -8.99378583e-03
  5.59103861e-02 -6.47103265e-02  8.46064538e-02  1.76164195e-01
 -1.77811757e-01  9.98407230e-02  4.57838953e-01 -4.64424431e-01
 -1.15377903e-01 -1.19114108e-01  1.62985757e-01  2.85613328e-01
  5.55920720e-01 -4.94608104e-01  1.22617930e-05 -1.14496790e-01
 -2.28158347e-02  3.73089135e-01  2.20626630e-02  2.05554396e-01
 -2.70400614e-01  6.36932850e-01  2.04408109e-01 -3.13928902e-01
  5.00962973e-01 -2.03940958e-01 -3.47393572e-01 -2.29299739e-01
  2.61718512e-01 -2.49272197e-01 -2.04217225e-01 -5.08797050e-01
  1.38415515e-01  2.01300353e-01 -1.52509928e-01  2.69504189e-02
  2.04089265e-02  2.13793054e-01 -4.55010712e-01 -8.44554752e-02
 -4.49506879e-01  3.00446779e-01 -6.72520548e-02 -3.67890388e-01
 -1.16433889e-01 -2.22666830e-01 -1.25354648e-01 -1.93697751e-01
 -7.97010958e-02 -2.59797752e-01  5.05590022e-01  3.14422429e-01
 -2.65424371e-01  7.60015249e-02 -6.18704110e-02  2.48546630e-01
 -5.30323029e-01  7.47061372e-02  1.89013645e-01  1.47176921e-01
 -1.16246700e-01  4.20107692e-02  5.51365376e-01  3.63955021e-01
 -1.87163323e-01  3.14193428e-01 -3.31941009e-01 -6.09393008e-02
  1.12696290e-01 -2.17027575e-01 -4.62370247e-01 -4.31458838e-02
  5.49074970e-02  4.03605491e-01 -3.78077477e-03  3.76144320e-01
 -6.09048605e-01  1.82575792e-01  7.02444315e-01 -4.71908152e-01
 -4.02339756e-01  3.18192571e-01  1.24911025e-01 -2.93294460e-01
  9.62405056e-02 -1.77275583e-01  3.61683309e-01 -1.98732279e-02]"
TensorFlow.js: export failure  0.6s: [Errno 2] No such file or directory: 'tensorflowjs_converter' stat:awaiting response stale TFLiteConverter,"System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MAC OS
TensorFlow installed from (source or binary): binray
TensorFlow version (use command below): 2.13.0
TensorFlowJS version: 4.11.0
Python version: 3.9
When I using tensorflowjs_converter to convert pt model:

TensorFlow SavedModel: export success  29.7s, saved as yolov5n-seg_saved_model (7.9 MB)

TensorFlow GraphDef: starting export with tensorflow 2.13.0...
TensorFlow GraphDef: export success  3.7s, saved as yolov5n-seg.pb (7.9 MB)
WARNING  invalid check_version(4.11.0, ) requested, please check values.

TensorFlow.js: starting export with tensorflowjs 4.11.0...
TensorFlow.js: export failure  0.6s: [Errno 2] No such file or directory: 'tensorflowjs_converter'

tensorflowjs_converter not founded",False,"[-0.23515469 -0.71847093 -0.19448793  0.11750519  0.2390338  -0.20341209
 -0.00880331 -0.07604973 -0.06334369 -0.21586707  0.15412794 -0.12511803
 -0.17836335  0.28655362 -0.4068524   0.21569678 -0.32582757 -0.14040563
  0.29728746 -0.04042608 -0.20990533 -0.00604501 -0.09718088  0.2612629
  0.30388552  0.2960244  -0.10841045 -0.29183763  0.03739249  0.10519237
  0.21319297 -0.11630662 -0.08263861 -0.02451192 -0.14681797  0.2921495
 -0.33326635 -0.01015613 -0.263941   -0.1360841   0.23946944  0.07014122
  0.19151148  0.06361339  0.09334729  0.11869201  0.23701943 -0.07451631
 -0.07692228 -0.03040911 -0.06335235 -0.02444078 -0.3571551  -0.22241314
 -0.06164169  0.09710468  0.2321425   0.0456692  -0.03167288  0.07386457
  0.03299725  0.11646398 -0.2763004  -0.17008412  0.0301259   0.09972088
  0.2671128  -0.3553025   0.3753732  -0.24957481  0.03528862 -0.06894558
 -0.1848097   0.23541293 -0.05037531 -0.01986609  0.05859848  0.26901948
  0.20624503 -0.07086208 -0.10198788 -0.174484    0.14946401 -0.03937404
  0.19980726  0.07368514  0.23849767  0.00278862  0.3592813  -0.04111081
  0.4930041   0.4382136   0.0178884   0.03002629  0.3914547  -0.00263123
 -0.07929677  0.54234177  0.30257863 -0.03152478 -0.11320901 -0.24635804
 -0.2137461   0.10815519  0.10942566 -0.17757541  0.25918067  0.01063679
  0.05196821 -0.02268337  0.0177455  -0.14222081  0.22286287  0.0242015
  0.2117652  -0.04675891 -0.11981679  0.3424644  -0.22015809  0.9623189
 -0.01857425 -0.0658081   0.27875698  0.11434533  0.10702774 -0.08053421
 -0.20078632 -0.09687644  0.09795146  0.10178003  0.33422834  0.09895052
 -0.4159097   0.07004818 -0.11359811 -0.07415468 -0.14481002 -0.20534256
 -0.4146101  -0.00597196 -0.24438782  0.1931612  -0.18946582 -0.2760808
  0.12900928  0.0947151  -0.0174274   0.30284148  0.07206634  0.2614805
 -0.15540004 -0.09426658 -0.00316565  0.37173522  0.17595991  0.23938966
  0.30032963 -0.11736815 -0.07147717 -0.56308746 -0.03268473  0.19251311
 -0.24549222 -0.24858466  0.11661366  0.19211562 -0.522363   -0.22219552
 -0.06589839  0.23171657 -0.13184953 -0.08380485 -0.06371636  0.16543637
  0.22032373 -0.0861236   0.5867455  -0.52722514 -0.1349963  -0.05765151
  0.02955866  0.04686056 -0.07544394  0.03455659 -0.09285931 -0.006093
  0.14796372 -0.03236985 -0.1746417   0.1415354  -0.17140493 -0.11711645
  0.32492226 -0.02246787 -0.04315797 -0.11825461 -0.04909523 -0.09855506
 -0.09499182  0.19379348 -0.16412474 -0.15726973  0.1711196  -0.07277509
  0.05995875 -0.37569642 -0.22351421 -0.21622261 -0.6130033  -0.01969955
  0.22162238 -0.5430181   0.0982278   0.06608845 -0.321459   -0.06547132
  0.01278383 -0.03169163 -0.3649618   0.12391041 -0.19709572 -0.02040115
 -0.02670235 -0.191871   -0.3314067  -0.14912066 -0.25302237  0.3543639
 -0.02718676  0.3878175  -0.0997185   0.3670673   0.18973088  0.0928033
  0.13834375 -0.294611   -0.15446933 -0.07938512 -0.38066766 -0.04057968
 -0.2746542   0.11944495 -0.11336184 -0.1366115  -0.13498414  0.34916657
  0.09145205 -0.17980854 -0.2943519   0.35799813 -0.16393095  0.12983036
  0.36544496  0.33524957  0.44938612  0.2133238   0.03424391 -0.259601
  0.10613494  0.19940935  0.22172743  0.17020068  0.00964061  0.6129412
  0.00617554  0.2691798  -0.21202332  0.21477431 -0.06501658  0.01968299
  0.00381299 -0.16156195  0.22884777 -0.4265014   0.04007806 -0.08480367
  0.24626279  0.19356634 -0.05881615  0.14708346  0.08512095  0.39935192
 -0.40300626 -0.10502243 -0.21928859 -0.30673778  0.10271475 -0.25680068
 -0.16650575  0.2334276   0.00886737  0.1557413   0.30890632 -0.00148896
 -0.33506316 -0.05783834  0.17939922 -0.00865689  0.0139023   0.15923855
 -0.16340989  0.08988884  0.28361672 -0.3408337   0.17832738 -0.18083669
  0.41284272  0.14984423  0.3658597  -0.4510165   0.37976444 -0.20954245
  0.02308816  0.39789778  0.0553227   0.03503251 -0.22310358  0.46119
  0.24075092 -0.07483834 -0.08801641 -0.02060066 -0.20965359  0.0234334
  0.1120528   0.1378543   0.19756946 -0.17455967 -0.08905552  0.03767414
 -0.254469   -0.23950788 -0.2407691   0.12987126 -0.06366045 -0.04498063
 -0.18602999  0.3316263   0.05612535 -0.37677717  0.17548478 -0.02231535
 -0.10494193 -0.4282892   0.04491461 -0.28252918  0.20350368  0.40106165
 -0.17482114  0.02048402 -0.13123168  0.05549724 -0.06630558 -0.09702828
 -0.16484419  0.36891484 -0.02913736 -0.09275341  0.22815812  0.645556
 -0.02886782 -0.09928808 -0.23851879 -0.22285059 -0.00702979 -0.16410705
 -0.29898632 -0.05030478  0.22211665  0.44316965  0.03422901  0.31371874
 -0.23877859  0.11834577  0.39774567 -0.38043094 -0.28703645  0.04547883
  0.08573958 -0.20060252 -0.12845406  0.0106526   0.07687438 -0.2414656 ]"
TF 2.12: InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version type:build/install subtype: ubuntu/linux TF 2.12,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.12

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

GTX970

### Current behavior?


I have install `pip install tensorflow ==2.12`. 
I followed the page: https://www.tensorflow.org/install/pip
and install 
`conda install -c conda-forge cudatoolkit=11.8.0` and
`nvidia-cudnn-cu11==8.6.0.163`
From the build from source it says that it requires cuda 11.8 
This is determined here `nvcc -version`.
```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Sep_21_10:33:58_PDT_2022
Cuda compilation tools, release 11.8, V11.8.89
Build cuda_11.8.r11.8/compiler.31833905_0
```

I tried to see if it is using GPU and it does. 

``` 2023-09-22 13:17:18.164612: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-22 13:17:18.813461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-09-22 13:17:21.116323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.117135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.117901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.118662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.141750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.142582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.143350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.144117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.144878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.145642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.146398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-09-22 13:17:21.147155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')] ```

However, when I tried to create a neural network: 
It gives me the following error: 

```shell
Traceback (most recent call last):
  File ""/home/trevor/distinguisher/main.py"", line 73, in <module>
    net_pp = train_preprocessor_triplet_loss(n=10000, nr=num_rounds, epochs=epoch)
  File ""/home/trevor/distinguisher/main.py"", line 53, in train_preprocessor_triplet_loss
    net_pp = make_resnet_preprocess(depth=1)
  File ""/home/trevor/distinguisher/main.py"", line 26, in make_resnet_preprocess
    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm)
  File ""/home/trevor/anaconda3/envs/tf_2/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/trevor/anaconda3/envs/tf_2/lib/python3.9/site-packages/keras/backend.py"", line 2101, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version
```

I have searched online and say that the CUDA version is too high but from build it seems to be 11.8.
May I know what is the issue here? 







### Standalone code to reproduce the issue

```shell
def make_resnet_preprocess(num_blocks=2, num_filters=32, num_outputs=1, d1=64, d2=64, word_size=16, ks=3, depth=5,
                           reg_param=0.0001, final_activation='sigmoid'):
    # Input and preprocessing layers
    inp = Input(shape=(num_blocks * word_size * 2,))
    rs = Reshape((2 * num_blocks, word_size))(inp)
    perm = Permute((2, 1))(rs)
    # add a single residual layer that will expand the data to num_filters channels
    # this is a bit-sliced layer
    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm)
    conv0 = BatchNormalization()(conv0)
    conv0 = Activation('relu')(conv0)
    # add residual blocks
    shortcut = conv0
    for i in range(depth):
        conv1 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(shortcut)
        conv1 = BatchNormalization()(conv1)
        conv1 = Activation('relu')(conv1)
        conv2 = Conv1D(num_filters, kernel_size=ks, padding='same', kernel_regularizer=l2(reg_param))(conv1)
        conv2 = BatchNormalization()(conv2)
        conv2 = Activation('relu')(conv2)
        shortcut = Add()([shortcut, conv2])
    # add prediction head
    flat1 = Flatten()(shortcut)
    dense1 = Dense(d1, kernel_regularizer=l2(reg_param))(flat1)
    dense1 = BatchNormalization()(dense1)
    dense1 = Activation('relu')(dense1)
    dense2 = Dense(d2, kernel_regularizer=l2(reg_param))(dense1)
    dense2 = BatchNormalization()(dense2)
    out = Activation('relu')(dense2)
    # out = Dense(num_outputs, activation=final_activation, kernel_regularizer=l2(reg_param))(dense2)
    model = Model(inputs=inp, outputs=out)
    return (model)
net_pp = train_preprocessor_triplet_loss(n=10 ** 7, nr=num_rounds, epochs=epoch)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/trevor/distinguisher/main.py"", line 73, in <module>
    net_pp = train_preprocessor_triplet_loss(n=10000, nr=num_rounds, epochs=epoch)
  File ""/home/trevor/distinguisher/main.py"", line 53, in train_preprocessor_triplet_loss
    net_pp = make_resnet_preprocess(depth=1)
  File ""/home/trevor/distinguisher/main.py"", line 26, in make_resnet_preprocess
    conv0 = Conv1D(num_filters, kernel_size=1, padding='same', kernel_regularizer=l2(reg_param))(perm)
  File ""/home/trevor/anaconda3/envs/tf_2/lib/python3.9/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/trevor/anaconda3/envs/tf_2/lib/python3.9/site-packages/keras/backend.py"", line 2101, in random_uniform
    return tf.random.stateless_uniform(
tensorflow.python.framework.errors_impl.InternalError: cudaGetDevice() failed. Status: CUDA driver version is insufficient for CUDA runtime version```
```
",False,"[-0.328117   -0.74315816 -0.26472142  0.13459614  0.19219604 -0.5111783
 -0.2741024   0.23119844 -0.41105834 -0.33479816  0.03783827 -0.10630576
 -0.18408614  0.06655122 -0.3364381   0.33085525 -0.0449566  -0.18786411
  0.2512466   0.10879615 -0.42313385  0.053071   -0.24969476  0.0792312
  0.17541645  0.02273256 -0.10122369 -0.14336145  0.04693578 -0.01559047
  0.34816802  0.07053486 -0.03284764  0.09844212  0.03267774  0.10638513
 -0.35502365 -0.393111   -0.20213819 -0.08771513 -0.15516694 -0.05973661
  0.03301008 -0.09328232 -0.12389424 -0.09364484  0.08314434 -0.02099783
 -0.16409329 -0.22170572  0.20876604 -0.07404582 -0.12175493 -0.24552542
  0.10535091 -0.0442727  -0.17601287  0.23283051  0.10337125  0.29478747
  0.3054113  -0.00822917  0.08525869 -0.16229865  0.11035518  0.27530992
  0.20575902 -0.26784796  0.57748556 -0.34785423  0.0651256   0.08743247
 -0.24332875 -0.01819772 -0.13171762  0.14940989  0.36565995  0.2023803
  0.07697292 -0.08613116  0.04118066  0.15954922  0.10115227 -0.26716864
  0.10017465 -0.17345493  0.3498537   0.22603719  0.17651671 -0.28430462
  0.41953814  0.5462673  -0.01324106  0.24556823  0.21608627 -0.01126659
  0.13294105  0.2634742  -0.27328628 -0.26325923 -0.05276557 -0.26836163
 -0.0938561   0.3887765  -0.26056826 -0.04918556  0.27528524 -0.07275914
 -0.11712754 -0.04440109  0.20188269 -0.13764143  0.20234682  0.0455958
  0.07894827 -0.13561286 -0.47046816 -0.12719434  0.09017573  0.5501344
 -0.14074028 -0.14991334  0.08647473 -0.00307759  0.54431117  0.17016098
  0.07074433 -0.09857626 -0.0679687   0.19987491  0.06804357 -0.00883847
  0.04935335  0.1783052  -0.20484635  0.30446365 -0.37652907 -0.24910408
 -0.19922738 -0.2459338  -0.25074714  0.2329827   0.00090234 -0.38456395
 -0.01325627  0.10064159 -0.08664508  0.291212    0.00478178  0.36293966
 -0.042519    0.00164323 -0.1650236   0.39218426  0.08320758  0.29914573
  0.484594   -0.21136717 -0.18174262 -0.7142314   0.23978657  0.44445306
 -0.10928649 -0.14860092 -0.12079819  0.20397517 -0.47962326 -0.14339516
  0.1284218   0.45064372 -0.24253991 -0.29445338  0.24062245  0.02208544
  0.07146472 -0.2416849   0.19776805 -0.32768473 -0.00163496  0.36574462
  0.04459616  0.14250468  0.26584178  0.18381116  0.23439892  0.17545143
  0.0746756   0.07562499 -0.3261335  -0.12647729 -0.27413282 -0.2716138
  0.06818511 -0.12011158 -0.11028934  0.05091241  0.16701244  0.00999621
  0.13369149  0.05136118 -0.15767685  0.06035249 -0.07219936  0.17729825
 -0.06507127 -0.29029483 -0.18472049 -0.23938939 -0.38513178 -0.13381004
 -0.08084081 -0.43839592  0.18508266 -0.16247137 -0.3409527   0.13384691
  0.13292608  0.15524049 -0.09946071  0.38908303  0.20247515 -0.10022153
  0.24931921 -0.37064475  0.13113028  0.00746607 -0.00672291 -0.0942113
  0.06624877  0.11557955 -0.0407276   0.15274192  0.02965722  0.22207732
  0.17547205  0.02980055 -0.11697315 -0.16213608 -0.18351926  0.07957377
 -0.4042149   0.11426947 -0.02449255 -0.04447547  0.4568278   0.525651
 -0.05797037  0.02005734 -0.4726712   0.2761088  -0.02266855  0.0413685
  0.04837146 -0.09637821  0.2989602  -0.06567304  0.25868583  0.08946244
  0.19300738  0.0356824   0.25804532  0.29983872 -0.10012068  0.3250753
  0.1833888   0.4782718  -0.3686486   0.5787744  -0.11187886 -0.0584957
  0.02687908 -0.30318323  0.6732204  -0.31341165  0.044695   -0.08956102
  0.5922289   0.01998026  0.1679793  -0.02550647  0.34715188  0.02560107
 -0.16504812 -0.0047096   0.07052386 -0.28686205 -0.22489783 -0.5066171
 -0.29196376 -0.08323894 -0.3389071   0.29699558  0.09426153 -0.0483326
 -0.34921652  0.03346632 -0.04541251  0.18884069  0.14198789  0.019739
 -0.31887883 -0.06731614  0.36069477 -0.49489024 -0.1017794  -0.24916089
  0.24756047  0.24689949  0.4758878  -0.26398975  0.1378807  -0.1270931
 -0.13078678  0.4989174   0.07556757  0.14751257 -0.25513622  0.5684675
  0.32232288 -0.20550215  0.05475731 -0.40903246 -0.4939947  -0.1312041
  0.26381811 -0.01332094 -0.14968899 -0.15818319  0.04505678  0.5464649
 -0.10703278  0.04739361 -0.02626035  0.15040226 -0.30277437  0.06150889
 -0.30457914 -0.02105315  0.07667251 -0.23479296 -0.11686671 -0.14600366
 -0.47388327 -0.2789445  -0.03899781 -0.25073406  0.43331507  0.5218044
 -0.23122597  0.01305499 -0.16063383  0.17035031 -0.3211614  -0.10485698
 -0.13226056  0.39332002  0.22569431  0.01554735  0.36803168  0.1841049
 -0.00303324  0.34042224 -0.2417655  -0.13883662  0.12815171 -0.26813892
 -0.3420819   0.0571317   0.22413127  0.30562526 -0.25019765  0.2515572
 -0.12582296  0.18143633  0.60887223 -0.35890642 -0.30508965  0.20559779
  0.09507232  0.04986964  0.12483118 -0.1284452   0.3822748  -0.07435499]"
The metrics values in fit and in evaluate do not match (tf+keras) type:support comp:keras TF 2.9,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have the issue which I described under in detail. Might be it's a my mistake but I try to fix it not a one day.

I simplified it as much as possible fitting the model to show you exactly an issue.

Let it be not a pre-trained model ResNet50. And let's I provide a dataset by easy way where X_train and Y_train are the ndarray with correspond shapes [N, 32, 32, 3] and [N, class_num=3].

I set batch_size=128, shuffle=False. Now I fitting my model and after the last epoch I get the metric equals 0.976, BUT If I just make model.evaluate(X_train), where X_train is the same data I use in fit I get absolutely different value - 0.456. The question is - WHY?

That's logs of my fitting: 263/263 [==============================] - 7s 27ms/step - loss: 0.2063 - auc: 0.9899 - mc_f1: 0.9326

That's after evaluate: 1052/1052 [==============================] - 11s 9ms/step - loss: 0.6186 - auc: 0.9053 - mc_f1: 0.4993

To prevent questions - mc_f1 is a custom metric - averaged f1 calculated for each class separately for the multiclass case.

So the code:

    # Change last layers
    last_layer = model.output
    output = tf.keras.layers.Dense(classes, activation=""softmax"")(last_layer)
    model = tf.keras.models.Model(inputs=model.inputs, outputs=output)

    return model


mcf1 = MulticlassF1(num_classes=3)

resnet50 = get_resnet50_model([32, 32, 3], 3)
resnet50.compile(
    optimizer=Adam(),
    loss=tf.keras.losses.CategoricalCrossentropy(),
    metrics=[tf.metrics.AUC(name='auc'), mcf1],
)
```


Fitting:

```
deb_metric_callback = DebugMetricCallback(resnet50, (X_train, Y_train), mcf1)
resnet50.fit(
    X_train, Y_train,
    epochs=100,
    batch_size=128,
    # verbose=0,
    shuffle=False,
    callbacks=[deb_metric_callback],
)
```

I have the issue which I described under in detail. Might be it's a my mistake but I try to fix it not a one day.

I simplified it as much as possible fitting the model to show you exactly an issue.

Let it be not a pre-trained model ResNet50. And let's I provide a dataset by easy way where X_train and Y_train are the ndarray with correspond shapes [N, 32, 32, 3] and [N, class_num=3].

I set batch_size=128, shuffle=False. Now I fitting my model and after the last epoch I get the metric equals 0.976, BUT If I just make model.evaluate(X_train), where X_train is the same data I use in fit I get absolutely different value - 0.456. The question is - WHY?

That's logs of my fitting: 263/263 [==============================] - 7s 27ms/step - loss: 0.2063 - auc: 0.9899 - mc_f1: 0.9326

That's after evaluate: 1052/1052 [==============================] - 11s 9ms/step - loss: 0.6186 - auc: 0.9053 - mc_f1: 0.4993

To prevent questions - mc_f1 is a custom metric - averaged f1 calculated for each class separately for the multiclass case.

So the code:

The metric:
```

class MulticlassF1(tf.keras.metrics.Metric):

    def __init__(self, name='mc_f1', num_classes=None, **kwargs):
        super(MulticlassF1, self).__init__(name=name, **kwargs)
        self.__zero_support = tf.cast(1e-7, dtype=tf.float16)
        self.__cm = self.add_weight(name='fn', initializer='zeros', shape=[num_classes, num_classes])
        if num_classes is not None:
            self.__num_classes = num_classes

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_pred = K.argmax(y_pred, axis=1)
        y_true = K.argmax(y_true, axis=1)
        m = tf.math.confusion_matrix(y_true, y_pred, num_classes=self.__num_classes, dtype=tf.float32)
        self.__cm.assign_add(m)

    def reset_state(self):
        self.__cm.assign(tf.zeros((self.__num_classes, self.__num_classes)))

    def result(self):
        denominator = 0
        m = self.__cm
        for i in range(m.shape[0]):
            tp = m[i, i]
            fn = K.sum(m[:, i]) - tp
            fp = K.sum(m[i, :]) - tp
            tn = K.sum(K.flatten(m)) - (tp + fn + fp)
            tp = K.cast(tp, dtype=tf.float16)
            tn = K.cast(tn, dtype=tf.float16)
            fp = K.cast(fp, dtype=tf.float16)
            fn = K.cast(fn, dtype=tf.float16)
            precision = tp / ((tp + fp) + self.__zero_support) + self.__zero_support
            recall = tp / (tf.cast(tp + fn, dtype=tf.float16) + self.__zero_support) + self.__zero_support
            denominator += (1 / precision + 1 / recall)
        f1_combined = K.cast(2 * m.shape[0] / denominator, dtype=tf.float32)
        return f1_combined
```
The model declaration

```
def get_resnet50_model(window_size, classes):
    # Model initialization
    model = tf.keras.applications.ResNet50V2(
        include_top=False,
        weights=None,
        input_tensor=None,
        input_shape=window_size,
        pooling='max',
        classes=classes,

    )

    # Change last layers
    last_layer = model.output
    output = tf.keras.layers.Dense(classes, activation=""softmax"")(last_layer)
    model = tf.keras.models.Model(inputs=model.inputs, outputs=output)

    return model


mcf1 = MulticlassF1(num_classes=3)

resnet50 = get_resnet50_model([32, 32, 3], 3)
resnet50.compile(
    optimizer=Adam(),
    loss=tf.keras.losses.CategoricalCrossentropy(),
    metrics=[tf.metrics.AUC(name='auc'), mcf1],
)
```
Fitting:

```
deb_metric_callback = DebugMetricCallback(resnet50, (X_train, Y_train), mcf1)
resnet50.fit(
    X_train, Y_train,
    epochs=100,
    batch_size=128,
    # verbose=0,
    shuffle=False,
    callbacks=[deb_metric_callback],
)
```
I tried to do some debug and make a CallBack:

```
class DebugMetricCallback(Callback):
    def __init__(self, test, metr):
        super().__init__()
        self.test = test
        self.metric = metr


    def on_epoch_end(self, epoch, logs=None):
        print(self.metric.result())
        x, y = self.test
        y_pred = self.model.predict(x)
        inline_measure = MulticlassF1(num_classes=3)
        inline_measure.update_state(y, y_pred)
        print(self.metric.result().numpy(), inline_measure.result().numpy())
```

Here I get absolutely different confusion matrices.

Also I thought that this is the problem with BatchNormalization, but I save and compare model before and after evaluate it wasn't changed.

### Standalone code to reproduce the issue

```shell
https://github.com/xxraytz/temp.git
```


### Relevant log output

_No response_",False,"[-2.80056894e-01 -4.35945898e-01 -2.62063503e-01  9.97284651e-02
  2.84357518e-02 -4.52132821e-01  1.17373511e-01 -6.03919327e-02
 -5.41635573e-01 -3.79984617e-01  2.80495673e-01 -2.47303739e-01
 -1.55407786e-01  9.05384272e-02 -2.72660911e-01  2.18481183e-01
 -1.84436724e-01 -1.55001264e-02 -1.71756074e-02 -1.23204859e-02
 -8.12039301e-02 -4.84965444e-02 -2.27855489e-01  3.77452135e-01
  8.44090953e-02  2.22114235e-01 -2.16092795e-01  9.76063162e-02
 -3.01361606e-02  1.15771048e-01  2.08946869e-01  4.19790149e-02
 -1.24718562e-01  2.86345650e-02  3.66586596e-02  3.86804163e-01
  1.13101736e-01 -9.98428315e-02 -4.34145689e-01 -1.25442699e-01
 -1.23110879e-02  1.12085223e-01  2.27648094e-01 -1.77453980e-01
  2.56034881e-01 -2.07813203e-01 -1.27351079e-02 -2.46007532e-01
 -7.58384764e-02 -1.94573432e-01  1.30501939e-02 -1.93956017e-01
 -4.48706508e-01 -4.49371696e-01 -1.46830469e-01  3.20675187e-02
  9.42265093e-02 -7.63480961e-02 -1.54866338e-01 -1.81532875e-01
  2.28384491e-02  3.96266803e-02 -4.72657308e-02 -6.52600005e-02
  1.59978047e-01  9.50749815e-02  3.49076092e-01  5.84832653e-02
  1.96213409e-01  4.64403257e-02  6.45699948e-02 -2.24636532e-02
 -4.72559690e-01  2.92223871e-01  2.13920280e-01  2.95051992e-01
  1.95418045e-01  1.35262266e-01  3.25043201e-01  3.91969681e-02
  3.02396566e-02 -2.00123161e-01 -2.14656234e-01 -2.04063162e-01
  2.70043109e-02 -1.77082211e-01  4.59899247e-01  9.82114822e-02
  5.55032074e-01 -3.71981859e-01  5.39236426e-01  3.70227963e-01
 -3.41026813e-01  8.66364762e-02  5.80395162e-01  2.15227693e-01
 -6.16601855e-03  2.98103318e-03  6.75790831e-02 -6.49433434e-02
 -6.80133775e-02 -8.23553950e-02  4.33219969e-02  1.07482970e-01
 -1.63783103e-01 -1.61380738e-01 -7.21504986e-02 -8.54002982e-02
  2.53895760e-01  2.00693458e-02  1.45983487e-01 -1.46543249e-01
  1.08871877e-01  2.53219694e-01 -1.54275149e-01 -1.39048249e-01
  2.81981647e-01  1.91678107e-01  1.95977584e-01  7.08874345e-01
 -1.92435324e-01 -1.76075205e-01  2.54206777e-01  1.56062454e-01
  5.10786891e-01  2.88589925e-01 -2.07890928e-01  8.78434852e-02
 -7.89470524e-02 -5.48502505e-02  1.46282062e-01 -9.75054130e-02
  1.16717450e-01  7.05671906e-02 -3.16830605e-01 -1.10441476e-01
 -1.25109730e-02 -3.19674909e-02 -3.16298306e-01 -2.62997508e-01
 -2.41969302e-01  1.37373686e-01 -1.74947772e-02 -3.71414602e-01
  1.38344720e-01  2.04972684e-01 -5.32226205e-01  1.16210938e-01
 -2.91167200e-01  2.21294332e-02  6.60462230e-02 -3.07889283e-03
  1.02316022e-01 -1.44448534e-01  1.45624638e-01  1.38599306e-01
  2.32815161e-01 -2.52002180e-01 -1.19671449e-01 -4.10461217e-01
  4.08755317e-02  1.60881281e-01 -3.15047175e-01 -2.06078351e-01
  2.08911926e-01  2.97990710e-01 -2.43264794e-01 -3.00559133e-01
  1.85359836e-01  3.57742459e-01  1.30147159e-01 -1.43394977e-01
  1.38800547e-01  2.33476102e-01  3.87555420e-01 -5.76005429e-02
  1.43685907e-01 -5.71626902e-01 -3.06230903e-01  2.74763286e-01
  1.67839944e-01 -5.53069375e-02  1.37976378e-01  2.60728359e-01
  1.27012700e-01  4.74957190e-03  8.70930403e-02  2.01960042e-01
 -3.45163554e-01  2.49193814e-02 -1.97600111e-01 -4.01423156e-01
  3.08177799e-01 -1.35320500e-01 -6.99949265e-02 -5.59979640e-02
  2.34488785e-01 -1.57264709e-01 -1.49167985e-01 -3.09929699e-02
 -2.35362917e-01 -1.19379029e-01 -1.58404514e-01 -1.83935359e-01
  1.42616704e-01 -4.48071301e-01  4.73464131e-02 -4.54385757e-01
 -3.43900621e-01  4.34358954e-01  1.41567528e-01 -5.51957667e-01
  1.63851187e-01 -7.02599660e-02 -3.13731492e-01 -7.14896768e-02
  2.19646171e-01 -4.48062085e-02 -2.81931221e-01  5.58780015e-01
 -3.13537717e-02 -2.99437165e-01 -1.29897922e-01 -3.55348170e-01
 -4.52796221e-01  9.34247300e-02 -3.36025268e-01  1.88284665e-01
  5.17461523e-02  8.14401805e-02  1.46539301e-01  4.50706556e-02
  6.01255417e-01  1.88672125e-01  4.36667442e-01 -3.15681219e-01
 -1.93917722e-01 -2.95736790e-01 -2.08133921e-01 -1.56758845e-01
 -6.04724944e-01 -6.95088878e-02  2.41862223e-01 -9.02293101e-02
  4.11980629e-01  4.80332494e-01 -1.91629052e-01  1.52161419e-01
 -4.84381437e-01  5.74500859e-03 -4.05813694e-01  8.11977610e-02
  2.71617949e-01  1.61000807e-02  4.73605812e-01  9.94331539e-02
  3.56823742e-01  2.21480832e-01  2.76397854e-01 -2.88968503e-01
  4.84206975e-01  1.19164601e-01 -1.37096886e-02  6.62959635e-01
  3.02038789e-01  5.07034600e-01 -3.33323032e-01  4.42431569e-01
 -1.18689597e-01 -3.87663245e-01  3.45958948e-01 -4.76928264e-01
  6.21651709e-01 -4.95017976e-01  2.25634798e-01 -1.12054646e-01
  2.73209035e-01  2.61727452e-01 -9.22921821e-02  1.97841730e-02
  1.66521102e-01  1.98918402e-01 -7.57761121e-01 -1.36707947e-01
 -3.70147556e-01 -1.82860538e-01  6.11879751e-02 -4.57611799e-01
  1.69774145e-02  1.44896396e-02 -4.00417484e-04  7.09091425e-02
 -5.51093854e-02 -5.95670659e-03 -1.05572022e-01 -2.16630958e-02
 -1.30188875e-02 -1.70661248e-02  1.85922623e-01  1.67397499e-01
 -3.34309153e-02  1.92648023e-01  4.48572069e-01 -2.97975421e-01
 -1.44008458e-01 -1.53369885e-02  4.14799333e-01  3.00887406e-01
  4.84956175e-01 -1.36733294e-01  8.95002484e-02 -4.44495566e-02
 -3.23737320e-03  6.79461360e-01  1.19310714e-01  3.98233049e-02
 -4.03572500e-01  5.60905039e-01  7.77613074e-02  1.41629040e-01
  1.18287481e-01 -1.97862327e-01 -4.50986415e-01  1.90192014e-01
 -2.11657107e-01 -1.33581966e-01  2.53084540e-01 -6.33777022e-01
 -1.09933585e-01  2.19950780e-01 -2.55438924e-01 -1.44231081e-01
 -2.33781934e-01  1.33167893e-01 -1.41985059e-01 -6.90177083e-02
 -6.20322466e-01  4.32542652e-01 -3.68855596e-02 -2.46208221e-01
 -1.51049465e-01  2.76424102e-02 -2.20710695e-01 -1.13048553e-01
  2.82185078e-01 -4.56313521e-01  2.51220047e-01  4.38977540e-01
 -1.68635741e-01  1.85328260e-01  1.93050444e-01  2.58655548e-01
 -3.72874558e-01 -3.34597714e-02  5.83952442e-02  2.87483573e-01
  1.27111390e-01 -2.91394256e-03  2.51386642e-01  5.39898872e-01
 -2.24305600e-01  2.19600759e-02 -2.15843096e-01  9.15745348e-02
  3.16561460e-01 -2.08934829e-01 -3.80850323e-02 -2.57939816e-01
  1.74366400e-01  2.09749818e-01  5.06890565e-02  2.61468798e-01
 -2.43502155e-01  1.93081319e-01  4.46389049e-01 -5.49906492e-01
 -4.52393979e-01  2.29116485e-01  2.56221235e-01 -2.50350714e-01
  3.17424655e-01 -2.55161941e-01  1.52297139e-01  1.20157577e-01]"
"TF Lite runtime error: ""Didn't find op for builtin opcode 'PLACEHOLDER_FOR_GREATER_OP_CODES' version '1'"" stat:awaiting response comp:lite TFLiteConverter TF 2.13","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 11, Python 3.10.11
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code

Repro steps:

1) Download the `lite-model_efficientnet_lite4_uint8_2.tflite` model from here: https://tfhub.dev/tensorflow/lite-model/efficientnet/lite4/uint8/2 (click the 14.34Mb download [link](https://tfhub.dev/tensorflow/lite-model/efficientnet/lite4/uint8/2?lite-format=tflite) ; leave this page open in your browser)
2) Start a TF Lite application using Android or Flutter. I use Flutter and the latest ML Kit.
3) Use the model in your code : it works perfectly fine.
4) Back in your browser, now click the `TF` tab on the left side on the page your left open. The TF tab allows you to download the TF (not TF lite) version of the model. Download the 46.61Mb model file `efficientnet_lite4_classification_2.tar.gz` ([here](https://tfhub.dev/tensorflow/efficientnet/lite4/classification/2)). Uncompress this archive into a `saved_model` folder somewhere.
5) Use the following code to convert the TF model into a TF Lite model;

```
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, tags='train')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
outputPath = os.path.join(cur_dir, 'tf_converted_to_tflite.tflite')
fo = open(outputPath, ""wb"")
fo.write(tflite_model)
fo.close()
```
6) Following the instructions from https://www.tensorflow.org/lite/models/convert/metadata, add model metadata by using `metadata_writer_for_image_classifier.py`, and with the following model specification (updated lines ~60 to 70, as instructed):

```
_MODEL_INFO = {
    ""tf_converted_to_tflite.tflite"":
        ModelSpecificInfo(
            name=""EfficientNetB4 image classifier"",
            version=""v1"",
            image_width=380, # As specified in the TF tab in the web page (not the TF Lite tab, which input dimension is listed as 300x300)
            image_height=380, #
            image_min=0,
            image_max=255,
            mean=[127.5],
            std=[127.5],
            num_classes=1000,
            author=""TensorFlow"")
}
```
The processing completes normally.

7) Now use that model `tf_converted_to_tflite.tflite` instead of the original, readily-available TF Lite model available on TF Hub, `lite-model_efficientnet_lite4_uint8_2.tflite` (that was working fine).
In other words, we are now trying to use approximately the same model, except that this time we ran the TF -> TF Lite model conversion ourselves (quantization is a bit different, but it does not matter at all).
8) Run the same application. The following PlatformException is raised:

```
""com.google.mlkit.common.MlKitException: Failed to initialize detector.
Didn't find op for builtin opcode 'PLACEHOLDER_FOR_GREATER_OP_CODES' version '1'.
An older version of this builtin might be supported.
Are you using an old TFLite binary with a newer model?
```

### 3. Failure after conversion

I would expect the conversion process and runtime behavior to be smooth considered the model is hosted on TF Hub: the TF Lite model proposed by TF Hub works fine, but if I manually convert the TF model myself, it doesn't.
Is there anything wrong in the above conversion code? I have been debugging this for days, to no avail.
I have another (custom) model with a similar bug, and so far this is the best repro I could build.

### 4. First investigations

One thing I figured is that I should try to use the same version of TensorFlow on my PC and on my embedded system - as suggested in the following post from the TensorFlow Team:

https://discuss.tensorflow.org/t/tensorflowlite-error-didnt-find-op-for-builtin-opcode-softmax-version-1-an-older-version-of-this-builtin-might-be-supported-are-you-using-an-old-tflite-binary-with-a-newer-model/3885/6

Currently I am using TensorFlow 2.13.0 on my PC, however, how can I determine the version used in Flutter ? I use ML Kit on the Flutter side, which uses, on Android, the following Maven artifact:

https://mvnrepository.com/artifact/com.google.mlkit/image-labeling-custom/17.0.1

Which TF version is this artifact using?
",False,"[-0.49741784 -0.43903184 -0.17337269  0.09572151  0.20087343  0.05614509
 -0.14723775  0.15815264 -0.21558951 -0.11037938 -0.091685   -0.01402778
 -0.27299443  0.15869229 -0.13045073  0.18809864 -0.15139712 -0.35668987
  0.27991313 -0.01759641  0.06760827 -0.05165415 -0.10866567  0.13032885
  0.26391572  0.17179202 -0.26316506  0.06950875 -0.0896351   0.32054424
  0.250614    0.03423569 -0.09997525  0.11128391  0.06835756  0.1066273
 -0.08090076 -0.06490888 -0.31245387 -0.07090165  0.03855976  0.08426908
 -0.10635403  0.10913961 -0.1160599   0.00346034  0.1917485  -0.02488798
 -0.3027534  -0.00960803 -0.0023045   0.06322128 -0.3161049  -0.12980047
  0.18931836 -0.0494456  -0.07292759  0.17335126  0.20842823  0.16468078
 -0.07490683  0.03233773 -0.13103305  0.090285   -0.07366095  0.31223398
 -0.02308928 -0.11497942  0.15058051 -0.54340374 -0.13158736 -0.07178347
 -0.08307043 -0.00458467 -0.1507208   0.11602139 -0.16879448  0.23124051
  0.09011257 -0.11110411  0.15892856 -0.09520108  0.00085943  0.2723434
  0.14415783 -0.02001901  0.13092184  0.16784868  0.23855174 -0.11118559
  0.40026408  0.3632551   0.03611183  0.16878897  0.2976774   0.00413906
  0.07814643  0.34960777 -0.06213434  0.02177279 -0.06498998 -0.34418792
 -0.3705216  -0.0315436   0.0986152  -0.03798718  0.09847794  0.06377211
  0.06835182 -0.10568474  0.17366673  0.04687132 -0.03789477 -0.16626772
  0.02228589 -0.02061679 -0.12234133  0.03519274  0.22804701  0.40923113
  0.00634966 -0.28468984 -0.02482526  0.14387834  0.33039123  0.1677057
 -0.06198278  0.00984089  0.04327394  0.2579416   0.1919356   0.1864952
 -0.2082501   0.08547093  0.08768218  0.02867688 -0.35703304 -0.03118112
 -0.31548423 -0.09786887 -0.2593342   0.16805771 -0.0070715  -0.16700983
 -0.0875904  -0.01360693 -0.01948165  0.26918283 -0.16194348  0.06890328
 -0.02240102  0.00523032 -0.07657803  0.28617495  0.00932969  0.03380265
  0.33518767 -0.0163003  -0.02609435 -0.3713634   0.04182681  0.13199438
 -0.05279003  0.05072265  0.11529766 -0.04495793 -0.61575127 -0.18324617
  0.17858088  0.11780824 -0.03539962 -0.16476795  0.25515658  0.02757112
 -0.00865096 -0.07901904  0.4871273  -0.15513176 -0.12652832  0.06424234
  0.14390716  0.08636133  0.07496773  0.00061684 -0.1050157   0.10752644
  0.26665586  0.24073473 -0.26805773 -0.07760812 -0.29736167 -0.10821599
  0.2767016  -0.04611996 -0.18853094 -0.23063132  0.16711837  0.01953867
  0.05447817  0.1490666   0.02831387  0.06060673 -0.02219798  0.02652613
  0.06231223  0.10670898 -0.07633942 -0.16477013 -0.37233585 -0.10293947
  0.18735366 -0.18145727  0.02381817 -0.17969096 -0.23850405  0.10445751
 -0.19754225 -0.094193   -0.15514389  0.15142031 -0.02928841 -0.19596472
 -0.04217606 -0.21277693 -0.19419299 -0.10543145 -0.23681775  0.19483444
 -0.00189664  0.26095212 -0.21456334 -0.14977816  0.48734832  0.17356987
  0.20119481 -0.1367769   0.232548   -0.36004573 -0.23760518  0.11730932
 -0.31521067 -0.11462362 -0.10805218 -0.13818139 -0.06424653  0.08709788
 -0.02378437 -0.01534689  0.01483905  0.37664193 -0.05330288 -0.09836829
  0.1663431   0.16650549  0.20111561  0.12312266 -0.13098556  0.08113882
  0.22585033  0.00338279  0.04804191  0.2835362  -0.01439663  0.5692216
  0.20942721  0.24985272 -0.20324793  0.10292709 -0.38650295 -0.06353028
 -0.09249528 -0.07907309  0.52665854 -0.11173245  0.04282149  0.01288333
  0.3539506  -0.03796421  0.06864054  0.08064284  0.12390426  0.43378693
 -0.4322044   0.19669876  0.2041822  -0.15963753 -0.05030756 -0.44411397
 -0.07737952 -0.0377672  -0.16326952 -0.02701093 -0.11983617  0.13446236
 -0.02475815 -0.15423071  0.14098248 -0.13244313  0.03763039 -0.03264266
  0.01130681  0.30110466  0.18193947  0.05430134 -0.03597097  0.00850584
  0.31288594  0.01884299  0.35591334 -0.26559773  0.29950988  0.11772697
  0.06219425  0.27792218 -0.03711381  0.1253144  -0.19941859  0.12833539
  0.05091559 -0.07059771  0.20881888 -0.07243708 -0.43411306 -0.04365671
 -0.00773424  0.14816518  0.04368958 -0.14410016 -0.0545455   0.07845189
 -0.11293699 -0.12227477 -0.15088527  0.05624171 -0.10918092 -0.20101927
 -0.22783631  0.10766385 -0.09434164 -0.1822936   0.10790438 -0.28631634
 -0.13715746 -0.28160372 -0.13764563 -0.26819697  0.14218786  0.11961254
 -0.01864408  0.18314743  0.05460397 -0.03641194 -0.18041873 -0.03440896
 -0.02996467  0.21363539 -0.07728788 -0.00202325  0.00394157  0.31425026
 -0.13947852  0.17243329 -0.34077412 -0.2854452   0.07427806 -0.0066049
 -0.21424526 -0.02937017  0.05960834  0.4137163   0.05037597  0.3693304
 -0.3956228  -0.06865723  0.17365214 -0.2889502  -0.03013368  0.13577837
  0.07002255 -0.20922779 -0.0357793  -0.02127028  0.09994405  0.0964424 ]"
lite hexagonResizeNearestNeighbor comp:lite,"**System information**
- OS Platform and Distribution Linux Ubuntu 18.04
- TensorFlow installed from (github):
- TensorFlow version (dev):


**Provide the text output from tflite_convert**
none

**Standalone code to reproduce the issue** 

tensorflow/lite/delegates/hexagon/builders/resize_nearest_neighbor_builder.cc

**Any other info / logs**
STARTING!
Duplicate flags: num_threads
Log parameter values verbosely: [0]
Min num runs: [1]
Graph: [/home/wangzhiqun/yolox_resize_nearest_neighbor.tflite]
Use Hexagon: [1]
Loaded model /home/wangzhiqun/yolox_resize_nearest_neighbor.tflite
INFO: Initialized TensorFlow Lite runtime.
Hexagon delegate created.
INFO: TfLiteHexagonDelegate delegate: 1 nodes delegated out of 1 nodes with 1 partitions.

INFO: Replacing 1 node(s) with delegate (TfLiteHexagonDelegate) node, yielding 1 partitions.
[hexagon/nn] Add Node: tid(-1) nid(1)
[hexagon_nn] hexagon_nn_append_const_node(gid=1 tid=2 nid=2 type=0x3)
[hexagon/nn] Add Node from Op(331) mid(0) nid(3)
[hexagon_nn] hexagon_nn_append_const_node(gid=1 nid=4 type=0x3)
[hexagon_nn] hexagon_nn_append_const_node(gid=1 nid=5 type=0x3)
[hexagon/nn] Add Node: tid(-1) nid(6)
[hexagon_nn] hexagon_nn_append_node(gid=1 nid=1 type=0x0)
[hexagon_nn] hexagon_nn_append_node(nis=0 nos=1)
[hexagon_nn] hexagon_nn_append_node(outputs[0].elementsize=1)
[hexagon_nn] hexagon_const_node(gid=1 nid=2 type=0x3)
[hexagon_nn] hexagon_nn_append_node(gid=1 nid=3 type=0x14b)
[hexagon_nn] hexagon_nn_append_node(nis=6 nos=3)
Error adding node: id:3, op_type:331
[hexagon_nn] hexagon_const_node(gid=1 nid=4 type=0x3)
[hexagon_nn] hexagon_const_node(gid=1 nid=5 type=0x3)
[hexagon_nn] hexagon_nn_append_node(gid=1 nid=6 type=0x1)
[hexagon_nn] hexagon_nn_append_node(nis=1 nos=0)
----------------
Timestamp: Thu Sep 21 08:06:06 2023


Log
hexagon/src/newnode.c:413:node 3 (ResizeNearestNeighbor_8): bad input count 6
hexagon/src/newnode.c:763:node id=0x3 ctor fail
hexagon/src/prepare.c:4830:input 0x6:0 refers to nonexistent node 0x3
hexagon/src/prepare.c:4644:can't find id 0x3

----------------
ERROR: Failed: Failed to prepare graph.
.
ERROR: Node number 1 (TfLiteHexagonDelegate) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
Failed to apply Hexagon delegate.
Benchmarking failed.

",False,"[-3.29290658e-01 -2.40373984e-01 -3.21676552e-01  4.62569594e-02
 -1.34890884e-01 -3.59130323e-01 -1.12304658e-01  7.28281811e-02
 -3.15604985e-01 -1.88288502e-02 -1.10988408e-01  1.58343866e-01
 -6.93720579e-02  1.04821011e-01 -3.08912009e-01  2.14912921e-01
 -2.34373450e-01 -1.36063188e-01  2.02547356e-01 -9.85029787e-02
  9.37985480e-02 -1.24937147e-01 -1.89455628e-01  1.47020280e-01
  4.88684177e-01  3.28500211e-01  9.38039273e-02  8.15497339e-02
  1.33538380e-01 -3.93206999e-02  2.26010866e-02  3.11361730e-01
 -8.06627125e-02 -1.49042189e-01 -6.60565719e-02  3.41711223e-01
 -1.70762092e-01 -6.58492818e-02 -1.14591539e-01 -9.96095836e-02
  1.12224594e-01  5.56138940e-02  4.52801362e-02  2.20783234e-01
 -1.16493195e-01  8.80174637e-02 -1.99283659e-02  5.43902516e-02
 -1.15477756e-01 -1.70740902e-01 -3.67698371e-02 -9.78349969e-02
 -2.37902462e-01 -2.22010225e-01 -1.02586053e-01 -1.31755203e-01
  2.68771887e-01 -7.15839863e-02  1.59103930e-01  2.21726954e-01
  1.39196664e-01 -1.13052070e-01 -8.07569250e-02  2.74533331e-01
 -7.27073699e-02  1.02390409e-01  1.20032415e-01 -2.09499270e-01
  4.17680182e-02 -3.26351449e-02 -4.64138668e-03 -1.90204591e-01
 -6.02243505e-02 -1.03186823e-01  1.11594118e-01 -1.64402902e-01
  4.28043008e-02  4.62639093e-01  1.84640765e-01 -7.54548311e-02
 -2.93079987e-02  8.81607682e-02 -7.88411275e-02 -1.38789698e-01
 -1.18603036e-01  1.33020118e-01  1.27706617e-01  1.71069145e-01
  7.11131245e-02 -3.41614902e-01  3.39439899e-01  7.91791201e-01
 -2.25796759e-01  1.86834663e-01 -3.76413390e-03 -4.83758152e-02
 -8.90384763e-02 -5.63071817e-02  5.77619001e-02 -2.08143681e-01
  1.25817209e-01 -1.84831303e-02  1.67216867e-01  1.77330047e-01
  4.45427224e-02 -1.01586655e-01  1.96308374e-01  1.63034946e-01
  1.17912419e-01  1.86692297e-01  1.51939839e-01  4.78387028e-02
  2.02612340e-01  1.01849407e-01  2.69752353e-01  8.49286169e-02
  5.53496033e-02  6.76086321e-02  1.95067339e-02  6.01344347e-01
 -8.48650783e-02 -7.03146160e-02  8.27125907e-02  2.32926711e-01
  4.56370622e-01 -4.16905992e-03 -1.62292361e-01  1.58462912e-01
  1.61528075e-03  2.64973402e-01 -9.03381854e-02  2.73577303e-01
 -1.78345740e-01  1.15776889e-01  1.00083187e-01  2.12397233e-01
 -3.45630556e-01  4.11275961e-02 -2.26815104e-01  2.05865771e-01
 -1.30430937e-01  3.61662924e-01  3.24502140e-02 -4.98376489e-01
  5.60393296e-02  2.73909748e-01 -3.56849790e-01 -6.59212619e-02
 -2.02280313e-01  3.77694368e-01 -1.27103746e-01 -1.43990040e-01
  7.19854906e-02  4.45294101e-03  3.08692575e-01  6.68650568e-02
  5.48159555e-02  1.05130047e-01  7.01802038e-03 -4.47466075e-01
 -1.29828691e-01 -5.94520718e-02 -1.65040314e-01 -4.29576151e-02
 -1.04203448e-01 -4.55826148e-02 -6.05015099e-01 -2.22952038e-01
  5.89784384e-02  2.01934546e-01 -6.53838515e-02 -5.03735431e-03
 -2.01850403e-02  4.57491189e-01  5.11447728e-01 -4.51122671e-02
  5.54015219e-01 -6.23300910e-01 -3.18431675e-01  1.85343385e-01
  1.59066811e-01  1.60478175e-01  3.48313190e-02 -2.25079488e-02
  8.19955915e-02  2.56397836e-02  3.42393279e-01  3.01854312e-01
 -2.85256743e-01 -2.36949295e-01 -2.63292730e-01 -9.58590358e-02
  1.21161193e-01 -2.75916159e-01 -1.54491127e-01  1.71799019e-01
  6.63514957e-02 -1.32429689e-01  7.64539093e-02 -6.01167008e-02
 -1.34049982e-01 -9.40597802e-02 -1.82218291e-02  1.42801195e-01
  6.25340939e-02 -1.80484112e-02 -3.18302006e-01 -2.33414114e-01
 -4.82853144e-01  2.50060074e-02  4.62344624e-02 -6.52584136e-01
 -5.58054484e-02  4.51840609e-02 -1.35700583e-01 -1.28674030e-01
 -8.12951624e-02 -7.90726021e-03 -4.72311378e-01  2.16437221e-01
  1.57811135e-01 -1.42169103e-01 -1.83055717e-02 -5.51725388e-01
 -3.66588086e-01 -1.16623357e-01 -2.80627906e-01  1.08482458e-01
 -7.10456222e-02  2.67502844e-01 -2.93753624e-01  1.69136509e-01
  1.68615490e-01 -1.03298556e-02  1.78526774e-01  6.35069162e-02
  2.45400578e-01 -4.27268922e-01 -4.95354772e-01  2.49069601e-01
  1.01675175e-01 -3.62115264e-01  2.08024338e-01  8.06962624e-02
  1.15154132e-01 -5.51691577e-02 -2.90351033e-01  8.41121934e-03
 -5.78671135e-03  7.05515146e-01  1.37074292e-01  7.20201246e-03
  2.54134893e-01  1.97456539e-01  2.46651232e-01 -1.88509241e-01
 -5.77968657e-02  1.01880185e-01  2.04244435e-01 -1.11863688e-01
  7.98961818e-02  2.25883588e-01 -1.01425663e-01  7.24786818e-01
  3.53271514e-01  1.83657289e-01 -2.75680125e-01  3.28443110e-01
 -2.96698511e-02 -1.49563104e-01 -1.69049934e-01 -8.76881927e-02
  1.39525235e-01 -2.89847016e-01  3.90124768e-02 -3.21233273e-01
  4.87247139e-01 -1.58647060e-01  2.80043751e-01  1.77175820e-01
  2.69314289e-01  3.50645006e-01 -5.14401719e-02  1.47075772e-01
  1.00469306e-01 -1.15091637e-01 -2.08428642e-03 -5.21585524e-01
 -4.13214803e-01  1.98849179e-02 -8.60115290e-02  6.02120273e-02
 -4.28845771e-02  6.45290315e-02 -1.74614623e-01  2.09827051e-01
  1.53104067e-01 -2.14280486e-01  1.59035064e-02  9.79605317e-02
  2.74306893e-01  1.22434899e-01  1.24159433e-01 -1.22554705e-01
 -1.06402382e-01  8.82338434e-02  2.55421638e-01  5.37421182e-03
  5.65767050e-01 -3.68525326e-01  2.45719463e-01 -5.71323112e-02
  2.18865767e-01  3.30570608e-01 -1.24287277e-01  8.32603201e-02
 -1.94421470e-01  2.07335651e-01 -4.25640680e-03 -3.32330406e-01
  2.41588473e-01 -8.47932920e-02 -2.67521858e-01  4.32704668e-03
  6.06919192e-02 -2.53002405e-01  7.74420202e-02  3.65272649e-02
 -2.79158711e-01  2.00560510e-01 -1.52912810e-01 -1.77648216e-01
 -2.88111754e-02  2.14150265e-01 -2.72557557e-01 -1.02904305e-01
  1.12764351e-03  1.91228807e-01 -2.41562873e-01 -2.85212904e-01
 -1.42785311e-01 -1.26927733e-01  1.55008268e-02 -1.92003623e-01
 -5.22861257e-04 -1.06496632e-01  9.81962159e-02  2.46406063e-01
 -2.32647285e-02 -1.65117785e-01 -5.95530942e-02  1.05917811e-01
 -3.18948686e-01  1.36641428e-01  2.08195560e-02  1.44690380e-01
 -3.05426151e-01  1.69456482e-01  3.90689671e-02  7.01011598e-01
  8.14960003e-02  1.74290091e-01 -1.58352852e-01 -1.21391848e-01
  1.17687926e-01 -5.35234101e-02 -2.16533303e-01  4.33739647e-03
 -4.60986085e-02  2.86439270e-01  8.53638202e-02  3.65603149e-01
 -1.92281023e-01  1.97459921e-01  4.17966723e-01 -2.37863421e-01
 -2.26788938e-01  3.58711556e-02 -8.92502517e-02 -8.96848142e-02
  1.53780222e-01  9.09282919e-03 -2.97274478e-02 -1.16974719e-01]"
lite hexagonResizeNearestNeighbor comp:lite TFLiteHexagonDelegate,"hexagon/src/newnode.c:413:node 3 (ResizeNearestNeighbor_8): bad input count 6
hexagon/src/newnode.c:763:node id=0x3 ctor fail
hexagon/src/prepare.c:4830:input 0x6:0 refers to nonexistent node 0x3
hexagon/src/prepare.c:4644:can't find id 0x3

----------------
ERROR: Failed: Failed to prepare graph.
.
ERROR: Node number 1 (TfLiteHexagonDelegate) failed to prepare.
ERROR: Restored original execution plan after delegate application failure.
Failed to apply Hexagon delegate.
Benchmarking failed.

",False,"[-0.32554257  0.06626953 -0.18674487  0.24229404 -0.5649973  -0.4876036
 -0.02405253  0.45384786 -0.10374877 -0.10777459 -0.2692792   0.31053156
  0.06634216 -0.12424366 -0.5124589  -0.0497086  -0.3155166  -0.05448934
  0.23076552  0.22812445  0.07266735 -0.07333439 -0.38398576 -0.21481487
  0.37032768 -0.03980672  0.01795198  0.13022412  0.03385083 -0.03589568
 -0.12468107  0.45184684 -0.08915222 -0.23230718  0.03972975 -0.15852003
 -0.58633137  0.07377613 -0.10007571 -0.06010877  0.0557794   0.31062946
 -0.0321648   0.3078276  -0.06156613 -0.04464089 -0.0926902   0.2219725
  0.13221878  0.04418696 -0.02429115  0.26047534 -0.17941171 -0.1206516
 -0.15151793 -0.5833819  -0.10670888  0.14135802  0.22903997  0.32990685
  0.26264971 -0.07484981  0.12279604 -0.11807398 -0.13320568 -0.01339806
 -0.06148179 -0.45554996  0.14176084  0.14642279 -0.07835727  0.23140195
  0.18011126 -0.10499194  0.19067296 -0.11575676 -0.2525229   0.34242314
 -0.0072707  -0.15439186 -0.01549587  0.3453086   0.07058244 -0.2671908
  0.2690803   0.30550283 -0.07826677  0.06988905  0.24471241 -0.04652185
  0.21633752  0.62745744 -0.3004443   0.08234266 -0.16864818 -0.02508249
  0.17821136 -0.2885012  -0.2951604  -0.19748019  0.2752338  -0.08750421
 -0.02404412  0.36058915 -0.01071217 -0.1809637   0.23404635  0.14433904
 -0.1829696   0.15725401  0.18909447 -0.04807194  0.00962079  0.01092361
  0.01818575  0.21245044  0.03318783 -0.2925532  -0.2912089   0.39109507
 -0.18942381  0.11511086 -0.292081    0.25151062  0.37910783  0.02471279
 -0.07347924  0.05718272 -0.04882539  0.41337124 -0.0664966   0.30394638
 -0.19942845  0.03520205  0.00243076  0.35975146 -0.18511535 -0.3994719
 -0.38577998  0.26590416  0.22680998  0.04778943  0.03527972 -0.5180675
 -0.03007402  0.03030893 -0.4213935  -0.28145102 -0.00811811  0.39869422
  0.05875256 -0.00291167 -0.31681293  0.5377336  -0.13543332  0.00954047
 -0.375454    0.01978154  0.03615745 -0.38603774 -0.18257016 -0.00231269
 -0.01624675  0.11643648  0.25761557 -0.1645914  -0.3231865  -0.14158598
 -0.2050785   0.2990482  -0.1050539   0.11970448  0.05436983  0.13550499
  0.2941538  -0.10818928  0.3170944  -0.1324082  -0.2637018   0.33467454
  0.10857426  0.28705305  0.10261366  0.0943803   0.00725192  0.22029713
  0.22107366  0.24019101  0.22194171 -0.2035748  -0.11156996  0.24771251
  0.04099022 -0.3157532  -0.07416583  0.06126279 -0.1907585  -0.42049608
  0.5046161  -0.3082057   0.27262563  0.31709087  0.15207279  0.04746179
  0.07972994  0.24182445 -0.29988426 -0.41490042 -0.03299906 -0.11572787
 -0.11818902 -0.48662615 -0.22216266 -0.3387673   0.03992359 -0.02576213
 -0.13178879  0.20531856 -0.17361882  0.06901956  0.07304223 -0.20520103
  0.04350388  0.0361094  -0.3513393   0.12050461  0.10547271 -0.20916846
 -0.13876215  0.11533525 -0.02263759  0.01757518  0.38177562  0.07230833
  0.2232614   0.43648773 -0.1573112  -0.1960873  -0.10468337  0.09853373
  0.1083015  -0.34416652 -0.029044   -0.05596562  0.03211644 -0.11716691
 -0.23933569 -0.12770206  0.12843369  0.20797583 -0.13216263 -0.27631545
 -0.10003911  0.2199744   0.02394174 -0.13351619  0.10962895  0.27067345
  0.26095015 -0.40761167  0.1751457   0.10994811 -0.16688512  0.1308302
  0.31623152 -0.2131315  -0.05153898  0.08577611  0.14388745  0.01983511
 -0.05221312 -0.08586222  0.281469    0.03549495  0.0370698  -0.6405709
  0.20178664 -0.11780746  0.24050757  0.00265689  0.182651    0.16567287
  0.23326689  0.18360466  0.442038   -0.23665677 -0.01720136 -0.12825653
 -0.1565909   0.01534578 -0.01441617  0.08151687 -0.02281608  0.09653041
 -0.02925213  0.15349871  0.2506209  -0.24845934  0.00583962 -0.02150692
  0.2937703   0.07250406 -0.11875917 -0.41438538 -0.00386924 -0.13224068
 -0.11623231  0.01765979  0.2883769  -0.25953346  0.21278477  0.02813861
  0.02372816  0.41119316 -0.23690902  0.23066925 -0.06285272 -0.08598541
 -0.10184185  0.22306524 -0.02227483 -0.14953756 -0.26570457 -0.16954324
  0.01953498 -0.26281798 -0.24817395  0.06692554 -0.29710886  0.15596244
  0.21546325  0.06411627  0.20473112 -0.14720073  0.02849473 -0.12010079
  0.03879596  0.07748915 -0.11544967 -0.17887974 -0.39433143 -0.31650096
  0.16559735 -0.20647608  0.1444538  -0.1584884   0.11298808  0.36196998
 -0.03735036 -0.3137008   0.08878867  0.10240857  0.07308053  0.08035511
 -0.09705314  0.28449276 -0.17742732  0.3672967   0.14355704  0.02331348
 -0.10512069  0.16423129 -0.2019442   0.04379028  0.06794925  0.14203733
 -0.12976475  0.19727188  0.01687344 -0.18584886 -0.09462253  0.2253806
 -0.14224869 -0.22097635  0.03442248 -0.16439438  0.10300646 -0.07525877
 -0.00414703 -0.18012023 -0.02223308  0.10784692  0.08229569  0.1027712 ]"
Can't get optimizer to apply gradients with Keras and DTensor based model type:support comp:keras comp:tf.function,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0 (also tried with 2.9.1)

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Trying to do distributed training, for a rather wide model, with DTensors and Keras. I'm creating an Adam optimizer, trying to update it manually with the following code for a custom DTensor training step (this is within my own model object, with self.model being a Keras model):

 ```
  @tf.function
    def train_step(self, x, y, w, optimizer, train_metrics): 
        if not self.init:   # this runs once to initialize the model variables
            self.model(x)
            self.init = True 
        with tf.GradientTape() as tape:
            logits = self.model(x, training=True)
            logits = tf.reshape(logits, (logits.shape[1], logits.shape[0]))
            loss = tf.reduce_sum(tf.math.multiply(
                tf.keras.losses.binary_crossentropy(
                    y, logits, from_logits=True), w))
        gradients = tape.gradient(loss, self.model.trainable_variables)

        optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))

        loss_per_sample = loss / len(x)
        results = {'loss': loss_per_sample}
        for metric in train_metrics.values():
            metric.update_state(y_true=y, y_pred=logits)
        return results 
        
    def dtensor_fit(        
        self,
        x_train,
        y_train,
        x_val,
        y_val,
        w_train=None):
        num_epochs = 5
        train_metrics = {
            ""accuracy"" : metrics.Accuracy(),
            ""tp"": metrics.TruePositives(),
            ""fp"": metrics.FalsePositives(),
            ""fn"": metrics.FalseNegatives(),
            ""tn"": metrics.TrueNegatives(),
            ""auc"": metrics.AUC(curve=""PR""),
        }
        optimizer = tf.keras.dtensor.experimental.optimizers.Adam(mesh=self.mesh)

        eval_metrics = dict(train_metrics)   
        for epoch in range(num_epochs):
            print(""============================"") 
            print(""Epoch: "", epoch)
            for metric in train_metrics.values():
                metric.reset_state()
            step = 0
            results = {}
            pbar = tf.keras.utils.Progbar(target=None, stateful_metrics=[])
            def batch(x, y, w, n):
                num_samples = x.shape[0]
                l = 0
                while l < num_samples:
                    yield x[l:l+n], y[l:l+n], w[l:l+n]
                    l += n
            self.init = False
            for inputs, labels, weights in batch(x_train, y_train, w_train, NNModel.BATCH_SIZE):
                indices = np.transpose(inputs.nonzero())
                inputs.eliminate_zeros()
                values = inputs.data
                inputs, labels, weights = self.pack_dtensor_inputs(
                    tf.SparseTensor(indices=indices, values=values, dense_shape=(inputs.shape[0], self.input_size)),
                    tf.convert_to_tensor([labels], dtype =tf.bfloat16),
                    tf.convert_to_tensor([weights], dtype=tf.bfloat16), self.input_layout, self.label_layout, self.weight_layout)
                optimizer.build(self.model.trainable_variables)

                results.update(self.train_step(inputs, labels, weights, optimizer, train_metrics))
                for metric_name, metric in train_metrics.items():
                    results[metric_name] = metric.result()

                pbar.update(step, values=results.items(), finalize=False)
                step += 1 
``` 

I get the following error when I reach the apply_gradients step in the train_step function:

ValueError: in user code:

```
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/_main/keystone/training/model.py"", line 263, in train_step  *
    optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/dtensor/optimizers.py"", line 141, in apply_gradients  **
    optimizer_lib._BaseOptimizer.apply_gradients(self, grads_and_vars)
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/optimizers/optimizer.py"", line 650, in apply_gradients
    iteration = self._internal_apply_gradients(grads_and_vars)
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/dtensor/optimizers.py"", line 153, in _internal_apply_gradients
    optimizer_lib._BaseOptimizer._internal_apply_gradients(
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/optimizers/optimizer.py"", line 680, in _internal_apply_gradients
    self._update_step(grad, var)
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/optimizers/optimizer.py"", line 240, in _update_step
    self.update_step(gradient, variable)
File ""/usr/local/home/bill/.cache/bazel/_bazel_bill/254c50c69c3701cca4e904bef759573b/execroot/_main/bazel-out/k8-fastbuild/bin/keystone/training/training.runfiles/rules_python~0.21.0~pip~pip_keras/site-packages/keras/optimizers/adam.py"", line 194, in update_step
    m.assign_add((gradient - m) * (1 - self.beta_1))

ValueError: Dimensions must be equal, but are 5 and 0 for '{{node sub_2}} = Sub[T=DT_FLOAT](gradient_tape/keystone/feature/MatMul_1/Cast/Cast, sub_2/ReadVariableOp)' with input shapes: [8299614,5], [0].

```
It looks like the optimizer momentums are not of the correct shape. I tried explicitly adding `optimizer.build(self.model.trainable_variables) `after the `self.model(x)` in `train_step` to force correct population, but the variables are still wrong.

I have seen the same results with TF versions 2.9.1 and 2.13.0.

Is this a bug? How do I get the optimizer to be correctly set up for training?



### Standalone code to reproduce the issue

```shell
see above
```


### Relevant log output

_No response_",False,"[-0.5055375  -0.43144393 -0.27995026 -0.0165014   0.15041153 -0.36310303
 -0.12444285 -0.19063039 -0.3597288  -0.17454797  0.22341213  0.01490466
 -0.3099257  -0.0010785  -0.21409717  0.30303222 -0.07787573  0.2872718
  0.01663638 -0.02458282 -0.15714332 -0.22400959 -0.12192789  0.19041869
  0.07063676  0.04243353 -0.08410461 -0.07693169  0.02781766 -0.14075968
  0.40377054  0.19183187 -0.03228894  0.10166219 -0.3619316   0.18173207
 -0.0186252  -0.25907657 -0.1787071  -0.04048855 -0.01965911  0.16665654
 -0.01687851 -0.13026345  0.06986106 -0.28892058  0.05618008 -0.13529512
 -0.21290615 -0.22475047 -0.01257418 -0.33590704 -0.31300986 -0.609697
 -0.23501448 -0.04197298  0.2864885  -0.16851503 -0.03405103 -0.17361332
  0.04577797  0.1416251   0.05520555  0.11387584  0.17995003  0.0662
  0.16423418 -0.1241035   0.4951396   0.08691861  0.10958339 -0.3331962
 -0.4246474  -0.02046991  0.01111983  0.1806243   0.21925679  0.16246885
  0.2665953  -0.19377214  0.27377993 -0.21801783 -0.22688556 -0.19947372
  0.13316384 -0.15940484  0.30464357  0.04672059  0.572245   -0.27279305
  0.5108468   0.2772815  -0.32801595  0.04056247  0.43818513  0.11641259
  0.03534557 -0.1291044  -0.05494485 -0.19098514 -0.17957939 -0.06537008
 -0.02083696  0.19449612 -0.07938357 -0.10869844  0.23541024 -0.17892392
  0.26832548  0.12581627  0.14368793  0.0164372   0.09554705  0.01046814
 -0.02360488 -0.19905454 -0.08547346  0.14462133 -0.04375734  0.78155315
 -0.0633498  -0.24683478  0.17765455  0.23201177  0.6109725   0.30381992
 -0.24055308  0.10325765  0.01868746  0.07423322  0.09020494  0.06682451
  0.13463463  0.03949847 -0.00488322  0.11108695 -0.1652841   0.05282285
 -0.32026082 -0.17234297 -0.5079655   0.29507092 -0.26841378 -0.39260852
  0.198926   -0.03547174 -0.33348837  0.03000547 -0.09524217 -0.11718206
 -0.2516962   0.10493825 -0.08119287  0.13971782  0.32644135  0.3016687
  0.12635916 -0.13950184 -0.0506448  -0.39087188  0.1338241   0.27576438
 -0.08721931 -0.14056277  0.222669    0.2430899  -0.13386399 -0.17435013
  0.11710472  0.35411614  0.10499418  0.03502452 -0.00995198  0.13342987
  0.29184714 -0.02895721  0.07491611 -0.52030903 -0.17616484  0.37651357
  0.19130674  0.13970868  0.26821074  0.20160395  0.03091005  0.06107531
  0.40723437  0.09081375 -0.49321708  0.03038497 -0.3758548  -0.34172177
  0.3839671  -0.2939897  -0.15770042  0.08312491  0.23185632 -0.15244782
 -0.10129401  0.01746763 -0.26250023 -0.23346391 -0.22858734 -0.11997832
 -0.09196655 -0.34905094 -0.01373289 -0.23860756 -0.54313034  0.32694137
  0.19735345 -0.6874002   0.22197464 -0.0026132  -0.25017887  0.22412828
  0.37172568 -0.09056275  0.0091316   0.5555189   0.2147994  -0.14676985
  0.0525438  -0.3094983  -0.38511258 -0.0683751  -0.17480183  0.15261576
 -0.15104163  0.09758194 -0.11303382 -0.16907972  0.41488254  0.18890417
  0.3268892  -0.05768134 -0.05436164 -0.1557608   0.01559494 -0.06830496
 -0.50677574  0.08079805  0.1858997  -0.10170861  0.33861122  0.41188598
 -0.39957488 -0.02254718 -0.32084998  0.08357888 -0.32878825  0.08156444
  0.21595956  0.23256461  0.29673547  0.07630463  0.04831604  0.04953299
  0.2885863   0.02373847  0.30213666  0.30724108 -0.09611157  0.538058
  0.17126699  0.28394765 -0.31249732  0.49647027  0.05175143 -0.30719936
  0.17046684 -0.35843226  0.5860853  -0.3424183   0.18385954  0.1280503
  0.16890292  0.15958585 -0.11090863 -0.12694024  0.12884606  0.06534598
 -0.46842572 -0.06694286 -0.20878184 -0.16195802 -0.05546256 -0.7807709
  0.02592346  0.04219434 -0.07488415  0.20854287  0.35375142 -0.06701567
 -0.221282    0.07243928  0.1767241   0.02598051  0.09571028 -0.10225288
  0.05967043  0.20842421  0.48200926 -0.5659814  -0.1492105  -0.14325711
  0.574494    0.09051377  0.36484224 -0.23315375 -0.13290599  0.12068143
  0.03588759  0.5575275  -0.06427701  0.07918002 -0.15125516  0.6488354
  0.19693694  0.07527142  0.04892603 -0.2367669  -0.2782143   0.096335
  0.25591767 -0.06043407 -0.11945805 -0.4564774  -0.02284629 -0.10202374
 -0.0840573   0.04968235 -0.06357885  0.3606059   0.00490999  0.04762848
 -0.35064375  0.54658705 -0.12269956 -0.28316912 -0.29545784  0.03545445
 -0.04301021 -0.4067832  -0.04434213 -0.17422834  0.04111518  0.35329568
 -0.083997    0.06735241 -0.20016661  0.27672964 -0.50901055 -0.01093902
  0.20977716  0.510269   -0.03762616 -0.108185    0.28267634  0.35400808
 -0.20076564  0.06998259 -0.28670967  0.05105972  0.13836999 -0.10011351
  0.02709476 -0.34721166 -0.08682886  0.47228536 -0.1420119   0.35305685
 -0.16555831  0.4170451   0.34457982 -0.54285    -0.16145891  0.13504589
  0.09588346 -0.00094024  0.12761182 -0.11784852 -0.02972272  0.07151043]"
Failure to create build_pip_package Ubuntu 22.04 LTS / Python 3.10 / Cuda 11.7 stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.9,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9.1

### Custom code

No

### OS platform and distribution

Ubuntu 22.04 LTS

### Mobile device

/

### Python version

3.10

### Bazel version

5.0.0

### GCC/compiler version

11.4.0

### CUDA/cuDNN version

11.7/8.5.0

### GPU model and memory

RTX A6000, GTX 1070

### Current behavior?

I've installed CUDA 11.7 toolkit only and libcudnn 8.5.0 on Ubuntu 22.04 LTS.
I'm using python 3.10 using the `python3.10` cmd.

Here's the script I'm running

```bash
export PATH=$PATH:/usr/local/cuda-11.7/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.7/lib64
export TF_CUDA_VERSION=11.7
export TF_CUDNN_VERSION=8.5.0
export TF_CUBLAS_VERSION=11.10.1

python3.10 -m virtualenv venv
venv/bin/pip install numpy==1.23.5 wheel packaging requests opt_einsum
venv/bin/pip install keras_preprocessing --no-deps

mkdir tmp
cd tmp
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout v2.9.1

cd ..
. venv/bin/activate
cd tmp/tensorflow
./configure  # No ROCm, Yes CUDA, No TensorRT, 3.5,5.2,6.0,6.1,7.0,7.5,8.0 capabilities, No clang, default for the rest

bazel build --config=opt --verbose_failures //tensorflow:libtensorflow_cc.so

bazel build --config=opt --verbose_failures //tensorflow:install_headers

bazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package  # Failure here

./bazel-bin/tensorflow/tools/pip_package/build_pip_package ./bazel-bin/tensorflow/tools/pip_package
```

The output of `bazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package` is available in the relevant log output section.

The interesting line seems to be:
```
cp: cannot stat '/home/cluster/CN_TF/ContextCapture/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_dtype_api.h': No such file or directory
```
Indeed this file does not exist. I'm unsure why it is needed, but I tried different `numpy` version and failed to find one that works (either I'm missing this file, or `.doxyfile` or something else).

Can you advise which version of numpy should I be using for the compilation to be successful?

### Standalone code to reproduce the issue

```shell
I'm using vanilla Tensorflow checkout, no modifications are applied.
```


### Relevant log output

```shell
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=116
INFO: Reading rc options for 'build' from /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library
INFO: Reading rc options for 'build' from /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin/python3 --action_env PYTHON_LIB_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages --python_path=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin/python3 --action_env TF_CUDA_VERSION=11.7 --action_env TF_CUBLAS_VERSION=11.10.1 --action_env TF_CUDNN_VERSION=8.5.0 --action_env CUDA_TOOLKIT_PATH=/usr/local/cuda-11.7 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0,7.5,8.0 --action_env LD_LIBRARY_PATH=:/usr/local/cuda-11.7/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 --config=cuda
INFO: Reading rc options for 'build' from /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:opt in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/cluster/work/ThirdParty/Tensorflow/distrib/tmp/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (236 packages loaded, 7837 targets configured).
INFO: Found 1 target...
ERROR: /home/cluster/.cache/bazel/_bazel_cluster/350d8f2feeb21f42226977fe6efcfb0a/external/local_config_python/BUILD:254:8: Executing genrule @local_config_python//:numpy_include failed: (Exit 1): bash failed: error executing command 
  (cd /home/cluster/.cache/bazel/_bazel_cluster/350d8f2feeb21f42226977fe6efcfb0a/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.7 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 \
    LD_LIBRARY_PATH=:/usr/local/cuda-11.7/lib64 \
    PATH=/home/cluster/.cache/bazelisk/downloads/sha256/399eedb225cff7a13f9f027f7ea2aad02ddb668a8eb89b1d975d222e4dc12ed9/bin:/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin:/home/cluster/.vscode-server/bin/8b617bd08fd9e3fc94d14adb8d358b56e3f72314/bin/remote-cli:/home/cluster/.local/bin:/home/cluster/miniconda3/bin:/home/cluster/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda-11.7/bin \
    PYTHON_BIN_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin/python3 \
    PYTHON_LIB_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CUBLAS_VERSION=11.10.1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0,7.5,8.0 \
    TF_CUDA_VERSION=11.7 \
    TF_CUDNN_VERSION=8.5.0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; 
cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/.doxyfile"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/.doxyfile"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/__multiarray_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/__multiarray_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/__ufunc_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/__ufunc_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_dtype_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_dtype_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_neighborhood_iterator_imp.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_neighborhood_iterator_imp.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_numpyconfig.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_numpyconfig.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_numpyconfig.h.in"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_numpyconfig.h.in"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/arrayobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/arrayscalars.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/arrayscalars.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/experimental_dtype_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/experimental_dtype_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/halffloat.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/halffloat.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/libdivide/LICENSE.txt"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/libdivide/LICENSE.txt"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/libdivide/libdivide.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/libdivide/libdivide.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ndarrayobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ndarraytypes.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/noprefix.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/noprefix.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_1_7_deprecated_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_3kcompat.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_3kcompat.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_common.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_common.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_cpu.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_cpu.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_endian.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_endian.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_interrupt.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_interrupt.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_math.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_math.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_no_deprecated_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_no_deprecated_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_os.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_os.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/numpyconfig.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/numpyconfig.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/old_defines.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/old_defines.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/oldnumeric.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/oldnumeric.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/random/bitgen.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/random/bitgen.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/random/distributions.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/random/distributions.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ufuncobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ufuncobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/utils.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/utils.h""
   ')
# Configuration: d04d20a1a4a46df9c7580a4efee273c7e78fdcec51eaaa1a5c3a4ea89b71ce88
# Execution platform: @local_execution_config_platform//:platform
cp: cannot stat '/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_dtype_api.h': No such file or directory
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/cluster/.cache/bazel/_bazel_cluster/350d8f2feeb21f42226977fe6efcfb0a/external/local_config_python/BUILD:66:11 Middleman _middlemen/@local_Uconfig_Upython_S_S_Cnumpy_Uheaders-BazelCppSemantics_build_arch_k8-opt failed: (Exit 1): bash failed: error executing command 
  (cd /home/cluster/.cache/bazel/_bazel_cluster/350d8f2feeb21f42226977fe6efcfb0a/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-11.7 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 \
    LD_LIBRARY_PATH=:/usr/local/cuda-11.7/lib64 \
    PATH=/home/cluster/.cache/bazelisk/downloads/sha256/399eedb225cff7a13f9f027f7ea2aad02ddb668a8eb89b1d975d222e4dc12ed9/bin:/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin:/home/cluster/.vscode-server/bin/8b617bd08fd9e3fc94d14adb8d358b56e3f72314/bin/remote-cli:/home/cluster/.local/bin:/home/cluster/miniconda3/bin:/home/cluster/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/cuda-11.7/bin \
    PYTHON_BIN_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/bin/python3 \
    PYTHON_LIB_PATH=/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages \
    TF2_BEHAVIOR=1 \
    TF_CUBLAS_VERSION=11.10.1 \
    TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.2,6.0,6.1,7.0,7.5,8.0 \
    TF_CUDA_VERSION=11.7 \
    TF_CUDNN_VERSION=8.5.0 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; 
cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/.doxyfile"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/.doxyfile"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/__multiarray_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/__multiarray_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/__ufunc_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/__ufunc_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_dtype_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_dtype_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_neighborhood_iterator_imp.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_neighborhood_iterator_imp.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_numpyconfig.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_numpyconfig.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/_numpyconfig.h.in"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/_numpyconfig.h.in"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/arrayobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/arrayobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/arrayscalars.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/arrayscalars.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/experimental_dtype_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/experimental_dtype_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/halffloat.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/halffloat.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/libdivide/LICENSE.txt"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/libdivide/LICENSE.txt"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/libdivide/libdivide.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/libdivide/libdivide.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ndarrayobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ndarrayobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ndarraytypes.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ndarraytypes.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/noprefix.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/noprefix.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_1_7_deprecated_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_3kcompat.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_3kcompat.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_common.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_common.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_cpu.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_cpu.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_endian.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_endian.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_interrupt.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_interrupt.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_math.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_math.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_no_deprecated_api.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_no_deprecated_api.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/npy_os.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/npy_os.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/numpyconfig.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/numpyconfig.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/old_defines.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/old_defines.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/oldnumeric.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/oldnumeric.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/random/bitgen.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/random/bitgen.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/random/distributions.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/random/distributions.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/ufuncobject.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/ufuncobject.h"" && cp -f ""/home/cluster/work/ThirdParty/Tensorflow/distrib/venv/lib/python3.10/site-packages/numpy/core/include/numpy/utils.h"" ""bazel-out/k8-opt/bin/external/local_config_python/numpy_include/numpy/utils.h""
   ')
# Configuration: d04d20a1a4a46df9c7580a4efee273c7e78fdcec51eaaa1a5c3a4ea89b71ce88
# Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 136.025s, Critical Path: 44.21s
INFO: 977 processes: 370 internal, 607 local.
FAILED: Build did NOT complete successfully
```
",False,"[-0.54466426 -0.6639149  -0.24347378  0.40005177  0.08214241 -0.36941493
 -0.3580618   0.07184132 -0.3226937  -0.18244925 -0.05790217  0.11130743
 -0.25568986  0.14456934 -0.4103375   0.39289832 -0.13031633 -0.3445186
  0.14339703  0.30612266 -0.25046462  0.05453781 -0.3956426   0.23307452
  0.6116847  -0.00334915 -0.39005917  0.07333361  0.18529683  0.09809987
  0.51545894  0.15475067 -0.08003187  0.1632026   0.19194813  0.249699
 -0.03808023 -0.25466722 -0.09043647 -0.19907376 -0.06040954  0.11765316
 -0.07465991 -0.08889934 -0.11710218 -0.4754032   0.15443465 -0.2134629
 -0.07154465 -0.19348572 -0.3350018   0.14737351 -0.37316358 -0.29214972
 -0.02983159 -0.29132044  0.0554578   0.28743604  0.09553172 -0.02426114
  0.15152791  0.15470622  0.22091922  0.16697358 -0.07574056  0.47736958
  0.08253869 -0.04100247  0.4982947  -0.3048178  -0.04320787 -0.06121524
 -0.35160986  0.01979831 -0.0200896   0.0770665   0.02533944  0.22664393
  0.02478324 -0.04641888 -0.0366579  -0.18858643 -0.09823348 -0.01397658
 -0.00223027 -0.01270723  0.18545938  0.22107199  0.38392147 -0.4309234
  0.37797213  0.5539946   0.1886456   0.23502502  0.2890071  -0.04002867
  0.09259349  0.35412812 -0.19047612 -0.17360306  0.04363016 -0.2762433
 -0.02051473 -0.00127678  0.00332334  0.04411132  0.4880907  -0.23630375
  0.00129    -0.09157936  0.15395764  0.02530059  0.09767635 -0.13408443
 -0.02927124  0.06586005 -0.46019813 -0.11775585  0.04482493  0.881907
 -0.06358202  0.08976348 -0.09254985  0.13183361  0.40059066  0.22929925
  0.2552771  -0.11865667  0.00619302  0.20064434  0.12105595  0.3094443
  0.02700248  0.30231857  0.27975532  0.11084048 -0.14558265 -0.21548894
 -0.07040562 -0.12381367 -0.21197625  0.526769   -0.2023103  -0.63228303
 -0.03857745  0.19319217 -0.09953079  0.39105204 -0.24699123  0.2351169
 -0.04972364 -0.12784432 -0.48746327  0.48021203  0.14998183  0.22648078
  0.16645458 -0.03526221 -0.13074306 -0.54655766 -0.01602818  0.4964182
 -0.2894107  -0.2885635  -0.18337712  0.18806317 -0.62951565 -0.4458245
  0.22656533  0.32305318 -0.25962207 -0.11948614  0.30591348  0.1602802
  0.21248946 -0.09537993  0.28927326 -0.39164287  0.15765138  0.5504026
  0.22078535  0.42595387  0.01875049 -0.00551049  0.08091846  0.04571495
  0.08084543 -0.16669285 -0.19142894  0.06021164 -0.3453011  -0.05200438
  0.4506693  -0.1589139  -0.22982249  0.3443722   0.48168445  0.01088911
  0.2365171  -0.10598056 -0.14026016  0.04537122 -0.05729477  0.13084915
 -0.04466944 -0.2654941   0.16989538 -0.32014322 -0.43895322 -0.09868247
 -0.13625646 -0.40394276  0.32937363 -0.44992796 -0.07872269  0.29807115
  0.24276131  0.03611599 -0.15306708  0.09162375  0.21594508 -0.20557585
  0.31828642 -0.32131928  0.14101264 -0.02058438 -0.37745565 -0.08127405
 -0.13946684  0.07952145 -0.07359705 -0.05389373  0.4482347   0.45695174
  0.35427445 -0.12951955  0.11045901 -0.06227059 -0.10556     0.35230005
 -0.31054783 -0.03924154  0.17543638 -0.11977194  0.10887976  0.12020185
 -0.19260582 -0.01150773 -0.5869134   0.2342485  -0.05192176 -0.22582051
  0.12469442  0.1275132   0.29472655  0.2690057   0.01592157  0.05615526
  0.03566728 -0.11761643  0.0174627   0.42351884 -0.0950904   0.1128176
  0.16867524  0.16459802 -0.36495537  0.5661655  -0.12618391 -0.20791891
  0.29892606 -0.34526533  0.6393879  -0.3273261   0.08474839  0.13683408
  0.35946202 -0.10909513  0.13562244  0.01976322  0.01416742  0.29540834
 -0.5311701   0.17471558 -0.06840585 -0.08591542 -0.24752787 -0.58655417
 -0.30315736  0.11845543 -0.34388775  0.12263131 -0.1319847  -0.07891323
 -0.09504982 -0.13533765  0.10654615 -0.01789737  0.32317844  0.04463275
 -0.25665426 -0.03957393  0.1385938  -0.2696635  -0.14101325 -0.11803633
  0.04030088  0.39442635  0.44087082 -0.48193505  0.2658804  -0.02544328
 -0.06982606  0.6837976   0.20892493  0.16810283 -0.5164373   0.47566214
  0.37114602 -0.20089066  0.13059133 -0.12839904 -0.70310724 -0.08323972
  0.0311303  -0.15263954 -0.37140995 -0.1778347  -0.28049788  0.3075685
 -0.01702526  0.02551858 -0.02620786  0.34174567 -0.31183726 -0.04751925
 -0.35693276 -0.03781903 -0.03228439 -0.40731296  0.13316345 -0.23278047
  0.07064917 -0.3717715  -0.2257106  -0.3920823   0.5041696   0.280724
 -0.28875822  0.07387003  0.09653803  0.22293046 -0.68787456  0.08693783
  0.05181105  0.23186478  0.27460158 -0.01328542  0.3941318   0.16915584
 -0.40412104  0.27134418 -0.35435602 -0.07688033  0.20469299 -0.07821294
 -0.5060066  -0.02997242 -0.08981784  0.40470755 -0.07545714  0.28365916
 -0.33338517  0.33287838  0.669706   -0.40899533 -0.52344555  0.38267794
  0.07682882 -0.28660285  0.14376807  0.17732166  0.36114502 -0.11196946]"
Build issue tenserflow 2.11.0 for tensorflow quantum stat:awaiting response type:build/install stale subtype:macOS TF 2.11,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.11.0

### Custom code

Yes

### OS platform and distribution

Mac OS M1

### Mobile device

_No response_

### Python version

3.9

### Bazel version

bazel 5.3.0

### GCC/compiler version

Apple clang version 14.0.3 (clang-1403.0.22.14.1) Target: arm64-apple-darwin22.5.0 Thread model: posix

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Trying to build package using bazel compiler, as given [here](https://www.tensorflow.org/quantum/install).  Fails to build the file. Gives the following output mentioned in log output, after running the given command.

Output: 
<img width=""1680"" alt=""Screenshot 2023-09-20 at 11 47 36"" src=""https://github.com/tensorflow/tensorflow/assets/69144860/e699f11d-fc31-49af-83b7-34e3330f3358"">


### Standalone code to reproduce the issue

```shell
bazel build -c opt --cxxopt=""-O3"" --cxxopt=""-march=native"" --cxxopt=""-std=c++17"" --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=1"" //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /private/var/tmp/_bazel_gb/4de1e45218bdc87c870302861e9b2675/external/boringssl/BUILD:161:11: Compiling src/crypto/x509/t_x509.c [for host] failed: (Exit 1): cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics ... (remaining 44 arguments skipped)
external/boringssl/src/crypto/x509/t_x509.c:321:18: error: variable 'l' set but not used [-Werror,-Wunused-but-set-variable]
    int ret = 0, l, i;
                 ^
1 error generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 709.728s, Critical Path: 52.92s
INFO: 4419 processes: 1508 internal, 2911 local.
FAILED: Build did NOT complete successfully
```
",False,"[-5.12064934e-01 -2.74830043e-01  1.11395679e-01  2.32748061e-01
  1.80539697e-01 -3.61209899e-01 -1.61602080e-01  6.05064668e-02
 -3.10835183e-01 -4.45231616e-01  2.69040372e-02 -1.08748063e-01
 -2.42052168e-01  1.19783759e-01 -1.60246015e-01  4.56478387e-01
 -3.25399101e-01 -1.38989821e-01  1.20635450e-01  1.32593006e-01
 -2.02059627e-01 -1.85229391e-01 -2.22562492e-01  1.42757595e-01
  7.61474296e-02  2.17907876e-01 -1.03838898e-01  1.80161104e-01
 -2.79581435e-02  2.26822495e-01  2.74692297e-01  1.31222919e-01
 -6.89299330e-02  8.74764100e-02  9.75782424e-02  3.40466559e-01
 -2.23677158e-02 -1.18697569e-01 -4.33883965e-01 -1.51381865e-01
 -6.29907399e-02  4.55087572e-02  3.94788206e-01 -3.44479755e-02
 -5.50765544e-05 -1.47083163e-01  1.83501139e-01 -2.16446623e-01
  1.73721928e-02 -2.94323444e-01 -1.08506814e-01 -6.99594803e-03
 -3.69081140e-01 -2.87088037e-01 -1.19509727e-01  1.28454685e-01
  1.75827175e-01 -7.25923553e-02  4.68045399e-02  8.45957696e-02
  1.14385314e-01 -9.69954673e-03  8.66157934e-02  3.81962731e-02
  2.93259203e-01  2.79842824e-01  1.92743048e-01 -1.32048965e-01
  4.10603583e-01 -9.31142047e-02  2.27030694e-01 -3.04886326e-02
 -4.02749777e-01  2.35320106e-01 -1.71372667e-02  1.31700933e-01
  1.22245751e-01  1.04485594e-01  3.20575178e-01 -9.82135981e-02
 -1.02351099e-01 -2.88864017e-01  1.78446472e-02 -9.49866101e-02
  1.23308167e-01 -1.69463694e-01  2.96941936e-01 -1.16635766e-02
  3.81525874e-01 -2.82426327e-01  4.84921604e-01  3.50433409e-01
  8.80862325e-02 -1.00692995e-01  5.45293331e-01 -1.98427476e-02
  1.91892788e-01  3.98930550e-01 -7.61083663e-02 -1.73808411e-02
 -6.76514059e-02 -1.24429896e-01 -6.25966042e-02 -5.85576929e-02
 -1.54350596e-02 -2.79316783e-01  2.73518562e-01 -2.01294988e-01
  1.54785156e-01  6.97094351e-02  2.33915329e-01  1.54467337e-02
  2.67572343e-01 -6.16543144e-02  1.93888210e-02 -1.43915668e-01
 -2.66319782e-01  1.79649174e-01  4.99701686e-02  9.37753499e-01
  6.58019707e-02  5.21825030e-02 -5.44437878e-02  1.01924859e-01
  3.29798758e-01 -7.38759898e-03 -1.86686829e-01  8.27298090e-02
  2.48673797e-01 -5.80339357e-02  1.10288665e-01  2.00249419e-01
  1.37912899e-01  2.36094743e-01 -7.51709640e-02 -4.41547334e-02
 -8.84545967e-02 -1.52439490e-01 -8.65514129e-02 -2.91208506e-01
 -3.22498143e-01  1.69655919e-01 -3.35247666e-02 -6.51871026e-01
  1.58739567e-01 -4.26589735e-02 -1.88057005e-01  2.05759883e-01
 -3.17172468e-01  2.25259010e-02 -7.76561722e-02 -3.40039916e-02
 -9.98569652e-04  2.04492927e-01  4.28158134e-01  1.21519759e-01
  2.65823454e-01 -2.15574279e-02 -1.34788096e-01 -4.49074626e-01
  5.24826869e-02  4.68467116e-01  2.25433353e-02 -6.74409717e-02
  2.35540345e-01  1.05851606e-01 -3.88803959e-01 -2.03997597e-01
  1.64158612e-01  4.55068618e-01 -1.56132385e-01 -1.90060258e-01
  1.32153809e-01  9.65553522e-02  7.34459460e-02 -2.99694866e-01
  2.91004419e-01 -6.52861893e-01  1.13173183e-02  2.19880283e-01
 -2.19997391e-03  6.54203519e-02 -1.86124444e-02  1.69208467e-01
  2.08574310e-02 -2.57395990e-02 -8.12918767e-02  1.94869876e-01
 -2.15910882e-01  3.15731652e-02 -4.28568423e-01  1.59901008e-02
  4.29259628e-01 -9.62679088e-02 -7.42923766e-02  1.02769405e-01
  6.62507862e-02 -1.82857662e-01 -1.83983088e-01  1.55214697e-01
  2.47690231e-02 -1.29819989e-01 -1.44545004e-01 -1.45700827e-01
 -1.84753537e-03 -4.76945013e-01 -4.46263216e-02 -4.24099684e-01
 -3.12946498e-01 -1.83453441e-01  2.64853425e-03 -3.21835041e-01
  1.89329803e-01 -1.43336251e-01 -3.53207916e-01  1.93288177e-01
  4.73981053e-02 -8.78881961e-02 -2.00373426e-01  1.88008368e-01
  1.28235757e-01 -3.47351372e-01 -2.41508573e-01 -3.56595993e-01
 -2.91738331e-01  1.11728497e-02 -4.14334893e-01  6.31854609e-02
 -4.21974957e-02  3.11692119e-01  1.61474526e-01  4.83093783e-03
  4.66312885e-01  1.32730514e-01  3.82884681e-01 -9.50066447e-02
 -6.54828176e-02 -3.04488420e-01 -1.38419062e-01  1.84569672e-01
 -4.50472474e-01 -5.66833466e-02  5.91261126e-03  1.65365450e-02
  3.42301965e-01  1.54283941e-01 -7.36500323e-02 -1.18422195e-01
 -2.72583485e-01  3.73731345e-01 -1.45308420e-01  2.45021716e-01
  4.29561466e-01  1.51498288e-01  2.58150399e-01  2.38586053e-01
  7.03462735e-02  1.20589353e-01  9.19407308e-02 -1.67365462e-01
  4.37233031e-01  1.93133485e-03  1.63787469e-01  2.17127442e-01
  1.81622893e-01  2.76951820e-01 -4.64900136e-01  3.41010749e-01
  8.71806815e-02 -1.44340517e-02  1.17985696e-01 -2.59848475e-01
  6.04213834e-01 -4.27143544e-01 -1.22127146e-01 -1.03438556e-01
  2.19068527e-01  7.83212297e-03 -2.38493368e-01  7.43123963e-02
 -2.87484676e-02  3.62713993e-01 -2.71225631e-01  1.95966750e-01
 -1.05512798e-01 -1.83296949e-01  1.08587474e-01 -6.32265687e-01
 -1.70220762e-01  7.00895935e-02 -3.36077869e-01  1.14366800e-01
 -2.97111541e-01 -1.06937192e-01 -1.70774698e-01  1.13416918e-01
 -3.06777656e-05 -1.56180814e-01 -9.34297219e-03  1.84397757e-01
  7.62370974e-02  3.00103165e-02  2.71509290e-01 -1.93315417e-01
 -1.70766801e-01 -5.59692830e-03  3.15107137e-01  2.75883913e-01
  3.09158504e-01 -3.55561972e-01  3.40781927e-01 -6.51992634e-02
 -8.83271545e-02  4.53965157e-01  6.38009794e-03  7.90788084e-02
 -4.30038661e-01  6.17096305e-01  2.21939057e-01 -1.46076351e-01
  2.13125989e-01 -1.62697852e-01 -4.96287227e-01 -1.53723359e-02
  3.89490008e-01 -9.79117155e-02 -1.96919683e-02 -4.48574841e-01
  7.12046325e-02  7.94844925e-02 -9.06859636e-02 -1.12231612e-01
 -2.46972412e-01  1.42000407e-01 -1.69908911e-01  1.51081353e-01
 -2.48511970e-01  1.23672038e-02 -1.32036120e-01 -2.36789465e-01
 -8.25415775e-02 -1.39149986e-02 -5.58609925e-02 -3.85293085e-03
 -6.60519022e-03 -2.83015847e-01  2.91909069e-01  4.17016238e-01
 -1.76546216e-01  2.57799327e-01 -1.25339627e-01  3.65671605e-01
 -4.52536464e-01  4.52879816e-02 -1.76970512e-01  2.32772708e-01
  8.89988691e-02 -1.37485042e-01  5.94416380e-01  2.89022565e-01
 -2.93756127e-01  8.05186033e-02 -2.67748058e-01  1.32891610e-01
  1.42966181e-01 -2.42597595e-01 -3.55144501e-01 -1.59023046e-01
 -6.74133003e-02  3.22214991e-01  2.58251652e-03  1.88300937e-01
 -2.42954552e-01  2.85324454e-01  5.89565814e-01 -4.00416315e-01
 -3.48572254e-01  1.52752727e-01 -7.58636147e-02 -1.90503389e-01
  1.47975683e-01 -8.79077613e-02  2.56634336e-02  1.63034573e-02]"
Keeping last layer names in Stacked Model stat:awaiting response type:feature stale comp:keras,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS 13.4

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using Model Stacking, I would like to access the last layer in the last stacked model to pass individual loss functions to them
```python
# ------------------ Encoder ------------------
encoder_model = Model(inputs=all_inputs, outputs=latent_space, name=""encoder"")
# ... irrelevant code 

# ------------------ Decoder ------------------
# Output Layers
numeric_output = Dense(
    self.numeric_dim, activation=""linear"", name=""numeric_output"")(decoder2)
binary_output = Dense(
    self.binary_dim, activation=""sigmoid"", name=""binary_output"")(decoder2)

decoder_output = [numeric_output] + [binary_output]
decoder_model = Model(inputs=latent_input, outputs=decoder_output, name=""decoder"")

# ------------------ Autoencoder ------------------
autoencoder_output = decoder_model(encoder_model(all_inputs))
autoencoder = Model(inputs=all_inputs, outputs=pass_through_layers, name=""autoencoder"")

# This will not work:
losses = {
            ""numeric_output"": ""mse"",
            ""binary_output"": ""binary_crossentropy""
}
autoencoder.compile(optimizer=Adam(learning_rate=lr), loss=losses)
```

The last layer will be renamed to decoder1, decoder2, etc. 
It would be much nicer if there was some passthrough argument in a model that would allow referencing the last layer in the stacked model directly

### Standalone code to reproduce the issue

```shell
This will fail


import numpy as np
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Input dimensions
input_dim = 10
latent_dim = 5
numeric_dim = 3
binary_dim = 2

# Learning rate
lr = 0.001

# Number of samples
n_samples = 1000

# Generate random data
X = np.random.rand(n_samples, input_dim)
y_numeric = np.random.rand(n_samples, numeric_dim)
y_binary = np.random.randint(0, 2, size=(n_samples, binary_dim))

# ------------------ Encoder ------------------
all_inputs = Input(shape=(input_dim,), name=""all_inputs"")
latent_space = Dense(latent_dim, activation=""relu"", name=""latent_space"")(all_inputs)
encoder_model = Model(inputs=all_inputs, outputs=latent_space, name=""encoder"")

# ------------------ Decoder ------------------
latent_input = Input(shape=(latent_dim,), name=""latent_input"")
decoder2 = Dense(10, activation=""relu"", name=""decoder2"")(latent_input)

numeric_output = Dense(numeric_dim, activation=""linear"", name=""numeric_output"")(decoder2)
binary_output = Dense(binary_dim, activation=""sigmoid"", name=""binary_output"")(decoder2)

decoder_output = [numeric_output, binary_output]
decoder_model = Model(inputs=latent_input, outputs=decoder_output, name=""decoder"")

# ------------------ Autoencoder ------------------
autoencoder_output = decoder_model(encoder_model(all_inputs))

# This will not work:
losses = {
    ""numeric_output"": ""mse"",
    ""binary_output"": ""binary_crossentropy""
}
autoencoder = Model(inputs=all_inputs, outputs=autoencoder_output, name=""autoencoder"")
autoencoder.compile(optimizer=Adam(learning_rate=lr), loss=losses)

# Fit the model
autoencoder.fit(
    x=X,
    y=[y_numeric, y_binary],
    epochs=10,
    batch_size=32
)
```


### Relevant log output

_No response_",False,"[-6.01686478e-01 -4.53705974e-02 -8.87538195e-02 -1.04486696e-01
  2.93036222e-01 -3.04672360e-01 -1.11931287e-01 -3.37366760e-02
 -1.67044222e-01 -2.50455350e-01  3.24293107e-01 -1.97426111e-01
 -5.28683849e-02 -4.87375446e-03 -2.42064759e-01  3.48788381e-01
 -4.01794940e-01  3.11458856e-01  2.42416263e-01  4.09977376e-01
 -1.71790689e-01 -8.81401077e-02 -8.36700648e-02  2.24291250e-01
  2.23443568e-01  1.46169335e-01 -2.91393459e-01  9.17498954e-03
 -1.18737996e-01  8.21899623e-02  3.94530892e-01  2.68615365e-01
 -9.82526839e-02  2.83370376e-01  1.94655955e-02  2.41033345e-01
 -3.76165658e-01 -3.11631441e-01 -3.87908041e-01  9.81271863e-02
  6.06854483e-02  1.36816412e-01  6.23474792e-02 -2.62489319e-01
  4.27036956e-02 -2.27573067e-01 -9.76170599e-02 -3.97046387e-01
  4.51928470e-03 -3.75141025e-01 -5.94131351e-02 -2.32186407e-01
 -5.11558890e-01 -5.45483947e-01 -2.78509974e-01 -1.87052131e-01
  2.51061529e-01 -2.75615573e-01 -1.00567833e-01  1.53471291e-01
 -5.20550683e-02 -7.27420002e-02  5.76720908e-02 -1.78213805e-01
  4.70551729e-01  9.89945605e-02  1.27209693e-01  1.10831507e-01
  4.51938033e-01 -1.33799285e-01  5.60863987e-02 -4.32341620e-02
 -3.53589803e-01  3.94230783e-02  8.26825574e-02  2.05648974e-01
  9.84593853e-03 -6.13193773e-03  3.89016479e-01 -2.27086693e-01
 -2.41214642e-03 -2.40676001e-01 -2.49620140e-01 -2.01322377e-01
  1.20220378e-01 -4.30706516e-02  3.72893125e-01 -3.13278884e-02
  3.65790546e-01 -2.80317426e-01  4.07014102e-01  4.04523641e-01
 -5.27748019e-02  2.54501224e-01  4.55139875e-01  1.52097687e-01
  1.02098942e-01  4.01756428e-02 -7.42696449e-02 -7.03979582e-02
 -9.84230042e-02 -1.24571837e-01 -2.48068776e-02 -4.02061678e-02
 -1.05248407e-01 -2.21552849e-01  2.69653857e-01 -2.13227957e-01
 -8.67926106e-02 -2.66788602e-02  1.82351023e-01  5.33657745e-02
  1.19185001e-01 -2.48817265e-01 -1.23698756e-01 -3.33474994e-01
  1.04314253e-01  1.56347245e-01 -2.07000196e-01  5.67770362e-01
  2.12575272e-01 -2.70187497e-01  2.34374508e-01  2.49408647e-01
  6.18110478e-01  1.84325963e-01 -1.84584647e-01  1.64538212e-02
  7.43190497e-02 -1.89645350e-01  2.74695873e-01  7.90748745e-03
  4.13809061e-01  1.06287993e-01 -1.76490337e-01  7.01513588e-02
 -2.03219354e-01  4.81963158e-04 -5.77223241e-01 -4.78110537e-02
 -2.59574533e-01  1.06386408e-01 -1.85027659e-01 -6.28067434e-01
  3.56609941e-01  2.21080929e-01 -3.68188441e-01 -1.77452713e-02
 -3.42906490e-02  2.53573447e-01  1.11125171e-01  4.90798950e-02
  1.78780928e-02  2.91428596e-01  1.93305880e-01  1.43124431e-01
  6.22810870e-02 -9.43865627e-04  4.93202507e-02 -5.08234739e-01
  1.71928465e-01  3.30703437e-01 -5.87617531e-02 -3.70272063e-02
  3.31385732e-01  1.16300777e-01 -2.28612274e-01 -3.57147247e-01
  1.69019341e-01  3.48765612e-01 -3.88467491e-01  2.33599301e-02
  4.31891978e-02  5.52047864e-02  3.07990342e-01 -2.16307253e-01
 -1.12769611e-01 -5.85883081e-01  5.65819331e-02  2.63464034e-01
 -1.05286047e-01  1.61302492e-01  2.50953771e-02  1.20349020e-01
  3.82575095e-02  7.69153237e-02  9.69081223e-02  3.29155400e-02
 -2.93684065e-01  6.07899129e-02 -5.04149437e-01 -7.36706629e-02
  4.90635723e-01 -1.19033173e-01  4.79159132e-02  6.38372153e-02
  1.87885761e-01  1.24241561e-02  1.56786852e-03 -5.11536375e-02
 -4.36515361e-01 -1.85740530e-01 -1.68878809e-01 -7.15190992e-02
 -1.44433290e-01 -4.21497822e-01  9.77448970e-02 -3.05514604e-01
 -1.80977866e-01  3.93994033e-01 -7.20284879e-02 -7.59348333e-01
  3.56426090e-02  1.43646359e-01 -2.48841703e-01  3.28439772e-01
  1.49424691e-02  1.16914839e-01 -2.83914149e-01  1.04211122e-01
  3.01013827e-01 -3.23315591e-01  1.13758110e-01 -3.50231111e-01
 -2.07442582e-01  2.14177191e-01 -1.55349821e-01  1.94745764e-01
  1.93826668e-02  3.83345857e-02  1.89316452e-01  1.33330524e-01
  3.50607693e-01  2.46558666e-01  4.42277074e-01 -3.10279131e-01
 -6.72282372e-03 -7.87667334e-02  2.25013569e-01  2.47791000e-02
 -6.47777557e-01 -3.61105919e-01 -4.87712398e-02 -6.26850128e-02
  2.15010881e-01  4.62699980e-01 -4.26210403e-01  1.77575015e-02
 -3.56737763e-01 -3.52162346e-02 -4.48074341e-01  1.56535685e-01
  6.61733508e-01  6.76939264e-02  2.65465200e-01 -6.97090626e-02
  2.87979960e-01  3.02638531e-01  3.68467182e-01 -1.81733128e-02
  4.83606398e-01  1.43006369e-01  1.66913241e-01  3.71559143e-01
  1.38275206e-01  1.80778876e-01 -3.90017331e-01  5.87063849e-01
 -3.78609784e-02 -3.23278069e-01  1.15836576e-01 -2.94046819e-01
  4.54529941e-01 -5.68974495e-01  1.85945369e-02  1.16912864e-01
  3.36416334e-01  4.30992898e-03 -1.97749995e-02  1.87500849e-01
  6.48289397e-02  3.24715257e-01 -3.54054511e-01  1.00940719e-01
  4.34209406e-02 -3.95393193e-01 -9.12706107e-02 -6.12902522e-01
  4.29922603e-02 -3.94700617e-02 -6.16952926e-02  1.49579301e-01
 -7.22308904e-02 -1.92038510e-02 -2.51054019e-01  2.11156666e-01
  1.05723493e-01 -2.48497799e-01  1.12623855e-01  4.12890576e-02
 -3.78699839e-01  1.61337018e-01  5.56401134e-01 -5.93989491e-01
 -2.70554662e-01 -2.76531816e-01  3.84319425e-01  5.51134586e-01
  4.14492577e-01 -2.07932994e-01  2.72392511e-01 -6.98067397e-02
 -1.16721615e-01  5.76010585e-01 -2.31967606e-02 -1.67806044e-01
 -7.91562498e-02  6.82663202e-01 -1.76389754e-01 -4.73282784e-02
  2.71308750e-01 -1.09615080e-01  1.47522055e-02  1.06443055e-01
  3.66245627e-01 -1.86013859e-02 -3.03419121e-02 -1.39243722e-01
  2.15802446e-01  4.22320724e-01  2.93768849e-02  1.55123556e-02
 -6.80547506e-02  5.07944077e-02 -6.89382926e-02 -7.29653835e-02
 -4.09997523e-01  5.11857033e-01  2.29693111e-02 -3.79834712e-01
 -5.12628794e-01  6.35762289e-02 -7.82347322e-02 -1.09531462e-01
  1.48061290e-01 -3.09808195e-01  3.45915332e-02  6.30835593e-01
  3.06450874e-02  3.33057523e-01  1.81812406e-01  8.72860998e-02
 -5.11219382e-01  6.24055117e-02 -8.73425379e-02  6.60789967e-01
  1.95728183e-01 -2.73041070e-01  3.69146824e-01  3.29562306e-01
 -1.32866532e-01 -3.33854929e-02 -1.42798483e-01  1.23290576e-01
  7.21127689e-02 -2.23024622e-01  9.13578868e-02 -5.51591694e-01
 -7.50975385e-02  7.05054477e-02 -1.98412746e-01 -5.13956398e-02
 -3.89245152e-01  3.50454807e-01  2.56283909e-01 -4.34375346e-01
 -8.65666270e-02  6.16543740e-02  2.73263305e-01  1.42392978e-01
 -3.81116793e-02 -4.42983627e-01 -1.60903171e-01  3.30683142e-02]"
Problem installing tensorflow 2.13.0 stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04, Lambda Labs TensorBook

### Mobile device

n/a

### Python version

3.11.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

8.6.0.163

### GPU model and memory

_No response_

### Current behavior?

TensorFlow 2.1.13 won't run because of an undefined symbol in libtensorflow_cc, apparently a google.protobuf.Message symbol.

### Standalone code to reproduce the issue

```shell
I am on a LambdaLabs TensorBook. This does come with a preloaded tensorflow installation in /usr/lib/python3/dist-packages
However my reading of the error messages on install does not seem to suggest that conflict with that package is to blame here.

Installing per instructions here: https://www.tensorflow.org/install/pip

~~~
conda create --name cuda11
conda activate cuda11
conda install python
conda install -c conda-forge cudatoolkit=11.8.0
python3 -m pip install nvidia-cudnn-cu11==8.6.0.163 tensorflow==2.13.
python3 -c ""import tensorflow""
~~~

...yields the following error message...

~~~
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/yam/miniconda3/envs/cuda11/lib/python3.11/site-packages/tensorflow/__init__.py"", line 38, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/yam/miniconda3/envs/cuda11/lib/python3.11/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/home/yam/miniconda3/envs/cuda11/lib/python3.11/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 26, in <module>
    self_check.preload_check()
  File ""/home/yam/miniconda3/envs/cuda11/lib/python3.11/site-packages/tensorflow/python/platform/self_check.py"", line 63, in preload_check
    from tensorflow.python.platform import _pywrap_cpu_feature_guard
ImportError: /home/yam/miniconda3/envs/cuda11/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2: undefined symbol: _ZN6google8protobuf7Message19CopyWithSourceCheckERS1_RKS1_
~~~

pip package info:

~~~
(cuda11) yam@TensorYam:~$ python -m pip list
Package                      Version
---------------------------- ---------
absl-py                      2.0.0
astunparse                   1.6.3
cachetools                   5.3.1
certifi                      2023.7.22
charset-normalizer           3.2.0
flatbuffers                  23.5.26
gast                         0.4.0
google-auth                  2.23.0
google-auth-oauthlib         1.0.0
google-pasta                 0.2.0
grpcio                       1.58.0
h5py                         3.9.0
idna                         3.4
keras                        2.13.1
libclang                     16.0.6
Markdown                     3.4.4
MarkupSafe                   2.1.3
numpy                        1.24.3
nvidia-cublas-cu11           11.11.3.6
nvidia-cudnn-cu11            8.6.0.163
oauthlib                     3.2.2
opt-einsum                   3.3.0
packaging                    23.1
pip                          23.2.1
protobuf                     4.24.3
pyasn1                       0.5.0
pyasn1-modules               0.3.0
requests                     2.31.0
requests-oauthlib            1.3.1
rsa                          4.9
setuptools                   68.0.0
six                          1.16.0
tensorboard                  2.13.0
tensorboard-data-server      0.7.1
tensorflow                   2.13.0
tensorflow-estimator         2.13.0
tensorflow-io-gcs-filesystem 0.34.0
termcolor                    2.3.0
typing_extensions            4.5.0
urllib3                      1.26.16
Werkzeug                     2.3.7
wheel                        0.38.4
wrapt                        1.15.0
~~~
```


### Relevant log output

_No response_",False,"[-0.47634006 -0.49694425 -0.01582225  0.22703008  0.25044978 -0.45008844
 -0.15908074  0.01748157 -0.20657866 -0.39553723  0.11079541  0.0728731
 -0.25041398  0.10541971 -0.25404924  0.5678938  -0.29058576 -0.21109006
  0.22476478  0.24013792 -0.13179156 -0.11325476 -0.30873048  0.17539498
  0.24624613  0.20118353 -0.3861884  -0.01537254  0.13639648  0.17111057
  0.6373297   0.06857066 -0.041077    0.14935417  0.21241394  0.28705853
 -0.27342305 -0.22470273 -0.18297121 -0.03556447  0.20781957  0.07376879
  0.21447827 -0.22875215  0.02573889 -0.25978696  0.08426304 -0.15866597
  0.04579606 -0.31162775 -0.13094676 -0.09949464 -0.37678546 -0.24847956
 -0.15194558 -0.02848847  0.17471704  0.0355567   0.02799041  0.26149124
  0.217537    0.07193945  0.1641971   0.03218634  0.00536498  0.15318269
  0.13709342 -0.2532599   0.60270953 -0.18751097  0.2260216  -0.12313899
 -0.32095742  0.10896716 -0.02207932  0.22218835  0.09784552  0.21190917
  0.33554563 -0.11675857 -0.16973281 -0.25632545  0.0596063  -0.26610774
  0.10847533 -0.17817248  0.33031788  0.21425338  0.4159142  -0.32765713
  0.5857586   0.35456222  0.13308474  0.00315535  0.48595065  0.0613505
  0.14454821  0.40298855 -0.07426073 -0.27899462 -0.15613873 -0.31768107
  0.09804776  0.09700133 -0.15405142 -0.07948957  0.18131486 -0.2586017
  0.1104963  -0.17776996  0.18516442 -0.11990781  0.41822335  0.17139491
  0.07546384  0.02480353 -0.4354568   0.16994964 -0.0232748   0.9112735
 -0.11316601 -0.05465604  0.06739625  0.02556217  0.36484247  0.05110736
 -0.10618354 -0.1409544   0.16849858  0.10026744  0.00305527  0.21237971
 -0.13135704  0.201165   -0.03216995  0.09035376 -0.13068298 -0.16195908
 -0.02846069 -0.20623103 -0.14460501  0.33325982 -0.10914448 -0.59452355
  0.22871713 -0.05728178 -0.03423514  0.21808186 -0.20572236  0.15463671
 -0.15143861  0.05019377 -0.14699611  0.36324048  0.19782177  0.16639416
  0.29089284 -0.06104775 -0.1140607  -0.6225524   0.14102511  0.64410627
 -0.01874463 -0.14856961  0.0420712   0.24721043 -0.5254823  -0.39191085
  0.19097778  0.54209685 -0.08048271 -0.17422608  0.19676247  0.27577108
  0.22028592 -0.11960089  0.5915711  -0.6890088  -0.1133977   0.33367282
  0.13450754  0.17957273  0.09063359  0.16904871  0.10301329 -0.03518717
 -0.0487334  -0.09422892 -0.2908666   0.07409491 -0.37183225 -0.08490469
  0.50473094 -0.09446926 -0.20621777  0.20919743  0.31787002 -0.02940801
  0.02388093  0.04240942 -0.02451369 -0.06575944 -0.18596533  0.00194958
  0.02303285 -0.3654077  -0.11076093 -0.26722527 -0.41711664 -0.17769134
  0.06745464 -0.37991595  0.18812132 -0.11468089 -0.30973586  0.2086858
  0.14147632  0.07758756 -0.18476917  0.2730011   0.07771897 -0.16894996
 -0.11210965 -0.31647092 -0.22700563  0.04365048 -0.215025    0.08382984
  0.05590321  0.2548194  -0.05614509  0.09955567  0.4200293   0.26134327
  0.37770492 -0.22111034 -0.11434172 -0.1088698  -0.21504799  0.10470739
 -0.41034967 -0.13317338 -0.01069667  0.03397465  0.2561822   0.18334204
 -0.18613324 -0.18922703 -0.3285523   0.3453167  -0.00889614  0.14154539
  0.23366174  0.14107944  0.42537493  0.33317414  0.08828171  0.16062577
  0.23753133 -0.10995315  0.29987     0.24000168 -0.09576039  0.36654603
  0.22407734  0.27232957 -0.5499503   0.4571227   0.14724883 -0.13068643
  0.1613796  -0.43285155  0.5912581  -0.45886543 -0.17898893 -0.06241707
  0.4307666   0.10856302 -0.06999896 -0.08587684  0.08798589  0.467743
 -0.4007939  -0.08583775  0.05160958 -0.11030556  0.02890461 -0.8325576
 -0.35069132  0.27364296 -0.37465894  0.15625383 -0.29386914  0.00312003
 -0.36364788 -0.01364966  0.2039032  -0.06270552  0.16093203  0.3660171
 -0.3066283  -0.04935108  0.46211863 -0.565943   -0.16303323 -0.13978283
  0.17555529  0.22522295  0.45161867 -0.52867776  0.2770685  -0.04707443
  0.00324358  0.47865832  0.20685247  0.07373537 -0.3850563   0.5292897
  0.33620387 -0.21279848  0.25205934 -0.28213271 -0.41570526 -0.07656956
  0.45492435 -0.17864457 -0.14176229 -0.48169243 -0.15795779  0.2722224
 -0.1065305   0.01067493 -0.02643926  0.19529557 -0.43028426 -0.05707245
 -0.50453895  0.11794159 -0.00758243 -0.50772965 -0.08974236 -0.25680515
 -0.0979589  -0.378106   -0.06840575 -0.42329133  0.54474574  0.34091568
 -0.33882582  0.07999863 -0.08985674  0.2849482  -0.4410835  -0.0593917
 -0.06025828  0.20184693  0.13630761 -0.176067    0.548768    0.35595307
 -0.24296063  0.25238705 -0.26127112 -0.01359781  0.29873547 -0.30822816
 -0.23139025 -0.08115311 -0.00530776  0.5107516  -0.10977969  0.31661716
 -0.3827182   0.19564459  0.5963424  -0.596462   -0.5503945   0.28156197
 -0.11212674 -0.29127342 -0.13103665 -0.06545703  0.19673213 -0.17698336]"
TensorFlow lite cmake compilation failed to allocate memory stat:awaiting response type:build/install comp:lite wsl2,"Any update with this issue? I'm having the same problem described by @Luca-Stefanescu

I am building Tensorflow Lite with cmake following the instruction given on the minimal example. I am building in a docker container with ubuntu using WSL 2 with docker desktop. The build seems to work until 91%. Then it will start to allocate all the memory (16gb of ram + 8gb of swap) until it fails to allocate throwing an allocation error or sometimes an input/output error.
I think that this error message could be helpful:
```
In file included from /workspaces/tfl-dev-env/tie/../tensorflow_src/tensorflow/lite/kernels/internal/runtime_shape.h:22,
                 from /workspaces/tfl-dev-env/tie/../tensorflow_src/tensorflow/lite/kernels/internal/types.h:24,
                 from /workspaces/tfl-dev-env/tie/../tensorflow_src/tensorflow/lite/kernels/internal/tensor_ctypes.h:22,
                 from /workspaces/tfl-dev-env/tensorflow_src/tensorflow/lite/kernels/embedding_lookup_sparse.cc:72:
/usr/include/c++/13/memory:81:12: fatal error: /workspaces/tfl-dev-env/tie/../tensorflow_src/bits/shared_ptr_atomic.h: Cannot allocate memory
   81 | #  include <bits/shared_ptr_atomic.h>
      |            ^~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```

However trying to follow the same steps on a ubuntu VM using VMWare (and with less memory) seems to work.

_Originally posted by @lorenzodellagiustina in https://github.com/tensorflow/tensorflow/issues/61485#issuecomment-1725196272_
            ",False,"[-1.79760814e-01 -2.42422760e-01 -2.53921151e-01 -1.85569935e-02
  4.04750183e-02 -1.10837758e-01 -2.91514933e-01 -2.36077048e-03
 -2.71901190e-01 -1.45787001e-01 -2.22036570e-01 -7.84163829e-04
 -1.45511955e-01 -7.12666512e-02 -1.22145750e-02  2.52359420e-01
 -1.67177081e-01 -9.46096331e-02  1.33913219e-01 -7.39029050e-02
 -2.93383867e-01  6.56897947e-02 -9.20113996e-02 -6.73370734e-02
  2.79264510e-01  9.32710767e-02 -1.05402738e-01 -1.38142165e-02
  1.55463904e-01  1.91965252e-01  5.75261176e-01 -1.74051262e-02
  3.49651463e-03  1.04990214e-01 -3.97115387e-03  2.43835866e-01
 -3.79330367e-01 -1.90824911e-01 -1.03607913e-02 -2.49899440e-02
 -5.52487094e-03  2.97084779e-01  1.15123332e-01  2.07498193e-01
 -8.23204741e-02  1.55709326e-01 -2.70772576e-02  9.79823843e-02
 -1.48616999e-01 -6.07211888e-02 -8.57406333e-02  3.46267931e-02
 -1.37925074e-01 -1.35894299e-01 -3.70569825e-02  3.51559371e-01
 -1.66389585e-01  7.73630217e-02  2.25718282e-02  5.45484498e-02
  2.62415223e-02 -9.00184065e-02  1.96099907e-01 -1.74639761e-01
  1.53083708e-02  1.98337391e-01  4.07390684e-01  2.05751285e-02
  5.04240155e-01 -1.05659001e-01  3.21589150e-02 -6.01118617e-02
 -4.53972220e-01 -2.62996137e-01  9.13228691e-02  1.83524847e-01
 -3.25879812e-01  3.47348928e-01  1.63846344e-01 -1.05903424e-01
 -7.51891285e-02 -7.85718709e-02  5.60351163e-02 -1.60637885e-01
  1.20115578e-01  5.05784303e-02  2.46954083e-01  1.93409212e-02
  8.09422061e-02 -1.00605428e-01  1.88898355e-01  1.80575013e-01
 -2.92391866e-01  2.42125988e-01  3.88383478e-01  2.51212895e-01
  5.31387050e-03  2.09932521e-01 -1.29667550e-01 -9.10770893e-02
 -7.38905966e-02 -1.49615943e-01 -5.89956641e-02  2.78485894e-01
 -1.80755436e-01 -4.00134057e-01  3.02750170e-01  4.22720432e-01
 -5.69028631e-02 -2.82862633e-01  2.88526177e-01  7.71177709e-02
  2.48145565e-01  9.95672643e-02  9.50952545e-02  3.15313898e-02
  5.78511730e-02  9.58753228e-02  2.29154713e-02  5.18257916e-01
 -3.29331309e-02  7.29629546e-02 -2.06762075e-01  6.63361251e-02
  3.46070044e-02 -9.15677026e-02 -1.64390221e-01 -7.63775110e-02
  5.68403304e-02 -1.12893865e-01 -7.36662969e-02  2.32986033e-01
 -1.13122016e-01 -2.13760771e-02  2.16888934e-01 -1.59262121e-02
 -3.50220613e-02 -1.97309703e-01 -4.61289063e-02 -2.45271966e-01
 -1.58556074e-01  3.09480786e-01 -4.88830172e-02 -4.62373614e-01
 -2.17439294e-01  3.60089362e-01 -4.33593988e-03  4.44483273e-02
 -1.21666789e-01  5.76157123e-03 -8.43033344e-02  1.85362101e-02
 -2.64595985e-01  3.36943775e-01  3.19728613e-01  7.19624832e-02
  1.93146378e-01 -2.88940407e-02 -2.01956686e-02 -2.61809856e-01
 -2.43316472e-01  3.41027230e-01 -6.03934750e-02  1.14015900e-02
 -8.53279978e-02  8.25443417e-02 -2.43407235e-01  3.21100652e-02
 -1.65173501e-01  1.73864946e-01 -8.19791108e-02 -6.76976144e-02
  1.36520453e-02 -1.29403204e-01  3.18694592e-01 -1.22957483e-01
  4.44377720e-01 -3.96840632e-01  7.78878340e-03  2.54142694e-02
  4.20389324e-02  1.97996527e-01  2.74260521e-01 -1.42651051e-01
  2.05919612e-02  1.00994281e-01  2.21450433e-01  2.70756364e-01
 -2.70986408e-01 -1.79151893e-01 -5.89615345e-01  9.27649140e-02
  1.01165876e-01  1.03034340e-01 -6.05722778e-02 -2.44409189e-01
  1.19677223e-01  4.79725748e-02  1.55148953e-01  2.63328552e-02
  2.69357003e-02  2.59819359e-01  9.77042615e-02  1.58069402e-01
  2.25393787e-01 -1.13205664e-01 -2.32801720e-01 -6.13952503e-02
 -1.53545737e-01 -4.56311032e-02 -2.09808443e-02 -1.50584504e-01
 -5.02114780e-02 -1.12221181e-01 -2.43983433e-01  1.84535727e-01
  3.53949629e-02  4.53306735e-03 -1.01298243e-01  2.32416853e-01
  1.05449744e-01 -6.40568659e-02 -7.15510249e-02 -2.14242458e-01
 -2.28182256e-01 -9.26970989e-02 -1.40685946e-01 -1.19506516e-01
  2.22132299e-02  2.59058207e-01 -1.51789129e-01 -8.05050731e-02
  7.53114969e-02 -2.87941158e-01  1.04334638e-01  6.95367306e-02
  3.01816016e-02 -6.73520565e-02 -4.59972411e-01  5.07731922e-02
 -6.03334606e-03 -5.07996678e-01 -3.86377871e-02  8.55528042e-02
  1.78635985e-01  2.18100131e-01 -2.24097535e-01  7.66018629e-02
  1.44834250e-01  1.86270565e-01  1.59376357e-02 -3.69574875e-04
  2.63865113e-01  2.48215199e-01  4.83208001e-01  4.59712893e-02
 -2.58640256e-02  9.79635715e-02  2.95643695e-02  6.74882345e-03
  1.98878571e-01  4.24680412e-01  2.50998251e-02  5.30319631e-01
  2.27203384e-01  7.26675838e-02 -1.28303573e-01 -7.39004165e-02
 -2.39875242e-02  5.85549697e-03  1.63788110e-01 -5.09864688e-02
 -1.43801328e-02 -4.17560041e-02  5.96877374e-02 -1.79160386e-03
  3.16247702e-01 -2.61611581e-01 -1.93278551e-01  1.50925189e-01
  1.42117858e-01  1.55918449e-01 -5.87446205e-02 -2.28627026e-01
  8.23104680e-02  4.22129594e-02  3.60610008e-01 -4.55712616e-01
 -2.61166513e-01  2.45783687e-01 -2.19265640e-01 -1.18628085e-01
 -8.84965248e-03  2.81504482e-01 -2.13998288e-01 -1.51150703e-01
 -3.88214476e-02 -2.03068312e-02  3.28308880e-01  2.80580912e-02
 -1.81560785e-01  2.61191726e-01  1.71197504e-01 -2.22483844e-01
 -2.48344362e-01  2.47492701e-01  2.46170834e-01 -2.91654408e-01
  5.12375057e-01 -4.45413947e-01  2.11236387e-01 -4.45190519e-02
 -2.00929269e-01  2.79104590e-01 -2.19666213e-01 -7.50826821e-02
 -3.22374225e-01  3.44605684e-01  1.32859290e-01  3.48557867e-02
  4.97841686e-02 -1.24630012e-01 -3.56878757e-01 -4.07444425e-02
  1.19321793e-01 -1.17254099e-02 -8.48825574e-02 -4.64231558e-02
 -1.57584980e-01  3.80457379e-02 -2.79935133e-02 -1.92740560e-01
 -1.40973955e-01  3.44236232e-02 -3.51698130e-01 -3.18616033e-02
 -1.70801401e-01  2.50709560e-02 -1.88857898e-01 -5.34040451e-01
  1.48513108e-01 -3.11654359e-01  1.93533808e-01 -2.90950507e-01
  5.44138923e-02  4.72646840e-02  2.17600659e-01  3.53980780e-01
 -1.34068877e-01 -1.07529037e-01  1.13869339e-01 -9.50218365e-03
 -1.16051182e-01 -1.40509039e-01 -2.77411286e-02  1.34767875e-01
 -1.58193663e-01  1.71769351e-01  3.04063857e-01  2.11371630e-01
 -3.69586766e-01  1.23282023e-01 -1.98249549e-01 -1.01575524e-01
  1.14519365e-01 -1.51507586e-01 -4.53228951e-01  3.85328606e-02
  2.04865366e-01  4.19087827e-01 -1.40808197e-02  2.05181107e-01
 -2.90029794e-01  1.82835370e-01  2.63196975e-01  9.67712887e-03
 -9.94226485e-02  2.71516256e-02 -1.21207438e-01 -3.75257969e-01
 -2.70014912e-01  1.08494848e-01 -7.24471360e-02 -8.76702070e-02]"
DenseFeatures Feature column combine order stat:awaiting response type:support stale comp:apis TF 2.13,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have this code to create DenseFeatures layer.

 ```
       # Define the input for each feature column.
        inputs = {}
        for col in feature_columns:
            if type(col) == type(tf.feature_column.numeric_column(""temp"")):
                dtype = tf.float32
                key = col.key
            else:
                dtype = tf.int64
                key = col.categorical_column.key
            inputs[key] = tf.keras.layers.Input(name=key, shape=(), dtype=dtype)

        // Now use a DenseFeatures layer to combine them
        x = tf.keras.layers.DenseFeatures(feature_columns)(inputs)
```

-----------------

Now, I am trying to implement the output of the DenseFeatures layer for inference. I am facing a situation where I cannot use the TensorFlow model due to latency constraints.

However, my issue is that the manner in which DenseFeatures combines the inputs does not follow the order specified in feature_columns, nor is it sorted based on the feature names.

Is there a way I can determine the order in which these feature columns are combined within the DenseFeatures layer?

As I understand, in most places it is mentioned that it should follow the same order as the feature columns, but this is not what I am observing. I am using TensorFlow 2.13.

### Standalone code to reproduce the issue

```shell
# Define the input for each feature column.
        inputs = {}
        for col in feature_columns:
            if type(col) == type(tf.feature_column.numeric_column(""temp"")):
                dtype = tf.float32
                key = col.key
            else:
                dtype = tf.int64
                key = col.categorical_column.key
            inputs[key] = tf.keras.layers.Input(name=key, shape=(), dtype=dtype)

        // Now use a DenseFeatures layer to combine them
        x = tf.keras.layers.DenseFeatures(feature_columns)(inputs)
```


### Relevant log output

_No response_",False,"[-6.29407525e-01 -1.00727662e-01 -7.08077848e-02  1.79405600e-01
  3.11900645e-01 -4.35070753e-01 -1.69783026e-01 -6.36117160e-02
 -4.44333881e-01 -2.07600847e-01  5.32012135e-02 -1.27959251e-01
 -7.66495317e-02  1.25983171e-02 -1.81615591e-01  2.77520537e-01
 -2.13658258e-01 -2.66805347e-02  1.71950281e-01  2.06700981e-01
 -1.46371096e-01 -4.50205430e-02 -4.28280830e-01  2.35426247e-01
  1.14389658e-01  2.31646091e-01 -5.62675953e-01 -1.00158289e-01
 -1.69891771e-02  1.01632610e-01  6.05901361e-01  1.78988069e-01
  8.58569443e-02  2.36235827e-01 -1.71708204e-02  1.72214299e-01
 -1.80065274e-01 -2.31712893e-01 -1.73220024e-01  3.03505734e-02
 -1.67027578e-01  5.67173101e-02  2.15045825e-01 -1.24936372e-01
  1.20453306e-01 -2.32443437e-01 -1.70326531e-01 -9.25635844e-02
  2.36843936e-02 -1.50186613e-01 -2.91515917e-01  9.67405736e-02
 -5.68662286e-01 -2.87819058e-01 -1.42195910e-01 -4.00735252e-02
  9.33793336e-02 -2.14494094e-01 -2.11293444e-01  1.65878266e-01
  2.86345482e-01 -9.02996212e-02  1.02275982e-03  6.14362676e-03
  7.96828941e-02  1.84693992e-01  3.03847849e-01  1.73991337e-01
  4.71314728e-01 -9.54085737e-02  1.39493689e-01  7.25404099e-02
 -4.71814692e-01  8.42711609e-03  1.02407604e-01  1.59514636e-01
 -2.44279981e-01 -7.53406808e-03  3.39250803e-01 -2.06705898e-01
 -3.57083231e-02 -2.04207122e-01 -2.73429632e-01 -3.07017744e-01
  9.81467068e-02 -8.16402137e-02  4.14477587e-01  4.34087999e-02
  3.50519985e-01 -2.89260864e-01  5.19237638e-01  4.07845140e-01
 -7.39976838e-02  3.12292278e-01  4.91809309e-01  2.52301037e-01
 -5.43442667e-02 -2.95705944e-02  5.36483154e-02 -1.18764088e-01
 -2.53058225e-01 -1.91489324e-01 -6.04522526e-02  4.42372523e-02
 -1.36620209e-01 -3.15694749e-01  5.12194075e-02  1.25085130e-01
  6.22998625e-02  1.10600173e-01  3.11257005e-01  2.18151167e-01
  5.15089035e-02 -1.62088931e-01 -1.53713137e-01  4.99396771e-02
  1.90118968e-01 -8.86789635e-02  6.70072287e-02  5.86598933e-01
  5.70501685e-02 -4.89771217e-02 -6.26127496e-02  9.64873135e-02
  6.08687282e-01  6.46021143e-02 -1.57630295e-01  1.81526858e-02
  9.42892209e-02 -3.12219292e-01  1.89050078e-01 -1.82218105e-02
  1.83612764e-01  2.24246591e-01 -1.93619132e-01 -8.37121978e-02
 -1.35503381e-01 -4.92132120e-02 -1.80480927e-01 -8.80018771e-02
 -2.91442156e-01  3.52240026e-01 -2.15969726e-01 -7.81518102e-01
  2.05157340e-01  3.08512300e-01 -3.92322004e-01  6.39448985e-02
 -4.38355207e-01  2.37775952e-01  1.27808638e-02  5.95766231e-02
 -8.06458220e-02  3.17976385e-01  7.49714114e-03  1.54960692e-01
  2.92144597e-01  1.76195130e-02  2.03283057e-01 -4.43101466e-01
 -1.00124002e-01  5.36043525e-01 -9.57825780e-02 -1.11194380e-01
  6.32316172e-02  8.90331790e-02 -3.74760628e-01 -2.93224037e-01
  1.45792276e-01  3.57618302e-01 -2.49697417e-01 -1.21132746e-01
 -7.83780776e-03  2.46765818e-02  1.78296089e-01 -5.74735738e-02
  4.80775610e-02 -7.75712848e-01  5.16034886e-02  3.38885278e-01
  1.85923547e-01  2.28816271e-01  2.40519360e-01 -4.08646688e-02
  4.18560132e-02  3.83749232e-02  1.72917813e-01  2.13642582e-01
 -1.32358745e-01 -1.09301753e-01 -4.86488670e-01 -4.70216349e-02
  5.62926829e-01 -1.34358570e-01 -7.39596561e-02  1.67183369e-01
  2.19462037e-01 -1.16474099e-01  2.21221432e-01  1.55841202e-01
 -1.71730608e-01 -2.27728933e-01  1.31071508e-02  5.30458242e-03
  7.51893818e-02 -2.13948801e-01 -2.32072249e-01 -4.17792946e-01
 -1.34912461e-01  1.36573985e-01 -3.57363857e-02 -6.53541327e-01
 -1.31838918e-01  1.36690766e-01 -3.78659219e-01  5.65828681e-01
  9.05198902e-02  2.48508230e-02 -3.37979823e-01  1.73359618e-01
  3.62039298e-01 -3.23287159e-01  9.78699550e-02 -4.95094895e-01
 -2.47630104e-01  2.79307216e-01 -2.07750440e-01  2.76084363e-01
 -2.62262002e-02  1.40284359e-01  9.22806561e-02 -2.07942314e-02
  4.37174797e-01  2.18314961e-01  3.64293575e-01 -2.41067968e-02
 -4.97667678e-02 -2.23963112e-01  2.99315192e-02  1.27643287e-01
 -4.04123694e-01 -4.65857208e-01 -3.72973718e-02 -8.82232562e-04
  2.84426183e-01  3.16444099e-01 -2.28094548e-01  6.51981235e-02
 -4.79075760e-01  1.36374593e-01 -2.97433674e-01  2.45628133e-02
  4.30001080e-01 -9.94853862e-03  3.68260622e-01  5.82823902e-02
  1.81488648e-01  1.18737072e-01  2.90300488e-01 -4.25366968e-01
  3.34678292e-01  8.56676698e-02  1.98935449e-01  5.89518428e-01
  2.01928720e-01  1.70543164e-01 -2.56297171e-01  5.77080965e-01
 -2.93767154e-02 -3.00304651e-01  4.96041402e-02 -4.42745626e-01
  4.55593318e-01 -6.28379226e-01  9.81646031e-02  7.98457861e-02
  3.00920218e-01  1.16815008e-02  2.27509737e-02  2.79007971e-01
 -7.54848402e-03  4.01621580e-01 -2.71047950e-01  1.71161771e-01
 -1.28087640e-01 -1.20668128e-01 -1.65363878e-01 -5.56626558e-01
 -5.55789508e-02  1.30235970e-01 -2.26949230e-01  1.63546056e-01
 -6.74181432e-02  6.01007193e-02 -2.93264836e-02  6.51929080e-02
  6.11173660e-02 -2.31425554e-01  1.31923825e-01  8.72310251e-02
 -1.87082052e-01  1.98144495e-01  2.99587607e-01 -2.18356758e-01
 -3.74609888e-01 -4.39058058e-04  5.45988798e-01  2.33324811e-01
  3.59727561e-01 -3.00804377e-01  2.27495655e-01 -1.93861306e-01
 -7.75497034e-02  6.46813869e-01 -1.18733048e-01 -1.36152506e-01
 -2.58753538e-01  7.05365062e-01  1.79040626e-01 -4.95908260e-02
  2.57706940e-01 -2.89107651e-01 -1.92875594e-01  1.77933335e-01
  3.10621321e-01 -1.82287812e-01 -1.32853854e-02 -2.25646958e-01
  5.30534238e-02  1.92136735e-01  6.98979050e-02 -8.36800635e-02
 -1.16630092e-01 -6.13709539e-02 -7.03705996e-02 -2.69828856e-01
 -4.80285317e-01  1.02268010e-01 -1.11091575e-02 -1.53774261e-01
 -2.78896540e-01 -1.44260019e-01  8.79793614e-03 -7.28218555e-02
  7.72984624e-02 -2.12836236e-01  3.17872763e-01  4.87798244e-01
 -1.84235722e-01  3.50731522e-01  8.53212401e-02  1.29567817e-01
 -4.85022604e-01 -1.12957433e-01 -4.17786092e-02  3.45721364e-01
  5.56549281e-02 -1.42504424e-01  1.95069954e-01  1.85604736e-01
 -8.49869475e-02  1.80561036e-01 -1.71571583e-01  1.15841635e-01
  4.72307280e-02 -3.26532960e-01 -1.34371787e-01 -3.54950756e-01
  2.16821402e-01  2.71724463e-01 -3.49579975e-02  2.17145368e-01
 -3.44372213e-01  4.91332710e-01  2.91632742e-01 -4.50520903e-01
 -1.77828580e-01  8.50826129e-02  2.33841658e-01 -2.18270496e-02
  5.48417494e-02 -2.58955240e-01  1.00542665e-01  1.24511316e-01]"
implementation 'org.tensorflow:tensorflow-lite-support:0.1.0' stat:awaiting response type:support stale comp:lite,"implementation 'org.tensorflow:tensorflow-lite-support:0.1.0'
Why do I run this instruction to generate two dependent libraries, TensorFlow Lite and TensorFlow Lite Support?

How can I generate only tensorflow site support as a dependency library?",False,"[-3.21938783e-01 -5.49727142e-01 -1.37936532e-01  1.52186677e-01
  4.15148847e-02  7.77947530e-02 -2.42122725e-01 -1.28766209e-01
 -1.56533986e-01 -1.52919814e-01 -9.59843770e-02 -5.99853732e-02
 -5.15257716e-02  4.09940183e-01 -5.40999547e-02  3.15246582e-01
 -2.80897677e-01 -1.16144791e-01  2.19289847e-02 -1.22358508e-01
 -2.32389756e-02 -1.44773528e-01 -1.81507215e-01 -4.97767515e-02
  1.27941400e-01 -6.68829754e-02 -1.93232879e-01 -1.00339077e-01
  1.35824829e-01  1.91132590e-01  6.31470859e-01  6.68141320e-02
 -2.34362379e-01  1.09086365e-01 -1.75380781e-01  1.54970601e-01
 -1.44486338e-01 -3.21559049e-02 -2.04489484e-01  7.92778283e-02
  7.65374079e-02  1.84735075e-01  7.14393184e-02 -1.87539443e-01
  1.23403475e-01  6.57609403e-02  1.99378282e-01 -1.72784983e-03
 -6.40168600e-03 -7.88857415e-02  1.12389505e-03 -6.82085231e-02
 -2.79976010e-01 -3.12533021e-01  1.20657131e-01  4.56016250e-02
  1.25347540e-01 -3.09372507e-02 -2.23452017e-01  3.32085341e-02
  1.16870597e-01 -5.47941169e-03  2.01164261e-01  3.57475653e-02
 -3.60395908e-01  4.23010617e-01  3.23136300e-01 -2.54237533e-01
  4.66158599e-01 -2.10704803e-01 -6.94918409e-02 -4.88649383e-02
 -8.51028189e-02  1.59600880e-02 -6.59877732e-02  2.26465955e-01
 -2.31167167e-01  2.60222226e-01  2.84915000e-01 -5.97108379e-02
 -4.97861207e-02 -1.24679506e-01  9.90627855e-02 -1.88490033e-01
  3.29800472e-02 -8.16903859e-02  9.49082449e-02 -6.08368590e-02
  2.58011371e-01  8.07344094e-02  2.75484294e-01  1.24138348e-01
 -5.07368743e-02  7.63901621e-02  2.30043486e-01 -1.29303381e-01
 -1.00898400e-01  2.69331008e-01 -2.05553740e-01 -2.22185850e-01
 -1.92927256e-01 -5.53199574e-02 -4.90196757e-02  2.84568131e-01
  2.43620932e-01 -1.57808468e-01  2.73623168e-01 -7.67632425e-02
 -6.64830729e-02 -5.90851344e-02  2.85243213e-01 -1.13840364e-01
  4.91560578e-01  9.97107700e-02 -3.00327465e-02  5.11178151e-02
  2.46000271e-02  1.63764045e-01  1.81733035e-02  8.04710567e-01
 -1.05713092e-01 -1.24673292e-01  1.79064080e-01  1.04074612e-01
  2.31571764e-01 -2.16848448e-01 -2.59608299e-01 -1.19749218e-01
 -2.89372593e-01  6.16960265e-02  8.29483569e-02  2.12617710e-01
 -7.41627663e-02  2.62545771e-03  1.13419510e-01  1.14355899e-01
 -4.89735305e-01 -5.13115644e-01 -3.51253748e-02 -4.40963283e-02
 -1.37454927e-01  3.23351502e-01 -1.38812944e-01 -2.00336054e-01
  2.08099082e-01  3.83374393e-01  1.17195971e-01  1.45113185e-01
 -3.07066645e-02 -2.59350717e-01 -3.18463109e-02 -1.15921363e-01
 -1.76657826e-01  6.75314367e-02  2.65313774e-01  1.61263481e-01
  3.26247871e-01 -1.54735282e-01 -8.61005411e-02 -2.80898899e-01
  6.25623018e-02  3.02384466e-01 -2.83274710e-01 -2.05640808e-01
  4.83364090e-02  4.70407959e-03 -2.58106768e-01 -1.72261462e-01
 -7.28819370e-02  2.67075837e-01  2.32401505e-01 -1.58220619e-01
  1.32335881e-02  2.86108833e-02  3.46493751e-01 -2.54175402e-02
  3.59235913e-01 -7.62312770e-01 -6.03806712e-02  5.37248962e-02
  1.19333472e-02  2.26323187e-01  5.98736107e-02 -9.32961926e-02
 -2.27333784e-01  9.53947455e-02  1.99997947e-01  2.64031943e-02
 -1.05900072e-01  1.82515591e-01 -5.23233056e-01 -6.04178943e-02
  1.11813627e-01 -1.74043208e-01 -1.80638015e-01 -2.81531036e-01
  3.61062557e-01  2.87755668e-01 -9.87675623e-04  6.84095025e-02
 -1.21853046e-01  3.07426807e-02 -9.84255001e-02  1.02712229e-01
  1.39697120e-01 -3.54873061e-01 -7.08334669e-02 -3.57573526e-03
 -2.62769878e-01 -1.25119641e-01  1.21319994e-01 -1.29384115e-01
  6.90156221e-02 -1.05772786e-01 -4.10453647e-01  8.49593878e-02
  3.40911262e-02 -7.31527805e-03 -2.99622118e-01  3.06294918e-01
  8.57984051e-02 -7.68609047e-02 -1.21960111e-01 -1.85469463e-01
 -1.90835148e-01 -1.89047188e-01 -1.53027713e-01  1.69631556e-01
  8.39315206e-02  4.29935932e-01 -2.08575241e-02 -2.89941788e-01
  4.46586162e-01  2.39929155e-01  2.38759190e-01 -2.83168048e-01
  4.84055243e-02 -2.11432632e-02 -2.98258930e-01 -2.64817625e-01
 -2.57472008e-01 -2.35723164e-02  1.57679707e-01 -4.50886376e-02
 -8.40321630e-02  1.46177426e-01 -3.01696826e-02 -1.68314710e-01
 -2.85493266e-02  3.20346236e-01 -1.69260189e-01 -1.26269549e-01
  4.60371673e-01  1.92155346e-01  4.72491562e-01  2.82062411e-01
 -8.77617598e-02 -3.84802312e-01  1.25489444e-01  2.95483232e-01
  3.06019783e-01  3.39067429e-01  7.56356865e-02  5.52428544e-01
  2.01627165e-01  2.53988355e-01 -2.94711709e-01 -2.78870482e-02
  4.65050861e-02  1.85006350e-01 -1.57687426e-01  1.22864649e-01
  1.31619394e-01 -3.89174849e-01 -4.80804853e-02  1.66851893e-01
  2.44633406e-01 -8.56980588e-03 -7.44776204e-02  7.33913202e-03
  4.85800877e-02  4.63771254e-01 -3.88094753e-01 -2.81948566e-01
 -4.77274284e-02 -4.05978829e-01  5.80081344e-02 -2.98617572e-01
 -1.49334997e-01  2.12038830e-01 -2.52671987e-01  8.46176520e-02
  3.00744139e-02  2.80672796e-02 -4.72361952e-01 -1.63290173e-01
  1.18586622e-01 -1.06572665e-01 -2.02321750e-03  3.17568302e-01
 -4.32077870e-02 -2.34264843e-02  3.65053505e-01 -2.36176670e-01
  2.59379029e-01 -3.69433425e-02  3.34639549e-01 -3.40947211e-02
  4.28730249e-01 -4.59943980e-01  3.81747872e-01  2.52891839e-01
  1.86042905e-01  4.85207707e-01  6.55132085e-02 -3.92914526e-02
 -2.68824279e-01  1.14098489e-01  1.28912657e-01 -2.95201927e-01
 -7.18222931e-02 -6.85856864e-02 -1.42384902e-01  7.36581385e-02
  2.73151614e-04 -2.50618812e-02 -6.85830116e-02 -2.30987445e-01
 -3.01774502e-01 -2.53262613e-02 -1.29977956e-01 -1.04243390e-01
  1.13622695e-01  2.78514743e-01 -1.04411636e-02  7.62990713e-02
 -3.70388031e-01  1.98344439e-01 -3.36497426e-02 -2.89610714e-01
  1.76146656e-01 -6.32204637e-02 -1.01375245e-01 -3.30650330e-01
 -8.72647613e-02 -1.69528723e-01  3.28571469e-01  2.05889925e-01
 -3.75454634e-01  1.65864110e-01 -1.64605185e-01  2.82086432e-01
 -3.01608890e-01 -1.64898127e-01  2.81531274e-01  1.63669378e-01
 -9.47257504e-02 -6.72105253e-02  4.83764149e-02  5.05368114e-01
 -2.49417812e-01 -1.16392700e-02 -1.73181295e-01 -3.14924330e-01
  2.39906698e-01 -3.79953510e-03  7.11620599e-03 -1.40852883e-01
  1.37717515e-01  7.63203382e-01  9.29273199e-03  2.17463896e-01
 -1.03143193e-01 -4.60176095e-02  3.12750429e-01 -2.84820795e-01
 -2.61801004e-01 -3.85484248e-02  4.64056395e-02 -5.70279844e-02
 -2.33198911e-01  2.25745142e-01  3.93985897e-01 -4.32496935e-01]"
"How to measure data fetching, forward and backward pass time during training stat:awaiting response type:support stale TF 2.11","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:   No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: Binary
-   **TensorFlow version (use command below)**: 2.11.0
-   **Python version**: 3.8
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 12
-   **GPU model and memory**: A100
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
How to measure Data fetching, Data preparation, forward and backward pass time for training [script](https://www.tensorflow.org/tutorials/images/segmentation)?



### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",False,"[-0.41552058 -0.5015845  -0.29924268  0.0287353   0.18558998 -0.10817067
 -0.25059575  0.06059674 -0.24869767 -0.10712556 -0.06033158  0.18563637
 -0.00787712  0.08347956 -0.18460846 -0.0208772   0.04482825 -0.07159095
 -0.00610083 -0.06138897 -0.01504914 -0.0882441   0.11377697  0.2086282
  0.04537837  0.01894124 -0.15919064 -0.04983416 -0.0231697   0.00664413
 -0.10288993  0.06607213 -0.17548722 -0.12788339 -0.07526691  0.1435516
  0.0823423  -0.22993141 -0.2025549   0.04135461  0.02903008 -0.19964796
  0.05080345  0.31824258  0.07563356  0.23576131 -0.05891812 -0.01995017
 -0.13659355  0.03960135 -0.17156288 -0.18066698 -0.17740886 -0.21957748
 -0.03926613  0.05271436  0.13028179  0.10595655 -0.02446292  0.1569808
 -0.16435593 -0.12071642  0.11900543  0.08572742  0.04099561  0.27869365
 -0.05890706 -0.17577985  0.3817911  -0.01143762  0.00195288 -0.03214416
 -0.40729493  0.07705492 -0.08892678 -0.0210772  -0.20744415  0.3143034
  0.39387673 -0.02884972  0.03904099 -0.08615497  0.15347129  0.05417594
  0.22661445 -0.09794467  0.19853377  0.21801578  0.25139868  0.06655115
  0.40639496  0.1398685  -0.21457496  0.03149854  0.31806213  0.29837558
 -0.01178095  0.33887935 -0.17269327 -0.08679541 -0.12121494 -0.27371687
 -0.11784142  0.14707889  0.03149086 -0.02206341  0.10166773 -0.01149244
  0.20768066 -0.13069473  0.2627172  -0.05990733  0.03873832  0.18948667
 -0.01238929  0.17056303  0.10210371  0.23816656  0.04689755  0.31771237
 -0.14620416 -0.23028706  0.13682243 -0.06561609  0.27774173 -0.07870845
 -0.15283972  0.2612032  -0.0899673   0.00813691  0.15183523  0.06948772
 -0.34982696 -0.2535013  -0.19954017 -0.2587382  -0.10626368 -0.00905613
 -0.26526526  0.0487565   0.06345701  0.15562345 -0.0436873  -0.19930354
  0.07050651  0.1892992  -0.1578404   0.10658941  0.12907279 -0.13139725
 -0.00541735 -0.06748036 -0.00991536  0.37569404  0.02605823  0.08393653
 -0.06115435 -0.04022214  0.02557191 -0.49110606 -0.11895364  0.0883315
 -0.12216856 -0.03304344  0.35373497  0.14013784 -0.17084295 -0.0699316
  0.08467792  0.10137069  0.12414277 -0.06068946 -0.06357621 -0.10519112
  0.5771966  -0.04320361  0.26976627 -0.33993208 -0.18256727 -0.00557477
  0.29736498 -0.13469584  0.05893883  0.15607433  0.04596826  0.10205354
 -0.20475538  0.15333924 -0.08441506 -0.00859108 -0.23889299 -0.07679254
  0.03424447  0.08772737 -0.30298752 -0.15736502  0.15468462 -0.13641785
  0.09890379  0.03462803 -0.05275818  0.0583408  -0.14193074 -0.18485159
  0.0183459  -0.07099784 -0.11512428 -0.3773684   0.07268202 -0.1318002
  0.06385049 -0.2058897  -0.10879721  0.06719445 -0.15342972 -0.1604126
 -0.08782183  0.15574983 -0.33753115  0.11402951 -0.01121654 -0.03804826
  0.17869641 -0.2958179  -0.3237286   0.15723495  0.04088978  0.24787472
  0.10365807  0.1722278   0.3168057  -0.021471    0.4237126   0.17007741
 -0.01479918 -0.13171993 -0.28168735 -0.06877244 -0.07792967 -0.04399429
 -0.32936528 -0.05072754 -0.13688137 -0.1421741  -0.01850612  0.21375397
 -0.15337235  0.00843595 -0.05936189  0.37458086 -0.0980451  -0.24742219
  0.15493837  0.04346438  0.1713655  -0.04595322  0.04199676  0.19299522
  0.16796857 -0.03272557  0.31146938  0.2424248   0.20834339  0.6363881
  0.5010322   0.30596375 -0.26847863  0.16617185 -0.41525042  0.00825736
 -0.14524302 -0.09272562  0.1632497  -0.05518845 -0.06198573 -0.18441504
  0.41797417  0.01588963 -0.1520336   0.13206148  0.12021482  0.1402115
 -0.1915979   0.20496592 -0.03899027 -0.01118974 -0.03224527 -0.4107656
  0.18582691 -0.00549763 -0.1186192   0.0540866   0.13739881  0.2096807
  0.17631367 -0.02742048  0.02942243 -0.2176244   0.23718837  0.29562908
 -0.17794949  0.23678094  0.2112651  -0.14148784 -0.0724     -0.03067964
  0.18494172  0.01976447  0.28204885  0.02782699  0.23042682  0.246873
  0.08658012  0.29890543  0.0977431   0.12804762 -0.21940021  0.24041098
 -0.16820034 -0.1110566  -0.00523811 -0.15100029 -0.23249045  0.10842523
  0.15295307  0.17982362 -0.11269889 -0.15181823 -0.21849483  0.13101117
 -0.0707472  -0.3562411   0.03149671 -0.07777892  0.01295087 -0.2626179
 -0.145342    0.4449475  -0.13107507 -0.30912343 -0.15246136 -0.15973368
 -0.16992807 -0.22216421  0.03355629 -0.12454104  0.18121906  0.7051604
  0.04155708  0.08183767 -0.00104714  0.10213398 -0.28806308 -0.06317224
 -0.13313484  0.29508728 -0.02035637 -0.07276683 -0.00978611  0.36325026
 -0.28032753  0.07395929 -0.19461286 -0.04215565  0.07304589  0.00302668
 -0.22074936 -0.19524617  0.25511318  0.20534128 -0.08999492  0.06734975
 -0.32672375  0.09022031  0.32423675 -0.15320335  0.07791832 -0.11042523
  0.02339524  0.07397692 -0.19237565 -0.13326028 -0.2309533  -0.15534626]"
Extra semicolon in tensorflow/lite/micro/micro_profiler.h:90 type:docs-bug comp:lite comp:micro,"Remove extra semicolon in tensorflow/lite/micro/micro_profiler.h on line 90

Before:
```
TF_LITE_REMOVE_VIRTUAL_DELETE;
```

After:
```
TF_LITE_REMOVE_VIRTUAL_DELETE
```",False,"[-3.28695476e-01 -8.42311457e-02  1.41241610e-01 -4.58537117e-02
  3.02506179e-01 -4.03167963e-01  2.30194360e-01  2.57996991e-02
 -2.45858029e-01  1.53294608e-01  1.04984619e-01  2.39749402e-01
 -3.66755694e-01 -1.53605089e-01  1.56345233e-01  1.62509128e-01
 -7.23267794e-01  3.27391960e-02 -1.79323345e-01 -3.44576947e-02
  2.46990159e-01  9.47665423e-02 -4.19745088e-01  4.05017525e-01
  1.32762626e-01 -6.54847100e-02 -3.28355223e-01 -3.02427113e-01
  3.08561907e-03  2.22051859e-01  4.55458701e-01  3.27682972e-01
 -2.72941470e-01  2.72185743e-01  4.19532925e-01 -1.20225608e-01
 -4.04413491e-01  4.18747544e-01  3.33574936e-02 -4.06964391e-01
  2.02421725e-01 -2.97562331e-01 -7.82803893e-02 -4.35724929e-02
  2.39929006e-01 -5.61132371e-01 -6.65508926e-01 -1.05789127e-02
  2.55412072e-01 -1.64556965e-01 -5.03203459e-02  8.54090154e-02
 -6.45741582e-01  2.60958552e-01 -1.53102070e-01 -1.38315007e-01
  2.38936186e-01  1.41358539e-01  4.50023979e-01  2.75975943e-01
  1.90194070e-01  2.24368155e-01  1.49906324e-02  1.11687474e-01
 -6.33716658e-02  9.52955633e-02  3.83600853e-02  7.35444902e-03
  5.07279873e-01  2.64475564e-03  2.30454281e-01 -1.78459167e-01
 -3.67945582e-01 -9.13375765e-02 -8.22434202e-02 -7.27720335e-02
 -4.35684502e-01  9.63387936e-02 -3.93075719e-02 -1.24299601e-02
 -1.20731220e-01  1.62072375e-01  4.11010295e-01 -2.55192250e-01
  1.81741863e-01  3.77976060e-01 -5.21471724e-02 -2.53092591e-02
  3.75321805e-01 -3.32439959e-01 -1.43494233e-01 -1.78207666e-01
  2.09529206e-01  3.59468043e-01  9.36665572e-03 -5.42178273e-01
  1.96613163e-01 -1.49104476e-01 -2.15904191e-01  7.73789827e-03
 -2.07296491e-01 -1.55181497e-01  5.73447272e-02  2.03450888e-01
 -3.19158971e-01  3.42445999e-01  3.68100196e-01  2.75586903e-01
  9.86494347e-02 -2.51858890e-01  4.10877973e-01  7.25541338e-02
  2.01803893e-01 -3.40000242e-02  1.94518361e-03  2.51842320e-01
  1.81472413e-02  1.75046250e-01  1.95115000e-01  4.88749832e-01
 -3.86660844e-01 -2.49875337e-01  1.49268461e-02  2.26492971e-01
  1.57899469e-01  1.36711791e-01  1.24081425e-01 -3.76277114e-03
 -9.63142216e-02 -7.62566254e-02  3.86371836e-02  2.05981344e-01
  8.92977640e-02  3.66610646e-01  6.51226640e-02 -3.32755372e-02
 -3.00445229e-01 -4.06588882e-01  1.69893265e-01  9.86607671e-02
 -2.88862139e-01  8.30738023e-02 -2.07103416e-01 -3.48899543e-01
  7.55157173e-01  8.01035911e-02 -3.42589021e-01 -5.58676541e-01
 -2.73803771e-01  3.22471708e-01 -2.35494807e-01  3.84659708e-01
  4.27711047e-02  2.98499435e-01 -6.00354522e-02  3.91506433e-01
  2.86130726e-01 -8.21877122e-02 -3.47633630e-01 -5.00750124e-01
  4.82349366e-01  5.06027341e-01  1.28737867e-01  7.25467876e-02
  4.90409464e-01 -1.71346784e-01 -4.95988220e-01 -9.89185050e-02
 -2.59885877e-01  1.62476808e-01  1.70069605e-01 -2.74349809e-01
 -5.10736346e-01 -1.71995208e-01 -1.16359092e-01 -1.34830803e-01
  3.59375477e-01 -5.47836959e-01 -1.76085174e-01  3.12956154e-01
  7.40041807e-02  2.31394827e-01  1.73193544e-01 -1.16147004e-01
  9.60306451e-02  2.58736879e-01  2.98284113e-01 -6.48079291e-02
 -4.82586473e-02  1.27746269e-01 -2.19378710e-01  2.26545036e-01
  4.33098018e-01  1.93339884e-02 -2.76371419e-01  3.53010327e-01
  5.98163381e-02  1.29354686e-01 -1.18685968e-01  1.71853870e-01
 -1.38226151e-01 -2.66709596e-01  4.19801623e-01  1.35476768e-01
  1.73543036e-01 -4.33496803e-01 -4.44090813e-01 -2.40356639e-01
  2.28167395e-03 -6.52738195e-03  6.29970580e-02 -1.54552028e-01
 -4.30175304e-01 -1.52861327e-01 -3.85359824e-01  2.50621468e-01
  5.79601452e-02 -4.52911824e-01 -1.18848950e-01  2.11194754e-01
  5.84022522e-01  3.10313553e-02 -3.86054754e-01 -1.72137558e-01
 -3.91118318e-01  4.75262180e-02  4.09830324e-02 -4.44741803e-04
 -9.27600786e-02  3.00226361e-01 -1.17610835e-01  2.18133479e-01
  3.39536071e-01 -1.36727929e-01  4.81388181e-01  1.16521053e-01
 -7.32964650e-02 -6.48309290e-01 -2.53500104e-01  4.80772704e-01
 -4.01710749e-01 -2.29166627e-01 -4.28005755e-01  3.82500380e-01
  4.27448660e-01 -1.37147069e-01 -1.14617407e-01  1.15062505e-01
 -2.49426380e-01  2.22974405e-01 -1.49421602e-01  2.71644324e-01
  1.31032631e-01  6.80443645e-02  4.21324939e-01  1.20441921e-01
  9.64356810e-02  2.87089735e-01  1.76578671e-01 -1.72724515e-01
  2.30672762e-01  3.34300101e-01 -5.25526069e-02  4.27499354e-01
 -1.86494961e-01 -3.56873237e-02 -4.28806454e-01  3.05165380e-01
 -2.08263025e-01  2.69914102e-02  1.68592613e-02 -2.57905483e-01
  2.14977711e-01 -3.91436398e-01  1.61962390e-01 -4.88396436e-02
  3.36217612e-01  6.09867014e-02 -7.89940953e-02 -1.61406279e-01
  9.85672921e-02  6.91099226e-01 -4.22841102e-01  3.86019319e-01
  3.94862086e-01 -4.03201394e-03 -2.83592701e-01 -3.44391167e-01
 -1.08735055e-01 -1.71306089e-01 -2.36797333e-01  1.69682801e-01
 -1.42305166e-01 -1.98735520e-01  1.82886362e-01  6.71152174e-01
 -3.43968302e-01 -1.41492158e-01 -2.38040119e-01 -2.52966017e-01
 -3.89209925e-03 -4.93785471e-01 -1.35536641e-01 -4.92275283e-02
  2.47897461e-01 -3.57520819e-01  4.58430164e-02  2.23523024e-02
  1.37331365e-02 -1.96828201e-01 -1.15403652e-01 -6.14403374e-02
 -4.21459638e-02  2.99921870e-01  1.36760604e-02 -2.13543952e-01
  1.52568877e-01  5.60336292e-01  1.37869373e-01 -3.37119773e-02
  1.67326704e-01 -4.27879512e-01 -1.65287092e-01 -1.67463303e-01
  2.62300819e-01 -8.03907886e-02 -2.87847906e-01  1.02102302e-01
 -2.33981863e-01 -3.11391711e-01  9.02338028e-02 -9.68527123e-02
 -1.40683025e-01  1.43452063e-02 -2.26731841e-02 -1.75518900e-01
 -6.12728074e-02  7.00353906e-02  2.50291854e-01 -2.26411402e-01
 -5.27692912e-03 -1.61043540e-01  2.31812838e-02 -2.85791695e-01
 -7.41543993e-02 -3.43155503e-01  1.97619960e-01  5.04266858e-01
  5.11758551e-02  1.19492114e-01 -1.20962188e-01  5.10138512e-01
 -2.24170849e-01  2.14110285e-01  1.66970536e-01  7.05450416e-01
  8.32357164e-03  2.84629583e-01 -1.34590089e-01  5.73974371e-01
 -2.92015225e-01 -1.23419702e-01 -5.91386437e-01 -4.51398604e-02
 -2.19289482e-01 -4.94832456e-01  5.61285764e-02 -1.00733163e-02
 -3.41060795e-02  3.00590158e-01  2.54786253e-01  2.55516797e-01
 -3.48144919e-01  3.98142375e-02 -2.14992911e-01 -9.68465433e-02
 -1.13951899e-01  2.20644817e-01 -3.57052147e-01 -2.08489329e-01
  2.33892426e-01 -3.93478433e-03  3.87434871e-03  1.00210384e-01]"
Building benchmark_model using Buildroot failed type:build/install comp:lite wsl2 TF 2.11,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

WSL2/Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

gcc compiler for aarch64

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello,

I am trying to build the benchmark_model from tensorflow-lite using Buildroot for my i.MX 8 platform. The tensorflow-lite and label_image examples are built successfully. But now when I am building the benchmark_model I am getting below error from `benchmark_tflite_model.cc`

``undefined reference to `absl::lts_20220623``

Buildroot is using `cmake` and build command is
`/usr/bin/cmake --build /.../build/tensorflow-lite-2.11.0/tensorflow/lite/buildroot-build -t benchmark_model`

Same build command was used for label_image and there were no errors related to linking.

### Standalone code to reproduce the issue

```shell
`/usr/bin/cmake --build /.../build/tensorflow-lite-2.11.0/tensorflow/lite/buildroot-build -t benchmark_model`
```


### Relevant log output

```shell
[ 98%] Building CXX object tools/benchmark/CMakeFiles/benchmark_model.dir/__/delegates/external_delegate_provider.cc.o
[100%] Linking CXX executable benchmark_model
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: CMakeFiles/benchmark_model.dir/benchmark_tflite_model.cc.o: in function `absl::lts_20220623::strings_internal::Splitter<absl::lts_20220623::ByChar, absl::lts_20220623::AllowEmpty, std::basic_string_view<char, std::char_traits<char> > >::ConvertToContainer<std::vector<std::basic_string_view<char, std::char_traits<char> >, std::allocator<std::basic_string_view<char, std::char_traits<char> > > >, std::basic_string_view<char, std::char_traits<char> >, false>::operator()(absl::lts_20220623::strings_internal::Splitter<absl::lts_20220623::ByChar, absl::lts_20220623::AllowEmpty, std::basic_string_view<char, std::char_traits<char> > > const&) const [clone .isra.0]':
benchmark_tflite_model.cc:(.text+0x204): undefined reference to `absl::lts_20220623::ByChar::Find(std::basic_string_view<char, std::char_traits<char> >, unsigned long) const'
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: benchmark_tflite_model.cc:(.text+0x284): undefined reference to `absl::lts_20220623::ByChar::Find(std::basic_string_view<char, std::char_traits<char> >, unsigned long) const'
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: CMakeFiles/benchmark_model.dir/benchmark_tflite_model.cc.o: in function `tflite::benchmark::SplitInputLayerNameAndValueFile(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >&)':
benchmark_tflite_model.cc:(.text+0xcf0): undefined reference to `absl::lts_20220623::StrReplaceAll[abi:cxx11](std::basic_string_view<char, std::char_traits<char> >, std::initializer_list<std::pair<std::basic_string_view<char, std::char_traits<char> >, std::basic_string_view<char, std::char_traits<char> > > >)'
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: benchmark_tflite_model.cc:(.text+0xdd4): undefined reference to `absl::lts_20220623::StrReplaceAll[abi:cxx11](std::basic_string_view<char, std::char_traits<char> >, std::initializer_list<std::pair<std::basic_string_view<char, std::char_traits<char> >, std::basic_string_view<char, std::char_traits<char> > > >)'
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: CMakeFiles/benchmark_model.dir/benchmark_tflite_model.cc.o: in function `tflite::benchmark::BenchmarkTfLiteModel::ValidateParams()':
benchmark_tflite_model.cc:(.text+0x8388): undefined reference to `absl::lts_20220623::numbers_internal::safe_strto32_base(std::basic_string_view<char, std::char_traits<char> >, int*, int)'
...per-package/tensorflow-lite/host/opt/ext-toolchain/bin/../lib/gcc/aarch64-buildroot-linux-gnu/10.3.0/../../../../aarch64-buildroot-linux-gnu/bin/ld: benchmark_tflite_model.cc:(.text+0x83a4): undefined reference to `absl::lts_20220623::numbers_internal::safe_strto32_base(std::basic_string_view<char, std::char_traits<char> >, int*, int)'
collect2: error: ld returned 1 exit status
```
",False,"[-4.77184236e-01 -3.50208193e-01 -1.32959664e-01  7.66348690e-02
  2.66983688e-01 -2.78907895e-01 -4.65986103e-01  1.19529895e-01
 -5.37741244e-01 -2.27406129e-01 -9.35588181e-02 -3.17251384e-02
 -7.34666139e-02  1.79575216e-02 -1.65123604e-02  3.72442901e-01
 -1.22496814e-01 -2.80833006e-01  3.73595238e-01  2.43328780e-01
 -3.22304159e-01 -2.09834158e-01 -2.21620589e-01  9.64138061e-02
  1.80642515e-01  8.04766268e-02 -2.91829288e-01  2.80533522e-01
  2.86692008e-02  2.96515822e-01  5.28054118e-01 -5.32555245e-02
 -2.13205874e-01  2.64963746e-01  3.05874228e-01  3.60147357e-01
 -1.44311100e-01 -3.22626114e-01 -3.60984713e-01  7.32804611e-02
 -7.30480701e-02 -1.71559639e-02  5.32123446e-02 -2.00254083e-01
  2.02172343e-02 -6.68463409e-02  5.30066043e-02 -9.74322334e-02
 -3.20548415e-01 -2.70650089e-01  5.57889529e-02 -1.71243772e-03
 -5.11603773e-01 -2.82896906e-01  2.28936467e-02  1.53017730e-01
 -6.63959086e-02 -3.39158326e-02  2.23605223e-02  2.43697241e-02
  1.77144587e-01  1.56670228e-01  6.63991943e-02 -1.46140113e-01
  2.08925039e-01  2.83861309e-01  3.42946202e-01 -8.31709951e-02
  4.95507896e-01 -1.73711240e-01 -7.51850978e-02 -7.87082613e-02
 -2.29626745e-01  2.66248174e-03  1.54942364e-01  1.41137792e-02
 -1.00770921e-01  1.38337418e-01  2.65830278e-01 -1.21942811e-01
  1.25270337e-02 -4.65083122e-01 -3.27655315e-01 -9.45177376e-02
  1.68329060e-01 -2.19276786e-01  3.81592989e-01 -5.60002401e-03
  3.43698174e-01 -3.77429247e-01  3.19824606e-01  4.10269797e-01
 -2.18557239e-01  3.54285210e-01  4.41533417e-01  2.40860641e-01
  4.49096411e-02  3.39039564e-01 -2.97761727e-02  5.85917756e-02
 -7.51610994e-02 -5.30343056e-01  3.10123451e-02 -4.21881713e-02
 -1.73836946e-01 -2.63556361e-01  3.50017905e-01  2.54964858e-01
 -1.77249629e-02 -1.78231671e-01  1.65022179e-01  1.47361591e-01
 -4.23740223e-02 -1.04670480e-01 -1.23937972e-01 -1.77219957e-01
 -1.97884560e-01 -1.66298047e-01 -3.88308428e-02  5.11704683e-01
  1.90217167e-01 -1.70438327e-02  1.23510346e-01 -4.15399894e-02
  4.40179944e-01  1.18456092e-02 -8.17304254e-02 -4.16069329e-02
  1.37937009e-01  7.18285143e-02 -7.88283944e-02  1.41277269e-01
  1.51426420e-01  2.61572480e-01 -6.51333481e-02  1.66515693e-01
 -1.72209859e-01 -1.00285567e-01 -1.57668948e-01 -4.13869619e-01
 -2.73259461e-01  3.13862622e-01  3.03216875e-02 -6.93932176e-01
  6.19981065e-02  2.19617113e-01 -1.65573046e-01  4.11359280e-01
 -3.66599262e-01  4.88896556e-02  2.50177644e-02  5.35195805e-02
 -1.79642186e-01  3.92186761e-01  1.27224445e-01  5.99032715e-02
  3.73952150e-01 -2.81154178e-03  1.22194082e-01 -3.14485520e-01
 -9.62822586e-02  3.29585493e-01 -1.91248849e-01 -3.06378424e-01
  4.09934949e-03  8.16833675e-02 -5.27957320e-01 -2.73072839e-01
  1.42506987e-01  4.72358108e-01 -4.45956737e-02 -8.07132423e-02
  2.51521289e-01  4.44467664e-02  1.99884430e-01 -1.11595988e-01
  4.64811713e-01 -3.97681773e-01  3.26963402e-02  4.66937125e-01
  1.07698992e-01  2.35236436e-01  1.80173054e-01  8.14514607e-03
  1.05779544e-01 -8.09888765e-02  3.16715002e-01  2.60322720e-01
 -2.35733360e-01 -1.46439120e-01 -6.56873584e-01 -9.16768238e-02
  5.33581495e-01 -1.37907982e-01 -2.05222949e-01 -4.31797504e-02
  2.50932008e-01  2.39845514e-02  2.78746068e-01  9.13015939e-03
 -1.43122226e-01  1.85449660e-01 -2.18588352e-01  2.16135047e-02
  1.48155183e-01  1.66598428e-02  1.20987948e-02 -3.77273440e-01
 -2.74441838e-01  5.60101606e-02 -3.92777845e-02 -5.27683258e-01
  7.99211487e-02 -2.92331749e-03 -1.89708889e-01  1.95301473e-02
 -5.96401393e-02 -7.98920542e-02 -1.49387091e-01  2.66838014e-01
  3.00120339e-02 -3.10215652e-01 -8.97150412e-02 -4.90660608e-01
 -2.80299306e-01  2.44953513e-01 -3.67779553e-01 -3.08688115e-02
  1.76667348e-01  7.13539030e-03  7.94601738e-02 -1.77690476e-01
  4.59840953e-01  2.59834230e-01  5.96037865e-01 -9.93570909e-02
 -1.26874059e-01 -3.01771671e-01 -4.06226933e-01 -8.94974172e-03
 -2.68456995e-01 -5.10461926e-01  2.78198309e-02 -6.30352274e-03
  4.89528745e-01  4.89427805e-01 -3.47148597e-01  1.38582811e-01
 -1.84955359e-01  3.09118092e-01 -1.22251078e-01  1.88188612e-01
  1.20064452e-01  1.69526443e-01  6.23349428e-01  2.79420733e-01
  2.17028767e-01  3.35411549e-01  1.98551551e-01  6.05880842e-02
  1.04703426e-01  2.42934003e-01  2.28465915e-01  2.87677884e-01
  5.25263906e-01  3.64908069e-01 -3.30270648e-01  2.36508951e-01
 -2.92086340e-02 -9.50391963e-02  3.41285735e-01 -3.20833743e-01
  5.98552883e-01 -3.40008050e-01  2.02231556e-01  1.57581687e-01
  4.27386045e-01 -1.45032167e-01 -8.91033933e-02  2.85183266e-02
 -3.24882045e-02  3.92689466e-01 -1.76892638e-01  3.84186246e-02
  1.18470974e-01  1.58633552e-02  1.78992838e-01 -6.26093984e-01
 -3.03803921e-01  6.61673918e-02 -2.41418511e-01 -9.31034684e-02
 -1.65880114e-01 -2.94857193e-04 -4.33299065e-01 -7.03208745e-02
 -7.90686980e-02 -1.27482444e-01  1.42225802e-01 -5.32203466e-02
 -2.88055599e-01  3.68144751e-01  4.65515882e-01 -2.70249784e-01
 -1.71159118e-01 -5.73779978e-02  4.48734522e-01  2.07632720e-01
  5.74196100e-01 -4.79568958e-01  3.44572216e-02 -1.22877583e-01
 -2.91552007e-01  4.56398308e-01 -1.57389268e-02  1.38191015e-01
 -4.39701796e-01  5.74871421e-01 -6.63268566e-02 -2.46341646e-01
  3.41050178e-01  4.62714545e-02 -3.91636372e-01 -1.37294978e-01
  8.91889632e-02  5.20798080e-02 -1.54644310e-01 -4.56288606e-01
  1.36666104e-01  1.13989778e-01 -2.08297610e-01 -1.15144759e-01
 -2.21518278e-01  2.43978962e-01 -5.24118185e-01 -1.46944314e-01
 -5.20128727e-01  2.59804517e-01 -6.36484325e-02 -3.90997738e-01
 -7.51610547e-02 -3.11450481e-01  4.08971636e-03 -2.16226041e-01
  2.15001311e-02 -9.42680091e-02  3.02621841e-01  3.61334294e-01
 -1.26491383e-01  1.01848938e-01  2.01481551e-01  1.19485632e-01
 -4.20743525e-01  4.85090762e-02  3.00079823e-01  2.99241245e-01
 -1.81102708e-01  1.78376399e-02  3.34926188e-01  4.85064268e-01
 -1.78643674e-01  3.92245770e-01 -3.44555140e-01 -1.95263065e-02
  9.02791694e-02 -1.71886951e-01 -3.13223958e-01 -1.62257195e-01
  1.62794456e-01  3.61960381e-01  4.44908813e-03  2.21716508e-01
 -4.77917314e-01  2.15715900e-01  4.76636827e-01 -4.55754846e-01
 -8.47651586e-02  1.83173358e-01  1.19117692e-01 -3.06801826e-01
  1.45987719e-01 -1.60822853e-01  1.80583879e-01  1.62360281e-01]"
Build problems when using TensorFlow Lite from another project in Windows stat:awaiting response type:build/install comp:lite subtype:windows TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11

### Bazel version

compiled using CMake

### GCC/compiler version

MSVC 17 2022

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When trying to create a project wheel which uses TensorFlow Lite, it is expected to work fine with:
`python setup.py bdist_wheel`

This command works fine for MacOS and Linux but does not work for Windows raising an error from the `tensorflow_src\tensorflow/lite/core/interpreter.h` file (you can see the error in the Relevant log output section).

For using TensorFlow Lite from the project, we followed the instructions in https://www.tensorflow.org/lite/guide/build_cmake#create_a_cmake_project_which_uses_tensorflow_lite as can be seen in its CMake files (https://github.com/Blosc/blosc2_btune/blob/main/src/CMakeLists.txt).

Also, there is no issue even on Windows for building TensorFlow Lite alone with 
```
cmake ../tensorflow_src/tensorflow/lite
cmake --build . -j
```

### Standalone code to reproduce the issue

```shell
The issue project is blosc2_btune (https://github.com/Blosc/blosc2_btune). You can clone it

`git clone https://github.com/Blosc/blosc2_btune.git`

install the requirements

`python -m pip install -r requirements-build.txt`

and reproduce the error with


prebuild.sh
python setup.py bdist_wheel
```
```


### Relevant log output

```shell
Generating Code...
  btune_model.cpp
C:\Users\marta\blosc2_btune\tensorflow_src\tensorflow/lite/core/interpreter.h(1000,40): error C2665: 'std::atomic_flag:
:atomic_flag': no overloaded function could convert all the argument types [C:\Users\marta\blosc2_btune\_skbuild\win-am
d64-3.11\cmake-build\src\blosc2_btune.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2022\BuildTools\VC\Tools\MSVC\14.37.32822\include\atomic(2886,1): messag
e : could be 'std::atomic_flag::atomic_flag(const std::atomic_flag &)' [C:\Users\marta\blosc2_btune\_skbuild\win-amd64-
3.11\cmake-build\src\blosc2_btune.vcxproj]
C:\Users\marta\blosc2_btune\tensorflow_src\tensorflow/lite/core/interpreter.h(1000,40): message : 'std::atomic_flag::at
omic_flag(const std::atomic_flag &)': cannot convert argument 1 from 'bool' to 'const std::atomic_flag &' [C:\Users\mar
ta\blosc2_btune\_skbuild\win-amd64-3.11\cmake-build\src\blosc2_btune.vcxproj]
C:\Users\marta\blosc2_btune\tensorflow_src\tensorflow/lite/core/interpreter.h(1000,41): message : Reason: cannot conver
t from 'bool' to 'const std::atomic_flag' [C:\Users\marta\blosc2_btune\_skbuild\win-amd64-3.11\cmake-build\src\blosc2_b
tune.vcxproj]
C:\Users\marta\blosc2_btune\tensorflow_src\tensorflow/lite/core/interpreter.h(1000,40): message : while trying to match
 the argument list '(bool)' [C:\Users\marta\blosc2_btune\_skbuild\win-amd64-3.11\cmake-build\src\blosc2_btune.vcxproj]
Traceback (most recent call last):
  File ""C:\Users\marta\miniconda3\envs\blosc2_btune2\Lib\site-packages\skbuild\setuptools_wrap.py"", line 674, in setup
    cmkr.make(make_args, install_target=cmake_install_target, env=env)
  File ""C:\Users\marta\miniconda3\envs\blosc2_btune2\Lib\site-packages\skbuild\cmaker.py"", line 697, in make
    self.make_impl(clargs=clargs, config=config, source_dir=source_dir, install_target=install_target, env=env)
  File ""C:\Users\marta\miniconda3\envs\blosc2_btune2\Lib\site-packages\skbuild\cmaker.py"", line 742, in make_impl
    raise SKBuildError(msg)

An error occurred while building with CMake.
  Command:
    'C:\Users\marta\miniconda3\envs\blosc2_btune2\Lib\site-packages\cmake\data\bin/cmake.exe' --build . --target install --config Release --
  Install target:
    install
  Source directory:
    C:\Users\marta\blosc2_btune
  Working directory:
    C:\Users\marta\blosc2_btune\_skbuild\win-amd64-3.11\cmake-build
Please check the install target is valid and see CMake's output for more information.
```
",False,"[-3.30218881e-01 -2.98984885e-01 -2.77022589e-02  1.93134293e-01
  8.53154659e-02 -2.35078201e-01 -4.30087388e-01  2.85588428e-02
 -2.06340477e-01 -2.70089388e-01 -2.86070138e-01 -8.54870975e-02
 -2.00294808e-01 -4.06796187e-02  5.19118160e-02  3.70461613e-01
 -3.17970306e-01  8.61888677e-02  1.93719536e-01  1.08246431e-01
 -1.50672629e-01 -2.43379086e-01 -3.20004314e-01  2.62473002e-02
  2.43463501e-01  1.55775964e-01 -1.56272233e-01 -6.74483180e-02
  6.15817308e-02  3.79141659e-01  2.94978380e-01 -7.00357929e-02
 -3.53917517e-02  1.16176084e-01  1.01087019e-01  1.35362163e-01
 -1.11015007e-01 -2.87696179e-02 -3.33797604e-01 -3.33077908e-02
  1.26552835e-01  1.44540876e-01  2.01044887e-01 -1.87203556e-01
  5.66862412e-02 -1.40640780e-01  2.53223721e-03 -2.04664350e-01
 -2.18792751e-01 -1.59163207e-01 -1.27417356e-01 -6.75396435e-03
 -3.48019630e-01 -2.74129093e-01 -2.67585874e-01 -2.42389157e-01
  1.14005186e-01 -2.09530685e-02 -8.73605832e-02  1.20922089e-01
 -3.13040614e-02 -2.43249871e-02  2.39264607e-01 -2.32160717e-01
 -1.78387925e-01  2.04998285e-01  1.28665343e-01  1.50690556e-01
  5.30520558e-01 -4.72227365e-01  2.63113827e-01 -2.99485922e-02
 -1.95372075e-01 -1.08410388e-01 -5.73360696e-02 -7.68129341e-03
 -1.66837871e-02  1.49083495e-01  4.15487170e-01 -2.62231827e-01
  1.04000866e-01 -2.51698703e-01  3.40176784e-02 -5.29135019e-02
  3.39955270e-01 -9.95443538e-02  3.16520244e-01  6.98267072e-02
  4.57613528e-01 -1.89973012e-01  4.04230118e-01  1.29042685e-01
  1.26726910e-01  1.62630722e-01  4.41875368e-01  1.20838836e-01
  1.12606168e-01  9.88887437e-03 -1.71162188e-04 -1.57240897e-01
 -1.91038966e-01 -2.48064995e-01 -3.59254442e-02  8.18615705e-02
  1.02664061e-01 -3.67029130e-01  3.45870376e-01  1.85144424e-01
  1.95390042e-02 -1.89287782e-01  3.95194918e-01 -7.89411813e-02
  1.70725554e-01 -1.96182400e-01 -1.48000032e-01 -6.50865734e-02
 -1.63270384e-01  1.03358023e-01 -1.16132595e-01  7.23255754e-01
  6.04810975e-02 -1.28734782e-01  8.60405564e-02  1.97177790e-02
  2.98833519e-01 -1.23731606e-01 -2.12107956e-01 -1.22686401e-02
 -3.09175923e-02 -5.11918887e-02  8.72595310e-02  3.57621491e-01
  3.66276681e-01  1.51789606e-01  2.57076085e-01  1.26316592e-01
 -3.50403607e-01 -3.26190770e-01 -2.10253060e-01 -3.08758795e-01
 -1.51731730e-01  2.29029194e-01 -2.90612906e-01 -5.94014406e-01
  3.70766073e-02  7.01362044e-02 -1.93995833e-01  2.27391630e-01
 -2.04959705e-01 -4.54324186e-02 -6.36022165e-03  4.32009757e-01
 -1.54829890e-01  2.01267377e-01  2.07272321e-01  2.22643912e-02
  3.38372648e-01  1.09113324e-02 -6.19167909e-02 -4.97053146e-01
 -1.46883419e-02  3.92269552e-01  2.95180827e-04 -1.03958175e-01
  2.13003546e-01  1.03204265e-01 -4.39909518e-01 -1.48841441e-01
  1.74806103e-01  3.57476950e-01  5.75698018e-02 -2.46670485e-01
  6.36423230e-02  1.36451423e-01  2.49172196e-01  1.47914141e-03
  2.59419948e-01 -4.51774120e-01  1.58176827e-03  3.63772392e-01
  2.07817823e-01  2.66258061e-01  2.54550219e-01  9.11509246e-02
  5.78265786e-02  1.17589653e-01  4.00469661e-01  3.92700732e-01
 -1.02084070e-01  1.54151872e-01 -5.53273261e-01 -6.52330071e-02
  3.64636600e-01 -2.10323006e-01 -2.29839697e-01  1.10406309e-01
  1.27341539e-01 -1.00462332e-01 -2.67135072e-02  1.43028498e-02
 -7.34717101e-02  8.42770487e-02 -7.49835372e-02 -3.89306732e-02
  1.46846920e-01 -1.93036050e-01 -1.53000489e-01 -3.70389521e-01
 -4.90317643e-01 -3.36086005e-02  8.59115571e-02 -4.86138105e-01
 -2.98459344e-02  4.65357080e-02 -3.23557884e-01  3.03353131e-01
  1.29847258e-01 -1.33990377e-01 -8.02802891e-02  1.80530995e-01
 -5.65593876e-03 -1.45932674e-01 -2.49386013e-01 -3.53581965e-01
 -2.35788614e-01 -2.42555924e-02 -3.93758714e-01  3.48528884e-02
  1.13974437e-01  2.23239824e-01  1.02333605e-01 -1.55675352e-01
  3.92063797e-01  1.09829858e-01  6.38926148e-01 -2.40436211e-01
 -1.34992123e-01 -7.40822777e-02 -2.78402895e-01  1.89684071e-02
 -2.95552790e-01 -2.00550273e-01 -7.16041550e-02  1.09257419e-02
  1.49781525e-01  4.14008319e-01 -2.09963784e-01 -1.34769738e-01
 -2.87907422e-01  2.18108356e-01 -9.23449919e-02  5.74035086e-02
  3.44552994e-01  2.26496711e-01  5.13760209e-01  2.59960949e-01
  1.00670941e-02  1.44387007e-01  8.78308117e-02 -9.57044065e-02
  5.49206734e-01  7.16893934e-03  1.32487848e-01  1.62169144e-01
  1.54219985e-01  1.56256273e-01 -2.92749763e-01  4.43552315e-01
  6.53098598e-02  2.66225599e-02  2.32660070e-01 -2.89019644e-01
  4.16182250e-01 -5.15813708e-01 -1.59917623e-02 -4.84657437e-02
  1.42628834e-01  1.46049738e-01 -2.69986391e-01 -1.93779737e-01
  5.99704012e-02  5.39170325e-01 -4.39584285e-01  1.52952641e-01
  1.65940255e-01 -2.29060769e-01  1.27991717e-02 -6.28269792e-01
 -2.01881766e-01  9.28117186e-02 -2.72577703e-01 -4.46601026e-02
 -1.95270013e-02 -5.67203853e-03 -3.93473476e-01 -1.25460267e-01
 -6.23101108e-02 -1.50547892e-01  2.06410345e-02  4.46136892e-02
 -1.96081802e-01  1.64650932e-01  4.36821043e-01 -3.56547564e-01
 -1.69456109e-01 -1.96520537e-01  3.96877080e-01  9.51878130e-02
  4.73428845e-01 -5.01429141e-01  2.78345108e-01 -8.72079581e-02
 -7.69780502e-02  5.17059386e-01 -1.94696248e-01 -7.60387778e-02
 -3.47395778e-01  6.25442922e-01  1.43013954e-01 -1.65444106e-01
  1.76048011e-01 -1.33401617e-01 -3.31012011e-01 -2.21727565e-02
  1.68395177e-01  4.26685512e-02 -7.35337362e-02 -3.91362607e-01
  3.33468392e-02  1.91257540e-02 -8.75538290e-02 -1.49431512e-01
 -1.78787708e-01  1.31816357e-01 -6.64493889e-02 -1.41794264e-01
 -3.73614490e-01  1.79803818e-01  1.28841233e-02 -2.44314983e-01
  4.80849762e-03 -1.53147489e-01  2.95357406e-01 -8.05141777e-02
  5.02240993e-02 -1.40168697e-01  3.18393946e-01  2.19622746e-01
 -5.16735762e-02  1.26086801e-01 -1.09471813e-01  2.82846272e-01
 -3.78974795e-01  2.66556218e-02 -5.09374812e-02  3.13308418e-01
  4.70874757e-02 -1.45783424e-02  2.97825396e-01  3.20952237e-01
  6.88059032e-02  1.86158732e-01 -3.59764308e-01 -3.17892842e-02
  2.61487573e-01 -8.01560655e-02 -2.75995642e-01 -2.11120188e-01
  9.63927805e-02  3.35863352e-01 -1.03191182e-01  1.95683867e-01
 -2.50090748e-01  3.38845968e-01  4.83281642e-01 -2.77443051e-01
 -2.15872586e-01  1.26710325e-01  1.02345578e-01 -1.84240807e-02
  1.15046576e-01  3.10619771e-02  1.97366267e-01  4.53683063e-02]"
org.tensorflow:tensorflow-lite-task-vision: type:support comp:lite,"implementation 'org.tensorflow:tensorflow-lite-task-vision:0.1.0'

Can I generate this dependency locally?

What instructions should I use to generate in TFlite?",False,"[-2.21282959e-01 -4.50127333e-01 -1.45108700e-02  1.28100768e-01
 -5.08906245e-02  1.50552288e-01  2.92790253e-02 -2.33430803e-01
 -1.40100062e-01  3.02761197e-02 -1.19610347e-01 -1.05258889e-01
 -1.43745586e-01  4.58661854e-01 -1.77537173e-01  2.70472229e-01
 -2.52342552e-01 -2.05061466e-01  2.16654226e-01  5.19314446e-02
  1.36930227e-01 -1.48435041e-01 -5.09194694e-02  1.11663379e-01
  2.21049041e-01  1.42963186e-01 -5.37537597e-02  6.18424907e-04
  4.27571461e-02  1.88050698e-02  2.92794079e-01  2.66728312e-01
 -2.50801206e-01  2.37123482e-02 -9.69779342e-02  4.74048033e-02
 -2.23065332e-01  3.30567621e-02 -1.15133971e-01 -1.73845753e-01
  5.98044805e-02 -4.97403601e-03 -2.41184920e-01 -2.36488149e-01
  3.30577753e-02  8.73509720e-02  2.82767624e-01  2.49951720e-01
 -1.80664942e-01  6.57549547e-03 -1.73240945e-01 -3.48968685e-01
 -4.97887343e-01 -2.68312305e-01  1.99192196e-01  2.05508754e-01
  2.61768460e-01 -1.25816613e-01  1.06656171e-01 -3.63910608e-02
  6.67723864e-02  1.09055936e-01  1.87751547e-01  2.07974445e-02
 -2.08314344e-01  5.29164016e-01  5.58356456e-02 -1.17234416e-01
  1.35463908e-01 -2.87700146e-01  3.71351875e-02  3.36813517e-02
 -1.54145369e-02 -2.52554446e-01 -1.58059672e-01 -3.92014645e-02
 -1.76464140e-01  2.93139160e-01  1.55853108e-01 -9.51139107e-02
 -3.64946239e-02 -5.77372313e-02  3.79550806e-03  2.18501732e-01
  3.37514579e-01 -8.36460590e-02  1.63361505e-01  2.77710427e-02
  2.76411980e-01  1.80173397e-01 -1.46553144e-01 -2.68310253e-02
 -1.46914095e-01 -8.24265406e-02  9.02653337e-02  2.31653918e-02
 -1.21307485e-01  2.98472762e-01  1.02248833e-01 -4.02421094e-02
 -1.11798458e-01 -1.17096290e-01 -5.97996078e-02  2.20500857e-01
  3.44979554e-01 -7.00508505e-02  3.89236122e-01 -6.03902712e-03
 -1.27624884e-01  3.79306823e-02  3.13159555e-01  9.25772116e-02
  4.22106951e-01  1.71937961e-02 -1.88471690e-01 -2.29789078e-01
  8.16228986e-03  6.87624067e-02  2.26013526e-01  4.48120981e-01
  3.55369151e-02 -2.73577362e-01  1.67982474e-01  1.31894231e-01
  3.92190039e-01 -4.46988344e-02 -3.79094601e-01  8.90936628e-02
 -8.37352127e-02  2.37841845e-01  7.55490661e-02  3.58457714e-01
 -3.34876142e-02  4.58058305e-02  1.16002209e-01  1.69911683e-01
 -5.67851305e-01 -6.76655471e-01 -3.18591118e-01 -1.72304034e-01
 -1.29104882e-01  5.06617486e-01  7.01818764e-02 -2.10279465e-01
  3.29649031e-01  3.83841217e-01 -2.20503852e-01  1.85652345e-01
 -1.65366679e-01 -2.29629382e-01 -1.45986199e-01 -2.52611995e-01
 -9.03419778e-02  1.98315874e-01  1.60462558e-01 -4.29525524e-02
  6.93662139e-03 -1.99467819e-02 -1.18991412e-01 -2.75460482e-02
 -7.18300790e-02  1.17917821e-01 -2.51031548e-01 -4.25457954e-03
 -4.62526120e-02  3.42202000e-02 -2.64380336e-01 -1.56080887e-01
  3.16767901e-01  1.05853491e-01  1.85680136e-01 -2.48406738e-01
 -9.53758061e-02  1.78502411e-01  3.90965998e-01  9.39346701e-02
  2.33974546e-01 -4.44503695e-01 -1.38234599e-02 -1.98641017e-01
 -2.18213368e-02 -1.36431873e-01  8.78038853e-02 -2.82609582e-01
 -1.68898568e-01  2.19649705e-03  5.50409436e-01  2.88197339e-01
 -2.29282171e-01 -1.47713453e-01 -4.44370508e-01  2.37183154e-01
  1.68966323e-01  9.91648063e-02 -2.71919757e-01 -1.75823644e-01
  2.99508452e-01 -1.33175636e-03 -4.41286080e-02  5.87425660e-03
 -1.42622158e-01  8.74709785e-02 -1.76961020e-01  8.08212385e-02
  1.25939548e-01 -1.34751305e-01 -1.82158202e-01  4.25793864e-02
 -3.69293809e-01  2.40836702e-02  3.05822074e-01 -3.99647921e-01
  4.81284931e-02 -1.74965650e-01 -1.84731647e-01 -7.11994320e-02
  1.30791627e-02 -6.63964599e-02 -2.38390163e-01  8.94640386e-02
  1.88023776e-01 -6.49900287e-02 -2.18429282e-01 -3.41293663e-01
 -8.58057290e-02 -2.96596121e-02 -3.46331179e-01  1.36828497e-01
  5.67389876e-02  2.78331161e-01 -3.74621630e-01 -4.14549977e-01
  2.50818044e-01  1.26967520e-01  1.48691550e-01 -1.87392935e-01
  2.28094950e-01 -2.49757543e-01 -3.17643285e-01 -1.48031652e-01
 -2.47245312e-01 -2.77508795e-01 -1.44276261e-01 -1.67395920e-01
 -4.50450599e-01 -4.43013608e-02  1.00595430e-01 -1.09887376e-01
  9.27846953e-02  6.65857017e-01 -6.90626800e-02  9.81699862e-03
  3.95864367e-01  3.03956002e-01  2.22485706e-01  3.76578309e-02
  4.07561986e-03 -1.29451290e-01  6.98359311e-02  2.25314498e-01
  1.98071778e-01  1.78385228e-01  2.78222769e-01  5.83210707e-01
  2.76855230e-01  2.38959491e-01 -1.07655808e-01  1.36574656e-01
 -3.54744613e-01 -4.63913232e-02 -2.16191381e-01  2.71972120e-01
 -3.68038751e-03 -6.78698346e-02 -4.57409397e-02 -9.96950492e-02
  1.16228856e-01 -2.68644989e-01 -6.42524809e-02  1.24250978e-01
  1.36378214e-01  2.76634216e-01 -3.29464406e-01 -4.75828983e-02
 -8.97253677e-02 -2.09856942e-01 -3.89616704e-03 -2.77594626e-01
 -1.71190292e-01  1.09314054e-01 -3.50016236e-01  1.77674279e-01
  5.26848994e-02 -5.33595635e-03 -3.04091901e-01 -1.40890718e-01
  3.19878250e-01 -1.05285987e-01  1.21242426e-01  2.90072672e-02
  4.06265296e-02  1.91563249e-01  3.58031183e-01 -1.78776160e-01
  3.47051948e-01  6.74461275e-02  3.70827794e-01 -9.57200006e-02
  3.56549382e-01 -3.33494604e-01  1.18145175e-01  2.13435829e-01
  1.48219034e-01  4.90214437e-01  6.37038611e-03 -3.87364775e-02
 -1.08074896e-01  1.52693525e-01  3.48016061e-02 -3.48717064e-01
 -3.36652957e-02  8.45501851e-03 -2.33474135e-01  7.96425343e-02
 -7.84239825e-03 -8.78504142e-02  1.81983680e-01  3.53688709e-02
 -1.20028540e-01 -2.09420457e-01 -2.90223598e-01 -3.77846599e-01
  2.94273436e-01  3.28331113e-01  4.94910665e-02  4.08713296e-02
 -1.73472449e-01  1.14648892e-02 -3.67832124e-01 -1.07319988e-01
  8.51517841e-02  7.10158935e-03 -1.12634763e-01 -1.67766973e-01
  1.00510940e-01 -1.65160432e-01 -1.40683532e-01  1.47058442e-01
 -1.50129125e-01  2.19895259e-01 -9.33562368e-02  3.39568585e-01
 -4.87606555e-01  7.28666261e-02  3.00038517e-01  6.01972379e-02
 -3.28952670e-01  1.42572597e-01  1.25254631e-01  3.85257155e-01
 -8.24511573e-02  1.12036005e-01 -1.18869595e-01 -2.36900881e-01
  1.55361205e-01  4.05172408e-02 -1.79458901e-01 -1.98644012e-01
  1.23654529e-02  6.77738488e-01 -4.12534811e-02  4.59344387e-01
  9.07382816e-02 -4.19579297e-02  3.55820268e-01  8.65264237e-02
  2.72479713e-01 -3.94350365e-02  1.94872960e-01 -1.50945723e-01
  8.11525956e-02  9.27657932e-02  7.85778686e-02 -2.15612143e-01]"
ERROR: No matching distribution found for tensorflow==2.12.0 stat:awaiting response type:build/install stale TF 2.12,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 pip3 install tensorflow==2.12.0
 
 

### Standalone code to reproduce the issue

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow==2.12.0 (from versions: 1.13.1, 1.13.2, 1.14.0, 1.15.0, 1.15.2, 1.15.3, 1.15.4, 1.15.5, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 
2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 
2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0)
ERROR: No matching distribution found for tensorflow==2.12.0
```


### Relevant log output

_No response_",False,"[-0.72806174 -0.33958417 -0.0656372   0.10576998  0.39621052 -0.36019614
 -0.01040448 -0.01909497 -0.31730536 -0.46161914  0.22292686 -0.15613458
 -0.10909438  0.06197997 -0.2829446   0.21179458 -0.28458655 -0.2175644
  0.30275595  0.1544078  -0.20446405 -0.098666   -0.30142862  0.2782402
  0.19579402  0.2319293  -0.25216317  0.04278784  0.08799908  0.19542447
  0.4650101   0.203502   -0.23951584  0.18659571  0.14479136  0.13960566
 -0.41372913 -0.11829571 -0.35737038 -0.15272899  0.06243766  0.01428877
  0.21445575 -0.12079863 -0.02841512 -0.16987082  0.10282871 -0.09489882
  0.03515235 -0.22157907  0.10835163  0.0361208  -0.58372283 -0.20352374
 -0.03224945  0.01462435  0.14531186 -0.03794321  0.10351693  0.2757392
  0.12629645  0.13924311 -0.12751509 -0.13302593  0.14548694  0.0746889
  0.21824937 -0.18805051  0.44973755 -0.11891215  0.10419329 -0.06560847
 -0.24812582  0.03476653  0.0207285   0.18160138  0.10128739  0.21644674
  0.19033262 -0.12086402 -0.18526351 -0.352604   -0.18682577 -0.19015065
  0.09548341 -0.35436332  0.43225127  0.21856228  0.5655893  -0.3431551
  0.4884483   0.48837674  0.04941846  0.06259564  0.31774223  0.16494809
 -0.00831383  0.21447513  0.09367771  0.10565357 -0.11698717 -0.28758794
  0.20258269  0.07610584 -0.10786524 -0.10142471  0.00234182 -0.12224267
  0.18230999  0.02802736 -0.008967    0.06598696  0.22815563  0.1024262
 -0.11366791 -0.13446122  0.04907195  0.03313313 -0.00103003  0.525082
  0.10480036 -0.11401209  0.09059061  0.02016332  0.44715524  0.01666467
 -0.20288242 -0.10478172  0.18495321 -0.02291169  0.10974943  0.13359109
 -0.11490354  0.2889402  -0.1714732   0.10307389 -0.30063793 -0.13641262
 -0.3131043  -0.05988281 -0.26522684  0.24808668 -0.02854614 -0.50488144
  0.24063092  0.07816924 -0.11923677  0.40050185 -0.32810247  0.09582011
 -0.04515412 -0.16093114 -0.13999596  0.30100024  0.0900255  -0.06144269
  0.39880824 -0.15685387  0.07012999 -0.34590268  0.10880132  0.4505738
 -0.2274607  -0.16232982  0.00709887  0.243835   -0.5405885  -0.31744194
  0.15784578  0.45949188 -0.03512958 -0.23695713  0.09663678  0.07225834
  0.20812233 -0.29641485  0.5169722  -0.5142757  -0.2375438   0.2858398
 -0.08854687  0.02640981 -0.01507942 -0.04215477  0.09546505  0.00971543
  0.14920811  0.15953684 -0.33671662 -0.0804427  -0.43437266 -0.14073776
  0.5099896   0.06330188 -0.05585965  0.04324921  0.17676502 -0.21010509
  0.12606615  0.0714733  -0.22826548 -0.17126876  0.04330419 -0.08745271
  0.13284007 -0.25780216  0.04806012 -0.5027907  -0.3949004   0.1047029
  0.159453   -0.58423436  0.03020931 -0.07808026 -0.3979197   0.32995456
  0.05113269  0.00632327 -0.20781492  0.29121524  0.19892423 -0.18868816
  0.06651061 -0.3725354  -0.37655774  0.20207496 -0.2742008   0.12025091
  0.04570699  0.17974415  0.12280361  0.13362905  0.44103658  0.3620096
  0.37807494 -0.32877904 -0.12632817 -0.30384833 -0.18027598  0.00421059
 -0.33455473 -0.15187141  0.01270835 -0.17776228  0.36184567  0.66533256
 -0.21547869 -0.15298203 -0.4603421   0.21648464 -0.22617006  0.33800274
  0.31076574  0.26043475  0.5549647   0.26866686  0.27972373  0.31580114
  0.3117994  -0.2794079   0.38519987  0.3860916   0.00595037  0.48188198
  0.19981463  0.351513   -0.37044322  0.5422543  -0.03182462 -0.22934836
  0.0612935  -0.50495267  0.7079279  -0.46999413 -0.09342693  0.07287824
  0.3446077   0.03640461  0.02732974  0.28436673 -0.06796589  0.4776716
 -0.43277353 -0.06370572  0.02459307 -0.16258219  0.02634145 -0.6284328
 -0.21611743  0.21448597 -0.27409106  0.15753844 -0.14976948 -0.00133565
 -0.2027396  -0.0829488   0.07836572 -0.1829652   0.34759712  0.35631734
  0.00830715  0.07972848  0.42023566 -0.36234742 -0.11401474 -0.10994804
  0.25287384  0.38852566  0.41725302 -0.50549436  0.20111126 -0.2976963
  0.1110577   0.59770536  0.06455332  0.151544   -0.37503463  0.5289101
  0.28837606 -0.0802382   0.32972082 -0.23564392 -0.3539306  -0.06227163
  0.27656156 -0.17363778  0.28433076 -0.5325456  -0.01198625  0.27741003
 -0.2692385  -0.00976366 -0.03616595 -0.02254616 -0.29871616 -0.02480863
 -0.54893804  0.09565286 -0.01536999 -0.34412313 -0.37852204 -0.16223086
 -0.1050363  -0.1154728   0.07105807 -0.22026464  0.28529322  0.33806023
 -0.24106944  0.07929615  0.16384925 -0.01301102 -0.40345785 -0.10504201
  0.06800902  0.13325278  0.07033549 -0.24055493  0.5177415   0.29236835
 -0.15236273  0.02571955 -0.17214063  0.03387666  0.19227472 -0.26066723
 -0.21497506 -0.18633945  0.03172134  0.28762847  0.05644301  0.25001615
 -0.3619148   0.12817138  0.48716912 -0.69261914 -0.35578477  0.18326855
  0.26705432 -0.36938217 -0.0074111  -0.20139712  0.2823916  -0.08854419]"
tf-nightly import fails with keras stat:awaiting response type:build/install stale comp:keras subtype: ubuntu/linux,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15.0.dev20230914

### Custom code

No

### OS platform and distribution

Arch Linux

### Mobile device

_No response_

### Python version

Both 3.10 and 3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Running `import tensorflow.compat.v2 as tf` causes the following when keras is installed alongside tf-nightly:

```
/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/tensorflow/__init__.py:29: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptool
s or check PEP 632 for potential alternatives                                                                                                                                                  
  import distutils as _distutils                                                                                                                                                               
2023-09-14 14:47:40.395318: I external/local_tsl/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.                                                
2023-09-14 14:47:40.429289: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9511] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has alre
ady been registered                                                                                                                                                                            
2023-09-14 14:47:40.429321: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has alrea
dy been registered
2023-09-14 14:47:40.429377: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has a
lready been registered
2023-09-14 14:47:40.436510: I external/local_tsl/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-09-14 14:47:40.436868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-09-14 14:47:42.270861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/tensorflow/__init__.py"", line 483, in <module>
    importlib.import_module(""keras.optimizers"") 
  File ""/home/gap/.pyenv/versions/3.10.2-debug/lib/python3.10/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/__init__.py"", line 3, in <module>
    from keras import __internal__
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/__internal__/__init__.py"", line 3, in <module>
    from keras.__internal__ import backend
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/__internal__/backend/__init__.py"", line 3, in <module>
    from keras.src.backend import _initialize_variables as initialize_variables
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/__init__.py"", line 21, in <module>
    from keras.src import models
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/models/__init__.py"", line 18, in <module>
    from keras.src.engine.functional import Functional
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/engine/functional.py"", line 25, in <module>
    from keras.src import backend
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/backend.py"", line 35, in <module>
    from keras.src.engine import keras_tensor
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/engine/keras_tensor.py"", line 19, in <module>
    from keras.src.utils import object_identity
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/utils/__init__.py"", line 20, in <module>
    from keras.src.saving.serialization_lib import deserialize_keras_object
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py"", line 28, in <module>
    from keras.src.saving.legacy.saved_model.utils import in_tf_saved_model_scope
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/saving/legacy/saved_model/utils.py"", line 30, in <module>
    from keras.src.utils.layer_utils import CallFunctionSpec
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/utils/layer_utils.py"", line 26, in <module>
    from keras.src import initializers
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/initializers/__init__.py"", line 23, in <module>
    from keras.src.initializers import initializers_v1
  File ""/home/gap/work/SHARK/venv2/lib/python3.10/site-packages/keras/src/initializers/initializers_v1.py"", line 32, in <module>
    keras_export(
TypeError: api_export.__init__() got an unexpected keyword argument 'allow_multiple_exports'
```

### Standalone code to reproduce the issue

```shell
python3 -c 'import tensorflow.compat.v2 as tf'
```


### Relevant log output

_No response_",False,"[-0.4742976  -0.35773307 -0.28794262 -0.02277554  0.29714528 -0.47273228
 -0.10846528 -0.07094577 -0.4002536  -0.28759325  0.11362359 -0.01268845
 -0.24978834 -0.04674413 -0.21671274  0.17176302 -0.32370397 -0.09704524
  0.30358458  0.08159044 -0.14250717  0.04274375 -0.11236649  0.2973753
  0.14535135  0.1964616  -0.16397676  0.10375658 -0.09152532  0.21287784
  0.25903407  0.2350026  -0.25703803  0.08936563  0.06792034  0.20007059
 -0.09769779 -0.19633377 -0.37442744 -0.1316132   0.25315472  0.00597897
  0.02400853 -0.24079624  0.01787876 -0.21467827 -0.0184455  -0.21741217
 -0.10978369 -0.38791907 -0.18030563 -0.002049   -0.4426097  -0.32585263
  0.02959203 -0.18972164  0.211306    0.09963288 -0.01512161  0.06772998
  0.05580674 -0.01226693  0.10736749  0.06009723  0.18625571 -0.09462959
  0.06053781  0.05768576  0.3157618  -0.22644332 -0.08657297 -0.12680863
 -0.36709362  0.15569867 -0.09474587  0.21514346  0.16730618  0.00304618
  0.3968191  -0.10282306 -0.09802752 -0.3293594  -0.18405262 -0.2973984
  0.06778455 -0.16083723  0.28701276  0.03300939  0.56301993 -0.17753439
  0.6477813   0.5683851   0.10288547  0.22331765  0.53593564  0.1269899
  0.20891756  0.22480288 -0.13899887 -0.08343968 -0.1987023  -0.32109672
  0.07621251 -0.0064429  -0.06192593 -0.16896348  0.13516738 -0.15168777
  0.22585678 -0.19616942  0.19413105  0.05774583  0.1895161   0.08228187
 -0.05061325  0.21077585 -0.12621415  0.07344729  0.15215544  0.74856764
 -0.06431662 -0.14047265  0.08641035  0.15050302  0.36284506  0.2678762
 -0.16961077  0.17555687 -0.00362495 -0.00839736 -0.11506006  0.02998635
  0.11202966  0.24179132  0.0102988   0.05432332 -0.2979585  -0.09183048
 -0.2804786  -0.30961728 -0.35858405  0.27337813 -0.01135128 -0.46212935
  0.1643447   0.08800431 -0.13562474  0.20946828 -0.24961661  0.29956752
  0.09180388 -0.09894643 -0.05438728  0.32071686  0.0919995  -0.11705445
  0.19563656  0.01006526  0.04146114 -0.36060062  0.05167215  0.41590422
  0.05632727 -0.17926961  0.14717594  0.17931801 -0.6252451  -0.43726712
  0.1556659   0.3816663  -0.11650521  0.02662252  0.13621432  0.03490644
  0.3061986  -0.06306188  0.38018554 -0.37494037 -0.17859498  0.4149344
  0.1228045   0.17884237  0.03790704  0.2773562   0.19646582 -0.05790247
  0.3160684  -0.03937304 -0.26773843  0.05695192 -0.3960852  -0.10700136
  0.59821737 -0.19266388 -0.07802976  0.0962956   0.21889749 -0.19204797
  0.13680018 -0.1708369  -0.11116567 -0.10198338 -0.08376332  0.04574912
  0.01144702 -0.315166   -0.00555096 -0.34264183 -0.22736628  0.13227688
  0.16544467 -0.5313866   0.0852834  -0.08364001 -0.37446332  0.24515337
  0.17084134  0.08367753  0.05383156  0.3914058   0.21335344 -0.35135895
  0.01413901 -0.5152677   0.01128517  0.13112032 -0.30424675  0.24075046
  0.00847137  0.10111341 -0.12430328  0.07928455  0.43139666  0.2120198
  0.42009693 -0.21445484 -0.02427005 -0.22094476  0.01737503 -0.04372844
 -0.41819912 -0.10944794  0.09797733 -0.3091392   0.22748667  0.3221245
 -0.25398058 -0.02503334 -0.4909624   0.14690822 -0.10088041  0.30903232
  0.00723561  0.21085179  0.59321666  0.14228675  0.1214143   0.24267292
  0.3283043  -0.17732504  0.46088716  0.11487991  0.1008068   0.48659766
  0.2972153   0.2706737  -0.4441807   0.5742002  -0.14510842 -0.28019333
  0.01998604 -0.41347903  0.7523515  -0.35834417  0.20887075 -0.15038346
  0.38538396 -0.0822116   0.07264929  0.16316408  0.14551926  0.42989194
 -0.44581687  0.13362163  0.12758042 -0.17755216 -0.08921893 -0.8954795
 -0.13849285  0.11561174 -0.19776043 -0.02246089  0.05591152 -0.06855616
 -0.18968964  0.00559458 -0.07863234 -0.20590469  0.02135863  0.15816832
 -0.29920053  0.21615334  0.5872615  -0.43238133 -0.09708629 -0.24095854
  0.26949728  0.4066401   0.5198436  -0.44754675  0.25524867  0.13263157
 -0.06832404  0.5342616   0.17479065  0.02785504 -0.40475667  0.61432004
 -0.00200672 -0.15350267  0.2640177  -0.24274762 -0.5118231   0.20083924
  0.36032793 -0.05617149  0.02260437 -0.46794844  0.12457052  0.25212613
 -0.24159107 -0.04411611 -0.15017644  0.11171473 -0.2564996  -0.00710271
 -0.5002984   0.20130254 -0.19354668 -0.47409004 -0.20806634 -0.05301376
 -0.04556913 -0.13721368  0.02876779 -0.35647988  0.2581616   0.44311482
  0.18132275  0.00662094 -0.14838286  0.19683395 -0.45920765  0.0728732
  0.00549539  0.18977708 -0.08812576 -0.1441012   0.35333657  0.357491
 -0.11175152  0.03920373 -0.40046614  0.03310192  0.08312549 -0.17113599
 -0.22176902 -0.09793697 -0.06489672  0.32494947 -0.00218011  0.46098578
 -0.3512218   0.2457392   0.46714255 -0.60170543 -0.31665665  0.260559
  0.15204087 -0.1774593   0.09024555 -0.14735028  0.07294666  0.02254489]"
Using TFLite GPU in Android gives warning about duplicate namespaces stat:contribution welcome type:build/install comp:lite Android TF 2.12,"**Standalone code to reproduce the issue**
See: https://github.com/bvschaik/tflite-android-warning

Basically:

1. Create an empty Android project
2. Add tflite gpu dependency to build.gradle: `implementation(""com.google.android.gms:play-services-tflite-gpu:16.2.0"")`
3. Compile app: `./gradlew assembleDebug`
4. Note the warning emitted: 

> [org.tensorflow:tensorflow-lite-api:2.12.0] ~/.gradle/caches/transforms-3/2eb840e608fe786447e3ad08d13f4541/transformed/tensorflow-lite-api-2.12.0/AndroidManifest.xml Warning:
> Namespace 'org.tensorflow.lite' is used in multiple modules and/or libraries: org.tensorflow:tensorflow-lite-api:2.12.0, org.tensorflow:tensorflow-lite-gpu-api:2.12.0. Please ensure that all modules and libraries have a unique namespace. For more information, See https://developer.android.com/studio/build/configure-app-module#set-namespace

Cause:

Using tflite-gpu adds both the -api and -gpu-api libraries (output from `./gradlew :app:dependencies`):

```
\--- com.google.android.gms:play-services-tflite-gpu:16.2.0
     +--- com.google.android.gms:play-services-base:18.1.0 (*)
     +--- com.google.android.gms:play-services-basement:18.1.0 (*)
     +--- com.google.android.gms:play-services-tasks:18.0.2 (*)
     +--- org.tensorflow:tensorflow-lite-api:2.12.0
     \--- org.tensorflow:tensorflow-lite-gpu-api:2.12.0
```

Both org.tensorflow:tensorflow-lite-api and org.tensorflow:tensorflow-lite-gpu-api specify the same namespace (`org.tensorflow.lite`) in their mainfest:

```xml
<manifest xmlns:android=""http://schemas.android.com/apk/res/android""
    package=""org.tensorflow.lite"">
...
```

Suggested solution:

Change the namespace in the manifest of org.tensorflow:tensorflow-lite-gpu-api to `org.tensorflow.lite.gpu` to make them unique.",False,"[-0.29835278 -0.15338756 -0.17644064 -0.20143995 -0.00269363 -0.08496013
 -0.08648751  0.24705476 -0.2574418  -0.18595788 -0.00537338  0.069915
  0.24127588  0.12575859 -0.01895061  0.04808097 -0.3061596  -0.11415629
  0.22597025  0.24255314 -0.04380453  0.09175609 -0.08205391  0.2820928
  0.3518247   0.39035383 -0.1588665   0.14449397 -0.01552263  0.31071073
  0.17597988  0.3324412  -0.24455078  0.11200827  0.09869982 -0.16040823
 -0.62596345  0.00381029 -0.24061564 -0.01271037  0.05811862  0.10520953
 -0.04059655  0.1584984  -0.52329314 -0.00907148  0.03199141  0.25037572
 -0.23481764  0.04347967  0.17148364 -0.10862655 -0.42316225 -0.44174683
  0.05930196  0.0581653  -0.20611152  0.1828934   0.00974085  0.16379142
  0.44916058 -0.06347515 -0.00267672  0.17385827 -0.11267472  0.27620518
 -0.17660226 -0.10914508  0.23028141 -0.111606    0.0025913  -0.05201907
  0.15763855 -0.1175731   0.2119837   0.1762098  -0.24663939  0.3488444
 -0.25018224 -0.28153938 -0.1186102  -0.1195777  -0.08137329  0.29685754
  0.2546836  -0.11950469 -0.02182522  0.22210532  0.1537242   0.18068555
  0.25335097  0.4583416   0.2826635   0.20201057 -0.13168913  0.19263628
 -0.29447496  0.10482106 -0.10929776 -0.03445635  0.01961045 -0.27434435
  0.09079982  0.26898044  0.15924892 -0.38432628  0.02970508  0.11004573
 -0.03972214  0.18705413  0.32962468 -0.16974208  0.1525375   0.19064802
 -0.15852693  0.3668011   0.14989541  0.04525559  0.06340262  0.12706432
 -0.254546   -0.13140728  0.00737591 -0.28858775  0.19375893 -0.10371375
 -0.41630915 -0.15403156  0.32631844  0.41887602 -0.05228259  0.1252414
 -0.13161653 -0.10354379  0.03372012  0.08436792 -0.39638853 -0.28386283
 -0.12393672  0.25466394  0.02454882  0.2292933   0.00248411 -0.07876804
 -0.07972603  0.2857148  -0.10373595  0.24274936  0.06483205  0.11428452
 -0.19688216  0.13310203 -0.06433874  0.524631    0.29732382 -0.33255917
  0.05096152 -0.06067798  0.20919    -0.29119986 -0.11704102  0.11430053
  0.15783995  0.01590137  0.20600545  0.11965009 -0.57871264 -0.04187198
  0.10700063 -0.06402495  0.0530503  -0.10869855  0.13518408  0.21921512
  0.22221437 -0.20243195  0.3829435  -0.58159125  0.06884927 -0.16736645
  0.11795697  0.08196276 -0.26294893 -0.25409338 -0.5040727  -0.03172047
  0.0932357   0.00850462 -0.04128093 -0.22063446 -0.36870307  0.3436494
 -0.06759353 -0.10545449 -0.09724126 -0.0268517   0.39079174  0.0131757
  0.40815845  0.00736162  0.15050542  0.0566312   0.01168658 -0.01247429
  0.3106012  -0.199078   -0.14216134 -0.16198003 -0.31467763 -0.01462222
  0.23413303 -0.26034692 -0.33592308 -0.148298   -0.0532279   0.21073912
 -0.26297364  0.21063872 -0.21734665  0.16698028  0.18512811 -0.07100109
 -0.23937017 -0.22660458 -0.46361676 -0.1472896  -0.11270442 -0.21255673
 -0.13017292  0.18076375 -0.35004675 -0.01713571  0.29873425 -0.10081812
  0.0533087  -0.00932259  0.48652247 -0.48082644 -0.4050297  -0.14491045
  0.09777136 -0.20372039 -0.13213149 -0.34862104  0.26746237 -0.03831892
  0.26373172  0.1127077   0.06077198  0.21707836 -0.08548672 -0.26822138
  0.25814554  0.16489826  0.16564263  0.34560084 -0.08165557 -0.29922265
  0.05361106 -0.05825242 -0.27558115  0.2072051  -0.01730434  0.4172915
  0.1353197  -0.18988928 -0.09780305 -0.03524406 -0.21235032 -0.30456758
  0.31368667 -0.07043165  0.18236093 -0.1953631   0.00753657  0.05422438
  0.34626287 -0.28835732  0.04956857  0.17872587  0.21054718  0.28815237
 -0.17999434 -0.08289967  0.41642672 -0.20755646 -0.1796342  -0.02655978
 -0.35584787  0.10326901 -0.0722632   0.04941676 -0.10004131  0.14579102
 -0.3256652   0.14516823  0.04019145 -0.01831159  0.22124918 -0.02251933
 -0.04266077  0.38333654  0.12375603 -0.21662544 -0.189476    0.04482729
 -0.04676793 -0.13112172  0.350065   -0.52380955  0.26369637  0.05650897
 -0.08936603  0.3193553  -0.2386405   0.2337127  -0.18942325  0.28602383
 -0.02590305  0.03019851  0.12055088 -0.16064733 -0.2215747   0.1694402
  0.10287239 -0.00098857 -0.01571214  0.08751738  0.13952187 -0.07164407
 -0.08747165 -0.34782034 -0.07410013  0.0892999   0.00340506 -0.1394393
  0.130323    0.28027195 -0.11271095  0.05631951 -0.42222968 -0.19701248
 -0.08439275 -0.04327207 -0.19140038  0.12791425  0.21470731  0.19834867
  0.36463398  0.21564336 -0.01681043 -0.12284826 -0.01963835 -0.18312803
 -0.19795156  0.18794018  0.10527152 -0.00738115  0.10600398  0.554631
  0.02616297 -0.24252051 -0.07789371 -0.25316703 -0.17398068 -0.25429347
 -0.2287375   0.08021452  0.24709684  0.38574222 -0.03662629  0.3411222
 -0.17993256 -0.38093916  0.16027124  0.03262176  0.23657702 -0.09073491
 -0.07839754 -0.08628817 -0.07143554  0.11242716  0.04168943  0.16822506]"
ConverterError: 'tf.FastWordpieceTokenizeWithOffsets' op is neither a custom op nor a flex op stat:awaiting response stale type:performance TFLiteConverter,"While doing tflite lite model conversion with input signature as string data type i am getting this error
ConverterError: /usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py:670:0: error: 'tf.FastWordpieceTokenizeWithOffsets' op is neither a custom op nor a flex op
:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py:670:0: note: Error code: ERROR_NEEDS_CUSTOM_OPS
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py:670:0: error: 'tf.TFText>FastWordpieceDetokenize' op is neither a custom op nor a flex op
:0: note: loc(fused[""StatefulPartitionedCall:"", ""StatefulPartitionedCall""]): called from
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py:670:0: note: Error code: ERROR_NEEDS_CUSTOM_OPS
:0: error: failed while converting: 'main':
Some ops in the model are custom ops, See instructions to implement custom ops: https://www.tensorflow.org/lite/guide/ops_custom
Custom ops: FastWordpieceTokenizeWithOffsets, TFText>FastWordpieceDetokenize
Details:
tf.FastWordpieceTokenizeWithOffsets(tensor<?x!tf_type.string>, tensor<241460xui8>) -> (tensor<?x!tf_type.string>, tensor<?xi64>, tensor<?xi64>, tensor<?xi64>, tensor<?xi64>) : {device = ""/device:CPU:0""}
tf.TFText>FastWordpieceDetokenize(tensor<40xi32>, tensor<2xi64>, tensor<339052xui8>) -> (tensor<?x!tf_type.string>) : {device = """"}

kindly please check this below colab link
https://colab.research.google.com/drive/14_ajVH4r4NXN6cDw96XJNBtltoueoGfN?usp=sharing",False,"[-0.47035372 -0.25477135 -0.35015067 -0.00786184 -0.07256248 -0.09828719
  0.07427831  0.23050997 -0.18729097 -0.05986338  0.03954417  0.05320366
  0.06240787  0.05825004 -0.05875821  0.3797003   0.04584395 -0.12194595
  0.33711898  0.16257507 -0.12560144 -0.12264311 -0.05318075  0.20461574
  0.16277973  0.2842128  -0.35577267  0.04577956  0.10341029  0.51871836
 -0.14423458  0.13341334 -0.11136164  0.03938401 -0.09430248  0.1124901
 -0.43978864 -0.03104807 -0.31260115 -0.15108138  0.07147093 -0.23566462
 -0.24341317  0.13703623 -0.35397923 -0.01755074  0.18326102  0.20545158
 -0.20769191  0.16532868 -0.16318826  0.00112184 -0.3930791  -0.3554772
  0.27833137  0.19174553 -0.06592615  0.36661494  0.26355487  0.0627572
 -0.00491382 -0.09891795 -0.17234248 -0.26628655  0.10270689  0.20991078
  0.00822642 -0.13062136  0.08542761 -0.51073605 -0.08457389 -0.12984508
 -0.03910407  0.07037213 -0.01380348  0.06636536 -0.28213927  0.15591922
 -0.08148216 -0.2429977  -0.07202375 -0.40495652 -0.28715745  0.15896046
  0.5280756  -0.10475099  0.22850126 -0.07779586  0.29284757  0.21637759
  0.26887184  0.36343724  0.074645    0.2506179   0.23832427  0.10760456
 -0.01063166  0.13685435 -0.22396733  0.05218825  0.2073225  -0.39332968
 -0.24226624  0.3545967   0.12310484 -0.33529842  0.01931222  0.11028735
 -0.12437735  0.07202144  0.12898704 -0.05975941  0.04599189 -0.11569676
  0.11017438  0.12232462  0.1856561   0.19850373  0.14077598  0.31866968
  0.04407758 -0.18625736 -0.14384013  0.2502508   0.2158569  -0.01476775
 -0.08770819  0.00919161  0.15649813  0.43262672  0.23848787  0.05580937
 -0.6013596   0.1040168  -0.1242391   0.15581843  0.0096285  -0.3887028
 -0.2782067  -0.0933326   0.00463276  0.03970768 -0.00470072 -0.3651573
  0.21023238  0.12909824 -0.242183    0.39433992  0.06964049  0.20773852
 -0.02964092  0.05918568 -0.29992184  0.24626611 -0.13159187 -0.12555087
  0.19500431 -0.01259174  0.20993985 -0.38476893 -0.01984626  0.09713068
  0.2806316   0.00838396  0.19669104 -0.21300833 -0.4276687  -0.09840215
 -0.3514059   0.1987803  -0.03514776 -0.02444982  0.25892276 -0.03799086
  0.06480125  0.20708962  0.30757207 -0.39374042  0.10148309  0.0727371
  0.22489208  0.36032122  0.04649151 -0.04180934 -0.06808902 -0.01437275
  0.16254646 -0.02100478 -0.37997347 -0.07184981 -0.24637677 -0.20848788
  0.1355559   0.23171598 -0.21167234 -0.26936832 -0.04104495 -0.03775874
  0.3742341   0.19034967  0.01181944  0.16430843  0.33380598 -0.12596607
  0.26564002  0.03848121 -0.13673267 -0.5168816  -0.31050727  0.06144312
  0.13413064 -0.59424144 -0.16626996 -0.15582928 -0.34293526  0.00192653
  0.01821052  0.04348522  0.01634888 -0.0210781  -0.1237824  -0.21207441
 -0.04131985 -0.20847617 -0.26455483  0.0246491  -0.1764617   0.26837245
 -0.03256904  0.14625654 -0.15967286 -0.06129521  0.33164626 -0.01984873
  0.22922899 -0.36928087 -0.03472253 -0.34714687 -0.19790336 -0.14386362
 -0.05078157 -0.08486269 -0.27695912 -0.35276693 -0.03336984 -0.28053018
  0.18063161  0.08406488 -0.14738737  0.3446792  -0.3108451   0.0550398
  0.25819206  0.26199263  0.32244742  0.2889263   0.11656299 -0.23373348
  0.04234997  0.1762129   0.06878198  0.3212297   0.02373748  0.79845273
  0.00178817  0.01066806 -0.13774765  0.31179523 -0.44028285  0.06547095
  0.12434004 -0.13403213  0.47836095 -0.20815462  0.39895284 -0.1072994
  0.39398354  0.01957852  0.0545784   0.1344977   0.2176553   0.2991407
 -0.29091957  0.19566157  0.25112495  0.01751461 -0.07936278 -0.35241455
 -0.00339973 -0.05996117  0.00774141  0.05107472  0.2350375   0.02405108
 -0.17751403 -0.01981396  0.19618724 -0.2647074   0.09701049 -0.25606197
 -0.10941144  0.2523518   0.09292805 -0.08761691  0.16964072 -0.09431159
  0.24047667 -0.06972799 -0.05338707 -0.47683436  0.15649487  0.08145582
 -0.02924342  0.09479506 -0.08315517  0.3425851   0.07358928  0.16517313
  0.04102476 -0.06939692  0.09461511 -0.00835805 -0.37134618 -0.09944572
  0.07753564  0.26391053  0.04001587 -0.2810822   0.12250745 -0.2263947
 -0.17907023 -0.06981373 -0.22535296 -0.08527319  0.05148817 -0.20497715
  0.09951684  0.08728137  0.0066805  -0.10437247 -0.07727171 -0.25571537
 -0.19314265 -0.36644024 -0.00280836 -0.24673803  0.04542813  0.3107559
  0.09284673  0.1715881   0.04882743 -0.05764646  0.16528371 -0.05821344
 -0.19144164  0.39902076 -0.01289095  0.03566716  0.03829182  0.48703012
 -0.07430606  0.30342352 -0.4883099  -0.21166447 -0.04473475 -0.12201173
 -0.12687927  0.0079223   0.14829749  0.23506752 -0.1453039   0.34450692
 -0.2871654  -0.20437089  0.03368706  0.00402695  0.30258036  0.00407863
  0.22294585 -0.18941     0.02407    -0.05943808  0.1849538   0.17267083]"
"tensorflow-macos 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible. stat:awaiting response type:feature type:build/install stale subtype:macOS TF 2.13","### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

MacOS 13

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I can't use pydantic because it needs typing-extensions>=4.6.1 but tensorflow-macos doesn't support typing-extensions>4.6.0

please update tensorflow-macos for newer typing-extensions

pydantic-core 2.6.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.
pydantic 2.3.0 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.

tensorflow-macos 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.

Thanks

### Standalone code to reproduce the issue

```shell
just install the pip packages in MacOS terminal.
```


### Relevant log output

_No response_",False,"[-4.74467069e-01 -3.78284335e-01  6.18981943e-02 -9.92468596e-02
  1.92164302e-01 -2.69057751e-01 -1.40108675e-01 -5.12621775e-02
 -2.88866460e-01 -4.01026994e-01  1.00819707e-01 -2.00173743e-02
 -2.72613794e-01  2.21207917e-01  1.42936548e-02  3.27945590e-01
 -3.48211408e-01  3.52011286e-02  1.31846964e-01  1.01830937e-01
 -1.20648704e-01 -1.04645692e-01 -2.28921011e-01  3.08656543e-02
  4.17411514e-02  3.16323459e-01 -3.33125554e-02  6.12524934e-02
 -2.72691250e-03  2.06358954e-01  4.07620490e-01  2.43038118e-01
  7.10062869e-03  9.87604409e-02  8.71690586e-02  1.60668597e-01
 -1.65897131e-01 -2.40877688e-01 -3.39262128e-01 -1.54503770e-02
 -1.17144831e-01 -1.32186972e-02  1.18358985e-01 -4.03086841e-02
 -5.86019717e-02 -2.31902957e-01 -3.23274806e-02 -1.39564052e-01
  1.44644842e-01 -5.62879145e-02  2.45189965e-02 -1.11937433e-01
 -2.53620803e-01 -2.82562256e-01 -1.61960751e-01  1.81228876e-01
 -8.58681276e-02  7.36055449e-02  1.25127077e-01  1.96315467e-01
  2.21939653e-01  6.02357537e-02  6.07986115e-02  1.80307627e-01
  6.99337050e-02  9.75628942e-02  3.18501085e-01 -1.10644080e-01
  4.97672260e-01 -1.66278973e-01  3.30504626e-02  5.29971085e-02
 -3.70928884e-01  1.58219889e-01  5.13853058e-02  1.78139254e-01
 -1.39070868e-01 -3.01173646e-02  2.20130876e-01  2.90753786e-02
 -6.46456033e-02 -1.24788411e-01  1.16000324e-01  2.46682670e-02
  1.84085175e-01  1.01149697e-02  1.72087863e-01 -3.36894132e-02
  2.04147488e-01 -9.12984386e-02  4.45824981e-01  1.55615002e-01
  9.71757472e-02 -9.29806083e-02  5.53883731e-01 -4.76808064e-02
  6.61739260e-02  3.15142155e-01 -1.83247663e-02 -1.03987716e-01
 -1.43676057e-01 -1.49213105e-01  1.05938524e-01  1.15683720e-01
  3.89562175e-02 -2.58007437e-01  2.52642214e-01 -2.04080269e-01
  1.00282304e-01  9.20540094e-02  2.13655591e-01 -3.43813598e-02
  2.68415451e-01  2.80086547e-02 -1.12513557e-01 -1.23120941e-01
 -2.84414709e-01  1.02083832e-01  5.96654117e-02  8.76696765e-01
  1.24317192e-01 -2.42574006e-01 -2.75460184e-02  2.79970706e-01
  3.83180499e-01  2.03252025e-02 -4.71359074e-01  2.76785903e-02
  9.43207145e-02 -2.48674341e-02  1.56885251e-01  1.55822158e-01
  1.23263136e-01  6.68481588e-02 -2.69882418e-02 -1.04511477e-01
 -3.07168305e-01 -2.47448742e-01 -4.28328276e-01  2.07018666e-02
 -2.27910250e-01  1.35583714e-01 -6.14118688e-02 -5.37116468e-01
  2.89841115e-01  2.30986223e-01 -1.83182940e-01  2.23552108e-01
 -1.34424508e-01 -2.12533861e-01  2.01266240e-02  5.31696342e-02
  3.08680162e-03  3.73053610e-01  3.60163391e-01  1.72613293e-01
  4.97203112e-01 -4.31477278e-02 -2.74158269e-01 -7.10837841e-01
  1.26556993e-01  4.81260628e-01 -1.33532539e-01  2.45623849e-03
  3.78889799e-01  6.51008263e-03 -3.25381964e-01 -6.58104643e-02
 -1.90385543e-02  3.67045134e-01 -1.48338318e-01 -3.50768492e-02
  3.52837406e-02 -1.81336589e-02  3.02731991e-01 -1.21515244e-01
  3.27647001e-01 -7.00172067e-01 -4.17653471e-02  1.65121496e-01
  1.08336002e-01 -2.64606271e-02  1.22034959e-02  4.15465003e-03
 -9.85014737e-02 -1.23209748e-02  7.81304687e-02  1.65236399e-01
 -2.90093601e-01  2.20107973e-01 -4.50777024e-01  3.25965807e-02
  3.56428504e-01 -8.02560747e-02  6.98155817e-03  1.93207897e-03
  6.70115948e-02  2.08935551e-02 -1.64080262e-01  2.49337852e-01
 -2.05563381e-01 -7.63092339e-02 -1.47288635e-01 -1.76101044e-01
  1.59850448e-01 -3.21079671e-01 -2.52290126e-02 -3.62809598e-01
 -4.68242466e-01 -4.17522490e-02  1.50591403e-01 -2.15488270e-01
  4.27854508e-02  1.02025308e-02 -3.89965594e-01  3.96838449e-02
  3.79053354e-02  1.00811772e-01 -2.76908636e-01  2.14799598e-01
 -1.62356198e-01 -3.58199626e-01 -2.37860143e-01 -2.33594060e-01
 -3.29374373e-01 -4.96530756e-02 -3.67338657e-01  5.18434867e-02
 -9.72860903e-02  3.77192140e-01  3.96103188e-02 -1.05152521e-02
  4.75098610e-01 -9.27719772e-02  2.42510736e-01 -2.00944811e-01
  4.07751799e-02 -3.03498775e-01 -1.83130026e-01  1.06425002e-01
 -6.18755877e-01  7.27024972e-02 -5.77608421e-02 -1.67058602e-01
  3.21306080e-01  1.39681309e-01 -2.30033137e-02 -5.38450591e-02
 -1.20801866e-01  1.97059616e-01 -2.20055759e-01  1.89969316e-04
  3.78613949e-01  2.92842507e-01  1.98477447e-01  2.30191350e-01
  5.29227257e-02 -2.47988105e-02  1.70321301e-01 -6.35287091e-02
  1.47391111e-01  2.51912832e-01  3.10906470e-01  4.03201103e-01
  1.67821467e-01  2.91551977e-01 -1.84607923e-01  2.73409873e-01
  6.62911534e-02 -1.77355148e-02 -1.09346490e-03 -7.55123943e-02
  5.03129542e-01 -6.32987618e-01 -1.79919824e-02  4.36503403e-02
  2.12002814e-01 -1.47998929e-01 -1.82681888e-01  1.27888843e-02
 -7.39218742e-02  3.64313841e-01 -5.79862118e-01  3.49448696e-02
 -6.80165179e-03 -2.81349957e-01  3.41263339e-02 -5.61983109e-01
 -5.13759255e-03  8.05312693e-02 -1.73340797e-01 -5.92005290e-02
 -1.28557369e-01  8.82125646e-02 -2.03088075e-01  9.49095339e-02
  5.63335530e-02 -1.11233458e-01 -1.37551166e-02  1.23149291e-01
 -1.30621493e-01  7.37130344e-02  2.89370030e-01 -3.51953387e-01
 -4.35359403e-02  1.02212466e-03  2.95048177e-01  6.79560602e-02
  4.34921414e-01 -1.87675416e-01  3.06917012e-01  2.21398830e-01
 -2.29197234e-01  5.16004860e-01  7.86177516e-02  3.64461392e-02
 -6.20198488e-01  6.42059088e-01  8.07181466e-03 -4.56488878e-02
  1.17817193e-01 -1.91980705e-01 -3.18159133e-01 -7.92641491e-02
  2.82844305e-01  1.65173821e-02 -3.80636342e-02 -2.82267243e-01
  1.77094266e-01  1.94767490e-02  2.48826854e-02 -3.51001583e-02
 -6.75065592e-02  2.04407722e-01 -7.35383928e-02  7.66385198e-02
 -2.88234651e-01  2.02977687e-01 -7.83335567e-02 -2.71739244e-01
 -1.28982723e-01 -7.30347037e-02 -1.61972433e-01 -2.60501415e-01
  9.98537540e-02 -2.81116724e-01  2.32550323e-01  4.88528162e-01
 -1.83189631e-01  2.66280979e-01 -2.07062885e-02  2.51437306e-01
 -4.96100813e-01 -4.16756943e-02 -6.44100979e-02  2.76392043e-01
  4.50913422e-03 -2.27336138e-01  1.06265053e-01  2.61565715e-01
 -3.26070070e-01  1.02608308e-01 -4.26067829e-01  6.47781044e-02
  2.11089514e-02 -2.13351369e-01 -1.26690209e-01 -6.52649552e-02
  8.14678520e-02  4.08897161e-01 -7.55280703e-02  7.95891508e-02
 -3.08155209e-01  2.77805805e-01  4.51243192e-01 -2.54803836e-01
 -2.12602094e-01  2.36359192e-03  1.09807178e-01 -3.35173495e-02
  1.40589811e-02 -6.81918487e-03  2.06313998e-01  2.80702710e-02]"
Build fail on TFLite C API for Android with CMake stat:awaiting tensorflower type:build/install comp:lite awaiting PR merge TF2.14,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14.0-rc1

### Custom code

No

### OS platform and distribution

macOS 13.5.2

### Mobile device

Android

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On version 2.14.0-rc1, Building TFLite C API for Android target with CMake fails by link error.
It works fine on version 2.13.0.

`ld: error: unable to find library -lpthread`
Error is caused by linking `pthread`. I guess pthread is unnecessary for android.

#### Extra information to my environment
- CMake: 3.26.4
- Android NDK: 25.2.9519653(r25c)

### Standalone code to reproduce the issue

```shell
mkdir tf-build
cd tf-build
cmake -DCMAKE_TOOLCHAIN_FILE=<NDK path>/build/cmake/android.toolchain.cmake -DANDROID_ABI=arm64-v8a ../tensorflow/lite/c
cmake --build . -j
```


### Relevant log output

```shell
...
[ 70%] Built target absl_status
[ 75%] Built target absl_flags
[ 95%] Built target tensorflow-lite
[100%] Linking CXX shared library libtensorflowlite_c.so
ld: error: unable to find library -lpthread
clang++: error: linker command failed with exit code 1 (use -v to see invocation)
make[2]: *** [libtensorflowlite_c.so] Error 1
make[1]: *** [CMakeFiles/tensorflowlite_c.dir/all] Error 2
make: *** [all] Error 2
```
",False,"[-3.89303446e-01 -3.35754305e-01 -1.44705344e-02 -5.84514886e-02
  3.33168745e-01 -3.59157711e-01 -2.46916205e-01  1.69959098e-01
 -4.11535531e-01 -3.00543070e-01  4.69579697e-02 -1.63787842e-01
 -2.01570034e-01  7.42445216e-02 -5.74136078e-02  4.02643174e-01
 -2.86557525e-01 -1.12923414e-01  3.38144481e-01  7.51839057e-02
 -1.74567461e-01  4.36553247e-02 -2.29402527e-01  1.76527739e-01
  2.10631162e-01  3.10839295e-01 -2.81144768e-01  3.93229239e-02
  4.11447138e-02  3.23137760e-01  4.52713877e-01 -1.34051582e-02
 -1.83396369e-01  2.18255222e-01  7.70591050e-02  1.52711481e-01
 -5.29300034e-01 -2.25535780e-02 -2.25020081e-01 -2.03187130e-02
 -6.21705130e-02  1.42828360e-01  1.14588626e-01 -1.21669888e-01
 -1.70795590e-01  3.30278277e-02 -3.50017287e-02  1.42307013e-01
 -1.95507079e-01 -1.60665870e-01  1.05137080e-01 -1.75265878e-01
 -3.76701534e-01 -4.35381860e-01 -6.69529438e-02  1.87158793e-01
 -1.35462508e-01  9.13607515e-03  2.39113301e-01  1.75416455e-01
  3.54050010e-01 -3.08226198e-02 -2.78417952e-02 -5.54499254e-02
  5.55597693e-02  1.89141124e-01  8.30083191e-02 -1.11073650e-01
  4.15118992e-01 -3.10302287e-01  1.40330061e-01 -8.55143666e-02
 -1.01934642e-01 -2.80778594e-02 -4.05059233e-02  2.20084116e-01
 -4.82492372e-02  3.33045632e-01  9.17728990e-02 -2.14045465e-01
 -1.01806875e-02 -1.75458640e-01 -1.95058048e-01  1.36422053e-01
  3.65434647e-01 -1.72087148e-01  3.11026216e-01  7.24281818e-02
  4.47133005e-01 -1.40775725e-01  3.20175648e-01  8.13474298e-01
  2.27356255e-02  2.38227844e-01  2.88608253e-01  2.94169903e-01
  2.92464010e-02  1.26236960e-01  4.81945351e-02  4.53379899e-02
 -9.25446749e-02 -2.98634261e-01 -2.74296626e-02  3.77479829e-02
  1.69679895e-02 -4.58964646e-01  1.97186381e-01  1.17084701e-02
  1.45488739e-01 -5.57559431e-02  2.31719792e-01  6.44164607e-02
  1.61081269e-01 -2.11178325e-02 -1.48142213e-02  1.24629691e-01
  6.16859943e-02  1.23765066e-01  4.70898375e-02  5.81550896e-01
  2.49242596e-02 -5.30190170e-02 -2.14582831e-01 -1.53457135e-01
  5.09535253e-01  3.25037464e-02 -2.62828439e-01 -9.07452703e-02
  2.24773511e-01  2.10098356e-01 -1.04425400e-01  1.90717131e-01
 -6.79016858e-02  1.30666852e-01 -2.52952017e-02  2.80630291e-01
 -1.55782998e-01 -2.58131057e-01 -1.56894326e-01 -2.95872509e-01
 -3.62395465e-01  3.23377371e-01 -1.75398011e-02 -7.30820298e-01
  2.34398991e-01 -3.56012285e-02 -1.72380388e-01  1.61100745e-01
 -4.09320116e-01 -1.64274096e-01 -1.45513654e-01  6.37631342e-02
 -1.80906743e-01  5.28045774e-01  4.60215658e-01 -1.15306742e-01
  3.79449368e-01  3.47396284e-02  1.58273607e-01 -4.12506729e-01
 -4.99297306e-02  4.14212048e-01 -1.83357894e-01 -1.10962600e-01
 -1.51725918e-01  1.74287558e-01 -4.77417618e-01 -6.99664652e-02
  8.04598555e-02  2.91910768e-01 -7.57488087e-02 -3.06702722e-02
  1.54391661e-01  8.73900652e-02  4.70748078e-03 -2.04129457e-01
  5.04798830e-01 -5.24971128e-01  1.53790280e-01  7.83235431e-02
  6.28311634e-02  1.15943342e-01  2.86766648e-01  4.65529300e-02
 -9.71112326e-02  3.44997793e-02  7.83477724e-02  1.40890181e-01
 -1.54433340e-01 -1.12009831e-01 -5.62192976e-01  1.16129946e-02
  2.85932720e-01  6.09656349e-02 -2.02220649e-01  3.65547091e-02
  1.74897522e-01 -3.09161961e-01  1.76869184e-01  3.42905261e-02
  9.33625028e-02  2.34894902e-02 -8.52497518e-02 -2.61654574e-02
  7.93671086e-02 -2.81264424e-01 -2.08878554e-02 -3.71393383e-01
 -4.05347347e-01 -1.62998483e-01  1.94026351e-01 -4.47036415e-01
  5.86905181e-02 -2.87845016e-01 -1.20272309e-01  2.60401249e-01
 -3.80512164e-03 -5.97579554e-02 -2.70412207e-01  2.13231325e-01
 -2.81885080e-02 -1.93840146e-01 -1.05106011e-01 -3.82487535e-01
 -3.12836409e-01  1.29152343e-01 -2.84175158e-01  3.77380699e-02
 -2.64035445e-02  2.17057437e-01 -2.56795734e-01 -5.08385375e-02
  3.78379285e-01  3.28892730e-02  3.96501094e-01 -2.06785426e-01
 -3.00013572e-02 -2.80175149e-01 -3.68731618e-01  2.28936523e-02
 -2.15892836e-01 -1.56555772e-01 -4.82151508e-02 -1.82645440e-01
  3.66608560e-01  3.93370688e-01 -3.71959955e-02  1.02310199e-02
 -2.68349409e-01  2.95801729e-01 -3.12635005e-02  1.27261907e-01
  2.25735784e-01  1.76468819e-01  5.54211140e-01  2.39594638e-01
  1.63350910e-01  2.50417888e-01  1.22643411e-01 -4.24176902e-02
  2.39947900e-01  1.37360424e-01  2.11272687e-01  4.09363329e-01
  1.85525328e-01  5.47900237e-02 -1.98626816e-01  2.48690933e-01
 -4.59225029e-02 -2.91139722e-01  2.45975047e-01 -1.84984505e-01
  4.47162300e-01 -3.52337807e-01 -4.99418452e-02 -4.55069467e-02
  2.32966825e-01 -7.14221895e-02 -3.94919589e-02  1.05628163e-01
  1.38194770e-01  2.74022460e-01 -2.80205846e-01 -9.25956070e-02
  1.33279860e-01 -1.54141322e-01  7.51580894e-02 -5.15476882e-01
 -4.00187790e-01  2.27846056e-01 -2.67398089e-01  2.19226003e-01
 -1.31126180e-01  2.40126222e-01 -2.69713521e-01  1.29631698e-01
  2.45444104e-02 -2.20929399e-01  1.48216814e-01  1.80844501e-01
 -4.83750403e-02  2.80010641e-01  2.66091496e-01 -4.57309604e-01
 -1.12882525e-01  1.01014450e-01  2.48475537e-01 -9.75562856e-02
  6.39364958e-01 -5.71665764e-01  3.19863819e-02 -9.62653756e-02
 -1.81640238e-01  6.32462680e-01 -1.17649272e-01  1.09274358e-01
 -3.84721816e-01  5.76332867e-01  2.39271149e-01 -1.27101243e-01
  3.78817677e-01 -1.19797722e-01 -3.52997243e-01 -2.07289070e-01
  4.15789813e-01  2.34573092e-02 -1.37580745e-02 -2.64865100e-01
  2.44065985e-01  6.10405356e-02 -1.94144681e-01  3.44470143e-04
 -9.62097123e-02  2.47953534e-01 -3.50756645e-01 -1.96103886e-01
 -2.25980997e-01  9.50155556e-02 -2.14792788e-01 -2.84774512e-01
 -3.17936897e-01 -2.45387867e-01 -1.08561978e-01 -1.40026659e-01
  7.74164796e-02 -1.98632061e-01  2.55126178e-01  2.82833993e-01
 -1.40856802e-01  1.58679709e-01 -1.25047088e-01  1.04513831e-01
 -3.47590446e-01 -6.18081838e-02 -3.81882749e-02  3.29917550e-01
 -1.16466627e-01 -1.52374625e-01  4.31861997e-01  3.78575563e-01
 -5.32848984e-02  1.93121433e-01 -3.82306278e-01 -9.10586417e-02
  1.16950408e-01 -2.59916395e-01 -4.28711176e-01 -5.37517816e-02
  5.52171916e-02  5.11629045e-01 -9.40187573e-02  3.67117405e-01
 -3.15967232e-01  5.22369109e-02  4.44893837e-01 -2.32873499e-01
 -5.15118912e-02  2.73613632e-01  1.11914799e-03 -1.98881775e-01
 -9.39336866e-02 -4.45971936e-02  1.30335361e-01 -4.02853899e-02]"
Model train failing stat:awaiting response comp:apis type:performance TF 2.4,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.4.1

### Custom code

No

### OS platform and distribution

Windows 11 WSL Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.9.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Cudatoolkit 10.1.234 Cudann 7.6.5

### GPU model and memory

NVIDIA GeForce RTX 4080 deviceMemorySize: 15.99GiB

### Current behavior?

Running : jupyter notebook: TensorFlow 2 quickstart for experts
https://www.tensorflow.org/tutorials/quickstart/advanced

2023-09-08 23:55:01.853625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 4080 computeCapability: 8.9
coreClock: 2.505GHz coreCount: 76 deviceMemorySize: 15.99GiB deviceMemoryBandwidth: 667.63GiB/s


TRAINING THE MODEL:

2023-09-09 14:21:13.106797: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-09-09 14:21:13.108589: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3417595000 Hz
2023-09-09 14:21:13.135568: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2023-09-09 14:22:05.711952: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7

NOT LEARNING!!!
Epoch 14, Loss: 2.3014447689056396, Accuracy: 11.233333587646484, Test Loss: 2.3011932373046875, Test Accuracy: 11.34000015258789
Epoch 15, Loss: 2.301426410675049, Accuracy: 11.236666679382324, Test Loss: 2.3012006282806396, Test Accuracy: 11.329999923706055



### Standalone code to reproduce the issue

```shell
https://www.tensorflow.org/tutorials/quickstart/advanced
```


### Relevant log output

_No response_",False,"[-4.01720464e-01 -4.56735909e-01 -1.50714844e-01 -5.75343557e-02
  1.51075482e-01 -8.65357965e-02 -2.63084710e-01  2.17182845e-01
 -3.63785237e-01 -1.79842979e-01  2.85078168e-01  7.08508305e-03
 -1.34245545e-01  4.17407043e-03 -1.53005481e-01  2.20066041e-01
  1.15367532e-01  1.08090632e-01  1.97701871e-01  2.32629851e-01
 -2.45911181e-01 -4.42927152e-01 -2.47441530e-01  1.87626407e-01
  9.58429873e-02  1.34247303e-01 -3.82884800e-01  9.23691988e-02
 -1.23435594e-01  1.81626618e-01 -7.31776468e-04 -4.45227958e-02
  3.43499184e-02  6.31948113e-02  4.66317460e-02  2.82203436e-01
 -1.20045245e-01 -3.92157674e-01 -3.68279755e-01  1.40772849e-01
 -1.61846101e-01 -2.11032838e-01  6.19009025e-02 -2.40358442e-01
  1.84593275e-01 -1.11350149e-01  2.41199322e-03 -2.61455715e-01
 -2.41623089e-01 -1.08342387e-01 -1.11063659e-01  6.02544434e-02
 -3.04480523e-01 -4.50675666e-01 -1.94188267e-01  5.20495176e-02
  2.13510752e-01 -2.66636834e-02 -2.66800337e-02  1.34245247e-01
 -9.08977762e-02  1.79417372e-01 -5.65562472e-02 -2.98561215e-01
  2.74864316e-01  2.26798877e-01  2.43718445e-01  5.30107990e-02
  5.58235765e-01 -1.28874213e-01 -7.33967032e-03 -1.57417506e-01
 -3.39767724e-01  3.33556682e-02  3.68746109e-02  1.71695706e-02
  2.61365116e-01  1.12257838e-01  4.08685237e-01 -1.27595767e-01
 -2.44782493e-03 -5.58580697e-01 -4.47697639e-01 -1.80478662e-01
  1.41473144e-01 -2.50919223e-01  4.49452043e-01 -7.54290214e-03
  6.33532465e-01 -3.17495614e-01  2.90582657e-01  4.12831426e-01
 -2.79180527e-01  2.50523686e-01  4.68021095e-01  3.11722785e-01
 -6.33785352e-02  2.31919035e-01 -5.32599948e-02 -2.48731486e-02
 -4.26331274e-02 -2.22336307e-01  4.15950119e-02  1.38636068e-01
 -2.37056106e-01 -4.73991513e-01  2.48134181e-01 -1.46404440e-02
  1.51118517e-01  1.44341141e-02  1.66848823e-01 -9.14338790e-03
  6.86735846e-03 -1.93962026e-02 -1.10285103e-01 -8.84737074e-02
 -5.91389239e-02  5.65361492e-02 -2.90851854e-02  7.32057929e-01
  3.45726579e-01 -3.42320204e-01  2.89058059e-01 -1.78574678e-02
  4.47027683e-01 -7.80078918e-02 -6.11003414e-02  3.63845564e-02
 -7.90694356e-02 -6.06481880e-02  3.90699267e-01  8.35648775e-02
 -6.97956830e-02  5.25995493e-02 -3.35483402e-01  5.85574955e-02
  1.72192365e-01 -4.61512096e-02 -5.37418604e-01 -1.68662384e-01
 -4.51577604e-01  2.28966445e-01  5.57710975e-03 -6.27512336e-01
  3.17884535e-02  1.84132323e-01 -2.11188227e-01  4.41694856e-01
  4.71064821e-02  2.67785843e-02  1.27891064e-01 -1.00457951e-01
 -3.12657356e-02  4.41011846e-01  1.61131620e-01  7.99286515e-02
  2.85797298e-01 -1.47865787e-02  1.11412175e-01 -6.06726289e-01
 -8.68872404e-02  3.38895500e-01 -3.39272842e-02 -3.94278288e-01
  2.41613075e-01  2.23196775e-01 -2.19013467e-01 -1.28189698e-01
  1.36071548e-01  6.38998270e-01 -5.97314462e-02 -1.14444479e-01
  6.48980290e-02  1.03798620e-01  2.18274847e-01 -1.80085659e-01
  5.66252321e-02 -5.14687836e-01 -4.16733846e-02  3.14071238e-01
  2.09738731e-01  8.84126723e-02  2.92847812e-01  2.32573912e-01
 -9.27543454e-03  6.82079196e-02  1.35558248e-01  9.01795402e-02
 -1.91052064e-01 -1.38871580e-01 -4.54041123e-01 -3.08296204e-01
  5.08005798e-01 -1.81435332e-01 -2.63544828e-01 -5.84658384e-02
  1.80688694e-01 -4.99859154e-02  7.46048689e-02 -4.34145331e-02
 -3.66081223e-02 -2.85675079e-02 -2.64643542e-02 -1.56602055e-01
  2.97850072e-01 -1.04233444e-01 -1.78610176e-01 -5.19968867e-01
 -2.94805586e-01  7.79083222e-02 -2.13117033e-01 -6.33579969e-01
  3.00831109e-01  2.37003863e-01 -1.35329962e-01 -3.67408618e-03
 -2.41210386e-02  7.69799054e-02 -1.65902615e-01  1.53583393e-01
 -1.17805958e-01 -1.06766291e-01  4.52810153e-02 -4.40720320e-01
 -2.55531043e-01  2.96247363e-01 -2.55029738e-01 -2.55924929e-02
  2.06909806e-01 -1.05274916e-02  2.76428819e-01  6.41090721e-02
  3.38651955e-01  4.63805944e-01  4.61956859e-01 -3.04279149e-01
 -2.58330196e-01  6.11821190e-02 -2.10925579e-01 -1.31420106e-01
 -2.55604625e-01 -2.25770652e-01 -5.94876707e-02 -1.16321914e-01
  4.38422859e-01  6.11965775e-01 -4.14798647e-01  4.63030525e-02
 -2.57859945e-01  4.02979493e-01 -4.42476928e-01  2.19295725e-01
  1.49536759e-01  1.43812541e-02  4.73407388e-01  1.88304126e-01
  3.83295119e-01  2.86243349e-01  3.69188398e-01 -2.72911265e-02
  3.42829049e-01  4.06313747e-01  2.56701171e-01  6.27166331e-02
  5.07197440e-01  2.62896270e-01 -4.53168273e-01  4.59289074e-01
 -1.17229950e-02  1.19494293e-02  2.79025704e-01 -5.93774676e-01
  4.56897259e-01 -3.66130739e-01  3.09963584e-01 -1.42363101e-01
  1.64244056e-01 -3.38914916e-02 -2.56624252e-01 -9.67907012e-02
  1.33025214e-01  1.84138551e-01 -4.56255555e-01  1.87143032e-02
 -1.12777941e-01 -2.19492555e-01  5.21029998e-03 -5.83534241e-01
 -2.05819786e-01 -1.40867848e-02 -2.10596219e-01  8.01030397e-02
  8.46514404e-02 -1.01477653e-01 -3.06294739e-01  1.45780385e-01
  5.98369129e-02  1.19764037e-01  2.45393217e-02  1.53306574e-02
 -5.15282035e-01  4.47925255e-02  5.71900308e-01 -4.59301233e-01
 -2.06889957e-01 -3.06104898e-01  5.12567639e-01  2.14187384e-01
  3.57814193e-01 -2.89148808e-01  4.85741496e-02  6.80651963e-02
 -8.50729048e-02  3.26160192e-01  1.41885892e-01  1.40534714e-01
 -4.05271232e-01  8.25269759e-01 -2.33559936e-01 -6.82960972e-02
  2.16266841e-01 -2.58199535e-02 -3.05416614e-01  5.15650958e-02
 -2.51500197e-02  5.83407730e-02 -2.97655910e-02 -3.75280738e-01
  2.05964223e-01  7.48222172e-02  1.44535750e-02  2.90038735e-02
 -4.20769118e-02  1.09821454e-01 -1.00753658e-01 -7.40864202e-02
 -3.88223052e-01  5.16092539e-01  8.77274424e-02 -4.50411916e-01
 -6.31351098e-02 -1.21884987e-01 -2.79558063e-01 -2.88722456e-01
  9.23744962e-02 -2.89548367e-01  1.87621489e-01  5.00546694e-01
 -1.30149633e-01  2.18136668e-01  1.11893550e-01  1.78481027e-01
 -2.33862072e-01  3.15556349e-03 -9.69373435e-02  5.51075161e-01
  2.07024336e-01 -1.12083405e-01  3.38181376e-01  2.96963811e-01
  4.39003818e-02  2.16505781e-01 -4.59344476e-01  1.24896929e-01
  1.80463016e-01 -1.65824890e-01 -2.28025049e-01 -5.23884237e-01
  2.34193295e-01  2.43942440e-01 -1.61793500e-01  1.67381555e-01
 -3.06420803e-01  8.50816369e-02  6.17844045e-01 -4.29774165e-01
 -1.59876332e-01 -5.01929782e-02  2.56776810e-01 -7.92062562e-03
  1.57088742e-01 -3.45182151e-01  9.55358297e-02  4.21796739e-02]"
Tersorflow deterministic training not working for different workflows even when GPU and envs are same stat:awaiting response stale type:performance comp:core TF 2.5,"### System information

- Ubuntu 20.04
- Tensorflow version :-  2.5.0
- python version :- 3.7
- GPU model and memory :- Nvidia RTX 4090, 24 GB GPU memory


### Describe the problem
We recently started setting environmental variable for enabling deterministic training on tensorflow version 2.5. We have 5 to 6 models, most of which are regression models and one of them is a classification model. The following seed values were set :- 
   ```
    os.environ[""PYTHONHASHSEED""] = str(seed)
    
    import tensorflow as tf
    import random
    import numpy as np
    
    # set seed for random, numpy and tensorflow
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

    # set env for determinism
    os.environ[""TF_DETERMINISTIC_OPS""] = ""1""
    os.environ[""TF_CUDNN_DETERMINISTIC""] = ""1""
   
    # to ensure determinism set threads to one
    tf.config.threading.set_inter_op_parallelism_threads(1)
    tf.config.threading.set_intra_op_parallelism_threads(1)
``````

 We created multiple workflows by keeping the same GPU version and all of the above environmental variables and seed values. But, for different workflows we get different results even though the initial weights for all models on both workflows are same. On the other hand, on training in the same workflow, we get similar results. kindly let us know what other seed values do we need to set and let us know a solution so that we can train the models in a deterministic way


### Source code / logs

Here is the code to compare initialised weights of model in different workflow but when performing inference after training for 70 epochs the results vary a lot

```
import tensorflow as tf
import numpy as np


tf_det_ini = tf.keras.models.load_model(""./tf_det/nobrainer_initial.h5"")
wb_diff_ini = tf.keras.models.load_model(""./wb_diff/nobrainer_initial.h5"")

diff_arr = []

cnt = 0
for tf_m, wb in zip(tf_det_ini.layers[1:], wb_diff_ini.layers[1:]):
    if tf_m.name == 'MobilenetV3large':
        tf_m_mob = tf_m.layers
        wb_mob = wb.layers

        for tf_m_mobl, wb_mob_l in zip(tf_m_mob[2:], wb_mob[2:]):
            try:
                diff = np.unique(tf_m_mobl.weights[0].numpy() - wb_mob_l.weights[0].numpy())
                diff_arr.extend(diff)
            except:
                continue
            # print(tf_m_mobl.weights[0].numpy())

    try:
        print(tf_m.name)
        diff = tf_m.weights[0].numpy() - wb.weights[0].numpy()
        diff_arr.extend(np.unique(diff))
    except:
        continue

print(max(diff_arr))
print(min(diff_arr))
```

output :- 

```
MobilenetV3large
global_average_pooling2d
extradata
concatenate
dense
0.0
0.0
```",False,"[-0.03987938 -0.35343757 -0.23938027  0.00451666  0.5202655  -0.01846838
  0.06745793 -0.02198909 -0.42322105 -0.1954777  -0.13041632  0.1875726
 -0.18086745  0.21228062 -0.14927316 -0.06024051  0.0156626  -0.30360156
  0.13864204 -0.07162334 -0.06786557 -0.30665165 -0.16051483  0.13308154
  0.13632023  0.08582278 -0.17570004 -0.16257565  0.16166644  0.25499415
  0.01060414 -0.01897939  0.15157123 -0.03289409 -0.2608508  -0.05076049
 -0.2685691  -0.08914378 -0.31851423  0.05512426  0.09590529 -0.00613882
 -0.14930803  0.1232505  -0.06978156 -0.03553207 -0.08036456  0.28487954
 -0.13618097 -0.2637933  -0.24534146  0.23758659 -0.1352246  -0.32022998
  0.02229022 -0.13413435  0.17954049  0.00666482 -0.23869765  0.09409506
  0.02967332 -0.04824944 -0.06461647 -0.12639904  0.03189251  0.4554655
  0.1840353   0.185369    0.62747884  0.08114184  0.1747854   0.02042964
 -0.49690163  0.07461308 -0.02967249  0.05100424 -0.02069502  0.23763222
  0.265212   -0.2470876  -0.19416738 -0.26008123 -0.11346107 -0.13133052
  0.085135    0.03813321  0.24704581  0.25780547  0.43215972 -0.11113627
  0.41391015  0.21528624 -0.23008686  0.04066308  0.548859    0.27784514
  0.13229361  0.15761435 -0.24980456 -0.13532342 -0.173376   -0.06959881
  0.09723721  0.26125365 -0.05328128 -0.18537803  0.3159619   0.03423602
  0.22499263  0.01127988  0.22559911  0.07978772  0.02199059  0.2883331
 -0.13206498  0.16794692 -0.22504874  0.18753526  0.05186058  0.57760143
 -0.04767557  0.01221724  0.172471    0.15142137  0.22317004 -0.25388092
 -0.12796208 -0.05115611 -0.01009371 -0.08930118  0.29035264  0.12071861
 -0.29040027 -0.08132774  0.11870514 -0.16631934  0.03262535 -0.08225984
 -0.38102132 -0.09899273 -0.29216635  0.27208248 -0.05071063 -0.21514831
 -0.02330854  0.46041927 -0.23636734  0.10342142  0.11516988 -0.08187506
 -0.21052226 -0.14669202 -0.10565773  0.2793216   0.11168675  0.01169205
  0.23719928  0.00467409 -0.1257608  -0.44010907 -0.1662838   0.2848275
 -0.01805867 -0.21305212  0.2655238   0.31161994 -0.07160251 -0.30739486
  0.01132635  0.28821534  0.13943577 -0.40989667 -0.18151613 -0.22482535
  0.32581854 -0.16353543 -0.16654244 -0.40943626  0.05317389 -0.06895443
  0.07499216 -0.01843589 -0.0124627   0.135147    0.04732438  0.15130424
  0.1078113  -0.00946557 -0.27695063 -0.08651716 -0.29883367 -0.25224692
  0.2605759  -0.09378672 -0.2907137  -0.15466297  0.2967348   0.08698633
 -0.19675803  0.12998249 -0.05321083 -0.03161614  0.02542422  0.07238139
  0.1326874  -0.17969611 -0.3900281  -0.1956506  -0.24049862  0.15325512
 -0.05712005 -0.21325815  0.18745741 -0.1423994  -0.28566003 -0.13808246
 -0.08887444 -0.08107793  0.06271408  0.36313936  0.32704988 -0.0197159
 -0.12887996 -0.2307561  -0.1544298  -0.19066253  0.14579843  0.02514867
  0.15393473  0.29461023  0.17327283 -0.17008789  0.1464358   0.09194046
  0.3032233  -0.0160978  -0.09012514 -0.03646935 -0.16699234  0.03702027
 -0.1064712   0.16050524  0.1661053  -0.17185967  0.15825999  0.05008478
 -0.18543012 -0.24704435 -0.1270601   0.1508598  -0.2791756  -0.20827663
  0.25806883  0.14906989  0.31284     0.10452151 -0.17211393  0.07461584
  0.16173986 -0.11417472  0.1495919   0.25804937 -0.09211048  0.5135559
  0.45486856  0.12900883 -0.2786734   0.4024018   0.06483059 -0.07378349
  0.01044655 -0.3105709   0.15449266 -0.37524888  0.4073212   0.02212416
  0.41306558 -0.03836516 -0.03190994  0.04076146  0.1017275   0.1986692
 -0.31011227 -0.0058566   0.10540966 -0.17153642 -0.10253855 -0.32019362
 -0.1724841  -0.07218917 -0.3227237   0.15234642  0.26410383 -0.25014126
 -0.01056003  0.07864933  0.0106788   0.14016128 -0.06552689  0.08913447
 -0.21685372 -0.11871612  0.15413997 -0.25576708 -0.21529308 -0.0993654
  0.12666279  0.08506542  0.19759329 -0.07422829  0.3234776   0.14186491
  0.14624155  0.22104353 -0.2948714   0.00128344 -0.18918279  0.8473979
  0.1560047   0.02768437 -0.06755389 -0.1521153  -0.17669459  0.23049062
 -0.00235594  0.00093705 -0.27028832 -0.24311319 -0.28909922  0.05224627
  0.17770582 -0.3468338  -0.03038993  0.10388209  0.14587034 -0.04786724
 -0.21962683  0.21856478  0.10912082 -0.5065478   0.17052464 -0.19167787
 -0.11486851 -0.08512679 -0.12722269 -0.14317779  0.19911352  0.59859294
  0.25683916  0.06086382 -0.05797242  0.17453942 -0.21949218 -0.13868362
 -0.11810021  0.49868882 -0.17867747 -0.01501381  0.11747093  0.24256104
 -0.14361545 -0.06612991 -0.42489693  0.06281888  0.35892555 -0.18800695
 -0.03019454 -0.23612371 -0.02802191  0.31593883 -0.03638527  0.20722467
  0.01480608  0.31385595  0.5348155  -0.19843704 -0.23661755 -0.33805376
  0.02004594 -0.12073113 -0.2393016   0.08212082 -0.15493575  0.19025473]"
[rocm] [build] sh: line 1: /opt/rocm/hip/bin/hipcc: No such file or directory stat:awaiting tensorflower type:build/install subtype: ubuntu/linux TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Arch Linux

### Mobile device

_No response_

### Python version

3.11

### Bazel version

5.4.0

### GCC/compiler version

12.3.0

### CUDA/cuDNN version

ROCm 5.6.0

### GPU model and memory

_No response_

### Current behavior?

Build fails with the following error:
```
sh: line 1: /opt/rocm/hip/bin/hipcc: No such file or directory
```

### Standalone code to reproduce the issue

```shell
Build the code from source with ROCM 5.6.0
```


### Relevant log output

_No response_",False,"[-0.43228215 -0.4186195  -0.16624194  0.1236982   0.23107946 -0.48195717
 -0.29801112  0.03340405 -0.42723888 -0.1911982   0.1707493   0.06487563
 -0.2513317   0.08593884 -0.33742487  0.33603206 -0.18585141 -0.19499366
  0.4712038   0.27096564 -0.29004568 -0.08923924 -0.29557705  0.22444522
 -0.08440974  0.31908655 -0.28588122  0.0967714   0.03729038  0.01741402
  0.49655932  0.29835042  0.03775018  0.18842694  0.11221201  0.3361054
 -0.17139983 -0.08353264 -0.25742877 -0.01645509 -0.06076374 -0.19542174
  0.20573795 -0.1281837   0.04002186 -0.15947373  0.23705578 -0.22365797
  0.03591224 -0.22198069 -0.05299892 -0.10644187 -0.36750913 -0.4813118
 -0.19494012  0.04482887  0.12141662  0.12078153  0.07473676  0.22111501
  0.1639486   0.22740307 -0.03795275  0.10870372  0.06594668  0.3106897
  0.34631613 -0.16997403  0.5413076  -0.1046513  -0.02739501 -0.11429677
 -0.13222148  0.11975968  0.09242205  0.3990664   0.29190782  0.09384821
  0.2902714  -0.32154712 -0.16278782 -0.08188803 -0.0727719  -0.19311254
  0.18278691 -0.24840927  0.38635397  0.25401747  0.39170828 -0.38322523
  0.42797184  0.6477083   0.00115515 -0.09008542  0.31615567  0.12003517
 -0.05983299  0.34986803 -0.04403692 -0.04971925 -0.19200051 -0.16610172
  0.03368895  0.00703175 -0.30538762 -0.14291072  0.26466888 -0.17126599
  0.06987234 -0.04863643  0.12711653  0.15612721  0.04997656 -0.13436562
 -0.0755085  -0.08388489 -0.29780853 -0.2050146   0.14016426  0.88085103
 -0.15673442 -0.3651758  -0.00274389  0.14092068  0.377571    0.05848776
 -0.19568315 -0.08561036  0.06196648  0.3932839  -0.09247337  0.01763434
  0.13818765  0.19080605 -0.12227549  0.10603474 -0.18552841  0.01043454
 -0.01623275 -0.19271134 -0.12355332  0.30661047 -0.16758797 -0.6099114
  0.05839068 -0.01919541 -0.40079483  0.06781742 -0.3212767   0.11095731
 -0.10090002 -0.05719637  0.09744616  0.44282943  0.17832287  0.14915758
  0.37320584  0.01217747 -0.10280853 -0.51617426 -0.14488402  0.2866162
 -0.19555423 -0.1812047  -0.03815038  0.07620925 -0.47024074 -0.2867449
  0.16575396  0.59898114  0.23239861 -0.03010766  0.23021296  0.27717644
  0.30257207 -0.09038576  0.46423125 -0.43274248 -0.08829328  0.37833872
  0.18024057  0.01871029  0.11428714  0.18707761 -0.07658576  0.00199882
  0.16220865  0.16758767 -0.2289443  -0.01010834 -0.5097089  -0.00689083
  0.22807634 -0.20809199 -0.39003077  0.15817842  0.22567758 -0.17301974
  0.0529215  -0.06389152 -0.05005441 -0.05643126 -0.40537432  0.07811124
  0.13686374 -0.19787465 -0.10417513 -0.41131914 -0.7262157   0.04141055
  0.25578904 -0.48123884  0.18670788 -0.03238843 -0.08176994  0.04591607
  0.02524726 -0.07793933 -0.31004548  0.29258978  0.14237916 -0.30710492
  0.1419427  -0.37143636 -0.03447542 -0.08329816 -0.23922105 -0.05022918
  0.0676259   0.26546896 -0.02314804 -0.02999778  0.28813404  0.21715905
  0.46440524 -0.13294321 -0.04715793 -0.3403267  -0.09552038  0.07915694
 -0.36709884 -0.12638277 -0.06957703  0.15129358  0.31735826  0.50747764
 -0.00823074  0.15050997 -0.37561405  0.27406693 -0.19607323  0.24371305
  0.19439562  0.3692007   0.2284925   0.36951858 -0.08544608  0.28148335
  0.23801294 -0.10048391  0.23978823  0.14120424  0.05718466  0.47192466
  0.3186586   0.3751326  -0.47438064  0.39547986 -0.08415306 -0.07594935
  0.05451963 -0.16426055  0.48863295 -0.46168268 -0.1541051  -0.02146179
  0.275199   -0.00116021  0.0908123  -0.06386038  0.01579025  0.2943147
 -0.54216033  0.13606718  0.04673449 -0.14629972 -0.17527461 -0.8017649
 -0.30468267  0.07466975 -0.22068226  0.08348855 -0.21475253  0.0975522
 -0.3122101  -0.02692685 -0.02110859  0.01354405  0.06406087  0.1303524
 -0.02604304 -0.09464093  0.4015766  -0.36248893 -0.03491785 -0.21606058
  0.26487297  0.22019237  0.4374587  -0.3658328   0.04399353 -0.02458823
 -0.02914001  0.5226086   0.05608834  0.07793328 -0.4430942   0.6998404
  0.06239236 -0.24052884  0.26356214 -0.12556134 -0.18096595 -0.00657716
  0.06890084 -0.02126447 -0.20041174 -0.471247   -0.0583761   0.13306439
 -0.13658942 -0.22896095 -0.08876628  0.20935756 -0.42014322 -0.02994614
 -0.3259386   0.3327317  -0.209348   -0.30982217 -0.26077297 -0.055551
 -0.0215875  -0.38123322  0.09386624 -0.23779017  0.4501191   0.12625179
 -0.2752506   0.21305373 -0.0531716   0.24212924 -0.5020736   0.03679924
  0.19270577  0.20183522  0.00114026  0.04162657  0.3325302   0.21459845
 -0.0017105   0.38887942 -0.41974324  0.03297389  0.13698082 -0.19450115
 -0.256522   -0.02367209  0.06674399  0.31188175  0.07031237  0.2406624
 -0.24013846  0.27929914  0.5388363  -0.43852454 -0.22944543  0.27635098
  0.1627427  -0.04891632  0.22855487 -0.24334095  0.1370716  -0.06373656]"
Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations stat:awaiting response comp:apis type:performance TF 2.11,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have an issue with memory consumption during training. After each epoch more memory is used. In case of bigger dataset, the training crashed mid training. 
I use loading the training data from generator, where I sample the batch using pandas sample method. 
One of the warnings from tf is: 

`W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.`

I am a bit confused, whether using the generator and sampling from a pandas dataframe can cause memory accumulation during the training?



### Standalone code to reproduce the issue

```shell
def generate_samples(epochs: int, steps_per_epoch: int, token_y_df: pd.DataFrame, num_fields: int, batch_size: int = 32, max_len: int = 1024) -> tuple[np.array, np.array]:
        pad_value_y = get_padding_value_y(num_fields)
        while True:
            batch_df = token_y_df.sample(batch_size, replace=True)
            _x, _y = get_fixed_batch(batch_df, max_len, pad_value_y=pad_value_y,
                                     use_additional_features=use_additional_features)
            yield _x, _y

model.fit(generate_samples, ....)


The `get_padding` and the `get_fixed_batch` are longer functions, that pad and create the X and y np.arrays from the sampled pandas dataframe.
```


### Relevant log output

```shell
W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
```
",False,"[-0.26572907 -0.41371995 -0.29739815 -0.07832772  0.21910846 -0.45518485
 -0.32196617  0.1285254  -0.62188643 -0.15086207  0.04690147  0.04192676
 -0.00814931  0.11068904 -0.13349804  0.30868503  0.09588642 -0.03376396
  0.1872507  -0.13134877 -0.12410645 -0.12075873 -0.3194246  -0.01977637
  0.09755246  0.05630963 -0.33419698 -0.07840969  0.02235625  0.18455678
  0.17190857  0.04079608  0.03592725  0.02228085 -0.05369937  0.17420268
 -0.21458876 -0.19257852 -0.3559195   0.04812595 -0.17477599 -0.10446665
 -0.02302064 -0.07738277 -0.15109979 -0.16939186 -0.01805446  0.18481213
 -0.27503693  0.01269229 -0.22054322  0.0309803  -0.54982793 -0.3692423
 -0.15648784  0.15942681 -0.08982906 -0.06972565 -0.03938331  0.00151767
  0.2604497   0.00454959  0.04094134  0.01706531  0.17062129  0.2727548
  0.40341127 -0.07506183  0.45201945  0.07836881  0.24735242 -0.1184129
 -0.5442585   0.07480611  0.04635719  0.20461497  0.02556353  0.19022381
  0.40282747 -0.2727466  -0.02641099 -0.22845751 -0.14955568 -0.15252958
  0.23928216 -0.38096046  0.2899453   0.08397534  0.29859424 -0.08094195
  0.40350732  0.3497712  -0.24140003  0.03536023  0.3015085   0.13913208
  0.0468098   0.19881266  0.02455182 -0.12319347 -0.14462699 -0.19774534
 -0.17844738  0.20893371 -0.0787819  -0.26897612  0.13363321 -0.03937147
  0.20209685  0.16632682  0.03239515  0.10146533  0.04910681 -0.1609303
 -0.12449785  0.06058415 -0.17195651  0.09286986  0.00292537  0.61085355
  0.07414021  0.05886582 -0.03342976  0.23649798  0.6191573   0.14467436
 -0.18616632  0.01047197  0.03051618  0.0371361   0.2776106  -0.07759462
 -0.11841829  0.2190927  -0.14005235  0.07965368 -0.162083   -0.08010329
 -0.28475764 -0.17123923 -0.2699249   0.34080416  0.14900178 -0.4374825
  0.2318379   0.2846706  -0.31229037  0.25300497 -0.05622074 -0.15082785
  0.05725962  0.2691505  -0.2492994   0.22651008  0.12675087  0.29836488
  0.39333865 -0.09749036 -0.01643129 -0.548156   -0.04577065  0.25061196
 -0.20319217 -0.46352503  0.1368685   0.10849827 -0.2606315  -0.14570233
  0.02709509  0.5123863  -0.09880094 -0.13329966  0.01883018  0.180657
  0.31870663 -0.02751761  0.18730086 -0.6141646  -0.09166174  0.20083454
  0.29564124  0.1863344   0.21941823  0.27972907  0.16942276 -0.13582794
  0.16667017  0.2925902  -0.2454697  -0.23689838 -0.37289482 -0.27203915
  0.2820351  -0.02536075 -0.13021769 -0.20399661  0.23740643 -0.34371075
  0.2302812   0.19986036  0.01832849  0.08968933 -0.25614253 -0.02086741
  0.146256   -0.13296914 -0.25476727 -0.38666648 -0.41191256  0.21976139
 -0.08622798 -0.44179404  0.21680482 -0.18620974 -0.2649838   0.07428084
  0.1513015   0.13268304 -0.23482203  0.41835576 -0.04778294 -0.12283064
  0.07696119 -0.4238166  -0.28589016  0.18723497 -0.18443224  0.03470185
 -0.00608784  0.1170373   0.17580004 -0.20956291  0.31512326  0.2442467
  0.07534161 -0.2066528   0.0607733  -0.35351044 -0.47759637  0.11345978
 -0.32040346  0.01916485 -0.02219276 -0.11315165  0.31574544  0.35242212
 -0.10230627 -0.00567235 -0.3054672   0.16964948 -0.3982156  -0.07114669
  0.2217022   0.06777263  0.19986315  0.2203055   0.04996981 -0.04325294
  0.22195807 -0.1349735   0.3930057   0.04221967  0.04755814  0.57088673
  0.37550688  0.2894727  -0.3025431   0.2841465   0.30509228 -0.23386037
  0.00611127 -0.3092737   0.48637185 -0.25327915 -0.02628748 -0.01087953
  0.19610702  0.00424962  0.02042398 -0.16874468  0.03406399  0.21727611
 -0.3507743   0.02143833 -0.16360457 -0.20842463 -0.05263979 -0.3494208
 -0.16376173  0.19314736 -0.1661292   0.18043058 -0.00149806  0.12254643
 -0.2202553  -0.1373257   0.12681356  0.01220739  0.07506552 -0.01027669
 -0.12145475  0.12861422  0.47420877 -0.20690534 -0.21040356 -0.02579051
  0.50066364  0.11871637  0.33951247 -0.285714    0.08149242  0.06604209
 -0.07731092  0.58224005 -0.08556847  0.01209367 -0.37910104  0.66261464
  0.35681078 -0.00599951 -0.03951593 -0.20227471 -0.17783687  0.16433048
  0.19215974  0.06094     0.03031566 -0.23021752  0.1733118  -0.01335719
 -0.06654589 -0.07384515 -0.07234751  0.2983938  -0.16380316 -0.25779468
 -0.28870225  0.221284   -0.02616817 -0.0676659  -0.07397465  0.13122982
 -0.21426892 -0.30842873  0.01197407 -0.29115248  0.3214696   0.45095193
 -0.42024422  0.13695282 -0.06867998  0.18954834 -0.266679   -0.16764341
  0.09494542  0.10364584  0.02071439  0.04744711  0.15526669  0.14211923
 -0.24491659  0.41812417 -0.3541132   0.09796959  0.4399341  -0.12279399
 -0.1948864   0.01516542  0.32866874  0.31169146 -0.15702108  0.24411876
 -0.27451655  0.25768614  0.4639505  -0.4090281  -0.20334631  0.18214911
  0.26635918  0.06270935 -0.0047685  -0.15189698  0.07754532 -0.03189005]"
ADD Suppport for VEDV (https://github.com/yunielrc/vedv) type:support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
",False,"[-3.86830002e-01 -5.22635460e-01 -2.24031016e-01 -4.01775055e-02
  3.20648067e-02 -2.10264429e-01 -4.05652255e-01 -2.06570834e-01
 -3.37500498e-02 -2.31113791e-01 -1.20385243e-02  3.66075218e-01
 -1.24501809e-02  3.83103192e-02 -6.18742108e-02  2.93448448e-01
 -1.34717733e-01 -5.04691899e-03 -8.37349668e-02  2.04134673e-01
  1.04455918e-01 -1.55498795e-02 -3.22819948e-02 -1.39262825e-02
  1.18388996e-01  2.19562165e-02 -1.74491435e-01 -1.90274894e-01
  8.56381208e-02  1.08853877e-01  4.25610751e-01  6.59524202e-02
 -8.52129310e-02 -5.50233573e-02  1.88194871e-01  2.69025862e-02
 -1.31188840e-01 -2.49903113e-01 -1.64151371e-01  4.45562676e-02
  1.70247138e-01 -1.54249147e-02 -3.70167270e-02  1.56604826e-01
 -1.72910169e-02  1.39769316e-01 -3.72677669e-02 -2.19768584e-01
 -1.10093895e-02 -1.77818239e-02 -1.26873508e-01 -2.80736595e-01
 -2.75763988e-01 -1.10218219e-01  8.64447728e-02 -1.74539924e-01
 -1.42199397e-01  1.13180220e-01 -3.53071503e-02  1.30409032e-01
  1.24735713e-01 -4.78734300e-02  2.28809088e-01  2.00284690e-01
  1.53296515e-01  8.46337304e-02 -7.84360766e-02 -4.61033955e-02
  3.23063970e-01 -3.39798033e-02  1.74034294e-02  4.11884785e-02
 -2.59729266e-01  9.45410430e-02  5.89540750e-02 -1.65427953e-01
 -2.78065443e-01  3.44993383e-01  3.04603577e-01  3.53725106e-02
  2.75287211e-01  7.80431181e-02  1.41552150e-01 -1.78008922e-03
  1.49008334e-01  7.23381191e-02  1.29874021e-01  1.45286754e-01
  8.20396692e-02 -5.21721765e-02  2.96313733e-01  8.98763090e-02
  2.35396624e-02  1.04822606e-01  3.12733233e-01 -2.93910727e-02
  3.04486156e-02  3.32752392e-02 -2.85393924e-01 -1.64166614e-01
 -2.14815542e-01 -3.37880790e-01  1.30114332e-03  1.17543280e-01
  2.43857335e-02 -9.59072113e-02  1.68848723e-01  2.88218021e-01
 -9.00755227e-02 -5.85767552e-02  2.60478914e-01 -6.83859885e-02
  1.37310117e-01  1.04627851e-02 -1.57564253e-01  1.94399171e-02
  2.38012131e-02  1.09009974e-01 -2.01180339e-01  5.82890064e-02
 -6.39370829e-03 -1.67829961e-01 -2.79121473e-02  1.09721996e-01
  4.81217951e-01  1.36617988e-01 -3.72352362e-01  2.07019687e-01
 -8.58559310e-02  1.31576881e-01  7.14477450e-02 -1.19315814e-02
  9.98475850e-02 -4.67246361e-02 -1.13842398e-01 -4.54552919e-02
 -3.47462833e-01 -8.59720185e-02 -1.26684746e-02 -2.81920135e-02
  1.25440091e-01  2.52797335e-01 -1.24601722e-02 -3.37647945e-01
  2.13053152e-02  1.54218689e-01 -1.29847983e-02 -7.38462061e-02
  7.45122433e-02 -2.22171962e-01 -1.51397049e-01  3.47060263e-02
 -2.70253122e-01  3.96546245e-01  1.52001977e-01 -6.53991709e-03
  1.63519844e-01 -8.70090052e-02 -8.29382241e-02 -3.03328097e-01
  1.33941993e-02  5.46310358e-02 -1.39949098e-01  1.33884668e-01
  2.17206270e-01  9.48611461e-03 -3.89722705e-01 -1.78031430e-01
  4.62713540e-02  7.62927681e-02 -5.90516850e-02 -3.76863666e-02
  7.02164173e-02 -9.66332257e-02  4.56558883e-01  1.15511447e-01
  3.96889985e-01 -2.23317236e-01 -1.93244323e-01  1.49063498e-01
  1.93919539e-01  6.90080132e-03 -1.65161975e-02  7.49598369e-02
  1.20782688e-01 -2.45168619e-02 -2.78491080e-01 -4.50476445e-02
 -2.30336308e-01 -3.06095600e-01 -3.14955056e-01 -1.30282104e-01
  2.32141055e-02 -4.82441112e-02 -2.67114639e-01  1.03826649e-01
  8.48646685e-02 -1.55179888e-01 -5.27168289e-02  8.21029991e-02
 -1.77664012e-01  1.45406798e-01 -2.13498056e-01 -1.15710303e-01
 -5.30506037e-02  1.04998983e-01  1.65809132e-03 -2.37715334e-01
 -3.85264933e-01  2.12086178e-03  9.04647559e-02  4.01124358e-04
 -1.44596219e-01  1.82386227e-02 -1.57576352e-01  1.38018839e-02
 -4.85934950e-02  8.93540978e-02 -2.17020065e-01  2.15449989e-01
  3.12550403e-02 -1.71219021e-01  2.19977796e-01 -2.88988411e-01
 -4.49817002e-01  7.79088438e-02 -8.14439803e-02  3.42261255e-01
  6.71469420e-02  2.74346471e-01  1.98261559e-01 -4.15860638e-02
  4.51366723e-01  7.04281032e-02 -8.32929462e-02 -7.74298608e-03
 -2.21999854e-01 -1.47169948e-01 -4.81266938e-02  5.84863834e-02
 -4.63586032e-01 -3.71222794e-02 -5.53320572e-02 -1.80292249e-01
  2.49395035e-02  2.82318711e-01  1.37576640e-01  1.35015979e-01
 -3.68502401e-02  3.07517201e-01  2.08110169e-01 -3.72715056e-01
  2.99439669e-01  1.10517748e-01  1.49473682e-01 -8.28687102e-03
 -9.59599167e-02  3.77126485e-02 -1.62104949e-01  2.10335270e-01
  3.37259889e-01  2.54021704e-01  8.94119143e-02  4.42676544e-01
  3.04726183e-01  2.26188421e-01 -2.69872576e-01  1.60984233e-01
 -1.52244508e-01 -9.13096964e-02  1.31569177e-01 -3.58535200e-02
  3.87556136e-01 -1.17030196e-01 -1.66276246e-01 -1.34374633e-01
  2.76516199e-01 -1.90256953e-01 -5.32247312e-03 -1.93581637e-03
  5.28175905e-02  1.02143332e-01 -4.94808927e-02 -4.53610644e-02
  2.42080629e-01 -7.91864842e-02 -8.96733999e-02 -3.73367876e-01
 -2.35366449e-03 -5.52866310e-02  3.34303938e-02  1.51696697e-01
  2.08012126e-02  1.49145380e-01 -2.04777837e-01 -2.07267046e-01
  7.24328533e-02 -8.77581015e-02  3.12412828e-01  2.21147314e-01
  1.81430951e-02  2.35425800e-01  1.11647621e-01 -1.96869448e-01
 -3.14382434e-01  6.85960129e-02 -1.13534637e-01  5.46043031e-02
  5.46004534e-01 -2.36563794e-02  2.61953175e-01  2.01684356e-01
 -1.73136547e-01  3.95950377e-01 -1.58329293e-01  2.16614693e-01
 -1.29127160e-01  2.05978513e-01 -2.84697991e-02 -1.38528809e-01
  2.44331509e-02 -7.25788027e-02 -2.93036699e-01  5.06179929e-02
  1.37212753e-01  1.87696666e-01 -3.63919556e-01  7.68214464e-02
 -1.41200766e-01  1.88321501e-01 -2.10858196e-01  3.42775173e-02
 -6.54531121e-02  1.94581777e-01 -2.72757530e-01 -5.57001606e-02
 -1.89597830e-02  3.51584226e-01 -1.16099246e-01 -2.85561740e-01
 -1.81601495e-01 -1.87021926e-01 -6.37437925e-02 -1.11166373e-01
 -2.32887447e-01 -2.42890269e-01  3.27430904e-01  5.49951434e-01
  1.46972314e-02 -2.94314101e-02 -7.65961707e-02  1.98333383e-01
 -3.50686967e-01  6.48788512e-02 -1.44596770e-01  1.96362197e-01
 -1.46128491e-01 -6.95837438e-02  8.63931179e-02  3.88531089e-01
 -1.69299185e-01  1.65268138e-01 -8.08628201e-02 -5.37585542e-02
  7.55980164e-02  2.10028052e-01 -8.89405906e-02 -1.33032918e-01
  1.68942481e-01  4.79668289e-01 -6.90759271e-02 -1.33544533e-02
 -1.87845126e-01  6.16023168e-02  1.83620304e-01 -1.10072726e-02
  9.15003195e-02  2.31802426e-02  7.91417584e-02  6.38895184e-02
 -3.02842528e-01 -1.08723074e-01 -1.37672961e-01 -4.91062775e-02]"
import tensorflow as tf delegate = tf.lite.experimental.load_delegate('/content/drive/MyDrive/test_delegate/libtensorflowlite_gpu_delegate.so')#with this we ca get faster predictions OSError: libEGL.so: cannot open shared object file: No such file or directory type:build/install comp:lite TFLiteConverter TFLiteGpuDelegate,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",False,"[-0.22939716 -0.5919096  -0.42874318  0.13033664  0.20414895  0.0956044
 -0.07999174 -0.14751501 -0.10890743 -0.226484   -0.16071339 -0.04296299
  0.08348643  0.18364923 -0.31083423  0.11408456  0.05166696 -0.44714978
  0.19172022 -0.10733666  0.14920674 -0.11896236 -0.05915726  0.19088869
  0.15328911  0.03516193 -0.01609454 -0.22525635  0.08332646  0.11958448
  0.28213686 -0.03518088 -0.34258115 -0.0896843   0.1588762   0.03013195
 -0.14819038 -0.21016318 -0.21763408 -0.139339    0.24817443  0.00222887
  0.03429066  0.27465862 -0.34087783  0.06512381  0.11556777  0.02854492
 -0.32949007  0.06324003 -0.05989153 -0.03922333 -0.22394256 -0.25663966
  0.136583   -0.13320008  0.21810289 -0.10201572  0.22708184  0.1010804
 -0.08929823  0.12176384  0.04028204  0.13688654 -0.22022524  0.22686097
  0.04757779  0.0228468   0.2358829  -0.33659196 -0.23444554  0.1244591
 -0.1617738  -0.11787789  0.07925471 -0.16079214 -0.17715015  0.38355482
  0.2093527   0.05027156  0.01556417  0.03265027  0.02221912  0.08695036
  0.10292514 -0.3212349   0.12892044  0.22857505  0.3618986  -0.13341348
  0.35699862  0.36929667 -0.13968949  0.37682647  0.09779954  0.10585075
  0.11683853  0.18153368  0.15095314 -0.07633379 -0.31083083 -0.05557975
 -0.11553278  0.14592478  0.08004504 -0.05400784  0.02024993  0.20398542
  0.12707458 -0.03079367  0.18791366 -0.04165122  0.00244744  0.01103307
 -0.01705883  0.15733993 -0.02917093  0.4365091   0.10735187  0.00361131
  0.03368554 -0.3018929   0.12376693 -0.08468506  0.5229442   0.15482262
 -0.38226035 -0.00408281  0.1393156   0.04855228 -0.10260522  0.42323515
 -0.2942103  -0.13547495  0.2343347   0.10466692 -0.42516625 -0.11746559
 -0.16614267  0.13241667 -0.1539096   0.29313904 -0.11817589 -0.20333722
 -0.14347163  0.11801492 -0.16209178  0.11641314  0.03008181 -0.0435504
 -0.00647932 -0.16380428 -0.13561991  0.12068342  0.11868867 -0.03353068
  0.40904713 -0.10296519  0.04383554 -0.27226016 -0.12339136  0.40505335
  0.00249621  0.07011434  0.0839992   0.06203001 -0.35621458 -0.04046233
  0.3359249   0.05564089  0.07097263 -0.32751006 -0.00212518  0.16410404
  0.3436422  -0.15413328  0.5473596  -0.3449948  -0.08323377 -0.05734661
  0.39403525  0.07661172  0.20169705  0.12678258 -0.23709996 -0.07201428
  0.3546704   0.02639381 -0.4172176  -0.24632423 -0.16912669  0.04239453
  0.38055915 -0.16130269 -0.3446474  -0.26952392  0.14694706 -0.06750731
  0.16717006  0.17588434  0.01563757 -0.14418977  0.07568294  0.01056612
  0.12243763  0.1679632  -0.27029163 -0.34002888 -0.2556783  -0.07667999
 -0.04436528 -0.1944414  -0.11579917 -0.20912233 -0.31038395  0.09555993
 -0.20123214 -0.3465674  -0.23987564  0.07438371  0.07701743 -0.14557403
  0.05507488 -0.25254506 -0.15281628 -0.29568642 -0.02882573  0.32696903
  0.17210539  0.28917134 -0.1436667  -0.23896988  0.34764928  0.20990102
  0.15552841 -0.21865885  0.12309154 -0.27076006 -0.13186175 -0.00782603
  0.00201739  0.02813598 -0.14999804 -0.17490482 -0.10784926  0.20397866
 -0.02932784 -0.02918945 -0.03623859  0.5035576   0.00596039 -0.03233416
  0.0759601   0.10911565  0.33068445  0.14734176 -0.10246846 -0.20890032
 -0.0154061   0.13746199 -0.04679735  0.4453116  -0.07706004  0.53591293
  0.15682115  0.2093437  -0.26738814  0.16195184 -0.1193178  -0.07573184
  0.130618   -0.37179327  0.3763065  -0.04492987  0.07951824 -0.04114227
  0.3285107  -0.19285636 -0.0664622   0.2860058   0.1907002   0.45187208
  0.1507409   0.0082163   0.26317054  0.02690878 -0.02660626 -0.368952
 -0.3971063   0.107742   -0.1125446  -0.01778381  0.18846689  0.07452558
 -0.23046352 -0.09667085  0.10843588 -0.08710445  0.51925814 -0.09368575
  0.15808311  0.23838277  0.15667978 -0.22154464 -0.00105844  0.04432986
  0.14598903 -0.1078445   0.29147273 -0.48814347  0.46126437 -0.16036236
  0.15772822  0.24578977 -0.24575028  0.07971248 -0.26530108  0.09643379
 -0.04579808 -0.10429521  0.23078565  0.03805787 -0.4605899  -0.01104855
 -0.06702297  0.14461221 -0.08011521 -0.13674815 -0.25201255  0.08400682
  0.06365468 -0.40239966  0.02143735  0.04959774 -0.3214574   0.12508789
 -0.23673107  0.22544527 -0.1357136  -0.20344624  0.06206607 -0.20436382
  0.01316779 -0.05782533  0.05391199 -0.06274091  0.02709888  0.02439549
  0.17109291  0.04378332 -0.21634918  0.10420317 -0.45008445  0.22932905
  0.18314685  0.28205788 -0.28181332  0.04036886  0.14336374  0.6620459
  0.05319665  0.3694231  -0.23641239 -0.17599952 -0.13437442 -0.06454171
 -0.4045293  -0.2812104  -0.02562331  0.5527843  -0.08163423  0.26289582
 -0.3770236   0.18627876  0.23853001 -0.1159942   0.12568596 -0.00523048
 -0.04186509 -0.14564611 -0.02317368 -0.21008506 -0.09442738  0.11669476]"
TensorFlow Lite in Play Services issue aguinigacervantesjerardo@gmail.com not my devices are the login in just this phone device that are not this one  getting them out of my server please send them a little massage to buy there phone and don't use my phone number and my email at all stat:awaiting response type:support stale comp:lite,"**System information**
- Android Device information (use `adb shell getprop ro.build.fingerprint`
  if possible):
- TensorFlow Lite in Play Services SDK version (found in `build.gradle`):
- Google Play Services version
  (`Settings` > `Apps` > `Google Play Services` > `App details`):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to or attach code demonstrating
the problem.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
",False,"[-4.55818117e-01 -3.82762462e-01 -7.31709599e-02 -2.19495773e-01
  1.39269590e-01 -2.49202967e-01 -5.20392694e-03  4.30809557e-02
 -1.50012806e-01 -1.36074081e-01  5.39997481e-02 -3.71017233e-02
 -2.42646486e-01  1.18024915e-01  3.06163371e-01  1.68095633e-01
 -3.83988842e-02 -1.86615855e-01  1.42088234e-01  7.22765997e-02
  1.00137949e-01 -1.90017194e-01 -2.18854189e-01 -1.33877456e-01
  1.40249394e-02  2.21707001e-01  3.65336277e-02 -2.96955675e-01
  8.38517845e-02  3.17966342e-01  4.78924513e-01  1.18993394e-01
 -4.51752916e-04  9.76055488e-02 -1.99698824e-02  6.62107021e-02
 -4.45370138e-01 -2.04658628e-01 -2.13796973e-01 -1.09988220e-01
  1.26861706e-01  2.13956788e-01  5.73248938e-02  1.81640059e-01
  1.20725185e-01  3.32486123e-01 -1.81345284e-01  1.03816688e-02
 -8.51915330e-02  1.14597324e-02 -1.50012314e-01 -1.33893583e-02
 -4.73850787e-01  1.14691257e-02 -1.23568237e-01  2.33398154e-02
 -1.36309803e-01  2.55428910e-01  4.32927348e-02  1.50163114e-01
  8.70757326e-02 -1.35001928e-01 -6.10970557e-02 -5.74696027e-02
 -2.29649127e-01  1.31139576e-01  1.86465651e-01 -9.88897532e-02
  3.63394082e-01 -2.15389192e-01 -9.96087044e-02 -6.87067397e-03
 -8.66511390e-02  2.23996356e-01  1.28549114e-01  2.40065977e-01
 -1.45153344e-01  6.71661794e-02  1.73688233e-01  1.12933800e-01
 -5.23802713e-02 -7.48626366e-02  3.40888262e-01  1.14886343e-01
  4.34982836e-01  5.32360524e-02  2.05254823e-01  1.71023935e-01
 -1.30740896e-01  7.55958855e-02  1.28182262e-01  2.57041514e-01
 -6.34594709e-02  1.09901331e-01  2.83835560e-01  1.06908038e-01
  3.22072245e-02 -7.01929703e-02 -3.04486126e-01  1.16694011e-02
 -1.23358741e-01 -6.56196326e-02  8.97116959e-03  2.40298629e-01
 -1.66015830e-02 -2.18866915e-01  9.73793864e-02  1.07789211e-01
 -2.17649974e-02  1.82533637e-01  1.87022954e-01 -2.40757957e-01
  3.12151890e-02  5.24387881e-02  8.16390514e-02  1.78580694e-02
 -3.44109952e-01  1.22203156e-01  1.71100721e-01  6.11056507e-01
 -1.99894726e-01 -1.18541971e-01  1.33818269e-01  9.94281564e-03
  1.42696649e-01 -4.89006937e-02  9.67786461e-03 -3.76516320e-02
 -2.22930208e-01 -3.81754488e-02  3.09778191e-02  1.61412165e-01
 -1.99957296e-01 -3.04414667e-02 -4.90174145e-02  9.23629999e-02
 -2.41375923e-01 -1.68726295e-01 -3.31467032e-01  3.33686136e-02
  1.71867125e-02  1.17498554e-01  9.85452905e-03 -1.02768481e-01
  2.02079356e-01  3.23234685e-02 -4.74611297e-03  3.24957669e-02
 -1.75505802e-01 -1.56393290e-01 -5.23058735e-02 -1.25901714e-01
 -1.15620151e-01  4.49934542e-01  1.34336740e-01  1.58362076e-01
  2.96727210e-01  4.61020730e-02  6.69436529e-02 -6.65366411e-01
  1.08235143e-03  8.74855816e-02  6.31722510e-02  2.45041400e-01
  1.15896828e-01 -1.76774919e-01 -5.15781760e-01 -1.13548890e-01
 -2.90366590e-01  1.35796487e-01 -1.95018813e-01  1.39417192e-02
 -2.94837475e-01  8.00522119e-02  1.48281664e-01 -6.79582953e-02
  2.60805637e-01 -4.00445938e-01 -1.83883786e-01 -1.81519881e-01
  1.73207015e-01  9.85459238e-02  9.71759483e-02 -1.99297056e-01
 -1.87193111e-01  1.42823711e-01  2.60656253e-02  4.99540120e-02
  2.21298579e-02  1.11913234e-01 -3.16179961e-01 -2.57257611e-01
 -3.95943001e-02 -9.63502154e-02 -1.31979376e-01 -1.66107342e-01
  3.26816998e-02 -1.47368431e-01  3.58013734e-02  1.30200833e-01
  9.28824469e-02  1.73806950e-01 -2.36762196e-01  6.62192702e-02
  1.89492434e-01 -1.89206541e-01 -3.85487139e-01 -7.72323906e-02
 -2.48904526e-01 -7.49746859e-02  3.20436239e-01 -4.35660034e-01
 -3.21174085e-01  6.21488020e-02 -1.22094162e-01 -1.47413611e-01
 -1.43407509e-01  8.11523572e-02 -5.25140584e-01  3.43664944e-01
 -2.04158664e-01 -2.05726139e-02 -1.41816556e-01 -7.96212479e-02
 -4.70569015e-01 -2.45966911e-01 -2.23455012e-01 -3.33714336e-02
 -7.39778578e-02  3.83178443e-01  7.50225484e-02  1.76322848e-01
  2.21513748e-01 -4.91870828e-02  1.03206761e-01 -1.25960827e-01
  1.98738780e-02 -1.90628678e-01 -3.10370088e-01 -6.34194016e-02
 -3.09615672e-01 -2.26294473e-02  2.97779590e-02 -1.17806152e-01
 -1.39838845e-01  1.31092966e-01  1.22212797e-01  3.48687381e-01
 -1.91660196e-01  3.10613453e-01  7.14126304e-02 -9.74017382e-02
  1.64787963e-01  2.51546115e-01  4.87302303e-01 -1.57296890e-03
  1.33607872e-02  9.22209304e-03  1.23972386e-01  8.43527392e-02
  4.60007846e-01  1.38324112e-01 -4.54208553e-02  3.13267708e-01
  4.06504631e-01  2.18372911e-01 -4.19060946e-01  5.27117662e-02
  1.64936662e-01 -2.02373385e-01  3.81701738e-02 -1.36076972e-01
  1.19730935e-01 -2.79844284e-01  6.74294159e-02 -5.07795550e-02
  4.35381979e-01 -7.09880143e-04 -2.14352593e-01  1.32790148e-01
  2.68245757e-01 -4.84715067e-02 -2.56159186e-01 -1.61256388e-01
  1.18274204e-01 -5.56985021e-01 -7.79044777e-02 -4.40424263e-01
  1.16864657e-02 -1.56109408e-02  7.78254271e-02  1.14614621e-01
  1.62422672e-01  1.90825313e-01 -3.44361186e-01  8.34068954e-02
 -1.12870224e-01  8.93006101e-03 -3.34373415e-02  2.19646156e-01
 -1.59465283e-01  1.55524507e-01  4.53572497e-02 -1.46046907e-01
  6.10839874e-02 -5.75783923e-02  2.56872833e-01 -1.49655268e-01
  3.79518211e-01 -2.29513757e-02  2.96926796e-01 -1.99563466e-02
 -1.71802435e-02  3.05488795e-01 -2.79304087e-01  2.03778684e-01
 -2.48111278e-01  6.69196308e-01  2.08807424e-01  7.93542638e-02
  3.12395394e-01 -2.59688020e-01  1.26921743e-01  2.09245160e-02
 -2.27440804e-01 -3.18707079e-02 -1.62519634e-01 -2.69536614e-01
 -7.81429335e-02  1.96929350e-02 -2.35331967e-01 -2.06497386e-01
 -2.07035571e-01  2.88326561e-01 -2.55173482e-02  2.27377638e-02
 -1.93601102e-01  2.18135267e-01 -8.28107260e-03 -1.87874079e-01
  1.14815161e-01 -5.46719506e-02  3.24876681e-02 -3.07892740e-01
 -1.83231141e-02 -2.58265555e-01  5.35232902e-01  2.53456831e-01
  5.58010265e-02  2.87327375e-02 -1.36854276e-01  1.28774405e-01
  2.71449368e-02 -1.23342462e-01 -3.01200420e-01  2.85352618e-01
 -9.45038907e-03 -1.98460013e-01  8.92452523e-02  7.93632746e-01
 -1.40429839e-01 -1.27157211e-01 -9.83653963e-02 -1.92537904e-01
  1.10198133e-01 -3.80429402e-02  2.58328378e-01 -1.48475885e-01
  1.28719956e-01  6.21807218e-01 -1.41713351e-01  1.31657392e-01
 -3.09784353e-01  1.29869636e-02  2.54743636e-01 -2.58379042e-01
 -8.38072598e-02  2.06545949e-01 -1.76047653e-01  1.00582771e-01
 -1.52670667e-01  5.46929007e-03  9.97839868e-02  2.15455532e-01]"
How do I set the threshold(iou/score) of TFLite Detection PostProcess via interpreter? stat:awaiting response comp:lite TFLiteConverter comp:lite-support TF 2.13,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- TensorFlow installation (pip package or built from source): pip package, python3.8
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
Download tflite model from https://tfhub.dev/iree/lite-model/ssd_mobilenet_v1_100_320/fp32/nms/1?lite-format=tflite

#### Interpreter code
`
import numpy as np
import tensorflow as tf

//TensorFlow Lite model path
tflite_model_file = 'lite-model_ssd_mobilenet_v1_100_320_fp32_nms_1.tflite'

interpreter = tf.lite.Interpreter(model_path=tflite_model_file)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']
input_array =  np.array(np.random.random_sample(input_shape), dtype=np.float32)

//HOW TO SET THRESHOLD POST-PROCESS
// is there any method like 'interpreter.set_threshold()' ?

interpreter.set_tensor(input_details[0]['index'], input_array)
interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])

print(output_data)
`

### 3. Failure after conversion


### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",False,"[-3.70149255e-01 -4.27103877e-01 -2.57510394e-01 -6.38378859e-02
  2.02719837e-01  1.36956060e-02 -7.16035068e-02  6.88931197e-02
 -2.29725793e-01 -1.00375995e-01 -1.03409607e-02 -2.22822595e-02
  5.45089170e-02  1.18648425e-01 -2.91603476e-01 -8.13753437e-03
  1.04135640e-01 -2.03308046e-01  2.98078805e-01 -3.13058019e-01
  4.20945659e-02 -8.63965079e-02 -4.77942415e-02  1.96884722e-01
  2.14827165e-01  1.03951823e-02 -3.10426891e-01  3.75575647e-02
 -1.06688097e-01  2.96698093e-01  1.11571588e-01  1.14929408e-01
 -1.56839639e-01 -1.46198064e-01 -4.07770090e-02  1.56147629e-01
  1.83321804e-01 -4.31752875e-02 -3.82858574e-01 -6.94719180e-02
  9.33275223e-02  8.67562816e-02 -2.61483401e-01  1.10235788e-01
  9.30402987e-03  1.06603235e-01  1.87233716e-01  2.02553183e-01
 -4.79240358e-01 -8.09642151e-02 -3.71689424e-02 -1.70389235e-01
 -2.39767849e-01 -3.28694936e-04  1.02527022e-01 -5.32064512e-02
  9.11276788e-02  3.31868619e-01  4.24126908e-02  8.30955207e-02
 -5.69537282e-03  6.06017411e-02  9.03798416e-02  4.47511226e-02
  2.19681144e-01  4.06913608e-01  8.63704979e-02 -2.69409835e-01
  2.67333686e-01 -2.32206702e-01 -1.89763919e-01 -5.37738651e-02
 -8.44686553e-02 -1.25837281e-01 -2.38432065e-01  1.72452897e-01
 -2.28249550e-01  2.02734128e-01  2.08415627e-01 -9.81994942e-02
  2.36576766e-01 -2.08988160e-01  3.17947939e-03  2.61930823e-01
  8.25427622e-02  4.82748784e-02  1.52725875e-01  3.38200152e-01
  2.69029737e-01 -1.53551102e-02  2.56592274e-01  1.85480833e-01
 -1.62220716e-01  8.98145735e-02  1.20537356e-04  2.69183405e-02
  8.10493976e-02  2.17628032e-01  1.49177462e-01  2.46342301e-01
  2.45512240e-02 -5.09436965e-01 -4.33759987e-01 -5.02678528e-02
  2.11253747e-01 -2.59891395e-02  1.74817026e-01  1.25494152e-01
  4.73212600e-02  1.21314466e-01  2.03386843e-01  7.19199404e-02
 -8.79100151e-03  1.30673632e-01  1.11529693e-01  1.10621780e-01
  5.51866218e-02  1.43613234e-01  2.93916345e-01  3.76330316e-01
  1.81106105e-02 -2.18858898e-01  6.52851909e-02  1.39601631e-02
  5.04791021e-01  1.26467168e-01 -7.56952912e-02 -2.54040770e-02
  5.72330877e-02  2.13353887e-01  2.46161036e-02  6.55951127e-02
 -2.82228559e-01  8.23580176e-02  7.32290223e-02  9.99444723e-02
 -9.88769829e-02 -5.82861565e-02 -3.55488062e-01 -8.51366296e-02
 -1.69126078e-01 -1.24215350e-01  6.07739463e-02 -1.26245525e-02
  8.11665133e-03  1.30744159e-01 -2.60783583e-02  2.17753291e-01
 -1.03520796e-01 -8.71251225e-02 -1.38458461e-01  1.32307038e-01
  6.72993809e-03  2.33309656e-01  1.03315294e-01 -1.38324618e-01
  1.38593882e-01 -2.70651747e-03  9.82475579e-02 -7.50900060e-02
 -8.13188255e-02  1.20404586e-01 -6.08264580e-02 -1.69398487e-01
 -2.62052789e-02  9.24412012e-02 -4.25088018e-01 -1.75991386e-01
  1.38470083e-01  1.11946613e-01  1.03256837e-01 -1.06584504e-01
  1.66389361e-01 -6.33948445e-02 -1.54403076e-01 -1.16915658e-01
  3.86547744e-01 -1.51739731e-01  1.06582597e-01  1.03184953e-02
  3.81365865e-02 -5.86071610e-03  1.23592257e-01  1.15083598e-01
 -2.56240726e-01 -5.93843237e-02  2.29745761e-01  2.31200337e-01
 -2.71174729e-01 -6.75143450e-02 -1.73899949e-01 -1.16879463e-01
  3.61038983e-01 -1.65682703e-01 -4.70098257e-01 -1.13095388e-01
  1.36549041e-01  5.55645078e-02  8.20094198e-02  1.32243633e-01
 -8.33056420e-02  9.40746367e-02 -6.88832775e-02  1.69865303e-02
  1.24545299e-01  4.45464738e-02  1.10427223e-01 -2.99382508e-01
 -3.47345173e-01  2.51116287e-02  1.26166493e-01 -3.94095063e-01
 -5.17686233e-02 -1.99956849e-01 -1.32978499e-01 -1.64975878e-02
 -1.66940659e-01 -9.28169414e-02 -1.59834862e-01  1.61514282e-01
 -8.54688287e-02 -9.30186734e-02 -1.84153065e-01 -1.75856307e-01
 -1.92112982e-01 -3.38702984e-02 -2.40512982e-01  1.40220657e-01
 -1.09095782e-01  1.97803885e-01 -1.96833089e-01  3.04037541e-01
  5.20751953e-01  2.21426457e-01  1.56165138e-01 -2.00676575e-01
  1.72423050e-02 -1.06847599e-01 -2.84143448e-01  2.14030266e-01
 -2.87427068e-01 -2.56975710e-01 -1.56784579e-02 -7.04213455e-02
 -1.64861247e-01 -9.20289755e-03 -6.07566983e-02 -4.59632836e-02
 -1.11732289e-01  1.64199591e-01  6.19436651e-02 -3.03029120e-01
 -3.45167108e-02  9.63228270e-02  1.99071169e-01  1.73499763e-01
 -1.50204867e-01 -3.26411724e-02 -2.19891034e-02 -6.93921968e-02
  2.70858765e-01  1.51627302e-01  4.16274443e-02  6.15482152e-01
  2.74860680e-01  2.57805884e-01 -1.40550435e-01  3.91764402e-01
 -3.60492349e-01 -2.51091361e-01  1.63100496e-01 -1.02604337e-01
  3.19313765e-01 -1.90573558e-03  9.38447714e-02 -1.43749535e-01
  3.91757697e-01 -7.02470392e-02  4.36792076e-02  1.50134772e-01
 -2.25522313e-02  2.74956822e-01 -4.62797374e-01  1.11833178e-01
  9.83171687e-02 -1.37943715e-01 -1.31162435e-01 -5.14721096e-01
  1.82551704e-02  1.44335583e-01 -1.94144964e-01  2.82501746e-02
 -3.31660286e-02  8.93526971e-02 -1.05371848e-01  2.15808861e-02
  3.43196929e-01 -1.94116145e-01  1.41399577e-01 -2.33055189e-01
 -1.30840153e-01  3.98156345e-01  1.95742175e-01 -6.00118227e-02
 -1.08930856e-01  5.16548157e-02  1.77214652e-01  1.72563568e-02
  4.14762408e-01 -3.34159583e-01  1.52889356e-01  2.01821864e-01
  2.75342107e-01  1.58801988e-01 -1.83492899e-03  5.88556789e-02
 -3.72571886e-01 -7.64610991e-03  1.38916016e-01  2.80035585e-02
  1.90190911e-01  1.17364675e-01 -5.40022969e-01  1.50817692e-01
  5.32716960e-02  4.00866687e-01 -5.17221689e-02 -2.47968659e-01
 -6.75525069e-02 -2.75237739e-01 -1.45629704e-01 -2.16908053e-01
 -1.12812370e-01  1.13082901e-02  4.01443476e-03 -2.86989629e-01
 -1.55169114e-01  5.36546521e-02 -2.00623900e-01 -2.04456180e-01
  1.30903274e-01 -3.56566608e-01  1.14299268e-01 -2.26615191e-01
  1.91651080e-02 -2.36951709e-01  1.25940749e-02  5.62742949e-02
 -1.13346651e-01 -8.40880200e-02 -1.19778171e-01 -1.22218087e-01
 -2.36303791e-01  4.72115651e-02  8.94569755e-02  3.58418822e-01
 -3.25973898e-01  7.14536756e-02  2.30741892e-02  5.30670226e-01
 -1.01709016e-01 -1.30489290e-01 -2.71465689e-01 -3.29654813e-01
  1.32238358e-01 -1.61569435e-02 -8.99387747e-02 -1.18045397e-01
  1.93852544e-01  5.32474697e-01 -7.09841475e-02  3.43502343e-01
 -2.76954174e-01  1.69479996e-01  3.35284442e-01 -2.14259297e-01
  3.14300299e-01 -1.86413378e-01 -7.29835257e-02 -9.72963572e-02
 -1.56133603e-02  5.74299917e-02 -7.24369884e-02 -2.39865482e-03]"
Quantization aware training - label smoothing drops performance? comp:lite type:performance TFLiteConverter ModelOptimizationToolkit TF 2.13,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu18.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code
```python
# fine-tunining baseline model for few epochs
history = run_model(args, save_model_file)
    
# load trained weights for quantization aware training
model = setup_pretrained_model(args, save_model_file)
    
def apply_quantization_to_dense(layer):
    if isinstance(layer, tf.keras.layers.Dense):
        return tfmot.quantization.keras.quantize_annotate_layer(layer)
    return layer

annotated_model = tf.keras.models.clone_model(
    model,
    clone_function=apply_quantization_to_dense,
)

# Build Model
annotated_model.build((None, args.input_dim, args.input_dim ,3))

# Now that the Dense layers are annotated,
# `quantize_apply` actually makes the model quantization aware.
quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)

#     quant_aware_model.summary()
n_sample, train_ds, val_ds = load_data(args, args.input_dim)

# `quantize_model` requires a recompile.
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    args.lr / 10.0,
    decay_steps=100000,
    decay_rate=0.96,
    staircase=True)
    
# Compile the model
quant_aware_model.compile(optimizer=tf.keras.optimizers.Lion(lr=lr_schedule), 
              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=args.label_smoothing),
              metrics=['accuracy']
              )
```

### 3. Failure after conversion
model successfully converted into tflite with the example code flow of quantization aware training,
but the test accuracy for tflite model is so low (~80%) compared to .pb model file. (~97%)

Before adopting label smoothing, quantization aware trained model (tflite) was working as good as original .pb model.

Is there any relationship between label smoothing and quantization aware training process?

Thanks!
",False,"[-3.95450652e-01 -5.12709796e-01 -1.03350401e-01 -6.96353465e-02
  1.40232682e-01  4.54653986e-03  8.21567178e-02 -6.31659254e-02
 -3.72776806e-01 -1.40090123e-01 -5.98802492e-02  1.49997801e-01
 -1.69909030e-01  3.07501912e-01 -2.78354883e-01 -1.94134600e-02
  1.38155863e-01 -3.68446112e-02  2.32818618e-01 -2.94287562e-01
 -3.26073840e-02 -1.41935110e-01 -2.26081237e-01  3.43551397e-01
  2.83783674e-01  9.32132006e-02 -5.77054583e-02 -1.46680430e-01
  7.14269578e-02  2.26896614e-01 -5.58903180e-02  5.70204705e-02
  2.09612530e-02 -5.93532175e-02 -1.71063811e-01  2.52317905e-01
  5.80281857e-03 -1.28916828e-02 -2.66517043e-01 -1.54388905e-01
  5.34796193e-02  5.87627068e-02 -1.21840112e-01  7.22497106e-02
 -8.96849632e-02  9.20306891e-02  9.83719453e-02  3.21127027e-02
 -4.33158100e-01 -7.64812976e-02 -4.13107090e-02  1.93794407e-02
 -1.12064898e-01 -2.71332920e-01 -2.55556460e-02  2.22039670e-02
  3.14280272e-01  8.26439559e-02 -1.86136328e-02  2.55532153e-02
 -1.25219494e-01 -1.01888619e-01 -8.20360631e-02  1.28642246e-01
  3.80651876e-02  5.64388752e-01  1.81324720e-01 -2.05424428e-01
  2.13611573e-01 -2.34223157e-01 -1.14852011e-01 -2.86986232e-01
 -1.67728662e-01  1.61403626e-01 -2.16952786e-02 -5.83098084e-02
  6.04145378e-02  2.20305771e-01  2.48042971e-01 -2.22564302e-02
  1.58749998e-01 -3.25479507e-01  9.27146003e-02  5.26500419e-02
  3.15709505e-04 -3.39253545e-02  1.47135273e-01  4.53486219e-02
  2.40664676e-01 -2.48808682e-01  1.80379033e-01  3.46907794e-01
 -6.29296526e-02  1.10149860e-01  2.75552571e-01  7.54783079e-02
 -2.27837823e-04  1.67832717e-01  1.91970065e-01  4.38389480e-02
  1.12444200e-02 -3.16083699e-01 -3.17368031e-01 -8.56271386e-03
 -9.38618407e-02 -9.11754519e-02  2.01944441e-01  4.27048020e-02
  1.89891756e-01  9.64916199e-02  7.17504770e-02  6.93457127e-02
  8.13303608e-03 -2.40203571e-02  1.09558918e-01  4.83669899e-02
 -4.61663790e-02  2.00861588e-01  1.62052214e-01  5.72668672e-01
  1.02758296e-02 -2.48261184e-01  1.39429778e-01  1.43063277e-01
  3.68893623e-01  1.61666408e-01 -1.32059138e-02  2.28320621e-02
  7.23242536e-02  3.67026255e-02 -1.73442047e-02  2.68070042e-01
 -2.18636960e-01  1.59923092e-01  1.21005476e-01 -1.11387901e-01
 -8.92210975e-02  1.32580586e-02 -9.25236493e-02  1.09630994e-01
 -5.95045388e-01  2.10061893e-01 -8.03485811e-02 -2.77328908e-01
 -1.36610027e-02  2.01604038e-01 -3.35108280e-01  2.11888939e-01
 -1.11237772e-01  8.74789655e-02 -1.94728792e-01  1.45359114e-01
  1.70595348e-01  3.28773230e-01  3.00366938e-01  1.40817299e-01
  3.06092381e-01 -1.09436572e-01 -3.19981202e-03 -3.19387257e-01
 -1.37371615e-01  9.03845578e-02 -1.44694686e-01 -3.39620650e-01
 -1.45599097e-01  1.04249507e-01 -3.98065567e-01 -2.00961992e-01
  9.55837294e-02  1.39459953e-01 -1.92825377e-01 -3.95886123e-01
  1.11124888e-01  6.54779561e-03  1.24708720e-01 -1.15216479e-01
  2.37681031e-01 -3.23486656e-01  1.31409705e-01 -7.29223564e-02
  1.72561392e-01  1.64039612e-01  8.89786705e-02  2.01987922e-02
 -1.49568655e-02 -2.01654248e-03  2.37011075e-01  2.19137907e-01
 -3.09803963e-01  1.43128075e-02 -2.59324282e-01 -2.68603712e-01
  2.48632520e-01 -2.71064285e-02 -1.43047020e-01 -7.98293501e-02
  1.54078752e-01  2.06507251e-01 -1.22230098e-01  1.34545371e-01
 -8.06167051e-02  1.53340146e-01  8.86698067e-02 -9.76416022e-02
  1.30302429e-01 -2.59486288e-02 -1.61477953e-01 -4.62642878e-01
 -4.31107789e-01  9.34113935e-02  9.33373421e-02 -3.35437953e-01
  6.69355392e-02 -2.76741177e-01 -1.52304888e-01  1.22271776e-01
 -2.05900684e-01 -1.67488649e-01 -3.39013577e-01  2.35764042e-01
 -2.29277268e-01  8.28356519e-02  2.42407471e-02 -1.86582655e-01
 -1.85988441e-01 -1.57346785e-01 -1.61409795e-01  3.70376587e-01
 -1.95417672e-01  1.69744357e-01  3.36539894e-02  1.33686230e-01
  4.43763435e-01  2.41985232e-01  2.04503328e-01 -3.40553045e-01
 -3.29277366e-02 -1.66206270e-01 -3.04946721e-01  1.71485126e-01
 -1.31328091e-01 -8.95499736e-02  1.58019122e-02 -4.13104258e-02
  1.29534364e-01  2.19824672e-01 -2.92327236e-02 -1.93802610e-01
 -7.32381791e-02  2.44833112e-01 -2.31952325e-01 -1.11436576e-01
  2.28869855e-01  1.74369887e-01  1.33722365e-01 -7.86723047e-02
  4.82115187e-02 -1.06186867e-01  9.08628255e-02 -1.58947855e-01
  2.51889408e-01  4.44690168e-01 -4.78107445e-02  6.36610389e-01
  2.82763183e-01  2.80110031e-01 -2.80043304e-01  3.26148570e-01
 -1.11009777e-01 -1.53777286e-01  7.95621425e-02 -3.10728759e-01
  8.45262632e-02 -2.13487491e-01  1.96424246e-01 -1.15104197e-02
  4.48457927e-01  8.34981129e-02 -2.24732146e-01  2.89846569e-01
 -3.22422758e-03  3.77044916e-01 -5.43335199e-01  3.42324018e-01
  2.92162560e-02 -1.51370198e-01 -1.30970195e-01 -3.84981096e-01
 -3.62219475e-02  4.21918444e-02 -5.92543744e-02  8.81430209e-02
 -5.28651625e-02  1.04977719e-01  1.64326886e-03  6.94194883e-02
  3.56891364e-01  3.13383974e-02  1.88753590e-01 -2.96047091e-01
 -2.18391493e-02  1.69816464e-01  1.37017593e-01 -1.62459090e-01
 -5.76777384e-03 -4.74755652e-02  1.60724357e-01 -2.03536395e-02
  1.77936777e-01 -1.69126272e-01  5.09342961e-02  2.93499947e-01
  1.62990481e-01  2.36276329e-01 -9.54587609e-02  8.66624564e-02
 -2.78879017e-01  3.95881861e-01 -4.92288023e-02  3.63271162e-02
  8.73572156e-02  9.73936468e-02 -3.62018526e-01 -7.24456012e-02
 -8.97080451e-02  1.00127503e-01  6.27454296e-02 -1.83974147e-01
 -2.52308309e-01 -1.68113425e-01 -5.54468483e-02 -1.73013777e-01
 -3.25839698e-01  1.62975729e-01  4.40789945e-02 -1.13765970e-01
 -1.43505648e-01  4.49781656e-01  2.42207404e-02 -1.46105915e-01
  6.17174022e-02 -2.45283499e-01 -2.92041227e-02 -2.56461173e-01
 -1.82811946e-01 -2.24912792e-01  2.46199489e-01  2.00737685e-01
  7.54929930e-02  6.94354028e-02 -8.68491232e-02  1.02224216e-01
 -1.32775024e-01  6.22737482e-02 -4.94511239e-02  4.63543504e-01
 -2.82121867e-01 -1.03039458e-01  6.63805455e-02  4.39857900e-01
 -2.49565676e-01 -1.08020470e-01 -4.50958043e-01 -1.71066523e-01
  1.02711394e-01 -1.07486300e-01 -9.40323994e-02 -3.07993144e-01
 -2.78768186e-02  6.06580794e-01 -1.07895546e-02  3.24816525e-01
 -3.06554914e-01  1.27156705e-01  3.08565527e-01 -3.09641421e-01
  1.64870232e-01 -4.92831469e-02 -1.06308848e-01 -2.40931436e-02
 -1.58214152e-01 -4.81567942e-02  1.47020876e-01  1.46948040e-01]"
Can't run bert_vocab_from_dataset without TypeError: Tensor is unhashable stat:awaiting response type:support stale comp:apis TF 2.13,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This is the code from you [manual ](https://www.tensorflow.org/text/guide/subwords_tokenizer#generate_the_vocabulary)and I really don't understans that I get this error. Why is it? 

If I add 
tf.compat.v1.disable_eager_execution()
tf.compat.v1.disable_v2_behavior()

I get
RuntimeError: input_dataset: Attempting to capture an EagerTensor without building a function.

### Standalone code to reproduce the issue

```shell
data = tf.data.TextLineDataset([SENTENCES_PATH, TAGS_PATH])

from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab

tokens = bert_vocab.bert_vocab_from_dataset(
    data,
    # The target vocabulary size
    vocab_size = 50000,
    # Reserved tokens that must be included in the vocabulary
    reserved_tokens=[""[PAD]"", ""[UNK]"", ""[START]"", ""[END]""],
    # Arguments for `text.BertTokenizer`
    bert_tokenizer_params=dict(lower_case=True),
    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`
    learn_params={},
)
```


### Relevant log output

```shell
TypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.
```
",False,"[-5.19645214e-01 -4.96054202e-01 -1.96695685e-01  1.59712359e-01
  2.48149842e-01 -3.97049397e-01 -1.34548366e-01 -9.33538228e-02
 -3.23010564e-01 -3.75625998e-01  1.43882439e-01 -1.96377799e-01
 -2.10663229e-01 -3.45861800e-02 -1.10289112e-01  2.66848832e-01
 -2.36649960e-01 -6.94398433e-02  3.36319208e-01  2.25951031e-01
 -1.54715613e-01 -7.44680315e-02 -4.50471222e-01  2.11799994e-01
  6.69581071e-02  1.65195689e-01 -2.78816372e-01 -5.98026365e-02
 -3.41749750e-04  2.69302011e-01  5.37532747e-01 -5.25028035e-02
 -1.21793516e-01  2.63616536e-02  1.36428513e-03  1.60912439e-01
 -2.78753370e-01 -2.43589357e-01 -2.72830248e-01  7.66678378e-02
  1.30785674e-01  2.46292293e-01  2.39273712e-01 -2.07389444e-01
  1.97748005e-01 -6.39755428e-02 -8.56926292e-02 -1.96149528e-01
 -1.25300899e-01 -2.24991664e-01 -7.91654140e-02 -3.60189490e-02
 -5.27154386e-01 -4.24410164e-01 -2.36298531e-01 -1.37906045e-01
  1.35237724e-01  1.21319763e-01 -4.79111895e-02  1.82803065e-01
 -2.30892114e-02  8.71772468e-02  6.91169575e-02 -9.53293592e-02
 -6.54152781e-03  1.75156027e-01  3.17073226e-01 -1.09025054e-01
  6.06837571e-01 -1.64690822e-01  2.80967712e-01 -8.80357102e-02
 -3.91098320e-01  9.40538347e-02  7.95746595e-02  1.76369131e-01
  7.90057331e-02  1.29393980e-01  4.28683490e-01 -1.32749856e-01
 -2.07196288e-02 -2.73089707e-01 -9.64915603e-02 -2.25881755e-01
  3.66500229e-01 -1.00170098e-01  3.81327033e-01  1.59398183e-01
  4.42827106e-01 -3.28230381e-01  4.91032600e-01  5.52697718e-01
 -2.79395152e-02 -3.94751020e-02  5.52590370e-01  2.06069499e-02
  2.73687243e-01  2.75682241e-01 -1.99050792e-02 -1.11104436e-01
 -1.49288908e-01 -2.64314801e-01 -7.43196905e-02 -1.93793830e-02
 -9.16898698e-02  6.39085695e-02  2.28239283e-01 -2.02706605e-01
  2.38629669e-01 -1.26325607e-01  2.84609854e-01  1.03825750e-02
  3.29594731e-01 -1.90386653e-01  1.97462849e-02 -2.64761806e-01
 -2.33564839e-01 -3.27324681e-02  6.25180304e-02  8.97852421e-01
 -6.23481069e-03 -1.28287643e-01  2.78545879e-02  1.66253805e-01
  3.65681529e-01  2.90758967e-01 -8.22390020e-02 -4.80121970e-02
  3.84732336e-02 -1.15082696e-01  2.16919497e-01 -1.02070041e-01
 -1.53419733e-01  2.65746415e-01 -9.66979638e-02 -6.41951151e-03
 -2.47077242e-01 -1.31998003e-01 -2.17366382e-01 -1.70010448e-01
 -2.72215426e-01  3.05595752e-02 -1.10202335e-01 -5.34055829e-01
  1.19447991e-01  7.20930845e-02  4.35163788e-02  2.66206741e-01
 -1.64652005e-01  1.09397143e-01 -4.71372381e-02  2.00613692e-01
 -2.09193215e-01  3.52961779e-01  1.81455433e-01  8.72627422e-02
  4.32465822e-01 -6.91851601e-02 -4.03210111e-02 -5.66370606e-01
 -4.33850959e-02  3.77441645e-01 -3.72147202e-01 -2.34917000e-01
  1.17946364e-01  1.77598819e-01 -3.67633760e-01 -1.43166721e-01
  1.34972781e-01  4.67547953e-01 -2.01553881e-01 -1.48832619e-01
 -4.31448668e-02  1.01784579e-02  7.66274557e-02  5.07910103e-02
  2.55390525e-01 -6.15934968e-01 -3.09423536e-01  3.59004676e-01
 -3.08004543e-02  2.49875009e-01  1.07254028e-01  1.43534586e-01
  1.14428118e-01  1.39138699e-01  7.88373351e-02  1.79622650e-01
 -1.92769974e-01 -1.72367338e-02 -3.47632408e-01 -1.68831378e-01
  4.47695255e-01 -9.59804356e-02 -1.99664429e-01  8.53040144e-02
  1.40221044e-01 -2.73638964e-01  9.53339785e-02 -3.97306159e-02
 -1.07466102e-01 -2.00768322e-01 -1.57578558e-01 -9.88147631e-02
  6.70042410e-02 -3.89164358e-01 -1.49805129e-01 -3.08619082e-01
 -2.49989361e-01  1.01642758e-01 -1.09455641e-02 -3.59877974e-01
  1.78556144e-01 -4.81814928e-02 -1.94831684e-01  7.02965111e-02
  1.62978068e-01  3.01187020e-02 -1.13009855e-01  1.84155107e-01
  7.59877190e-02 -1.36728272e-01  8.00105929e-03 -4.17551637e-01
 -2.68495053e-01  8.55935812e-02 -3.61671329e-01  1.79856539e-01
 -1.62979588e-01  1.96801543e-01  1.52564868e-01  1.42522305e-01
  3.89881134e-01  2.18687519e-01  4.66525614e-01 -3.30227196e-01
 -1.01444319e-01 -1.45971775e-01 -8.24160725e-02  1.58268988e-01
 -5.71463287e-01 -1.22142747e-01 -4.84878570e-03  1.08597893e-03
  2.60950744e-01  3.01434249e-01 -2.96235561e-01 -1.25282496e-01
 -4.41671133e-01  2.69839615e-01 -2.12466225e-01  7.39955381e-02
  3.89518082e-01  6.20719530e-02  5.63169241e-01  2.59742469e-01
  1.68209016e-01  3.09121013e-01  1.32498652e-01 -1.97622985e-01
  4.44539189e-01  2.75597665e-02  2.47652847e-02  3.16121608e-01
  3.24288487e-01  3.57547581e-01 -3.19137990e-01  6.45843029e-01
  1.81987539e-01  7.46542662e-02  9.38027501e-02 -4.00411844e-01
  5.92393637e-01 -2.96122313e-01 -1.75138563e-02 -6.89647906e-03
  3.10961992e-01  8.70816559e-02 -2.28904486e-01 -1.40738562e-02
  3.22972894e-01  3.26344967e-01 -5.35116255e-01 -4.48702611e-02
  7.39362165e-02 -2.62981951e-01 -1.33379564e-01 -4.28002298e-01
 -1.40358537e-01  1.70751378e-01 -3.27885866e-01  3.47591102e-01
  1.19555347e-01 -1.07930288e-01 -2.51448750e-01  1.21768340e-01
  8.96593630e-02 -2.57529050e-01  7.99093843e-02  4.86542791e-01
 -2.21072003e-01 -6.44624308e-02  4.59073037e-01 -3.12305391e-01
 -1.59493744e-01 -1.42753586e-01  5.94617784e-01  1.33467197e-01
  4.62115586e-01 -4.05143917e-01  8.76574963e-02 -9.86161605e-02
  1.89304985e-02  5.99285543e-01  1.56017719e-02 -6.39661551e-02
 -3.35429758e-01  7.50904441e-01  4.92041588e-01 -1.95750937e-01
  2.57738948e-01 -1.34867147e-01 -3.33658397e-01  1.11928046e-01
  1.31245971e-01 -6.41493965e-03 -2.57669762e-02 -5.46287894e-01
 -1.83761530e-02  3.27488542e-01 -1.36353046e-01 -8.05972889e-02
 -3.22761774e-01  1.00183383e-01 -1.60114124e-01 -1.61917746e-01
 -3.98471892e-01  1.97481543e-01  6.37025386e-02 -2.95177817e-01
 -1.82669267e-01  2.44757161e-03  7.92593285e-02 -3.21253836e-01
 -1.08675003e-01 -3.86013687e-01  4.21957374e-01  5.88184536e-01
 -3.67858142e-01  1.12581916e-01 -1.60915554e-01  7.05241412e-02
 -5.87666929e-01 -6.73952922e-02 -5.49210012e-02  3.26177299e-01
 -2.76823714e-02 -1.22256711e-01  3.73649240e-01  2.72012889e-01
 -1.91282809e-01  1.59294158e-01 -3.63425136e-01 -3.89071479e-02
  4.22907472e-01 -3.04268837e-01 -1.38585597e-01 -3.16957027e-01
  2.81053215e-01  3.20179194e-01 -1.35562927e-01  2.42719859e-01
 -3.40391040e-01  3.57845992e-01  5.06617427e-01 -2.71992803e-01
 -3.74575973e-01  2.31429517e-01  5.91575801e-02 -1.44466355e-01
  1.76804543e-01 -1.06664032e-01  2.28852630e-01 -4.91318367e-02]"
What is generate_vocab func?  stat:awaiting response type:support stale,"You referenced in [this ](https://www.tensorflow.org/text/guide/subwords_tokenizer#generate_the_vocabulary)tutorial to [generate_vocab.py](https://github.com/tensorflow/text/blob/master/tensorflow_text/tools/wordpiece_vocab/generate_vocab.py), if I understand correct, as a ready to prod highlevel func that I can use. But I don't have it in downloaded repository of tensorflow-text. 

Can you explain me a bit more how should be my attitude to this reference? 
",False,"[-0.59357715  0.14372952 -0.5461404   0.1599319  -0.33708712 -0.24483424
  0.1820159  -0.01404774 -0.18930897  0.12530777  0.00127452 -0.2181884
  0.44228935 -0.133961    0.17308946  0.34244907  0.07681865 -0.07638746
 -0.06123742  0.06742296  0.34250176  0.17648742  0.06052422  0.06032178
 -0.10603681 -0.06253503 -0.31608278 -0.03548268  0.23206155  0.2122013
  0.34763232  0.14726768  0.06121039 -0.1279563  -0.10076948  0.20757113
 -0.17662618 -0.10740209  0.06301194 -0.07740764 -0.20429087 -0.13699496
 -0.02867292  0.13593909 -0.09321228  0.08328258  0.05102601  0.49816978
 -0.14067712  0.08518851 -0.33634365 -0.24075298 -0.49156007 -0.06022719
  0.07953508  0.05170266 -0.1300103  -0.15415509 -0.06035119 -0.23575145
 -0.06027475  0.2763831   0.29272318  0.21041164 -0.29793072  0.25275493
 -0.02423565  0.01751795  0.489454   -0.19770527 -0.04825126 -0.06161669
 -0.3234566   0.10455871  0.07278555  0.10951991 -0.44853663 -0.08150311
 -0.07375543 -0.00292945  0.14082949  0.04830548  0.01201516 -0.20477986
  0.3182819   0.28368044  0.43075395  0.13635221  0.06391464  0.14022171
  0.02130479  0.04523715  0.01794744 -0.20533594 -0.05580686  0.02401704
 -0.03319291 -0.16737412 -0.01112296 -0.15948988 -0.24126479 -0.20520507
  0.06643958 -0.1236359   0.04267197  0.0224522   0.09448325  0.19296655
  0.08270408 -0.20827077 -0.10569699  0.06895671  0.14332162 -0.15382665
  0.03857861  0.33391532 -0.26009995 -0.2731834   0.0494485   0.5029043
  0.15373373  0.0067941   0.0147274   0.27170402  0.37763536 -0.11087789
 -0.08619635  0.18803221 -0.00078993  0.21825197  0.39407405  0.24201936
  0.28357702  0.2821894  -0.05620981 -0.28675225 -0.24109402 -0.02845505
 -0.10025766  0.09853545 -0.1031433   0.5924503   0.05499604 -0.45535573
 -0.18519664  0.08036372 -0.1753098   0.18377642  0.12338354 -0.14003581
  0.23322752 -0.10194613 -0.1579538   0.30206192  0.18678674  0.24025165
  0.266107    0.03376436 -0.02057805 -0.50463706 -0.02825147  0.04260847
  0.0391237   0.15149684 -0.20728502 -0.11827545 -0.3457105   0.16917664
 -0.11758357  0.26890785  0.03395829  0.24880835 -0.18513353 -0.01273994
  0.29873908  0.19611391 -0.12378234 -0.39413443 -0.14893165  0.18164659
  0.2016214   0.25872016  0.17298153 -0.1617328  -0.08987942  0.01314861
 -0.00422321  0.09298563 -0.34578457 -0.14319596 -0.2843802   0.14841732
 -0.03429    -0.05725063 -0.11190428  0.00087802  0.3878398  -0.3479271
  0.20526528  0.02795633 -0.35297066 -0.20133738 -0.10402215  0.10264371
  0.31281263  0.07952362 -0.35867065 -0.16602573 -0.3052305   0.2892177
 -0.23581302 -0.11301417  0.02047759  0.01732113 -0.01399988  0.07398882
  0.1060481   0.07217851 -0.19929585 -0.01335164  0.17331389 -0.19314459
 -0.05637981 -0.23625928 -0.024916   -0.16992718 -0.22851697  0.23779768
  0.14711738  0.16900626 -0.03668505 -0.35641265 -0.00091945 -0.13827702
 -0.12661001  0.09595166 -0.02164569 -0.14920688  0.03775126 -0.06870183
 -0.01741239  0.07692422  0.03823854  0.13540635 -0.32613903 -0.12557684
 -0.20099825 -0.12794149 -0.22555557 -0.06115115 -0.13359645 -0.32912755
  0.46764576  0.472319   -0.1884296   0.2487193   0.025512   -0.05168569
  0.02456771  0.02902197  0.23306811  0.137793    0.1979757   0.3875199
  0.32159835  0.04847763 -0.62453794 -0.02292863 -0.24184826  0.06993759
 -0.16330066 -0.23599371  0.1186059   0.03149635  0.31874925 -0.17228872
 -0.04066882 -0.32133088 -0.11521833 -0.03556826  0.0867887  -0.19167612
  0.06173431  0.11715694 -0.50177747 -0.12109306 -0.19642985 -0.24221879
 -0.15714106 -0.00628517 -0.06693258  0.25722477  0.27220207 -0.2736854
 -0.06583749 -0.00681327  0.11959723 -0.10210176 -0.09755436  0.04126555
 -0.09059144  0.04692862  0.3136259  -0.05810603 -0.27087283  0.10545303
  0.4257565   0.09395726  0.3663584  -0.18366632  0.05483187  0.12972814
 -0.06668054  0.6148602  -0.09483385  0.24665302 -0.1119109   0.6540085
  0.22929338  0.19882913  0.06324866  0.02892526 -0.25242746  0.255725
 -0.18202224  0.13634339 -0.17952627 -0.37605885 -0.03260282 -0.1314487
  0.15118876 -0.03811078 -0.02023572  0.06231322 -0.268194    0.13163668
 -0.1001647   0.2505765  -0.06787267 -0.25844115  0.03890334  0.13655388
  0.0758747  -0.5635126  -0.19643317 -0.3348353  -0.12158982  0.24014434
 -0.29332858  0.12333905  0.04426958  0.36881307 -0.68205523 -0.29874414
  0.1624746   0.01728471  0.07273567 -0.13184687 -0.01342914  0.37731844
 -0.17305031  0.2638179  -0.2540276  -0.3221717   0.1661346  -0.10232656
 -0.17473045 -0.4664748   0.05234116  0.12163696 -0.17567731  0.3189534
 -0.02825422  0.28264257  0.31503078  0.2401306  -0.07746762  0.10959111
  0.3206538   0.06491979  0.15913382  0.23847976  0.05039329 -0.21804091]"
Float16 mixed precision training stat:awaiting response type:support comp:ops TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

Python 3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A6000

### Current behavior?

Enabled float16 training by setting the mixed precision policy, but why I still need to manually cast the y tensors to float16 before calculating the loss?


Error when no manual cast the tensor:
![image](https://github.com/tensorflow/tensorflow/assets/25906607/46c5efcb-f2c2-4b7a-9483-46656009071b)


### Standalone code to reproduce the issue

```shell
Confidential.
```


### Relevant log output

_No response_",False,"[-0.4023255  -0.2082575  -0.31468302 -0.13000824  0.01855385 -0.5462979
  0.09420341  0.20736709 -0.4274038  -0.4670889   0.13994421 -0.08316765
 -0.07207946  0.08759651 -0.3018803   0.20052862 -0.07380398  0.14313795
  0.01103642 -0.16774192 -0.23104179 -0.15441352 -0.12650058  0.3084931
  0.30891943  0.33662778 -0.35355145  0.11407443 -0.10965785  0.31980643
  0.11418964  0.14438277  0.15538771  0.01001993 -0.12515868  0.25327057
 -0.25793242 -0.3235352  -0.3831006   0.03893121 -0.15914825  0.19213162
  0.2556336  -0.01619603  0.19570576  0.14112726 -0.34516144  0.00638951
  0.00987081 -0.06568987 -0.16394217  0.1369894  -0.41345122 -0.41944438
 -0.12041881 -0.10452054  0.05292473 -0.28220996 -0.09654424 -0.06085166
 -0.00362363 -0.04500001  0.03503251 -0.12084309  0.03309882  0.14844316
  0.40648443 -0.05049593  0.35557687 -0.22968067  0.07293983 -0.03203351
 -0.3953285   0.20034447 -0.01552387  0.03301867 -0.15561254  0.15033722
  0.37003362 -0.18011078  0.167445   -0.33619267 -0.03277098 -0.3152302
  0.30927867 -0.10984492  0.5924201   0.29506397  0.32239613 -0.10708883
  0.5178895   0.41926548 -0.46257058  0.0588446   0.50508404  0.21040112
  0.1975794  -0.05953269 -0.20801955 -0.1342775   0.02929531 -0.23057942
 -0.31014144 -0.08807905 -0.2896912  -0.05956079  0.14905685 -0.03990349
  0.21612856  0.0689126   0.14289407 -0.01158789 -0.00198955 -0.02388942
 -0.19485942  0.08413945 -0.0576812   0.15533377  0.17251053  0.7402675
  0.06259252 -0.06279258 -0.2012977   0.25062674  0.5385166   0.06302813
 -0.08915816  0.10931656  0.11822531 -0.03933711  0.17979154 -0.12797995
 -0.09141869  0.1591583  -0.02690528  0.08819802  0.05851307  0.00952154
 -0.38883308 -0.31204918 -0.07833853  0.10116218 -0.11901376 -0.68091446
  0.11910866  0.18907326 -0.39320272  0.43494317  0.11402756 -0.04553064
  0.08396202  0.05236847 -0.0516611   0.48183593 -0.06345978  0.04595276
  0.52000237  0.03264062  0.02566394 -0.5397933  -0.16813737  0.25766566
  0.01936417 -0.28201437  0.24949951  0.12435252 -0.39196873 -0.24945578
 -0.13673672  0.58635294 -0.20308477 -0.23897648 -0.02305537  0.15067413
  0.16839191  0.01591079  0.03336903 -0.5886513  -0.14914864  0.2508273
  0.09310751  0.0273091   0.20658931  0.15230063  0.07751353  0.14250585
 -0.06480419  0.1720835  -0.33352807 -0.06923313 -0.5463269  -0.16487296
  0.38704687 -0.13655171 -0.17233375  0.03430102  0.35270935 -0.05923259
  0.1068619   0.02139825 -0.31342548 -0.06999815 -0.08027907 -0.0914748
  0.25912404 -0.27008384 -0.12204371 -0.6679916  -0.12169583  0.4516876
 -0.04499479 -0.6230289   0.11590564  0.07916424 -0.36187878  0.2852594
 -0.03997539  0.07152404 -0.1811314   0.16102618  0.11029112 -0.19164315
  0.0362337  -0.4009254  -0.27698076  0.3425932  -0.51117265  0.20156994
 -0.07469618  0.10214148  0.26947784 -0.03576704  0.43681985  0.2033444
  0.26942942 -0.13843286 -0.3278099   0.04559476 -0.07223507 -0.04373643
 -0.49622422 -0.09436412  0.19261855 -0.07120241  0.46026888  0.4848832
 -0.01275785 -0.03428489 -0.23273164  0.06055643 -0.30451652  0.00347787
  0.36150628  0.10632969  0.50619483  0.0601777   0.463017    0.18221796
  0.18934642 -0.5241971   0.35252064  0.14285356  0.10952224  0.62380844
  0.32628754  0.16774681 -0.3778348   0.6120641   0.07487672 -0.40924507
  0.2861511  -0.2921757   0.71331906 -0.26100218  0.19984251 -0.32852006
  0.29521906  0.02732448 -0.18682724  0.03549577 -0.00716575  0.01220229
 -0.33755162 -0.05066641 -0.28393897 -0.2501205   0.00286624 -0.43003333
 -0.04247238  0.12561607 -0.2493135   0.21685925 -0.07689501  0.1805526
 -0.21824446  0.25203723  0.21882415 -0.17761753  0.07189784  0.14616972
 -0.32760212  0.25855744  0.35100582 -0.33990562 -0.12782171 -0.15976116
  0.40699187  0.38788164  0.28513598 -0.27045828  0.22891696 -0.12590927
 -0.18960074  0.39468995 -0.06347375  0.1633883  -0.20795652  0.48951894
 -0.03584562  0.04962407  0.00929115 -0.07246473 -0.36923802  0.3532359
  0.04725508  0.08085977 -0.01710681 -0.50698805 -0.01228669  0.10242517
 -0.0809401  -0.07486669 -0.09094099 -0.2962072   0.20518464 -0.24646676
 -0.37077013  0.33051953 -0.02697954 -0.34045166 -0.11877012 -0.13177902
 -0.03064384 -0.10657632 -0.03008283 -0.39239007  0.44865113  0.6988976
 -0.06532259  0.04494619  0.13748997  0.04462198 -0.13355124  0.06953073
 -0.11183909  0.33991432  0.10654847 -0.07637179  0.35703468  0.1406413
 -0.5385896   0.17907584 -0.5030776   0.16557471  0.2982946  -0.23080695
 -0.08520057 -0.53373575  0.07227269  0.194681   -0.03141066  0.4172256
 -0.34533876  0.43198943  0.7213271  -0.32201415 -0.23488024  0.10900369
  0.14427519 -0.08261123  0.05562819 -0.20215717  0.03435321  0.17216301]"
I am not able to install the correct version for typing-extensions stat:awaiting response type:build/install stale subtype:windows TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Their are different support problems when I install tensorflow

I installed it by entering
`pip install tensorflow`

The output was

> Collecting tensorflow
  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/9e/b8/ed5f794359d05cd0bffb894c6418da87b93016ee17b669d55c45d1bd5d5b/tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata
  Downloading tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)
Collecting tensorflow-intel==2.13.0 (from tensorflow)
  Obtaining dependency information for tensorflow-intel==2.13.0 from https://files.pythonhosted.org/packages/2f/2f/3c84f675931ce3bcbc7e23acbba1e5d7f05ce769adab48322de57a9f5928/tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl.metadata
  Downloading tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)
Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)
Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Collecting flatbuffers>=23.1.21 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for flatbuffers>=23.1.21 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata
  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)
Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)
Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/d1/93/0f4cf5058095d749d464e4f770d2bf339930e5f3374331f0d2fa6ddfbf28/h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata
  Downloading h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)
Collecting libclang>=13.0.0 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/02/8c/dc970bc00867fe290e8c8a7befa1635af716a9ebdfe3fb9dce0ca4b522ce/libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata
  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)
Collecting numpy<=1.24.3,>=1.22 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached numpy-1.24.3-cp311-cp311-win_amd64.whl (14.8 MB)
Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Requirement already satisfied: packaging in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)
Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/14/ff/10f746c03212fe48576b2c0f5ada73c3400b6d90f769728c4f07656d8b27/protobuf-4.24.2-cp310-abi3-win_amd64.whl.metadata
  Downloading protobuf-4.24.2-cp310-abi3-win_amd64.whl.metadata (540 bytes)
Requirement already satisfied: setuptools in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from tensorflow-intel==2.13.0->tensorflow) (65.5.0)
Requirement already satisfied: six>=1.12.0 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)
Collecting termcolor>=1.1.0 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)
Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)
Collecting wrapt>=1.11.0 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached wrapt-1.15.0-cp311-cp311-win_amd64.whl (36 kB)
Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/8d/58/ede228c07bdf3780c5332660c89f3c7a37fe8bfb9bd73a97ad2614420bd4/grpcio-1.57.0-cp311-cp311-win_amd64.whl.metadata
  Downloading grpcio-1.57.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)
Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)
Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata
  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata
  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)
Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.13.0->tensorflow)
  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)
Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for wheel<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/b8/8b/31273bf66016be6ad22bb7345c37ff350276cfd46e389a0c2ac5da9d9073/wheel-0.41.2-py3-none-any.whl.metadata
  Using cached wheel-0.41.2-py3-none-any.whl.metadata (2.2 kB)
Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata
  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)
Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)
Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata
  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)
Requirement already satisfied: requests<3,>=2.21.0 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)
Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/da/61/6e9ff8258422d287eec718872fb71e05324356722ab658c8afda25f51539/tensorboard_data_server-0.7.1-py3-none-any.whl.metadata
  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl.metadata (1.1 kB)
Requirement already satisfied: werkzeug>=1.0.1 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.3.7)
Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata
  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)
Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)
Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached rsa-4.9-py3-none-any.whl (34 kB)
Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Obtaining dependency information for urllib3<2.0 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata
  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)
     ---------------------------------------- 48.4/48.4 kB 1.2 MB/s eta 0:00:00
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.0)
Requirement already satisfied: idna<4,>=2.5 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.7.22)
Requirement already satisfied: MarkupSafe>=2.1.1 in c:\users\wadhw\appdata\local\programs\python\python311\lib\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.3)
Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)
  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)
Using cached tensorflow-2.13.0-cp311-cp311-win_amd64.whl (1.9 kB)
Using cached tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl (276.6 MB)
Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)
Using cached grpcio-1.57.0-cp311-cp311-win_amd64.whl (4.3 MB)
Using cached h5py-3.9.0-cp311-cp311-win_amd64.whl (2.7 MB)
Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)
Using cached libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)
Using cached protobuf-4.24.2-cp310-abi3-win_amd64.whl (430 kB)
Using cached tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)
Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)
Using cached Markdown-3.4.4-py3-none-any.whl (94 kB)
Using cached tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)
Using cached wheel-0.41.2-py3-none-any.whl (64 kB)
Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)
Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)
Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, markdown, keras, grpcio, google-pasta, gast, cachetools, absl-py, rsa, pyasn1-modules, opt-einsum, h5py, astunparse, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow
  Attempting uninstall: urllib3
    Found existing installation: urllib3 2.0.4
    Uninstalling urllib3-2.0.4:
      Successfully uninstalled urllib3-2.0.4
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.7.1
    Uninstalling typing_extensions-4.7.1:
      Successfully uninstalled typing_extensions-4.7.1
  Attempting uninstall: numpy
    Found existing installation: numpy 1.25.2
    Uninstalling numpy-1.25.2:
      Successfully uninstalled numpy-1.25.2
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
pydantic 2.3.0 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.
pydantic-core 2.6.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.
Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.57.0 h5py-3.9.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.2 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 typing-extensions-4.5.0 urllib3-1.26.16 wheel-0.41.2 wrapt-1.15.0

Due to this error, I installed typing-extensions 4.6.1 by
`pip install typing-extensions==4.6.1`

I got the output as

> Collecting typing-extensions==4.6.1
  Obtaining dependency information for typing-extensions==4.6.1 from https://files.pythonhosted.org/packages/82/ed/8ccf53a0ed10bf8fc8877b5833b40f5f99093cadfe6632b8892f74aead0f/typing_extensions-4.6.1-py3-none-any.whl.metadata
  Downloading typing_extensions-4.6.1-py3-none-any.whl.metadata (2.8 kB)
Downloading typing_extensions-4.6.1-py3-none-any.whl (31 kB)
Installing collected packages: typing-extensions
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.5.0
    Uninstalling typing_extensions-4.5.0:
      Successfully uninstalled typing_extensions-4.5.0
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.6.1 which is incompatible.
Successfully installed typing-extensions-4.6.1

Then, I again reinstalled typing-extensions 4.5.0 by
`pip install typing-extensions==4.5.0`

Then I again received the error

> Collecting typing-extensions==4.5.0
  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)
Installing collected packages: typing-extensions
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.6.1
    Uninstalling typing_extensions-4.6.1:
      Successfully uninstalled typing_extensions-4.6.1
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
pydantic 2.3.0 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.
pydantic-core 2.6.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.
Successfully installed typing-extensions-4.5.0

Please let me know how to fix this error

### Standalone code to reproduce the issue

```shell
Just use 
pip install tensorflow 
and you may receive this error
```


### Relevant log output

_No response_",False,"[-0.38593858 -0.26795113  0.01085289  0.03847848  0.30375013 -0.13873583
 -0.5048824   0.12217051 -0.23698497 -0.35020834 -0.00245294 -0.0497104
 -0.22924423  0.06159779 -0.2325405   0.49879122 -0.4321528   0.1526792
  0.3235184   0.19001089 -0.37917253 -0.1647577  -0.34741807  0.14977461
  0.22065148  0.01180704 -0.13952348  0.08363166  0.07924529  0.34800246
  0.36145175  0.00564282 -0.03091106  0.07310018  0.2716156   0.0991598
 -0.1287061  -0.14636904 -0.40913963 -0.03881278 -0.0393564   0.0382317
 -0.06718914 -0.14189482  0.03786135 -0.02511487  0.01651949 -0.250574
 -0.19006516 -0.01027908 -0.09779188  0.05021378 -0.27670455 -0.37838337
 -0.13283345 -0.20277278 -0.06519824  0.07992715 -0.03441388  0.27909335
  0.08775166  0.05089554  0.12550211 -0.12399571 -0.13741808  0.05903349
  0.21482593 -0.24582532  0.41938096 -0.4842384   0.20779926 -0.06977169
 -0.19494417  0.11428621 -0.06036417 -0.02190061  0.02398383  0.13093162
  0.37450355 -0.21768758 -0.0206656  -0.4427319  -0.13785043 -0.08912572
  0.14683296 -0.09896566  0.19187024  0.09980252  0.40678218 -0.30928713
  0.5425628   0.32041666  0.14800754  0.09821376  0.54875016  0.08113635
  0.18667164  0.466146   -0.08507083 -0.09466039 -0.08868925 -0.29626477
 -0.04655679 -0.08977792  0.02644316 -0.42839614  0.3272399   0.01844099
  0.12952018 -0.07955325  0.23974577 -0.0516326   0.18538286 -0.169395
 -0.25225762  0.00879943 -0.20539297  0.00913043 -0.02253625  0.677497
  0.14743906 -0.13292122 -0.0391408  -0.02177037  0.27753776  0.0020329
 -0.3874166   0.00996982  0.13991043  0.25527114  0.1547935   0.2051555
  0.14952123  0.05841467 -0.01305003 -0.01714662 -0.2283825  -0.18919909
 -0.3763049  -0.3251415  -0.2676099   0.13064794 -0.16123496 -0.64970005
 -0.03778509  0.09242739 -0.18140562  0.22440864 -0.25799987  0.19826175
 -0.04893296  0.33775276 -0.13907543  0.2010323   0.26868296 -0.08628087
  0.4967182  -0.05603855 -0.1608311  -0.5896964   0.03086197  0.41317043
  0.02998897 -0.11169332  0.10361105  0.15245605 -0.4660847  -0.09034828
  0.34932327  0.62589765  0.06994125 -0.1639837   0.1626822   0.02696898
  0.27941     0.03628426  0.40356663 -0.33086962 -0.06903866  0.40649417
  0.19327617 -0.10067081  0.07278379  0.04909798  0.04595379  0.0454409
  0.15148725  0.22274445 -0.1140568   0.12405375 -0.45443672 -0.15155938
  0.3351338  -0.13100904 -0.30784404  0.05912544  0.07446248 -0.0459275
  0.16260725 -0.03432915 -0.11133622  0.06740771  0.03161997 -0.07041702
  0.15699318 -0.15031862 -0.07535009 -0.5576172  -0.46734068  0.01826073
  0.21282859 -0.39031875  0.08390017 -0.0782976  -0.3203337   0.3578513
  0.06432931 -0.13816229 -0.00523619  0.150336   -0.24196947 -0.13668935
 -0.0829085  -0.35923645 -0.30120742  0.06418741 -0.2458354   0.11033396
  0.04081012  0.26353002  0.14552733 -0.03666978  0.4286032   0.07229482
  0.69669306 -0.37779185 -0.09151115 -0.05664229 -0.15454939  0.12040649
 -0.39564615 -0.15044793 -0.07483833  0.09015949  0.32004434  0.36165386
 -0.23404318 -0.02818409 -0.27679926  0.2586665  -0.14092714  0.22887352
  0.36922616  0.11947942  0.42329517  0.16726933  0.01031445  0.3351074
  0.26833123 -0.15282138  0.47812608  0.1707783   0.16539563  0.17763749
  0.1921488   0.10354166 -0.30990466  0.5278108   0.12989675 -0.06338319
  0.21010517 -0.35273853  0.44802552 -0.5814252  -0.27526212  0.06915653
  0.24198675  0.01957545 -0.2274907   0.03257648  0.00502099  0.48230374
 -0.42421395  0.15502432  0.03656062 -0.01565408  0.15564474 -0.711923
 -0.18231878  0.31419748 -0.20315708 -0.06367727 -0.277399   -0.11180719
 -0.37097323 -0.06970106 -0.15824161 -0.29579517 -0.10042761  0.09397479
 -0.0818703   0.18253097  0.56282514 -0.39654455 -0.1122777  -0.2701801
  0.2954089   0.1139418   0.40514952 -0.31918228  0.13592725  0.00481507
 -0.17497644  0.43610823  0.01000094 -0.03060364 -0.49632528  0.5876112
 -0.03850456 -0.06357265  0.31891674 -0.08826406 -0.403448    0.00234397
  0.34847882  0.00901445 -0.10511152 -0.39196444  0.18706319  0.18580619
 -0.03507683 -0.0189518  -0.09662037 -0.00640455 -0.21609247 -0.124927
 -0.52505594  0.30729383  0.08721098 -0.1585181  -0.14641652 -0.31734553
  0.16439587 -0.2990378   0.05165288 -0.21812657  0.19773549  0.28649867
  0.08925192  0.23396707  0.05030854  0.19928536 -0.5037394   0.05311163
 -0.01429515  0.10200099 -0.00230029 -0.00781194  0.20133159  0.3445596
  0.13455921  0.25753182 -0.46021575  0.01712917  0.18200082 -0.27269733
 -0.2523878  -0.21459839  0.05746894  0.2734859   0.07443283  0.28592002
 -0.29980743  0.35199225  0.41816938 -0.31177565 -0.14668809  0.0743469
  0.01632485 -0.04555     0.25320572 -0.23171206  0.2795437   0.19944079]"
Android  ,"
- Android Device information (use `adb shell getprop ro.build.
- Google Play Services version
  Settings

",False,"[-0.18258983 -0.07923695  0.12568031 -0.5113629  -0.18968342 -0.20597902
 -0.4255471   0.24526611 -0.36848086 -0.20625594  0.09406616  0.01224819
  0.1104297   0.04758222  0.7311555  -0.10304484  0.44960263 -0.09888291
  0.32035872  0.38203666  0.2421746  -0.3016469  -0.09132587 -0.22260985
 -0.25033098  0.1653191  -0.23625246 -0.35482678  0.34887648  0.44482753
  0.6682341  -0.17221944  0.43205273  0.1672658  -0.12218361 -0.25378448
 -0.5313273  -0.02717648 -0.28755352 -0.04432169 -0.06117247  0.06412417
 -0.4064013   0.44504613 -0.43134344  0.1117767  -0.20059063  0.02688102
 -0.28448984  0.02280499  0.02947666  0.08646909 -0.43498576 -0.06791752
 -0.27309144  0.3750704  -0.4287519   0.5222446   0.09148959  0.30445036
  0.07253487  0.11595333 -0.22410962 -0.10242929 -0.20567472  0.07784028
  0.07383069  0.01723396  0.10098967 -0.37236288 -0.629713    0.16355649
  0.30633613  0.32001168 -0.01100578  0.35431576  0.01943906  0.02148059
 -0.16762522 -0.14585003 -0.06186634 -0.3055101  -0.42211273  0.46383476
  0.30250502  0.18187395  0.04821322 -0.03324967 -0.18587217  0.5570438
 -0.32663834  0.0933895  -0.1303773  -0.07402314 -0.26255178  0.12450843
 -0.25394955  0.1335217   0.07077211  0.13425037  0.1973703  -0.07148923
  0.23889595  0.08770148  0.08433558 -0.8575852  -0.4212003   0.12757614
  0.13697137  0.2982898   0.03039403 -0.42567348 -0.19369061 -0.1301081
  0.07017223 -0.00786501 -0.73345095 -0.28981996  0.08346898  0.06829925
  0.3399475  -0.08359599  0.18935919 -0.44868928 -0.06581809  0.10710312
  0.31906748 -0.05948818  0.02714674 -0.05588152  0.16258925  0.12840298
 -0.3481623  -0.20541888  0.03464798  0.30810913  0.09790259 -0.096182
 -0.14877155 -0.02263674 -0.04798336  0.48854673  0.5283588   0.04739062
 -0.18006212  0.07923506  0.44304106  0.5864461  -0.03868647 -0.39540803
 -0.08757507 -0.4346642   0.07735234  0.43162033  0.15519975 -0.1317752
 -0.1832287   0.07157549  0.27522418 -0.6547181  -0.21908785  0.04359733
  0.35395378 -0.00220319 -0.07794081 -0.02145356 -0.20870484 -0.14048347
  0.03876704 -0.0231999  -0.16828567  0.03937861  0.13678604 -0.01436687
 -0.02920091 -0.17112762  0.05812353  0.26989576  0.42844117 -0.12546074
 -0.12274714 -0.32926634 -0.12582806 -0.01676632 -0.24979313 -0.04638547
 -0.19603059  0.05156108  0.34155062  0.24281499  0.33282408 -0.15352514
 -0.37540394 -0.09201762 -0.14991127 -0.48494008  0.14247659  0.20784272
  0.5147799  -0.3835572   0.18183312  0.38912293 -0.3755182   0.2681535
  0.34400696  0.08612096 -0.33840173 -0.03009845 -0.13743888 -0.240124
  0.4214462  -0.30317307 -0.24657871  0.17862883  0.55844    -0.05108485
 -0.16519848  0.08574863 -0.4161049   0.16737586 -0.49392557  0.2663291
  0.1232769   0.27648014 -0.44245794 -0.0579356  -0.08396965 -0.4960006
 -0.06358698  0.15940578 -0.22982648  0.5014223   0.18976419  0.02379079
 -0.32460392 -0.2509602   0.30625743 -0.21658462 -0.516472   -0.21438733
 -0.10825854 -0.08315235  0.11542717 -0.25579965 -0.21759936  0.5403394
  0.5743632   0.5998041  -0.07767606 -0.3716199  -0.1540648   0.04833017
  0.44917676 -0.00567475  0.37788868 -0.2095431  -0.02269152  0.02259925
 -0.05312636  0.01316661  0.21299316  0.14603092  0.28166294 -0.32179466
  0.16268888 -0.23902114 -0.21931744 -0.61134493  0.08301636  0.08954379
  0.44751084 -0.06899113  0.08328212  0.01565294  0.5031331   0.15890485
  0.05504866 -0.16341935 -0.06587576  0.38520902  0.28369948 -0.31138355
 -0.04275064 -0.8167604   0.08688875 -0.55442697 -0.18469535  0.00581457
 -0.33280897 -0.10259676  0.73073375 -0.17817043 -0.40120617 -0.03638611
 -0.68408144  0.0309708  -0.52459496  0.33757815  0.04710641  0.14433812
 -0.03773107  0.21075012 -0.03679741  0.13001299 -0.4021483   0.07824531
 -0.19331582 -0.18817104  0.185822   -0.15091935 -0.31108987  0.15774778
 -0.36039665 -0.04748572 -0.33764333  0.70066077 -0.31677648  0.63458484
 -0.00932519  0.04346526  0.45791098 -0.08286342  0.29705518  0.01706391
 -0.272737    0.17248072 -0.12917918 -0.00949469  0.44984385 -0.15581338
 -0.06467109  0.40858752 -0.06123568  0.37694013 -0.1614224   0.06107153
 -0.16363986  0.06026449 -0.02043009  0.20099401  0.12676027 -0.00184158
  0.17387408 -0.26037553  0.09542955 -0.17123961  0.49776998 -0.0812576
  0.16197293  0.00341963  0.09019159 -0.04584011  0.2663365  -0.50908864
  0.18885054  0.16304635  0.39505804 -0.6701184   0.10758507  1.0522803
  0.15746926 -0.16260141  0.13313618 -0.24117967 -0.07484209 -0.13763013
  0.22797745 -0.14598526  0.29778376  0.21205606 -0.30740735 -0.1752177
 -0.41013572 -0.73833776  0.28420827  0.06519672 -0.22381361 -0.20988604
  0.35183498  0.12504585 -0.07991798  0.15654637  0.4382617   0.30861902]"
_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory stat:awaiting response type:build/install stale subtype: ubuntu/linux 2.6.0,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.6.2

### Custom code

Yes

### OS platform and distribution

ubuntu20.04

### Mobile device

rk3588

### Python version

python3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory

### Standalone code to reproduce the issue

```shell
import tensorflow
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: cannot open shared object file: No such file or directory
```


### Relevant log output

_No response_",False,"[-5.25954604e-01 -7.46558070e-01 -3.54013443e-01  2.56059647e-01
  2.05325961e-01 -4.25635397e-01 -3.55872124e-01 -8.31480697e-03
 -3.76113921e-01 -2.05613554e-01  2.74194647e-02  4.47978824e-02
 -3.16648990e-01  2.19675675e-01 -5.60283102e-03  2.22943440e-01
 -6.25690296e-02 -4.61658597e-01  3.27672243e-01  1.95786327e-01
  7.54403770e-02 -7.37033337e-02 -1.87146693e-01 -4.85487320e-02
  1.71143025e-01  1.76992521e-01 -3.29254083e-02 -9.95142311e-02
  4.99981642e-03  1.51628792e-01  4.77838784e-01  7.01510012e-02
 -1.70999482e-01  6.76501244e-02  6.01430684e-02  3.60195458e-01
 -3.02102894e-01 -2.85174042e-01 -7.23493546e-02 -1.42868217e-02
 -1.46835476e-01  2.22271025e-01  2.20100015e-01 -8.91038477e-02
 -1.63608044e-03 -1.51055858e-01  9.91273299e-02 -1.16741128e-01
  8.70979130e-02 -3.40280592e-01 -5.58441058e-02  2.24165861e-02
 -2.58375168e-01 -4.04116750e-01 -4.21071351e-02 -1.31127939e-01
  1.07669190e-01  1.05736293e-01 -1.19070098e-01 -1.36079550e-01
  2.14242846e-01  7.56367445e-02  1.68155938e-01  9.25391912e-02
 -1.24451004e-01  2.49318436e-01  3.14002275e-01 -2.26916485e-02
  6.49073839e-01 -2.12843165e-01 -2.02680808e-02  9.15550217e-02
 -4.54703480e-01  1.09409153e-01 -3.05767749e-02  1.79476798e-01
  5.98212034e-02  1.45466372e-01  1.24805190e-01 -1.30883366e-01
 -1.50218591e-01 -1.89846575e-01 -1.50550157e-03 -1.44390494e-01
 -1.14670172e-02 -2.31270641e-02  2.55301654e-01  2.14243472e-01
  1.13222457e-01 -2.25447282e-01  7.39525497e-01  4.10355926e-01
  1.05484119e-02  4.57147621e-02  2.78195173e-01  8.84758085e-02
  1.07416138e-01  3.53642434e-01 -1.02048337e-01 -2.06565976e-01
 -1.30215466e-01 -3.41146588e-01 -1.69090163e-02  9.01999697e-02
 -9.52183083e-03  2.75548398e-02  3.21190327e-01 -7.85651952e-02
  5.52301295e-02 -1.66072994e-01  5.58469817e-02 -1.22238979e-01
  2.04078302e-01 -9.08475965e-02  2.18989149e-01 -5.84056750e-02
 -4.30210531e-01  2.24911690e-01  2.61870362e-02  7.31644094e-01
 -5.08059934e-03 -5.89560047e-02  3.51637304e-02  1.08827256e-01
  3.57296646e-01  7.69187883e-02  1.17644675e-01  4.18082960e-02
  7.47466385e-02 -1.11477710e-02  9.34028849e-02  7.50315189e-03
 -1.16939493e-01  2.53313959e-01  1.63885474e-01  1.51820824e-01
 -3.11472118e-01 -3.03866565e-01  4.09173593e-02 -1.12594157e-01
 -2.26737484e-01  1.60024315e-01 -1.49175733e-01 -6.50776863e-01
  2.37740614e-02  2.10727051e-01 -1.58037573e-01  3.91244829e-01
 -1.19327851e-01  1.24626964e-01 -2.01892853e-02 -6.90555722e-02
 -1.97063118e-01  2.94767082e-01  2.56298542e-01  2.94566393e-01
  3.42434794e-01 -2.52045691e-02  1.82536040e-02 -6.00386500e-01
 -1.09461680e-01  5.20178080e-01 -1.95744425e-01 -9.17990953e-02
 -1.74611896e-01  5.40224500e-02 -4.75852996e-01 -3.69854689e-01
  1.08069450e-01  2.22476184e-01 -1.85264498e-01 -7.70723075e-02
  1.72446668e-01  1.41937315e-01  2.76347380e-02  4.01314199e-02
  2.38352925e-01 -7.86755025e-01 -1.79044530e-02  3.16921681e-01
  4.85686809e-02  3.85043383e-01 -1.17791981e-01  2.75054783e-01
  3.09355371e-02  3.88705879e-02  1.44114926e-01 -1.20674074e-01
 -2.32658386e-02  1.08924538e-01 -2.08233669e-01 -1.75452214e-02
  2.95206159e-01 -2.22892448e-01 -1.06775343e-01  2.37501189e-02
  4.36042875e-01  1.35182664e-01 -1.13224216e-01  1.39082018e-02
 -1.22572213e-01  4.46624458e-02 -1.10653579e-01  1.50365129e-01
  1.76068693e-01 -2.96994805e-01 -1.88486408e-02 -1.85460597e-01
 -3.75524819e-01 -8.35529268e-02 -9.34565961e-02 -2.86943644e-01
 -1.24147907e-03 -6.21983781e-04 -3.85981619e-01  9.16653872e-02
  9.24671516e-02  1.21919379e-01 -1.38880774e-01  2.38483138e-02
  9.09339078e-03 -2.43071690e-01  7.23166466e-02 -4.68696207e-01
  6.97350428e-02 -1.61692843e-01 -2.53295779e-01 -3.07244249e-02
  2.50575542e-02  2.27454752e-01 -1.43847242e-02 -1.12624383e-02
  3.49006236e-01  4.43680435e-01  3.65408778e-01 -2.47662216e-02
  8.21133852e-02 -9.07821506e-02 -5.11343889e-02  3.95143449e-01
 -3.34189892e-01 -1.81534886e-03 -3.26189548e-02  2.56866980e-02
  1.82168871e-01  2.29034662e-01  3.56155485e-02 -1.94811642e-01
 -3.17377269e-01  2.59734899e-01 -2.86071390e-01  2.32164580e-02
  2.70541906e-01  8.05470720e-02  4.82635289e-01  2.73889422e-01
 -2.13841528e-01 -9.22126882e-03 -4.31940258e-02 -4.70729135e-02
  1.27815872e-01  5.72159328e-02  1.47490308e-01  3.53661120e-01
  2.14022622e-01  3.95714045e-01 -4.82557833e-01  6.88638926e-01
  1.95954889e-01 -7.75604546e-02  7.78205991e-02 -3.44123483e-01
  4.88939106e-01 -3.02482009e-01 -2.41027288e-02  9.73723736e-03
  4.61859286e-01 -7.09525570e-02  8.41942849e-04  1.24826819e-01
  1.20645151e-01  4.02055293e-01 -2.03036338e-01 -4.92247716e-02
  2.94656694e-01 -1.32403940e-01 -3.46127823e-02 -3.79197687e-01
 -4.10391927e-01  1.36450096e-03 -1.63916811e-01  1.24787442e-01
  3.27097140e-02 -2.08966851e-01 -3.16564441e-01 -4.99144047e-02
  1.27765417e-01 -4.52029184e-02  4.17093039e-01  3.29419434e-01
 -2.58727252e-01 -1.59513146e-01  9.62574780e-02 -1.17420048e-01
 -2.57509291e-01 -6.98968619e-02  2.74143636e-01  8.36744159e-02
  4.57391769e-01 -5.46243250e-01  5.52599192e-01 -1.10085860e-01
 -1.67484328e-01  4.04827893e-01 -2.87259873e-02  2.64121871e-03
 -4.86866623e-01  6.37180030e-01  4.23353553e-01 -2.64345497e-01
  3.04169208e-02 -1.81714267e-01 -2.70675957e-01  7.62390867e-02
  1.32593110e-01 -2.14846522e-01 -1.19438492e-01 -4.23336685e-01
 -1.61227822e-01  1.64494187e-01 -1.66871786e-01 -1.13627240e-01
 -1.74419537e-01  2.12028027e-01 -2.02535242e-01  2.66427994e-01
 -5.74331522e-01  1.09508544e-01 -1.78220659e-01 -1.60283893e-01
  1.16607055e-01 -1.74965620e-01 -3.72754410e-04 -2.38349080e-01
 -9.16760564e-02 -1.40921757e-01  5.87990999e-01  2.52380252e-01
 -3.77620101e-01 -6.65609762e-02 -3.72364521e-02  1.21285684e-01
 -4.50042456e-01  7.14687854e-02  7.06299543e-02  3.75772297e-01
  4.08271253e-02 -2.67007314e-02  1.96415827e-01  3.33901316e-01
 -2.52348572e-01  3.22636575e-01 -3.17791045e-01  1.69872288e-02
  1.90157846e-01 -1.71404019e-01 -3.33818138e-01 -1.09551281e-01
 -2.41508305e-01  3.82085919e-01 -5.69708273e-03  2.05796674e-01
 -3.12455535e-01  4.52394187e-01  5.97551882e-01 -3.49139452e-01
 -4.75760043e-01  4.36064363e-01 -1.19928062e-01 -4.63699549e-02
  8.74513984e-02  1.22162253e-01  2.25234687e-01 -4.35217135e-02]"
How to limit GPU memory usage when only prediction in c++? stat:awaiting response type:feature comp:runtime,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.4

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.7.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.0 / 8.1.0

### GPU model and memory

RTX 3090 / 24GB

### Current behavior?

I loaded the saved model using the already compiled tensorflow-gpu 2.4.0.
When this model was used for prediction, it was confirmed that all available memory of the gpu was used.
I've seen limiting using the growing method in python, but I don't know how to use it in c++. could you please tell me how?



### Standalone code to reproduce the issue

```shell
#include <stdlib.h>
#include <stdio.h>
#include <tensorflow/c/c_api.h>
#include <iostream>
#include <fstream>
#include <sstream>
#include <string>
#include <vector>
void NoOpDeallocator(void* data, size_t a, void* b) {}

int main() {
    TF_Graph* Graph = TF_NewGraph();
    TF_Status* Status = TF_NewStatus();

    TF_SessionOptions* SessionOpts = TF_NewSessionOptions();
    TF_Buffer* RunOpts = NULL;

    const char* saved_model_dir = ""H:\\my_model\\""; // Path of the model
    const char* tags = ""serve""; // default model serving tag; can change in future
    int ntags = 1;

    TF_Session* Session = TF_LoadSessionFromSavedModel(SessionOpts, RunOpts, saved_model_dir, &tags, ntags, Graph, NULL, Status);
    if (TF_GetCode(Status) == TF_OK)
    {
        printf(""TF_LoadSessionFromSavedModel OK\n"");
    }
    else
    {
        printf(""%s"", TF_Message(Status));
    }
}
```


### Relevant log output

_No response_",False,"[-0.18249556 -0.03580496 -0.2898537  -0.06387769  0.0942135  -0.0713982
 -0.30522108  0.18979345 -0.3056582  -0.3340369  -0.1884141  -0.07274944
 -0.14568749 -0.0185964  -0.32413045  0.16309696 -0.02585214  0.37342143
  0.07840931  0.06952448  0.07998423 -0.02769421 -0.32154858  0.2724637
  0.25825083  0.12090576 -0.12892497 -0.03522547 -0.00476103  0.28246096
  0.31886828  0.03141977  0.5302404   0.11406314 -0.00800635 -0.02601791
 -0.23075499 -0.50358516 -0.29565993  0.09050353 -0.00241893  0.18565
 -0.01558184  0.20609033  0.07273313 -0.06891771 -0.16940945 -0.07314759
 -0.11600487 -0.20720005  0.04953186  0.02802609 -0.47876024 -0.45537558
 -0.23255916 -0.1921418   0.2858447  -0.42417014 -0.12697561 -0.01363647
  0.07607711 -0.15552713  0.40013105 -0.32239372 -0.04303898  0.30312008
  0.3364114   0.2948832   0.64977854  0.00388755  0.12372778  0.14898227
 -0.64551914 -0.3777358   0.13452263  0.2091776   0.16974601  0.10133265
  0.19151294 -0.47614378  0.19247603 -0.18155634 -0.16418886 -0.5342031
 -0.01037614 -0.03330455  0.6297634   0.29824027  0.4847538  -0.2709879
  0.11967575  0.24254282 -0.29386908 -0.1112254   0.41931    -0.03099076
 -0.04478168 -0.34366316 -0.19048162 -0.23042916 -0.01797059 -0.05616479
  0.02162911  0.18122226 -0.22823754 -0.23464185  0.11767451  0.33015144
 -0.13812679 -0.08647243  0.24676752  0.24155042  0.22443065  0.15804452
 -0.16931538  0.08682182  0.08262365  0.18287015 -0.23746467  0.51274836
 -0.02609591 -0.02529414  0.23114944  0.2115806   0.23975024 -0.13432282
 -0.28077704  0.0245498   0.23927885 -0.34584352  0.13418397 -0.14221738
  0.23176879  0.28471133 -0.06332701  0.1423547  -0.06276055 -0.35916245
 -0.23903482 -0.21562211  0.06196958  0.08130707 -0.23639643 -0.7430717
  0.1417342   0.52514744 -0.24066079  0.17525753 -0.10794176 -0.07240625
 -0.18309833  0.17869912 -0.25008735  0.3144235   0.03405274 -0.18405113
  0.30050832 -0.21367821 -0.19209802 -0.49276084 -0.1656206   0.2469691
  0.08317161 -0.21531291  0.26309836  0.27707297 -0.13068196 -0.25446352
  0.19838601  0.5144606  -0.18749866 -0.16389044 -0.16154587 -0.00960518
  0.21099338 -0.32146376 -0.20150226 -0.77187127 -0.06976493  0.14605904
 -0.35542816  0.04567202  0.1550138   0.11239249  0.11616717 -0.04868908
  0.16639687  0.18429226 -0.1012556  -0.1587055  -0.42959934 -0.24998707
  0.01967158 -0.2887076  -0.20232226  0.1117039   0.15915707  0.25918794
 -0.06006136  0.13259953 -0.37355945 -0.31317377 -0.11180332  0.29926747
  0.2797866  -0.18625753 -0.54222333 -0.18318938 -0.2029942   0.19902219
 -0.03697261 -0.5299834  -0.04543816  0.06127948 -0.10005058  0.28139943
 -0.0828364  -0.02222497 -0.1225726   0.17468977  0.29598692 -0.34529364
 -0.06879438 -0.30066094 -0.12960905 -0.04498625 -0.10212518  0.07533188
 -0.08976701  0.13947554  0.13660252 -0.07196991  0.09914315 -0.12957373
  0.4519106   0.25657356 -0.05728992 -0.08425822 -0.11371183  0.01316553
 -0.22856316 -0.16531329 -0.03783889  0.26712683  0.22412068  0.5917809
 -0.3480647  -0.08244209 -0.1864141   0.04681147 -0.31880122 -0.04972298
  0.36545035  0.01235097  0.39099026  0.08797423 -0.01730669  0.28544307
  0.15765879 -0.13625458  0.47674796  0.24573284 -0.18214057  0.5232991
  0.4862712   0.38435024 -0.30127436  0.50486165  0.23502633 -0.26283574
  0.18671328 -0.33326566  0.57154405 -0.3550541   0.13930473 -0.10742685
  0.1786654   0.02042973 -0.07997317  0.00641167  0.1892477   0.2822873
 -0.02649042 -0.07613821  0.20747584 -0.19072138 -0.13219517 -0.69379056
 -0.18108875  0.23702887 -0.1573058   0.28935063  0.22996175 -0.01716696
  0.32048082  0.24573942 -0.04359868  0.25455248  0.00919257  0.11298777
 -0.3926131   0.02748232  0.3365616  -0.4202993  -0.55790544 -0.06276118
  0.15282738  0.1975137   0.4915638  -0.25438792 -0.00892679 -0.15124992
 -0.0957635   0.21231541 -0.4024492  -0.07362121 -0.36308205  0.6305512
 -0.05488728  0.07368125  0.21656586 -0.28123677 -0.2522732   0.501276
  0.01091873 -0.39983928  0.00991948 -0.43287355  0.1870738   0.16628896
  0.07483254 -0.16304706  0.0957277  -0.35096228 -0.09564327  0.05651519
 -0.0645439   0.20984112  0.12282056 -0.29454958 -0.4664204  -0.02329412
  0.06820504 -0.25830844  0.10974291 -0.25521138  0.6424465   0.67565984
  0.0760375  -0.03288691  0.29798105  0.13221326 -0.3601489  -0.0651979
  0.17516272  0.4504447   0.13675025  0.04017816  0.4864048   0.14543459
 -0.2652179  -0.07928535 -0.1543957   0.18948409  0.05759476 -0.25121006
  0.13256487 -0.47827107  0.22214678  0.29933918 -0.05692529  0.26688117
 -0.20689979  0.48194605  0.6973026  -0.05426497  0.00588139 -0.24427918
  0.17696425 -0.24963954 -0.06646779 -0.04531267 -0.03719895  0.07505891]"
Will there a MLP model in the future version? type:feature comp:keras TF 2.13,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When building deep learning models like Multi-Layer Perceptrons (MLPs), code reusability and conciseness are crucial factors. Currently, using `tf.keras.Sequential` in TensorFlow allows for convenient creation of sequential models. However, manually adding common operations such as Batch Normalization or Dropout to each layer can lead to code redundancy and an increased burden in terms of coding and maintenance. Therefore, proposing the addition of a feature in TensorFlow to directly create MLPs with Batch Normalization and Dropout is highly beneficial.

Here are several reasons why this feature would be advantageous for TensorFlow users:

1. **Simplified Code**: Users won't need to manually add Batch Normalization and Dropout operations to each layer, resulting in cleaner, more readable, and maintainable code.

2. **Reduced Error Rate**: Manual copy-pasting of code is error-prone, especially as model complexity increases. Automating the integration of Batch Normalization and Dropout operations can reduce issues arising from code errors.

3. **Increased Productivity**: Developers can build and iterate on models more swiftly, focusing on design and parameter tuning rather than rewriting the same code segments for every new model.

4. **Education and Learning**: For newcomers to TensorFlow, this feature can provide a quicker onboarding process, lowering the learning curve and enabling them to grasp and apply deep learning concepts faster.

Certainly, here's the additional information:

I also believe that PyTorch has implemented MLP functionality quite effectively. An example of this can be found in the following URL: [PyTorch MLP](https://pytorch.org/vision/main/generated/torchvision.ops.MLP.html). PyTorch's approach to creating MLPs provides a good reference for how TensorFlow could potentially integrate similar features.

### Standalone code to reproduce the issue

origin
```python
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.ReLU(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10),
])
```


with MLP model
```python
model = tf.keras.MLP(
    hidden_channels=[128, 64, 32, 10],
    norm_layer=tf.keras.layers.BatchNormalization,
    activation_layer=tf.keras.layers.ReLU,
    dropout=0.2,
)
```

### Relevant log output

_No response_",False,"[-0.42264932 -0.3307993  -0.08505794  0.00135138  0.16009171 -0.31634682
 -0.3598438  -0.2245683  -0.33850986 -0.29475054  0.21694475 -0.12851177
 -0.22628957 -0.05420264 -0.03809978  0.24187781 -0.19337852  0.01479657
  0.09269041  0.22936843 -0.12135398 -0.17930174 -0.16465886  0.26214537
  0.04606243  0.09548482 -0.3011587  -0.08304391 -0.03230495  0.10725018
  0.50345504  0.26487175 -0.06395356  0.08610674  0.0943006   0.14036444
 -0.08607905 -0.23384953 -0.33448523  0.06470841  0.04215039  0.10546134
  0.02412043 -0.26979846  0.1261871  -0.27960685  0.02769394 -0.37283018
 -0.21568662 -0.33327618  0.03342553 -0.36933935 -0.3621213  -0.43474224
 -0.31878197 -0.1523011   0.22615829 -0.31441724 -0.06358951  0.06768179
 -0.07832053 -0.03217211  0.12611091 -0.06350677  0.24718803  0.08397554
  0.22984374  0.15145615  0.5323459  -0.18441114 -0.05218084 -0.1525492
 -0.4924283  -0.03520997  0.04208105  0.20302375  0.3139959   0.05870641
  0.4916914  -0.12187478  0.20997053 -0.24683914 -0.18749234 -0.36919713
  0.04314299 -0.1306478   0.5004928   0.17851546  0.32338256 -0.35776573
  0.19413939  0.44083968 -0.1437006   0.2636904   0.49437243  0.40862954
  0.00337098 -0.09495291  0.16663828 -0.16488066 -0.25960088 -0.12120923
 -0.03270273 -0.08811563 -0.09392409 -0.22752821  0.10055424 -0.14656141
  0.25931334 -0.10121918  0.2446104   0.09523612  0.2254273  -0.20426808
 -0.06039926 -0.09125506  0.02676565  0.15509196 -0.1039353   0.677492
 -0.00145144 -0.27255726  0.21168333  0.02392152  0.6261337   0.3546602
 -0.30071154  0.15855765  0.01039106 -0.33778292  0.18422452  0.05245063
  0.33923125  0.29444003 -0.05163742  0.09054565 -0.03195162 -0.2194851
 -0.24170248 -0.35634392 -0.42772168  0.21631923 -0.15583748 -0.5718126
  0.2110153   0.1379063  -0.18298848 -0.0036276  -0.09357135 -0.07459706
 -0.02808709  0.12291597  0.18367232  0.22846757  0.33263925  0.08145206
  0.18950322 -0.1572099   0.01293655 -0.3177302  -0.09295154  0.31062627
 -0.14529592 -0.26050857  0.16692364  0.237638   -0.1696867  -0.3394214
  0.26733184  0.50802976 -0.15351778 -0.1071571   0.08422358 -0.0590441
  0.2538401  -0.16782701  0.1336017  -0.63831747 -0.11199246  0.4213043
  0.0085526   0.19274467  0.33590364  0.12130794  0.04746552 -0.09730884
  0.30385566  0.14664021 -0.2520558   0.01582813 -0.4803849  -0.10830808
  0.5673895  -0.15892118 -0.12024388  0.11598419  0.26998553 -0.14164707
 -0.02917669 -0.08608137 -0.4425612  -0.26842275 -0.25295377  0.02908891
  0.09424317 -0.3396114  -0.03397963 -0.07478771 -0.34196267  0.22065535
  0.003218   -0.5354264   0.26042593  0.07868052 -0.31045312  0.361673
  0.14044699  0.05930686 -0.13262418  0.25159854  0.19828399 -0.19312376
  0.0472847  -0.38044947 -0.15739715  0.3492617  -0.4383164   0.28646109
  0.01183608  0.1819333   0.08900609  0.24447167  0.41303882  0.2973533
  0.38654488 -0.11475649 -0.0240805  -0.1765469  -0.09670408 -0.17251383
 -0.61031497 -0.2567571  -0.01002467 -0.09529966  0.181934    0.55406255
 -0.45971093 -0.05945088 -0.39591956  0.00894551 -0.33134556  0.11512396
  0.36475503 -0.01004051  0.48253292  0.15837789  0.28839773  0.3115958
  0.50303304 -0.24808685  0.60516655  0.0421773   0.06515142  0.5217197
  0.41009846  0.32148203 -0.43569678  0.49067655 -0.15916517 -0.27658647
  0.2294049  -0.42355913  0.8077576  -0.33929548  0.20777078  0.0127598
  0.2904661   0.02949923 -0.02551886  0.04182816  0.27965483  0.11513741
 -0.5716082   0.05284941 -0.12062383 -0.34472907 -0.04914506 -0.84374785
 -0.175346    0.13609892 -0.19318324  0.33447015  0.06495106 -0.02966136
 -0.11136241  0.11362901  0.0434249  -0.11842403  0.13611723 -0.03222313
 -0.3360125  -0.03866044  0.51833683 -0.6133012  -0.31244048 -0.02225669
  0.57489824  0.2308679   0.54459953 -0.37840137 -0.00451831 -0.32370552
 -0.09728354  0.49716622 -0.1566835  -0.07384646 -0.38746563  0.73762333
  0.08942966  0.00292219  0.25796753  0.02075625 -0.16902235  0.19660316
  0.23126438 -0.05092756 -0.06632866 -0.5059527   0.24842885  0.2548261
 -0.03509792  0.06421269  0.09866558  0.12341781 -0.20003061 -0.01249249
 -0.38582873  0.3048826  -0.02166466 -0.4034728  -0.3992564   0.13510421
 -0.11745072  0.06258527  0.03300557 -0.28107855  0.23593697  0.6951158
 -0.14386865  0.10937492  0.09076249  0.20245746 -0.6301397  -0.06294397
 -0.07423543  0.29124182  0.07124015 -0.19574645  0.498741    0.2852745
 -0.11937746  0.09484499 -0.18757744  0.13826296  0.17705697 -0.24700013
 -0.11773144 -0.5864476  -0.06787412  0.316059   -0.07801381  0.2087712
 -0.2037785   0.31813416  0.5463605  -0.394069   -0.21920197  0.16337903
  0.11972877 -0.04175279  0.10153635 -0.40556374  0.05964358 -0.05990909]"
Cannot build package with bazel stat:awaiting response type:build/install stale subtype: ubuntu/linux,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

newest

### Custom code

Yes

### OS platform and distribution

Debian 12

### Mobile device

_No response_

### Python version

Python 3.9

### Bazel version

newest

### GCC/compiler version

Clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have searched everywhere but cannot find anything about this error

### Standalone code to reproduce the issue

```shell
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /home/drowsiness/
TensorFlow Version: 2.15.0
TensorFlow Major Version: 2
TMPDIR: /tmp/tmp.BINOHRyJHF
Fri Aug 25 09:26:15 PM PDT 2023 : === Preparing sources in dir: /tmp/tmp.BINOHRyJHF
Symlink already exists: bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/tensorflow/libtensorflow_cc.so.2
~/tensorflow ~/tensorflow
~/tensorflow
~/tensorflow ~/tensorflow
~/tensorflow
~/tensorflow/bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow ~/tensorflow
~/tensorflow
./bazel-bin/tensorflow/tools/pip_package/build_pip_package: line 255: patchelf: command not found
```


### Relevant log output

_No response_",False,"[-7.61435091e-01 -1.03296638e-02 -2.24666417e-01  1.55721575e-01
  1.72350109e-01 -3.32417220e-01 -2.92673379e-01  1.73440993e-01
 -2.03431413e-01 -3.35507691e-01  2.76493784e-02 -1.53321832e-01
 -1.78339891e-02 -1.29208609e-01  6.09205291e-03  3.60810310e-01
 -9.34221372e-02  2.65230052e-03  3.69156271e-01  1.42581344e-01
 -2.11963221e-01  8.04957598e-02 -1.91689938e-01  2.91237921e-01
  4.40728545e-01  9.91114005e-02 -3.98521483e-01  1.16320878e-01
  6.44402802e-02  2.37151571e-02  4.57313299e-01  3.01310539e-01
  3.71866226e-02  1.64318979e-01  3.85055214e-01  3.25561285e-01
 -1.63161442e-01 -2.84074545e-01 -2.55558282e-01 -2.22280975e-02
 -9.96716917e-02  5.98871261e-02  3.02910805e-01 -3.24130237e-01
  1.97182015e-01 -3.75233769e-01 -1.68461114e-01 -2.21924886e-01
  2.17630491e-02 -4.09488738e-01 -2.34594107e-01 -9.43359137e-02
 -3.39521766e-01 -5.49269378e-01 -1.09850779e-01 -1.30217731e-01
 -7.34817050e-03 -1.59188733e-02 -1.13452308e-01  2.86480159e-01
  1.69913828e-01  3.47130746e-02  5.27900457e-02 -1.94838308e-02
  5.57929948e-02  1.66758239e-01  1.12733684e-01 -3.93612534e-02
  4.77769136e-01 -3.24928403e-01  1.71439260e-01 -2.31895566e-01
 -4.69574094e-01 -4.71462794e-02  5.09277508e-02  1.18026873e-02
  1.26405329e-01  1.89359397e-01  1.47558585e-01 -2.39176840e-01
 -1.84608594e-01 -3.79997611e-01 -2.26711452e-01 -3.45566243e-01
  8.12172890e-02 -4.89629135e-02  5.43969393e-01  1.15823090e-01
  3.48754287e-01 -6.51498809e-02  3.60657573e-01  5.26718497e-01
 -4.32358645e-02  1.29364610e-01  1.11662142e-01  2.52061725e-01
  3.07062626e-01  1.67325020e-01 -6.20618910e-02 -6.74024522e-02
 -2.23620608e-02 -2.76466250e-01  7.02663958e-02  2.03472003e-02
 -1.15128122e-01 -3.34793180e-01  3.59570086e-01  6.58883154e-02
 -5.57713658e-02 -2.27907091e-01  1.94597363e-01 -3.62957418e-02
  5.77573180e-02 -1.12928934e-01 -1.69987474e-02  8.95835534e-02
 -9.19483788e-03  6.41929917e-03 -7.28946626e-02  8.33622515e-01
 -5.82178161e-02  1.87364578e-01 -9.27434415e-02  1.75085112e-01
  5.51224828e-01  2.93304771e-02  2.41480581e-03 -5.43610901e-02
  2.69356430e-01  8.29890650e-03  1.07117534e-01  1.76159427e-01
 -4.90953103e-02  3.59408736e-01 -1.09637789e-02  1.71974123e-01
 -1.22913167e-01 -4.63648066e-02 -1.50864646e-02 -3.92694056e-01
 -2.23915443e-01  1.30536467e-01  8.31777453e-02 -8.30305696e-01
 -3.82010378e-02 -4.91697527e-02 -2.29150116e-01  9.59332734e-02
 -4.60293531e-01 -5.02523705e-02 -1.87507808e-01  2.44832225e-02
 -3.05838466e-01  6.02838337e-01  3.36669356e-01  2.29535140e-02
  2.13747948e-01  6.40260801e-02  1.32960796e-01 -2.64366627e-01
  3.00641377e-02  5.86771369e-01 -9.83868912e-02 -2.68835545e-01
 -2.34676793e-01  1.82391897e-01 -4.75116849e-01 -2.82132685e-01
  2.46354908e-01  5.40576100e-01 -3.21469814e-01  1.60338953e-02
  2.30719119e-01  9.92455781e-02  3.99695456e-01 -5.35473041e-02
  3.10885698e-01 -2.77281582e-01 -2.99011581e-02  6.31354094e-01
  1.45471781e-01  3.49299967e-01 -5.61346300e-02  8.31589699e-02
  1.29225910e-01  9.05201770e-04 -8.02551210e-02 -1.86691601e-02
 -1.28905267e-01  3.85629945e-02 -5.06209254e-01 -2.25425772e-02
  4.84192342e-01 -2.84043878e-01 -1.95009232e-01  1.85234025e-01
  2.72894919e-01  4.84154150e-02  1.94664776e-01 -1.29787847e-01
 -2.29866743e-01  6.18502684e-02  1.37361690e-01  2.78904848e-02
 -5.29276952e-02 -1.60000354e-01  3.93339321e-02 -1.90496385e-01
 -1.72488764e-01  1.05224259e-01 -1.42689213e-01 -6.26415491e-01
  1.49616271e-01 -6.78323135e-02 -9.62066650e-02  4.38804090e-01
  4.44354594e-01  3.46716866e-02 -1.42424002e-01  1.01845816e-01
  2.33311683e-01 -3.97355646e-01  9.07572657e-02 -4.12220210e-01
  1.22997805e-01  2.29861349e-01 -1.95291519e-01  7.03442283e-03
  6.20477349e-02 -9.78967398e-02  1.45287197e-02  9.36371014e-02
  4.74899262e-01  4.24406946e-01  3.04473162e-01 -9.88879502e-02
 -1.11865252e-03 -4.47562188e-02 -2.44426802e-01  6.95062354e-02
 -3.24817479e-01 -5.27397454e-01  8.38077888e-02  9.09216329e-02
  1.87142447e-01  3.18892241e-01 -1.85561150e-01 -8.87168422e-02
 -5.24632752e-01  2.32074410e-01 -2.57768661e-01  3.07549179e-01
  3.46521020e-01  1.02834299e-01  3.87531340e-01  2.54442632e-01
  4.77342308e-03  1.38556018e-01  1.17167741e-01 -3.57028246e-01
  2.79490799e-01  3.08334380e-01 -7.04974458e-02  2.32900120e-02
 -3.62867191e-02  1.19961619e-01 -5.82809091e-01  4.23869520e-01
  3.81900549e-01 -3.81537795e-01  4.01380181e-01 -3.64938438e-01
  4.86652434e-01 -2.91489005e-01  5.49003892e-02 -4.66464497e-02
  1.81167468e-01  3.94130796e-02 -2.50132512e-02 -9.85562801e-05
 -1.49246156e-01  1.07045889e-01 -2.06721872e-01  3.21249478e-02
  1.24377504e-01 -1.24880329e-01  1.26488224e-01 -6.44937754e-01
 -3.00807834e-01  1.50479585e-01 -2.90418267e-01  2.68247873e-01
 -1.45017177e-01  4.96881567e-02 -2.58599669e-01 -4.11680639e-02
 -5.41030318e-02 -1.50733739e-01  1.89800262e-02  1.94043130e-01
 -2.73220658e-01  1.26455054e-02  2.99141616e-01 -2.84551442e-01
 -1.87083706e-01 -8.14456195e-02  1.14510491e-01  2.84112424e-01
  6.12120271e-01 -4.42432731e-01  1.40453309e-01 -9.04799718e-03
 -1.91659003e-01  5.55036902e-01 -9.90942866e-02  2.13289693e-01
 -4.69068676e-01  7.29239345e-01  1.64557099e-01 -1.22515157e-01
  1.41823798e-01 -2.52746373e-01 -3.50247264e-01  3.00311923e-01
  4.04982030e-01 -1.10978074e-01 -3.08330119e-01 -5.71813464e-01
  9.10131037e-02  2.58597255e-01 -7.36082792e-02  1.48410439e-01
 -2.01764300e-01  1.05462559e-01 -3.55820417e-01 -2.44404152e-02
 -3.72521967e-01  1.24536470e-01 -1.61690578e-01 -2.29761332e-01
 -2.24114329e-01 -1.21570177e-01  6.80657029e-02 -2.91526437e-01
 -7.91185796e-02 -2.84778357e-01  1.24753147e-01  5.37426233e-01
 -1.69728070e-01  4.41875029e-03  9.90401730e-02  2.39384323e-01
 -4.54227746e-01  1.47934526e-01  4.10062447e-02  1.09015077e-01
  4.61002626e-03  1.95237175e-02  5.26874781e-01  2.37186790e-01
 -1.11846812e-01  7.11735785e-02 -5.13674855e-01  3.60269956e-02
  2.11483255e-01 -4.31062818e-01 -1.96150705e-01 -2.45974958e-01
 -1.65977240e-01  2.16135174e-01 -1.30148828e-01  7.57299364e-02
 -3.45885754e-01  3.38220984e-01  5.94100952e-01 -3.70180458e-01
 -2.01673001e-01  3.26807857e-01  5.05060032e-02  2.98837516e-02
 -8.66328627e-02 -2.39523232e-01  3.74003112e-01 -8.82255193e-03]"
TensorFlow Not Detecting GPU on Compute Node stat:awaiting response type:build/install type:support stale comp:gpu TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-99044-gc6ecfeac886 2.15.0-dev20230825

### Custom code

No

### OS platform and distribution

CentOS Linux release 7.9.2009 (Core)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8.0 / 8.7.0.84

### GPU model and memory

_No response_

### Current behavior?

## Behavior summary 

I'm trying to set up TensorFlow on a compute node within our scientific computing cluster and am running into a problem. After setting up a new environment and installing TensorFlow, running TensorFlow's `tf.config.list_physical_devices('GPU')` method returns an empty list, indicating no GPU devices are detected. However, `nvidia-smi` shows a Quadro RTX 6000 GPU present on the system. The TensorFlow CPU validation works without issues, but the GPU validation does not.

## Possible Areas of Concern 

1. **Modules vs. Conda/Pip Installations:** I've loaded specific versions of CUDA and cuDNN using the cluster's module system. However, I also used conda and pip to install these within my environment. Could this mixed approach cause any conflicts for TensorFlow? I'm unsure which version TensorFlow might prioritize or if it would create any confusion.
2. **CUDA Version Mismatch?:** When I run `nvidia-smi`, it indicates a CUDA version of 12.2. Yet, I've loaded and installed a CUDA version of 11.8 using both modules and conda. I'm wondering if this difference could lead to any issues. Does TensorFlow need a strict match with the CUDA version?
3. **Different cuDNN Version:** The TensorFlow installation guide mentioned cuDNN version 8.6.0.163, but I installed 8.7.0.84 for what I thought might be better consistency with the modules I loaded. Could this version difference be problematic? Is TensorFlow particular about the cuDNN version it works with?

I'm looking for insights from anyone who might have navigated similar issues or can provide clarity on these points. Also of course if anyone knows how I might go about further troubleshooting/diagnosing this that'd be great. Thanks for the help!  

### Standalone code to reproduce the issue

```shell
# Setting up a new conda environment with python 3.9
conda create --name myenv python=3.9
conda activate myenv

# Running NVIDIA's System Management Interface to check GPU
nvidia-smi

# Loading necessary modules for CUDA/cuDNN
module load cuda-11.8.0-gcc-8.2.0-rqftjjg
module load cudnn-8.7.0.84-11.8-gcc-8.2.0-qibz3ue

# Installing CUDA Toolkit and cuDNN using Conda and Pip
conda install -c conda-forge cudatoolkit=11.8.0
pip install nvidia-cudnn-cu11==8.7.0.84

# Configuring system paths
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo 'CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
echo 'export LD_LIBRARY_PATH=$CUDNN_PATH/lib:$CONDA_PREFIX/lib/:$LD_LIBRARY_PATH' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh

# Upgrading pip and installing TensorFlow
pip install --upgrade pip
pip install tensorflow==2.13.*

# Running CPU validation
python3 -c ""import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))""

# Running GPU validation
python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output

```shell
# CPU validation
2023-08-25 14:11:07.056941: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-08-25 14:11:07.105836: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-25 14:11:08.620525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-25 14:11:10.459520: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2023-08-25 14:11:10.459591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: scu-node018.scu
2023-08-25 14:11:10.459608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: scu-node018.scu
2023-08-25 14:11:10.459712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.104.5
2023-08-25 14:11:10.459783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.104.5
2023-08-25 14:11:10.459811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.104.5
tf.Tensor(-964.9978, shape=(), dtype=float32)

# GPU validation
2023-08-25 14:11:35.537281: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-08-25 14:11:35.582391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-25 14:11:36.286683: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-08-25 14:11:36.956393: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2023-08-25 14:11:36.956458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: scu-node018.scu
2023-08-25 14:11:36.956495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: scu-node018.scu
2023-08-25 14:11:36.956591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.104.5
2023-08-25 14:11:36.956655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.104.5
2023-08-25 14:11:36.956683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.104.5
[]

# nvidia-smi Outpu
Fri Aug 25 13:59:33 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 6000                Off | 00000000:AF:00.0 Off |                    0 |
| N/A   31C    P0              54W / 250W |      0MiB / 23040MiB |      5%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
```
",False,"[-5.28167605e-01 -4.31028157e-01 -1.26189291e-01 -5.19552194e-02
  1.98618785e-01 -3.40923995e-01 -3.07873428e-01  1.52904660e-01
 -7.37282038e-02 -3.70333612e-01  2.50497144e-02  1.35261148e-01
 -2.02009439e-01  2.50947416e-01 -2.46468455e-01  2.34340981e-01
 -1.32769704e-01 -2.05078870e-01  3.36445063e-01  3.01780283e-01
 -2.44029492e-01 -1.85980901e-01 -2.13671684e-01  6.53685480e-02
  2.43506595e-01  3.10097992e-01  2.00453833e-01 -1.67862214e-02
 -2.71076895e-02  2.37051979e-01  5.52764475e-01  6.84582591e-02
  7.59380311e-02  2.82781899e-01  2.55069174e-02  1.85428172e-01
 -2.47109756e-01 -2.15334922e-01 -1.69400573e-01  1.16818706e-02
  1.97925478e-01  1.55633062e-01  2.00621963e-01  7.02271760e-02
  1.03644125e-01 -1.23877652e-01  5.14737843e-03 -2.12850347e-02
 -2.98287161e-02 -3.30842614e-01  7.08953962e-02 -1.36923209e-01
 -3.22573870e-01 -2.33871430e-01 -1.54102370e-01  5.78264855e-02
  2.73356944e-01 -2.52781045e-02 -1.03253070e-02  2.79845327e-01
  2.05515921e-01 -6.28491491e-02  2.25861728e-01 -1.30943581e-02
  7.17130899e-02  1.84777170e-01  2.24637210e-01 -3.62563193e-01
  4.96461540e-01 -1.68828279e-01  1.54228315e-01 -7.46959746e-02
 -2.45543972e-01 -1.43041968e-01 -6.31855652e-02  1.75542474e-01
  2.98938416e-02  1.26696840e-01  1.67637810e-01 -6.84950501e-04
 -7.98271596e-02 -1.34276107e-01  1.56100929e-01  1.66246854e-02
  1.63725376e-01  8.19835067e-03  2.94245124e-01  2.32510880e-01
  4.35646415e-01 -4.74998832e-01  3.76059592e-01  4.31225002e-01
 -1.31175205e-01 -2.97107518e-01  4.36145216e-01  1.06198095e-01
  1.48075640e-01  1.56344965e-01 -6.59670681e-03 -1.69927955e-01
 -1.70559660e-01 -2.25272536e-01  1.48754448e-01  1.78487897e-01
 -1.31233454e-01 -8.13192874e-02  6.17474392e-02  7.45204650e-03
 -6.77018613e-02 -2.02681571e-01  2.09133416e-01  1.29845202e-01
  1.40746534e-01  2.12892964e-01 -4.12928574e-02 -4.40098830e-02
 -3.43521535e-01  2.33695030e-01  6.43496066e-02  8.05547774e-01
 -2.94403613e-01 -1.79955006e-01  4.15209979e-02 -7.42555484e-02
  4.02119845e-01 -3.01055796e-03 -1.47048801e-01 -3.47131677e-02
  1.19579069e-01 -8.62569883e-02 -4.75919843e-02  1.16629481e-01
  1.66614540e-03  4.59631346e-03 -9.09613520e-02 -3.87249254e-02
 -7.57178292e-04 -1.61495924e-01 -2.14427784e-01 -1.41562119e-01
 -6.93808272e-02  2.16343507e-01 -8.00812840e-02 -4.35415685e-01
  6.92157373e-02  1.57296002e-01 -2.57559270e-01  2.41964370e-01
 -1.14217304e-01  1.64166510e-01  1.42102931e-02  7.35396817e-02
 -6.23339787e-03  4.46225405e-01  9.49615613e-02  1.79253548e-01
  1.87450916e-01 -2.02541575e-01 -1.11446552e-01 -5.59409261e-01
  1.79568231e-01  4.19206709e-01 -6.64101318e-02  4.97616157e-02
  1.27305835e-01  1.32540226e-01 -2.64934182e-01 -3.63930434e-01
  1.97463080e-01  2.74133444e-01 -1.78324088e-01 -9.02345181e-02
 -2.42301803e-02  1.76903099e-01 -9.09680426e-02 -1.67923227e-01
  4.88111615e-01 -5.79907119e-01 -2.06559002e-01  1.33986920e-01
 -8.65286887e-02 -5.04444614e-02  6.34892285e-02  2.18856167e-02
  1.58444300e-01  1.34816870e-01  7.13180080e-02  1.06860489e-01
 -4.80142236e-01 -9.29866508e-02 -3.37235630e-01 -6.98347762e-02
  2.40357235e-01 -3.62026334e-01 -2.50041991e-01 -2.43653581e-02
  1.83016539e-01  1.98599324e-01 -5.88092022e-02  2.28117272e-01
 -1.32358044e-01 -1.91826209e-01  1.08696986e-02  1.83219798e-02
  8.92650895e-03 -2.24094003e-01 -3.29492807e-01 -2.04358310e-01
 -4.17093724e-01 -1.39259964e-01  1.67170897e-01 -3.90026420e-01
  7.05597103e-02  6.80485070e-02 -2.50429243e-01  1.63081110e-01
 -3.41009069e-03  8.55896771e-02 -2.30418131e-01  3.70858848e-01
  7.93432668e-02 -2.44512558e-01 -2.92288989e-01 -3.35169137e-01
 -4.21801686e-01 -1.11473560e-01 -9.34200138e-02  1.46253079e-01
  2.20855270e-02  1.58899322e-01  2.69298274e-02 -2.97357403e-02
  2.94221580e-01  4.12171558e-02  3.31405014e-01  1.37138158e-01
 -1.10729605e-01 -8.48526955e-02 -1.92493320e-01  3.36798698e-01
 -3.72590423e-01 -1.39647663e-01  1.74231082e-03  9.19883978e-03
  3.68391335e-01  3.09735745e-01 -1.00687228e-01 -4.43252027e-02
 -1.81075752e-01  2.84299612e-01 -7.05716088e-02 -9.78927594e-03
  2.35305533e-01  2.22960025e-01  4.49163437e-01  1.64328560e-01
 -1.09554574e-01  2.51018792e-01  2.00977564e-01  1.39609408e-02
  3.21952224e-01  3.05968195e-01 -4.30198312e-02  3.44385922e-01
  3.16482276e-01  3.20910037e-01 -5.27879357e-01  5.21502376e-01
  5.69484606e-02 -1.39407590e-01  3.08742672e-02 -2.20914215e-01
  4.48433220e-01 -5.15649557e-01 -7.09847361e-02  3.76218557e-02
  4.81938303e-01  1.21334612e-01 -1.33607507e-01  1.87439784e-01
  3.49395424e-02  3.67987454e-01 -2.80852467e-01 -1.29358381e-01
  2.40593359e-01 -3.07995141e-01 -1.11086190e-01 -5.37013054e-01
 -2.79348195e-01  1.54726416e-01 -2.92111158e-01  1.76108271e-01
 -3.76073085e-02  2.04475731e-01 -2.30235949e-01  2.73240134e-02
  3.66736725e-02 -2.68743932e-02  1.39947101e-01  3.32880706e-01
 -1.90281808e-01 -2.67295409e-02  3.64266157e-01 -3.93446058e-01
 -5.98417744e-02 -7.41233826e-02  1.02309138e-01  7.50957802e-02
  4.81659800e-01 -2.73540199e-01  1.23210877e-01 -1.28693953e-02
  1.24598995e-01  1.83370858e-01 -9.57779437e-02  1.91109210e-01
 -5.18642724e-01  4.36360419e-01  1.42840832e-01 -1.72631875e-01
  2.97535509e-01 -2.01077521e-01 -2.55758226e-01  5.54088466e-02
  3.23796123e-01 -3.00541520e-01 -4.42583114e-04 -4.18282568e-01
 -7.35731274e-02  2.74597168e-01 -1.30535930e-01 -2.36290336e-01
 -3.27495188e-02  6.57823533e-02  2.76485085e-02  1.87695757e-01
 -4.49724674e-01  7.98115283e-02 -2.06965208e-03 -3.75630736e-01
 -2.88585752e-01 -1.60527527e-01 -2.90983357e-02 -3.73528361e-01
  3.82496379e-02 -1.32128924e-01  4.81433094e-01  4.17658806e-01
  2.09428091e-02  2.64902800e-01 -1.58342302e-01  1.78480074e-01
 -4.74459708e-01 -1.15590647e-01 -1.27485007e-01  3.91137302e-01
  1.38612002e-01 -1.54199496e-01  6.84032977e-01  2.83872068e-01
 -1.95067316e-01  6.26327768e-02 -3.23266178e-01  6.56213760e-02
  2.51504406e-02 -3.17446530e-01 -1.05265841e-01 -3.77217233e-02
  4.15513664e-03  4.68831182e-01 -1.84373930e-01  1.00650609e-01
 -4.19570386e-01  4.67490181e-02  4.79013741e-01 -5.49131989e-01
 -4.47838873e-01 -1.22739501e-01 -7.77759552e-02 -3.30995172e-01
 -6.86874390e-02  9.66847315e-02  1.95618570e-01 -3.95585746e-02]"
Nvidia RTX 3500 Ada generation graphic card not recognised by tensorflow? stat:awaiting response type:build/install subtype:windows TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

v11

### GPU model and memory

RTX 3500 Ada generation mobile 12GB

### Current behavior?

The RTX 3500 card cannot be recognised by tensorflow. I ran tf.config.list_physical_devices('GPU') and tf.test.is_gpu_available(). The results returned ""0 GPUs available"" and ""False"".

The microsoft visual studio 2017 is also installed.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))

tf.test.is_gpu_available()
```


### Relevant log output

_No response_",False,"[-0.29037338 -0.22904071 -0.3091994   0.27647394  0.3471747  -0.21611288
 -0.39564642  0.16979793 -0.17445922 -0.41416335 -0.02079206 -0.0583101
 -0.2684397  -0.11466247 -0.2356353   0.3536096  -0.21155143  0.02453271
  0.22442451  0.1868265  -0.32586572 -0.28180993 -0.39894146  0.15573017
  0.12485598  0.1121154  -0.1451082  -0.0779909   0.08538809  0.22079828
  0.37268668  0.26064348  0.26653686  0.15215573  0.08045816 -0.12752002
 -0.03855406 -0.2621187  -0.38138452 -0.17462112 -0.08667418  0.14844504
  0.07073621 -0.09176157  0.20213515 -0.069607    0.06624296 -0.26322418
 -0.13643864 -0.10787755 -0.15181082  0.01048501 -0.32044154 -0.35238242
 -0.1981534  -0.08143037  0.14563766 -0.14109027  0.18345398  0.286743
  0.16079195 -0.2087177   0.21662916 -0.02958585 -0.18664806  0.12191501
  0.02728561  0.05290086  0.5135205  -0.45397806  0.2655456  -0.2683776
 -0.35621253  0.134709   -0.02516445  0.07762123  0.28203648  0.01646543
  0.19038107 -0.253393   -0.0347291  -0.22911097 -0.04956951 -0.28382978
  0.2551912   0.11836588  0.29972386  0.1257726   0.24894208 -0.32324803
  0.4090669   0.2875561   0.12042134 -0.01865615  0.4262504   0.04274134
  0.23942222  0.08787025 -0.12149816 -0.09042007 -0.09942113 -0.06250681
 -0.05544374  0.1661027  -0.2137667  -0.20438866  0.37002158  0.18802842
 -0.01477029 -0.25180113  0.2626205   0.12161139  0.06064746 -0.15149747
 -0.3078032   0.14912021 -0.29010218  0.09891549  0.11980268  0.7570183
  0.02271672 -0.20245495  0.09048097  0.04414435  0.33145624 -0.17922068
 -0.28212106  0.12900646 -0.0789889   0.06931979  0.13309145  0.1630308
  0.3808779   0.07987841  0.07789294 -0.01958849 -0.07327439 -0.02818144
 -0.260732   -0.30100822 -0.08479844  0.40085435 -0.20627527 -0.6777427
 -0.1055155   0.08600782 -0.24112579  0.26050606 -0.08947467  0.11629143
  0.13151275  0.13611092 -0.07425235  0.5838942   0.07729617 -0.12496115
  0.4899066  -0.01557107 -0.23193972 -0.6070398   0.06322332  0.3196915
 -0.02868336 -0.15059885  0.15709212  0.05588037 -0.51200104 -0.11474132
  0.393582    0.41586292 -0.18886359 -0.1901985   0.1816789   0.3102197
  0.23807523 -0.17531694  0.14643213 -0.25799903 -0.25457606  0.4564288
  0.15967901 -0.17951837  0.33061245 -0.0643514  -0.02757705  0.23620671
  0.30927673  0.16112497 -0.16764316 -0.09280281 -0.50940526  0.12019654
  0.16058016 -0.20789805 -0.33728474  0.1130936   0.06480776 -0.09369509
  0.04097833 -0.00084487 -0.2352187  -0.08474994  0.15741035  0.3447938
 -0.03782094 -0.02717582 -0.21299323 -0.47631148 -0.7956513   0.13822919
  0.21008188 -0.3734374   0.12727699  0.19195455 -0.09138459  0.29289824
  0.02205097 -0.15870485 -0.19376433  0.24899301  0.18649958 -0.28040856
 -0.02255409 -0.3050756  -0.22905691  0.01171035 -0.31771088 -0.19544691
 -0.20555621  0.20638251  0.03254126 -0.132985    0.39776346  0.050891
  0.73343414 -0.07646526 -0.33956593 -0.18536156 -0.24634121  0.14442483
 -0.22850658 -0.13445997 -0.36301062  0.02711168  0.2623568   0.45071965
 -0.06311209 -0.08358849 -0.26664335  0.4004488  -0.04334779  0.00986565
  0.32259524  0.22161198  0.22547138  0.2212659   0.10334287  0.37752885
  0.1723414  -0.06511123  0.34522307  0.13529661 -0.1028432   0.28006828
  0.13515246  0.34340328 -0.46113756  0.56417704 -0.20568633 -0.2166199
  0.0178325  -0.22546339  0.62312925 -0.4098407  -0.07514969 -0.28201586
  0.33889008 -0.04405285 -0.14668477 -0.15714425  0.28595734  0.38389096
 -0.28237998  0.11627865 -0.1526767  -0.04404422 -0.11760201 -0.8078214
 -0.04848304  0.15963337 -0.25247866  0.08915186  0.02087527 -0.09366605
 -0.06198249  0.26482928 -0.29314825 -0.0068777  -0.2181802   0.11811427
 -0.18896401  0.1978133   0.50069374 -0.3781523  -0.10409384 -0.37742212
  0.46760863  0.24439429  0.48015854 -0.5669352  -0.02657766 -0.27170366
  0.11227011  0.29470775  0.00380342  0.04293114 -0.5368724   0.5055493
  0.02233891 -0.02468389  0.15025772 -0.15627533 -0.43351066  0.12250441
  0.1722864  -0.08482532 -0.20125532 -0.49483627  0.02682991  0.08463614
 -0.16112855 -0.11665009  0.02711998 -0.02825969 -0.10746922 -0.03917006
 -0.21703     0.25013763  0.00332452 -0.15477931 -0.2012128  -0.09399167
  0.1721946  -0.12404748 -0.1553635  -0.40184432  0.39241207  0.35510713
  0.2655737   0.12284561 -0.00303604  0.24408393 -0.36061892  0.22302732
  0.14071384  0.13895862  0.16607997 -0.04806468  0.40606552  0.23356138
  0.0145161   0.25324404 -0.52512753 -0.05169521  0.15912786 -0.27451885
 -0.37704045 -0.28102398  0.06891169  0.4126693  -0.02068683  0.3755327
 -0.20728664  0.37412465  0.43795103 -0.39028567 -0.13250315  0.15546298
  0.11412509 -0.06974909  0.24546823 -0.14070609  0.22520712 -0.1084508 ]"
Error starting Tensorflow in Python stat:awaiting response type:build/install type:support stale type:others TF 2.13,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Rocky Linux 8.8

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12

### GPU model and memory

24 GB

### Current behavior?

Unable to load tensorflow in python with CUDA 12.2. Looking for some pointers

### Standalone code to reproduce the issue

```shell
pip install tensorflow
python
>>> import tensorflow as tf
 python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""                                                                                
2023-08-24 15:39:47.107770: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-08-24 15:39:47.108805: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.                                                                          
2023-08-24 15:39:47.130307: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.                                                                          
2023-08-24 15:39:47.130612: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.                       
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.                                                                            
2023-08-24 15:39:47.496076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT                                                                                      
2023-08-24 15:39:48.008342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355                                                                                
2023-08-24 15:39:48.023762: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would
like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.                                                              
Skipping registering GPU devices...
[]
```


### Relevant log output

_No response_",False,"[-0.6178125  -0.47702354 -0.16113046  0.35564074  0.22382587 -0.40210557
 -0.2304974   0.16984549 -0.26513818 -0.31699595  0.02418645  0.01937883
 -0.24521229  0.07694098 -0.27595294  0.36454827 -0.28300786 -0.09982328
  0.23167822  0.2144457  -0.02991903  0.10330208 -0.3208867   0.18353531
  0.12729326  0.13507329 -0.3356328  -0.05933975  0.06019507  0.24441414
  0.39981627  0.08878401  0.06525029  0.19636154  0.08007582  0.34944412
 -0.19819489 -0.29267278 -0.2958253   0.03568276  0.1480645   0.01209021
  0.26717037 -0.15677437  0.11567763 -0.34124702  0.13291219 -0.18428048
  0.08698162 -0.28801382 -0.1402854   0.0694458  -0.3924968  -0.44611275
 -0.15032828 -0.21354684  0.19876938  0.14937988 -0.06182736  0.0593762
  0.11462577  0.03973814  0.05500519 -0.2008301   0.04369185  0.21474972
  0.18524125 -0.11450657  0.58796614 -0.3093256   0.12820789 -0.01446873
 -0.45841363  0.06683033 -0.16892867  0.16230476  0.05307299  0.07019428
  0.2473543  -0.19757345 -0.11153837 -0.19277808 -0.0909844  -0.35993162
  0.13524377 -0.03381652  0.4083666   0.19822061  0.4534955  -0.1892386
  0.61752725  0.3404073  -0.04418097  0.14325279  0.42080593  0.0805458
  0.06699651  0.09313411 -0.01797423 -0.19851562  0.06253366 -0.27654886
 -0.06387877  0.14362204  0.0341295  -0.16619888  0.12234646 -0.15906596
  0.13632472 -0.23507734  0.2766761  -0.08412709  0.29445842 -0.06040529
  0.00151279  0.00981616 -0.33280626  0.20738904 -0.13543507  0.93259346
  0.1535474   0.05002093  0.11038678  0.18254463  0.40421718  0.00344938
 -0.06096487 -0.06920718  0.03660977 -0.05329709  0.20013392  0.08633944
 -0.29912066  0.34486064 -0.13148504  0.10114419 -0.00308867 -0.2469588
 -0.1637467  -0.14916208 -0.14226185  0.04539931 -0.24843214 -0.7630021
  0.11434402  0.06076628 -0.20285332  0.23077229 -0.19630453  0.11811867
 -0.0706934   0.12696987 -0.39823848  0.40776846  0.07541887  0.09502669
  0.39572766 -0.00373763 -0.13178843 -0.6831199   0.09058318  0.5395664
 -0.12812766 -0.19739646  0.11063761  0.1442824  -0.38893706 -0.4826347
  0.06583393  0.51022196 -0.21309143 -0.05122211  0.11347732  0.06230495
  0.00837244  0.03168821  0.3044244  -0.6656819  -0.06743865  0.44149274
  0.13317668  0.35407037 -0.01339434  0.14936753  0.1214371   0.05696471
 -0.22626562 -0.01978171 -0.12070911  0.11453307 -0.32137892 -0.26985928
  0.3373282  -0.16860303 -0.08284302  0.09562286  0.17185119 -0.09718736
  0.0302766  -0.0230919  -0.24358308  0.04596561 -0.14176913  0.02853374
  0.00993856 -0.33254674 -0.07337263 -0.29297554 -0.24235423 -0.01342855
 -0.08293067 -0.6223487   0.09239632 -0.11807066 -0.40114126  0.26057655
  0.3165782   0.0811411  -0.22687843  0.10286607  0.16649698 -0.1650951
  0.04323425 -0.38453707 -0.05280592  0.05642606 -0.22346887  0.10573983
  0.0167656   0.2407115   0.13909099  0.02002547  0.38788408  0.30384302
  0.47279692 -0.27622193 -0.07127152 -0.11990042  0.056735    0.14175442
 -0.4479765  -0.00895598  0.07045174  0.1092087   0.19572541  0.24968755
 -0.3528548  -0.18897386 -0.44454104  0.19144115 -0.00969418  0.08352557
  0.28730905  0.20322394  0.49526477  0.30966043  0.0829846   0.24480757
  0.18437788 -0.16437224  0.39776427  0.16832337 -0.00261073  0.29477376
  0.3108531   0.18282294 -0.459229    0.60761917  0.24387062 -0.24222982
  0.09951095 -0.34252083  0.55063057 -0.541532   -0.0256954   0.02358497
  0.44827515  0.10866932  0.08617477 -0.10867789 -0.0343605   0.4257777
 -0.489408   -0.09440456  0.08830342 -0.20076582 -0.0197704  -0.6955787
 -0.34424263  0.10629098 -0.22124608  0.23210153  0.00543759  0.03871723
 -0.15573679  0.09818502  0.06610608 -0.14368413  0.20615067  0.36827016
 -0.23102942  0.01211581  0.26214248 -0.4515351  -0.16799799 -0.11998056
  0.310278    0.4083454   0.4414726  -0.29618782  0.3477776  -0.04918694
  0.01729836  0.69706106 -0.00741614 -0.09523812 -0.33804286  0.7421172
  0.24317083 -0.15798771  0.18875071 -0.3507109  -0.2822309   0.04964713
  0.34694338 -0.0794076  -0.0327761  -0.43854088 -0.1410978   0.15404624
 -0.12773672  0.15284395 -0.04185027 -0.06091422 -0.17151222  0.06336232
 -0.44290063  0.12963414  0.07917773 -0.3358459   0.03221287 -0.14269578
 -0.01510102 -0.42633057 -0.02694287 -0.45106846  0.5948248   0.55932117
 -0.22070411  0.022016    0.09077327  0.16807549 -0.5829899  -0.17466792
 -0.02365602  0.2837312   0.23208421 -0.15641564  0.5628111   0.34252262
 -0.2972217   0.20548812 -0.38938627 -0.00145759  0.323359   -0.21843132
 -0.18915516 -0.08093873 -0.010941    0.25285894 -0.32663667  0.23210962
 -0.3788967   0.32528257  0.5979754  -0.38098422 -0.4616232   0.27467555
  0.06093855 -0.26443315 -0.05731237 -0.06182789  0.19063503 -0.12857492]"
Recent update of xnnpack broke compilation of TensorFlow Lite stat:awaiting response type:build/install comp:lite subtype: raspberry pi comp:lite-xnnpack,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Didn't tried

### Source

source

### TensorFlow version

7701a45e3893bbf8c451f6437d039172db89d548

### Custom code

No

### OS platform and distribution

Raspberry Pi 4, Raspbian 

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Raspberry Pi 4

### Current behavior?

Updating **xnnpack** in https://github.com/tensorflow/tensorflow/commit/7701a45e3893bbf8c451f6437d039172db89d548 broke the compilation with this error:

```
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c: In function init_f16_gemm_config:
gmake[3]: Entering directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
cd /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build && /home/pi/.local/pipx/venvs/cmake/lib/python3.9/site-packages/cmake/data/bin/cmake -E cmake_depends ""Unix Makefiles"" /home/pi/src/tensorflow_src/tensorflow/lite /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/abseil-cpp/absl/strings /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/_deps/abseil-cpp-build/absl/strings /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/_deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_cordz_handle.dir/DependInfo.cmake --color=
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:159:18: error: cpuinfo_uarch_cortex_a715 undeclared (first use in this function); did you mean cpuinfo_uarch_cortex_a710?
  159 |             case cpuinfo_uarch_cortex_a715:
      |                  ^~~~~~~~~~~~~~~~~~~~~~~~~
      |                  cpuinfo_uarch_cortex_a710
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:159:18: note: each undeclared identifier is reported only once for each function it appears in
gmake[3]: Entering directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:162:18: error: cpuinfo_uarch_cortex_x3 undeclared (first use in this function); did you mean cpuinfo_uarch_cortex_x2?
  162 |             case cpuinfo_uarch_cortex_x3:
      |                  ^~~~~~~~~~~~~~~~~~~~~~~
      |                  cpuinfo_uarch_cortex_x2
gmake[3]: Leaving directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
/usr/bin/gmake  -f _deps/abseil-cpp-build/absl/flags/CMakeFiles/absl_flags_program_name.dir/build.make _deps/abseil-cpp-build/absl/flags/CMakeFiles/absl_flags_program_name.dir/build
```

The version of **cpuinfo** downloaded in `tensorflow/lite/tools/cmake/modules/cpuinfo.cmake` doesn't include `cpuinfo_uarch_cortex_a715` or `cpuinfo_uarch_cortex_x3`

### Standalone code to reproduce the issue

```shell
Follow steps from https://www.tensorflow.org/lite/guide/build_cmake_pip
```


### Relevant log output

```shell
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c: In function init_f16_gemm_config:
gmake[3]: Entering directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
cd /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build && /home/pi/.local/pipx/venvs/cmake/lib/python3.9/site-packages/cmake/data/bin/cmake -E cmake_depends ""Unix Makefiles"" /home/pi/src/tensorflow_src/tensorflow/lite /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/abseil-cpp/absl/strings /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/_deps/abseil-cpp-build/absl/strings /home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/_deps/abseil-cpp-build/absl/strings/CMakeFiles/absl_cordz_handle.dir/DependInfo.cmake --color=
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:159:18: error: cpuinfo_uarch_cortex_a715 undeclared (first use in this function); did you mean cpuinfo_uarch_cortex_a710?
  159 |             case cpuinfo_uarch_cortex_a715:
      |                  ^~~~~~~~~~~~~~~~~~~~~~~~~
      |                  cpuinfo_uarch_cortex_a710
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:159:18: note: each undeclared identifier is reported only once for each function it appears in
gmake[3]: Entering directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build/xnnpack/src/configs/gemm-config.c:162:18: error: cpuinfo_uarch_cortex_x3 undeclared (first use in this function); did you mean cpuinfo_uarch_cortex_x2?
  162 |             case cpuinfo_uarch_cortex_x3:
      |                  ^~~~~~~~~~~~~~~~~~~~~~~
      |                  cpuinfo_uarch_cortex_x2
gmake[3]: Leaving directory '/home/pi/src/tensorflow_src/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/cmake_build'
/usr/bin/gmake  -f _deps/abseil-cpp-build/absl/flags/CMakeFiles/absl_flags_program_name.dir/build.make _deps/abseil-cpp-build/absl/flags/CMakeFiles/absl_flags_program_name.dir/build
```
",False,"[-6.58945024e-01 -6.06358767e-01  9.40262973e-02  3.34160663e-02
  8.05259943e-02 -3.98690701e-01 -2.88975418e-01  2.63236929e-04
 -5.37929058e-01 -2.91905820e-01  8.95010903e-02  2.24865954e-02
 -1.85557842e-01  8.41610692e-03 -6.55829757e-02  3.18161488e-01
 -4.45677817e-01 -2.16122776e-01  1.17921896e-01  3.43341269e-02
 -1.22296669e-01 -1.63036156e-02 -4.15004283e-01  5.41973710e-02
  2.60987610e-01  2.04475284e-01 -6.98928237e-02 -7.89120570e-02
 -2.07549483e-02  2.09381610e-01  6.56637073e-01  1.67701215e-01
  2.73389369e-02  2.78587133e-01 -3.87425125e-02  4.27977622e-01
 -3.15707564e-01 -2.09605128e-01  1.79806188e-01  1.34673178e-01
  2.21142769e-01  1.62693322e-01  1.57997489e-01 -6.24417141e-02
  3.10152173e-01 -1.15599342e-01 -1.06957160e-01 -3.62642616e-01
  1.77748263e-01 -2.14557081e-01  5.87543100e-02  3.00710145e-02
 -2.92109907e-01  1.41897798e-03 -1.54079795e-01  5.52049875e-02
  3.99891943e-01  8.96585956e-02 -1.06359143e-02  3.48057508e-01
  1.70240685e-01  2.19173152e-02 -1.68999940e-01 -1.22304596e-01
  1.45959496e-01  1.11606196e-01  3.60641032e-01 -1.95205420e-01
  5.75660646e-01 -1.50809720e-01  2.94682503e-01 -1.74662799e-01
 -3.61764014e-01 -1.27983779e-01 -1.08412385e-01  3.21697444e-01
 -1.37382761e-01  8.23658034e-02  3.17660481e-01 -2.20467165e-01
 -2.55504221e-01 -2.45302111e-01  2.70101517e-01 -5.73985241e-02
  2.14939684e-01 -4.90169786e-02  1.80514231e-01  8.68958309e-02
  3.69703650e-01 -1.80671990e-01  3.58789563e-01  5.07152677e-01
 -8.54395181e-02  7.14810118e-02  4.64713156e-01  5.74568026e-02
 -2.68320907e-02  9.96671170e-02 -2.85339169e-03 -2.00098872e-01
 -3.53710502e-01 -1.46899015e-01 -1.99361756e-01 -5.54551631e-02
 -1.38016447e-01 -1.18038051e-01  4.26133335e-01 -7.99613073e-04
  1.10449120e-01  6.07727766e-02  2.42740318e-01 -7.87779763e-02
  1.88968062e-01  1.18681788e-01  1.28966361e-01  1.61699265e-01
  4.27239649e-02  2.82051474e-01 -3.03264081e-01  9.45085168e-01
 -1.07145078e-01  9.77565497e-02  1.10382967e-01  2.27457389e-01
  1.48895338e-01  3.54061052e-02  2.31426023e-02 -3.44376564e-02
  9.96969547e-03  7.94913471e-02 -2.25095928e-01  2.02836543e-01
 -3.30764130e-02  3.07041407e-01 -1.12132952e-02  2.07466632e-02
 -1.07112847e-01 -2.76380062e-01 -3.61824483e-02 -1.86130673e-01
 -2.54365802e-01  2.10173070e-01 -2.70046473e-01 -5.24418771e-01
  1.94669738e-01  8.25148076e-02 -3.50209236e-01 -6.38496969e-03
 -1.48948491e-01 -6.92814589e-04 -2.33320817e-01  1.31103948e-01
 -1.84360176e-01  5.83687425e-01  1.15736499e-01  3.39664400e-01
  4.92648035e-01 -8.82831588e-02 -7.42977113e-02 -4.61646229e-01
  2.03166395e-01  5.10102510e-01 -8.64614397e-02 -8.39109346e-02
  3.48124444e-01 -1.20686144e-01 -5.76699734e-01 -2.76217647e-02
 -7.67969191e-02  2.43451372e-01 -2.00354606e-01 -1.56847656e-01
 -6.74347132e-02  8.37908238e-02 -7.13316649e-02 -1.33524105e-01
  5.01116633e-01 -6.15214705e-01 -2.36827746e-01  2.23852381e-01
  1.92581683e-01  9.29908976e-02  2.55686581e-01  3.28460243e-03
  1.32078916e-01  1.51690274e-01  9.82503816e-02  1.52058944e-01
 -1.56801015e-01 -3.16934437e-02 -4.96086925e-01 -1.53633505e-02
  2.04531461e-01 -1.62464917e-01 -2.15560328e-02  1.65102586e-01
  2.15708211e-01 -1.21874446e-02  4.77017686e-02  3.61602269e-02
 -2.23377034e-01 -3.99454236e-02 -1.20402098e-01 -5.84521443e-02
 -8.81619565e-03 -3.87355804e-01 -3.56435329e-01 -3.25135499e-01
 -4.88091767e-01 -2.61834949e-01  1.32152196e-02 -3.89829159e-01
 -9.09828097e-02  1.12550311e-01 -1.97657585e-01  3.52493197e-01
  1.17058888e-01  3.37001532e-02 -2.39644736e-01  2.86082238e-01
  3.92278433e-02 -4.34572518e-01 -1.66457474e-01 -4.63185817e-01
 -2.27974549e-01 -2.24811323e-02 -2.71370828e-01 -3.80817801e-03
  1.29797265e-01  1.95629925e-01  3.06441933e-02 -6.84115216e-02
  4.16600615e-01  1.80199653e-01  3.60030919e-01 -1.83422416e-01
 -1.49285048e-01 -1.29030362e-01 -3.15146565e-01  2.70997643e-01
 -3.07619333e-01 -6.80004358e-02 -1.90177113e-02  1.38759300e-01
  1.95716828e-01  2.44506821e-01 -1.85901865e-01 -7.46322870e-02
 -1.81252971e-01  4.28711772e-01 -6.14240691e-02  4.99955285e-03
  2.84530461e-01  3.06492865e-01  4.44628417e-01 -8.95409733e-02
 -9.68219340e-02  2.34990597e-01  1.55844241e-01  1.19429238e-01
  5.23681819e-01  1.74579278e-01 -1.34563938e-01  2.44508117e-01
  1.72976032e-01  1.54568836e-01 -4.08071458e-01  5.25152564e-01
  2.34391987e-01 -2.37808883e-01  1.56826198e-01 -1.54255301e-01
  1.83574721e-01 -3.27587664e-01  2.95805819e-02  1.19513810e-01
  2.31408373e-01  9.22143310e-02 -3.36833209e-01 -2.39778757e-01
 -6.97292984e-02  4.97695714e-01 -6.43331647e-01 -1.30659133e-01
  9.32620019e-02 -3.85988116e-01  9.26146656e-02 -5.26208639e-01
 -4.18625385e-01  8.50327834e-02 -5.31087071e-02  1.73469707e-01
  2.11883649e-01  1.90153569e-01 -1.07486844e-01 -7.18180090e-05
  1.59280270e-01 -6.29822016e-02  1.61692426e-01  4.17905420e-01
 -4.59134802e-02  6.18357845e-02  3.60476434e-01 -4.45738971e-01
 -2.09264364e-03 -2.03053594e-01  1.61807597e-01  1.40676945e-01
  5.36623478e-01 -3.36503834e-01  1.85703009e-01 -6.80664927e-03
  1.54495183e-02  3.11565876e-01 -1.30724221e-01 -1.11927390e-01
 -2.95066804e-01  6.79177403e-01  1.71142906e-01 -3.78935099e-01
  3.37206185e-01 -6.46419972e-02 -2.30127677e-01  1.01274028e-02
  3.20677936e-01  7.93034956e-03 -1.61320537e-01 -2.54780799e-01
 -1.74598932e-01  3.30582291e-01 -1.37173459e-01 -9.47319251e-03
 -2.54185349e-01  1.05567545e-01  3.14223468e-02 -2.19664797e-01
 -3.64760101e-01  3.87370348e-01 -9.85696614e-02 -3.75707716e-01
 -1.25087649e-01  1.55150414e-01  9.68833715e-02 -3.21477592e-01
 -1.56051278e-01 -3.13075066e-01  4.96901840e-01  2.01934814e-01
 -5.29014349e-01 -3.66075933e-02 -2.21639983e-02  2.84922361e-01
 -2.16718704e-01 -7.79722184e-02 -4.16113064e-03  4.63868856e-01
 -1.40078411e-01  1.08932517e-01  4.97499943e-01  5.68963766e-01
 -3.71233046e-01  1.56489134e-01 -3.20892125e-01 -1.01639070e-02
  2.43044019e-01 -2.12151051e-01  2.48600412e-02 -9.30200219e-02
  9.77366567e-02  5.43460250e-01 -1.21245764e-01  2.06653357e-01
 -3.38869572e-01  2.63346940e-01  4.77725208e-01 -4.61405069e-01
 -3.90804589e-01  3.20902377e-01 -1.12172469e-01 -4.55694422e-02
 -8.32479633e-03 -1.10765263e-01  1.35762289e-01 -9.69587862e-02]"
Image storage type:support comp:lite,"When TFLite processes image data, which variable will the read in image data be stored in? Is it TfLiteTensor.data?


How can I directly process a single image using TensorFlow Lite source code without relying on an Android app?",False,"[-0.03822931 -0.40714198 -0.0891834  -0.15411837  0.2527532  -0.06068759
 -0.06268993 -0.00550272 -0.13058409  0.2077844   0.11680041  0.07779627
 -0.0946508   0.32535398 -0.0655092  -0.023179   -0.04404993  0.01281232
  0.13664757  0.20592168  0.272291   -0.17879377  0.06284693 -0.23466052
  0.16011854  0.34672973  0.1170024  -0.38146973 -0.01662908  0.21829906
  0.12997079 -0.12179904 -0.04632572  0.2523721  -0.01242611 -0.00592541
 -0.38434643 -0.05162764 -0.17795405 -0.01106688  0.02527661 -0.03632181
 -0.00153017 -0.21919598  0.12464076  0.33531517 -0.01085725 -0.10112044
 -0.32730165  0.0724469  -0.00228429 -0.12205426 -0.3349301   0.06671044
 -0.14233777  0.30890843  0.12994784 -0.17039424  0.11513116 -0.12768497
  0.12409171 -0.02405815  0.27802148 -0.18643881  0.19551885  0.26196995
 -0.02667135 -0.07646494  0.33015698 -0.7217566   0.03122785  0.2923459
 -0.13912138 -0.12155971 -0.32159728  0.01286789 -0.0467138   0.23690712
  0.03481995 -0.34012735  0.27214634 -0.20980197  0.13661988  0.15609387
  0.2005233  -0.02657766  0.13195585 -0.04752573 -0.10252048  0.17485476
  0.02109457  0.4089843  -0.27277225  0.14183076  0.00153005 -0.05869814
 -0.09767032  0.0626354   0.04219642 -0.03481253 -0.14311019 -0.02713169
  0.00991775  0.22434089  0.07860434 -0.08084394  0.11866383  0.23175877
 -0.17209822  0.1573264   0.11279839 -0.0252262  -0.13445736 -0.07562885
  0.08068765 -0.09228516 -0.03482962  0.10285027 -0.08073286 -0.01301158
  0.1170077  -0.45221663 -0.01564623 -0.09336928  0.25356972 -0.28344867
 -0.12797737 -0.34706274 -0.04793731 -0.04149553 -0.00732427 -0.007507
 -0.2144961  -0.21922562  0.17676449  0.31836256 -0.32975042 -0.54871625
 -0.3841033  -0.10117094 -0.14147773  0.19290715 -0.27756655 -0.01549471
  0.23435557  0.32995218 -0.06360365 -0.10196774 -0.37537026 -0.09082313
  0.01191459 -0.06546266 -0.06985418 -0.01009929  0.04081733 -0.2964387
  0.2531662  -0.03688791 -0.09950037  0.04047643 -0.1462      0.16608578
  0.01865192  0.06276527  0.05728077  0.05718429 -0.2516513  -0.10599526
  0.11899757  0.16954997  0.01171591 -0.1691477  -0.06298836  0.01233611
 -0.04551373  0.01454501  0.07597622 -0.17038682  0.25388542 -0.25557825
  0.11354797  0.23257548  0.19241521  0.08841658 -0.08854709 -0.40866697
  0.28904888  0.26506948 -0.00951375 -0.05544352 -0.13264577  0.2516674
  0.0261832   0.08119389 -0.01016943 -0.11804443  0.08810413  0.29557243
 -0.14980814  0.10299158  0.22127147 -0.07377464 -0.19095646  0.02948826
 -0.12307222 -0.03602251 -0.3918773  -0.03645507 -0.29166898 -0.09410391
  0.21081078 -0.20138893 -0.5525015  -0.05363379  0.09115101 -0.02792932
 -0.04815016 -0.22071844 -0.15608469  0.3620575  -0.11212677 -0.24746357
  0.04634655 -0.09096172 -0.25026348  0.00960874 -0.2743325   0.2819132
 -0.24429256  0.19285724 -0.12126508 -0.13184643  0.34252536 -0.09654699
 -0.02325208 -0.05976305 -0.20834382 -0.2029435  -0.44137555 -0.07316677
  0.01057295 -0.5425463  -0.22796744 -0.24795671 -0.24429594 -0.03342417
  0.48842448  0.21988413 -0.13754429  0.36078292  0.05369862 -0.0944355
  0.40222543  0.10035102  0.3470509  -0.24852662  0.05407757 -0.24938346
 -0.17795768 -0.05498936  0.33577082  0.29072216  0.2346821   0.44406623
  0.24297251  0.10341544 -0.02635707 -0.03347225 -0.03866352 -0.25839478
  0.05312026  0.15226436  0.1733444  -0.01951833  0.19472522 -0.07941087
  0.0646085  -0.05465879  0.01074151  0.26737761  0.21907301  0.35311186
 -0.40529048  0.08662759  0.27097562 -0.17916098 -0.34338468 -0.35523015
 -0.2788846  -0.12669183 -0.0014252  -0.30799925  0.01935573  0.17894465
 -0.01637101 -0.22118387  0.43913573  0.30655712  0.24775082 -0.02112512
 -0.2503671   0.06788111  0.27986786 -0.10291456  0.03343042  0.0447879
  0.07057963 -0.06344788  0.20452975 -0.14282392 -0.06914121 -0.25897577
 -0.05718713 -0.12582573 -0.01232699  0.01590119 -0.07493582  0.13583186
  0.1555657   0.00907546 -0.0442253  -0.26738918  0.1915996  -0.03811882
 -0.04470496 -0.06800023  0.5958959   0.2394202   0.33666807 -0.34567723
 -0.24112459 -0.32987788  0.1606466   0.20741096 -0.20416713 -0.1037697
  0.15694445  0.00542387 -0.22407532 -0.16184989 -0.07231931 -0.36991218
 -0.20391569 -0.0320908   0.0462786   0.03646842  0.25211346  0.0695973
 -0.02491824  0.02676624 -0.04881006  0.20383415  0.36651975  0.16572708
  0.35606647  0.29191265  0.10960248 -0.02324438 -0.00425798  0.14960442
  0.17219107  0.23002207 -0.048988   -0.26742747  0.14708515  0.0520733
  0.26974112 -0.19309692  0.1971595   0.63394344 -0.16438341  0.018631
 -0.11780843  0.06438271  0.26449788 -0.01948287  0.3686124   0.02927902
  0.3624512   0.28129986 -0.06220012 -0.12654959  0.07192379  0.20571752]"
Use old project folder on tensorflow 2.13 using wls stat:awaiting response stale subtype:windows wsl2 TF 2.10,"Hi everyone, I was working on a project using tensorflow 2.10 with native gpu support. I wrongly updated the tf version to 2.13 that does not support gpu. So, I installed tensorflow 2.13 using wls2, now i don't know how to use the previous folder of my project on the conda environment created. Or intsead how to reinstall tf 2.10",False,"[-0.33282048 -0.23867504 -0.10802641  0.01499195  0.14515607 -0.01486906
 -0.3646217   0.12401062 -0.14116053 -0.21334675 -0.07959349  0.19162032
 -0.07159714  0.13548276 -0.23521118  0.26479194 -0.20348498 -0.10427584
 -0.05470808  0.05677571 -0.3914641   0.00974519 -0.0577797   0.2506495
  0.29532778 -0.00939579 -0.15935469 -0.25981838  0.21814555  0.24048258
  0.11069396 -0.10226275 -0.33014295  0.10799737  0.07973753 -0.04381921
 -0.19370969 -0.09388532 -0.18529363 -0.07230576  0.11105932  0.0476662
 -0.00722012  0.0207691  -0.21305236 -0.061565    0.20024809 -0.04674363
 -0.20457244  0.24789031 -0.04086943 -0.02679018 -0.24193802 -0.13566558
  0.1102647   0.35469902  0.04562189  0.40790206  0.01764683 -0.0280045
 -0.01740177  0.0171517   0.23610772 -0.03013789  0.05671357  0.04009118
  0.09754478 -0.21703574  0.61785823 -0.38265088 -0.03400313  0.11327721
 -0.21011172 -0.14143166 -0.04218318  0.01265189 -0.11643041  0.17180717
  0.23714109 -0.25699574  0.18503575  0.01282517  0.08509538 -0.00522195
  0.07899902 -0.11081127  0.06330884 -0.09087552  0.0260947  -0.14363322
  0.53480464  0.06387389  0.30896848  0.17393734  0.24461438  0.13985819
 -0.15132867  0.7038303   0.11983073 -0.01499019 -0.19969764 -0.5503678
 -0.14826207  0.17783073 -0.10772827 -0.21775894  0.11080325  0.13422364
  0.08218998 -0.09272684  0.19794114 -0.02122412  0.14937982  0.09739824
  0.07100447 -0.03160047 -0.2307097   0.20551136 -0.02153233  0.3122196
  0.08096531 -0.30979657  0.1845674  -0.16901685 -0.06649535  0.05196788
 -0.09658089 -0.22284323  0.2698018  -0.02236494  0.1535177   0.28712988
 -0.28532073 -0.14784874  0.10538471 -0.00863781 -0.49171612 -0.26894012
 -0.16823073  0.05552194 -0.26812086 -0.09104335 -0.05192374 -0.20387514
 -0.09345586  0.15560903  0.27397737  0.32597187 -0.03815483  0.49326986
  0.02218654 -0.05000387  0.01840652  0.20630614  0.1293246   0.09153109
  0.13941215 -0.07644671 -0.11789888 -0.41969797 -0.11619794  0.28910196
 -0.21569404 -0.06317069  0.0134775   0.24046677 -0.35126606 -0.14541578
  0.2507316   0.23903714  0.14708754 -0.1423592   0.20524667 -0.12904198
  0.43436748 -0.11263391  0.32947728 -0.27860764  0.06712354  0.08073416
 -0.01090538  0.01683008  0.03734025 -0.06670217  0.04306575  0.04129299
  0.1911187   0.08031587 -0.14466143  0.17066826 -0.13747668 -0.01609892
  0.21912175  0.16459283 -0.11087342 -0.18718955  0.23311527  0.03713184
  0.10619903  0.04308765 -0.04492296  0.18590793  0.17317115 -0.00159721
  0.23627043 -0.17070971 -0.17292972 -0.26207995 -0.21364675 -0.25274238
  0.15627775  0.12204199 -0.22554262 -0.02523977 -0.26670152  0.06273637
 -0.03119296 -0.17110328 -0.22575423 -0.03411575  0.05443735  0.14857543
  0.07648899 -0.07475556 -0.17634918  0.04373715 -0.04346194  0.1467781
 -0.11031447  0.2986956  -0.24060191 -0.03060493  0.11733587 -0.01919261
  0.3620749  -0.24136047 -0.01497767 -0.12490141 -0.04791023  0.00361958
 -0.31879753 -0.16012128 -0.3153034  -0.19184321 -0.02349192  0.2852129
 -0.13935132 -0.07745481 -0.0414789   0.19736032 -0.0013713  -0.13999805
  0.2511265   0.10674085  0.13143307  0.02453731  0.05079139 -0.09466603
  0.07203272  0.1184889   0.17655282  0.09440335  0.07705075  0.36666277
  0.0996955   0.23512875 -0.00149706  0.08146604 -0.24463959  0.20158923
 -0.10625912 -0.24009687  0.19235341 -0.23235404  0.08444937  0.09835108
  0.40835837 -0.12161242 -0.12566784  0.11777259  0.24318217  0.55705816
 -0.2538605  -0.06657167  0.17791247  0.01450103 -0.28994718 -0.35229033
 -0.15747762 -0.01019051 -0.16023351 -0.23435637 -0.23963077 -0.27839577
 -0.32995543 -0.11755279  0.11323982  0.0008509   0.22545223 -0.14456996
 -0.30989164  0.19441104  0.3644213   0.00783918  0.04557015 -0.18136376
  0.02622654 -0.04184246  0.20191845 -0.21306789  0.29801142 -0.05804608
 -0.00806447  0.30999365  0.04385523  0.09924553  0.02196103  0.2645101
  0.16076168  0.00661878  0.08313233 -0.0989591  -0.17818208  0.05608686
  0.19118598  0.18865003 -0.07319022  0.13067475  0.0125841   0.28270495
 -0.14289981 -0.17960955 -0.17680947  0.07244822 -0.47041065 -0.13189383
 -0.21283282  0.16470902  0.20075631 -0.33185232 -0.03175541 -0.3534392
 -0.10331087 -0.29003295 -0.06061253  0.03354426  0.18750523  0.16347857
  0.1322214   0.09245384 -0.09487724  0.004253   -0.14529142 -0.09787055
 -0.2866194   0.14692761  0.08359492 -0.02997372  0.11697509  0.3073723
 -0.0398201   0.17722428  0.06463502 -0.23635608  0.05353022 -0.20055988
 -0.20800076  0.04952905  0.13503066  0.44893378 -0.011033    0.19387703
 -0.28549594 -0.07657611  0.18943818 -0.0925869  -0.11918065  0.08917604
 -0.09093204 -0.18164277 -0.29117242  0.06487706 -0.06477984 -0.20172535]"
Performance difference of passing np array vs tf tensor into tf.train.FloatList stat:awaiting tensorflower comp:apis type:performance TF 2.13,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

docker tensorflow/tensorflow:latest-gpu and Ubuntu

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA Version: 12.0 NVIDIA_SMI 525.125.06

### GPU model and memory

RTX4090

### Current behavior?

Processing numpy array is much faster than processing the same tensor. This is likely connected to https://github.com/tensorflow/tensorflow/issues/30372 (they also serialize TF tensors). Happy to submit a workaround (conversion from tensor to np array). Both CPU and GPU are affected.

### Standalone code to reproduce the issue

```shell
""""""Easily reproducible in Google Colab:
https://colab.research.google.com/drive/1gzI7kYRICSepX7OpC715zYj5_0qwrbPx
""""""

import numpy as np
import tensorflow as tf
import timeit


np_val = np.random.random(size=(10_000))
tf_val = tf.constant(np_val)
  
print(timeit.timeit(lambda: tf.train.FloatList(value=np_val), number=10))
print(timeit.timeit(lambda: tf.train.FloatList(value=tf_val), number=10))
print(timeit.timeit(lambda: tf.train.FloatList(value=tf_val.numpy()), number=10))
```


### Relevant log output

```shell
0.02872585691511631
25.905678499955684
0.02939283801242709
```
",False,"[-1.32705659e-01 -6.17194831e-01 -2.10824519e-01 -1.47364423e-01
  8.28431398e-02 -6.50062740e-01 -7.98106417e-02  9.84527096e-02
 -5.53932309e-01 -2.22736627e-01 -1.01191342e-01  7.27095157e-02
 -1.67675555e-01  2.21923113e-01 -1.82188988e-01  2.08927214e-01
  5.10677472e-02  1.68721214e-01  2.69331336e-01  3.33644487e-02
 -2.47506618e-01 -3.70224923e-01 -6.31921887e-02  3.58236670e-01
 -1.76173709e-02  2.39124790e-01 -1.27709508e-01 -2.62833964e-02
 -9.15265381e-02  1.71550721e-01  1.67505175e-01  6.44865334e-02
 -7.99570158e-02 -3.81514616e-02 -4.12099361e-01  4.78368580e-01
 -2.50693500e-01 -1.62965864e-01 -1.80236250e-01  4.41620797e-02
  3.95836681e-02 -7.73369968e-02  2.33439982e-01 -5.47400676e-02
 -6.01185970e-02  9.93255526e-02 -4.71580327e-02  6.47528619e-02
 -1.49785101e-01  7.81552196e-02 -1.29542202e-02 -8.00039545e-02
 -3.89265954e-01 -4.22619164e-01 -2.56037712e-02  1.94899186e-01
 -6.13543727e-02 -1.23195186e-01  2.52830237e-02  1.45206928e-01
  4.16702144e-02 -1.45433042e-02  4.67893109e-02  8.83466378e-02
  4.23196375e-01  2.23894387e-01  4.09106195e-01 -1.51645867e-02
  2.48623088e-01 -1.27228126e-01  1.12218887e-01  1.73433628e-02
 -2.91099250e-01  1.75830517e-02 -1.06796343e-02  2.32591495e-01
  1.87376440e-01  1.78172916e-01  4.49651897e-01 -1.28928065e-01
 -1.48477145e-02 -1.03139505e-01  5.37757054e-02 -2.84087360e-01
  2.21881807e-01 -2.81812727e-01  3.82351637e-01  2.27044404e-01
  2.10682422e-01 -4.81144309e-01  6.16737843e-01  5.03627062e-01
 -4.24448222e-01  1.16783172e-01  4.79932547e-01  8.41484070e-02
  1.38760149e-01  2.88520269e-02 -8.39137137e-02 -4.45613228e-02
  3.76332216e-02 -2.17627034e-01 -7.80091435e-03  6.71550035e-02
 -1.38392067e-02 -2.63074487e-01  1.18577600e-01 -1.21757410e-01
  1.44020438e-01  2.46535897e-01  2.64059007e-01  2.58378148e-01
  1.80486441e-01 -9.38907415e-02 -1.38240293e-01  1.93819255e-01
 -2.60320634e-01  2.21182123e-01  3.50433171e-01  7.61804104e-01
 -1.29184544e-01 -2.88031846e-01  2.34402958e-02  8.08346048e-02
  2.98013151e-01 -3.12464070e-02 -1.91226259e-01  3.64382491e-02
 -4.91926074e-02  8.47638845e-02  1.44686364e-02 -8.46024081e-02
 -4.84664798e-01  1.73625663e-01 -5.87528385e-03  2.51722038e-01
 -1.79045871e-01 -1.90943956e-01 -4.32055414e-01 -9.57156718e-02
 -2.53476143e-01  3.99156809e-01  3.99616361e-02 -5.89461803e-01
  3.22041541e-01  3.08906674e-01 -2.58339226e-01  1.08275078e-01
 -2.47625545e-01  5.77013940e-04 -1.15306571e-01 -1.38908951e-02
  3.17025244e-01  2.13142157e-01 -5.45849502e-02  1.44797236e-01
  1.76299363e-01 -7.43532032e-02  1.76401570e-01 -4.51149613e-01
 -2.05485389e-01  5.90112686e-01 -1.36560634e-01 -3.04049522e-01
  1.55582139e-03  1.35433257e-01 -2.24688426e-01 -3.51136357e-01
 -2.45255232e-01  5.77589512e-01 -2.23410577e-02 -1.88349798e-01
 -2.00559765e-01  1.27736181e-01  2.05714792e-01 -2.26682290e-01
  2.11727187e-01 -6.92700088e-01  1.72462448e-01 -1.56043284e-02
 -4.16821837e-02  1.17929175e-01  2.45783597e-01  2.43029773e-01
  7.42969289e-03 -1.80470943e-01  1.77152619e-01  2.64129013e-01
 -1.55677840e-01 -7.78903291e-02 -3.29532504e-01 -3.71425182e-01
  3.32869351e-01 -4.91464511e-02 -1.31969288e-01 -1.82591155e-02
  4.82134521e-01  4.51581143e-02  1.60127208e-01  8.24741870e-02
  4.37615253e-02 -1.35463271e-02 -9.39797759e-02  2.64786668e-02
  1.73250467e-01 -3.17075342e-01 -2.67040640e-01 -4.79573548e-01
 -2.42165685e-01  1.10434510e-01 -2.18385786e-01 -3.35297078e-01
 -6.17850386e-02  9.53806788e-02 -2.76662350e-01 -8.57111663e-02
 -7.96504468e-02  4.24742132e-01 -1.82288095e-01  3.77175868e-01
  2.10968312e-02 -2.43193254e-01  8.01411569e-02 -3.68848979e-01
 -2.77743697e-01  2.40888208e-01 -4.31819558e-01  1.87155023e-01
 -7.47403968e-03  1.76119789e-01  1.76373988e-01 -1.36909410e-01
  2.54832953e-01  2.62331009e-01 -3.22524011e-02 -2.59684145e-01
 -1.10703625e-01 -8.81672800e-02 -2.19190061e-01 -4.66674417e-02
 -2.11005926e-01 -1.16356676e-02  1.61896899e-01 -2.10895658e-01
  3.83168399e-01  3.73630047e-01 -6.34821802e-02 -9.54151154e-02
 -3.36068332e-01  3.79056841e-01 -3.37731272e-01 -2.23404728e-04
  5.85106127e-02 -4.35353816e-02  3.70606184e-01  6.78600967e-02
  1.13058314e-01  1.07391998e-01  3.41225713e-01 -2.72145629e-01
  3.82515132e-01  1.03897184e-01  1.06781557e-01  5.66505790e-01
  2.76299447e-01  2.23292992e-01 -2.57297337e-01  3.61412853e-01
  6.15072176e-02 -2.31861204e-01  6.99721277e-04 -1.39811009e-01
  5.92647195e-01 -3.31116468e-01  1.68567643e-01 -2.29603112e-01
  2.70799696e-01 -1.55209780e-01 -1.51636958e-01  1.20914951e-01
  1.11253843e-01  2.13446245e-01 -2.83528835e-01  7.77769610e-02
 -1.69752717e-01 -3.14082950e-01  2.73699593e-03 -4.00126994e-01
 -1.67330623e-01  9.94673520e-02 -4.02072877e-01  1.15765408e-02
 -1.78293198e-01  1.61393166e-01 -3.38544637e-01  1.31626487e-01
  4.01602238e-02  4.94443029e-02 -1.65336341e-01  6.94821477e-02
 -1.02882937e-01  1.83431506e-02  3.93600702e-01 -3.41212630e-01
 -2.34700114e-01  9.59657133e-02  3.41902554e-01  1.29111409e-01
  1.59731686e-01 -1.99035376e-01  2.96696484e-01 -2.46297449e-01
 -2.35060081e-01  1.03498936e-01  2.35482574e-01  2.41601244e-01
 -9.10889953e-02  3.89343113e-01  8.03664476e-02 -1.43203393e-01
 -1.10495193e-02 -2.83746094e-01 -4.55132037e-01  1.62462234e-01
 -6.94200471e-02  2.11720504e-02  2.05775686e-02 -2.34160364e-01
  2.21799351e-02 -1.99009106e-02 -7.44518936e-02 -1.52018815e-01
 -1.35359436e-01  3.72258276e-02 -1.11464649e-01 -1.94280252e-01
 -2.50231534e-01  6.73649162e-02 -6.65666610e-02 -2.65720010e-01
 -1.56753100e-02 -5.39759472e-02 -2.48821735e-01 -2.37292200e-01
 -3.45427692e-02 -2.49063045e-01  6.22712433e-01  5.52577496e-01
 -1.93390906e-01  9.36647654e-02  4.18337174e-02  3.23617995e-01
 -2.74011999e-01 -4.16451171e-02  1.75981224e-01  3.49601090e-01
  1.68551002e-02 -2.89695382e-01  3.07513624e-01  2.72601366e-01
 -9.40231010e-02  1.86923519e-01 -2.12870777e-01  5.54281808e-02
  4.31758404e-01 -2.31888250e-01  6.52377605e-02 -7.85301328e-02
  3.27606201e-01  3.14373612e-01  4.46775928e-02  3.04063559e-01
 -4.47835997e-02  2.55838722e-01  7.06198990e-01 -3.81941497e-01
 -2.66842097e-01  7.25335628e-02  4.35047209e-01  6.07763007e-02
  1.04694769e-01 -2.67824620e-01  1.58695579e-02 -1.93743810e-01]"
`GLIBCXX_3.4.30' not found stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

r2.13

### Custom code

No

### OS platform and distribution

amazon linux 2023

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/compiler version

gcc 12.2

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

ERROR: /root/tensorflow/tensorflow/core/transforms/remapper/BUILD:14:18: TdGenerate tensorflow/core/transforms/remapper/pdll/MklPDLLPatterns.h.inc [for host] failed: (Exit 1): mlir-pdll failed: error executing command bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll '-x=cpp' tensorflow/core/transforms/remapper/pdll/mkl_patterns.pdll -I ./tensorflow/core/transforms/include -I ... (remaining 15 arguments skipped)
bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll)


### Standalone code to reproduce the issue

```shell
ERROR: /root/tensorflow/tensorflow/core/transforms/remapper/BUILD:14:18: TdGenerate tensorflow/core/transforms/remapper/pdll/MklPDLLPatterns.h.inc [for host] failed: (Exit 1): mlir-pdll failed: error executing command bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll '-x=cpp' tensorflow/core/transforms/remapper/pdll/mkl_patterns.pdll -I ./tensorflow/core/transforms/include -I ... (remaining 15 arguments skipped)
bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll)
```


### Relevant log output

```shell
ERROR: /root/tensorflow/tensorflow/core/transforms/remapper/BUILD:14:18: TdGenerate tensorflow/core/transforms/remapper/pdll/MklPDLLPatterns.h.inc [for host] failed: (Exit 1): mlir-pdll failed: error executing command bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll '-x=cpp' tensorflow/core/transforms/remapper/pdll/mkl_patterns.pdll -I ./tensorflow/core/transforms/include -I ... (remaining 15 arguments skipped)
bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by bazel-out/host/bin/external/llvm-project/mlir/mlir-pdll)
```
",False,"[-5.50678313e-01 -2.93080568e-01 -2.43682101e-01  1.25890702e-01
  3.09486449e-01 -3.63220990e-01 -2.54591256e-01  2.33480871e-01
 -2.38789424e-01 -4.36641306e-01  1.19738281e-01  7.33861029e-02
 -1.81045473e-01  7.43705332e-02 -2.42767543e-01  3.16482008e-01
 -3.08713496e-01 -2.13697910e-01  2.48650476e-01  2.09940672e-01
 -2.74157405e-01  3.46170440e-02 -1.95776314e-01  2.03384638e-01
  2.31808245e-01  1.50887236e-01 -2.79205084e-01  1.13394395e-01
 -7.15866536e-02  1.43094718e-01  5.40713668e-01  2.27222472e-01
  6.53202757e-02  2.94001680e-02  2.11526126e-01  2.96930075e-01
 -1.81418508e-01 -2.45907530e-01 -3.37085396e-01  4.37310115e-02
 -1.79252267e-01 -7.44618401e-02  1.53563142e-01 -1.91206098e-01
  5.65944575e-02 -2.00974673e-01  5.83539717e-02 -1.04862988e-01
  7.14804530e-02 -2.71807432e-01 -2.09490553e-01 -1.97267123e-02
 -2.51371920e-01 -3.22233856e-01 -4.91881296e-02 -6.51534870e-02
  1.11323342e-01 -2.92860419e-02 -6.01857640e-02  2.69248784e-01
  2.70971060e-01  1.39852822e-01  4.20432091e-02 -1.29853617e-02
  1.38859153e-01  2.58660674e-01  4.31847423e-01 -2.16606647e-01
  5.57158887e-01 -9.59883779e-02  7.53153861e-02 -1.14877403e-01
 -3.39567721e-01  3.71843204e-02 -5.20047061e-02  5.94447255e-02
  1.72022209e-01  4.33455035e-02  2.69023120e-01 -1.80028990e-01
 -2.44056746e-01 -3.06132078e-01 -1.20134227e-01 -2.24861413e-01
  1.18451435e-02 -1.31434470e-01  3.36786628e-01  2.25300819e-01
  4.49210852e-01 -2.53113508e-01  3.53553176e-01  6.09601498e-01
 -4.18404862e-02  3.26321572e-02  3.26315850e-01  7.62365982e-02
  6.14571162e-02  2.58873880e-01 -1.08695067e-02 -9.07058716e-02
 -1.29032880e-01 -2.20704854e-01  9.70816538e-02 -3.60158160e-02
 -1.33665144e-01 -1.82702303e-01  3.79626393e-01  9.44093801e-04
  5.23367599e-02 -9.22425389e-02  7.94098675e-02  2.25716114e-01
  6.37343749e-02 -4.21439335e-02 -1.04481138e-01  4.80575524e-02
 -2.22605973e-01 -4.86049708e-03 -2.77487282e-02  8.28280687e-01
  6.58365563e-02 -1.49211854e-01  1.78849027e-02  2.42485955e-01
  3.76961827e-01  6.67608678e-02 -1.47176147e-01 -5.95038645e-02
  1.21575102e-01  2.19899397e-02  1.62125587e-01  8.18315670e-02
 -5.27249947e-02  2.38257483e-01 -9.48461816e-02  3.99635732e-02
 -9.08889100e-02 -5.85916303e-02 -1.50510982e-01 -2.83731222e-01
 -2.02271670e-01  2.23224342e-01 -1.80979341e-01 -6.93764448e-01
 -5.64902313e-02  4.81808074e-02 -2.16425315e-01  1.80457935e-01
 -1.90674365e-01  2.46405631e-01  5.51802814e-02 -4.43231761e-02
 -1.02292649e-01  3.45979810e-01  2.08857954e-01  7.23041221e-02
  3.21572125e-01 -7.16831982e-02 -3.12751979e-02 -4.91058350e-01
 -2.05237605e-02  3.49477679e-01  2.37067565e-02 -1.76472485e-01
 -1.30385626e-02  2.02082992e-01 -5.41964054e-01 -2.73060650e-01
  2.21688956e-01  5.94088137e-01 -9.01775360e-02 -5.50804287e-02
  2.65852153e-01  2.00779080e-01  2.23231673e-01 -1.04075953e-01
  3.15280259e-01 -3.77447486e-01 -7.15981722e-02  4.27955389e-01
  1.31523922e-01  8.70589912e-02  8.99742916e-03  2.41248071e-01
  1.03189185e-01  3.93691473e-02  6.37281165e-02  8.85699391e-02
 -3.20783675e-01  1.67569071e-02 -4.63268310e-01 -2.74882279e-02
  2.75195211e-01 -1.48275793e-01 -2.27934062e-01  1.38500899e-01
  1.72407076e-01  1.40203740e-02  1.36302128e-01  2.74215899e-02
 -1.66273311e-01  7.00836033e-02 -1.31128460e-01  4.37487289e-02
  9.86272767e-02 -3.49214941e-01  2.84799114e-02 -3.59432220e-01
 -4.06829894e-01 -2.09440012e-02  1.17321067e-01 -4.08905327e-01
  1.47265136e-01 -1.09355122e-01 -3.12827915e-01  1.83799967e-01
  9.53393504e-02 -5.12799546e-02 -7.94800669e-02  3.03706169e-01
  4.96217981e-04 -4.19062555e-01  7.09002912e-02 -3.85697842e-01
 -1.06760368e-01 -4.24013548e-02 -2.49222368e-01 -5.14699779e-02
  5.55326194e-02  1.71093911e-01  5.71582653e-02  1.48476716e-02
  4.44648147e-01  3.14352125e-01  3.78974795e-01 -1.91430375e-01
 -8.05033967e-02 -2.01049417e-01 -1.54317375e-02  1.77901238e-01
 -3.26061010e-01 -1.50456607e-01  2.61799470e-02  1.16511747e-01
  3.92511129e-01  3.94349486e-01 -5.44084311e-02 -3.78094986e-02
 -3.05348337e-01  2.45051622e-01 -1.20808810e-01  2.93461859e-01
  2.03024715e-01  7.12367594e-02  4.34631437e-01  1.87229738e-01
 -4.06675525e-02  2.09871054e-01  2.35570222e-01 -2.87573457e-01
  1.63937390e-01  2.46714458e-01  3.34373489e-02  1.63387641e-01
  1.87383205e-01  2.55639374e-01 -4.79510635e-01  5.00656247e-01
 -4.09669131e-02 -4.53921184e-02  1.99485809e-01 -3.58450234e-01
  6.05255127e-01 -5.06419539e-01 -1.22644007e-01 -1.03883341e-01
  2.55438507e-01  2.87491661e-02  3.28047946e-02  1.06877796e-02
 -2.83193626e-02  3.27682674e-01 -3.15241754e-01  4.77361605e-02
  6.68261051e-02 -2.13258371e-01 -7.24479631e-02 -7.45261073e-01
 -3.06275725e-01  5.75730056e-02 -2.28324175e-01  5.73015288e-02
 -2.56148994e-01 -3.06725204e-02 -1.12521701e-01  1.58423036e-02
 -9.42286104e-02 -1.20800927e-01  3.46226953e-02  1.92345098e-01
 -2.19465509e-01 -3.67624089e-02  3.69451642e-01 -2.89272249e-01
 -2.39485931e-02 -1.92778409e-01  1.55835301e-01  2.64850944e-01
  4.15683836e-01 -3.73608321e-01  1.67645693e-01 -8.52568671e-02
 -7.08492696e-02  4.30125356e-01  3.20878811e-03  8.49327743e-02
 -5.38931966e-01  6.88826442e-01  1.72646254e-01 -1.50406465e-01
  2.98785716e-01 -9.72246826e-02 -3.00571859e-01  1.59247115e-01
  2.25987598e-01 -6.95054978e-02 -1.16594791e-01 -3.55455160e-01
  7.10365772e-02  2.59021938e-01  2.81254612e-02 -1.06794313e-01
  3.63760740e-02  3.26079018e-02 -4.23425227e-01 -1.04735941e-02
 -4.50928330e-01  1.79501951e-01 -9.10468996e-02 -2.18195230e-01
 -1.34784102e-01 -1.49673671e-01 -1.72364593e-01 -2.22016171e-01
  4.71240580e-02 -2.71027982e-01  5.30506611e-01  4.20050830e-01
 -4.86518964e-02  9.84690636e-02  5.01917005e-02  2.31741160e-01
 -5.02141654e-01  9.48367827e-03 -5.63269034e-02  2.29904190e-01
 -1.02813169e-03  1.85785666e-02  4.69194889e-01  2.58875608e-01
 -1.60352096e-01  2.30245218e-01 -4.15600777e-01  1.37115806e-01
  7.13914111e-02 -3.38577151e-01 -3.67396683e-01  3.75075266e-04
 -8.46846960e-03  1.89240023e-01  1.27031401e-01  1.55223116e-01
 -3.87790501e-01  4.52792436e-01  5.56742668e-01 -5.29622436e-01
 -4.35576737e-01  1.60060823e-01 -7.07403272e-02 -2.49860734e-01
  4.81358878e-02 -2.24225581e-01  2.49002248e-01 -9.71661881e-03]"
ERROR: C:/users/ayush/_bazel_ayush/xv6zejqw/external/llvm_openmp/BUILD.bazel:233:34: Compiling external/llvm_openmp/z_Windows_NT-586_asm.S failed: not all outputs were created or valid stat:awaiting response type:build/install stale subtype:windows TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.11.4

### Bazel version

5.3.0

### GCC/compiler version

Visual Studio Build Tools 2019 with C++ tools

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It was fetching, extracting and compiling everything was going well and afterwards suddenly it outputs failed to build 

### Standalone code to reproduce the issue

```shell
C:\tensorflow>echo %BAZEL_VS%
C:/Program Files(x86)/Microsoft Visual Studio/2019/BuildTools

C:\tensorflow>set BAZEL_VC=C:/Program Files(x86)/Microsoft Visual Studio/2019/BuildTools/VC

C:\tensorflow>set BAZEL_SH=C:/msys64/usr/bin/bash.exe

C:\tensorflow>set BAZEL_WINSDK_FULL_VERSION=10.0.19041.0

C:\tensorflow>git checkout r2.13
Updating files: 100% (6982/6982), done.
Switched to a new branch 'r2.13'
branch 'r2.13' set up to track 'origin/r2.13'.

C:\tensorflow>set PYTHON_BIN_PATH=C:\Users\Ayush\AppData\Local\Programs\Python\Python311\python.exe

C:\tensorflow>set PYTHON_LIB_PATH=C:\Users\Ayush\AppData\Local\Programs\Python\Python311\Lib\site-packages

C:\tensorflow>set PYTHON_DIRECTORY=C:\Users\Ayush\AppData\Local\Programs\Python\Python311

C:\tensorflow>python configure.py
You have bazel 5.3.0 installed.
Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.


WARNING: Cannot build with CUDA support on Windows.
Starting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v1             # Build with TensorFlow 1 API instead of TF 2 API.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.

C:\tensorflow>bazel build --config=opt --config=mkl --copt=-msse --copt=-msse2 --copt=-msse3 --copt=-msse4.1 --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (614 packages loaded, 36592 targets configured).
INFO: Found 1 target...
ERROR: C:/users/ayush/_bazel_ayush/xv6zejqw/external/llvm_openmp/BUILD.bazel:233:34: output 'external/llvm_openmp/_objs/libiomp5md.dll/z_Windows_NT-586_asm.obj' was not created
ERROR: C:/users/ayush/_bazel_ayush/xv6zejqw/external/llvm_openmp/BUILD.bazel:233:34: Compiling external/llvm_openmp/z_Windows_NT-586_asm.S failed: not all outputs were created or valid
MASM : warning A4018:invalid command-line option : /bigobj
MASM : warning A4018:invalid command-line option : /Zm500
MASM : warning A4018:invalid command-line option : /Z500
MASM : warning A4018:invalid command-line option : /Z00
MASM : warning A4018:invalid command-line option : /Z0
MASM : warning A4018:invalid command-line option : /EHsc
MASM : warning A4018:invalid command-line option : /wd4351
MASM : warning A4018:invalid command-line option : /wd4291
MASM : warning A4018:invalid command-line option : /wd4250
MASM : warning A4018:invalid command-line option : /wd4996
MASM : warning A4018:invalid command-line option : /showIncludes
 Assembling: bazel-out/x64_windows-opt/bin/external/llvm_openmp/z_Windows_NT-586_asm.S
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 6810.134s, Critical Path: 970.02s
INFO: 1408 processes: 307 internal, 1101 local.
FAILED: Build did NOT complete successfully
```
",False,"[-4.86352324e-01  6.32451847e-03 -2.49376088e-01  1.95002168e-01
  2.09941208e-01 -1.93491608e-01 -2.52876937e-01  1.20804161e-01
 -3.73613328e-01 -1.58815652e-01 -1.46343887e-01 -2.40365028e-01
  3.80910933e-03 -1.81502059e-01 -3.12071711e-01  5.34891605e-01
 -1.89532459e-01 -1.40031815e-01  2.31155992e-01  1.15507379e-01
 -3.18375617e-01 -1.59728780e-01 -1.87885940e-01  6.63471743e-02
  1.28328249e-01  1.56072974e-01 -1.40019149e-01  3.52766663e-02
  8.71408284e-02  2.77139783e-01  1.89729244e-01 -2.96974815e-02
  1.51678815e-01 -1.04827173e-02  3.53808552e-01  1.18839011e-01
 -3.12162433e-02 -1.28690153e-01 -3.76213640e-01 -1.61404967e-01
  3.78739089e-02  3.76504287e-02  1.74482793e-01 -2.14995503e-01
  9.68299955e-02 -1.50878951e-01 -1.20376321e-02 -9.87793580e-02
 -2.23309666e-01 -1.39220089e-01 -2.20983461e-01 -1.36769004e-03
 -1.60125405e-01 -5.74793816e-01 -7.35152736e-02 -1.64255410e-01
 -1.70814961e-01  8.47071186e-02 -1.08454883e-01  2.31270224e-01
  1.76758170e-01 -1.09364577e-01 -8.81806910e-02  1.32741407e-04
  2.93573678e-01  1.56753570e-01 -1.15350215e-02  6.16178773e-02
  2.10922271e-01 -1.57227010e-01  3.68974745e-01 -3.45936157e-02
 -2.18827784e-01 -7.03682080e-02  1.92970276e-01  7.29324371e-02
 -1.14337124e-01  1.78285420e-01  1.71385258e-01 -2.15241969e-01
 -1.03537813e-01 -2.45672122e-01 -1.24658853e-01 -9.89294983e-03
  2.42358699e-01 -1.61517281e-02  1.67920381e-01  2.52214503e-02
  4.25988317e-01 -6.03119433e-02  5.44832647e-01  8.25803280e-02
  3.17727178e-02  1.92902118e-01  4.24862385e-01  6.97511435e-02
  2.34022200e-01  3.55154693e-01 -1.78311795e-01 -9.82656702e-03
 -1.12295076e-01 -3.07193607e-01 -1.14121344e-02  8.70922506e-02
  1.51957246e-03 -2.92852223e-01  3.17310393e-01  1.90141022e-01
 -9.60197113e-03 -1.86216354e-01  1.93734631e-01 -6.37728870e-02
  6.47850707e-02 -2.97662355e-02  4.16650623e-02 -9.88232158e-03
 -1.00195684e-01  1.76843360e-01 -3.83817315e-01  4.13190067e-01
 -4.90599722e-02 -4.06461880e-02 -8.66738930e-02 -2.20813781e-01
  2.25004792e-01 -9.67039466e-02 -4.61290836e-01  7.48676360e-02
  7.60576576e-02  2.60216296e-01  1.06531456e-01  3.12595844e-01
 -1.07930973e-01  1.43877491e-01  1.43721268e-01  1.92440569e-01
 -2.94809133e-01 -4.38044965e-03 -1.68751732e-01 -7.18394592e-02
  1.40369479e-02  3.30279380e-01  1.36709541e-01 -4.63870287e-01
 -3.57435569e-02 -1.92434534e-01 -1.54758364e-01  8.95842612e-02
 -1.58766210e-01 -1.02002397e-01 -1.46270454e-01  6.47148490e-02
 -3.75291079e-01  4.31278110e-01  1.12680428e-01 -2.58814543e-06
  3.63485843e-01  1.51716530e-01 -2.97768041e-02 -4.50801671e-01
  7.74155408e-02  3.72704268e-01 -4.11996171e-02 -5.62034324e-02
  9.61510912e-02  1.38872430e-01 -3.97671014e-01 -7.47561008e-02
  2.63277978e-01  2.50384569e-01 -1.00791857e-01 -3.75345498e-02
  2.12735422e-02 -1.49827860e-02  2.87687600e-01 -1.83004528e-01
  3.01658332e-01 -1.25254154e-01 -2.04224408e-01  2.79950321e-01
  2.77440220e-01  1.44229475e-02 -1.48460455e-03  8.25692266e-02
 -5.57913445e-03  2.00449541e-01  4.18788418e-02  2.84074694e-01
 -3.55510451e-02  4.47020233e-02 -2.89309919e-01  1.25458583e-01
  2.77023286e-01 -3.81052196e-02 -6.25133514e-02 -1.38474673e-01
  1.29843518e-01 -2.29961842e-01  2.65361279e-01  1.10037895e-02
  8.36062059e-03  2.47510761e-01 -6.51566908e-02 -1.34145357e-02
 -1.16741545e-02  2.41433401e-02  5.93183264e-02 -2.91025758e-01
 -5.77677600e-02  1.29726917e-01 -1.91001743e-01 -3.89517903e-01
 -7.28652477e-02  4.43336926e-03 -2.17317149e-01  3.31014305e-01
  2.26640046e-01 -7.25556910e-02  5.68230078e-02  1.22765273e-01
 -3.95087153e-03 -3.17739964e-01 -2.99679548e-01 -3.78779352e-01
 -5.67631721e-02 -7.93304443e-02 -1.72639444e-01 -5.73036075e-02
 -5.68366908e-02  1.43669307e-01  1.18503451e-01 -4.23224628e-01
  2.54941344e-01  4.76353206e-02  5.74395359e-01 -2.46579841e-01
  1.14673607e-01 -1.21783383e-01 -2.91750729e-01  1.38464063e-01
 -3.73382270e-01 -2.75153518e-01 -1.09176375e-01 -1.96033448e-01
  9.11910459e-02  1.08187430e-01 -3.61988433e-02 -1.88957751e-01
 -4.22958642e-01  4.63593900e-01 -2.12360844e-02  2.24994153e-01
  1.10773280e-01  2.69376606e-01  3.50761354e-01  4.86862510e-01
 -1.77248120e-01  1.64265126e-01  1.92009881e-01  5.41549269e-03
  2.44854942e-01  2.88937449e-01  4.89269011e-02 -8.01041499e-02
 -4.83455956e-02  2.41000772e-01 -2.98207819e-01  1.81866989e-01
  1.78899661e-01 -1.23393267e-01  3.13442737e-01 -3.49416226e-01
  3.77121121e-01 -2.20784605e-01 -1.05512537e-01 -1.67803526e-01
  3.33263278e-01 -4.79731336e-02 -3.05421948e-01 -3.53925228e-02
 -3.65683846e-02  2.69018590e-01  1.00640528e-01  2.44466029e-02
  3.50654960e-01  8.70070700e-03 -2.28847209e-02 -5.00267684e-01
 -2.51263648e-01  9.89965424e-02  4.94596213e-02  1.77698910e-01
  4.75298613e-02 -1.80920184e-01 -2.14467317e-01 -1.22648850e-01
 -5.78220226e-02 -1.13116346e-01 -1.89560413e-01  1.21417366e-01
 -1.45234436e-01  2.13704869e-01  3.17938685e-01 -5.64248189e-02
 -6.92305565e-02 -1.41003177e-01  2.46031135e-01  2.16264337e-01
  5.05665243e-01 -4.33714420e-01  2.59778142e-01 -9.38211568e-03
 -1.68266505e-01  4.86545980e-01 -1.75610155e-01  1.81743905e-01
 -3.11577201e-01  4.57678437e-01  2.12254107e-01 -7.62285143e-02
  1.10694356e-01 -3.99810612e-01 -2.08237439e-01  7.32185543e-02
  3.09270024e-01 -2.85146255e-02 -4.57405448e-01 -4.36515391e-01
  1.10308208e-01  1.80880904e-01  2.69546378e-02 -2.46171549e-01
 -1.91570356e-01  1.63605005e-01 -1.66497469e-01  7.07242042e-02
 -2.38433003e-01  2.89013118e-01 -2.83025373e-02 -3.02487105e-01
  6.38691336e-02 -2.18806490e-01  3.87822956e-01 -1.78828150e-01
  8.55541322e-04 -2.16805801e-01  3.99541780e-02  1.90615699e-01
  1.21004544e-02  1.91009283e-01  8.96761715e-02  1.75303414e-01
 -1.26831293e-01  5.35106286e-02 -9.95274931e-02 -5.53773418e-02
  1.21378735e-01  1.39733464e-01  1.66627377e-01  4.28874135e-01
 -6.59183785e-02  1.15584239e-01 -3.38146091e-01  9.10161212e-02
  1.14223786e-01 -2.94909060e-01 -5.07114470e-01 -7.83029478e-04
 -1.21703416e-01  6.80864751e-02 -1.12683177e-01  2.76113093e-01
 -2.98256993e-01  2.18335658e-01  4.42509830e-01 -2.54489243e-01
 -2.02058628e-03  2.60445535e-01 -8.04106072e-02  9.41634625e-02
  4.02043909e-02 -8.62570480e-02 -8.41526166e-02  1.30735338e-04]"
"Error in PredictCost() for the op: op: ""CropAndResize"" attr stat:awaiting response type:others comp:ops TF 2.13","### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We have a model that used to run on python 3.7 + tensorflow 2.4.1. When upgrading to tensorflow 2.13.0, we're seeing this error message, which seems to correlate with the degradation of our model performance. Any idea what might be leading to this error and what could be changed? 

It does seem like when I remove `tf.function` decorator, this error message goes away. So, I'm assuming it has something to do with tracing, but not entirely sure why it's erroring out in tf 2.13 and not in 2.4.1...

```
Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_FLOAT shape { dim { size: -569 } dim { size: 128 } dim { size: 224 } dim { size: 2 } } } inputs { dtype: DT_FLOAT shape { dim { size: -16 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -16 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } tensor_content: ""\030\000\000\000\020\000\000\000"" } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""Tesla V100-SXM2-16GB"" frequency: 1530 num_cores: 80 environment { key: ""architecture"" value: ""7.0"" } environment { key: ""cuda"" value: ""11080"" } environment { key: ""cudnn"" value: ""8600"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 6291456 shared_memory_size_per_multiprocessor: 98304 memory_size: 11614814208 bandwidth: 898048000 } outputs { dtype: DT_FLOAT shape { dim { size: -16 } dim { size: 24 } dim { size: 16 } dim { size: 2 } } }
```

### Standalone code to reproduce the issue

```shell
I don't have a standalone code to reproduce the issue, but would just like some feedback on what might be causing the above issue...
```


### Relevant log output

_No response_",False,"[-0.54830897 -0.24830954 -0.12312989  0.24607962  0.5012145  -0.32779244
 -0.02580056 -0.02930072 -0.3795243  -0.24537805  0.16908675  0.05690441
 -0.17975594  0.03116901 -0.30923003  0.25414017 -0.1540277  -0.05730846
  0.1200774   0.11502424  0.0883339  -0.09370448 -0.05195891  0.13866538
  0.02934489  0.28398186 -0.34646875  0.11176351 -0.10429145  0.33549267
  0.17642085  0.24528065 -0.05083891  0.01723716 -0.1391329   0.1797024
 -0.20874947 -0.02809022 -0.32298696  0.00675553 -0.08853222  0.132607
  0.12341996 -0.00331523 -0.07186911 -0.2760732   0.08171917 -0.05577287
  0.0015521  -0.23037888  0.01183304 -0.11668355 -0.57408965 -0.45336992
 -0.06643882 -0.05049528 -0.04546324 -0.03950638  0.06923246  0.07936761
  0.06866734 -0.00128825  0.05979275 -0.02645907  0.22563802  0.30465388
  0.2754104   0.04595627  0.40404254 -0.22118749  0.25322944  0.11155153
 -0.23790397  0.04792213  0.14699346  0.27932477  0.02145842  0.2572868
  0.3363343  -0.2573442   0.05692857 -0.14574215 -0.15361518 -0.14395455
  0.1786229  -0.16364507  0.4121406   0.21090235  0.54221755 -0.14821775
  0.631056    0.27937597 -0.15233327  0.09647922  0.46425346  0.35383296
  0.0158534  -0.05298982  0.03870985 -0.02425359 -0.09441337 -0.40083542
  0.03389283  0.19688143 -0.1316244  -0.15795946  0.00918209 -0.27050143
  0.08322273  0.1014198   0.14905302  0.1036146   0.06731473  0.01910764
 -0.09987406 -0.18779887  0.13244344 -0.05749924  0.12741005  0.43217927
  0.0694806  -0.08427669  0.071243    0.3390152   0.57855487  0.05799285
 -0.05723488  0.18919934  0.10011117 -0.0334553   0.20564206  0.00219134
 -0.12081373  0.16876876 -0.10543802 -0.16599835 -0.051387   -0.11501482
 -0.44678402 -0.06440766 -0.32912716  0.19522184 -0.19032358 -0.521682
  0.25905567  0.19479471 -0.20161635  0.2821618  -0.24418023 -0.20100896
 -0.17751515 -0.22618851 -0.22708273  0.56023     0.0614429   0.042928
  0.29034674 -0.07193309  0.27703074 -0.47747922 -0.09141672  0.31581348
 -0.11661295 -0.17005335  0.08607708  0.11937558 -0.31508094 -0.2628368
 -0.01493146  0.39294273 -0.10021941 -0.1292916   0.02553192  0.08577898
  0.04544334 -0.15614691  0.20016322 -0.54495156 -0.03433183  0.27656332
  0.13286987  0.01780217  0.01053997  0.31052226  0.08159162 -0.02836424
  0.06399762  0.11960639 -0.3868066  -0.1308853  -0.43338475 -0.2372084
  0.31275618  0.24883942 -0.06432821  0.07456575  0.2940235  -0.1481137
  0.01504468 -0.02499451 -0.22037885  0.13507052 -0.20178598 -0.08565227
  0.14320837 -0.10983819 -0.16579491 -0.46833807 -0.45127922  0.14358118
 -0.08681433 -0.5528352   0.19224249 -0.04055974 -0.2801416   0.34808478
  0.06683949  0.299455    0.04841597  0.1707205   0.06147359 -0.21777269
  0.01616058 -0.25071955 -0.3177362   0.18477798 -0.28689244 -0.07745052
 -0.13449128  0.14387399 -0.08027616  0.08036374  0.39841485  0.22741991
  0.29504746 -0.15052822 -0.05797671 -0.2629736  -0.13872522  0.17359641
 -0.50803936  0.080694    0.05747649 -0.17886779  0.18974759  0.40489322
 -0.17169511 -0.05420538 -0.4411403  -0.09772912 -0.32251787  0.07023942
  0.25663906  0.02665615  0.28016394  0.22318894  0.2912723   0.21846628
  0.22627112 -0.23272455  0.3387463   0.07127289  0.07465116  0.56919265
  0.3413081   0.33680224 -0.33526152  0.4834937  -0.07151215 -0.21295154
  0.32590342 -0.23898995  0.83872926 -0.29422528  0.09596313  0.05299688
  0.18592599 -0.19866166  0.00496619  0.24286658  0.13666837  0.11890506
 -0.35187796  0.13065721 -0.17944402 -0.2568944  -0.124919   -0.6890646
 -0.2544983   0.0048434  -0.22442183 -0.19061446  0.1772489   0.00350849
  0.03087397  0.09608869  0.14150378 -0.1517044   0.2426476   0.12002593
 -0.05599397  0.13736911  0.199963   -0.36663687 -0.14781657  0.07104848
  0.40747172  0.16349894  0.41775292 -0.5096311   0.27155027 -0.0917064
 -0.16912591  0.64228785 -0.07184544  0.06645995 -0.38286626  0.59715825
  0.24794415  0.09830623  0.19042878 -0.0904904  -0.20132113  0.15134703
  0.08510686 -0.07709549  0.12205434 -0.44431758 -0.01650364  0.05814614
 -0.15594494  0.11838414 -0.08551441  0.00696035 -0.17265505 -0.11306199
 -0.39232558  0.15653017 -0.02950769 -0.40073004 -0.19930701 -0.2270171
 -0.04037391 -0.16391009  0.10993432 -0.24712124  0.20472784  0.42830056
 -0.22433585  0.29179305  0.07961796 -0.03118244 -0.376769   -0.02773334
 -0.28909206  0.29415667  0.02976597 -0.12271985  0.3601339   0.26105553
 -0.3972327   0.10454991 -0.34780416  0.08479865  0.23977353 -0.20274174
 -0.3208517  -0.3314694  -0.07866122  0.04425637 -0.02975306  0.00355731
 -0.25879422  0.1668297   0.45608816 -0.45991647 -0.21181361  0.20633134
  0.31940523 -0.14755313 -0.02806916 -0.21158963 -0.0596091  -0.02719402]"
" W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: ""Conv2D"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""data_format"" value { s: ""NCHW"" } } attr { key: ""dilations"" value  stat:awaiting response stale type:others comp:grappler TF 2.7","### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf.2.7.0

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

GPU 8 / NVIDIA SMI 512.98

### Current behavior?

I am getting the below error message while loading the model on tf 2.7.0. But there is no issue with the prediction. 
I have converted Keras model into tf2 and loaded them in production. 



### Standalone code to reproduce the issue

```shell
: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: ""Conv2D"" attr { key: ""T"" value { type: DT_FLOAT } } attr { key: ""data_format"" value { s: ""NCHW"" } } attr { key: ""dilations"" value { list { i: 1 i: 1 i: 1 i: 1 } } } attr { key: ""explicit_paddings"" value { list { } } } attr { key: ""padding"" value { s: ""SAME"" } } attr { key: ""strides"" value { list { i: 1 i: 1 i: 1 i: 1 } } } attr { key: ""use_cudnn_on_gpu"" value { b: true } } inputs { dtype: DT_FLOAT shape { dim { } dim { size: 62 } dim { size: 4 } dim { size: 4 } } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 1 } dim { size: 62 } dim { size: 31 } } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""NVIDIA GeForce GTX 1070"" frequency: 1695 num_cores: 16 environment { key: ""architecture"" value: ""6.1"" } environment { key: ""cuda"" value: ""11020"" } environment { key: ""cudnn"" value: ""8100"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 2097152 shared_memory_size_per_multiprocessor: 98304 memory_size: 6952124416 bandwidth: 256256000 } outputs { dtype: DT_FLOAT shape { dim { } dim { size: 31 } dim { size: 4 } dim { size: 4 } } }
```


### Relevant log output

_No response_",False,"[-4.62527901e-01 -3.23362827e-01  5.32917958e-03  2.27378517e-01
  8.06519538e-02 -9.85301882e-02  5.77038899e-02  8.31717849e-02
 -2.15678245e-01 -2.76601911e-01  2.34091625e-01 -1.35989159e-01
 -1.10778436e-01  2.20256578e-02 -2.05775753e-01  1.72815055e-01
  1.00371644e-01 -3.67894396e-02  2.22021997e-01  9.19166952e-02
  4.24463525e-02 -1.65196955e-01 -2.34004676e-01  2.26397365e-02
  1.66236162e-01  1.72274888e-01 -2.34145194e-01  2.86938585e-02
 -2.84919478e-02  2.45035678e-01  2.92872369e-01  2.89039686e-04
  4.30287272e-02  8.29095393e-02 -1.53374150e-01 -1.07417434e-01
 -2.79945076e-01  9.11301821e-02  1.26541648e-02  1.24971122e-01
 -9.33948383e-02  2.41183132e-01 -6.77368641e-02  1.25101373e-01
  7.93580115e-02 -2.10794494e-01  1.01864517e-01 -4.63191420e-02
 -1.37447909e-01  1.43065467e-01 -5.92509843e-02 -5.83138838e-02
 -4.79310751e-01 -3.89669180e-01 -2.04584703e-01 -7.73010701e-02
  4.85624075e-02 -1.23174220e-01  2.44858176e-01  1.56135082e-01
  1.75540090e-01 -7.93190300e-02 -2.44555883e-02  4.20446470e-02
  1.22593679e-01  2.16394186e-01  2.31690720e-01  8.95264149e-02
  3.02709311e-01 -1.16154462e-01  1.30172491e-01  1.25867248e-01
 -3.23767394e-01 -8.88928995e-02  1.54858246e-01  1.77178174e-01
 -7.89259821e-02  1.88352078e-01  1.80987984e-01 -1.43513262e-01
  1.24381006e-01 -2.47009516e-01 -1.15182407e-01 -1.54566616e-01
  1.76251814e-01 -2.29428023e-01  4.00370300e-01  2.31836066e-01
  3.24914038e-01 -1.10739671e-01  6.88967705e-01  3.94339859e-01
 -3.28182817e-01  1.88867718e-01  3.23425412e-01  3.36075813e-01
 -7.23298192e-02 -1.34158134e-02  1.85800880e-01 -2.26107500e-02
 -1.27225831e-01 -2.02864885e-01 -1.88703895e-01  1.60058990e-01
 -2.45009568e-02  7.27763399e-04  9.75836962e-02  5.29485047e-02
  1.18632026e-01  3.29301238e-01  9.69007015e-02 -6.61769286e-02
  1.57603100e-02 -2.68657468e-02 -1.99777067e-01 -9.34800729e-02
  1.08236879e-01  1.98102772e-01  1.69326618e-01  2.89490163e-01
 -5.22177033e-02 -1.42794162e-01  1.35975063e-01  2.39989117e-01
  3.62519264e-01 -1.58469573e-01 -4.36514094e-02  1.53030202e-01
 -1.51571319e-01 -1.27750248e-01  1.91576242e-01 -1.89153016e-01
 -2.02963948e-01  1.38890922e-01 -1.93197146e-01 -2.09922388e-01
 -8.36056918e-02 -8.12321678e-02 -3.97862047e-01  1.21808924e-01
 -4.54272747e-01  1.78480640e-01 -2.84228742e-01 -3.08536351e-01
  2.84529775e-01  3.49685937e-01 -1.07163265e-01  5.47577627e-03
 -7.62219951e-02 -3.25712502e-01 -1.99627817e-01 -1.98583990e-01
 -3.95256281e-01  4.42820609e-01  7.50663057e-02  9.95879397e-02
  4.06621635e-01 -7.11930022e-02  2.49776766e-02 -4.21096772e-01
  8.74465257e-02  7.77269006e-02 -2.77661625e-02  8.50945860e-02
  3.35091412e-01  8.08333158e-02 -1.68514594e-01  8.47627819e-02
 -1.14439338e-01  3.67803335e-01 -4.00507562e-02  2.99761482e-02
 -1.83884054e-01  2.94745322e-02  3.86520445e-01 -1.09547488e-01
 -1.24254815e-01 -4.00796294e-01 -3.47795367e-01  4.24758866e-02
  3.04714665e-02  2.16411144e-01  2.15191454e-01  1.87408298e-01
 -9.93754119e-02 -2.13575304e-01  1.35219932e-01  1.68227598e-01
 -4.83003259e-01 -1.02284402e-01 -1.95648074e-01 -2.16338739e-01
 -1.24630660e-01  5.05812824e-01 -7.61390850e-02 -2.03061670e-01
  8.73367339e-02 -3.09833549e-02 -9.50639546e-02  3.24415356e-01
 -1.57718226e-01 -2.46259421e-01 -1.48975223e-01 -1.44496292e-01
  1.45458236e-01  9.26666185e-02  6.22254536e-02 -2.95462698e-01
 -4.84614491e-01  1.26895413e-01 -1.45158932e-01 -4.18678075e-01
  9.71868560e-02  1.89728662e-02 -2.50426173e-01  2.53992647e-01
 -7.41666481e-02  5.88502586e-02 -1.34833045e-02  1.49352580e-01
  4.30215150e-04 -2.18436986e-01  1.82286292e-01 -2.70104885e-01
 -4.81602848e-01  1.27427369e-01 -3.89614016e-01 -5.49763143e-02
 -6.35578111e-02  1.35404617e-01 -7.74639472e-02  1.97046492e-02
  3.10801566e-01  3.63088325e-02 -1.33296609e-01 -1.50894761e-01
 -2.17442319e-01 -2.00936496e-01 -1.87455282e-01 -8.37950110e-02
 -2.66641676e-01  2.48232365e-01 -1.80980340e-02 -4.09423411e-01
  7.33548030e-02  3.04720402e-02 -9.63192210e-02  3.56175415e-02
 -8.97412747e-02  1.76177725e-01 -3.49260449e-01 -1.51424929e-01
  2.20982879e-01  2.56910503e-01  6.44505620e-02  2.29412109e-01
  4.53863628e-02  1.92175359e-01 -2.75388602e-02 -3.31275761e-02
  2.87454963e-01  4.06270266e-01 -1.14962235e-01  5.87568700e-01
  2.70310760e-01  3.00125599e-01 -1.76610708e-01  2.44994909e-01
 -7.11082518e-02 -1.77375749e-01  2.79727042e-01 -2.96130627e-01
  8.13875854e-01 -2.92085886e-01  6.11555353e-02 -6.90727830e-02
  6.71798065e-02 -1.28893971e-01 -3.50883186e-01  1.62519559e-01
 -3.41340378e-02  9.41479802e-02 -2.13558972e-01  1.48761168e-01
 -9.26400796e-02 -2.40845531e-01 -9.92189795e-02 -2.75505781e-01
 -2.12863177e-01  2.22546887e-02  9.45179462e-02 -1.31497949e-01
 -4.59748916e-02  1.77664720e-02  5.83624989e-02  4.21703234e-03
  2.48564124e-01 -1.19128391e-01  3.69099379e-01 -7.56365061e-02
  1.56260341e-01  4.66796458e-01  3.32746923e-01 -7.62384534e-02
 -2.28779286e-01  3.07206124e-01  2.44670242e-01  9.39251631e-02
  3.37357938e-01 -2.08661407e-01  2.59829611e-01  2.57192664e-02
 -1.19557224e-01  4.74989176e-01 -2.62002021e-01  1.82298332e-01
 -2.41682112e-01  2.25576282e-01  5.95206171e-02  7.47809336e-02
  9.00095403e-02 -6.21282980e-02 -2.69135118e-01  6.10233173e-02
  8.59415233e-02  3.06015611e-01 -1.70209110e-01 -3.58022541e-01
  4.78494503e-02  2.11469740e-01 -7.22193625e-03  3.61156873e-02
 -1.75321817e-01  7.70324990e-02 -6.21438362e-02 -1.90154105e-01
 -1.56269670e-01  9.34076458e-02  3.11829522e-03 -2.28371635e-01
 -1.12303413e-01 -1.26735628e-01  1.69339687e-01 -3.03345501e-01
 -1.24865308e-01 -1.33353770e-01  2.35566795e-01  5.92818499e-01
 -3.98986816e-01  2.16593251e-01  2.53433846e-02 -2.99586095e-02
 -8.20854157e-02  2.33648457e-02  6.24951944e-02  7.75866881e-02
  2.87068821e-02  1.23804174e-01 -1.31214887e-01  6.05671644e-01
 -4.23398495e-01  1.55834347e-01 -1.99433267e-01  5.80249308e-03
  3.38140428e-01 -1.51216120e-01 -1.94732442e-01 -3.11287224e-01
  6.23785146e-02  1.47610515e-01 -3.47995609e-02 -8.35375786e-02
 -4.10458088e-01  6.25592172e-02  3.31461430e-01 -2.48430386e-01
  1.12189218e-01  2.14481235e-01  1.79791152e-01  8.25730115e-02
  3.24116834e-02 -6.31579041e-01 -2.33818173e-01  2.38754764e-01]"
SIGBUS from libc on Android during inference comp:lite TFLiteConverter Android TF 2.13,"### 1. System information

Server that generated the TFLite file:

- OS Platform and Distribution: `Linux debian 6.1.0-9-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.27-1 (2023-05-08) x86_64 GNU/Linux`.
- TensorFlow installation: Pip.
- TensorFlow library: tensorflow 2.13.0.

Android client that interprets this TFLite file and crashed:

- OS Platform and Distribution: Huawei `Hebe-BD00` running version 12.0.1 (presumably HarmonyOS).
- TensorFlow library: [Gradle dependencies on latest TFLite, GPU, support, and select TF Ops](https://github.com/FedCampus/FedKit/blob/f1ba4d438d19b5d984fe8d0fc6defd3710cc5892/android/fed_kit_train/build.gradle.kts).

### 2. Code

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

This code is [under `gen_tflite` in FedKit](https://github.com/FedCampus/FedKit/tree/f1ba4d438d19b5d984fe8d0fc6defd3710cc5892/gen_tflite).
It is used to create [the TFLite files](https://github.com/FedCampus/FedKit/files/12391810/fed_mcrnn1.tflite.zip).

<details>
<summary>gen_tflite/__init__.py</summary>

```python
import tensorflow as tf

SAVED_MODEL_DIR = ""saved_model""


def red(string: str) -> str:
    return f""\033[91m{string}\033[0m""


class BaseTFLiteModel(tf.Module):
    """"""Base TFLite model class to inherit from.
    # Usage
    Inherent from this class and then annotate with `@tflite_model_class`.
    Override these attributes:
    - `X_SHAPE`: Shape of the input to the model.
    - `Y_SHAPE`: Shape of the output from the model.
    - `model`: A `tf.keras.Model` initialized in `__init__`.
    # Functionality
    Provides default implementation of `train`, `infer`, `parameters`, `restore`.
    These methods are not annotated with `@tf.function`;
    they are supposed to be converted by `@tflite_model_class`.""""""

    X_SHAPE: list[int]
    Y_SHAPE: list[int]
    model: tf.keras.Model

    def train(self, x, y):
        return self.model.train_step((x, y))

    def infer(self, x):
        return {""logits"": self.model(x)}

    def parameters(self):
        return {
            f""a{index}"": weight.read_value()
            for index, weight in enumerate(self.model.weights)
        }

    def restore(self, **parameters):
        for index, weight in enumerate(self.model.weights):
            parameter = parameters[f""a{index}""]
            weight.assign(parameter)
        assert self.parameters is not None
        return self.parameters()


def tflite_model_class(cls):
    """"""Convert `cls` that inherits from `BaseTFLiteModel` to a TFLite model class.
    Convert `cls`'s methods using `@tf.function` with proper `input_signature`
    according to `X_SHAPE` and `Y_SHAPE`.
    The converted methods are `train`, `infer`, `parameters`, `restore`.
    Only `restore`'s `input_signature` is not specified because it need to be
    determined after examples of parameters are given.""""""
    cls.x_spec = tf.TensorSpec([None] + cls.X_SHAPE, tf.float32)  # type: ignore
    cls.y_spec = tf.TensorSpec([None] + cls.Y_SHAPE, tf.float32)  # type: ignore
    cls.train = tf.function(
        cls.train,
        input_signature=[cls.x_spec, cls.y_spec],
    )
    cls.infer = tf.function(
        cls.infer,
        input_signature=[cls.x_spec],
    )
    cls.parameters = tf.function(cls.parameters, input_signature=[])
    cls.restore = tf.function(cls.restore)
    return cls


def save_model(model, saved_model_dir):
    parameters = model.parameters.get_concrete_function()
    init_params = parameters()
    print(f""Initial parameters is {init_params}."")
    restore = model.restore.get_concrete_function(**init_params)
    restore_test = restore(**init_params)
    print(f""Restore test result: {restore_test}."")
    tf.saved_model.save(
        model,
        saved_model_dir,
        signatures={
            ""train"": model.train.get_concrete_function(),
            ""infer"": model.infer.get_concrete_function(),
            ""parameters"": parameters,
            ""restore"": restore,
        },
    )

    converted_params = [
        param.numpy() for param in parameters_from_raw_dict(init_params)
    ]
    shape = f""{[list(param.shape) for param in converted_params]}""
    print(f""Model parameter shape: {red(shape)}."")
    byte_sizes = f""{[param.size * param.itemsize for param in converted_params]}""
    print(f""Model parameter sizes in bytes: {red(byte_sizes)}."")
    return converted_params


def convert_saved_model(saved_model_dir):
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
        tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.
    ]

    converter.experimental_enable_resource_variables = True
    tflite_model = converter.convert()

    return tflite_model


def parameters_from_raw_dict(raw_dict):
    parameters = []
    index = 0
    while True:
        parameter = raw_dict.get(f""a{index}"")
        if parameter is None:
            break
        parameters.append(parameter)
        index += 1
    return parameters


def save_tflite_model(tflite_model, tflite_file):
    with open(tflite_file, ""wb"") as model_file:
        return model_file.write(tflite_model)
```

</details>

<details>
<summary>gen_tflite/fed_mcrnn_eg/run.py</summary>

```python
from os import path

from .. import *
from . import FedMCRNNModel

DIR = path.dirname(__file__)


TFLITE_FILE = f""fed_mcrnn1.tflite""


def main():
    model = FedMCRNNModel()
    save_model(model, SAVED_MODEL_DIR)
    tflite_model = convert_saved_model(SAVED_MODEL_DIR)
    save_tflite_model(tflite_model, TFLITE_FILE)


main() if __name__ == ""__main__"" else None
```

</details>

<details>
<summary>gen_tflite/fed_mcrnn_eg/__init__.py</summary>

```python
from tensorflow import keras

from .. import *


@tflite_model_class
class FedMCRNNModel(BaseTFLiteModel):
    X_SHAPE = [7, 8]
    Y_SHAPE = [1]

    def __init__(self):
        self.model = self.build_model()

    def build_model(self):
        """"""Written and tuned by Aicha Slaitane in Aug 2023.""""""
        model = keras.Sequential()
        # For the first LSTM layer, specify the input_shape
        model.add(
            keras.layers.LSTM(
                # Tune number of units separately.
                units=384,
                input_shape=self.X_SHAPE,
                return_sequences=True,
            )
        )
        model.add(keras.layers.LeakyReLU(0.523629795960645))
        model.add(keras.layers.Dropout(0.372150795833))

        # For subsequent LSTM layers, no need to specify input_shape
        model.add(
            keras.layers.LSTM(
                units=64,
                return_sequences=True,
            )
        )
        model.add(keras.layers.LeakyReLU(0.523629795960645))
        model.add(keras.layers.Dropout(0.372150795833))

        model.add(
            keras.layers.LSTM(
                units=480,
                return_sequences=True,
            )
        )
        model.add(keras.layers.LeakyReLU(0.523629795960645))
        model.add(keras.layers.Dropout(0.372150795833))
        model.add(keras.layers.Flatten())
        model.add(keras.layers.Dense(1))

        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.00668472266354),
            loss=""mean_squared_error"",
            metrics=[""mean_absolute_error""],
        )
        return model
```

</details>

Command to build the TFLite file: `python3 -m gen_tflite.fed_mcrnn_eg.run`.

---

Code used on the Android side is [in `FlowerClient.kt` in FedKit](https://github.com/FedCampus/FedKit/blob/f1ba4d438d19b5d984fe8d0fc6defd3710cc5892/android/fed_kit_train/src/main/java/org/eu/fedcampus/fed_kit_train/FlowerClient.kt).

<details>
<summary>Relevant code</summary>

```kotlin
/**
 * Flower client that handles TensorFlow Lite model [Interpreter] and sample data.
 * @param tfliteFileBuffer TensorFlow Lite model file.
 * @param spec Specification for the samples, see [SampleSpec].
 */
class FlowerClient<X : Any, Y : Any>(
    tfliteFileBuffer: MappedByteBuffer,
    val layersSizes: IntArray,
    val spec: SampleSpec<X, Y>,
) : AutoCloseable {
    val interpreter = Interpreter(tfliteFileBuffer)
    val interpreterLock = ReentrantLock()
    val trainingSamples = mutableListOf<Sample<X, Y>>()
    val testSamples = mutableListOf<Sample<X, Y>>()
    val trainSampleLock = ReentrantReadWriteLock()
    val testSampleLock = ReentrantReadWriteLock()

    /**
     * Run inference on [x] using [interpreter] and return the result.
     */
    fun inference(x: Array<X>): Array<Y> {
        val inputs = mapOf(""x"" to x)
        val logits = spec.emptyY(x.size)
        val outputs = mapOf(""logits"" to logits)
        runSignatureLocked(inputs, outputs, ""infer"")
        return logits
    }

    private fun runSignatureLocked(
        inputs: Map<String, Any>,
        outputs: Map<String, Any>,
        signatureKey: String
    ) {
        interpreterLock.withLock {
            interpreter.runSignature(inputs, outputs, signatureKey)
        }
    }
}
```

</details>


### 3. Failure after conversion

See also [issue `Training Fatel Signal` on FedKit](https://github.com/FedCampus/FedKit/issues/15):

<details>
<summary>Crash log</summary>

```ruby
F/libc    (25011): Fatal signal 7 (SIGBUS), code 2 (BUS_ADRERR), fault addr 0x77db8b7690 in tid 10606 (DefaultDispatch), pid 25011 (.cuhk.fedcampus)
*** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
Build fingerprint: 'Hinova/TINA-AN00/TS-TINA-Q:11/HinovaHebe-BD00/102.0.1.166C11:user/release-keys'
Revision: '0'
ABI: 'arm64'
Timestamp: 2023-08-21 09:42:39+0800
pid: 25011, tid: 10606, name: DefaultDispatch  >>> com.cuhk.fedcampus <<<
uid: 10263
signal 7 (SIGBUS), code 2 (BUS_ADRERR), fault addr 0x77db8b7690
    x0  b4000077a668b344  x1  00000077db8b7690  x2  0000000000000004  x3  0000000000000001
    x4  00000077db8b7694  x5  b4000077a668b348  x6  0000000000000008  x7  0000000000000008
    x8  00000077db8b7690  x9  0000000000000000  x10 b4000077da4d1bc0  x11 00000077db8b7690
    x12 3ddde2cdbc34570b  x13 3deefded3df00685  x14 0000000000000003  x15 00000000ebad6a89
    x16 00000077ce7e1ed0  x17 00000078ef4d8c40  x18 00000077d1238000  x19 0000000000000004
    x20 b4000077a668b340  x21 b4000077f8cc3b10  x22 0000000000000000  x23 0000000000000001
    x24 0000000000000001  x25 0000000000000001  x26 0000000000000001  x27 0000000000000001
    x28 0000000000000002  x29 00000078589791f0
    lr  00000077ce5999ec  sp  0000007858979110  pc  00000078ef4d8b44  pst 0000000080001000
backtrace:
#00 pc 0000000000086b44  /apex/com.android.runtime/lib64/bionic/libc.so (__memcpy+116) (BuildId: ed6fa1d1056492860af901caffabe1a6)
      #01 pc 000000000014e9e8  /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!libtensorflowlite_jni.so (offset 0x8263000)
      #02 pc 00000000002d5d68  /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!libtensorflowlite_jni.so (offset 0x8263000)
      #03 pc 00000000002d575c  /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!libtensorflowlite_jni.so (offset 0x8263000)
      #04 pc 00000000002c25a8  /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!libtensorflowlite_jni.so (offset 0x8263000)
      #05 pc 0000000000025668  /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!libtensorflowlite_jni.so (offset 0x8263000) (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+100)
      #06 pc 000000000014fed4  /apex/com.android.art/lib64/libart.so (art_quick_generic_jni_trampoline+148) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #07 pc 00000000001467e8  /apex/com.android.art/lib64/libart.so (art_quick_invoke_static_stub+568) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #08 pc 00000000001bc26c  /apex/com.android.art/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+236) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #09 pc 00000000003361cc  /apex/com.android.art/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+376) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #10 pc 000000000032c440  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+996) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #11 pc 00000000006ce704  /apex/com.android.art/lib64/libart.so (MterpInvokeStatic+548) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #12 pc 0000000000140994  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_static+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #13 pc 00000000003f9aea  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (org.tensorflow.lite.NativeInterpreterWrapper.run+122)
      #14 pc 00000000003236fc  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.llvm.15210458325005997145)+348) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #15 pc 00000000006abad0  /apex/com.android.art/lib64/libart.so (artQuickToInterpreterBridge+780) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #16 pc 000000000014fff8  /apex/com.android.art/lib64/libart.so (art_quick_to_interpreter_bridge+88) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #17 pc 000000000206b0f4  /memfd:jit-cache (deleted) (offset 0x2000000) (org.tensorflow.lite.NativeInterpreterWrapper.runSignature+1044)
      #18 pc 0000000000146564  /apex/com.android.art/lib64/libart.so (art_quick_invoke_stub+548) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #19 pc 00000000001bc250  /apex/com.android.art/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+208) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #20 pc 00000000003361cc  /apex/com.android.art/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+376) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #21 pc 000000000032c440  /apex/com.android.art/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+996) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #22 pc 00000000006cb78c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+848) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #23 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #24 pc 00000000003f8e14  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (org.tensorflow.lite.Interpreter.runSignature+36)
      #25 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #26 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #27 pc 00000000003f6440  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (org.eu.fedcampus.fed_kit_train.FlowerClient.runSignatureLocked+20)
      #28 pc 00000000006ce0c8  /apex/com.android.art/lib64/libart.so (MterpInvokeDirect+1248) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #29 pc 0000000000140914  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_direct+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #30 pc 00000000003f5aa0  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (org.eu.fedcampus.fed_kit_train.FlowerClient.inference+84)
      #31 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #32 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #33 pc 00000000003f616e  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (org.eu.fedcampus.fed_kit_train.FlowerClient.evaluate+146)
      #34 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #35 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #36 pc 0000000000001284  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!classes7.dex] (com.cuhk.fedcampus.train.FedmcrnnClient$evaluate$1.invokeSuspend+52)
      #37 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #38 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #39 pc 0000000000001234  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!classes7.dex] (com.cuhk.fedcampus.train.FedmcrnnClient$evaluate$1.invoke+16)
      #40 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #41 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #42 pc0000000000001208  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!classes7.dex] (com.cuhk.fedcampus.train.FedmcrnnClient$evaluate$1.invoke+4)
      #43 pc 00000000006cd464  /apex/com.android.art/lib64/libart.so (MterpInvokeInterface+1808) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #44 pc 0000000000140a14  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_interface+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #45 pc 0000000000001b00  [anon:dalvik-classes7.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk!classes7.dex] (com.cuhk.fedcampus.train.FedmcrnnClient$tryLaunch$1.invokeSuspend+72)
      #46 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #47 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #48 pc 0000000000363596  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlin.coroutines.jvm.internal.BaseContinuationImpl.resumeWith+42)
      #49 pc 00000000006cd464  /apex/com.android.art/lib64/libart.so (MterpInvokeInterface+1808) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #50 pc 0000000000140a14  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_interface+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #51 pc 00000000003a9a98  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlinx.coroutines.DispatchedTask.run+448)
      #52 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #53 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #54 pc 00000000003ea81e  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlinx.coroutines.scheduling.CoroutineScheduler.runSafely+2)
      #55 pc 00000000006cba2c  /apex/com.android.art/lib64/libart.so (MterpInvokeVirtual+1520) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #56 pc 0000000000140814  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_virtual+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #57 pc 00000000003e93fa  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.executeTask+34)
      #58 pc 00000000006ce0c8  /apex/com.android.art/lib64/libart.so (MterpInvokeDirect+1248) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #59 pc 0000000000140914  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_direct+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #60 pc 00000000003e9528  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.runWorker+56)
      #61 pc 00000000006ce0c8  /apex/com.android.art/lib64/libart.so (MterpInvokeDirect+1248) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #62 pc 0000000000140914  /apex/com.android.art/lib64/libart.so (mterp_op_invoke_direct+20) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #63 pc 00000000003e94d8  [anon:dalvik-classes.dex extracted in memory from /data/app/~~jOMJSmliax2W9nG0uNnx0g==/com.cuhk.fedcampus-KOBIK4l5h_A1eePLjPC9Jg==/base.apk] (kotlinx.coroutines.scheduling.CoroutineScheduler$Worker.run)
      #64 pc 00000000003236fc  /apex/com.android.art/lib64/libart.so (art::interpreter::Execute(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame&, art::JValue, bool, bool) (.llvm.15210458325005997145)+348) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #65 pc 00000000006abad0  /apex/com.android.art/lib64/libart.so (artQuickToInterpreterBridge+780) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #66 pc 000000000014fff8  /apex/com.android.art/lib64/libart.so (art_quick_to_interpreter_bridge+88) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #67 pc 0000000000146564  /apex/com.android.art/lib64/libart.so (art_quick_invoke_stub+548) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #68 pc 00000000001bc250  /apex/com.android.art/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+208) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #69 pc 0000000000591268  /apex/com.android.art/lib64/libart.so (art::JValue art::InvokeVirtualOrInterfaceWithJValues<art::ArtMethod*>(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, art::ArtMethod*, jvalue const*)+460) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #70 pc 00000000005e3bd0  /apex/com.android.art/lib64/libart.so (art::Thread::CreateCallback(void*)+1364) (BuildId: c83b725cb502ebcf2b3b28039ccfbfa6)
      #71 pc 00000000000ed068  /apex/com.android.runtime/lib64/bionic/libc.so (__pthread_start(void*)+64) (BuildId: ed6fa1d1056492860af901caffabe1a6)
      #72 pc 000000000008d5e0  /apex/com.android.runtime/lib64/bionic/libc.so (__start_thread+64) (BuildId: ed6fa1d1056492860af901caffabe1a6)
Lost connection to device.
Exited (sigterm)
```

</details>
",False,"[-5.16679585e-01 -2.64005661e-01 -9.36412066e-02 -4.32747640e-02
  2.22810209e-01 -3.66358340e-01 -1.27380416e-01  2.03441530e-01
 -2.32083738e-01 -1.17675038e-02  3.25221010e-02 -4.21055630e-02
  2.37987772e-01  1.15085833e-01 -3.37264836e-01  4.30046767e-02
  6.10850453e-02 -3.76905054e-01  2.68315822e-01  1.15513414e-01
  2.06171662e-01  1.47748381e-01 -3.22418939e-03  6.24587759e-02
  4.38205123e-01  9.89700034e-02  8.02331567e-02 -1.67599350e-01
  2.08134130e-02 -1.83913186e-02  1.97370619e-01  2.13460296e-01
  9.69977304e-03  7.04263523e-02 -1.33592039e-01  8.84529948e-03
 -1.65369987e-01 -1.22727513e-01 -1.19462997e-01  2.07570605e-02
 -1.02624953e-01  1.22475162e-01  2.33117372e-01  1.79491043e-01
 -2.91529000e-01  1.68193877e-01  1.49597237e-02  7.86064118e-02
 -2.76507050e-01 -4.01820689e-02 -7.20760077e-02 -1.62802860e-02
 -2.08882362e-01 -2.92033851e-01  2.55590156e-02  1.80109426e-01
 -3.66864279e-02  2.89924502e-01  1.31142944e-01  2.39993095e-01
  2.88758755e-01  1.45188142e-02 -1.88840851e-02  2.18774945e-01
  9.47914869e-02  3.13413113e-01  4.55590524e-02 -3.04045260e-01
  4.17546779e-01 -3.07315648e-01 -9.26377773e-02  1.63934901e-01
  6.68979660e-02 -1.39949443e-02 -1.61117110e-02  1.91862166e-01
  1.95136473e-01  3.57799172e-01 -3.41874138e-02 -1.54068336e-01
 -6.91551939e-02  9.24706608e-02 -6.50924742e-02  1.90982103e-01
  1.42626166e-01 -1.26295879e-01 -1.24169355e-02  5.32442182e-02
  2.63465703e-01  6.68168217e-02  3.51975113e-01  3.67097110e-01
 -1.01860188e-01 -1.71107389e-02 -1.18997753e-01  2.38915175e-01
 -8.48626122e-02  7.78704286e-02  5.72074652e-02 -4.04511765e-03
  9.02552307e-02  1.86395179e-03 -1.32083058e-01  1.17714610e-02
  1.10882714e-01 -4.06919867e-01  4.26703990e-02  9.45306793e-02
  1.89637318e-01  2.46930301e-01  1.68158233e-01 -9.06408727e-02
 -4.03867140e-02  1.66421652e-01 -2.74039898e-03  1.61139995e-01
  1.21860094e-02  7.32588917e-02  1.87012732e-01  8.68228674e-02
 -2.48926908e-01 -2.26677239e-01 -9.96122509e-02 -3.00911397e-01
  1.55085415e-01  1.63854361e-01 -3.17736477e-01 -2.08796650e-01
  1.38618067e-01 -2.18052138e-02 -1.36492774e-02  2.02715233e-01
 -3.39330196e-01 -1.02063797e-01  3.29762623e-02  2.41549551e-01
 -6.67103678e-02 -6.82002380e-02 -3.40434939e-01  4.54268716e-02
 -1.93392798e-01  1.62114546e-01  9.73323360e-02 -2.11544961e-01
 -2.06475317e-01  6.04353212e-02 -3.09964158e-02  1.73999161e-01
 -2.04944052e-02 -1.28265381e-01 -1.04266882e-01 -2.11371109e-01
  1.79336518e-01  3.70849401e-01  2.95198202e-01 -1.26977310e-01
  2.60384113e-01  5.91466203e-03  2.66019367e-02 -1.95167974e-01
 -8.52101147e-02  1.47027656e-01 -1.99027419e-01  7.59925544e-02
 -1.65660903e-02  1.69532001e-01 -3.89141470e-01 -5.66978455e-02
  2.02148110e-01  1.42581075e-01  3.12211439e-02 -1.61534458e-01
  9.94652510e-02  3.05414408e-01  3.63564253e-01 -3.06348026e-01
  2.30450332e-01 -3.06374580e-01  8.68476853e-02 -2.15912014e-01
  1.89824432e-01  4.25057188e-02  3.07133645e-01  1.02370325e-02
 -3.03496689e-01  1.79832324e-01  2.09971383e-01  1.94447428e-01
 -1.90881222e-01 -8.24268162e-02 -3.48875910e-01  2.52622783e-01
  2.23353505e-01 -1.31514996e-01 -2.80533254e-01 -1.20344348e-01
  1.24280132e-01  2.78207045e-02  9.21515226e-02  5.56615517e-02
  2.38693774e-01  3.18410434e-03 -1.15748137e-01  1.82798415e-01
 -1.29748315e-01 -1.83424652e-01 -1.10641815e-01 -1.94016129e-01
 -3.64161432e-01 -1.51570901e-01  1.93535864e-01 -2.62523621e-01
 -3.43253732e-01  1.59744844e-01  1.91156507e-01  6.57431483e-02
 -2.75402904e-01 -7.27776289e-02 -3.65088314e-01  3.30077797e-01
  1.27218723e-01 -1.60101369e-01 -9.05622095e-02 -1.78533196e-01
 -4.83063519e-01 -1.89456284e-01 -1.26174286e-01 -8.97518322e-02
 -4.26207483e-02  1.90565705e-01 -2.21167147e-01  1.06316209e-02
  2.46311009e-01 -1.06963441e-01 -3.73720638e-02 -1.45207375e-01
  1.68208592e-02 -4.44255948e-01 -3.00857395e-01 -6.37790263e-02
 -4.05976847e-02 -2.09410965e-01 -7.56933540e-02 -5.16201407e-02
  6.42499421e-03  1.66722149e-01  2.29626283e-01  1.70829967e-01
 -1.99437171e-01  2.25189030e-01  5.74577302e-02 -5.53962402e-02
 -3.25405002e-02  1.73100874e-01  2.47265249e-02  1.02421269e-01
  4.54193018e-02 -2.75236182e-02  5.59951626e-02 -2.40547899e-02
 -1.34175718e-01  2.78205335e-01  1.14137135e-01  3.17535013e-01
  5.85977323e-02  1.21685125e-01 -9.43128988e-02  1.05746239e-01
 -2.04503909e-01 -2.14824304e-01 -9.93781984e-02 -1.31792322e-01
  3.02525043e-01 -3.24173063e-01 -6.03745133e-02 -3.11235338e-02
  2.32224762e-01 -1.32950068e-01 -5.72625771e-02  8.15554708e-02
  2.85895411e-02  3.74694586e-01 -3.08909677e-02 -2.46254206e-01
  1.03455044e-01 -2.16825396e-01 -2.32928991e-01 -3.06130528e-01
 -3.08614731e-01  1.26808226e-01  1.54138505e-01  1.18691079e-01
 -6.55006617e-02  2.06984848e-01  1.08195553e-02 -1.91786021e-01
  1.63371146e-01  1.35002611e-02  1.39924735e-01  4.29429449e-02
 -4.81550507e-02 -1.58311427e-02  3.11376620e-02 -2.37336099e-01
 -7.96271265e-02  2.90109038e-01  1.29555166e-03 -1.10073961e-01
  4.83751774e-01 -3.83239686e-01  1.95897579e-01  2.03687102e-02
  8.31397623e-03  3.97440195e-01 -3.14377189e-01  1.42637447e-01
 -4.20797855e-01  3.44610900e-01  1.15582898e-01  1.60429075e-01
  1.02220178e-01 -5.10570705e-01 -1.68779582e-01 -5.75782694e-02
  1.04696169e-01  8.35768282e-02  1.16928995e-01  8.38774219e-02
  7.56547302e-02  8.68335888e-02  1.71706453e-02 -1.29510984e-01
 -1.54401422e-01  3.85625660e-01 -1.03393987e-01  1.00706808e-01
 -1.72055691e-01  1.87529415e-01 -2.35139236e-01 -1.29270703e-01
 -1.27616912e-01 -2.95023024e-01 -2.02627584e-01 -2.68870234e-01
  1.89419612e-01  9.82769430e-02  9.25043821e-02 -2.85376012e-02
  6.52074814e-02 -3.75124216e-02 -2.34584600e-01  9.31726322e-02
  4.90581989e-03 -2.14542866e-01 -4.93394211e-04  4.02850032e-01
 -2.67651118e-02 -1.01589598e-02  2.45123506e-01  1.79024652e-01
 -5.91799170e-02  1.24899566e-01 -2.18554884e-01 -4.66753989e-02
 -1.34647995e-01 -2.33345807e-01 -1.41291514e-01  1.30578905e-01
  1.77104119e-02  6.94142699e-01 -1.03202477e-01  5.51858880e-02
 -2.85600811e-01 -8.61026496e-02  2.39268795e-01 -1.81602597e-01
 -5.55606931e-02  1.92681290e-02 -8.69491324e-02  4.54609059e-02
 -1.74918294e-01  6.97736517e-02  9.48987529e-02  4.71351668e-04]"
Tensorflow on Manjaro stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.8,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.8

### Custom code

Yes

### OS platform and distribution

Linux Manjaro

### Mobile device

Android

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

How to setup and perform main configuration on Manjaro platform

### Standalone code to reproduce the issue

```shell
I see nothin expected...
```


### Relevant log output

_No response_",False,"[-0.6082112  -0.45467997 -0.17318104  0.09387302  0.17613935 -0.5844232
 -0.12099259 -0.04036746 -0.24362895 -0.36645156  0.15853836  0.04291929
 -0.20575061  0.19806108 -0.1312384   0.20483369 -0.36703852 -0.21487239
  0.36379957  0.11863166 -0.07658575 -0.02814344 -0.4089682   0.12413198
  0.1507772   0.2096982  -0.26514763 -0.06202506 -0.04223118  0.03219248
  0.5970964   0.0609594   0.09072129 -0.0093384  -0.07909259  0.37064326
 -0.28559336 -0.206381   -0.32326773  0.09676956 -0.11021288  0.0461266
  0.17635174 -0.05653682  0.07036552 -0.0799523   0.04012261 -0.22353826
  0.11328859 -0.2563356  -0.18891908  0.05910163 -0.44308275 -0.25569177
 -0.18806487 -0.2509247   0.25847495  0.02361225 -0.01606004  0.13972297
 -0.02541639  0.10657954  0.07837094  0.03954555  0.1066699  -0.03323238
  0.22886348 -0.19406429  0.48830226 -0.00098572  0.19912699 -0.13288665
 -0.3016383  -0.05125666 -0.15222682  0.13845326 -0.04535414  0.09353328
  0.3567412  -0.11297143 -0.10495213 -0.23446557  0.08373061 -0.36457753
  0.02695574  0.0179009   0.38129264 -0.01519985  0.53903186 -0.17261508
  0.5416275   0.51853526  0.12813747 -0.10836008  0.41036898  0.0938407
  0.18385153  0.50260305 -0.03273657 -0.19075954 -0.09331347 -0.20397288
 -0.02606727  0.00941326 -0.07229105 -0.01391253  0.06991535 -0.05509527
  0.15088716  0.10396439  0.01724633  0.00955266  0.18780768 -0.03396139
 -0.01462793  0.0101964  -0.08450276 -0.19394743 -0.22369872  0.9426223
 -0.17467952 -0.00616996  0.03607503  0.08857439  0.35025966  0.10853822
 -0.00219576 -0.02876906  0.258452   -0.04128792  0.10766481  0.02726331
 -0.09679739  0.25449783 -0.11531354 -0.00689285  0.01828598 -0.10923713
 -0.15793124 -0.09933147 -0.3356849   0.23526914 -0.21678858 -0.5537538
  0.12204558 -0.0079686  -0.23221642  0.0407731  -0.16634092  0.18394569
 -0.07582088  0.02708533 -0.19775023  0.28659928  0.28248096  0.14570877
  0.19804868 -0.0719364  -0.2511145  -0.48073342  0.0758196   0.47878602
 -0.0409227  -0.167193    0.20501141  0.27316165 -0.58693755 -0.22615534
  0.23327813  0.4538602  -0.16277741 -0.0648782   0.00429476  0.16871524
  0.30519542  0.01533153  0.4136398  -0.51002383 -0.13174897  0.60914063
  0.15320005  0.07733282  0.05044882  0.20593187  0.07534542  0.045775
 -0.00394788  0.08530521 -0.26888418  0.02584852 -0.16264433 -0.117331
  0.29829597 -0.30724016 -0.27689156  0.1418421   0.17307247 -0.16792938
  0.04287072 -0.07969359 -0.15943423 -0.09119821 -0.1121821  -0.18879026
  0.09029426 -0.2303779  -0.07269534 -0.3512987  -0.45431107  0.12619835
  0.16973403 -0.3560821   0.11598814  0.0113626  -0.27375618  0.073777
  0.03548864  0.08106363 -0.33117265  0.15571856  0.19102171 -0.32549116
  0.11293674 -0.3859698  -0.2165263   0.07956148 -0.30623192  0.15726233
  0.05542018  0.24571547  0.12343633  0.24038586  0.2190882   0.19220105
  0.42186046 -0.07216284  0.00919204 -0.15588772 -0.14175436  0.04169243
 -0.22101007 -0.1253004  -0.0293525   0.1090948   0.2931751   0.2398002
 -0.08641042 -0.11288558 -0.38839108  0.21934964 -0.0987969   0.20020957
  0.2957706   0.2205919   0.4929465   0.24215308 -0.0512046   0.3268869
  0.2535265  -0.40136105  0.2575738   0.3891012  -0.09390324  0.44411078
  0.43741453  0.30599296 -0.42973632  0.5387511   0.01269021 -0.00264483
 -0.12830043 -0.33437312  0.5463028  -0.4967076  -0.09624799 -0.09704345
  0.2796823   0.06681533 -0.10305151  0.03181     0.08642334  0.35537374
 -0.54010886  0.27470058 -0.0039404  -0.09856272  0.13804808 -0.5648202
 -0.16535158  0.11873706 -0.3051058   0.22595319 -0.15958117 -0.09788869
 -0.17124861 -0.14701112  0.16910997  0.16020292  0.15743119  0.23438865
 -0.02674224 -0.05865282  0.50924283 -0.5738362  -0.29996547 -0.17076448
  0.29172534  0.41520354  0.43594784 -0.34861875  0.26701105 -0.20386927
 -0.1104178   0.5106792   0.14806366  0.18863165 -0.44320852  0.798345
  0.280534   -0.2564276   0.19162756 -0.1306996  -0.16674814  0.07190596
  0.22265173 -0.03742406  0.0365132  -0.6151774  -0.08369654  0.27040377
 -0.04103153 -0.00975977 -0.12482747  0.15828519 -0.20102616  0.06305537
 -0.34012732  0.2964174  -0.10709327 -0.40876496 -0.22809464 -0.13430582
 -0.03110908 -0.19442582 -0.11713995 -0.25236854  0.4969147   0.42280084
 -0.20471519  0.07214127 -0.09725465  0.24767843 -0.61735594  0.04377546
 -0.0094591   0.30791032 -0.13497294 -0.14873439  0.34648493  0.29099482
 -0.28737617  0.17415318 -0.28921968  0.04270327  0.13381161 -0.25979516
 -0.20955205 -0.14081946  0.06746367  0.5208976  -0.02931139  0.30654532
 -0.32374147  0.24885607  0.7174862  -0.6647019  -0.298447    0.19982412
 -0.06669125 -0.2812283   0.03243074 -0.27787274  0.1327427   0.04956064]"
GPU error Tensorflow with 2.1 stat:awaiting response stale type:others comp:gpu TF 2.1,"### Issue type

Others

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.1

### Custom code

Yes

### OS platform and distribution

CentOS Linux release 7.4.1708 

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

10/7

### GPU model and memory

Nvidia A100 partitioned virtually in two 40GB GPUs, I am using one of them

### Current behavior?

I am working on 3D_ U-net. I am getting the ptax error with Tensorflow 2.1 when I run the 3D U-net , I am using tensorflow-large-model-support to scale the algorithm



### Standalone code to reproduce the issue

```shell
https://github.com/junaidjawaid1/3d_U-Net-TFLMS/tree/main```


### Relevant log output

```shell
Traceback (most recent call last):

File ""<string>"", line 1, in <module>

ModuleNotFoundError: No module named 'nvidia'

dirname: missing operand

Try 'dirname --help' for more information.

/opt/gridengine/default/spool/compute-0-3/job_scripts/108258: line 8: $'\342\200\213': command not found

/opt/gridengine/default/spool/compute-0-3/job_scripts/108258: line 10: $'\342\200\213': command not found

Traceback (most recent call last):

File ""<string>"", line 1, in <module>

ModuleNotFoundError: No module named 'nvidia'

dirname: missing operand

Try 'dirname --help' for more information.

2023-08-18 18:25:55.025052: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

2023-08-18 18:25:56.869820: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.7

2023-08-18 18:25:56.881355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.7

2023-08-18 18:26:00.093020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1

2023-08-18 18:26:00.188610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.192419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:

pciBusID: 2c8f7:00:00.0 name: NVIDIA A100 80GB PCIe MIG 1c.4g.40gb computeCapability: 8.0

coreClock: 1.41GHz coreCount: 14 deviceMemorySize: 39.25GiB deviceMemoryBandwidth: 901.22GiB/s

2023-08-18 18:26:00.192453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

2023-08-18 18:26:00.192490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10

2023-08-18 18:26:00.218166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10

2023-08-18 18:26:00.304953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10

2023-08-18 18:26:00.356784: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10

2023-08-18 18:26:00.396554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10

2023-08-18 18:26:00.396599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7

2023-08-18 18:26:00.396731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.398058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.399160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0

2023-08-18 18:26:00.411267: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA

2023-08-18 18:26:00.434767: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2249595000 Hz

2023-08-18 18:26:00.439151: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fc06e5d220 initialized for platform Host (this does not guarantee that XLA will be used). Devices:

2023-08-18 18:26:00.439189: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version

2023-08-18 18:26:00.648000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.648956: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fc06ec3b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:

2023-08-18 18:26:00.648984: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): NVIDIA A100 80GB PCIe MIG 1c.4g.40gb, Compute Capability 8.0

2023-08-18 18:26:00.649332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.650184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1558] Found device 0 with properties:

pciBusID: 2c8f7:00:00.0 name: NVIDIA A100 80GB PCIe MIG 1c.4g.40gb computeCapability: 8.0

coreClock: 1.41GHz coreCount: 14 deviceMemorySize: 39.25GiB deviceMemoryBandwidth: 901.22GiB/s

2023-08-18 18:26:00.650216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

2023-08-18 18:26:00.650232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10

2023-08-18 18:26:00.650249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10

2023-08-18 18:26:00.650258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10

2023-08-18 18:26:00.650267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10

2023-08-18 18:26:00.650276: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10

2023-08-18 18:26:00.650294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7

2023-08-18 18:26:00.650351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.651127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:26:00.651846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Adding visible gpu devices: 0

2023-08-18 18:26:00.651877: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.2

2023-08-18 18:32:04.446608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1099] Device interconnect StreamExecutor with strength 1 edge matrix:

2023-08-18 18:32:04.446938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] 0

2023-08-18 18:32:04.446953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1118] 0: N

2023-08-18 18:32:04.447365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:32:04.448403: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

2023-08-18 18:32:04.449678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1244] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 120000 MB memory) -> physical GPU (device: 0, name: NVIDIA A100 80GB PCIe MIG 1c.4g.40gb, pci bus id: 2c8f7:00:00.0, compute capability: 8.0)

2023-08-18 18:32:04.460014: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 117.19G (125829120000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.464233: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 105.47G (113246208000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.468360: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 94.92G (101921587200 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.472455: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 85.43G (91729428480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.476553: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 76.89G (82556485632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.480809: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 69.20G (74300833792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.484893: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 62.28G (66870747136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.488957: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 56.05G (60183670784 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.493121: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 50.45G (54165303296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.497205: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 45.40G (48748773376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

2023-08-18 18:32:04.501259: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 40.86G (43873894400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory

WARNING:tensorflow:sample_weight modes were coerced from

...

to

['...']

WARNING:tensorflow:sample_weight modes were coerced from

...

to

['...']

2023-08-18 18:33:25.857187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10

2023-08-18 18:35:29.287470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7

2023-08-18 18:46:02.247907: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal : Value 'sm_80' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. This message will be only logged once.

2023-08-18 18:46:36.474700: F tensorflow/stream_executor/cuda/cuda_dnn.cc:516] Check failed: cudnnSetTensorNdDescriptor(handle_.get(), elem_type, nd, [dims.data](https://dims.data/)(), [strides.data](https://strides.data/)()) == CUDNN_STATUS_SUCCESS (3 vs. 0)batch_descriptor: {count: 7 feature_map_count: 146 spatial: 64 0 64 value_min: 0.000000 value_max: 0.000000 layout: BatchDepthYX}

/opt/gridengine/default/spool/compute-0-3/job_scripts/108258: line 13: 237129 Aborted python $PYTHON_SCRIPT
```
",False,"[-5.16180038e-01 -5.01947165e-01 -1.12951040e-01  1.13012537e-01
  1.50612116e-01 -3.84690106e-01 -2.67057180e-01  1.35584116e-01
 -5.03461175e-02 -5.63588917e-01  1.55756734e-02 -7.36791119e-02
 -2.50189364e-01  1.24002829e-01 -3.62331212e-01  3.36350560e-01
 -3.56234491e-01 -1.12428125e-02  2.33238116e-01  1.77219465e-01
 -2.19699457e-01 -9.12443846e-02 -5.29165685e-01  3.66281211e-01
  1.81710988e-01  3.92521620e-01 -5.36232963e-02 -4.25715791e-03
  1.46890339e-03  2.87386537e-01  5.24027646e-01  1.55502200e-01
  1.38885573e-01  3.00297767e-01  1.00547656e-01  2.15837762e-01
 -3.15137923e-01 -4.36516315e-01 -3.75567734e-01  3.35708149e-02
  1.97089791e-01 -1.59539934e-03  2.87952781e-01  3.10517065e-02
  2.01073691e-01 -2.81746686e-01  1.21798769e-01 -1.82071477e-01
  2.27838755e-04 -2.41944939e-01 -4.92202155e-02  1.01250656e-01
 -5.13823748e-01 -2.36028641e-01 -2.51532584e-01 -2.05478191e-01
  3.09071362e-01  2.12480649e-02 -9.27027129e-03  2.09425390e-01
  1.11970372e-01 -4.90547642e-02  1.11235574e-01 -1.61162257e-01
  8.23739097e-02 -1.22922380e-02  3.36134791e-01 -2.25112915e-01
  6.30506158e-01 -2.57511586e-01  1.88183010e-01 -7.69402087e-02
 -3.77410293e-01 -1.42325401e-01  1.23905763e-01  2.12186128e-01
 -2.62427516e-03  1.61723197e-01  1.67986929e-01 -1.68355212e-01
  1.20947123e-01 -9.28048790e-02  4.27437313e-02 -4.01350915e-01
  1.42874569e-01 -1.44422799e-01  4.84881252e-01  7.42709264e-02
  6.26302600e-01 -4.25204277e-01  5.24964929e-01  4.03891087e-01
  6.80790022e-02  1.06606530e-02  5.57693481e-01  4.10085134e-02
  1.32571205e-01  5.98018244e-03  4.70707305e-02 -2.08645016e-01
 -5.92642166e-02 -1.77514106e-01  3.90527099e-02  1.90110251e-01
 -1.81557402e-01 -7.95701146e-02  7.11298436e-02  2.90098861e-02
 -8.25616047e-02 -1.41894221e-01  2.60519147e-01  1.08526312e-01
  3.70824754e-01  4.74696383e-02  5.62769361e-03  1.08888738e-01
 -2.08012104e-01  9.64645520e-02 -8.89453292e-02  7.66578138e-01
 -2.49702588e-01 -3.78805138e-02 -6.61980826e-04  7.29733482e-02
  2.23522142e-01 -4.07939255e-02 -1.96022421e-01 -1.01556674e-01
  2.23153263e-01 -1.57385916e-01  1.51213422e-01  8.90664160e-02
 -7.21748173e-02  1.59857333e-01 -1.42226011e-01  4.70344909e-03
 -9.75120515e-02 -2.27639288e-01 -1.98399752e-01 -1.07843615e-01
 -1.27987444e-01 -6.06035441e-02 -3.10378760e-01 -6.15442574e-01
  2.52512962e-01  3.14965487e-01 -2.08648592e-01  3.80188197e-01
 -5.21819405e-02  3.36596519e-01 -1.76696032e-01  1.85904711e-01
 -1.90202713e-01  5.72432995e-01  1.54938996e-01  1.23896703e-01
  3.68257940e-01 -2.93169349e-01 -1.21226184e-01 -6.01596534e-01
  1.45067155e-01  3.95624399e-01 -8.47675353e-02 -1.27049148e-01
  1.78422153e-01  2.72343755e-01 -3.95212770e-01 -2.68135220e-01
  1.37049511e-01  4.62387621e-01 -2.07822621e-01 -1.78397506e-01
  6.99408352e-04  9.64859575e-02  3.85980979e-02 -1.57117695e-01
  3.35896015e-01 -6.68081105e-01 -2.52742380e-01  2.86831319e-01
 -6.53127283e-02  1.14163615e-01  6.05380759e-02 -1.00814663e-02
  1.10729441e-01  5.06576113e-02 -1.13703519e-01 -4.32323255e-02
 -3.40881348e-01  4.57067527e-02 -4.19043422e-01 -2.28670329e-01
  2.90906698e-01 -2.59115607e-01 -2.86241770e-01  2.56482542e-01
  1.87688917e-01  9.75769237e-02 -1.20734923e-01  3.06426227e-01
 -2.56544471e-01 -2.98295796e-01  4.83090580e-02 -9.96975303e-02
  9.57660973e-02 -4.38151807e-01 -2.26525366e-01 -3.34314853e-01
 -4.37936962e-01  6.72502071e-02  1.10590011e-01 -5.38775325e-01
  6.90290891e-03  4.74822037e-02 -4.48357105e-01  3.10150623e-01
  5.09398468e-02 -1.28670372e-02 -3.62823725e-01  3.11427891e-01
  3.39123845e-01 -1.17058218e-01 -1.92040622e-01 -3.15783054e-01
 -4.21126783e-01  6.72676712e-02 -3.17801416e-01  3.01809311e-01
 -6.98921755e-02  2.61428595e-01  1.05697379e-01  1.32616848e-01
  4.48717624e-01  1.30388781e-01  4.72471863e-01  3.40651311e-02
 -8.30241144e-02 -2.29003668e-01 -1.02682434e-01  1.62514687e-01
 -2.56130695e-01 -1.77854449e-01  3.07305492e-02  1.19471505e-01
  4.33894157e-01  4.09339190e-01 -2.07934648e-01 -1.46230817e-01
 -5.30699611e-01  1.95058316e-01  1.71817467e-02  2.89292820e-02
  3.13195407e-01  2.43038580e-01  7.05642104e-01  2.28398144e-01
  7.75161162e-02  2.94423759e-01  2.34118700e-01 -1.80476025e-01
  3.39469492e-01  3.18664879e-01 -2.00535715e-01  5.58100283e-01
  2.58349478e-01  3.55405152e-01 -3.84808064e-01  5.62557578e-01
  5.24243005e-02 -1.13620929e-01  1.23120114e-01 -3.04300696e-01
  6.30798936e-01 -5.91747761e-01 -1.02272272e-01 -1.17837816e-01
  4.64333653e-01  1.79566190e-01 -9.93942544e-02  2.88094375e-02
  1.36076525e-01  5.08964539e-01 -3.97407889e-01 -1.35039598e-01
  2.00067610e-01 -1.26872599e-01 -1.90845817e-01 -7.44971395e-01
 -2.38074496e-01  1.12484150e-01 -3.09851229e-01  1.40399024e-01
  1.15445621e-01  1.31199166e-01 -1.07448444e-01  1.03304133e-01
 -2.40246542e-02  4.49990146e-02  1.55424416e-01  2.61572003e-01
 -3.09743762e-01  1.59992259e-02  2.60619462e-01 -5.65192103e-01
 -1.62809670e-01 -1.55672103e-01  2.35808596e-01  3.54463458e-01
  4.73188818e-01 -3.56249332e-01  2.30494186e-01 -1.24214604e-01
 -1.03350021e-02  4.47130203e-01 -1.29905418e-01  8.81420225e-02
 -4.01305884e-01  6.43250108e-01  2.56527245e-01 -5.41851074e-02
  2.58560240e-01 -2.68432349e-01 -2.81494945e-01  2.82875627e-01
  1.92717373e-01 -3.58926028e-01 -3.17582935e-02 -4.14413035e-01
 -5.29075116e-02  2.08157346e-01 -1.56689346e-01 -7.60460049e-02
 -3.85386869e-03 -1.08833082e-01  5.97205199e-03  7.01647401e-02
 -3.68340939e-01  1.50603652e-01  2.27851480e-01 -3.99808735e-01
 -3.51762712e-01 -2.65552521e-01 -4.86113727e-02 -4.72888708e-01
 -1.26780812e-02 -2.62041807e-01  6.68781042e-01  5.53108692e-01
 -7.26204067e-02  2.12442860e-01  5.53070121e-02  1.00105992e-02
 -4.14945573e-01 -3.85011062e-02 -2.20107764e-01  4.12814856e-01
  1.80526659e-01 -1.50008082e-01  6.42996788e-01  3.68714333e-01
 -2.46426240e-01 -1.40920980e-02 -2.70678371e-01  4.99921218e-02
 -4.24901471e-02 -3.50112885e-01 -1.44267291e-01 -1.59124091e-01
  1.90485537e-01  4.05272484e-01 -9.40324068e-02  2.01354086e-01
 -3.99198115e-01  2.64298737e-01  6.60016537e-01 -4.28746164e-01
 -4.10992205e-01 -5.06086908e-02 -2.61063874e-02 -5.04009783e-01
 -4.89573814e-02  4.07701768e-02  3.01928110e-02 -5.88388816e-02]"
CKPT to TFLite stat:awaiting response type:support stale comp:lite TFLiteConverter,"How can I convert ckpt file to TF Lite, while I've only .ckpt file. No meta-file present",False,"[-0.5315031   0.12774195 -0.06528166  0.01210596 -0.15667818  0.2166627
 -0.11701428  0.4085988  -0.16370663  0.03820334 -0.13146004  0.05397196
  0.1129339  -0.10766756 -0.03128267  0.03059472 -0.18052597  0.10275608
  0.22131509 -0.13016258 -0.29118827  0.1910397   0.2091258   0.09240896
 -0.0697303   0.05137964  0.14930305  0.1984786  -0.07480471  0.3449656
 -0.10721375  0.15798071 -0.49074528  0.2550748  -0.18833639 -0.21005331
 -0.1256431   0.32500437 -0.13630198 -0.3127406   0.3603224   0.14399387
  0.01939682  0.23763393 -0.16322778  0.21731797  0.03624365  0.08589304
 -0.17657395 -0.1319484  -0.06868558  0.09195361 -0.18371186  0.3272044
  0.02432323  0.04450078 -0.04880476  0.11193274  0.13510297 -0.20030916
  0.00107178 -0.11341653  0.23578012 -0.43696538 -0.06927288  0.29109874
  0.18753998 -0.19531493  0.267104   -0.0516664  -0.11385139  0.0040685
 -0.22579376  0.18515812 -0.2960161  -0.19831531 -0.5318994   0.23776047
  0.12119509 -0.17726444  0.08033277 -0.12571134  0.08966604  0.1560604
  0.16014577  0.04347604  0.14621842 -0.18085617  0.09740783  0.5402159
 -0.1533612   0.41249737  0.17268758 -0.10681816 -0.31915948 -0.11065312
  0.17614174  0.16076556 -0.04469886 -0.01969299 -0.04435415 -0.17856708
 -0.2543615  -0.01266473  0.05664379 -0.4023502  -0.04341225  0.1178713
 -0.1936531  -0.10958044  0.36102015  0.15900005  0.23610567 -0.34673816
  0.08037227 -0.1868118   0.19919227  0.04817279 -0.16704969  0.395914
  0.12629151 -0.11508137 -0.22979267  0.3000117  -0.03316838 -0.19134182
 -0.01491106 -0.06683763  0.02683148  0.24725202  0.110069    0.26681066
 -0.2047811  -0.13568288 -0.09375534 -0.12935875 -0.20841248 -0.14227974
 -0.09357718 -0.1081579  -0.15475307 -0.16333961 -0.17695895  0.11473099
  0.1667918  -0.02251451 -0.2536409  -0.01519192  0.17784694  0.14038931
  0.13173832  0.03460011 -0.37919417  0.04533983  0.26490793 -0.20521112
  0.11410798  0.01254334 -0.02403286 -0.15914288  0.14724806  0.07815649
 -0.02623095 -0.22231802 -0.08392784 -0.25357887 -0.0413424   0.11722485
 -0.01267258  0.1537149  -0.0920222  -0.4327251   0.02902887 -0.19016813
  0.05289685  0.06694646  0.6569405  -0.01352118 -0.07568958  0.0440792
  0.09587825  0.2937545   0.19236034  0.06648831 -0.01961133 -0.17301747
  0.08213726  0.2714009  -0.5233858  -0.31491488 -0.30400962 -0.07518115
  0.01899458  0.24115424 -0.30206195 -0.73119426 -0.09041664  0.10243963
  0.1947583  -0.39248556  0.3399998   0.09544331 -0.02906073 -0.11634926
  0.06819819  0.0211378  -0.6108312  -0.2700862   0.00714723 -0.337567
 -0.16203284 -0.1974019   0.16633783 -0.18530764 -0.23330685 -0.05769188
 -0.07912105 -0.15954088  0.09148363  0.0343869   0.16045456  0.03841482
 -0.11510561  0.00085965 -0.3031927  -0.02674208 -0.3639312   0.40498224
 -0.08638754  0.19045208 -0.1494948  -0.12154493  0.22671139  0.09943034
  0.13523395 -0.09953927  0.11665849  0.04727921 -0.5461597  -0.0185116
  0.00134773  0.04051461 -0.22128686 -0.29302427 -0.18278368 -0.29459897
 -0.03476715  0.22873737  0.3230641   0.10987107 -0.2408441   0.53745127
  0.04055063  0.09334121  0.45314595  0.34110904 -0.0083512  -0.24057153
 -0.03928948 -0.26088148  0.19502829  0.48861384  0.0766587   0.27690914
  0.16810988 -0.06621645  0.01251489 -0.15873323 -0.3012361   0.3436676
 -0.11373097 -0.01534483  0.41032788  0.03610974  0.15117118 -0.01356627
  0.23283462  0.03720473 -0.0350036   0.43263483 -0.01721654  0.4869826
 -0.16432272 -0.12759884  0.40012884 -0.14068912  0.11431743 -0.28417706
  0.02773077 -0.20837954  0.1836639  -0.2467448   0.14687623 -0.00221768
 -0.23458138 -0.20863974  0.4755709  -0.20488213  0.4805245   0.06965481
 -0.07604461  0.24977525  0.33832586  0.13094224  0.33911347 -0.08056736
  0.00529074 -0.03617854  0.16044599  0.02130078  0.12797953  0.3056516
  0.1823421   0.30251992 -0.35167146  0.05684723 -0.08017188 -0.19629607
  0.00829794 -0.12740506  0.3769979   0.08848407 -0.18074468  0.14884776
  0.0965246   0.12466292  0.30725053 -0.4680028  -0.05014182 -0.06264412
 -0.15230654 -0.02759969  0.20955668  0.22707114  0.10165015  0.0166616
  0.17005827 -0.19441414 -0.09151997 -0.24972673 -0.18239793 -0.13179612
  0.30490133  0.21716405  0.19354431  0.04278547  0.20352665 -0.25833747
  0.16670722  0.5097413  -0.43942142 -0.27437565  0.17433603 -0.13866693
 -0.28847066  0.13383031 -0.24982312 -0.00086445  0.3568558   0.20016438
 -0.1476699   0.34148362 -0.26524824 -0.16699697 -0.31825083 -0.18402858
  0.24222063 -0.14461938 -0.23510604  0.46187377  0.02569935 -0.05405745
 -0.16595574  0.00635074 -0.01353188 -0.16836046  0.08898561  0.02062168
 -0.04115406 -0.22083092 -0.05583818 -0.01742323 -0.2809228   0.15057012]"
"`tf.image.crop_to_bounding_box()` assumes `tf.int32` arguments, but not documented as such awaiting review type:feature type:docs-feature TF 2.13","### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04 (WSL 2)

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.image.crop_to_bounding_box()` implicitly assumes that the target width and height are `tf.int32`, but this is not documented anywhere. The cause for this is using `tf.shape()` which has the default `dtype` of `tf.int32`, in a stack operation:
https://github.com/tensorflow/tensorflow/blob/c9fafed9bc8cb0238a775fd4a0680e648c06b5b6/tensorflow/python/ops/image_ops_impl.py#L1250-L1254

### Standalone code to reproduce the issue

```python
import tensorflow as tf

image = tf.zeros([1000, 2000, 3], dtype=tf.uint8)
offset = tf.constant([0, 0], dtype=tf.int64)
size = tf.constant([900, 1500], dtype=tf.int64)
tf.image.crop_to_bounding_box(image, offset[0], offset[1], size[0], size[1])
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/.../test.py"", line 6, in <module>
    tf.image.crop_to_bounding_box(image, offset[0], offset[1], size[0], size[1])
  File ""/.../.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/.../.venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 6656, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Pack as input #1(zero-based) was expected to be a int32 tensor but is a int64 tensor [Op:Pack] name: stack
```
",False,"[-0.39676028 -0.31772262 -0.10869666  0.01755188  0.5083463  -0.22762391
 -0.19978258 -0.09101861 -0.5524039  -0.23102309 -0.00317963 -0.11920922
 -0.38783568  0.2533195  -0.15766653  0.35188454  0.00680911 -0.14080273
  0.27732342  0.15768796 -0.12475863  0.01781622 -0.1860759   0.26194546
 -0.02047436  0.08555824 -0.1889132  -0.09803574  0.06574571  0.10873167
  0.09343547  0.2922257  -0.2612368   0.10484198  0.02760536  0.14485937
 -0.15063307 -0.11505577 -0.23546758  0.01263469 -0.23447987 -0.12377343
  0.08377801 -0.04431691 -0.05501712 -0.25827044  0.0301474  -0.00561853
 -0.15985182 -0.0102423  -0.01320945  0.01585536 -0.34935716 -0.33094865
  0.02212524 -0.01594251  0.13754588  0.03193903  0.22431007  0.16876298
  0.06001132  0.15739042 -0.02297915  0.14655742  0.11941245  0.29312477
  0.21210143 -0.03357133  0.55723494 -0.2515134   0.18291548 -0.01407063
 -0.4062399  -0.00544547  0.07085462  0.02207818  0.03893531  0.23764396
  0.22146355 -0.2724182   0.098778   -0.19163218 -0.06943971  0.04266527
 -0.03304882 -0.13489062  0.29757458  0.26274526  0.36095148 -0.13838781
  0.54103243  0.16260839 -0.08641239  0.1115213   0.34552965  0.06722777
  0.02179968  0.0985878  -0.00330091 -0.06070412 -0.12023597 -0.52876294
 -0.0291028   0.10066634  0.00465448 -0.06294801  0.17051399 -0.1119009
  0.14213076 -0.03197759  0.19026351  0.16979721  0.05060825 -0.19239096
 -0.06877621 -0.2682104  -0.18447843 -0.1483911   0.18350154  0.37247694
  0.29228884 -0.22568832 -0.00649192  0.2233098   0.4877453   0.0849059
 -0.34487912  0.04304985  0.17985368 -0.19451106  0.11678576  0.09128281
  0.11205828  0.03732979  0.07650162  0.05847968 -0.4643159  -0.15245974
 -0.29154414 -0.13131405 -0.44242436  0.1856083  -0.03917782 -0.51428825
  0.05192228  0.1607897  -0.30249065  0.509793   -0.31419182  0.11968093
 -0.08684243  0.05595541  0.03493805  0.35903573  0.01986603  0.12136675
  0.44602096 -0.17050764  0.09844976 -0.43262383 -0.1030668   0.502035
 -0.10019461 -0.10469021 -0.0891837   0.17723522 -0.39599058 -0.2096523
  0.04603112  0.40994173  0.072776   -0.17436449  0.1440826   0.17023692
  0.21330926 -0.03522416  0.24747062 -0.6555184   0.14108437  0.39749572
  0.13305095  0.14073612  0.07716715  0.14062339  0.07459272  0.02729253
  0.21732584  0.22884487 -0.08534932  0.0689829  -0.26800245 -0.1887902
  0.2298511  -0.16971895 -0.08310126  0.26072735  0.29395196 -0.14891505
 -0.05414901  0.19974507 -0.028295   -0.02517647 -0.09668627 -0.04634194
  0.25595868 -0.13488755 -0.08454885 -0.5225368  -0.35275367 -0.1446451
  0.1437195  -0.4329746  -0.14515957 -0.1935723  -0.28271735  0.283519
  0.09583045 -0.1124272  -0.15929697  0.20890304 -0.0912942  -0.17755648
  0.13532707 -0.48571953 -0.1158701  -0.11363821 -0.1684134   0.11792392
 -0.22165927  0.16261461 -0.02600849 -0.0828778   0.36228925  0.11322047
  0.32133085 -0.04611237 -0.2573842  -0.30729017 -0.26604575  0.147463
 -0.5112182   0.05176618 -0.04945669 -0.05677127  0.24624431  0.32833403
  0.1199208  -0.00171182 -0.3173983   0.37022454 -0.36079615  0.24108663
  0.32086527  0.06461395  0.39641783  0.08239734  0.13542692  0.14367253
  0.27640218 -0.22287984  0.36424288  0.21372673  0.16692391  0.60785747
  0.26770756  0.4272139  -0.14462109  0.59174216 -0.1033245  -0.0280533
  0.01602354 -0.23197092  0.7264044  -0.38810286 -0.04833834 -0.03969525
  0.5035119  -0.14718267  0.02463466  0.20991826  0.09415208  0.28318557
 -0.13390253  0.22674297 -0.0040332  -0.01546454 -0.16352981 -0.7653578
 -0.17373465 -0.06951049 -0.43728733  0.03225436  0.01981295 -0.05734862
 -0.17374125 -0.04403293  0.04771275 -0.00644083  0.20688352  0.12802348
 -0.09958649 -0.111374    0.28759605 -0.35555863  0.00780415 -0.15732563
  0.36423928  0.24744885  0.3168984  -0.39452693 -0.00539211 -0.00685803
 -0.05871911  0.4631871   0.01457447  0.18203376 -0.3903398   0.642815
  0.23266983 -0.07663655  0.10545884 -0.30753338 -0.32101637 -0.21701208
  0.08365483 -0.02092675  0.10730212 -0.19839443  0.09714296 -0.02818548
 -0.24132879 -0.05506625 -0.2346145  -0.15654723 -0.31873596 -0.1793806
 -0.37556463  0.2115551  -0.07626888 -0.12505737 -0.22991675 -0.32974693
 -0.26795    -0.25749013 -0.1070123  -0.20713006  0.216378    0.48710734
  0.08130814  0.02317915  0.13339673  0.100577   -0.48361892  0.09115738
 -0.08280075  0.36711073 -0.1415011  -0.14004812  0.23392653  0.18634549
 -0.31234208  0.05884556 -0.3014219  -0.00318901  0.15681827 -0.13202792
 -0.20229113 -0.05720215  0.12078948  0.38531923 -0.02915929  0.27099496
 -0.3167544   0.43876234  0.42562014 -0.2930132   0.01387192  0.12244274
  0.38993847 -0.15543473 -0.05898122 -0.1036246   0.39222172  0.09315449]"
external/XNNPACK/src/qs8-igemm/gen/4x16c8-minmax-avx512skx.c:242:15: error: implicit declaration of function '_kshiftri_mask64' is invalid in C99 stat:awaiting response type:build/install stale comp:lite TF 2.4,"I run commands
""bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   //tensorflow/lite/java:tensorflow-lite""  
use 
tensorflow 2.4.0
 bazel 3.1.0 ,NDK  27 ,SDK 29 ,But what should I do to resolve the following errors

ERROR: /home/ferey/.cache/bazel/_bazel_ferey/97c7558b0863433a4def08aa5708cd20/external/XNNPACK/BUILD.bazel:3516:1: C++ compilation of rule '@XNNPACK//:avx512skx_ukernels' failed (Exit 1)
external/XNNPACK/src/qs8-igemm/gen/4x16c8-minmax-avx512skx.c:242:15: error: implicit declaration of function '_kshiftri_mask64' is invalid in C99 [-Werror,-Wimplicit-function-declaration]
      vmask = _kshiftri_mask64(vmask, 16);
              ^
1 error generated.
Target //tensorflow/lite/java:tensorflow-lite failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 229.018s, Critical Path: 17.13s
INFO: 1788 processes: 1788 local.
FAILED: Build did NOT complete successfully

",False,"[-5.96730709e-01  8.51732790e-02 -2.35786110e-01 -1.78903893e-01
 -4.40435112e-02 -1.41120344e-01 -1.11021169e-01  1.85485452e-01
 -2.62928724e-01 -4.31424119e-02  9.29191932e-02 -4.39892471e-01
  6.30901381e-03 -2.47754231e-01  1.60136949e-02  9.78010595e-02
 -2.16757447e-01 -1.61332175e-01  5.01583636e-01 -6.07808605e-02
 -7.90460408e-02  6.08005971e-02 -2.67934680e-01  1.84005931e-01
  4.64504361e-02  1.80837244e-01  1.31573752e-01  2.49934018e-01
  3.60306352e-04  2.93875247e-01  3.09064329e-01 -4.53787073e-02
  1.06948256e-01  1.61154836e-01  2.56753623e-01  3.52499336e-02
 -3.92914265e-01 -1.47389978e-01  1.32011259e-02 -1.06462404e-01
  1.26224816e-01  3.94239798e-02  1.17621787e-01 -1.19102523e-01
  1.60794005e-01 -2.60679051e-02 -1.56588063e-01  7.25686550e-02
 -2.08751544e-01 -2.49746203e-01 -1.49852797e-01  3.28527391e-01
 -2.69953370e-01 -7.80484602e-02  2.22895980e-01 -8.99775773e-02
  6.38199002e-02  1.90771669e-01 -1.05631217e-01  1.48421884e-01
  4.65191424e-01 -1.15521774e-01  1.40486524e-01 -6.25495687e-02
 -5.09414673e-02  1.54438704e-01  1.02109939e-01 -1.52773529e-01
  2.33456492e-01 -1.44158015e-02 -9.10129175e-02 -1.00168511e-01
 -1.08054101e-01  5.67739382e-02  1.16916232e-01  1.66218728e-02
 -3.34194332e-01  2.35111162e-01  9.53668877e-02 -1.66762352e-01
 -1.00256465e-01 -1.69940591e-01 -5.76269999e-02  1.85461774e-01
 -3.12952846e-02 -1.06001154e-01  3.55950892e-02 -1.79147094e-01
  3.07544142e-01  4.17168736e-02  3.32794547e-01  1.61915511e-01
  6.36040494e-02  1.15246803e-01  2.23012000e-01 -1.75509602e-01
  1.47011559e-02  1.12212785e-01 -1.99116766e-01  3.08857467e-02
 -1.13042369e-01 -4.71543968e-02 -1.84230298e-01 -1.01560026e-01
 -1.59775943e-01 -2.01812536e-01  1.99821517e-01  1.14526518e-01
  1.41629234e-01 -1.09686084e-01 -7.58124050e-03 -1.07523585e-02
 -1.47812665e-01 -1.55241236e-01  3.03356741e-02  2.80306280e-01
  5.57404757e-03  9.28790960e-03 -1.49491772e-01  6.58656508e-02
 -2.45944440e-01 -7.71291405e-02 -9.32043791e-02  1.75002232e-01
  6.91187084e-02 -1.66902155e-01 -2.97955304e-01 -1.40289098e-01
  1.03753522e-01  1.01622917e-01 -9.61391404e-02  1.79372698e-01
  7.01336116e-02 -7.80140311e-02  3.20607066e-01 -6.96369112e-02
 -2.58388698e-01 -1.83042288e-01  5.66539839e-02 -2.17511475e-01
  1.95376948e-03  7.08418563e-02  1.22840479e-01 -1.45588368e-01
 -8.50098208e-03  2.40474716e-01 -6.44000173e-02  7.93637782e-02
  1.04471505e-01 -1.40795916e-01  9.51521695e-02 -1.18224882e-01
 -1.75756335e-01  4.58712608e-01  1.91522330e-01  8.88688862e-02
  3.28434616e-01 -6.02506101e-03  8.87890905e-02 -4.27048057e-02
  3.22342962e-02  2.66130984e-01  3.38931084e-01  2.11439300e-02
  1.01143405e-01 -5.34370095e-02 -3.46642166e-01 -1.15812086e-01
  5.03544733e-02  7.49570206e-02 -2.93847710e-01  5.60262799e-02
  1.31541073e-01 -7.96317123e-03  1.77117839e-01 -1.74462736e-01
  5.43651342e-01 -1.10099301e-01 -7.16292709e-02  2.58062482e-01
  1.07848927e-01  5.40804639e-02  1.05644614e-01 -5.95552921e-02
 -7.91599695e-03  1.73411667e-01  2.21257359e-01 -1.15180999e-01
 -5.95344417e-02 -2.70560384e-04 -3.88484836e-01  3.33750904e-01
  8.34164917e-02  1.59883723e-02 -1.27926722e-01 -2.08950579e-01
  1.45554572e-01  4.67732102e-02  4.01782572e-01  1.33567989e-01
 -3.98165360e-03  2.00842828e-01 -4.23717499e-02 -2.65953690e-02
  4.54121269e-03 -2.07597241e-02 -3.53528440e-01 -1.82357758e-01
 -1.32741351e-02  5.43142036e-02 -2.32309531e-02 -9.96227264e-02
 -1.70058966e-01 -5.23575246e-02 -1.78288117e-01  3.89447093e-01
 -9.70841944e-02  2.10457332e-02  1.74772758e-02  2.16826349e-01
  9.86111760e-02 -1.35903776e-01 -3.27153832e-01 -1.55559316e-01
  2.93188840e-02 -1.79752916e-01 -1.57324746e-01 -7.17271492e-02
 -2.05905233e-02 -4.17020768e-02  1.69065669e-02 -4.14207160e-01
  5.73127627e-01 -9.92358290e-03  2.35943481e-01 -2.32459474e-02
  1.03478737e-01 -1.60008565e-01  2.17621252e-02  9.57391113e-02
 -3.42350125e-01 -2.76592433e-01 -9.59604830e-02 -1.07844055e-01
  3.14236045e-01 -2.05542207e-01 -3.37802097e-02  6.35381714e-02
 -3.43956500e-02  2.40255490e-01  3.21693644e-02  2.55491257e-01
  1.50628492e-01  2.13946909e-01  9.73237157e-02  1.22889884e-01
 -1.81068718e-01 -7.11496994e-02 -9.74753499e-02 -5.37089705e-02
  3.33075523e-02  3.43007505e-01  2.96964142e-02 -1.01736881e-01
  1.52496547e-01  5.08843176e-02 -3.20797801e-01  1.14249669e-01
  1.38988554e-01 -1.18433416e-01  3.27934921e-01 -3.79872322e-01
  1.54909268e-01 -1.08404353e-01  1.84754044e-01 -3.21384519e-01
  3.28088462e-01  5.56120835e-03 -7.77319595e-02  4.03122097e-01
 -2.15215057e-01  4.29170489e-01  2.12764651e-01 -2.91491393e-02
  4.87129748e-01 -3.74006540e-01  8.92358646e-02 -2.55220711e-01
 -2.69000113e-01  8.45643319e-03  8.77360702e-02 -6.29535094e-02
  3.78314108e-02  3.31541151e-02 -1.37594193e-01 -9.02004465e-02
 -1.71853974e-01  6.03554845e-02 -4.13022712e-02  4.59193140e-02
 -9.56515074e-02  1.73741937e-01  2.30445892e-01  1.28537133e-01
 -1.93850905e-01 -1.00725610e-02 -4.26505543e-02  6.93858564e-02
  5.50677240e-01 -3.81781012e-01  3.70561361e-01  2.92119712e-01
 -1.58309311e-01  3.79560679e-01 -1.67129129e-01  2.11272240e-01
 -2.19809815e-01  1.02278970e-01 -8.97531584e-02 -7.30829537e-02
  2.68412173e-01 -4.55798566e-01 -2.62335747e-01  2.00156838e-01
  1.68671429e-01 -8.67658760e-03 -3.46982181e-01 -4.76160884e-01
  4.11001295e-02 -8.64143521e-02  5.07431477e-03 -1.72143597e-02
 -1.34685397e-01  2.42916018e-01 -1.55956686e-01 -1.98203892e-01
 -3.75236511e-01  2.53792167e-01 -5.66092655e-02 -2.38222644e-01
 -9.23305526e-02 -3.31548601e-01  8.33383799e-02 -1.96377695e-01
 -1.26397640e-01 -2.78818756e-01  3.10716420e-01  3.32635164e-01
  1.22346140e-01  7.94468373e-02 -1.10543326e-01  1.77073181e-01
 -3.95425595e-03 -4.13498580e-02  3.93524617e-02  3.45973000e-02
 -4.42493819e-02  1.60102427e-01  2.62965411e-01  2.05491289e-01
 -1.79836065e-01  2.01461792e-01 -3.00990701e-01 -1.09913103e-01
  6.47131726e-03 -1.79495603e-01 -3.31771910e-01 -4.64504287e-02
 -7.88366422e-02  2.31637985e-01  3.11423391e-02  1.22977998e-02
 -3.11189324e-01  1.09265499e-01  1.48765162e-01 -6.79418519e-02
  1.30177021e-01 -1.82045698e-02 -8.10785294e-02 -1.70661926e-01
 -6.26429319e-02  6.81557879e-02  1.59191728e-01  1.12215802e-01]"
pybind11_proto from python to C++ type:feature comp:core TF 2.13,"@BlaziusMaximus thanks for the explanation.
I've been exploring how to update the [`import_graph_def()`](https://github.com/tensorflow/tensorflow/blob/v2.13.0/tensorflow/python/framework/importer.py#L353-L411) code-path to use pybind11_protobuf and I could use your help with the following:  Similar to how pybind11_protobuf allows us to pass protos directly from C++ to Python, is there a way to pass a `GraphDef` proto from python to C++ without performing serialization? This would be needed to invoke the [TF_GraphImportGraphDefWithResults](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L1801) from pywrap session in C++.

_Originally posted by @othakkar in https://github.com/tensorflow/community/issues/453#issuecomment-1674101660_
            ",False,"[-0.4748968  -0.47760797 -0.5101427  -0.04762135  0.13057029 -0.07599537
  0.02751221  0.04164784 -0.4657925   0.09428324  0.0174114  -0.23819774
 -0.25773007  0.11043786  0.18944976  0.26204944  0.07401021 -0.26406413
  0.06817389 -0.03371003  0.18737511  0.21983193 -0.00204091  0.15895557
 -0.03743693  0.05969227 -0.07266339 -0.21544212  0.01877172  0.05971926
  0.39890406  0.1756087  -0.08662242  0.20001608  0.01142278  0.26128918
 -0.22330683 -0.14219213  0.22238018 -0.07563491 -0.16071218  0.16361418
  0.00212858 -0.03571496  0.085454    0.07703149  0.21966732  0.3052934
 -0.03729833 -0.07232206 -0.02195384 -0.14848243 -0.3264352  -0.47616458
  0.24772961  0.13609406  0.23807392  0.24099109  0.13145861 -0.02189203
 -0.07331155 -0.08735083  0.23134357  0.07946022 -0.06346571  0.19907282
  0.03289805  0.02257659  0.6010955  -0.33502257 -0.0730003  -0.00121448
 -0.35824823 -0.2556683   0.01033646  0.21642584 -0.10185818  0.19387129
 -0.04783209 -0.30752736 -0.19130562  0.03741283  0.13109474 -0.09411357
  0.21750784  0.15205841  0.10372965  0.10624472  0.00405558  0.07589556
  0.5787241  -0.12160695 -0.02770205  0.23835137  0.0070652   0.23065513
  0.02330616 -0.08421212  0.03625318 -0.23534478 -0.21529654 -0.50012434
  0.04756619  0.06255089 -0.04844607 -0.046878    0.1914812   0.16759178
  0.08127423 -0.1608685   0.02590142 -0.05349041  0.27263442  0.05527158
  0.38796857 -0.16393429 -0.3614908   0.06164933 -0.10388458  0.3122235
  0.06131247  0.04565084  0.3330269   0.37827408 -0.06365973 -0.05209514
 -0.03984381 -0.03699107  0.16257441 -0.02877132  0.38792038  0.1893686
 -0.2745479   0.15171418  0.2535578  -0.10784049 -0.2199756  -0.2950875
  0.06056097 -0.25634593 -0.24546099  0.08628202 -0.07235865 -0.4631547
 -0.17730615  0.09218919 -0.10146233  0.3735003  -0.14553529  0.07435396
 -0.0017804   0.00774018 -0.1792821   0.56131816  0.14144802  0.41333804
  0.3496238   0.06809443  0.15034574 -0.45491332 -0.0979301   0.28445333
  0.05281829 -0.08263219 -0.33090508  0.1071638  -0.35341027 -0.25325692
  0.06366833 -0.02757473 -0.33017325  0.2069598   0.04178847 -0.05704362
  0.30225748  0.05478484  0.01947434 -0.40763727  0.01565285  0.29838228
  0.11613848  0.2023915  -0.1660817  -0.08729321 -0.02637029  0.22124705
  0.22623356 -0.15799253 -0.0450713   0.25614375 -0.17870028  0.10991474
  0.11043594 -0.28033894 -0.12164007  0.06983924  0.21884212  0.23874804
  0.11982492  0.16305622 -0.07235788  0.11928589  0.05894917  0.06148764
  0.02013336 -0.22415484 -0.28379533 -0.2441201  -0.37403598  0.16270702
 -0.21187407 -0.16078366  0.00158076  0.06743634 -0.07188657  0.08160312
  0.25827456 -0.17315578 -0.05355467 -0.18818963 -0.1073855  -0.07065947
  0.12776682 -0.08387444  0.15583035  0.10422391 -0.12438087 -0.04666821
 -0.02111781 -0.03846615 -0.16501755 -0.2007317   0.42341033  0.12196139
  0.18311428  0.06441521 -0.00606902  0.03411572  0.00438167  0.5231878
 -0.35707182 -0.04200344 -0.06707048 -0.04638376 -0.08998045 -0.30193174
 -0.21303058 -0.3615577  -0.2867722   0.07112162 -0.21965975 -0.06388906
  0.31932294  0.24904712  0.25848386  0.2969096  -0.29734805  0.00128838
 -0.17543414 -0.01158693 -0.06785676 -0.10136452  0.04388066  0.16663747
  0.16867226  0.03708089 -0.44475055  0.17446715  0.10163199  0.00940013
  0.14492594 -0.14235044  0.23458663 -0.1169446   0.24892107  0.15894993
  0.46147808 -0.17596877 -0.03383848 -0.07781446  0.08677125  0.13801125
 -0.10371169  0.00836003  0.16029975 -0.09907507  0.01565649 -0.26413876
 -0.5190631  -0.00175387 -0.15359381  0.10524943  0.17375875 -0.17029527
  0.12209082  0.06656645  0.09489031 -0.00143446 -0.12646843  0.14995998
 -0.16471347  0.11649099 -0.02786956 -0.03233033  0.04711483 -0.14177403
  0.27222195 -0.23127565  0.5736196  -0.39072922  0.22213499  0.00926065
 -0.23919727  0.36563596 -0.11685759 -0.02496501 -0.0836883   0.54143083
  0.38106912  0.10224713 -0.11904089 -0.19005191 -0.20155469 -0.11902273
  0.01213741  0.16957635 -0.41102263 -0.1531589  -0.11358659  0.02240527
 -0.10226952  0.0442567  -0.20141023  0.32220042 -0.1504084   0.13422158
 -0.16411293  0.05045813  0.01846681 -0.09862256 -0.0660918  -0.21940142
  0.12582517 -0.55933255 -0.25637016 -0.1991435   0.35540542  0.29369423
 -0.241846   -0.23974797  0.10891242  0.22567418 -0.21624713  0.06966765
  0.2610458   0.35649872 -0.15512194  0.0798298   0.00179341  0.34595567
 -0.4356952   0.21155186 -0.20819229 -0.22231144  0.24957895 -0.33541894
 -0.27273113 -0.23276809 -0.21065287  0.20181707 -0.32710236  0.20302963
 -0.40068656  0.3525674   0.378455    0.22792727 -0.32042488  0.33067903
 -0.05363854 -0.24059504 -0.18274078  0.21880594  0.31110144 -0.43480033]"
TensorFlow profiler running into OOM issue on GPU stat:awaiting response type:support stale comp:gpu TF 2.11,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.11, TF 2.4

### Custom code

No

### OS platform and distribution

Red Had

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current behavior?

Running TensorFlow profiler for longer than 10 second period results into OOM error, crashes the tf inference process and the profiler returns DEADLINE_EXCEEDED. Is there anyway to limit the sampling rate or way to reduce the amount of information being collected to avoid crashing the process?

Here is the code that I run:
tensorflow_profiler.experimental.client(""grpc://localhost:3222"", ""profiles"", 30000)

### Standalone code to reproduce the issue

```shell
tensorflow_profiler.experimental.client(""grpc://localhost:3222"", ""profiles"", 30000)
```


### Relevant log output

```shell
DEADLINE_EXCEEDED
```
",False,"[-4.11760032e-01 -2.58570910e-01 -9.49539542e-02  2.79758632e-01
  2.52728701e-01 -4.85693604e-01 -2.94878483e-01  9.27003324e-02
 -1.87547162e-01 -4.36152369e-01 -5.02162129e-02 -2.94279139e-02
 -1.81347609e-01  9.57200304e-02 -1.63570806e-01  3.77819240e-01
 -1.72460526e-01  3.93065140e-02  2.55352914e-01  1.66756734e-01
 -1.57976821e-01 -1.88850045e-01 -1.87070891e-01  2.45532785e-02
  1.75694555e-01  2.54610360e-01 -3.13757122e-01 -5.96395507e-02
 -1.41170472e-01  2.35383123e-01  5.78978539e-01  1.37654245e-01
 -8.53545144e-02  9.10410285e-02 -1.28783464e-01  2.30244875e-01
 -3.07170033e-01 -3.65886331e-01 -4.45304155e-01  8.83685052e-02
  2.13256836e-01  2.33444333e-01 -5.48895225e-02  1.05207562e-01
 -4.81177829e-02 -3.01570982e-01  1.96919367e-01 -2.69651800e-01
 -1.27580926e-01 -1.54644459e-01 -2.63386250e-01 -1.11578465e-01
 -5.89040279e-01 -2.19983697e-01 -2.45109677e-01 -7.04471767e-02
  2.56076157e-01 -7.71498904e-02 -4.75184470e-02  2.07909256e-01
 -1.13214090e-01  6.89489767e-04  2.37207577e-01 -1.51222870e-01
  2.08016276e-01  1.29198432e-01  2.11217195e-01 -1.32530063e-01
  4.93010670e-01 -1.19990475e-01  6.26709759e-02 -1.59994319e-01
 -4.11259294e-01 -1.08518049e-01  1.11911520e-01  2.77679980e-01
  8.28767568e-03  1.12460956e-01  4.01148647e-01 -4.78117764e-02
  7.07587749e-02 -3.42684507e-01 -8.66303593e-02 -2.84073263e-01
  2.33312815e-01  4.19815592e-02  4.18815017e-01  1.43458098e-01
  4.63418752e-01 -2.31705785e-01  3.36948574e-01  3.85360003e-01
  5.08825406e-02 -2.00273320e-02  6.42297983e-01  1.08222380e-01
  8.60812664e-02  1.93148106e-01 -3.87819372e-02 -2.97357917e-01
 -1.37015954e-01 -1.06938824e-01  1.33761913e-01  3.03275168e-01
 -1.88465625e-01 -1.94947809e-01  2.11627185e-01  7.68725425e-02
  6.13226816e-02 -9.26016793e-02  1.43754572e-01  2.69984268e-03
  2.99239367e-01 -1.52092442e-01 -1.04735428e-02  9.35810152e-03
 -3.11253846e-01  2.90767848e-02 -1.32541656e-01  9.20117497e-01
  1.44188059e-02 -5.07363155e-02  8.15170705e-02  7.99475610e-03
  3.42042923e-01  7.64821321e-02 -1.47502795e-01 -1.14692159e-01
  2.10404322e-01 -1.09204873e-01  2.23737627e-01  9.75926071e-02
  9.77554768e-02  3.12623590e-01 -6.88805655e-02  6.16609007e-02
 -1.33542225e-01 -2.50768125e-01 -3.09482694e-01 -1.63351029e-01
 -1.92702636e-01  1.65663987e-01 -2.81021237e-01 -5.92041016e-01
  2.44312122e-01  3.51157904e-01 -3.84462535e-01  3.13641518e-01
 -1.83516845e-01  1.81922525e-01 -2.41409525e-01  2.01384336e-01
 -1.61209300e-01  5.07214546e-01  1.60036966e-01  2.40769655e-01
  2.74562240e-01 -2.36292765e-01 -8.38401020e-02 -7.00605392e-01
 -7.51148164e-03  4.48316991e-01 -1.05363861e-01 -2.04567179e-01
 -2.19795965e-02  1.99140117e-01 -3.60544026e-01 -2.92928874e-01
 -9.85120237e-02  4.35709238e-01 -4.04740237e-02 -8.41635317e-02
 -1.39043823e-01  2.30151229e-02  1.58775538e-01 -1.03312507e-01
  2.92727381e-01 -5.93667507e-01 -3.64871062e-02  2.68765986e-01
 -1.10274382e-01  2.42999136e-01  8.95140618e-02 -2.29995400e-02
 -1.48662567e-01  5.75975329e-02 -6.81687146e-02  1.06637806e-01
 -2.05206543e-01  5.12966774e-02 -3.28567982e-01 -2.06072494e-01
  4.97567862e-01 -1.82264566e-01 -1.23847149e-01 -1.09320618e-02
  2.40923524e-01  2.21074462e-01 -2.20002979e-03  2.54726887e-01
 -4.38434370e-02 -2.51709551e-01 -1.29078478e-02  1.56354792e-02
  1.21712476e-01 -3.70665401e-01 -2.83655971e-01 -2.38741845e-01
 -3.77250940e-01 -4.87082861e-02  1.92385748e-01 -5.47997117e-01
 -4.49564904e-02  3.28430459e-02 -3.68822843e-01  2.13511467e-01
  1.39843851e-01  2.58263558e-01 -1.18310884e-01  3.20668295e-02
  1.79924458e-01 -2.36269161e-01 -1.30385846e-01 -3.61569196e-01
 -3.23609829e-01  2.63636380e-01 -9.08800066e-02  1.03185393e-01
 -1.44712001e-01  3.64896715e-01  1.01635024e-01  3.33389819e-01
  5.49394250e-01  1.80395797e-01  4.45523143e-01 -4.36992347e-02
 -1.97091792e-02 -1.37310654e-01 -2.03118071e-01  2.31621359e-02
 -2.56159186e-01 -2.55155981e-01 -8.15337673e-02 -6.99748546e-02
  2.25444585e-01  2.83464611e-01 -2.26870283e-01 -3.12857211e-01
 -2.52881855e-01  3.11128527e-01 -1.64466631e-02  3.05614769e-02
  3.44947010e-01  2.04867914e-01  5.39724231e-01  2.82588243e-01
  2.44867057e-02  1.86206073e-01  2.67330140e-01 -1.54118150e-01
  4.19605970e-01  1.43027112e-01  7.73989223e-03  4.71495479e-01
  3.82106602e-01  2.60094285e-01 -3.49691033e-01  5.22201896e-01
  9.43759754e-02 -7.77296349e-02 -3.03355716e-02 -3.66137564e-01
  7.04592943e-01 -5.88248014e-01  8.69987756e-02  1.48294792e-01
  3.48489046e-01  4.17572968e-02 -4.09876332e-02  6.90872818e-02
  1.49884358e-01  3.02854657e-01 -2.92748600e-01 -1.36216521e-01
  2.10454211e-01 -1.71651125e-01 -4.92242444e-03 -6.38943315e-01
 -2.77629167e-01  1.97679549e-01 -1.36859268e-01  6.23503178e-02
  8.57848525e-02  1.37336388e-01 -1.46971107e-01 -3.16658840e-02
  7.29578286e-02  1.61932066e-01  2.14531094e-01  1.92147434e-01
 -2.87966728e-01 -1.91338956e-01  2.56148815e-01 -4.83105749e-01
 -3.14719290e-01 -2.00338095e-01  1.91839874e-01  1.91832677e-01
  3.27692926e-01 -4.72220182e-01  6.01883084e-02 -7.20660612e-02
 -3.55996415e-02  3.31326485e-01 -9.20055285e-02  4.89138216e-02
 -3.94623280e-01  8.60473633e-01  2.51757294e-01 -9.26251858e-02
  1.92444995e-01 -2.84825295e-01 -2.84567535e-01  2.65956104e-01
  2.74186701e-01 -1.64218143e-01 -1.15126513e-01 -4.51170295e-01
 -1.03630513e-01  2.78711140e-01 -3.58807743e-02 -1.33513901e-02
 -1.54473454e-01 -1.16043679e-01 -5.11485562e-02 -1.53897613e-01
 -3.49851102e-01  2.43594795e-01  6.56895414e-02 -4.76948440e-01
 -2.83859134e-01 -1.09216437e-01 -2.01376230e-02 -2.39651099e-01
  2.71987300e-02 -3.46355349e-01  5.05557120e-01  5.71807504e-01
 -1.64704144e-01  4.79298010e-02  1.34106681e-01  2.24817991e-01
 -2.76164055e-01 -1.47074640e-01 -2.32445449e-01  3.77682030e-01
  8.29347149e-02 -2.02116847e-01  4.69156623e-01  3.76857728e-01
 -1.30854040e-01  9.16791409e-02 -1.27743840e-01  1.20802239e-01
  8.22499767e-02 -2.15107277e-01 -1.29646599e-01 -2.83640683e-01
  3.12389791e-01  3.68062675e-01 -4.32212241e-02  1.90798044e-01
 -3.05964023e-01  2.14670643e-01  2.86354721e-01 -5.10884464e-01
 -2.57175803e-01 -7.35319108e-02  3.96991447e-02 -2.43954763e-01
 -7.56610259e-02 -8.53661448e-02  2.33832359e-01 -6.20856285e-02]"
`numpy()` making copies with model parameters stat:awaiting tensorflower comp:apis type:performance TF 2.12,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling `numpy()` here no copies are made:
```python
import os, psutil
import tensorflow as tf

process = psutil.Process(os.getpid())
x = tf.random.normal((1, 300000))
print(process.memory_info().rss / 1e9). # 0.48
npy = x.numpy()
print(process.memory_info().rss / 1e9). # 0.48
```
However in the below example the increase in memory implies a copy is being made when calling `numpy()` on a model variable, is there any reason for this?

### Standalone code to reproduce the issue

```shell
import os, psutil
import tensorflow as tf

class TestKerasLinear(tf.keras.Model):
    def __init__(self, out_size):
        super(TestKerasLinear, self).__init__()
        self._linear = tf.keras.layers.Dense(out_size)

    def build(self, input_shape):
        super(TestKerasLinear, self).build(input_shape)

    def call(self, x):
        return self._linear(x)

tf_module = TestKerasLinear(1000)
tf_module.build((None, 300000))

process = psutil.Process(os.getpid())
print(process.memory_info().rss / 1e9). # 1.68

npy = tf_module.variables[0].numpy()

print(process.memory_info().rss / 1e9)  # 2.88
```


### Relevant log output

_No response_",False,"[-5.95374703e-01 -5.15826344e-01 -4.56026755e-02 -7.95423165e-02
  2.15135992e-01 -3.93742442e-01 -1.36954725e-01 -2.72811167e-02
 -2.94176787e-01 -2.86981106e-01  2.43198738e-01  2.04391927e-02
  1.21011641e-02  1.42353520e-01 -8.38890523e-02  2.69340754e-01
 -3.17809545e-02  1.72369361e-01  2.00039968e-01  2.54791319e-01
 -1.84436411e-01 -2.18085438e-01 -1.73468500e-01  1.75637662e-01
  1.60657644e-01  1.44369900e-01 -2.34963894e-01 -5.37744015e-02
 -2.91168168e-02  1.89247578e-02  3.76741171e-01 -6.43890947e-02
 -3.56332883e-02  7.86512122e-02 -1.35138826e-02  4.01391119e-01
 -3.42721701e-01 -2.99635082e-01 -2.19314903e-01  2.13365763e-01
  3.21680345e-02 -1.50585532e-01  1.36641353e-01 -2.76913166e-01
  1.19702823e-01 -9.05631483e-02  3.49078327e-02 -6.68294951e-02
 -1.23167463e-01 -4.97955866e-02  3.52700166e-02 -1.53940041e-02
 -4.81455445e-01 -5.20569682e-01 -2.40904659e-01 -1.64326932e-02
  8.51689652e-02 -1.94180459e-01 -1.38787583e-01  7.15916380e-02
  3.52015980e-02  7.80415013e-02 -1.13088362e-01 -1.04730904e-01
  2.65391171e-01  2.81230390e-01  2.70871818e-01  3.08229476e-02
  4.83988643e-01 -1.18703470e-01  1.98279873e-01 -3.45841050e-04
 -3.96546334e-01 -4.64259647e-02 -1.84133016e-02  1.37889415e-01
  2.94361264e-01  1.48754135e-01  2.54122108e-01 -3.49601328e-01
 -6.95372820e-02 -3.52339089e-01 -1.30305260e-01 -2.88372278e-01
  9.57320854e-02 -2.38993153e-01  6.07118785e-01  1.43877685e-01
  4.83610928e-01 -5.92518985e-01  4.50814962e-01  4.06939328e-01
 -1.64470091e-01  7.48597160e-02  5.27518034e-01  3.59833598e-01
  6.61130901e-03  2.09702030e-01  1.20776370e-01 -1.48421377e-01
 -4.35728058e-02 -2.07283348e-01 -2.03665346e-02 -1.49571411e-02
 -6.33655787e-02 -2.37705916e-01  8.86872783e-02 -1.47673547e-01
  5.28870299e-02  6.27513826e-02  1.62620813e-01  5.19810952e-02
  1.77321538e-01 -3.81797105e-02  6.01623114e-03  5.73250838e-02
  2.04796176e-02 -1.49718421e-02 -1.71954393e-01  6.46407723e-01
  1.01195425e-01 -3.06441426e-01  1.04265250e-01  9.68822930e-03
  2.76880205e-01  1.31155923e-01 -1.68596670e-01 -1.19472064e-01
 -3.06721833e-02  9.74582359e-02  2.38973707e-01  5.45801856e-02
 -7.19408542e-02  3.29176366e-01 -2.58417696e-01  9.83640030e-02
 -2.00308904e-01 -1.13858320e-01 -3.52925599e-01 -2.55495727e-01
 -4.57338482e-01  2.12070256e-01 -3.21101606e-01 -6.44632578e-01
  3.22494149e-01  2.57909566e-01 -2.78238833e-01  2.95965791e-01
 -1.36766657e-01  1.70121208e-01 -3.95988487e-02  1.85939521e-01
 -8.32304284e-02  3.31680357e-01  1.61695480e-01  7.39936233e-02
  3.64631712e-01 -1.26200259e-01  1.79484606e-01 -5.53692758e-01
 -6.23366572e-02  4.26479578e-01 -2.14973986e-01 -1.62575454e-01
 -1.50778696e-01  1.71934515e-01 -3.43911469e-01 -1.46825016e-01
  7.93789923e-02  5.75831175e-01 -9.70204845e-02 -1.71691671e-01
 -5.97502515e-02  5.18617034e-02  2.11075380e-01 -2.06332281e-02
  1.35722160e-01 -6.51240826e-01 -9.21319723e-02  1.93532467e-01
  5.49462140e-02  1.89136609e-01  2.35669166e-01  1.40935063e-01
  1.10043250e-02 -1.38438851e-01  2.01114058e-01  1.16715908e-01
 -1.43111497e-01 -5.16199879e-02 -4.52260345e-01 -2.78914154e-01
  4.29168105e-01  3.39096263e-02  1.84075534e-02  2.81343423e-02
  2.46717364e-01 -3.67706493e-02  7.37931952e-02  9.43504572e-02
 -2.91636646e-01 -2.42945135e-01 -1.58017784e-01 -1.58240125e-01
  2.45061755e-01 -2.81124532e-01 -1.30585909e-01 -4.89747673e-01
 -3.27609479e-01  1.10193804e-01 -1.16198763e-01 -6.28802776e-01
  1.16574839e-01  1.17046610e-01 -4.60186303e-01  4.33933377e-01
 -7.48932362e-04  1.11787975e-01  2.11054571e-02  2.96099663e-01
  3.37409973e-02 -2.26284370e-01 -3.76605242e-02 -4.81672943e-01
 -2.22090706e-01  2.52803087e-01 -3.44180793e-01  5.58576472e-02
  1.15530849e-01  1.72999978e-01  1.48774579e-01  7.77353942e-02
  2.73260325e-01  2.97355413e-01  1.79741040e-01 -2.69905329e-01
 -1.33435056e-01 -1.82331324e-01 -2.92105496e-01 -2.44091861e-02
 -2.80951798e-01 -2.26733297e-01  5.84352110e-03 -6.08826093e-02
  2.81390280e-01  5.64795971e-01 -8.90116990e-02  2.28557251e-02
 -5.36251783e-01  3.40819508e-01 -3.72600853e-01  1.53101280e-01
  3.05781901e-01  1.45175681e-02  4.71461475e-01  1.10571511e-01
  2.14520097e-01  2.13416502e-01  3.17246765e-01 -2.43114486e-01
  5.32935023e-01  2.72724062e-01  1.62956249e-02  4.10821915e-01
  2.61153162e-01  2.07613587e-01 -3.09281319e-01  5.97534060e-01
  1.30122676e-01 -2.27416366e-01  8.60185996e-02 -4.99965757e-01
  4.64314103e-01 -3.56012881e-01  4.00513113e-02 -1.28759652e-01
  3.12565804e-01  5.10280132e-02 -1.82718486e-01  1.63045764e-01
 -2.58911084e-02  2.19737932e-01 -2.97678113e-01  2.21501112e-01
 -1.17214784e-01 -2.15378642e-01 -1.25067309e-01 -5.47692657e-01
 -3.57722521e-01  1.39356121e-01 -4.08678949e-01  3.10839474e-01
  2.68163700e-02 -1.77026652e-02 -3.62559974e-01  2.07781583e-01
  2.43520573e-01 -1.16649970e-01 -5.31402044e-03  1.16388030e-01
 -3.44348490e-01 -5.82956299e-02  5.92684627e-01 -4.68001634e-01
 -3.42662007e-01 -1.26665801e-01  3.72405410e-01  2.23749220e-01
  2.97348678e-01 -4.12385106e-01  1.85671091e-01 -2.70797759e-01
 -1.15662336e-01  3.49629909e-01 -1.82179194e-02  7.94714987e-02
 -1.66781142e-01  1.01069295e+00  1.83582813e-01 -6.35089725e-02
  1.33686423e-01 -1.34072021e-01 -1.02730423e-01  3.56513858e-02
  1.69246808e-01  2.55269334e-02  1.34327874e-01 -4.10971940e-01
  1.91757679e-01  1.88305840e-01 -1.79551214e-01 -1.40247673e-01
 -7.56190196e-02 -4.34278473e-02 -1.87350720e-01  8.28943495e-03
 -1.90207899e-01  3.17562729e-01  1.06062992e-02 -3.04915756e-01
 -6.14794046e-02  4.64965589e-03 -7.68410787e-02 -1.83378547e-01
  1.32482156e-01 -3.41863751e-01  4.07022119e-01  5.53907156e-01
 -3.48995090e-01  9.47965682e-02  1.26871571e-01  1.47878915e-01
 -4.09354478e-01 -1.19476859e-03  6.40574396e-02  3.15650135e-01
  3.20762098e-02 -2.11015731e-01  3.03725272e-01  3.51231694e-01
 -8.03624392e-02  1.45210549e-01 -2.91418791e-01  1.72344312e-01
  3.06283236e-01 -2.33911574e-01 -1.45890191e-01 -5.95880270e-01
  1.70916706e-01  3.07118356e-01 -1.44179583e-01  2.30655208e-01
 -1.18877962e-01  4.39134777e-01  6.01544917e-01 -3.86668175e-01
 -1.75738305e-01  3.18902999e-01  5.12725413e-01  1.06590502e-01
  1.12134062e-01 -2.99088627e-01  1.82541341e-01  3.35485339e-02]"
"When converting tensorflow model to tflite model, is there any way to fix the output order during inference using tflite as Facing an issue of output order of tflite inference on meraki custom cv stat:awaiting response type:support stale comp:lite TF 2.10","I took a pretrained model (SSD MobileNet 320x320) for object detection from the TensorFlow Zoo and configured/tuned it according to my data. I trained a TensorFlow model which detects 2 labels.

I have used the latest checkpoint to save the model, then froze it, and finally performed TF Lite conversion. I did this because I need to upload the TF Lite model only to a Cisco camera. 

I'm facing an issue with the output order during TF Lite inference, as the output arrays get jumbled /rearranged. I need help on how to convert the TensorFlow model to TF Lite efficiently. My TensorFlow version is 2.10",False,"[-3.40580493e-01 -3.51390719e-01 -1.66535169e-01 -6.08836561e-02
  2.57754564e-01  6.41830042e-02  9.00949724e-03 -4.95572723e-02
 -8.21599551e-03 -1.08874381e-01  4.62802239e-02  6.77534044e-02
 -1.20385110e-01  1.82210490e-01 -3.23712945e-01  4.75646257e-02
 -2.72344828e-01 -7.39244968e-02  6.73438609e-02 -4.45747599e-02
  2.59382259e-02  3.67282368e-02 -1.27755612e-01  2.70903885e-01
  6.95660487e-02  1.91841662e-01 -1.08059309e-03 -1.80902742e-02
  1.47621185e-01 -1.01006478e-01 -1.15225144e-01  1.26927808e-01
 -1.30249545e-01  8.06790441e-02 -2.55095005e-01 -8.36608559e-03
 -2.01699033e-01 -9.72883552e-02 -2.29101747e-01  4.23293337e-02
  2.10524276e-01  5.38731515e-02  7.47543350e-02  2.19443627e-02
  9.35664251e-02  1.10212021e-01  2.40081191e-01  8.95442665e-02
 -5.81144467e-02  5.54337054e-02 -6.10488541e-02 -1.38223842e-01
 -2.80590087e-01 -2.15496540e-01 -6.83186203e-02  3.56370091e-01
  4.34888542e-01 -6.43778592e-04 -1.32106379e-01 -5.90784028e-02
 -7.08662346e-03  1.10946700e-01 -1.38269752e-01 -1.60474479e-01
  4.39923733e-01  3.17596972e-01  6.14770688e-03 -2.11376995e-01
  3.60050857e-01  8.38437006e-02  1.08758315e-01 -9.00062472e-02
 -4.02143538e-01 -1.18444771e-01 -2.67370999e-01  1.04464307e-01
 -7.66236614e-03  1.65402159e-01  4.04220492e-01 -7.89454877e-02
  1.13618243e-02 -1.84679314e-01 -1.22279972e-01  3.32977511e-02
  1.85888946e-01 -1.68536544e-01  3.07669193e-01  1.80965029e-02
  5.14719963e-01  2.46804133e-02  4.65981424e-01  1.33214906e-01
 -1.65275708e-01  1.21425942e-01  2.68834203e-01  3.18236709e-01
  3.37393358e-02  2.96082020e-01  3.63279641e-01  1.24732144e-01
 -1.97899807e-02 -2.80821264e-01 -2.30288729e-01  1.01986222e-01
  2.71072567e-01 -1.45742789e-01  1.42119035e-01 -3.89267057e-02
  2.25800335e-01  7.29006305e-02  1.95081830e-01  4.11404483e-03
  2.67369181e-01  2.83964157e-01 -5.35807461e-02 -2.40857154e-02
  2.92360872e-01  3.05743575e-01 -4.89006750e-02  1.15256995e-01
  8.98240134e-04 -3.40710402e-01  2.21009225e-01  8.04546326e-02
  2.11822301e-01  1.23919904e-01 -4.26833719e-01 -6.35848790e-02
  1.25889480e-01 -4.60875779e-02  2.39857092e-01  3.54593098e-02
 -3.10721010e-01 -1.25031143e-01 -1.38791680e-01 -1.16561122e-01
 -1.36036605e-01  1.04493566e-01 -4.13365662e-01 -4.25180458e-02
 -2.35390082e-01 -1.08982846e-01 -3.37855220e-01 -5.48816323e-02
  9.66867581e-02  1.45061299e-01 -1.16380468e-01  1.22530982e-01
 -5.63018471e-02 -5.14614545e-02 -8.21922719e-02 -1.32680655e-01
 -1.61785990e-01  3.52376014e-01  1.90859348e-01  5.30930310e-02
  4.69519198e-02  1.01436779e-01  1.14933848e-01 -1.75526395e-01
 -1.44070312e-01  1.73562497e-01  3.77067700e-02 -1.63333684e-01
  3.43714058e-01  2.88952589e-01 -2.79485643e-01 -5.46068847e-02
  2.45789558e-01  2.51854062e-01  8.80894363e-02 -1.75126165e-01
 -1.49655461e-01 -9.11665112e-02  1.28059089e-01 -1.73861727e-01
  3.06096554e-01 -3.84056956e-01 -5.83379045e-02 -3.47864687e-01
  1.28095776e-01 -2.67256908e-02 -1.88441984e-02 -4.85756546e-02
 -5.43776713e-02 -1.20759003e-01  2.53841698e-01  8.43563974e-02
 -3.65989178e-01 -1.34104908e-01 -2.05420896e-01 -1.00846961e-01
  2.50488698e-01  2.72236794e-01 -2.31500596e-01 -1.99065208e-01
  1.41469911e-01 -1.13321014e-01  1.51013553e-01  8.83312300e-02
 -3.39815840e-02 -9.69735757e-02 -5.88575676e-02 -2.68054247e-01
  7.55589902e-02 -1.60369337e-01  2.65052151e-02 -3.94197285e-01
 -2.15890959e-01  1.14078969e-01  1.86342865e-01 -4.60119128e-01
 -1.63635850e-01 -1.28127679e-01 -2.82233536e-01 -6.47608340e-02
 -1.24421410e-01 -1.17093138e-01 -4.03218299e-01  1.76481679e-01
  1.67172253e-01  9.61282551e-02  6.59713075e-02 -3.05120617e-01
 -4.81450170e-01  3.68801802e-02 -2.96532512e-02  9.24192667e-02
 -1.55267984e-01  1.19471297e-01  5.55766560e-02  2.98147529e-01
  2.52452493e-01 -6.85248747e-02  2.74875522e-01 -1.63220286e-01
 -2.37490371e-01 -2.48906270e-01 -7.70323053e-02 -1.96965635e-02
 -5.98802418e-02 -2.20682383e-01 -1.56318799e-01 -1.03737265e-01
 -4.88038845e-02 -4.54362035e-02 -9.78292227e-02  1.46054700e-01
 -8.55928063e-02  4.44857121e-01 -7.88855702e-02 -5.47733158e-02
  2.31122196e-01  1.13132134e-01  9.56281871e-02  1.69513404e-01
  7.76448697e-02  1.47781402e-01 -1.10582232e-01  3.96915749e-02
  4.28969622e-01  2.67075658e-01  1.28359079e-01  8.23332787e-01
  1.63106650e-01  3.38603556e-01 -2.78927356e-01  2.63263404e-01
 -1.03457026e-01 -1.25582010e-01 -1.09307244e-01 -8.64860192e-02
  3.00517261e-01 -3.03519785e-01  2.07348406e-01 -1.45877495e-01
  3.39828014e-01  1.17207356e-02 -1.86410457e-01  2.47030124e-01
  1.33215487e-01  2.04493165e-01 -3.80186915e-01  1.15513511e-01
  1.50454581e-01 -2.36265026e-02 -1.10100314e-01 -4.21901077e-01
 -2.41845220e-01  2.70229243e-02 -3.67759347e-01  8.21137428e-03
  2.69989699e-01 -1.11047491e-01 -1.95859641e-01  2.26074308e-01
  3.43471467e-01 -5.06238416e-02  2.77857363e-01 -2.18350708e-01
 -2.61016816e-01  1.30457342e-01  2.85492420e-01 -2.26765513e-01
  5.72355203e-02  4.61874716e-03  2.83318549e-01 -4.10016254e-02
  6.55883327e-02 -3.76785174e-04  1.31735966e-01 -7.49476701e-02
  8.35124180e-02  2.63620019e-01 -1.74472928e-01  2.34043021e-02
 -9.23240408e-02  4.17357534e-01  1.28789604e-01 -4.24864404e-02
 -4.01198342e-02 -3.51577759e-01  1.13757253e-01  1.04275867e-01
  1.71790153e-01  1.16283104e-01  6.05539978e-02 -3.67393456e-02
 -1.33061737e-01 -2.08897859e-01 -4.06512260e-01 -1.57370418e-01
 -1.16221160e-01  1.33356228e-01 -5.43478876e-04  3.62392291e-02
  6.62125461e-03  2.00776562e-01 -2.01666579e-01 -2.43322209e-01
  5.97133525e-02 -2.44501144e-01 -2.87818909e-02 -2.69277483e-01
  1.26095921e-01 -1.57307222e-01 -9.20826048e-02  4.56665725e-01
  1.34619057e-01  4.88844886e-02 -4.92000245e-02  7.92478025e-03
 -7.51386583e-02  4.74696346e-02 -1.21692605e-01  3.46608400e-01
 -1.54014722e-01 -1.83701962e-01  1.80799931e-01  4.49321777e-01
 -1.66258782e-01 -5.31809404e-03 -3.20366144e-01 -1.62761331e-01
  8.31997767e-02 -1.23880863e-01 -1.15434229e-01 -2.93430001e-01
  2.83221006e-01  4.91508722e-01 -8.45020115e-02  2.37281725e-01
 -1.98183477e-01  2.22326517e-02  2.62304217e-01 -3.37776423e-01
  3.52610350e-01 -3.50725651e-02  1.20772980e-02 -1.11826546e-01
 -5.11345305e-02 -2.36359417e-01 -3.24992120e-01 -2.06940919e-01]"
On-Device training for LSTM or GRU Model  stat:awaiting response type:feature stale comp:lite comp:lite-examples,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): web
- TensorFlow installed from (source or binary): colab
- TensorFlow version (or github SHA if from source):colab

Hi Im new to tensorflow and Im trying to make LSTM or GRU model to be enable to re-train on-device(Android) with tabular data (mostly customer interaction).

Im referencing these examples
[On-Device Training 1](https://www.tensorflow.org/lite/examples/on_device_training/overview)

This is an example of a CNN, but not able to understand how can I enable on-device training for lstm or gru model.
Are there any examples for reference? Thanks",False,"[-4.92793471e-01 -4.40761149e-01 -2.35617995e-01 -3.16488177e-01
  1.31813481e-01  1.41491413e-01 -7.08418861e-02  3.06409262e-02
 -3.41171592e-01 -1.12742469e-01 -4.14477922e-02  1.93172708e-01
 -4.81986031e-02 -9.69376415e-02 -7.36236945e-02 -1.34376407e-01
  4.22258675e-02  6.19758777e-02  2.95122713e-01 -6.03550225e-02
 -5.79521582e-02 -2.99987078e-01 -2.08492190e-01  1.00007758e-01
  1.85675770e-01  3.50371063e-01  5.09491712e-02 -1.54768914e-01
  7.52173737e-02 -4.10414934e-02  3.92487347e-01  3.51209104e-01
  3.73684056e-02  3.21756378e-02 -2.45411515e-01  2.73678184e-01
 -1.13270357e-02 -2.73226798e-01 -1.26729459e-01  1.77392989e-01
  8.98222998e-02  1.81562304e-01 -2.12557130e-02  8.99533629e-02
  1.58841193e-01  1.83077976e-02 -5.18487059e-02  2.82399356e-02
 -2.67771363e-01 -8.82945359e-02  1.36952162e-01 -5.86399771e-02
 -1.29080564e-01 -2.72356361e-01  4.40473668e-04  9.28634405e-02
  2.80553371e-01  9.77144092e-02  2.41994932e-02  3.50397825e-02
 -3.98925990e-02 -8.94724429e-02 -8.85752141e-02  3.26826274e-02
  3.30317989e-02  1.35617077e-01  1.23505779e-01  2.14293599e-03
  6.90067768e-01 -1.06923759e-01 -1.43506154e-01 -6.93493485e-02
 -2.86418796e-01  6.37013316e-02  3.27572376e-02 -2.88233673e-03
  7.16727525e-02  3.80116552e-01  2.88797557e-01 -1.17435664e-01
  1.34797633e-01 -1.47394389e-01  1.28494918e-01 -1.98280245e-01
  1.56045809e-01 -6.07243702e-02  7.39727840e-02  2.33036637e-01
  8.28302428e-02 -7.28842616e-03  2.09319577e-01  3.28427345e-01
 -1.29886746e-01  5.72598726e-02  2.32741654e-01  2.60939777e-01
 -1.30897045e-01 -2.92087477e-02  9.80640724e-02  8.70612860e-02
 -7.46954381e-02 -2.53155947e-01  5.28647155e-02  3.91777277e-01
 -1.24241784e-01 -4.53530729e-01  3.07090972e-02  1.35476440e-01
  6.69679195e-02 -1.81285679e-01  1.47122294e-01  3.15948799e-02
  7.43193924e-02  3.38039361e-02  1.49945438e-01 -1.54333115e-01
 -1.44128591e-01  2.67987430e-01 -8.40245485e-02  4.51029480e-01
 -1.19842082e-01 -2.35185325e-01  2.09129691e-01 -3.37086841e-02
  3.51713121e-01 -1.08596787e-01 -1.64518863e-01 -4.10160124e-02
 -8.08626637e-02  8.90545771e-02  1.60467163e-01  1.36217140e-02
 -3.79916728e-01 -7.30061084e-02 -3.57842334e-02  1.29523665e-01
  1.10209454e-02 -1.35890469e-01 -2.05758110e-01  1.29406273e-01
 -2.19785988e-01  1.99651569e-01  1.53247952e-01 -7.84204379e-02
 -9.61109623e-02 -5.70658483e-02 -1.15728319e-01 -1.01599246e-01
  9.77023169e-02 -5.84636182e-02 -5.72096705e-02 -2.58050829e-01
  2.20944032e-01  1.80439398e-01  3.34815443e-01  2.41209686e-01
  2.82037973e-01 -1.83920953e-02 -9.49604064e-02 -3.08384150e-01
 -3.46095674e-03  2.27933928e-01 -1.29731163e-01 -2.61803687e-01
  6.23530000e-02  9.80231240e-02 -9.40347314e-02 -2.12388009e-01
  1.11651450e-01  1.65386513e-01 -6.01143204e-03 -1.35943135e-02
 -7.10244104e-02  9.81706232e-02  1.48818240e-01 -8.28286111e-02
  2.95564294e-01 -4.92699802e-01 -1.43879831e-01  1.35591984e-01
  4.82845306e-03 -1.64586216e-01  8.03822279e-02  1.48287535e-01
  1.70657225e-02  2.30051413e-01  1.74700901e-01 -4.20478284e-02
 -2.51600742e-01 -3.17544565e-02 -8.75012428e-02  6.28882274e-02
  1.19180843e-01 -1.25789553e-01 -2.26607502e-01 -2.55092025e-01
  2.12071449e-01  1.16144821e-01 -2.78595328e-01  5.43904528e-02
 -2.29559124e-01 -6.52533025e-02 -1.70691498e-02 -1.12213627e-01
  2.28501156e-01 -2.69389711e-03 -5.11828005e-01 -2.11448725e-02
 -2.01866031e-01  1.27527118e-01  1.43574953e-01 -1.50159001e-01
  4.12066318e-02  1.10885397e-01 -3.20408404e-01 -3.38724166e-01
 -4.11320291e-02  1.60806343e-01 -2.93506742e-01  3.07432771e-01
 -8.70686397e-02  3.96108516e-02 -1.55426562e-02 -2.63087451e-01
 -3.94525617e-01  6.69594482e-02 -8.47063810e-02  1.61037624e-01
  2.05232739e-01  2.99851179e-01  5.28330449e-04  7.06565082e-02
  2.09571883e-01  1.76989943e-01  7.20570907e-02 -3.46501231e-01
 -2.37118825e-02 -1.36575311e-01 -1.07614376e-01 -4.73237708e-02
 -2.39248842e-01 -1.97652861e-01 -8.89850482e-02  8.17250907e-02
  4.79697101e-02  2.44802400e-01 -1.92627117e-01 -6.20417967e-02
  3.58234011e-02  2.44932741e-01 -1.12902537e-01 -1.75532907e-01
  1.63632855e-01  2.37794414e-01  3.42754513e-01 -1.77759141e-01
 -1.39491767e-01 -1.30223155e-01  2.41865873e-01  1.72043413e-01
  1.76564679e-01  5.05408883e-01 -1.81954026e-01  4.07729566e-01
  4.87527132e-01  3.04017961e-01 -2.85547018e-01  7.77686536e-02
  4.98978654e-03 -2.60772228e-01 -7.57508865e-03 -2.14029714e-01
 -1.36585198e-02 -7.30878115e-02  4.18069959e-02 -1.07447259e-01
  2.38222420e-01 -4.03836258e-02 -5.35656512e-02 -1.02412522e-01
  1.73797458e-01  1.11390397e-01 -4.04405862e-01 -9.12030786e-02
  2.48464257e-01 -3.59891593e-01  6.45503178e-02 -4.81407523e-01
 -2.36120760e-01 -1.52348861e-01 -2.88848355e-02  2.45761171e-01
  7.58012980e-02  1.65321082e-01 -2.09313899e-01  8.73868242e-02
  3.44547361e-01  2.81615615e-01  2.04786271e-01 -8.16368833e-02
 -2.99094915e-01 -1.19001508e-01  9.54138786e-02 -1.68843493e-01
 -1.32165432e-01  3.22887786e-02  2.01071978e-01  1.97871197e-02
  4.96757448e-01 -9.87961739e-02 -4.23213691e-02  1.89434052e-01
  1.67850316e-01  1.00075588e-01 -1.13876596e-01 -2.81735137e-03
 -1.62689835e-01  4.82753932e-01 -1.64620787e-01 -2.16450006e-01
  4.79912907e-02  3.04193757e-02  7.46014714e-03 -5.38638160e-02
  8.48249048e-02  4.48411629e-02 -3.63765299e-01 -3.15250218e-01
 -1.62935317e-01  8.82669687e-02 -1.27690196e-01 -6.40764832e-02
 -1.06736317e-01  8.58132020e-02  2.22891122e-02  5.43742366e-02
 -2.36677259e-01  3.19467485e-01  4.55447584e-02 -1.68503255e-01
  1.67403705e-02 -6.77158684e-03 -3.71772051e-02 -1.12837758e-02
  8.33339393e-02 -9.44202840e-02  1.83732957e-01  3.97857606e-01
  1.53148115e-01  1.34210065e-01 -1.26106039e-01  8.14011991e-02
 -6.90945089e-02 -9.27305073e-02 -2.37253252e-02  3.13031942e-01
  5.98168299e-02  6.34671673e-02  2.64694005e-01  6.24080420e-01
 -4.61680219e-02 -2.60218866e-02 -2.18521446e-01 -1.40982360e-01
 -1.01075307e-01 -1.30191013e-01  3.05777229e-03 -3.93015623e-01
 -6.64172769e-02  4.64455456e-01 -1.15517136e-02  1.43809825e-01
 -3.63819838e-01  5.12887239e-02  4.78361517e-01 -1.45369023e-02
 -1.33132890e-01 -1.21845208e-01 -7.93607160e-02 -1.88940857e-02
 -3.09358120e-01 -2.01386392e-01  5.81073835e-02 -9.01351590e-03]"
TensorFlow profiler running into OOM issue on GPU stat:awaiting response type:support stale comp:gpu TF 2.11,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.11.0.5

### Custom code

No

### OS platform and distribution

Linux CentOS 7.9.2009

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Running TensorFlow profiler for longer than 10 second period crashes the inference process because of OOM error and the profiler returns DEADLINE_EXCEEDED. Is there anyway to limit the sampling rate or way to reduce the amount of information being collected to avoid crashing the process?

### Standalone code to reproduce the issue

```shell
`tensorflow_profiler.experimental.client(""grpc://localhost:3222"", ""profiles"", 30000)`
```


### Relevant log output

_No response_",False,"[-0.411155   -0.27096313 -0.08235398  0.2144331   0.24946818 -0.5106873
 -0.2939049   0.06054778 -0.13953196 -0.4208738  -0.01829657 -0.0249689
 -0.19287468  0.13550484 -0.18980473  0.35389906 -0.13556983  0.03144081
  0.2774019   0.15251148 -0.20371827 -0.16915299 -0.11328804  0.00181728
  0.15068829  0.27034065 -0.28516203 -0.04437225 -0.1434272   0.22275583
  0.5593022   0.1302014  -0.01256843  0.10803382 -0.09122347  0.26268554
 -0.29165375 -0.35331008 -0.4143649   0.09541995  0.15677316  0.20772578
 -0.03174724  0.04533629 -0.03241744 -0.31835234  0.14559859 -0.25884098
 -0.14655215 -0.18895335 -0.18363401 -0.15019834 -0.5475188  -0.2027172
 -0.20640205 -0.06518824  0.19461651 -0.09005417 -0.06158931  0.18726759
 -0.08648007  0.00736352  0.22386932 -0.10210694  0.15080799  0.1173051
  0.23433839 -0.19094464  0.4873177  -0.06151361 -0.00206267 -0.12500352
 -0.3581668  -0.1004937   0.11281029  0.29579586  0.06302889  0.14264789
  0.3661448  -0.09862928  0.06602668 -0.28094965 -0.06666199 -0.24969256
  0.16123578  0.03207344  0.41381973  0.09637234  0.43885884 -0.28075254
  0.34183437  0.3632205   0.07776541 -0.05004643  0.6704348   0.12556621
  0.03250054  0.19866763 -0.03156887 -0.29763252 -0.13691193 -0.11972172
  0.1277549   0.2943371  -0.17327714 -0.21035893  0.22043061  0.04410508
  0.02236151 -0.09882218  0.14788905  0.01817532  0.2888162  -0.1230827
  0.00744415 -0.00683157 -0.3331961   0.01766009 -0.114272    0.9540789
 -0.02423318 -0.10200134  0.061108    0.03008195  0.39306068  0.08534507
 -0.16543356 -0.13280636  0.18293999 -0.12504907  0.23523086 -0.00959758
  0.13062029  0.28673473 -0.07627067  0.07974279 -0.12114219 -0.22140685
 -0.30647922 -0.17911777 -0.20141663  0.15645279 -0.23963289 -0.55271155
  0.18207191  0.37001753 -0.39529848  0.2939996  -0.17448808  0.17133395
 -0.24235338  0.18483531 -0.10658251  0.51483756  0.2076137   0.26417068
  0.24349429 -0.23247994 -0.07546761 -0.6932793   0.0328776   0.4052426
 -0.10662815 -0.14340451 -0.02389028  0.2695851  -0.38563043 -0.28941232
 -0.07707804  0.41946518 -0.09408619 -0.06396171 -0.0755549   0.04035753
  0.18020993 -0.10543711  0.34412873 -0.5908764  -0.03254969  0.2590236
 -0.13859785  0.2097187   0.09747492 -0.00583828 -0.16288163  0.08031472
 -0.044064    0.12134298 -0.2163885   0.06805769 -0.3627858  -0.19982098
  0.49183    -0.1741367  -0.11185108  0.00152858  0.18436643  0.25878987
 -0.08096512  0.31079477 -0.13502228 -0.2536911  -0.03581665  0.00969249
  0.13711679 -0.35063905 -0.2517692  -0.20910126 -0.38439122  0.00948844
  0.23091055 -0.56039405 -0.02705067  0.02037835 -0.3753357   0.22160977
  0.11612407  0.23915812 -0.1190118   0.09933156  0.13849784 -0.22571826
 -0.1891157  -0.36182952 -0.3275146   0.26787913 -0.10062316  0.14105853
 -0.15079345  0.35243976  0.05516693  0.33166236  0.48856068  0.13918753
  0.4544162  -0.01146155 -0.02579611 -0.15911455 -0.19916186  0.11519283
 -0.385691   -0.30053464 -0.14319655 -0.0564018   0.24040811  0.32893103
 -0.2221042  -0.23976433 -0.2779447   0.26723808  0.02927838  0.0470335
  0.35501456  0.18570346  0.47349024  0.29535413 -0.00813095  0.15969306
  0.2585408  -0.11879407  0.39243427  0.21537493  0.06569404  0.5255345
  0.39282483  0.2859399  -0.4094259   0.5409802   0.04960236 -0.09029035
 -0.01988815 -0.36078176  0.6424809  -0.62912464  0.07877594  0.14363694
  0.38555124  0.06653516 -0.03306802  0.10982672  0.11265685  0.27565753
 -0.22009182 -0.12602574  0.19063933 -0.20340824 -0.01753958 -0.6220581
 -0.26890868  0.19745208 -0.13334733  0.08447015  0.07501476  0.15560742
 -0.22863185 -0.02086339  0.04819981  0.10065772  0.16390732  0.1982902
 -0.26658136 -0.24735473  0.23562264 -0.46818614 -0.2851987  -0.1314026
  0.1613145   0.18428242  0.35200346 -0.45412672  0.08900519 -0.05258167
 -0.1096799   0.35587776 -0.05266614  0.05669915 -0.4244764   0.84239966
  0.2172856  -0.07259004  0.12778403 -0.3525838  -0.23632275  0.2667675
  0.33682758 -0.18154189 -0.09142646 -0.4545318  -0.09805658  0.28315812
 -0.00996714 -0.07180543 -0.15143739 -0.05496494 -0.05355746 -0.07018075
 -0.35519087  0.23493604  0.05957884 -0.41051394 -0.2990631  -0.10801975
 -0.08624717 -0.25501373  0.06766958 -0.295041    0.5071039   0.5472635
 -0.18634763  0.06603496  0.09610562  0.26523978 -0.33222666 -0.13493769
 -0.18453144  0.4407259   0.07171279 -0.20094359  0.39232156  0.3099132
 -0.08697141  0.12884194 -0.13816968  0.11257674  0.04174453 -0.24709195
 -0.11257884 -0.25967956  0.29578334  0.34294122 -0.03257803  0.14771804
 -0.2677201   0.25341508  0.32660735 -0.48829478 -0.26997256 -0.1231738
  0.00689819 -0.18123779 -0.09135976 -0.05800785  0.2231057  -0.08103876]"
Tensorflow 1.15 for Raspberry pi build fail stat:awaiting response type:build/install stale comp:lite subtype: ubuntu/linux TF 1.15,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 1.15

### Custom code

No

### OS platform and distribution

Linux and Ubuntu 18.05

### Mobile device

target platform: Raspberry pi 4

### Python version

3.7

### Bazel version

na

### GCC/compiler version

na

### CUDA/cuDNN version

na

### GPU model and memory

na

### Current behavior?

Hi

I am trying to install tensorflow 1.15 on Raspberry pi and found this page:
https://github.com/tensorflow/build/tree/master/raspberry_pi_builds

From my understanding, I can do this at another platform (GPU server, ubuntu 18.05 installed).
I followed the instruction in the page, but came across the following error message:
From my guess, it looks like pip is not recognized in Docker.
Could you tell me how to resolve this issue??

### Standalone code to reproduce the issue

```shell
# I followed the instruction here: https://github.com/tensorflow/build/tree/master/raspberry_pi_builds
# And use this command to build

tensorflow/tools/ci_build/ci_build.sh PI-PYTHON37 \
    tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
```


### Relevant log output

```shell
CI_DOCKER_BUILD_EXTRA_PARAMS: 
CI_DOCKER_EXTRA_PARAMS: 
COMMAND: tensorflow/tools/ci_build/pi/build_raspberry_pi.sh
CI_COMMAND_PREFIX: ./tensorflow/tools/ci_build/builds/with_the_same_user ./tensorflow/tools/ci_build/builds/configured pi-python37
CONTAINER_TYPE: pi-python37
BUILD_TAG: tf_ci
  (docker container name will be tf_ci.pi-python37)

Building container (tf_ci.pi-python37)...
[+] Building 2.9s (10/17)                                                                                                                         
 => [internal] load build definition from Dockerfile.pi-python37                                                                             0.0s
 => => transferring dockerfile: 866B                                                                                                         0.0s
 => [internal] load .dockerignore                                                                                                            0.0s
 => => transferring context: 2B                                                                                                              0.0s
 => [internal] load metadata for docker.io/library/ubuntu:16.04                                                                              1.4s
 => [internal] load build context                                                                                                            0.0s
 => => transferring context: 22.45kB                                                                                                         0.0s
 => [ 1/13] FROM docker.io/library/ubuntu:16.04@sha256:1f1a2d56de1d604801a9671f301190704c25d604a416f59e03c04f5c6ffee0d6                      0.0s
 => CACHED [ 2/13] COPY install/*.sh /install/                                                                                               0.0s
 => CACHED [ 3/13] RUN /install/install_bootstrap_deb_packages.sh                                                                            0.0s
 => CACHED [ 4/13] RUN add-apt-repository -y ppa:openjdk-r/ppa &&     add-apt-repository -y ppa:george-edison55/cmake-3.x                    0.0s
 => CACHED [ 5/13] RUN /install/install_deb_packages.sh                                                                                      0.0s
 => ERROR [ 6/13] RUN /install/install_pip_packages.sh                                                                                       1.4s
------                                                                                                                                            
 > [ 6/13] RUN /install/install_pip_packages.sh:                                                                                                  
#0 0.789 Searching for pip                                                                                                                        
#0 0.789 Reading https://pypi.python.org/simple/pip/                                                                                              
#0 0.867 Couldn't find index page for 'pip' (maybe misspelled?)                                                                                   
#0 0.867 Scanning index of all packages (this may take a while)                                                                                   
#0 0.867 Reading https://pypi.python.org/simple/
#0 0.936 No local packages or download links found for pip
#0 0.937 error: Could not find suitable distribution for Requirement.parse('pip')
------
Dockerfile.pi-python37:11
--------------------
   9 |         add-apt-repository -y ppa:george-edison55/cmake-3.x
  10 |     RUN /install/install_deb_packages.sh
  11 | >>> RUN /install/install_pip_packages.sh
  12 |     RUN /install/install_bazel.sh
  13 |     RUN /install/install_proto3.sh
--------------------
ERROR: failed to solve: process ""/bin/sh -c /install/install_pip_packages.sh"" did not complete successfully: exit code: 1
ERROR: docker build failed. Dockerfile is at /home/keondopark/tensorflow/tensorflow/tools/ci_build/Dockerfile.pi-python37
```
",False,"[-0.6917887  -0.49452057  0.08950973 -0.03072072  0.10051185 -0.34839377
 -0.24560216  0.0874418  -0.36944014 -0.19489442  0.03837972  0.14737308
 -0.07966195  0.10655021 -0.04923572  0.21636209 -0.30074602 -0.20159814
  0.49170798  0.11665404 -0.1064069   0.05194475 -0.33111572  0.02971344
  0.10030401  0.33330947 -0.14688829 -0.04435155  0.0242847   0.07703455
  0.52883476  0.22079325 -0.05109742  0.08038676  0.02331579  0.235072
 -0.31726772 -0.19302423  0.01258956  0.09587155  0.15504423  0.01605383
  0.28983903 -0.07418592 -0.03726769 -0.14616154  0.0039173  -0.12566361
 -0.04710755 -0.28130817 -0.01311676 -0.01072271 -0.28114015 -0.20245144
 -0.19399828 -0.01886728  0.20170404  0.11038597 -0.03847058  0.0987882
  0.01159598  0.01340291  0.16385272 -0.24561353  0.10388622  0.2215971
  0.24169539 -0.07354516  0.59755313  0.00443419  0.02168011 -0.10575265
 -0.3352285  -0.06011395 -0.179299    0.26618633 -0.07257797  0.16795376
  0.31715542 -0.02751498 -0.19745299 -0.19648463  0.10463865 -0.19268282
  0.21359457 -0.0713439   0.3219882   0.28846496  0.42436486 -0.1431318
  0.34983402  0.6316569  -0.00863992  0.0920487   0.37005594  0.08637313
 -0.02013552  0.11466673 -0.00268773 -0.2090245  -0.19711085 -0.2833685
  0.01715791  0.04704304 -0.07931171 -0.21735331  0.2144644  -0.05230935
  0.00640311  0.00588464  0.26487082 -0.06744786  0.12303435  0.04822009
 -0.11598338 -0.1493553  -0.21952245 -0.00690364 -0.01810987  0.65318763
 -0.01872181 -0.10184313  0.07501024  0.25496024  0.24395736 -0.02612599
 -0.07310515 -0.21286178  0.06210495  0.25345188 -0.06307615  0.21535976
 -0.12202519  0.29739144 -0.01753417  0.21016455 -0.12304382 -0.17562506
 -0.23288362 -0.03020622 -0.12926486  0.1277287  -0.22778581 -0.54653215
  0.16216369  0.00681926 -0.0993406   0.11167052 -0.16856472  0.12812743
  0.03096185  0.02102834 -0.19471535  0.5377507   0.0740078   0.09935465
  0.23679632 -0.14437589  0.16472605 -0.61513126 -0.00671578  0.4390251
 -0.03735821 -0.19451477  0.20478734  0.08201669 -0.4784887  -0.22524533
 -0.03246512  0.34753212 -0.15494457 -0.03736785  0.2299325   0.15452525
  0.14044845 -0.1632514   0.3144697  -0.49578345 -0.11063376  0.23576918
  0.14767405  0.29919776  0.15743032  0.05280838  0.10068034  0.10984758
  0.1535764   0.16957918 -0.185571   -0.06150606 -0.53830135 -0.1436897
  0.26891404 -0.19536531 -0.15609556  0.14221567  0.25834006  0.06943518
  0.22238624  0.03943983 -0.11975181  0.01352885 -0.18903531  0.02944402
  0.10976983 -0.31853992 -0.30471557 -0.28989798 -0.4117328  -0.08877277
 -0.0198328  -0.5751715  -0.02664721  0.02783833 -0.17612454  0.21548961
  0.04383881 -0.06715844 -0.27711403  0.19210674  0.09002417 -0.15947932
  0.07104822 -0.30502182 -0.24715853  0.06582689 -0.16794947  0.12163902
  0.18145145  0.18171732  0.24177511 -0.01816482  0.3960114   0.13695024
  0.14429921 -0.12039711 -0.11086765 -0.19949928 -0.22522444  0.14243713
 -0.24042511 -0.1700199  -0.10415272 -0.00519304  0.13032898  0.4502418
 -0.16986477  0.08304009 -0.2912714   0.36789584 -0.05442709 -0.19717288
  0.18392856  0.3059737   0.47936893  0.09222005  0.03893786  0.26501855
  0.19444335 -0.14276752  0.37036648  0.24312277  0.04035032  0.27901492
  0.31721687  0.12152534 -0.42194954  0.3935207  -0.00840494 -0.15816844
  0.13037142 -0.18021688  0.42232227 -0.31640902 -0.03945978  0.03414874
  0.23784891 -0.0402297  -0.09276842 -0.16854684  0.00276356  0.38911286
 -0.50592697 -0.00343554  0.09306933 -0.41781712 -0.09264632 -0.5709796
 -0.2038601   0.04773156 -0.20004566  0.16489217 -0.05494395  0.2824336
 -0.24933201 -0.06370424  0.20640118 -0.12867568  0.11920474  0.20721261
 -0.22236128  0.04620425  0.4153309  -0.45795357 -0.14791524 -0.08829996
  0.12025402  0.14314413  0.58024675 -0.26026127  0.10404459 -0.04802537
  0.07152657  0.45006204 -0.14554532 -0.0690314  -0.43589985  0.81300056
  0.26046544 -0.2475522   0.31323773 -0.21508795 -0.17176369  0.04644915
  0.20354238  0.00649984 -0.0365664  -0.3429531  -0.1134029   0.30287942
 -0.09138472 -0.19907603 -0.02967519  0.15539697 -0.06450352  0.00544827
 -0.30601895  0.24509056 -0.08287317 -0.324587   -0.20846269 -0.09334449
 -0.03993344 -0.34620523 -0.1467826  -0.26933706  0.5639589   0.15129343
 -0.46646184 -0.06320704 -0.1297185   0.15195957 -0.32630903 -0.07647356
 -0.08217672  0.38247102  0.10492821 -0.11436155  0.47787154  0.34282964
 -0.2616982   0.2335215  -0.37172234 -0.1544285   0.06243009 -0.12707037
 -0.07597789 -0.1489614   0.03690534  0.5659437  -0.08092515  0.1793281
 -0.28531858  0.2138769   0.52732754 -0.44817686 -0.33282197  0.2776632
  0.07140403 -0.03804816 -0.03490589  0.0379165   0.21053965 -0.15309995]"
tensorflow lite cmake compilation failed to allocate memory stat:awaiting response type:build/install stale comp:lite wsl2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

cloned the master branch

### Custom code

Yes

### OS platform and distribution

windows 11 wsl 2 with Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.3.1

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

Cuda compilation tools, release 10.1, V10.1.243

### GPU model and memory

rtx 2060 6GB dedicated 

### Current behavior?

I am using cmake 3.22.2
I cloned tensorflow into a directory tensorflow_src, then I ran ./configure and set ROCm and CUDA support to none, because I am building tflite. Then I made and moved into a build directory. I run `cmake ../tensorflow_src/tensorflow/lite` and `cmake --build . -j` I get quite a lot of errors, the first is ""failed to allocate memory"". But there's not enough space to paste the entire log here. 

I also tried `cmake ../tensorflow_src/tensorflow/lite/examples/minimal` and `cmake --build . -j` and the error I get is that 

I am trying to instantiate the tflite interpreter object in the easiest possible way.

### Standalone code to reproduce the issue

I also tried bazel build in the tensorflow_sec directory, which seems like it executed fine. then I used `g++ -std=c++17 inference.cpp model.cc -Ltensorflow_src/bazel-bin/tensorflow/lite -ltensorflowlite -o inference -Itensorflow_src/tensorflow/lite`

```shell
In file included from inference.cpp:6:
tensorflow_src/tensorflow/lite/interpreter.h:21:10: fatal error: tensorflow/lite/core/interpreter.h: No such file or directory
   21 | #include ""tensorflow/lite/core/interpreter.h""
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.
```

I am following exactly the instructions on tensorflolw website to set up tflite for C++, it seems like there is missing information

### Relevant log output

_No response_",False,"[-3.53510201e-01 -3.92663747e-01 -1.82779267e-01  2.07058340e-03
  1.61495760e-01 -2.66416401e-01 -2.63414025e-01  9.55620930e-02
 -2.44082451e-01 -4.11668152e-01 -5.76090328e-02 -1.26173168e-01
 -1.67149514e-01 -3.42293233e-02 -1.86521187e-02  3.22182238e-01
 -2.70505011e-01 -3.88099998e-02  1.83058366e-01  7.28546083e-02
 -2.14265436e-01 -2.81879399e-03 -2.86943495e-01  1.07392311e-01
  2.38030776e-01  3.73915255e-01 -1.98555470e-01 -1.06692582e-01
  6.69930503e-03  1.71584547e-01  5.34166396e-01 -1.20282471e-02
 -8.20016637e-02  1.86012983e-01  2.42616050e-02  2.48864055e-01
 -3.69384706e-01 -2.21960902e-01 -8.50244239e-02 -4.30005565e-02
  1.34947315e-01  2.45899469e-01  1.93957239e-01 -8.46580565e-02
  6.95923120e-02  2.81615183e-04 -1.42573357e-01  2.76052323e-03
 -9.03999060e-02 -2.92053550e-01  1.30598634e-01  1.51915951e-02
 -2.51039475e-01 -1.71896398e-01 -1.39590770e-01  2.19203055e-01
 -4.04159427e-02 -1.26364350e-01  9.67966858e-03  2.84143388e-01
  8.90230089e-02  2.33795755e-02  7.66436309e-02 -2.05635965e-01
 -9.07921121e-02  2.47801185e-01  4.43092823e-01  5.70446104e-02
  5.89456081e-01 -2.14821815e-01  8.87843519e-02 -2.89231613e-02
 -3.82854849e-01 -7.42062628e-02  1.58771574e-01  1.05549276e-01
 -2.24417344e-01  1.74183220e-01  1.54058367e-01 -1.80087090e-01
 -6.98826909e-02 -2.85950035e-01 -3.80448513e-02 -2.46878594e-01
  1.87900901e-01 -1.86850652e-01  3.48208427e-01  5.47289774e-02
  2.97009945e-01 -1.20424747e-01  3.66169035e-01  4.69615102e-01
 -2.94868797e-02  1.33188024e-01  3.80239666e-01  2.05714926e-01
  1.51524574e-01  1.64041340e-01 -2.67854892e-03 -1.69884086e-01
 -1.61536962e-01 -1.13331191e-01  7.90063813e-02  1.16935149e-01
 -2.02600777e-01 -4.58578795e-01  3.73358190e-01  1.93032950e-01
 -6.00904338e-02 -2.72495508e-01  2.37454116e-01 -3.93050164e-02
  2.51852185e-01 -7.00219646e-02  1.05325520e-01 -1.94271848e-01
 -8.13690647e-02  6.46287203e-02 -5.14502153e-02  6.37233436e-01
  7.47170821e-02  2.31352597e-02 -9.41784009e-02  2.16037124e-01
  2.61262834e-01  4.50501218e-03 -7.56326765e-02 -3.51418853e-02
  2.15268210e-01 -9.92021263e-02 -2.02457324e-01  2.02232063e-01
  1.32211437e-02  2.75289685e-01 -1.41783729e-02  1.68311208e-01
 -6.76092058e-02 -2.56902069e-01  8.71015117e-02 -3.66683066e-01
 -2.37346485e-01  1.22453824e-01  9.24796611e-03 -6.16896391e-01
 -1.11423403e-01  2.17632890e-01 -1.00063927e-01  1.91993356e-01
 -2.38670617e-01  1.87645182e-01 -5.57869226e-02  1.52748346e-01
 -3.65621090e-01  5.50739050e-01  2.29698241e-01 -7.27277026e-02
  4.89858270e-01 -1.07148523e-02  1.62200242e-01 -3.86765510e-01
  2.25754213e-02  5.37507057e-01 -3.20630781e-02 -6.67790622e-02
  1.20886832e-01  4.57881242e-02 -3.83248895e-01 -1.20519519e-01
  5.33592291e-02  3.74572814e-01 -1.84125781e-01 -1.68860525e-01
  3.45442817e-02 -7.96193182e-02  8.46614391e-02 -1.15135841e-01
  5.73137760e-01 -6.18386388e-01  3.36555019e-02  2.41517842e-01
 -1.03573367e-01  3.25890005e-01  1.54234871e-01  9.97902155e-02
  4.79395054e-02  2.25351304e-01  1.67503312e-01  2.46129572e-01
 -1.18866198e-01 -9.23919231e-02 -6.00834250e-01 -7.88968056e-02
  4.29042995e-01 -1.58360302e-01 -3.86846438e-02  5.37788793e-02
  1.46301925e-01  2.23672204e-02 -3.44059840e-02 -7.89055973e-02
 -1.29180118e-01  8.23275372e-02 -1.04048662e-01  6.13041259e-02
  3.01439762e-01 -2.27259487e-01 -3.44158411e-01 -2.96083540e-01
 -2.22073048e-01 -1.14515737e-01 -3.55690345e-02 -3.73293817e-01
  8.48471820e-02 -3.61052901e-03 -3.03513050e-01  2.70030200e-01
  8.77813473e-02  1.12194180e-01 -1.76994458e-01  2.14802176e-01
  4.06979136e-02 -2.42777824e-01 -1.14427194e-01 -4.39254999e-01
 -1.48524910e-01  5.95500320e-02 -3.05638611e-01  1.05547488e-01
  4.47251722e-02  1.70745134e-01 -4.35401872e-02  1.25987947e-01
  2.73184747e-01  3.94143537e-02  3.71366858e-01 -7.55971819e-02
 -1.64605156e-01 -5.91334179e-02 -3.74662429e-01  5.58695793e-02
 -2.86653370e-01 -3.70712936e-01 -1.63109582e-02  3.16418521e-03
  2.92886585e-01  4.76989895e-01 -1.55566722e-01 -6.00671470e-02
 -1.39486402e-01  3.79829198e-01 -1.80100352e-01  3.12216938e-01
  4.64412034e-01  4.05535847e-02  7.72043228e-01  2.31697589e-01
  1.31540567e-01  1.83426544e-01  3.50850001e-02 -2.14793608e-01
  4.10798371e-01  4.09228981e-01  8.69352221e-02  3.17260623e-01
  2.12401479e-01  1.94068775e-01 -4.00003552e-01  3.18507850e-01
  2.17729598e-01 -7.01279566e-02  1.07481688e-01 -2.45781839e-01
  4.62259293e-01 -1.20996520e-01  5.83731234e-02 -9.14078206e-02
  1.91667274e-01 -2.11293414e-01 -1.80235565e-01 -1.33965332e-02
  8.50298107e-02  2.08497271e-01 -1.14375085e-01 -2.74565697e-01
  8.55065137e-02 -2.73364931e-02  1.83457196e-01 -6.86070085e-01
 -2.76553184e-01  3.31113428e-01 -2.35844657e-01  7.10042268e-02
 -5.05733900e-02  2.07026064e-01 -2.77807236e-01  2.37717032e-02
 -3.17139514e-02 -2.15012640e-01  2.69505173e-01  3.19570422e-01
 -3.13041955e-01  1.81730725e-02  3.52021098e-01 -4.18847620e-01
 -1.94877997e-01  5.67110069e-02  3.54489326e-01 -6.96337819e-02
  4.86616760e-01 -5.43430567e-01  8.75826329e-02 -5.70597798e-02
 -1.57395169e-01  3.37960601e-01 -9.33770090e-02 -1.85196087e-01
 -4.44701731e-01  6.73202097e-01  2.36260012e-01 -2.07086846e-01
  2.61714578e-01  3.59704792e-02 -3.97694528e-01  8.09624940e-02
  2.65473366e-01 -2.04680152e-02  3.60258967e-02 -2.84563869e-01
 -7.56888241e-02  1.14416212e-01 -1.84218764e-01 -1.66817367e-01
 -2.46328533e-01 -2.59134285e-02 -2.70386219e-01 -2.32308686e-01
 -4.05088842e-01  1.26035348e-01 -1.13571003e-01 -5.33524692e-01
 -1.07911177e-01 -1.40213639e-01  1.43393904e-01 -2.94452488e-01
  1.84512045e-02 -1.63938358e-01  3.25727820e-01  4.72313046e-01
 -1.93145573e-01 -6.37488514e-02  7.33826589e-03  3.23789828e-02
 -1.15648881e-01 -7.92002678e-02 -7.97413141e-02  2.82967776e-01
 -1.66034341e-01 -9.17182937e-02  5.45143306e-01  2.90376455e-01
 -2.54801333e-01  1.15880966e-01 -3.73821527e-01  8.87595564e-02
  1.59066677e-01 -1.83445007e-01 -2.99923122e-01 -8.59460235e-02
  1.85558438e-01  4.66581881e-01 -1.47424579e-01  1.66805193e-01
 -3.18083793e-01  2.93730140e-01  4.01899219e-01 -2.00965598e-01
 -1.67061463e-01  7.65350461e-02  1.62890386e-02 -3.01775873e-01
 -6.28030896e-02 -6.04284033e-02  1.21853709e-01  1.08041773e-02]"
[Feature] The Heaviside step function as a activation function stat:awaiting response type:feature stale comp:keras,Some of the implementations like Single Layer Perceptron needs discrete outputs like 0 or 1. Adding this could make the model building ease.,False,"[-3.54364336e-01  2.32894704e-01 -2.03659192e-01 -3.66180032e-01
 -3.86752337e-01 -5.67314364e-02  1.76254436e-01  5.66111878e-02
 -1.49320299e-02  5.52983172e-02  2.61452556e-01 -1.26853287e-02
  1.68624505e-01  5.39288449e-04 -1.95161812e-02 -1.10639013e-01
 -1.95155829e-01  2.81057894e-01  1.35233831e-02 -5.93504496e-02
  1.45615101e-01  4.02004905e-02 -6.31723478e-02 -1.37231737e-01
 -8.27250555e-02 -2.31125951e-01 -4.88783047e-02 -1.51516989e-01
  3.34466457e-01 -2.45563462e-01  7.29782209e-02  2.77305394e-01
  1.06597595e-01 -5.63220568e-02 -2.54637033e-01 -5.33103272e-02
 -3.08351249e-01  1.01335421e-01 -1.85871020e-01  9.80140939e-02
  2.03051511e-02  6.46224292e-03 -1.42537102e-01  1.96253713e-02
  1.66838884e-01 -1.19333819e-03  1.97846100e-01  1.64443418e-01
 -7.01745376e-02 -3.58889140e-02 -3.61406580e-02  1.14141800e-01
 -1.00537710e-01 -2.67816395e-01 -2.53974169e-01  7.88147969e-04
  1.12247266e-01 -4.60481234e-02 -1.04367003e-01 -4.15249020e-01
 -2.90561991e-04  7.94252157e-02  2.75479127e-02  1.17566241e-02
  2.51853436e-01 -1.21839620e-01  8.95307213e-03 -2.85041332e-01
  2.86959499e-01  5.16310334e-01 -1.09088503e-01 -6.21964075e-02
 -1.08666234e-01 -3.23240787e-01 -4.13792320e-02  9.03062001e-02
 -7.46546984e-02  2.59805471e-01  3.81203651e-01 -5.71760461e-02
  1.49445564e-01 -4.65621706e-03 -6.64183348e-02 -4.05579023e-02
  3.93622443e-02  2.05480605e-02  2.78646089e-02 -1.99932724e-01
  3.56143534e-01 -4.92328070e-02  2.20888928e-01  1.22809045e-01
 -3.43950272e-01 -3.63259047e-01  2.06247911e-01  1.53647900e-01
 -7.34450817e-02  2.02921424e-02 -1.44903168e-01  1.79146633e-01
 -6.15763664e-02 -2.74095070e-02 -2.48866767e-01  1.42530784e-01
  1.76467389e-01  4.56877705e-03 -1.51275411e-01 -2.01842412e-01
 -4.14023474e-02  1.89207762e-01  1.76104277e-01 -3.10016964e-02
  1.29179612e-01 -1.18193328e-01  2.11340219e-01 -1.18710279e-01
  8.08383748e-02 -4.46542874e-02 -1.18216060e-01 -1.21690989e-01
 -1.33933261e-01 -2.31236205e-01 -1.05118111e-01  1.32058293e-01
  5.82727566e-02 -3.73559855e-02  6.15848377e-02  1.53939396e-01
  4.28030267e-02  3.12903933e-02  1.28539890e-01  6.01014495e-02
  1.54789045e-01 -2.04286143e-01 -9.26002208e-03 -1.71662718e-01
  6.04955554e-02 -1.73347164e-02 -3.40166658e-01 -6.51450902e-02
 -1.17913127e-01  4.69204098e-01 -1.63561791e-01 -1.85350195e-01
  4.78346437e-01  3.20102662e-01 -1.09247394e-01 -2.46446356e-01
  8.10012668e-02 -9.84848812e-02  2.61023305e-02 -2.23083779e-01
  2.39480525e-01  1.93345889e-01  2.45588273e-01  4.42330875e-02
 -6.49427414e-01  5.95993176e-02 -1.15296014e-01  6.89179376e-02
 -2.10001003e-02  1.68582648e-01  1.86171889e-01  4.32972610e-02
  2.37585708e-01  1.57168880e-01  1.30913228e-01 -1.27659157e-01
  9.56786871e-02  2.56607324e-01 -3.99105728e-01  1.18555330e-01
  4.76709306e-02 -2.29124546e-01  4.37425286e-01  3.56498748e-01
  2.56454311e-02 -2.63487190e-01  1.58884168e-01  3.08480740e-01
  6.84023425e-02 -6.58965632e-02 -3.42708826e-03  1.30646870e-01
  1.52005872e-03 -1.67322215e-02  3.56518418e-01  6.05868511e-02
 -4.13452625e-01 -3.90353203e-02 -2.22972125e-01  2.09501073e-01
 -7.85018690e-03  6.04285533e-03 -2.09052846e-01 -3.38169575e-01
 -6.93432689e-02  2.08198857e-02  1.34767547e-01 -1.50242597e-01
 -1.16675593e-01 -2.67890662e-01 -1.81306571e-01 -1.91599607e-01
 -2.53393322e-01 -1.81389675e-01 -3.82529020e-01 -8.94604698e-02
 -2.28566498e-01  4.85175811e-02 -2.48413160e-01 -4.91648346e-01
  1.60891935e-01  7.24064633e-02  2.16594666e-01  1.77829012e-01
  1.21829764e-03 -1.46680996e-01 -1.86863810e-01  2.11307377e-01
  1.15396082e-01  1.14123419e-01  9.53460764e-03 -2.83990651e-01
 -3.99907559e-01 -1.00258831e-02  3.26151517e-03  2.77500957e-01
  1.65429309e-01  6.50465637e-02  1.27705559e-01 -3.32441330e-01
  1.83680817e-01  8.69656205e-02 -1.32399246e-01  9.65624005e-02
 -2.25598231e-01 -4.30098474e-02  1.46315441e-01 -5.30939959e-02
 -3.14079106e-01 -1.70831203e-01 -1.35158882e-01  7.56394863e-02
  1.04748346e-01 -1.52600765e-01 -1.29163429e-01  2.05138102e-01
 -2.67506801e-02  1.79739762e-02 -2.24789023e-01  7.41660595e-03
  3.30400258e-01  1.21753208e-01 -2.38011882e-01 -2.03929450e-02
 -1.79613307e-01  1.14665933e-01  2.77071863e-01  9.62404832e-02
  2.84169614e-01  5.60732074e-02  1.89747721e-01  3.10972512e-01
  3.91029894e-01  1.16728656e-01  5.57948761e-02  1.67775869e-01
  1.64989475e-02  3.48951854e-02  9.77351815e-02 -1.20982625e-01
  3.76612023e-02 -2.46720105e-01  1.77634522e-01 -1.33677810e-01
  4.70952466e-02  1.46994606e-01 -1.22987457e-01  2.60918945e-01
 -7.06064031e-02 -3.76345403e-02 -2.20127717e-01  1.94560308e-02
 -3.27172689e-02 -3.85259151e-01 -1.39486805e-01 -1.51030617e-02
  1.05173349e-01 -6.91627264e-02  4.08176146e-02  6.16253205e-02
  6.33069053e-02 -1.54138375e-02 -8.79895166e-02  1.67919278e-01
  7.22409710e-02 -2.75148988e-01 -2.45723009e-01 -1.58467278e-01
  1.29724950e-01 -3.90566848e-02  3.53796005e-01 -2.22770333e-01
 -1.01489149e-01 -2.95517534e-01  5.20917356e-01  6.85006529e-02
  2.98751831e-01  1.31603645e-03  1.57706011e-02  4.39272404e-01
  4.44904491e-02  4.80785608e-01 -2.39804581e-01  8.25369284e-02
 -4.67302836e-02  7.35003501e-02 -1.98910162e-01  1.63888887e-01
  1.18648842e-01 -1.36100814e-01  4.91493605e-02  1.17033191e-01
  1.99080139e-01 -2.79230643e-02 -2.12809429e-01 -2.55527020e-01
  1.36738881e-01  1.60903201e-01  8.60602260e-02  4.39362042e-02
  9.23165213e-03  1.74582854e-01  1.51166007e-01  1.46456644e-01
  1.56111121e-02  2.33289003e-01 -3.11150223e-01 -2.94831604e-01
 -2.51399428e-01  6.62577003e-02  2.76649743e-02 -2.15528935e-01
  1.04730651e-01 -1.51599735e-01 -9.85177383e-02  5.13329983e-01
  7.22300932e-02  2.37285241e-01 -8.21494609e-02  1.09501742e-01
  1.22709787e-02 -9.24880616e-03  4.75408703e-01 -2.51981858e-02
  1.33140177e-01 -2.09130540e-01  2.15556353e-01  9.45531875e-02
 -3.53642195e-01  9.65500399e-02  5.92702329e-02  4.48747911e-02
 -2.58978993e-01  3.90437357e-02  2.40005385e-02 -3.09022903e-01
 -7.25388946e-03  2.57445991e-01 -2.86765575e-01  2.93630779e-01
  3.36426534e-02 -1.93876419e-02  6.08339980e-02 -1.95829421e-01
 -3.45770786e-05 -2.28493452e-01  1.39961794e-01 -1.89275146e-02
 -3.15785617e-01  2.62070477e-01  5.38132526e-02  6.80131540e-02]"
Blank invalid,,False,"[-0.98380244  0.66237485 -0.38376853 -0.4632802  -0.40074104 -0.7056
  0.07690724  0.49492463  0.3967783   0.3357915   0.5992155  -0.26862016
  0.30281717 -0.13575539 -0.3312897  -0.6849457  -0.59034085  0.5753523
  0.19186886  0.49726906  0.22923017  0.40497923  0.10981818 -0.29042995
  0.5824481   0.728648    0.8749646   0.03455337  0.227738   -0.51103055
 -0.05842037  0.54315394 -0.06888936  0.05351337  0.23144336  0.11567776
 -0.67145836 -0.11639555  0.10424044 -0.36425105 -0.08173351 -0.11913613
  0.4830083   0.3109146   0.19149005  0.8780147   0.37156156 -0.14587912
  0.4313894  -0.22731514 -0.38038695  0.21091323 -0.40530992  0.04640151
  0.39110738 -0.03165875 -0.5254805  -0.40403062 -0.01699769 -0.04354563
 -0.20887223  0.41686764 -0.11688431 -0.3679254   1.2734644  -0.07449462
  0.28320912 -1.0657493   0.12075698 -0.48024017  0.09918024  0.24774572
 -0.7028972   1.0334909   0.04847257  0.19792338 -0.23252377  0.3494041
 -0.1434783  -0.40420237 -0.9249834  -0.12680227  0.02721008 -0.5176077
  0.31664854 -0.25132602  0.09782423 -0.35118335 -0.0539097   0.62856174
  0.31494728  0.7000116  -0.06661375 -0.20157525  0.00680589 -0.7804618
  0.45610157  0.31870025 -0.7763482   0.9915199  -0.25157303  0.02202934
 -0.82532996 -0.1136547  -0.30912477 -0.20503005  0.2874679  -0.77211297
 -0.26561022  0.40873733  0.31163958 -0.3036848  -0.32024717 -0.00786053
  0.29501072 -0.22230215 -0.00680764 -0.4383306  -0.5301727  -0.00744821
 -0.31214163 -0.28014618 -0.02329274  0.6296089  -0.7066561  -0.3758855
 -0.01236181 -0.15164039 -0.13023643  0.33353215  0.0084446  -0.25566453
 -0.54990226  0.75126344 -0.29903227  0.10601816 -0.0409942  -0.22618194
 -0.50206435  0.63470757 -0.08807213 -0.44167843 -0.10860507  0.05365556
  0.42369476 -0.76335275  0.40091586 -0.3890232   0.6817167   1.1322234
  0.06071196 -0.19370031 -0.09959556  0.24658236 -0.37351507 -0.629573
 -0.17429624  0.7726575   0.06264911 -0.5955286   0.33475894 -0.1702902
  0.14930268  0.66846704  0.01482474 -0.0763586  -0.4668089   0.26239058
 -0.4840499   0.12427235 -0.66113335  0.7157522  -0.22652292 -0.01789243
  0.81345093 -0.91215914  0.29078475  0.48519474 -0.84277487 -0.1342383
 -0.6671036  -0.23138073  0.07971017 -0.10143708 -0.21605659  0.24815908
 -0.4534716   0.35918665 -0.3343357   0.6407554  -0.5972742   0.3984328
 -0.5351573   0.41845885  0.07322484 -0.50836664  0.46946684 -0.98411083
  0.14235505 -0.9018118   0.20974155 -0.07576387 -0.01541489  0.07407574
 -0.19515476 -0.98506516  0.12547271  0.07431875  0.13166179  0.4328571
  0.12340705  0.36298114 -0.84808695  0.35171276  0.5229931   0.37172836
 -0.18141519 -0.24217649  0.44725356  0.35621294  0.37139735 -0.43323326
  0.30822715 -0.14320278  0.02419468 -0.5807452  -0.21757774  0.30242705
 -0.85055685 -0.22123113  1.0469548  -0.09207382 -0.5428422  -0.6042614
  0.22581053 -0.0593276   0.02367108 -0.24526575 -0.27929947 -0.24319169
 -0.57319576  0.02376829 -0.1907632   0.30627337 -0.8717238   0.79257536
  0.04423116  0.00831012  0.3015987   0.57695484 -0.19060156  0.9634301
 -0.07512619  0.38797325  0.2946254  -0.14246121  0.48663425 -0.28998274
  0.00339739  0.13406757  0.54721606 -0.10228133  0.19825748  0.48205104
 -0.1320292   0.20505497 -0.4181339   0.16823831 -0.55567765  0.5293969
 -0.212811    0.32937708  0.7616227   0.13301475  0.2156116  -0.2995044
  0.5450393  -0.56953317 -0.17441794 -0.31757578 -0.36891964  0.5322646
  0.68635005  0.21936616  0.47830257 -0.55652636  0.10011043 -0.9258319
  0.15001318  0.04887062  0.5743982   0.4369492   1.0010507  -0.522261
  0.12914236 -0.48147362  0.13019626 -0.52023375  0.42589933  0.09785376
 -0.69794667  0.352081    0.37445992 -0.21635096  0.61234665  0.03747465
 -0.12168451  0.36003712  0.45254135 -0.4033882   0.20968965 -0.54520893
 -0.24850614  0.326292   -0.39755732  0.07271899 -0.30388254  0.66505396
  0.55169874  0.1095795   0.13383318 -0.4888925   0.09133097 -0.03131362
  0.33074817 -0.25479347 -0.09957597 -0.5688182   0.57302123  0.05480518
  0.2017101   0.21969374 -0.6010703  -0.4481288  -0.09169395  0.07723261
  0.18285052  0.22809525  0.00378179 -0.7823715  -0.663144   -0.00342296
 -0.9548771  -0.47361222  0.46676913  0.28160402  0.31089598  0.08274627
  0.27039742 -0.26685116 -0.17772922 -0.1475482   1.4834979   0.06825494
 -0.05946061  0.4553739  -0.15220723  0.6303991   0.9035773  -0.41893545
 -0.04128559  0.33899003 -0.42941695 -0.03555568 -1.155092    0.09810984
  0.7049001   0.15843372  0.00990481  0.20807686 -0.18570903 -0.10986102
 -0.06446853  0.18558708  0.11491867 -0.51596606  0.36930698 -0.08482856
 -0.36706713  0.7438644   0.73382914  0.18516648 -0.46682557 -0.22764678]"
Converter issue stat:awaiting response stale TFLiteConverter,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",False,"[-4.95820373e-01 -5.55413246e-01 -1.11618623e-01  6.97739571e-02
  1.42564714e-01 -1.12284891e-01 -5.34416921e-02 -6.64936081e-02
 -1.59273848e-01 -1.66428894e-01 -4.03464846e-02  6.96865842e-04
 -9.34556723e-02  1.59634918e-01 -2.89669186e-01  1.83578849e-01
 -1.46028966e-01 -1.45149946e-01  3.36968631e-01  6.28317446e-02
 -5.55406846e-02  7.32736103e-03 -3.54908794e-01  2.87237793e-01
  2.91421413e-01  2.43511349e-01 -2.22388238e-01  8.36145692e-03
  3.73113081e-02  1.69351235e-01  2.98916668e-01  7.48381019e-02
  3.14918533e-02  3.60075086e-02  2.51519550e-02  1.59470677e-01
 -1.36052787e-01 -7.22356364e-02 -2.53325701e-01 -2.57117242e-01
  2.45260969e-01 -5.94952181e-02 -4.56521064e-02  9.25214663e-02
  1.20152477e-02  5.76253012e-02  1.48367643e-01 -1.44885987e-01
 -1.66084468e-01 -3.54150161e-02 -1.78760350e-01 -4.86762859e-02
 -4.07161921e-01 -9.81203765e-02 -5.87034971e-02  5.25278524e-02
  2.54085153e-01  2.80174389e-02 -5.81309982e-02  4.91318777e-02
 -1.99949071e-02 -1.02067208e-02 -2.00075656e-01 -6.54210895e-02
  8.84734243e-02  2.90006250e-01  1.91941902e-01 -5.83168902e-02
  3.30482870e-01 -3.05452377e-01 -6.12225085e-02 -9.88683999e-02
 -1.89600781e-01 -1.70680601e-02  5.17590577e-03  6.60157204e-02
 -1.96404189e-01  2.48466864e-01  1.91782624e-01 -1.02845011e-02
 -1.06515326e-01 -1.93653017e-01 -3.85110406e-03  1.00619510e-01
  8.56358334e-02  2.16236226e-02  1.52595311e-01  1.17308483e-01
  2.49680609e-01 -1.94940746e-01  3.53129894e-01  4.19637263e-01
  1.40908748e-01 -1.77533105e-02  4.31474805e-01  1.31893203e-01
 -5.02888039e-02  2.91377664e-01  3.35225761e-01 -8.41979384e-02
 -1.17986664e-01 -2.25413084e-01 -2.52091885e-01 -1.16287850e-01
  1.21605195e-01 -1.97561026e-01  1.27211303e-01  1.36876345e-01
  2.05560401e-02 -1.16798997e-01  1.64414361e-01 -3.12148072e-02
  1.39731541e-01 -3.52621078e-02  1.47404149e-01  2.04603411e-02
  2.93100923e-02  1.04680508e-01  6.43587783e-02  6.26788497e-01
 -7.62345344e-02 -5.07274121e-02  2.00216562e-01  2.90393919e-01
  2.50940382e-01  1.34615764e-01 -1.75688326e-01 -5.56202158e-02
  3.63264084e-02  2.42301315e-01  2.27135718e-01  2.33494684e-01
 -2.17025399e-01  2.16527015e-01 -4.15218174e-02 -1.50413781e-01
 -1.48498500e-02 -3.72987874e-02 -3.06608498e-01  9.18016583e-02
 -9.78630558e-02  1.54142156e-01 -6.60644919e-02 -2.95866698e-01
 -1.32896930e-01  1.22885834e-02 -2.58633941e-01  1.88894838e-01
 -1.58062540e-02  2.44000107e-01 -6.70290217e-02  6.14313371e-02
 -5.17279543e-02  4.44581628e-01  1.09392613e-01  9.38687474e-02
  2.09238648e-01 -4.29951362e-02  5.08498475e-02 -4.80881333e-01
 -1.22876547e-01  2.46176228e-01 -2.39015192e-01 -1.35694325e-01
  4.42636907e-02  1.54899359e-01 -5.90287805e-01 -1.72990933e-01
  1.38963029e-01  2.27457494e-01 -1.04748383e-01 -1.33825451e-01
  1.18452385e-01  9.47934687e-02  1.80431396e-01  1.68411294e-03
  7.16700912e-01 -4.91523623e-01 -1.73926681e-01 -2.40840614e-02
  1.42376214e-01  3.60248052e-02 -1.71158947e-02 -6.14233017e-02
  7.45630357e-04  8.01471919e-02  1.80393845e-01  8.28326419e-02
 -4.42463875e-01  4.88416106e-02 -3.02639604e-01 -3.51649560e-02
  2.56307691e-01  5.26287109e-02 -2.87702560e-01 -2.87531409e-02
  1.76394936e-02  7.04686157e-03 -7.94097036e-02  1.04918934e-01
 -1.89247638e-01 -2.96172053e-02 -3.07000056e-02  7.23840762e-03
 -3.77786485e-03 -1.73280627e-01 -1.82928741e-01 -4.24153030e-01
 -6.81825399e-01  6.07484654e-02  1.50182441e-01 -3.73231590e-01
  6.99394420e-02 -7.49495551e-02 -1.86809644e-01  6.67126402e-02
 -1.27411515e-01 -1.42855980e-02 -2.34559625e-01  1.03325993e-01
 -9.78645831e-02  2.91685835e-02  1.67306289e-01 -1.75011814e-01
 -4.03779596e-01 -2.31484566e-02 -2.14569628e-01  3.10720384e-01
  6.71268031e-02  1.95803404e-01 -1.27153784e-01 -1.03542358e-02
  5.18305898e-01  1.87175274e-01  4.72028442e-02 -1.88716531e-01
  8.87070000e-02 -1.22638531e-01 -3.34698558e-01  8.36009532e-02
 -2.82177150e-01 -1.53106838e-01 -2.93096751e-02  4.71413322e-02
 -2.16917634e-01  2.95443296e-01  6.58566691e-03 -1.28663123e-01
 -2.92774856e-01  1.98175207e-01  1.89364403e-02 -1.90385878e-01
  4.43187177e-01  2.76314080e-01  2.57652730e-01  1.21389195e-01
 -1.36933085e-02 -2.28050929e-02  1.45626932e-01 -2.42350698e-02
  1.62149578e-01  2.29365826e-01  1.12134784e-01  5.76820016e-01
  3.19589794e-01  1.78325772e-01 -2.29820549e-01  2.19372153e-01
 -2.63303518e-01 -1.80052623e-01  9.20509249e-02 -2.62851954e-01
  2.30308652e-01 -2.35357225e-01  1.18106514e-01 -1.10653877e-01
  2.65675902e-01  1.92733034e-02 -7.78273912e-03  1.46326005e-01
  1.34007141e-01  3.94178122e-01 -5.64433455e-01  1.15042053e-01
  1.83410626e-02 -1.31833315e-01  1.33156180e-01 -4.83001173e-01
 -1.54010326e-01  1.46840632e-01 -1.52391121e-01  1.60197213e-01
  2.19710916e-02  1.02036253e-01  2.62580942e-02  2.48352848e-02
  2.53119618e-01 -5.62370270e-02  1.45413086e-01  8.97824764e-02
 -2.14764476e-01  3.20801675e-01  3.03716779e-01 -2.86400795e-01
 -1.83411673e-01  4.80400547e-02  3.49648118e-01  1.30057812e-01
  3.85468006e-01 -4.64291215e-01  2.33042479e-01  4.18512970e-02
  9.94084403e-02  1.96762487e-01  7.00729936e-02 -1.69717669e-02
 -3.39341164e-01  4.38700080e-01 -2.41163149e-02  8.22749585e-02
  2.37840235e-01 -2.26592366e-02 -3.53610069e-01  1.40552819e-01
 -5.64584974e-03  1.58640727e-01 -1.65999345e-02 -9.41833705e-02
 -2.25745708e-01  1.73838019e-01 -6.06722608e-02 -1.77394271e-01
 -8.60470086e-02  9.38739330e-02 -1.62320912e-01 -2.22361177e-01
 -1.76856160e-01  2.41583973e-01 -9.77303609e-02 -3.25056285e-01
 -1.08491734e-01 -2.81952113e-01 -1.28500722e-02 -4.50537324e-01
  8.15018639e-02 -3.29246342e-01  1.73916638e-01  3.19265366e-01
  1.80553049e-02  1.05047464e-01 -1.81122273e-02 -7.34580532e-02
 -2.85099804e-01 -3.58722210e-02 -1.11685172e-01  3.02898347e-01
 -1.31173626e-01 -2.61930861e-02  2.36462414e-01  4.60582703e-01
 -1.33421481e-01 -1.36555254e-01 -3.52440655e-01 -1.70818076e-01
  7.01259524e-02 -1.75762117e-01 -2.76872396e-01 -2.00264305e-01
  1.29752487e-01  5.20775676e-01 -1.19290441e-01  2.63541251e-01
 -2.52478063e-01  1.16994463e-01  4.90713000e-01 -2.50518143e-01
 -1.88480243e-01  1.03886366e-01  1.15137823e-01 -2.66378701e-01
 -1.02168500e-01  4.78416681e-02  7.67833591e-02 -1.08253196e-01]"
TFLite cross compile error --> fatal error: cpuid.h: No such file or directory stat:awaiting response type:build/install stale comp:lite,"I am trying to cross compile TFLite cpp code for ARM64 on ubuntu machine. After all the steps of installation I copied the cpp file to tflite_build  directory, I executed the following command:
![image](https://github.com/tensorflow/tensorflow/assets/43563075/c09083e9-a3fc-4124-a2d2-fab960b0503b)
After execution of some seconds I am getting the following error.
![image](https://github.com/tensorflow/tensorflow/assets/43563075/20615390-e05c-42d0-8c52-009df82f7098)
It will be really helpful if someone can help me to resolve the error.
",False,"[-6.47576630e-01 -1.48097396e-01 -4.72416699e-01  1.55213729e-01
  1.52326047e-01 -1.78508356e-01 -2.41901532e-01  2.92523742e-01
 -3.49769175e-01 -2.36089990e-01  2.57280357e-02  2.18760222e-03
 -3.12105656e-01 -2.38351934e-02 -8.06579813e-02 -5.74516356e-02
  7.20212236e-04 -2.51607716e-01  3.80369842e-01  1.28026128e-01
 -1.48145333e-01  3.12879384e-01  3.16686332e-02  3.04589629e-01
  1.79810435e-01  3.62385154e-01 -1.92753017e-01  1.91213176e-01
 -1.06499240e-01  5.72398119e-02  3.55336577e-01 -6.46188036e-02
 -3.05375814e-01  3.54157053e-02  4.10540611e-01 -1.47911161e-01
 -3.03352803e-01 -9.38945413e-02 -2.16831759e-01 -3.01851869e-01
  1.69598460e-01  3.20397735e-01  1.13509320e-01  6.50976896e-02
 -9.25955176e-02  1.19612694e-01 -1.74837470e-01  1.28131777e-01
 -6.27069026e-02 -3.69902670e-01 -1.14296965e-01 -1.45586789e-01
 -6.16054758e-02 -2.33646378e-01 -5.56663200e-02 -2.96935797e-01
  1.42914385e-01  2.29088575e-01  8.71825367e-02  1.19220570e-01
  2.25790828e-01 -5.95114343e-02  5.42025119e-02 -8.22878331e-02
  2.13104963e-01  1.96297318e-01  5.18514812e-02 -2.42637128e-01
  3.69251817e-02  2.47668158e-02 -1.02260716e-01 -8.98456275e-02
 -1.52966365e-01 -1.60147727e-01  1.91093441e-02  2.21440513e-02
 -2.83701658e-01  3.04860115e-01 -2.34749019e-02 -1.55003384e-01
  1.64607257e-01 -2.49725282e-01 -2.44045779e-01  2.72772849e-01
  2.13986740e-01  6.22677505e-02  9.92496535e-02  1.88710421e-01
  2.28365511e-01 -1.63695663e-01  3.62661660e-01  6.33786023e-01
  1.16362795e-01  4.05279875e-01  2.34052204e-02  1.53003782e-01
  1.02536872e-01  2.47354075e-01 -2.13870689e-01 -2.52350092e-01
  8.09281245e-02 -1.25920683e-01 -2.23009791e-02 -2.51273215e-01
  6.82976693e-02 -1.55333608e-01  3.47233653e-01  1.76678851e-01
  1.47439569e-01  1.67298075e-02  1.41177893e-01 -3.19269374e-02
  2.03404650e-02  7.68448506e-03  1.44983277e-01  2.82353878e-01
 -1.26784623e-01 -1.97314117e-02 -6.51042163e-03  1.65443465e-01
 -2.58781135e-01 -1.08157985e-01 -2.82032430e-01  1.02896422e-01
  3.98236930e-01 -1.05097145e-01 -4.78357732e-01 -1.07037686e-02
  2.69406736e-01  3.26686740e-01 -3.41846734e-01  3.22567999e-01
 -1.71789378e-01 -1.97806302e-02  2.01958507e-01 -3.10676266e-02
 -4.51346934e-01  3.47760320e-03 -1.65532589e-01 -2.92634666e-01
  3.80364284e-02  2.84120917e-01 -2.68590361e-01 -4.28908527e-01
 -1.76516071e-01 -4.20118868e-03 -2.57092714e-01  7.30480850e-02
 -7.42247701e-02 -1.66717432e-02  1.09776966e-01  2.54290923e-02
 -1.43427253e-01  5.83328903e-01  2.11579800e-01 -2.90493872e-02
  4.06722754e-01  3.00956964e-02  2.51268446e-01 -2.86247134e-01
 -1.79589018e-01  3.38844925e-01  3.09765935e-01  4.04381335e-01
  1.57030553e-01 -1.11939073e-01 -5.16007006e-01  2.23709136e-01
  6.68374449e-02  1.12937823e-01 -6.17420673e-02 -4.35751192e-02
  3.99910629e-01  1.07333481e-01  2.81296492e-01 -8.78037065e-02
  4.63552296e-01 -3.36843669e-01 -1.77499428e-02 -8.32651854e-02
  2.39279628e-01  1.09951884e-01  1.67959705e-02  5.38046062e-02
 -4.91527840e-03  1.46543354e-01  3.46949100e-01  5.59264608e-03
 -2.99175382e-01 -1.24959752e-01 -6.57326460e-01 -4.97828275e-02
  7.20686764e-02  4.94874269e-02 -2.59244382e-01  1.20436966e-01
  1.87055662e-01  3.93602788e-01  3.31738532e-01 -2.89959580e-01
  1.89622834e-01  1.34868130e-01  8.67195800e-02 -1.40398398e-01
 -1.37687176e-01 -5.66823296e-02  3.24447006e-02 -2.92174995e-01
 -2.87879288e-01 -1.09452039e-01 -2.37674685e-03 -2.37797096e-01
 -1.01824537e-01 -3.41836989e-01 -2.83029944e-01  3.17513704e-01
 -9.30594057e-02 -6.34210780e-02  2.29563527e-02  4.52879332e-02
  3.45213205e-01 -1.99665844e-01 -7.40316883e-02 -2.50262678e-01
 -1.52445123e-01 -4.90589365e-02 -3.53833735e-02  1.96303815e-01
 -2.68760979e-01  1.37253687e-01 -2.82664448e-01 -1.69368118e-01
  3.43289286e-01 -4.19889130e-02  2.21634910e-01 -8.71666074e-02
  2.08173394e-01 -2.48113915e-01 -1.52188689e-01  1.85903206e-01
  2.18424816e-02 -4.95791614e-01  7.07005560e-02 -3.10274750e-01
  2.24562079e-01 -2.28691146e-01  1.87264577e-01  3.29281807e-01
  3.43503989e-02  4.00599509e-01 -7.55914673e-02 -2.67728150e-01
  4.02462929e-01  7.59775378e-03  1.47858664e-01  3.98900568e-01
 -3.38953286e-02  8.29174891e-02 -2.40125820e-01  2.66162492e-02
  1.91226676e-02  4.91773427e-01  5.89550957e-02  2.63660550e-01
  2.64273956e-04  8.16498399e-02 -2.88449317e-01  1.91284120e-02
 -1.82757288e-01 -1.35821640e-01  9.93173867e-02 -7.37295747e-02
  1.21699750e-01  1.01661459e-01  9.97836143e-02 -1.87816009e-01
  4.51819807e-01 -2.13614285e-01  5.33024147e-02  3.40733856e-01
  6.04136474e-03  1.03229195e-01  3.43368538e-02  2.28884503e-01
  4.61457759e-01  1.54753119e-01 -9.21315048e-03 -4.50213581e-01
 -3.70759964e-01  1.68835074e-01 -2.65768945e-01 -2.80160785e-01
  1.96606770e-01  3.20697665e-01 -1.68784678e-01  1.43993437e-01
  1.27996489e-01 -1.54012933e-01  1.16364025e-01 -7.23906457e-02
 -1.02893800e-01  2.25351363e-01  1.14282221e-01 -1.44308984e-01
  9.94853377e-02  4.23573852e-02 -8.62826332e-02 -1.10652342e-01
  3.29611540e-01 -4.68147606e-01  1.25658885e-03 -1.92408469e-02
 -1.17183596e-01 -1.58785786e-02 -1.23976663e-01  9.50645357e-02
 -1.34628251e-01  2.14145660e-01  1.77762546e-02 -1.43385634e-01
  4.65160370e-01 -9.40439999e-02 -2.25609511e-01  4.88053225e-02
  6.83131441e-02 -2.24102400e-02 -7.78390914e-02 -1.40304580e-01
  5.59280775e-02 -2.81565767e-02  3.71135920e-02 -3.34940672e-01
  9.87270623e-02  1.34263217e-01 -2.51092583e-01 -1.26976132e-01
 -3.35621387e-02  1.86364472e-01 -1.79164410e-01 -3.57428074e-01
 -9.12075788e-02 -3.24127436e-01 -1.14172682e-01 -3.28998625e-01
 -3.17330286e-02 -1.01058021e-01  1.29937813e-01  1.82168007e-01
 -1.90128922e-01 -1.54273599e-01 -2.28803337e-01 -1.31268322e-01
 -1.53693989e-01  8.08849111e-02  2.20320344e-01  1.07094109e-01
 -5.34724183e-02  4.39174064e-02  2.71643579e-01  3.44450682e-01
  1.87413797e-01  2.63907015e-01 -4.94927049e-01 -2.46005431e-01
  7.85561204e-02 -2.32233569e-01 -2.80952692e-01  2.83079222e-02
 -4.51642573e-02  3.61814201e-01  4.98849750e-02  2.51741350e-01
 -1.89681083e-01  2.31445283e-02  2.40957528e-01 -2.16329843e-01
  9.84372199e-02  2.04930335e-01 -1.81438565e-01  9.04369913e-03
  2.21783370e-01 -3.77490185e-02  2.24689782e-01  1.53655291e-01]"
Distributed training with parameter servers example using a single binary stat:awaiting tensorflower type:support comp:dist-strat,"Hello everyone! I am sorry if this is a duplicate issue but from my considerable search - I could not find a single end-to-end distributed parameter-server example to run using tensorflow (using the keras api with `.fit()` method). Also, for some reason - the documentation for parameter-server strategy seems a lot more confusing and difficult to get started with, compared to multi-worker strategy. 

I have been running training jobs using the estimator api before and now trying to update it to TF2.x style distributed training job with parameter-server training strategy using a single binary file for all workers and parameter-servers. I started with the example in documentation here (https://www.tensorflow.org/tutorials/distribute/parameter_server_training) and modified the code to be used as a single binary. 

Code:
```
import tensorflow_datasets as tfds
import tensorflow as tf

import os

cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()
if cluster_resolver.task_type in (""worker"", ""ps""):
  # Start a TensorFlow server and wait.
  server = tf.distribute.Server(cluster_resolver.cluster_spec(),
                                      job_name=cluster_resolver.task_type,
                                      task_index=cluster_resolver.task_id,
                                      protocol=cluster_resolver.rpc_layer or ""grpc"",
                                      start=True)
  server.join()
else:
  ## parameter-server
  strategy = tf.distribute.ParameterServerStrategy(cluster_resolver=cluster_resolver)
  global_batch_size = 64
  x = tf.random.uniform((10, 10))
  y = tf.random.uniform((10,))
  dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(10).repeat()
  dataset = dataset.batch(global_batch_size)
  dataset = dataset.prefetch(2)
  with strategy.scope():
    model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
    model.compile(tf.keras.optimizers.legacy.SGD(), loss=""mse"", steps_per_execution=10)
  working_dir = ""./my_working_dir""
  log_dir = os.path.join(working_dir, ""log"")
  ckpt_filepath = os.path.join(working_dir, ""ckpt"")
  backup_dir = os.path.join(working_dir, ""backup"")
  callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir=log_dir),
    tf.keras.callbacks.ModelCheckpoint(filepath=ckpt_filepath),
    tf.keras.callbacks.BackupAndRestore(backup_dir=backup_dir),
  ]
  model.fit(dataset, epochs=5, steps_per_epoch=20, callbacks=callbacks)
```

To my understanding, all workers and paramter-servers will start and wait for chief to assign the tasks. Chief or coordinator (documentation uses them interchangeably but is there any difference between the two?) will automatically divide the work based on the information it gets from `cluster_resolver` (let me know if that's wrong interpretation). In any case, I would highly appreciate if someone can point out what I am doing wrong in this example because I have not been able to get it to work!",False,"[-3.54510427e-01 -4.72868741e-01 -2.21362859e-01 -3.33395451e-01
  1.83731586e-01 -1.84854537e-01 -7.51415193e-02 -8.44149515e-02
 -4.29871619e-01 -7.15088099e-02  4.93067205e-02  2.10534886e-01
  1.14618819e-02  1.52770877e-01  6.49781525e-02  5.13224788e-02
  1.35625079e-01 -3.68071310e-02  1.39432430e-01 -5.20477295e-02
 -1.19873397e-01  1.12693422e-02 -5.27063757e-03  4.51058298e-02
  9.76973679e-03  9.00873318e-02 -9.16179493e-02 -2.54159868e-02
  7.62460828e-02 -6.02186136e-02  1.66568547e-01  2.27268711e-02
 -2.62399137e-01  1.67516731e-02 -3.72203588e-01  3.44953716e-01
 -1.30023122e-01 -1.52004793e-01 -1.38190091e-01  2.75662273e-01
  1.40518546e-01 -1.07834667e-01  5.10845408e-02 -6.11753128e-02
  2.03946792e-03 -1.09295905e-01 -2.76948124e-01  9.02967602e-02
 -1.41469076e-01 -8.93762559e-02 -7.54820406e-02 -1.10851720e-01
 -8.16115141e-02 -1.30262882e-01  9.37185958e-02 -3.05801928e-01
  2.46809214e-01 -1.40363807e-02 -2.61641532e-01 -5.16626909e-02
 -1.12207517e-01 -4.46394570e-02  6.82039857e-02  1.41216636e-01
  1.34783611e-01  5.69008440e-02  2.03743249e-01 -1.11275241e-01
  5.99815845e-01 -6.42418414e-02  1.06079958e-01 -9.64820944e-03
 -4.13593143e-01  2.66702753e-02 -9.60227549e-02  8.82432014e-02
 -8.53542089e-02  1.82405353e-01  4.37083483e-01 -1.27316490e-01
  5.66527359e-02  6.43743947e-02  3.57859060e-02 -1.12778589e-01
  4.98068891e-03 -2.31311381e-01  2.32508957e-01  1.57352909e-01
  4.10478532e-01 -4.17296439e-02  2.07473993e-01  3.32454830e-01
 -2.81041503e-01 -7.10228682e-02  2.80721098e-01  2.89927542e-01
  9.08684731e-02  1.34571359e-01 -1.02185786e-01 -8.63865167e-02
 -2.73624539e-01 -3.37803990e-01 -2.01931223e-02  1.19290709e-01
  6.82993531e-02 -2.81314194e-01  3.44126523e-02 -1.46314204e-01
  1.75012693e-01 -2.00047463e-01 -3.46534774e-02  3.35520655e-02
 -2.39958689e-02  1.03865325e-01  2.97145289e-03  5.77015355e-02
 -2.64243968e-02  2.42110178e-01 -4.03826311e-02  4.86369133e-01
 -1.08351290e-01 -2.46493161e-01  1.81300014e-01  1.68781847e-01
  4.12349343e-01  1.39203206e-01 -1.19421914e-01 -3.76233980e-02
 -3.74338366e-02  2.01522950e-02  3.12209874e-01  1.39546189e-02
 -1.04122117e-01 -1.34409010e-01  2.64496267e-01  9.88167673e-02
 -1.23208463e-01 -1.79351673e-01 -2.42003441e-01  2.15806872e-01
  4.15878445e-02  2.91360736e-01 -5.25070876e-02 -2.27567047e-01
  8.99111703e-02  1.73234865e-01  1.07298419e-01 -2.08063245e-01
 -3.77483629e-02 -1.51993498e-01 -6.78326413e-02 -7.51581639e-02
 -1.29533205e-02 -1.01227604e-01  1.50611579e-01  2.94594258e-01
  3.21937531e-01  1.02253936e-01  1.75665282e-02 -3.90738189e-01
 -1.34115428e-01  2.82400727e-01 -2.66403586e-01 -7.23089501e-02
  2.14816675e-01  2.97543891e-02 -9.04025882e-02 -7.32646137e-02
 -1.89909518e-01  1.24146834e-01 -1.29744606e-02 -8.23270231e-02
 -2.12539986e-01 -2.59203136e-01  3.29428196e-01  5.09757474e-02
  1.68218255e-01 -4.00889516e-01 -2.52966285e-01  7.55193830e-02
  3.46803725e-01 -5.85453063e-02  1.64367706e-01  3.42301905e-01
  7.99143091e-02  1.36110425e-01  7.61191174e-02  3.44255343e-02
 -3.17003787e-01  1.50372013e-02 -4.95436668e-01 -7.79545456e-02
  2.19814360e-01 -9.69360918e-02 -8.65230188e-02 -3.13119709e-01
  3.87951791e-01  1.64442901e-02  6.87039495e-02  9.17936713e-02
  2.93141231e-02 -1.02235116e-01 -2.76468694e-04  7.41820112e-02
  4.32987139e-02 -4.81707305e-02 -1.82896495e-01 -3.95077229e-01
 -2.55203009e-01  2.64841259e-01 -8.64756331e-02  4.50901315e-02
 -2.77185403e-02  7.93116391e-02 -2.86331534e-01  2.47275922e-02
 -1.04453534e-01  1.24525070e-01  6.72554448e-02  1.32361665e-01
 -1.02511898e-01 -8.70600194e-02  1.25826254e-01 -1.15879804e-01
 -3.53331089e-01  2.38155454e-01 -2.11662367e-01  1.16255462e-01
  2.47244537e-01  1.03190169e-01  1.55703813e-01 -2.63963193e-01
  6.34729415e-02  9.01114866e-02 -3.65626290e-02 -2.96105653e-01
  1.31596431e-01 -1.90414153e-02 -2.98628807e-01 -4.26665023e-02
 -4.90326047e-01 -6.75908551e-02  5.75121865e-02 -1.97553094e-02
  7.09822550e-02  2.85175800e-01 -2.31877029e-01 -1.49312019e-01
  5.64633645e-02  1.82318836e-02 -3.77498031e-01 -1.26381293e-01
 -6.21904060e-03  1.11226723e-01  3.63633245e-01 -7.84989521e-02
  2.85965018e-03  4.05777059e-02  5.52704791e-03 -1.82230175e-01
 -2.60022655e-02  1.61410242e-01  2.50463128e-01  5.52405357e-01
  2.73432076e-01 -1.06373899e-01 -2.37982035e-01  1.58305660e-01
  1.09847691e-02 -2.98348784e-01 -1.18297458e-01 -1.20330423e-01
  2.60713488e-01 -2.18532652e-01 -9.63061750e-02  6.88924342e-02
  1.22219861e-01  3.05868424e-02  4.23621573e-02 -1.42627820e-01
  6.04499504e-03  1.03888124e-01 -3.11516941e-01  4.23751585e-02
  8.35124701e-02 -1.93179905e-01  1.11434743e-01  3.82236391e-03
 -1.11267507e-01  7.63596594e-02 -2.35040262e-02 -1.20317847e-01
  1.81548856e-02  1.23312131e-01 -1.53845698e-01 -7.32786953e-03
  3.31095994e-01 -1.07880617e-02  5.51893339e-02  1.42066360e-01
  6.12770729e-02 -1.34331375e-01  1.05356492e-01  4.16362509e-02
 -3.04652870e-01 -1.42465204e-01  1.87457144e-01 -2.30622336e-01
  3.29917759e-01  3.36892083e-02  2.31739476e-01  1.77117646e-01
 -4.05699834e-02  1.38787165e-01  6.80574626e-02  3.40223834e-02
 -2.45823234e-01  3.76747429e-01  1.04823224e-02  2.50078812e-02
  6.55970573e-02 -1.33061379e-01  1.01800680e-01  7.03296661e-02
  1.96753725e-01  2.31394190e-02 -1.12468332e-01 -2.92089581e-01
 -6.65455759e-02  1.20331071e-01 -7.03582726e-03 -2.74383873e-01
  1.45582139e-01  5.58824316e-02  1.26788914e-01 -1.65871546e-01
 -2.38198385e-01  3.98428202e-01 -1.08827770e-01 -1.49397552e-01
 -1.24796070e-02 -1.81790352e-01 -2.80899227e-01 -1.41545698e-01
 -4.63261195e-02 -1.16654217e-01  1.78090259e-01  4.49710488e-01
  9.10845101e-02 -1.22489266e-01 -5.82487918e-02  1.51548814e-02
 -2.69099176e-01 -1.50777623e-01  7.22716004e-02  3.74483705e-01
  1.34671152e-01 -4.46529686e-02 -1.28758863e-01  4.14134860e-01
 -1.89159542e-01  1.28238827e-01 -1.36779875e-01 -2.56107301e-02
  4.44940418e-01  2.77934298e-02  1.83296036e-02 -1.88077062e-01
  1.55476451e-01  2.90926576e-01 -9.43372548e-02 -5.06458059e-03
 -6.60920069e-02  2.45604724e-01  3.85885268e-01  8.59376192e-02
 -1.77377343e-01 -6.54129758e-02  7.91786909e-02  2.21343517e-01
 -1.86440051e-01 -7.75810182e-02  5.87792993e-02  1.11853033e-01]"
What is the reason sanitizer configs are regarded as outdated?  stat:awaiting response type:build/install type:support subtype: ubuntu/linux,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

mater (7a721887ec4616bd3347815f3ce873a0ab14ea37)

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I tried to build tensorflow with config asan, but I found that it was removed at 7a721887ec4616bd3347815f3ce873a0ab14ea37, by @kanglant 

So, I'm curious why these sanitizer flags were regarded as outdated.

Did the community decide to stop supporting sanitizers for tensorflow?
or just because it is not working now? 

Thank you:)

### Standalone code to reproduce the issue

```shell
bazel build --config=asan //tensorflow/tools/pip_package:build_pip_package --jobs `nproc`
```


### Relevant log output

_No response_",False,"[-5.47001302e-01 -3.31309795e-01 -7.32165724e-02 -4.72759642e-02
  4.25381482e-01 -4.66579765e-01 -1.16747431e-02  7.96057060e-02
 -2.78328955e-01 -2.27494881e-01  1.72734320e-01  3.15190732e-01
  9.36931074e-02 -9.01627019e-02 -2.34761596e-01  2.37552613e-01
  6.19368851e-02 -2.98791051e-01  1.45918921e-01  2.35220015e-01
 -1.93469375e-01 -1.67035848e-01 -2.33308703e-01  1.52069092e-01
  3.10098469e-01  2.41266936e-01 -2.03821003e-01  4.09494564e-02
  3.44470143e-02  4.06103790e-01  6.30886018e-01  1.77817926e-01
  2.59751260e-01  2.03336179e-01  1.43615797e-01  2.38552064e-01
  1.07770711e-01 -2.27482662e-01 -1.72796249e-01  1.11275669e-02
 -2.56159455e-01  7.92123526e-02  8.81720260e-02 -6.06084913e-02
  1.06434345e-01 -4.76295799e-02  2.47823521e-02 -2.65262097e-01
  2.13587552e-01 -6.41694844e-01  1.11680605e-01  2.56436914e-02
 -2.66214609e-01 -2.11641908e-01 -3.03106844e-01 -1.55339241e-01
  1.61711991e-01  1.18024461e-01 -2.31266409e-01  1.57862812e-01
  2.85571337e-01 -2.64986098e-01  4.06604260e-02  4.04264815e-02
  6.66336790e-02  1.51242852e-01  3.42070550e-01 -2.23931611e-01
  6.46134734e-01  8.80156830e-03  2.98892051e-01 -1.27778962e-01
 -1.38358623e-01  1.80695921e-01  2.28193521e-01  2.13251468e-02
  1.61683232e-01  8.38382542e-03  2.84051865e-01 -2.20837161e-01
 -5.85824344e-03 -3.95518243e-01  1.11821003e-01 -1.58261418e-01
  2.40189433e-01 -1.35503471e-01  2.67384231e-01  1.76261187e-01
  4.86481130e-01 -3.00717920e-01  5.03705084e-01  4.96466458e-01
  1.28347352e-01 -1.28242150e-01  4.43557680e-01  8.36535543e-02
  1.41623497e-01  3.30988944e-01 -1.33426815e-01 -1.41654834e-01
 -4.93287027e-01 -9.72831100e-02 -5.35210036e-02  4.02193852e-02
 -1.08744241e-01 -2.42055446e-01  2.46076494e-01 -7.75052831e-02
 -2.10120708e-01  3.81444506e-02  1.20041102e-01 -1.46947429e-02
 -5.20388968e-02 -9.16388035e-02  1.49458526e-02 -1.00927830e-01
 -1.17152289e-01  7.33420104e-02 -1.38852715e-01  4.33049858e-01
  8.61776695e-02 -7.05475509e-02 -3.84552538e-01  4.88756187e-02
  5.07181704e-01  3.16849947e-01 -1.59889117e-01  7.25552067e-03
 -1.26902852e-02  1.55095175e-01 -1.46314457e-01  1.86136160e-02
  8.15558136e-02  3.24103296e-01 -2.23447476e-03  2.89590620e-02
  2.80782789e-01 -2.81596541e-01 -2.16357410e-01 -2.41872147e-02
 -1.53143018e-01  5.18341184e-01  1.20375320e-01 -5.59053659e-01
  9.88627523e-02  8.33382383e-02 -4.11194891e-01 -8.42280164e-02
 -1.94632620e-01  2.23099902e-01  5.04541174e-02  1.88257322e-02
 -2.05277614e-02  4.96501088e-01  1.24045253e-01  2.21586809e-01
  2.90617645e-01 -1.75226182e-01 -1.57488793e-01 -4.01516855e-01
  1.38217524e-01  4.47803468e-01 -7.65883029e-02 -1.26814663e-01
  5.16568795e-02 -2.56356969e-02 -4.08038318e-01 -4.36537802e-01
  3.39139760e-01  6.87404573e-01 -2.71641314e-01 -1.17554747e-01
  3.39604378e-01 -1.40296826e-02 -1.33629248e-01 -4.28274386e-02
  3.06813776e-01 -4.29021031e-01 -1.36670589e-01  4.32517558e-01
  5.43284267e-02 -3.44521999e-01 -1.49439171e-01  2.06042200e-01
  1.13656655e-01  1.09489396e-01  8.01855326e-02 -4.92818654e-05
 -5.21371841e-01 -7.58952126e-02 -4.99598771e-01  2.41592556e-01
  4.08283949e-01 -3.06802005e-01 -1.01670310e-01  2.17855185e-01
  1.33987382e-01  9.28925052e-02 -2.75453087e-03 -1.30874515e-01
 -3.52050424e-01  1.14027500e-01 -8.64634216e-02  1.02440216e-01
  1.59763724e-01 -1.89092666e-01 -1.17428541e-01 -3.99210930e-01
 -5.88433027e-01  6.52088374e-02  1.77182518e-02 -7.31145889e-02
  3.09078217e-01  2.14075428e-02 -2.86160797e-01  2.64681935e-01
  9.65820476e-02  1.10456653e-01 -1.22413352e-01  2.16280177e-01
 -1.03248894e-01 -3.25859427e-01 -1.67865396e-01 -5.18751204e-01
  4.68789041e-02  4.34615463e-03 -2.02902406e-01  4.94447909e-03
 -2.67216079e-02  2.73885220e-01 -4.14358437e-01  9.57503021e-02
  3.85163963e-01  2.48420835e-01  3.16879988e-01 -2.99402356e-01
 -2.64205486e-01 -6.01754859e-02 -3.11066471e-02  3.36531073e-01
 -4.64487016e-01 -2.42148548e-01 -2.51759261e-01  1.85253425e-03
  2.45578706e-01  5.07245421e-01  5.06417044e-02 -5.34695014e-03
 -4.52376097e-01  1.34776443e-01 -7.56684318e-02  4.81421128e-02
  3.58015656e-01 -6.65132776e-02  1.04150504e-01  6.94506168e-02
  9.61172879e-02  1.55183032e-01  2.18134761e-01 -6.03624731e-02
  3.80976230e-01  2.40911469e-02 -1.11329742e-01  8.22902545e-02
  4.34357047e-01  3.75494361e-01 -4.76336718e-01  6.50473475e-01
 -5.07658496e-02 -4.50476229e-01  1.84953809e-01 -2.99130440e-01
  4.01992738e-01 -5.78251243e-01  3.09400141e-01 -2.34549761e-01
  2.65855730e-01 -2.34636068e-02 -2.78427482e-01 -7.06550386e-03
  4.62193042e-02  6.72382265e-02 -5.37530661e-01  1.35181651e-01
  2.61666596e-01 -3.81968975e-01 -2.27028206e-01 -6.42283261e-01
 -3.84547710e-01  1.71857566e-01 -3.17576490e-02  3.72351348e-01
  3.57460566e-02 -1.48883790e-01  1.36701941e-01 -4.03715856e-03
  1.89806148e-01 -1.70340523e-01 -1.96727142e-01  7.57584348e-03
 -1.19229957e-01 -3.07698250e-01  1.42791256e-01 -3.57059598e-01
 -2.21571088e-01 -1.88085586e-01  3.29091921e-02  1.86650351e-01
  4.04266447e-01 -4.16269660e-01  1.12786099e-01  1.11895241e-02
 -7.07461834e-02  3.11008692e-01  1.51166394e-01  1.61509924e-02
 -5.84726334e-01  1.03035080e+00  1.62289143e-01 -2.31898561e-01
  3.35816503e-01  2.47379109e-01 -5.71821369e-02  1.05062455e-01
  2.81816870e-01 -2.46970505e-01 -2.30029702e-01 -2.58907497e-01
  2.67859735e-02  2.93387383e-01 -3.49239558e-02  9.78436619e-02
  1.69450976e-03 -2.29687244e-02 -8.38866383e-02  1.72987267e-01
 -3.78792107e-01  1.93498760e-01 -2.13936090e-01 -1.96239054e-01
 -1.78040177e-01  1.64999902e-01 -3.22601050e-02 -1.83445603e-01
 -1.36216423e-02 -5.43358084e-03  2.65224874e-01  4.67671812e-01
 -2.17886828e-02  3.15753222e-01 -6.21947423e-02  2.40957454e-01
 -3.46994400e-01  2.38041114e-02 -1.22559793e-01  6.36440217e-02
  2.96362042e-01 -2.01869644e-02  2.23309338e-01  1.74896434e-01
 -2.40581632e-01 -7.35367090e-03 -4.86479640e-01  2.87407309e-01
  1.32271916e-01 -2.68988997e-01 -2.74434566e-01 -2.25426666e-02
 -3.26572269e-01  3.02826107e-01  1.23915121e-01  1.35244340e-01
 -2.13594839e-01  3.25961590e-01  4.16421682e-01 -4.11918551e-01
 -4.60938334e-01  1.38707638e-01 -1.20525301e-01  1.13572618e-02
  3.07046529e-02 -2.13193566e-01  2.21888542e-01  2.83309102e-01]"
Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found stat:awaiting response type:build/install subtype:windows wsl2 TF 2.10,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

N/A

### Python version

3.10(Microsoft Store)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

Cuda: 11.2

### GPU model and memory

RTX 3070 Ti 8GB
### Current behavior?

I installed CUDA 11.2 as recommended for tf 2.10.0, here's the install:
![Screenshot](https://github.com/tensorflow/tensorflow/assets/1494132/59352a2a-f90f-45bf-b8bd-861dc893a9ff)
At first, I thought it was a path issue, but after restarting my pc, I was able to access exe files in that folder:
![image](https://github.com/tensorflow/tensorflow/assets/1494132/5d9ccfca-4417-4045-ba74-fffde7b8a121)
If the files are in path, why can't tensorflow find them?
Many people say to use miniconda, so I did, but I got the same result. Other resolved issues were resolved as the OP's were using the wrong version of CUDA, I checked on the website and I can confirm that my version is the required one.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
2023-07-31 18:56:25.098058: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-07-31 18:56:25.098226: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2023-07-31 18:56:26.164080: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2023-07-31 18:56:26.164320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found
2023-07-31 18:56:26.164540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found
2023-07-31 18:56:26.164818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2023-07-31 18:56:26.368828: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found
2023-07-31 18:56:26.369092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
```
",False,"[-3.45317453e-01 -5.39171159e-01 -1.95112705e-01  1.93129450e-01
  1.99048489e-01 -2.25425258e-01 -4.21697021e-01  6.83471411e-02
 -2.43474782e-01 -2.69276619e-01 -6.64259642e-02  1.08412467e-01
 -6.97320104e-02  2.48275716e-02 -1.55624121e-01  4.24226910e-01
 -6.59154505e-02 -4.22265045e-02  3.61809209e-02  2.40473181e-01
 -3.92521620e-01 -2.98104919e-02 -3.20203871e-01 -1.10223986e-01
  2.41686657e-01 -1.59620047e-02 -2.90820688e-01  3.32745872e-02
 -3.89715470e-03  2.76500821e-01  4.02824163e-01 -1.25640750e-01
 -1.81645155e-01  3.72354597e-01  1.48976177e-01  2.47633398e-01
 -1.26515254e-01 -2.71568745e-01 -2.42311522e-01 -1.18541881e-01
 -1.68772846e-01  1.62015229e-01 -2.85200141e-02 -1.30882338e-01
 -1.34563640e-01 -1.95810765e-01  1.19346522e-01 -2.19311953e-01
 -1.89444378e-01 -1.02679931e-01 -1.99054107e-02  1.02453277e-01
 -2.19363883e-01 -3.27198803e-01 -1.24626026e-01  1.23233326e-01
 -8.21015239e-03  1.14075616e-01 -1.46217063e-01  1.84961468e-01
  1.97651088e-01 -1.20506465e-01  1.69214815e-01 -2.02147067e-01
  4.07625996e-02  3.07388544e-01  1.14228196e-01  1.95790827e-02
  4.76442784e-01 -4.54382688e-01  6.55893534e-02  1.31487533e-01
 -4.24273789e-01 -8.11622888e-02  2.05566853e-01  9.37528014e-02
 -2.45677549e-02  1.68803185e-02  1.89477906e-01 -9.67882946e-02
 -3.63347381e-02 -1.34522259e-01 -8.77502188e-02 -5.17948642e-02
  1.75945252e-01 -1.41423047e-01  2.91625023e-01  3.48891914e-02
  2.84715503e-01 -2.01229826e-01  5.11831522e-01  2.61811912e-01
 -1.80731416e-02  1.70037717e-01  2.29969040e-01  1.95325702e-01
  2.07608029e-01  2.34586716e-01 -7.19685853e-03 -9.65073481e-02
 -1.78939447e-01 -3.28107119e-01  2.46820338e-02  1.97193742e-01
 -4.56121005e-02 -4.43882823e-01  3.96724463e-01  1.78302914e-01
 -1.23830825e-01 -1.37025267e-01  2.44888812e-01  2.11207196e-03
  1.10770673e-01 -8.63616541e-02 -8.11725110e-02  5.53563833e-02
 -2.81279564e-01 -1.15342150e-02  7.93396682e-03  5.32161951e-01
 -1.07242679e-02  2.20597554e-02  7.70859122e-02 -9.40810740e-02
  2.79243708e-01  3.00232768e-02  2.50032172e-04 -2.02335007e-02
 -6.65366650e-02  2.76865885e-02  1.20375082e-01  1.82669759e-01
  3.27970684e-01  6.59698620e-02  5.44169731e-02  1.76449239e-01
 -1.64135650e-01 -1.88911587e-01 -2.04258397e-01 -1.80068925e-01
 -2.10957646e-01  1.91012383e-01 -1.27574682e-01 -6.17149115e-01
 -3.13133821e-02  1.96637630e-01 -1.38087764e-01  3.01214844e-01
 -1.02074012e-01 -1.77553985e-02  2.82554794e-02  1.59612909e-01
 -4.19077307e-01  3.88682961e-01  7.80623257e-02  1.70393422e-01
  5.02945065e-01 -8.12565535e-02  5.10074273e-02 -5.00612259e-01
  6.23165555e-02  4.29058760e-01 -1.73408445e-02 -1.21650822e-01
  1.00093797e-01  1.51908964e-01 -2.88940340e-01 -1.61631361e-01
  2.65664101e-01  3.37850869e-01 -2.10030332e-01 -1.54264659e-01
  7.18804896e-02  6.98641017e-02  2.06010029e-01 -2.00320438e-01
  3.12017761e-02 -4.64105844e-01  9.74745303e-02  3.75326276e-01
  2.09135026e-01  2.75530457e-01  1.81274861e-02  1.42533422e-01
  2.70289890e-02  9.50653851e-03  2.08107859e-01  2.45262474e-01
 -5.17397597e-02 -6.37058839e-02 -3.40875924e-01 -1.01478919e-01
  1.52730256e-01 -3.37483048e-01 -7.84663931e-02  7.72766769e-04
  1.70607328e-01  2.29784369e-01  7.99921751e-02  7.00857788e-02
 -3.70315090e-02  2.14642078e-01  7.04938732e-03  1.23440251e-01
  6.43487498e-02 -1.28016979e-01 -4.07499969e-02 -4.94914740e-01
 -3.55848253e-01 -2.42341999e-02 -2.08782017e-01 -4.18199778e-01
  7.18374252e-02 -4.31675054e-02 -2.37991199e-01  2.62591124e-01
  1.04843557e-01 -8.28501359e-02 -9.95160043e-02  1.41882241e-01
 -2.89232470e-04 -2.98355997e-01  4.88270596e-02 -2.69583613e-01
  4.13665511e-02 -1.51897874e-03 -4.70595062e-01 -1.53283030e-01
  8.01159441e-02  1.96087509e-01 -4.64768708e-02  4.00297567e-02
  9.03576016e-02  2.97444701e-01  5.29657960e-01 -2.79326886e-01
 -1.23975337e-01 -7.65916519e-03 -1.47204310e-01  2.11653322e-01
 -4.77546692e-01  1.32076070e-02 -1.76308490e-03 -6.99267536e-02
  2.04285756e-01  3.44649732e-01 -3.98823656e-02 -7.58935213e-02
 -3.63295048e-01  8.55501369e-02 -2.12561473e-01 -7.19952136e-02
  3.30540657e-01  1.40906185e-01  3.44965339e-01  3.30077201e-01
  1.73093349e-01  1.02111012e-01 -2.42421217e-02 -1.92697763e-01
  1.90402091e-01  1.78149909e-01  6.48109540e-02  1.53537184e-01
  2.47753918e-01  2.66306937e-01 -2.50675082e-01  4.56023157e-01
  1.36124566e-01 -4.92781848e-02  3.44424993e-01 -3.33016425e-01
  5.89187980e-01 -4.03130919e-01  2.26587623e-01  9.59107466e-03
  3.06715667e-01 -2.17586219e-01 -7.28933290e-02  7.89195821e-02
 -2.43320083e-03  9.07837003e-02 -2.55961955e-01  8.20610672e-04
 -8.09929520e-02 -1.75885618e-01 -1.87787816e-01 -4.76478636e-01
 -3.12459886e-01  1.03627563e-01 -1.60506591e-01 -2.74494160e-02
 -7.07830787e-02 -2.50501513e-01 -3.95867020e-01  5.08698858e-02
  9.38552916e-02 -1.57650277e-01  5.75595200e-02  3.24640423e-02
 -2.33226702e-01  1.88696191e-01  2.76051223e-01 -3.80524755e-01
 -2.10121989e-01 -1.43379837e-01  2.93857634e-01  1.36088496e-02
  4.03367579e-01 -5.90489686e-01  2.05053389e-01  5.15954047e-02
 -3.51952493e-01  4.28584874e-01 -5.08601516e-02 -6.76469877e-02
 -4.02401268e-01  7.22123265e-01  2.17131555e-01 -5.16216606e-02
  2.83294544e-02 -1.49350196e-01 -4.80248749e-01 -6.95663840e-02
  1.02517530e-01  1.19555630e-01  2.86689047e-02 -6.81805015e-02
  8.60290006e-02  2.37205535e-01 -1.29264235e-01 -7.63059705e-02
 -2.17442065e-01  1.50271744e-01 -2.20603213e-01  1.46118402e-01
 -2.86436290e-01  8.74338672e-02  1.11998469e-01 -1.35703921e-01
 -1.53302904e-02 -7.61527568e-02  1.69382468e-01 -1.10323429e-01
 -2.33178735e-02 -9.65188518e-02  2.44644344e-01  2.48473302e-01
  8.11638385e-02  1.33429423e-01  1.14139263e-02  9.58887395e-03
 -4.43223476e-01  2.85110762e-03 -1.32343203e-01  3.11923623e-01
  3.00456453e-02 -1.59958482e-01  2.09436387e-01  4.31893975e-01
  7.73593038e-03  2.29901835e-01 -3.76519978e-01 -7.08383396e-02
  2.73767054e-01 -7.64972791e-02 -5.34242272e-01 -1.61238402e-01
  1.22844636e-01  2.94344127e-01 -2.27623239e-01  1.33710846e-01
 -1.68863714e-01  2.85739183e-01  4.62063700e-01 -6.96599036e-02
 -2.23325223e-01  6.16565868e-02 -2.29332149e-02 -3.62799205e-02
  2.38447040e-01  3.62730063e-02  2.62370944e-01 -3.79392281e-02]"
"When building from source code, I always end up with a Python 3.10 whl file, instead of a Python3.8 whl file. stat:awaiting response type:build/install type:support subtype: ubuntu/linux","### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.14.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.8

### Bazel version

1.17

### GCC/compiler version

9.4.0

### CUDA/cuDNN version

11.8/8.7

### GPU model and memory

GTX 1050 Ti 4GB

### Current behavior?

When I try to build the source code from my machine I end up always with a wheel for Python 3.10, although I specified the python path for python3.8 and I don't even have python3.10 installed.

The generated wheel is called tensorflow-2.14.0-cp310-cp310-linux_x86_64.whl

Can you guide why this is happening and how to solve it?

### Standalone code to reproduce the issue

```shell
Just trying to build the source code following the steps from this two sites:

https://gist.github.com/kmhofmann/e368a2ebba05f807fa1a90b3bf9a1e03
https://www.tensorflow.org/install/source
```


### Relevant log output

_No response_",False,"[-4.15312648e-01 -3.08852851e-01 -3.74205053e-01  2.69953877e-01
  1.50136292e-01 -3.05672646e-01 -3.23731542e-01  1.27499267e-01
 -4.43789899e-01  3.25508192e-02 -1.39323264e-01  1.48051158e-02
 -1.99126035e-01  9.26589817e-02 -8.92133266e-02  5.09074092e-01
 -7.38306418e-02 -1.26245737e-01  5.85505143e-02  6.39916658e-02
 -1.86560601e-01 -2.95088738e-02 -1.97521254e-01  2.21119877e-02
  1.63728431e-01  4.53343801e-02 -4.40306246e-01  1.50541604e-01
  3.37757058e-02  3.43066633e-01  4.57439780e-01  1.90576583e-01
  6.30029142e-02  5.16933352e-02  2.30384082e-01  3.10024440e-01
 -1.17895842e-01 -3.80170822e-01 -2.71744251e-01 -4.56218310e-02
 -1.14470564e-01  1.61350429e-01 -1.35901853e-01  3.77661511e-02
 -1.55013099e-01 -1.90395325e-01  8.14936012e-02 -1.48329390e-02
 -9.10499245e-02 -2.37423182e-01 -2.04694122e-01 -1.34942252e-02
 -3.38501066e-01 -2.46445090e-01 -6.11696430e-02  6.41932487e-02
  8.75069425e-02  1.36098355e-01  1.63099229e-01 -2.62637176e-02
  1.56301439e-01  1.96908653e-01  6.90013263e-03 -7.49498829e-02
  9.41369459e-02  3.42780501e-01  4.56600673e-02 -4.18681093e-02
  4.34882641e-01 -3.12490165e-01  3.51921879e-02 -3.51261199e-02
 -4.16642576e-01  6.63568377e-02  5.82368672e-02  2.03949451e-01
 -3.71945575e-02 -2.77039930e-02  1.96883559e-01 -2.09437430e-01
 -5.57600893e-02 -3.33828688e-01 -1.89633101e-01  2.46986002e-02
  1.40905455e-01  4.95066941e-02  2.14258522e-01  1.78109109e-01
  2.35136107e-01 -5.90700619e-02  4.63770479e-01  1.50206774e-01
 -2.40459628e-02  5.63955531e-02  3.03132832e-01  1.28337845e-01
  7.20420927e-02  2.07422093e-01 -2.34821066e-02 -1.33551918e-02
  6.37148842e-02 -5.68306804e-01  4.91102636e-02  4.07893490e-03
 -1.24471575e-01 -9.66556966e-02  2.49931693e-01  2.45175555e-01
  9.51929241e-02 -2.99780846e-01 -1.47708561e-02  8.51081759e-02
 -1.48583334e-02 -9.58569050e-02  1.01211347e-01  5.48340455e-02
 -3.29707950e-01  1.20163582e-01  5.82981855e-02  8.65068078e-01
  1.86148524e-01  2.67895684e-02  3.38521264e-02  2.95123421e-02
  5.64365387e-01 -7.75915161e-02 -1.63421929e-01 -6.82508796e-02
  8.08350295e-02  2.12950706e-01  2.44263113e-01  1.82616621e-01
 -9.13895816e-02  1.99174047e-01 -4.74343151e-02  2.96892133e-02
 -3.95118892e-02 -7.10709095e-02  1.07778981e-02 -1.82368755e-01
 -1.63427189e-01  1.95052698e-01 -2.33422834e-02 -7.22608387e-01
 -1.58115670e-01  5.73531687e-02 -3.29503536e-01  5.39095283e-01
 -1.27327710e-01 -8.58655423e-02 -2.70695072e-02  4.72601876e-02
 -2.95720279e-01  2.66688645e-01  8.29068124e-02  1.16254598e-01
  3.71268630e-01  6.73678815e-02 -9.62195694e-02 -4.31370914e-01
 -1.25162244e-01  3.71576071e-01 -2.87828565e-01 -3.26109111e-01
 -2.94904470e-01 -1.56608932e-02 -4.16461915e-01 -3.12351972e-01
 -2.78460234e-02  3.33141208e-01 -4.40627709e-02 -1.24553461e-02
  4.38188732e-01  2.27175802e-01  7.83689171e-02 -5.94231226e-02
  3.05557847e-01 -3.52338254e-01  1.36623621e-01  3.95200729e-01
  1.75126761e-01  3.23773772e-01 -4.61479053e-02  1.98875308e-01
  1.58491164e-01 -4.51592207e-02  2.23702729e-01  1.00858167e-01
  1.83246564e-03  1.32152796e-01 -4.18397039e-01 -4.41162288e-02
  3.91587377e-01 -3.02926004e-01 -2.38302961e-01 -1.38973389e-02
  1.05316043e-01  1.07926190e-01  1.06060885e-01  1.21515036e-01
 -6.27356023e-02  2.37750053e-01 -9.84542519e-02  1.28035694e-01
  1.20983258e-01 -2.64461674e-02 -4.89934310e-02 -2.97097027e-01
 -3.29052627e-01 -1.39000207e-01 -1.23406515e-01 -4.35115635e-01
  2.12190256e-01 -1.16870537e-01 -2.77030736e-01 -9.91078764e-02
  8.38827193e-02  5.91083169e-02 -2.86882296e-02  1.09184824e-01
 -2.09579587e-01 -2.41506487e-01 -7.01347440e-02 -3.61408710e-01
  8.06435756e-03  1.24375476e-02 -2.79722810e-01 -1.05728813e-01
 -1.98761106e-01  1.67883396e-01 -2.99499426e-02 -1.87981352e-01
  4.85632181e-01  3.24815392e-01  4.17544067e-01 -2.07525969e-01
 -4.96233180e-02 -5.41282222e-02 -1.81395561e-01  2.01163545e-01
 -2.50653148e-01 -4.94312420e-02 -6.48446009e-02  1.39053255e-01
 -8.81110504e-03  2.31714025e-01 -3.26350570e-01 -1.30588412e-01
 -2.20931441e-01  1.78400889e-01 -2.17814907e-01  1.05774894e-01
  2.84585059e-01  2.80974567e-01  2.03517348e-01  3.84869307e-01
 -7.04279542e-03  1.00932106e-01  2.78429836e-01 -7.13874474e-02
  1.21387258e-01  9.71644819e-02  2.07685575e-01  5.50122336e-02
  2.77885437e-01  3.48531246e-01 -2.81280577e-01  4.41226274e-01
  1.68219209e-01 -7.18789771e-02  1.24761522e-01 -1.99448943e-01
  4.49120939e-01 -3.49083751e-01  1.75484687e-01  6.58764839e-02
  3.26403260e-01 -1.73064917e-01  1.00651607e-01 -5.63908890e-02
  4.81029302e-02  2.20865279e-01 -3.15225899e-01  2.72883009e-02
  1.72151942e-02 -1.25794392e-02  2.05002949e-01 -4.63184744e-01
 -2.54630357e-01  1.86969802e-01 -1.10798523e-01  1.12732306e-01
 -6.17904589e-03 -1.18737385e-01 -1.38953909e-01 -1.68563891e-03
  4.10390981e-02 -1.40405461e-01  7.60329813e-02  2.76778758e-01
 -2.68544734e-01  1.23241790e-01  1.54235676e-01 -1.74981743e-01
 -6.80525303e-02 -6.13871440e-02  2.07639009e-01  2.36541927e-01
  4.02381659e-01 -5.30524254e-01  2.19018869e-02 -5.46035767e-02
 -1.24855429e-01  3.40029210e-01  1.55472428e-01  1.85137063e-01
 -4.19775218e-01  5.14832616e-01  1.74647212e-01 -1.78296939e-01
 -7.79935904e-03 -1.00161452e-02 -2.82770455e-01 -4.16308194e-02
  2.67672837e-01  1.84726752e-02  8.60880315e-03 -4.46199834e-01
  4.62226123e-02  1.38004124e-01 -1.55821353e-01  3.42678763e-02
 -1.58074517e-02  2.10978195e-01 -2.08529741e-01  9.23303328e-03
 -5.20224690e-01 -5.46264164e-02 -8.85951743e-02 -1.82798401e-01
  4.13276330e-02 -1.51485339e-01  2.73616910e-02 -1.78935915e-01
 -9.09116566e-02 -2.96926737e-01  4.48929012e-01  3.98104161e-01
 -2.18573764e-01 -2.58862637e-02  1.43282816e-01  1.33860216e-01
 -7.46446371e-01  5.62440455e-02 -2.29975656e-02  3.91603634e-02
  3.09616514e-03 -1.17610022e-02  8.91908482e-02  2.55562603e-01
 -3.41261804e-01  2.55853057e-01 -4.60508704e-01 -1.83092337e-03
  2.74543583e-01 -1.70992374e-01 -3.19458604e-01 -2.90878396e-03
 -8.43065679e-02  1.04961112e-01 -5.51701989e-03  3.57147098e-01
 -3.52827728e-01  4.18792486e-01  4.79288161e-01 -2.20312506e-01
 -3.29738677e-01  2.33210653e-01  6.89314604e-02 -3.17239940e-01
  1.74343586e-05  2.57826149e-02  2.51759648e-01 -1.15233228e-01]"
absl update required to slove MSVC compile error stat:awaiting response type:build/install stale subtype:windows TF 2.13,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10 22H2

### Mobile device

_No response_

### Python version

Anaconda 2023.07-1

### Bazel version

6.3.0

### GCC/compiler version

Visual Studio 2022 (build tools 14.36) + msys2-x86_64-20230718

### CUDA/cuDNN version

CUDA 11.8 + CUDNN 8.6.0 + TensorRT 8.5.3

### GPU model and memory

GTX 750 Ti 2GB

### Current behavior?

Currently, MSVC address sanitizer isn't enabled during compilation and cause compilation error. https://github.com/abseil/abseil-cpp/commit/2927340217c37328319b5869285a6dcdbc13e7a7 (LTS Jan 2023 Patch 3) has fixed it. This update required developers to check if corresponding code is necessary to change.

### Standalone code to reproduce the issue

```shell
1. download https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.13.0.zip and extract
2. comment out Windows CUDA build rejection code in configure.py
3. run `python configure.py` to configure Windows CUDA build
4. run `bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`
```


### Relevant log output

```shell
error: ""no_sanitize_address"" is undefined
```
",False,"[-3.79498184e-01 -2.40706190e-01 -1.18894070e-01  2.76868224e-01
  2.72051990e-01 -3.05653214e-01 -4.33057606e-01  2.10088208e-01
 -3.79325122e-01 -4.67316508e-01 -1.88007101e-01 -2.17914134e-01
 -1.57219619e-01 -2.22096980e-01 -2.30156928e-01  4.70144242e-01
 -2.44313598e-01 -3.33524868e-02  1.22226402e-01  1.26797542e-01
 -2.01947272e-01 -5.59062250e-02 -2.98298329e-01  1.82423145e-01
  3.19555908e-01  4.22924012e-02 -2.98360318e-01  5.08600324e-02
  1.01558529e-01  2.99414992e-01  3.13610077e-01 -1.10480390e-01
  1.78278834e-01  5.10337092e-02  2.59785831e-01 -3.12390178e-02
  3.88723686e-02 -3.70675087e-01 -3.01389575e-01 -8.51981491e-02
 -1.30943805e-01  2.60465033e-02  8.32041577e-02 -3.89956951e-01
  9.00438428e-02 -2.92972326e-01 -1.68347172e-02 -1.63267016e-01
 -8.74073356e-02 -1.48042589e-01 -3.38285491e-02  6.87987581e-02
 -2.32192189e-01 -4.49087977e-01 -1.93598866e-01 -2.21046302e-02
 -1.40692905e-01 -5.00593670e-02 -1.41404942e-01  9.83737335e-02
  2.48981684e-01 -7.29685277e-02  1.39667690e-01 -1.22249708e-01
  8.14600289e-03  2.91910946e-01  3.16099703e-01 -1.18898705e-01
  4.50935483e-01 -2.81141251e-01 -7.46130943e-02 -1.89301252e-01
 -3.47207665e-01  1.44373685e-01  2.36757323e-01  8.82746056e-02
  1.91753179e-01  7.96657056e-03  3.11437845e-01 -2.09947050e-01
  1.52056878e-02 -2.28742570e-01 -5.80860768e-03 -1.63645864e-01
  1.47294179e-01 -8.50655138e-02  3.38237286e-01  1.09060273e-01
  3.49697143e-01 -2.10497111e-01  4.98227656e-01  6.26510978e-02
  1.80838063e-01  1.77311033e-01  4.52586472e-01  9.81919914e-02
  8.97621959e-02  1.49303764e-01 -7.46464059e-02 -1.29138529e-01
 -3.47718596e-01 -1.45453542e-01 -6.44658506e-02  6.00511953e-03
 -1.16916880e-01 -2.90970653e-01  4.69206810e-01  7.39612877e-02
 -1.80922542e-02 -2.78963801e-02  3.79397392e-01  1.17218882e-01
  1.32986262e-01 -3.10754538e-01 -1.20634571e-01  1.56852920e-02
 -6.37098402e-02 -2.07788404e-02 -2.09065557e-01  5.47112823e-01
 -1.36635844e-02 -9.93801802e-02  6.59125894e-02  2.82530665e-01
  3.67943943e-01  2.50379816e-02 -1.44074321e-01  5.27569391e-02
  1.01884954e-01 -1.41829580e-01  1.92046165e-01  1.12158462e-01
  4.39505816e-01  1.03821099e-01  1.17478579e-01  7.16558099e-02
 -1.86874747e-01 -1.35111481e-01 -7.79001415e-02 -3.03803384e-01
 -2.22755939e-01  8.01199302e-02 -4.23185714e-03 -4.69397008e-01
  5.73053882e-02  1.62790865e-01 -1.68185338e-01  7.99447298e-02
 -1.32389501e-01  6.87085092e-02  1.15572244e-01  2.47931480e-01
 -2.99288392e-01  3.27772379e-01  1.69068620e-01  8.22222382e-02
  3.63110214e-01 -1.23390481e-01 -4.05610949e-02 -4.84861106e-01
 -5.20138331e-02  3.82571340e-01  4.46558148e-02 -9.51187983e-02
  1.97748244e-01  1.56281218e-01 -5.00529885e-01 -7.10667856e-03
  1.23197690e-01  6.73393846e-01 -8.53042491e-03 -2.28934973e-01
  3.30645144e-01 -1.64309964e-01  4.75005955e-01  1.42727047e-04
  1.31864786e-01 -1.49097547e-01  4.20585424e-02  3.79293233e-01
  1.88682433e-02  1.11068726e-01  2.20210344e-01  1.01476550e-01
  9.46886092e-02  2.09581330e-01  2.28758454e-01  3.35131288e-01
 -1.24204502e-01  7.44444504e-02 -3.48178625e-01  2.19747275e-02
  1.04183547e-01  3.79579403e-02 -1.12946793e-01  2.35229164e-01
  1.27355784e-01  4.28893007e-02 -3.73625271e-02  8.19170848e-02
 -3.39005589e-01  8.97789374e-02 -1.69487774e-01  6.53089732e-02
  2.84146786e-01 -2.41858482e-01 -1.65193658e-02 -3.60849917e-01
 -1.77234769e-01  8.22373629e-02  1.79745674e-01 -3.81149888e-01
 -1.16836071e-01 -5.36908209e-03 -3.56032282e-01  3.40947479e-01
  2.57108718e-01 -1.98296383e-02 -5.83637208e-02  2.92622801e-02
  3.97157855e-02 -1.69134080e-01  1.06400466e-02 -2.86821753e-01
  1.45008892e-01 -5.03798909e-02 -4.71793830e-01 -1.23496376e-01
 -8.63328800e-02  7.87748173e-02  4.38926108e-02  6.20313510e-02
  1.45877957e-01  1.01611935e-01  5.14525175e-01 -2.03969106e-01
 -1.58203393e-01 -1.63378954e-01 -6.02843463e-02 -4.89951670e-02
 -3.70944083e-01 -1.78907067e-01 -1.11801617e-01  9.83107388e-02
  3.37179333e-01  6.06965065e-01  9.40928236e-03 -2.50622094e-01
 -3.22576731e-01  3.28564122e-02 -2.58735389e-01  1.23692431e-01
  4.32230830e-01 -6.29675612e-02  6.84065670e-02  3.22077632e-01
  1.94900006e-01  7.17899948e-02  9.27792192e-02 -1.82903394e-01
  3.32652003e-01  1.30598843e-01 -7.00884610e-02  1.45064577e-01
  2.21658856e-01  3.55511636e-01 -4.38479096e-01  4.66616929e-01
  4.52614725e-02 -1.63811818e-02  2.26437867e-01 -4.38669950e-01
  5.75094640e-01 -5.74566960e-01  7.07036555e-02 -9.48339850e-02
  4.01291907e-01 -4.32297848e-02 -9.62066427e-02 -2.64650770e-02
  1.06761701e-01  3.02369058e-01 -3.15581650e-01  9.53307301e-02
 -2.27598883e-02 -1.07842959e-01 -1.00827768e-01 -5.73390603e-01
 -9.54708233e-02  1.44767821e-01 -2.94663966e-01 -1.46073729e-01
 -3.62926364e-01 -1.18776105e-01 -1.82010889e-01 -3.40461582e-02
 -2.48816628e-02 -1.91379622e-01 -1.18232653e-01 -2.50356961e-02
 -2.97587097e-01  1.51561528e-01  2.87932932e-01 -3.76239955e-01
 -3.73339027e-01 -2.94426203e-01  2.96887398e-01  3.91821027e-01
  2.85373747e-01 -4.81692523e-01  2.46102989e-01 -1.57396600e-01
 -2.73320347e-01  4.91985708e-01 -1.28698394e-01 -8.34637880e-02
 -2.98483491e-01  6.52676046e-01  2.58512378e-01 -6.67888299e-02
  6.02964796e-02 -9.51739252e-02 -3.96369219e-01  3.33924919e-01
  2.95111328e-01  1.82907343e-01 -4.07793671e-01 -3.34547997e-01
  1.38995677e-01  2.63124704e-01 -1.13646286e-02  1.16007805e-01
 -1.14207253e-01 -1.61626823e-02 -2.45742917e-01 -1.34697080e-01
 -3.44847620e-01  3.11541587e-01  6.60425425e-02 -2.26595506e-01
 -2.27475077e-01 -1.54695526e-01  7.12445006e-02 -6.28446117e-02
  1.49543852e-01 -1.77603990e-01  2.39983916e-01  3.45299602e-01
  1.48665786e-01  1.86296791e-01 -4.99316305e-02  3.70785110e-02
 -2.60057628e-01 -5.23665920e-02 -8.12510848e-02  2.57934242e-01
  1.31628364e-01 -6.16389401e-02  1.01547904e-01  7.06397071e-02
 -8.30919482e-03  2.88233995e-01 -3.95069122e-01  9.58859175e-03
  2.11476177e-01 -1.99904859e-01 -3.06125611e-01 -4.61841598e-02
  8.39844048e-02 -5.42240404e-03  6.82872608e-02 -1.09328531e-01
 -1.82141379e-01  4.51926112e-01  3.55466455e-01 -2.66515732e-01
 -1.45356447e-01 -5.07394373e-02  1.36370778e-01  1.99616805e-01
  1.17842779e-01 -1.40598595e-01  2.12536216e-01  4.19944003e-02]"
`FAKE_REQUIRED_PACKAGES` tensorflow-intel prevent poetry installation of `tensorflow-rocm` awaiting review type:build/install subtype: ubuntu/linux TF 2.12,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0.560

### Custom code

Yes

### OS platform and distribution

Archlinux 6.1.38-2-lts

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

AMD Radeon RX 6700 XT - gfx1031

### Current behavior?

Updating  `tensorflow-rocm` using `poetry` produce a not resolvable dependency error due to `tensorflow-intel` ""fake required package"". 
The problem is due to the following lines:
https://github.com/tensorflow/tensorflow/blob/6d2f5ac299ef81e3bcd0a431b2375ebbd8252708/tensorflow/tools/pip_package/setup.py#L140-L142
https://github.com/tensorflow/tensorflow/blob/6d2f5ac299ef81e3bcd0a431b2375ebbd8252708/tensorflow/tools/pip_package/setup.py#L143-L144
In particular, the error is caused by the `_VERSION`. In fact, the same version of `tensorflow-rocm` (`2.12.0.560`) not exists in `tensorflow-intel`.


### Standalone code to reproduce the issue

```shell
poetry new fixme
cd fixme
poetry add tensorflow-rocm==""2.12.0.560""
```

### Relevant log output

```shell
Because tensorflow-rocm (2.12.0.560) depends on tensorflow-intel (2.12.0.560) which doesn't match any versions, tensorflow-rocm is forbidden.
So, because lstm-predictor depends on tensorflow-rocm (2.12.0.560), version solving failed.
```


### Possible fix
Simply truncate the version to the patch version before the FAKE_REQUIRED_PACKAGES list definition.
```python
# _VERSION=""2.12.0.560""
_VERSION = (""."").join(_VERSION.split(""."")[:3])
```

### Other open issue
I open the same issue on [ROCmSoftwarePlatform/tensorflow-upstream](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream) [here](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/2161#issue-1809512685)",False,"[-0.48581016 -0.6762879  -0.0864249   0.21300328  0.23763895 -0.34108865
 -0.15530246 -0.15845472 -0.27604258 -0.31537673  0.13017792  0.03861283
 -0.30545986  0.07698354 -0.35131076  0.28797364  0.03130596 -0.25695643
  0.33357033  0.1431132  -0.00355393 -0.09513236 -0.24086675  0.26918226
  0.22270003  0.23929411 -0.17982882 -0.00548896  0.20357615 -0.03088538
  0.6035271   0.04975774  0.1553628   0.08931842  0.2412883   0.2915362
 -0.23440257 -0.31875852  0.12694061 -0.25488225  0.02813715 -0.09178831
  0.28321832 -0.1689809  -0.01588131 -0.18851411  0.16687883 -0.14978465
  0.07172774 -0.45894793 -0.10321186 -0.21233872 -0.43901044 -0.19526488
 -0.14989725 -0.08289426  0.3913964  -0.07505625 -0.07938856  0.25421387
  0.12119098  0.08884554 -0.07641351  0.1650782  -0.2131502   0.3902278
  0.32332224 -0.18143523  0.70252836 -0.1811074   0.21310475 -0.09253257
 -0.31954974  0.24769682  0.01650087  0.31350034  0.31598797  0.06781095
  0.15363574 -0.10704369 -0.1556519  -0.15694329  0.20551354 -0.10149156
 -0.00557285 -0.13323325  0.27041417  0.24788469  0.33877087 -0.33469665
  0.3726511   0.24096511  0.16178861  0.13425982  0.40988982 -0.1266886
 -0.0297089   0.14436859 -0.02822953 -0.13797162 -0.11753332 -0.08806242
  0.03042974  0.0326576  -0.16473505  0.03698664  0.35262185 -0.13325597
  0.03706624 -0.04684658  0.12715095  0.01430333  0.16890344  0.05735214
 -0.06559183  0.06133936 -0.41082668 -0.03808497 -0.04823586  0.87492514
 -0.14957845 -0.07432458  0.32822338  0.1818343   0.3882391   0.07858581
 -0.21131864 -0.17037487  0.17046289  0.03556783 -0.06203477  0.19082361
  0.14952102  0.23802899  0.14965612  0.06733447 -0.25871354 -0.26740646
  0.1795553  -0.1390551  -0.16507894  0.32724765 -0.2694844  -0.6086061
  0.20865056  0.16657622 -0.14274423  0.21039999 -0.10743507 -0.10690448
 -0.00123948 -0.01905567 -0.08121853  0.3813434   0.15607375  0.24884707
  0.5081672  -0.06997161 -0.19491744 -0.45674855 -0.00586687  0.36550468
 -0.21206918 -0.1660712   0.01821405  0.03548278 -0.4531817  -0.26823965
  0.20566222  0.57630944 -0.17709139 -0.08822337  0.17256078  0.22369781
  0.22354718 -0.19697052  0.5504355  -0.7660762  -0.08404307  0.41673928
  0.06339544  0.15876715  0.31750274 -0.05162234  0.12204549  0.01482843
  0.2032282   0.00607875 -0.09222633  0.07433522 -0.47420323  0.14307813
  0.3161131  -0.2777393  -0.2681025   0.1884782   0.22965166 -0.00365313
 -0.18153226 -0.15033108 -0.04863708 -0.10988131 -0.30977336  0.13995257
 -0.08121597 -0.33384296 -0.10265638 -0.21669629 -0.5305742  -0.19972298
  0.01614768 -0.38130653  0.15977533 -0.03087822 -0.33044216  0.04039369
  0.0075396  -0.04156825 -0.17803878  0.30342025  0.04492296 -0.35076755
 -0.12329273 -0.3577146  -0.1251765  -0.21819195 -0.22395042  0.02747255
 -0.12067549  0.22908738  0.03274253  0.00516358  0.52667737  0.14697267
  0.32884586  0.08253364 -0.15067269 -0.23999694 -0.21522442  0.18890285
 -0.26350054 -0.13294473  0.17604272  0.17840761  0.22810754  0.40211028
 -0.2254126  -0.14478761 -0.25198132  0.43441325 -0.11859441  0.10573698
  0.24791154  0.18123783  0.2472773   0.32752526 -0.05038273  0.02360834
  0.17205457 -0.09429615  0.16407362  0.24953184 -0.09011813  0.34987324
  0.28407824  0.4451089  -0.5889676   0.3577593  -0.05333422 -0.07816273
 -0.25169414 -0.26910853  0.4121294  -0.39305562 -0.12011577 -0.10293581
  0.48386616 -0.10969058  0.09624943 -0.03480156  0.02811803  0.3989778
 -0.4566748   0.09257337  0.06145119 -0.28280005  0.09877066 -0.8094431
 -0.26180375  0.22285096 -0.1434712   0.3021285   0.05322818  0.0601951
 -0.14848402  0.08329199  0.09969585  0.19776735  0.07861081  0.33416724
 -0.14740747 -0.02126893  0.34564543 -0.3846208  -0.0672397  -0.14457658
  0.12028625  0.30178025  0.49166656 -0.5177629   0.22344366 -0.05342223
  0.05170956  0.37373078  0.03479787  0.12816888 -0.25665992  0.5354425
  0.20861551 -0.35218644  0.27385175 -0.28422445 -0.3099524   0.01470849
  0.11921474 -0.23378813 -0.255826   -0.50362027 -0.19090392  0.10773766
 -0.04581005 -0.08184835  0.03117305  0.20639077 -0.37230593 -0.10184523
 -0.32672542  0.23714739 -0.16185623 -0.45176893 -0.05317402  0.17908336
 -0.126451   -0.47439048 -0.09665176 -0.18339786  0.6403045   0.15207714
 -0.35937807  0.13642955 -0.12222695  0.3066792  -0.54925233  0.02088059
  0.16184285  0.21733941 -0.06462159 -0.19310445  0.4047734   0.2554947
 -0.14513853  0.05040503 -0.38321763 -0.06644551  0.25022653 -0.2866615
 -0.15590638 -0.09320752  0.12587227  0.6507448   0.02244251  0.32919976
 -0.3423212   0.32563186  0.6070075  -0.44663894 -0.29991636  0.24233976
 -0.07525372 -0.11846679  0.12388279 -0.06723906  0.29885986 -0.16964832]"
Since one week with a few different issues and two platforms I can't import Keras stat:awaiting response type:build/install subtype:macOS,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

SystemError: initialization of _pywrap_checkpoint_reader raised unreported exception

### Custom code

No

### OS platform and distribution

Mac13.4 

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I face the error ""SystemError: initialization of _pywrap_checkpoint_reader raised unreported exception"" when I import keras to build DL model! I worked on make sure that the TensorFlow library is installed also I update the TensorFlow library to the latest version.
I need I help to resolve this issue because I think I trying a lot of recommendations!
Thanks

### Standalone code to reproduce the issue

```shell
Update the TensorFlow library to the latest version.
```


### Relevant log output

_No response_",False,"[-1.35481507e-01 -4.19576228e-01 -3.38340223e-01 -2.23721303e-02
  2.90188193e-01 -3.78981829e-01 -1.73124567e-01 -5.27922288e-02
 -3.74447733e-01 -2.48309091e-01  2.83879161e-01  2.17091646e-02
 -1.42895147e-01  2.66843885e-02 -2.16395289e-01  2.34401554e-01
 -2.79615790e-01 -1.83415279e-01  2.46320486e-01  2.29997039e-01
 -1.81267440e-01 -1.07144386e-01 -1.84469163e-01  1.01591036e-01
  3.74288089e-03  2.29869172e-01 -1.28932014e-01  5.47405099e-03
 -4.46314998e-02  7.76223242e-02  4.32640046e-01  1.23356059e-01
 -1.41698001e-02  6.97627366e-02  1.76904470e-01  1.15087762e-01
  2.26034131e-02 -1.53775126e-01 -2.18250588e-01  1.86628383e-02
  1.67445570e-01  1.31237298e-01 -6.69484884e-02 -1.32478893e-01
  5.59220724e-02 -3.15816939e-01  4.32307757e-02 -2.67781705e-01
  1.75121240e-02 -4.19771910e-01 -8.45909715e-02 -1.33533413e-02
 -1.81685388e-01 -4.87385869e-01 -1.24619320e-01 -1.41896516e-01
  8.00431743e-02  1.94274157e-01  2.21747793e-02  1.53977156e-01
  9.27875489e-02 -1.21986624e-02  1.18474692e-01 -8.36681947e-02
  1.69217944e-01 -7.25360513e-02  1.64643258e-01  9.69870761e-02
  4.66255784e-01 -3.24508071e-01  1.01391733e-01  1.38312252e-03
 -4.07765567e-01  7.63685852e-02  1.03667051e-01  3.45648527e-01
 -6.17471114e-02 -1.49882227e-01  3.31018329e-01 -2.45368153e-01
 -2.64941782e-01 -1.40683144e-01 -9.32502374e-02 -1.35500893e-01
  2.20992148e-01 -7.99424276e-02  1.85444266e-01  1.32501394e-01
  4.83166456e-01 -7.12484419e-02  6.84541702e-01  4.33797985e-01
  5.07575907e-02  1.04232743e-01  3.82837713e-01  5.80682829e-02
  1.80432662e-01  2.66620606e-01 -1.46921277e-01 -8.33652094e-02
  7.04694837e-02 -3.00776005e-01 -1.30612224e-01  1.72358286e-02
  5.99788390e-02 -1.01824746e-01  2.74239451e-01 -2.83526242e-01
 -6.45567384e-03 -5.23631461e-02  2.51579046e-01  5.66599742e-02
  1.86273873e-01  9.27799046e-02  3.38981561e-02  8.80150199e-02
 -2.16184556e-01  1.92792892e-01 -6.74117953e-02  7.37267613e-01
  1.00659542e-02 -7.83043057e-02  1.82286501e-02  2.70280480e-01
  2.78608203e-01  2.77685791e-01 -1.64461866e-01  1.27591908e-01
 -7.68550038e-02 -2.82439068e-02 -6.88065886e-02 -7.35467747e-02
  2.50072241e-01  9.86709744e-02 -2.42471434e-02 -7.79163912e-02
 -7.08208680e-02 -2.22307146e-01 -3.32854003e-01 -1.67052329e-01
 -1.83165908e-01  4.57316935e-02  8.70041922e-02 -4.87693727e-01
  1.53681040e-01  2.10729793e-01 -1.47957802e-01  1.28393933e-01
  2.97976360e-02 -1.79477409e-01 -4.65545356e-02  2.11125370e-02
 -2.51991004e-01  4.44622278e-01  3.15787196e-02 -1.95941254e-02
  3.73197824e-01  3.84145379e-02 -1.03607826e-01 -4.90560412e-01
  3.54228690e-02  3.68908525e-01 -1.34983808e-02 -7.13481940e-03
  3.89471054e-01 -7.67456889e-02 -4.06397223e-01 -3.08170736e-01
 -7.28795826e-02  1.98386356e-01 -1.08704761e-01  8.28736424e-02
 -8.47334974e-04 -1.40643775e-01  1.45035475e-01  4.24091145e-02
  1.18702419e-01 -4.40405935e-01 -1.70550510e-01  2.38571882e-01
  1.00391485e-01 -9.24256165e-03 -1.30541220e-01  3.68711770e-01
  4.94035780e-02 -1.33074690e-02  1.23173997e-01 -3.67810838e-02
 -1.98820978e-01  2.53170848e-01 -3.16967607e-01  8.52733254e-02
  3.92619729e-01 -3.30033869e-01 -1.10720798e-01  4.90382090e-02
  9.99266803e-02 -1.99883096e-02  7.41548184e-03 -3.25322971e-02
 -2.54738033e-01 -5.88789247e-02 -1.63846493e-01 -6.58146106e-03
  2.38406770e-02 -3.58530968e-01 -2.13405386e-01 -2.97660798e-01
 -5.10196209e-01  1.29670978e-01 -1.21921273e-02 -4.16198254e-01
  2.71184623e-01  4.70479988e-02 -2.55198210e-01  1.84609696e-01
  2.00864226e-01  1.46471456e-01  9.83117744e-02  1.30455598e-01
 -1.01022609e-01 -4.04580712e-01 -1.25374645e-01 -2.97084332e-01
 -1.97141208e-02  5.63564524e-02  1.65832043e-03 -8.78932476e-02
  4.53841127e-03  1.89221203e-01 -9.69180614e-02  7.00081438e-02
  4.24233019e-01  3.51724140e-02  2.03621373e-01 -2.16094121e-01
  1.81026608e-01 -9.42666680e-02 -4.29877825e-03 -3.72157805e-03
 -5.77629924e-01  1.76156640e-01  7.22481087e-02 -1.61936760e-01
  1.12870187e-01  8.41512457e-02 -1.73329264e-01 -3.97256762e-02
 -3.75504673e-01 -5.58616593e-04 -1.07655123e-01  1.61395699e-01
  3.45437765e-01  1.37182325e-01  3.84821653e-01  2.66311109e-01
  2.76428144e-02  1.62588432e-01  2.12608576e-01  3.78289223e-02
  3.56126845e-01 -9.77603495e-02  6.58735186e-02  3.69451493e-01
  1.09496534e-01  1.66526198e-01 -5.05175412e-01  4.66342777e-01
 -1.39291529e-02 -1.94057614e-01  1.99077368e-01 -1.09794885e-01
  6.40295029e-01 -3.45954597e-01  1.33582443e-01  1.19453877e-01
  3.65604937e-01  2.04884969e-02  9.67863500e-02 -8.71434584e-02
  1.29243195e-01  3.24719906e-01 -4.93029833e-01 -1.36628583e-01
  1.25481738e-02 -3.05648685e-01 -2.39593893e-01 -5.40670633e-01
 -7.78860152e-02  8.77609849e-02 -1.07539251e-01  4.49477397e-02
  3.20179313e-02  8.68356880e-03 -2.38465574e-02  1.55344367e-01
 -5.08522503e-02 -2.24780783e-01  6.12532794e-02  1.43120095e-01
 -4.65477318e-01  1.35933161e-01  4.57122922e-01 -3.52413595e-01
 -9.57049057e-02 -1.64645970e-01  2.37092778e-01  2.01155052e-01
  5.96476436e-01 -2.94186771e-01  3.09787631e-01  1.26082793e-01
 -4.70016524e-02  4.17353749e-01  1.21663749e-01 -1.50592059e-01
 -4.07209605e-01  6.57277644e-01  2.91089211e-02 -1.09655797e-01
  2.54777908e-01  7.85897523e-02 -3.81934643e-01  2.53826559e-01
  2.45900825e-01  6.31905198e-02 -2.74678081e-01 -3.69622678e-01
  8.23726412e-04  1.39546663e-01 -3.60562019e-02  9.77464169e-02
 -9.98310596e-02  1.30873397e-01 -1.44759715e-01  5.71538880e-02
 -1.95577055e-01  3.02727997e-01 -9.80320126e-02 -3.33907485e-01
 -1.05407111e-01  2.70619672e-02  8.36753845e-02 -2.74373889e-01
  5.21438830e-02 -2.35877603e-01  2.79895782e-01  5.09464502e-01
 -3.56209418e-03  6.54719099e-02 -1.21282332e-01 -5.92104383e-02
 -5.18810213e-01  1.20578147e-01  1.98608786e-02  3.78150463e-01
 -2.01793984e-01 -2.25932710e-02  3.20803285e-01  3.29421908e-01
 -3.47817481e-01  7.01268483e-03 -4.47188914e-01 -4.59677279e-02
  4.44353633e-02 -3.76041718e-02 -4.64069307e-01 -6.63064271e-02
  3.74962501e-02  1.99540645e-01 -2.17007548e-01  1.52922690e-01
 -1.69467211e-01  3.66794527e-01  2.69171774e-01 -6.23013191e-02
 -2.15207607e-01  7.43086115e-02  6.73953444e-02 -8.41910839e-02
  1.78957209e-01 -1.23396888e-02 -5.58685102e-02 -2.24193819e-02]"
tflite-model-maker installation issue stat:awaiting response type:build/install stale comp:lite TF 2.12 TFLiteModelMaker,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

For Image Classification

### Standalone code to reproduce the issue

```shell
error: subprocess-exited-with-error
  
   Getting requirements to build wheel did not run successfully.
   exit code: 1
  > See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Getting requirements to build wheel ... error
error: subprocess-exited-with-error

 Getting requirements to build wheel did not run successfully.
 exit code: 1
> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
```


### Relevant log output

_No response_",False,"[-5.64808428e-01 -2.80816048e-01  6.34212941e-02  2.55605936e-01
  3.69696259e-01 -2.77421415e-01 -1.81902200e-01  4.19822782e-02
 -2.49280408e-01 -2.30619758e-01  8.35982040e-02  4.65955958e-02
 -2.39121974e-01  1.49897426e-01 -2.32146740e-01  4.15850043e-01
 -2.77572095e-01 -4.05236006e-01  3.45629215e-01  2.99006760e-01
 -1.89197540e-01 -1.62498385e-01 -3.39977920e-01  1.29147485e-01
  1.70861959e-01  3.05769324e-01 -3.21755290e-01  3.03416997e-01
  8.61563534e-03  2.90500015e-01  2.41713569e-01  1.06418729e-02
 -8.86160433e-02  2.38865942e-01  2.91304231e-01  1.20804600e-01
 -3.38119477e-01 -2.73592860e-01 -3.12059969e-01 -1.36702672e-01
  5.11398278e-02  5.65959103e-02  7.29531795e-02 -2.65684217e-01
 -6.68911962e-04 -1.46782603e-02  1.91889241e-01 -7.49163032e-02
 -1.94700062e-01 -3.42086792e-01  3.39075550e-02 -1.66265801e-01
 -3.37283373e-01 -3.75194728e-01 -8.11472610e-02  9.78456885e-02
  2.87632167e-01 -3.52692232e-02 -5.38115650e-02  1.98473543e-01
  1.76391557e-01 -1.02765873e-01 -1.48687109e-01 -1.79035574e-01
  2.71710008e-02  2.65113324e-01 -8.75458568e-02 -1.11154288e-01
  3.35488379e-01 -2.95389891e-01  1.00629911e-01 -5.43806590e-02
 -1.83783293e-01  1.39640212e-01 -8.63978118e-02  1.00337891e-02
  2.52854936e-02  1.56578898e-01  2.44211748e-01 -5.64770177e-02
 -3.40534747e-02 -4.97838289e-01 -3.16202819e-01 -7.06255734e-02
  1.09497830e-01 -2.34092727e-01  3.11402470e-01  2.59729594e-01
  5.12189507e-01 -3.12788010e-01  4.77178216e-01  7.05170274e-01
  1.18408084e-01  1.22070976e-01  4.53847528e-01  2.90191203e-01
  1.36021510e-01  3.71863514e-01 -2.36178599e-02 -8.42716768e-02
 -7.67539665e-02 -3.12827468e-01 -1.41875809e-02 -3.46603505e-02
 -1.33760780e-01 -2.34972849e-01  2.01524347e-01  8.59407708e-03
  1.13411946e-02 -8.29222351e-02  2.22250164e-01 -8.14755559e-02
  1.99553862e-01  5.03048562e-02 -3.89179252e-02  6.95817396e-02
 -2.30113149e-01  9.11953673e-02 -1.50436759e-02  4.78809386e-01
  7.62415156e-02 -1.45005435e-01  1.34318113e-01 -1.37606133e-02
  5.23209214e-01  6.16770331e-03 -3.02340806e-01 -7.80082271e-02
  1.22487329e-01  3.05257201e-01 -2.03508660e-01  2.28473812e-01
 -1.08776065e-02  2.61723995e-01 -7.00372458e-02  1.06147557e-01
 -1.00852571e-01 -1.62232161e-01 -1.19645178e-01 -2.02289909e-01
 -2.91592687e-01  1.83741614e-01 -9.99229401e-02 -6.04442835e-01
  8.43757838e-02  3.75452712e-02 -2.79269964e-01  2.07481384e-01
 -1.71124965e-01  1.22125126e-01 -9.28026885e-02  2.36932337e-01
 -2.19153434e-01  4.03099805e-01  1.03294864e-01 -1.37762308e-01
  3.13958466e-01 -2.34154165e-02  3.60601135e-02 -3.10529679e-01
  7.56120961e-03  3.72618079e-01  5.20556793e-02 -1.31208509e-01
 -5.70561737e-02  1.41629562e-01 -5.65131903e-01 -1.92715734e-01
  3.54986370e-01  3.73720706e-01 -6.98662996e-02 -9.78968292e-02
  2.47807682e-01  7.29492754e-02  1.51948765e-01 -6.26867190e-02
  6.28294230e-01 -4.84956861e-01 -1.24915361e-01  2.95579374e-01
  2.07699403e-01  1.82427689e-01  9.86272246e-02  6.92842305e-02
  1.21311776e-01 -1.14878111e-01  1.50402412e-01  2.31420230e-02
 -2.87662804e-01 -8.80608559e-02 -5.83431005e-01 -1.37954829e-02
  5.13864279e-01 -2.22791918e-03 -1.58066183e-01  1.22983098e-01
  1.65337056e-01  2.45068222e-02  1.54126450e-01 -1.97739363e-01
 -1.40746266e-01  1.07175604e-01 -2.66046703e-01 -2.11538330e-01
  2.46828515e-03 -1.75382942e-01 -5.25772125e-02 -3.79811019e-01
 -4.27801818e-01 -1.85288489e-01  6.92657977e-02 -5.01053154e-01
  1.98243886e-01 -1.71857223e-01 -2.94759870e-01  4.61471826e-01
 -2.37007719e-03 -2.38967873e-03 -1.52320638e-02  1.63715571e-01
  3.87625322e-02 -1.85170874e-01 -5.91620579e-02 -4.12459254e-01
 -3.05799127e-01  1.59995526e-01 -2.28189915e-01  1.95889622e-01
  1.17514029e-01  6.31192774e-02 -8.34472328e-02 -3.17347683e-02
  6.45725012e-01  8.76771733e-02  3.91924500e-01  4.82881442e-04
 -5.56367785e-02 -1.56870008e-01 -2.06236735e-01  1.23129450e-01
 -3.07131767e-01 -4.88523334e-01 -8.29388760e-03 -4.01556045e-02
  2.68340707e-01  3.28974277e-01 -3.02503943e-01 -2.86583453e-02
 -3.49709511e-01  3.77328128e-01 -2.61524934e-02  6.96043968e-02
  3.63208592e-01  2.61257172e-01  3.36688071e-01  1.86771974e-01
  1.45526558e-01  1.53173566e-01  1.68504551e-01 -8.03769603e-02
  3.04736614e-01  2.75699943e-01  1.46429136e-01  4.15468454e-01
  3.01149189e-01  1.39925778e-01 -4.81231809e-01  4.02337164e-01
 -3.73707190e-02 -2.52113581e-01  1.85770631e-01 -4.66284513e-01
  4.80225533e-01 -3.27332586e-01 -2.54241750e-02 -2.01446842e-02
  4.76898074e-01 -1.13844402e-01 -7.01599056e-03  7.13834614e-02
  1.09511629e-01  3.44535798e-01 -4.33779359e-01  1.44316733e-01
  2.75084496e-01  6.20596632e-02  1.29761934e-01 -7.70332932e-01
 -4.40702677e-01  2.48882174e-01 -3.72032881e-01  2.11259604e-01
 -1.48700416e-01  1.16808526e-01 -4.29899067e-01  3.19278091e-02
  3.05537879e-02 -3.39478552e-01  1.14831224e-01  1.36806458e-01
 -3.82677555e-01  3.53926718e-01  5.24956346e-01 -5.60443044e-01
 -1.21171214e-01 -3.52656394e-02  2.36676231e-01  9.99462754e-02
  4.13296640e-01 -6.25853837e-01  2.86440309e-02 -9.49842781e-02
  2.70314291e-02  4.08142805e-01  1.50225103e-01 -2.96147428e-02
 -2.99123615e-01  5.24348021e-01 -1.56538188e-02 -1.78216875e-01
  3.60937834e-01 -4.89822552e-02 -2.02366099e-01 -9.82188955e-02
  5.29318273e-01 -9.90445763e-02  1.16415203e-01 -2.82893538e-01
  1.63063526e-01  9.98714119e-02 -1.27642617e-01 -1.03955634e-01
 -2.90685706e-02  1.14094131e-01 -2.90065706e-01 -1.39344737e-01
 -3.39305758e-01  2.60522634e-01 -1.08018965e-01 -3.46895218e-01
 -1.51742652e-01 -2.80164480e-01 -1.42687783e-01 -1.71497688e-01
  1.12002455e-01 -1.91784233e-01  2.55349576e-01  3.32317472e-01
 -1.27337635e-01  2.08431259e-01 -5.51738124e-03  1.25303060e-01
 -3.33187997e-01  1.72753513e-01  9.23935473e-02  1.97712943e-01
 -5.70103303e-02 -1.15274444e-01  4.16728795e-01  2.84642011e-01
 -3.49593833e-02  3.43213379e-01 -4.55562115e-01 -3.84696908e-02
  6.68272227e-02 -2.23951906e-01 -4.01590824e-01 -2.26316452e-01
  1.53376758e-02  3.77894074e-01 -1.30574450e-01  3.42332691e-01
 -4.28593278e-01  1.57895297e-01  5.37228823e-01 -3.38570893e-01
 -8.17513987e-02  2.94207692e-01  1.08701482e-01 -1.28303602e-01
  4.65751626e-04 -1.92649543e-01  2.43477330e-01  9.24523324e-02]"
Tflite: C++ API format to add NNAPI delegate stat:awaiting response type:support stale comp:lite comp:apis TFLiteNNAPIDelegate TF 2.10,"
### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.10
-   **Python version**: 3.10


### Describe the problem
I want to know what is the python API equivalent of adding a tflite delegate to execute the model. 
In python, we can directly add the argument 'experimental_delegates' to tflite.Interpreter and provide the path to the delegate .so file. 

If i want to add NNAPI or GPU delegate when using C++ API, what is the command for that? I couldn't find effective documentation to enable a delegate when using C++.

ModifyGraphWithDelegate is used, but how do i define a delegate up here? I want to make use of NNAPI delegate and i have the .so file for the same as well. Below is the code snippet am using to enable NNAPI delegate

```
  std::map<std::string, tflite::Interpreter::TfLiteDelegatePtr> delegates;
    auto delegate = tflite::Interpreter::TfLiteDelegatePtr(tflite::NnApiDelegate(), [](TfLiteDelegate*) {});
    delegates.emplace(""NNAPI"", std::move(delegate));
    for (const auto& delegate : delegates) {
        interpreter->ModifyGraphWithDelegate(delegate.second.get());
    } 
```

but when i compile the code i get the error:

>  undefined reference to tflite::NnApiDelegate()'

How can i enable NNAPI delegate with C++?

thanks


",False,"[-3.52853924e-01 -4.98534828e-01 -4.18422431e-01  6.12380579e-02
  4.11156798e-03  8.74685496e-02  8.10818225e-02 -5.50586805e-02
 -1.78161755e-01  1.50034707e-02 -1.08629569e-01 -1.97583765e-01
 -1.22074924e-01  3.04500282e-01 -9.55017358e-02  2.80694187e-01
 -1.50226027e-01 -2.35092551e-01  3.14454168e-01 -8.33414048e-02
  7.14306831e-02 -8.44933558e-04 -2.76381284e-01  3.51388216e-01
  2.52142221e-01  1.27217978e-01 -2.03221321e-01 -7.18798488e-02
 -2.45447997e-02  1.20219678e-01  2.39548713e-01  1.03674635e-01
 -1.76454708e-01  1.09273959e-02 -1.34930655e-01  1.00902230e-01
 -2.42155716e-01 -5.44647798e-02 -3.13238859e-01 -8.21710974e-02
  2.11573914e-02  3.25212836e-01 -1.94711946e-02  1.19409561e-01
 -1.03468448e-01  9.90846008e-02  7.83220399e-03  2.35241398e-01
 -2.07259893e-01 -4.39271219e-02  4.50065285e-02  1.16750132e-02
 -3.77467871e-01 -1.42100587e-01 -4.85679135e-03  3.43271419e-02
  5.90068921e-02 -1.90712303e-01  8.77663642e-02  1.26702368e-01
  6.94539696e-02 -2.22659428e-02 -7.77001455e-02  1.47970051e-01
 -6.65123165e-02  3.88771266e-01 -1.28858816e-02 -1.66911379e-01
  3.06612790e-01 -2.36398667e-01 -9.16069373e-02 -4.06657904e-02
  2.56513320e-02 -1.04551027e-02  5.10078669e-02 -4.76407558e-02
 -3.38615030e-02  2.76076972e-01  4.06262577e-01 -6.02131300e-02
  1.37984425e-01 -1.62508711e-02  2.94270478e-02  6.89502656e-02
  1.04703560e-01  2.15773880e-02  4.18495126e-02  2.41339445e-01
  3.65514815e-01 -2.09815055e-03  3.95140976e-01  4.87207085e-01
  4.00336310e-02  1.08424865e-01  3.24240535e-01  1.01357497e-01
  1.25845432e-01  3.36093009e-01 -9.73072499e-02  2.16139480e-02
 -6.71935603e-02 -1.57837391e-01 -2.23347396e-01 -2.79981233e-02
 -5.08049354e-02 -2.41395831e-01  2.53552556e-01 -7.20978007e-02
 -7.35727847e-02  6.32333532e-02  1.33195788e-01 -1.23491455e-02
  1.59994036e-01 -1.30121678e-01  7.94027597e-02  2.32003719e-01
 -1.14145257e-01  1.08870849e-01  5.64399064e-02  4.37526494e-01
  4.87644784e-02 -2.04558775e-01  4.07399163e-02  3.10795486e-01
  4.98977363e-01  1.80261910e-01 -1.80471599e-01 -1.36913016e-01
  1.35257080e-01  2.67647535e-01  1.34626418e-01  2.89895654e-01
 -2.13914961e-01  6.47184849e-02 -1.04106605e-01 -2.09904581e-01
 -2.75397480e-01 -6.28008768e-02 -3.50849628e-01 -4.51580808e-02
 -1.57154962e-01  2.55992115e-01 -5.58850467e-02 -2.14122891e-01
 -6.46738410e-02  1.06845900e-01  2.67272126e-02  1.29178509e-01
 -6.34564012e-02  3.73300493e-01 -1.02914482e-01  1.76194176e-01
 -1.16300501e-01  3.10248315e-01 -4.03863788e-02  2.86297332e-02
  1.39288351e-01 -2.41235644e-02  1.02411650e-01 -5.33947110e-01
 -4.90956642e-02  1.00488067e-01  2.04272136e-01 -1.23511121e-01
  8.24209824e-02 -2.06342340e-03 -3.84693205e-01 -2.40138203e-01
  2.08947971e-01  2.82833487e-01 -2.17434436e-01 -1.96360886e-01
 -1.20535955e-01  1.62493065e-01  2.07262009e-01 -9.88655984e-02
  2.38853127e-01 -4.81012762e-01 -4.53329496e-02 -2.49878317e-03
  2.09384322e-01  6.93559647e-02 -2.59852484e-02 -1.11696526e-01
 -3.41506600e-01 -6.78156242e-02  5.34405261e-02  3.41665670e-02
 -1.85326427e-01 -1.53825611e-01 -3.79117906e-01  4.61204089e-02
  1.40654564e-01 -3.51122059e-02 -3.42996597e-01 -1.82782650e-01
  3.59580278e-01 -1.98791884e-02  2.21324295e-01  2.18539432e-01
 -1.10656023e-04  6.25842735e-02 -1.54061645e-01 -1.16129249e-01
  2.06421480e-01 -6.73742890e-02 -2.29029551e-01 -2.44471759e-01
 -2.06536442e-01  2.18118653e-02  1.13849491e-01 -2.09437296e-01
  1.45560548e-01 -2.40632832e-01 -1.05823174e-01 -2.20147669e-01
 -6.04562536e-02 -8.22093785e-02 -2.65509486e-01  2.23267972e-02
  2.23538086e-01 -1.45450443e-01  2.33375862e-01 -3.16399366e-01
 -2.48927638e-01 -1.75709695e-01 -1.70896247e-01  4.08394188e-02
 -3.02476026e-02  1.27762139e-01  8.72856230e-02 -4.97338362e-02
  4.13798034e-01  4.78572473e-02  1.62641495e-01 -1.86487406e-01
  1.70308545e-01 -1.68086663e-01 -1.64613500e-02 -6.02437705e-02
 -4.18180436e-01 -1.79172099e-01  1.96804758e-02 -1.36691585e-01
  8.79205689e-02  4.35286611e-02 -2.57968187e-01  6.77904710e-02
  3.83739881e-02  3.03348899e-01 -1.26368523e-01 -8.86915326e-02
  3.31811070e-01  9.21038389e-02  5.04359305e-02  2.49083452e-02
 -1.43982321e-01 -1.53426677e-01  5.26057482e-02 -1.45930678e-01
  6.53774068e-02  4.60771084e-01  2.20965501e-02  5.49874663e-01
  2.74114877e-01  1.71618074e-01 -1.60540253e-01  4.05700117e-01
 -5.05152307e-02 -8.35367441e-02 -5.59111536e-02 -2.22788557e-01
  2.54118860e-01 -2.61113852e-01 -6.51294887e-02 -2.43793622e-01
  4.09998059e-01 -1.96554154e-01  1.27769336e-02  3.26705039e-01
  1.96588323e-01  3.00824106e-01 -2.39486936e-02  1.15169331e-01
 -6.48901016e-02 -3.58167827e-01 -7.91011527e-02 -4.18210626e-01
 -7.12500662e-02  2.30611637e-02 -1.15142152e-01  3.61821279e-02
  1.65949315e-02 -5.03309667e-02 -9.36549604e-02  3.93543690e-02
 -3.38327549e-02 -8.16339031e-02  9.88130942e-02  7.69580156e-02
 -6.14835136e-02  2.24850476e-01  2.40568429e-01  5.01455180e-03
 -1.13798171e-01 -3.08049750e-02  3.17227751e-01 -5.28401434e-02
  5.68297744e-01 -2.88420975e-01  2.80845702e-01  1.40070856e-01
 -1.13658626e-02  4.17278051e-01 -5.41045144e-02 -7.30369985e-02
 -2.30098814e-01  2.82061189e-01  1.67091057e-01  5.20497151e-02
  1.26671210e-01 -1.53460070e-01 -2.92138726e-01  1.63294777e-01
  1.51563421e-01  1.02649823e-01  4.50977609e-02 -1.61129475e-01
 -1.48360640e-01 -5.16425446e-03  6.49660602e-02 -2.65216589e-01
 -1.45870671e-02 -8.68195295e-02 -7.01734200e-02 -1.36430949e-01
 -3.46485496e-01  3.09091061e-01 -2.03914434e-01 -2.07710654e-01
  2.55513191e-02 -1.53425127e-01 -1.96351513e-01 -2.41116896e-01
 -1.78537667e-01 -2.43851751e-01  2.90292025e-01  3.63508642e-01
 -5.18451482e-02  2.69715220e-01 -8.41448307e-02  5.08302115e-02
 -1.74486205e-01 -1.83660567e-01 -2.65833884e-01  2.55262017e-01
 -1.48042247e-01 -1.93983525e-01  1.39659822e-01  4.04931366e-01
 -1.61029935e-01  1.84776470e-01 -3.06987226e-01 -1.64718300e-01
 -4.03649062e-02 -1.80991083e-01 -2.98459232e-01 -2.25594267e-01
  2.94948459e-01  4.49921995e-01 -1.67260975e-01  3.85293692e-01
 -2.17644885e-01  2.51360714e-01  3.13749760e-01 -1.38771206e-01
  8.44773203e-02  1.04211599e-01  3.03093027e-02 -8.72516781e-02
 -1.89344846e-02 -1.22254096e-01 -8.16085637e-02 -6.39284030e-04]"
The issue of updating a formula. type:docs-bug stat:awaiting response stale comp:keras,"https://github.com/tensorflow/tensorflow/blame/d5422e3857a3bcab5063fdd01600d4c15393c887/tensorflow/python/keras/optimizer_v2/adam.py#L443

var.assign_sub(
        (m * alpha) / (math_ops.sqrt(v) - coefficients['epsilon']))
should be
var.assign_sub(
        (m * alpha) / (math_ops.sqrt(v) + coefficients['epsilon']))",False,"[-0.29452562 -0.31795126 -0.09347723  0.24544741  0.14125179 -0.0917361
  0.13835771  0.04747514 -0.1873131  -0.18097827  0.2355986  -0.02104427
 -0.11712436 -0.14186665 -0.3795451   0.12558761 -0.46924067 -0.08280872
 -0.22733702 -0.02199866  0.03868502 -0.00886221 -0.27491516  0.22123139
  0.11734952  0.29771578 -0.19365035 -0.10776284 -0.03764786  0.28644058
  0.42685443  0.43001333  0.16364451 -0.12536322 -0.06975896 -0.07865246
 -0.1631167  -0.07973048  0.2161422   0.18043073  0.26528382 -0.10937953
  0.21705283  0.02912246  0.08293083 -0.19623649 -0.0181532  -0.13107654
  0.08493896 -0.15919879 -0.32934377 -0.05088709 -0.2574024  -0.30155823
 -0.22204301 -0.1725665   0.36323345 -0.02062936  0.2149531  -0.00921616
  0.20406483  0.2133812   0.2892757   0.01548843  0.06574997 -0.16431616
  0.10823621 -0.07946907  0.39285874  0.3483657   0.1912992   0.14278758
 -0.23119853 -0.19505912 -0.02654125  0.08075069 -0.2802312   0.18102613
  0.43682814 -0.05314924 -0.06938105 -0.28013742 -0.20588747 -0.02055121
 -0.01576396 -0.22113267  0.209253    0.17734134  0.5653527  -0.16597399
  0.60489625  0.25339487  0.00835214  0.05850612  0.40915328  0.21066433
 -0.03421611 -0.03297959 -0.14591551 -0.05251456 -0.37276998  0.05916907
 -0.44112754 -0.01559617 -0.01512187  0.09657113 -0.05826164 -0.24373454
  0.19969267  0.02600265  0.25240785 -0.21041004  0.1596502   0.22754245
  0.02238157  0.04875922  0.18227422  0.2257953  -0.2650021   0.30596823
 -0.20367835  0.09720717  0.02858256  0.34779048  0.0825666   0.05299011
 -0.33039132  0.18781069 -0.09009364 -0.05098183  0.13608605 -0.08076927
 -0.1829344   0.07057948 -0.13873865 -0.374197    0.14871316 -0.11509357
 -0.35655093  0.06342545 -0.06155239 -0.10058716 -0.06413316  0.01200447
  0.26390147  0.24675983 -0.3999173  -0.08015382  0.2124992   0.10765437
 -0.2724405  -0.09243206 -0.13262519  0.3608067   0.16658233 -0.0576346
  0.07330341 -0.14783725 -0.14272088 -0.14631912  0.05337591  0.2305853
  0.14023173 -0.03422692  0.34501198  0.23201883 -0.35528123 -0.11809269
 -0.07397524  0.25604776  0.16403937  0.17146671  0.01851545  0.19568188
  0.13710456  0.03855912  0.20422885 -0.39513236 -0.2281569   0.20356761
  0.2565061  -0.00865942  0.1717527   0.16416726 -0.14687437  0.18596289
  0.11466022  0.00148145 -0.40491557 -0.22237244 -0.16627929  0.11291923
 -0.0215295   0.10317872  0.05196195  0.01311988  0.3062818   0.13019699
  0.18492864  0.19650906 -0.6965301  -0.06569682 -0.05477743 -0.21893497
  0.2506673  -0.3623194  -0.1377577  -0.32701266 -0.26004732  0.18829001
 -0.03712124 -0.32465655  0.20799008 -0.24425721 -0.5624175   0.5173442
  0.14873664  0.18248883 -0.0978197   0.10313485 -0.12720749 -0.06808806
  0.044297   -0.285673   -0.3680467  -0.1916587  -0.0926228   0.00704788
 -0.05944033  0.13989303 -0.16578919 -0.2704205   0.65053964 -0.09267227
 -0.08955687 -0.22703776 -0.23203053 -0.06079647  0.15829948 -0.05700245
 -0.3462772  -0.13951811 -0.18895668  0.00571353  0.20642395  0.08973195
 -0.06511803  0.11169957 -0.3720734   0.05643195 -0.10681093 -0.19882676
  0.11172356  0.06386787  0.03642555 -0.03168059 -0.04352643  0.19183744
  0.13894491  0.00454808  0.35419297 -0.01667967 -0.22321822  0.5408875
  0.2768203   0.19621518 -0.27985537 -0.03113481 -0.0807178  -0.36456078
  0.32611817 -0.24657626  0.46137786 -0.19168526  0.17839843 -0.30750045
  0.25595525  0.31099242 -0.2968946  -0.01073798  0.06088721  0.19302058
 -0.5639666   0.01833964  0.15863417 -0.2678895   0.08058956 -0.32224074
 -0.29750344  0.01542899 -0.1589315   0.00211973  0.37257284 -0.06355748
  0.31406918 -0.12881368 -0.0478576   0.02059456  0.24136949  0.2030548
 -0.15018821  0.19156697  0.37136042 -0.2459734  -0.18834257  0.14898515
  0.2280057   0.45968193  0.15898345 -0.08687034  0.40458605 -0.10198909
  0.1302667   0.3925035  -0.02937875 -0.0116025  -0.19174924  0.28571552
 -0.07634898  0.17443776  0.01535007  0.01765648 -0.33661193  0.3889361
  0.23680365 -0.05328634 -0.47757033 -0.33001044 -0.3490412  -0.02738257
  0.06025566  0.08459737  0.01454724  0.07107075  0.13651003 -0.20656869
 -0.27260178  0.25070608 -0.07387203 -0.2809579  -0.20536867 -0.10812955
  0.19641463 -0.3522773  -0.09491845 -0.31971017  0.33516905  0.52966374
 -0.15432043  0.12744093  0.17267229  0.14462107 -0.24939743  0.05787427
  0.1123634   0.15424043  0.22059523  0.12348273  0.12202297  0.41625667
 -0.53090847 -0.00227959 -0.25658172 -0.02034989  0.24024504 -0.34684044
 -0.05729464 -0.14840813  0.07059915  0.01058949 -0.11254118  0.17291792
 -0.34085363  0.06688575  0.1874438  -0.39550203 -0.1978842   0.20763604
 -0.15057862 -0.44256544  0.10909633 -0.0504774   0.0154516   0.21533798]"
`tensorflow-cpu` 2.13.0 missing Mac ARM wheels stat:awaiting response type:build/install stale subtype:macOS TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS 13 ARM

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

[`tensorflow` 2.13.0 added Mac ARM wheels](https://pypi.org/project/tensorflow/2.13.0/#files).

This allows Mac developers to use the standard `tensorflow` package rather than the `tensorflow-macos` package.

But in situations when CPU-only processing is needed, `tensorflow-cpu` is ideal (saving on network and disk usage). The problem is that `tensorflow-cpu` does not have Mac ARM wheels, so it cannot be used on that platform. This is important for teams with different architecture machines that want to share a common lock file produced by something like Poetry or `pip-tools`.

- https://pypi.org/project/tensorflow-cpu/2.13.0/#files

### Standalone code to reproduce the issue

```shell
python -m pip install tensorflow-cpu==2.13.0
```


### Relevant log output

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow-cpu==2.13.0 (from versions: none)
ERROR: No matching distribution found for tensorflow-cpu==2.13.0
```
",False,"[-0.38397732 -0.2808139   0.0379899   0.10247186  0.2946461  -0.4252488
  0.00821101  0.09585911 -0.24833731 -0.48492548  0.2375889   0.00243136
 -0.24643391  0.09803653 -0.19965555  0.1881779  -0.3803799  -0.3320251
  0.3200765   0.23619795 -0.24213532 -0.04083782 -0.13860473  0.20599362
  0.14857654  0.33061326 -0.05681059  0.11473058 -0.09509017  0.1481834
  0.33324575  0.25056618 -0.07591502 -0.00546061  0.43326902  0.19848219
 -0.17254971 -0.32525286 -0.59789145 -0.09914681 -0.11412786  0.11615364
  0.3260306  -0.00674602  0.07601257 -0.13552786 -0.05807595 -0.3959481
  0.05739366 -0.381747    0.00388053 -0.0925087  -0.0883414  -0.40792295
 -0.15775552  0.0723628  -0.05700055 -0.05523293  0.27223217  0.35700193
  0.26775622  0.08871146 -0.01737687 -0.13418707  0.23828322  0.07279115
  0.23567253 -0.1946567   0.40994242 -0.15034962  0.19396788 -0.2007298
 -0.46386635  0.22016743 -0.01472572  0.07489479  0.07562761 -0.10241026
  0.03456607 -0.17864847 -0.37126058 -0.2397657  -0.02292828 -0.05531627
  0.21252632 -0.23896879  0.1881181   0.07495706  0.3099674  -0.02090335
  0.41538197  0.3607207  -0.01321919 -0.17679597  0.42262235  0.06098853
  0.21080661  0.29471433 -0.09674195  0.04604813 -0.0276033  -0.05137075
  0.14480782 -0.036979   -0.12650505 -0.20550036  0.29785094 -0.27471185
  0.02206734 -0.12792777  0.10825147 -0.03380477  0.2583232   0.05640533
 -0.07565599 -0.13836089 -0.4330303   0.0394223   0.15093896  1.0131202
 -0.0484594  -0.10933037 -0.02824122  0.38886145  0.33610183  0.04847804
 -0.31290352 -0.00825794 -0.03774715 -0.15097103  0.03330203  0.00524166
  0.05924454  0.22270966 -0.0889024   0.02403206  0.03318096 -0.23865557
 -0.23010314 -0.16027601 -0.1924009   0.03638202 -0.02822593 -0.63241774
  0.2050907   0.00425472 -0.32854858  0.12610415 -0.170898    0.08739282
  0.00154609 -0.07474916  0.0155128   0.44809157  0.2360799   0.04279699
  0.51735383  0.01319432 -0.24741761 -0.49683255  0.06744125  0.42125893
 -0.14416921 -0.01185585  0.16539791  0.06378672 -0.42831415 -0.29210153
 -0.00999464  0.40390348 -0.20594183 -0.23988916  0.08847044  0.13249215
  0.16975889 -0.17660496  0.36255467 -0.6157233  -0.09670954  0.30721486
  0.13006178  0.07126537  0.0362381   0.36750636  0.11561203  0.15820885
 -0.12219383  0.16636622 -0.30920497  0.28379142 -0.37953168  0.03625895
  0.49744484  0.13183217  0.05697438  0.08975837  0.01303875  0.00485627
 -0.16909263 -0.0304281  -0.1135016  -0.09111601 -0.16147858 -0.15945536
 -0.05985508 -0.49157205 -0.19401781 -0.28816584 -0.3936317  -0.01518103
  0.21921682 -0.44533107  0.29898307 -0.03365772 -0.25085172  0.12178051
  0.10818337  0.11984446 -0.18582559  0.14163372 -0.21093738 -0.18448165
 -0.296331   -0.24657981 -0.20732059  0.06529132 -0.32630005 -0.05980065
 -0.14885458  0.27211338  0.13939226  0.05617344  0.39187452  0.04265204
  0.4177875  -0.25280547 -0.14466818 -0.211615   -0.10349838  0.17935795
 -0.45357072 -0.06640276  0.07087083  0.06941891  0.43031088  0.26917577
 -0.03475639 -0.05995025 -0.27506563  0.08557095 -0.09339206  0.43350556
  0.23718116  0.03156838  0.2254387   0.18064421  0.20700926  0.22665244
  0.16131875 -0.17018948  0.15028451  0.10380809  0.14112303  0.39263254
  0.1971918   0.21993205 -0.39705575  0.49140164  0.21747456  0.0425372
  0.02979481 -0.34095818  0.7641138  -0.49904102  0.01573391  0.03011699
  0.3101911  -0.00574805  0.02460054  0.02248793 -0.02178989  0.42692095
 -0.33825797  0.1653962   0.09809938 -0.29283598 -0.00970083 -0.8002162
 -0.136642   -0.00793546 -0.23784249 -0.01385624 -0.15906167  0.04933267
 -0.12714708  0.37675327  0.12106448 -0.14068042  0.05366985  0.36057734
  0.06455613  0.04718397  0.3578148  -0.63261753 -0.01876843 -0.1643323
  0.32272494  0.3860911   0.41275728 -0.2740254   0.20156452 -0.06871244
 -0.09874246  0.50101626  0.21634257  0.0034982  -0.46254808  0.7264751
  0.01990819 -0.10168608  0.20104596 -0.08687907 -0.36615148  0.09379272
  0.4355722  -0.03297444 -0.03596805 -0.41732726  0.01852948  0.14062068
  0.04860888 -0.09589586 -0.11454048  0.35877022 -0.25666237  0.07674178
 -0.32486385  0.216885   -0.17160225 -0.35960683 -0.20059586 -0.04977924
  0.05980734 -0.1422796   0.07806392 -0.31329572  0.34771207  0.5438245
 -0.03471885  0.04498881 -0.2018637   0.21202603 -0.45959115  0.06002757
 -0.07848319  0.3431622   0.03428207 -0.09950862  0.43517694  0.27121958
 -0.36231995  0.12703091 -0.658515    0.10207818  0.03575378 -0.19563034
 -0.32419163 -0.05563448 -0.12105953  0.16552329 -0.04807804  0.01026732
 -0.26378977  0.33439243  0.50191057 -0.25735137 -0.3140443  -0.01512646
 -0.09359266 -0.15831791  0.11661609 -0.1062686  -0.02577129  0.08384235]"
Failed building from source using clang compiler. Error: libtensorflow_framework.so.2 is a dangling symbolic link stat:awaiting response type:build/install stale comp:mkl subtype: ubuntu/linux TF 2.13,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:  No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 20.04.6 LTS. Building on Intel x86 CPU
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: Source
-   **TensorFlow version (use command below)**: 2.13
-   **Python version**: 3.10.11
-   **Bazel version (if compiling from source)**: 5.3.0
-   **Clang/Compiler version (if compiling from source)**: Clang 16.0.6
-   **CUDA/cuDNN version**: None (Building on Intel x86 CPU)
-   **GPU model and memory**: None (Building on Intel x86 CPU)
-   **Exact command to reproduce**: 
 bazel build  --config=mkl --config=dbg --verbose_failures  -c opt --copt=-march=native --spawn_strategy=sandboxed --sandbox_debug //tensorflow/tools/pip_package:build_pip_package


### Describe the problem

Error while building Tensorflow 2.13 from source with clang 16.0.6 and bazel 5.3.0. I am using the versions that were tested compatible  from this link: https://www.tensorflow.org/install/source#tested_build_configurations.
Errors:
ERROR: /home/ubuntu/builds/tensorflow/tensorflow/BUILD:1134:21: declared output 'tensorflow/libtensorflow_framework.so.2' is a dangling symbolic link
ERROR: /home/ubuntu/builds/tensorflow/tensorflow/BUILD:1134:21: Executing genrule //tensorflow:libtensorflow_framework.so.2_sym [for host] failed: not all outputs were created or valid

### Source code / logs

Output from above command mentioned:

Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=121
INFO: Reading rc options for 'build' from /home/ubuntu/builds/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/ubuntu/builds/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/ubuntu/builds/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/ubuntu/anaconda3/envs/tf_build/bin/python --action_env PYTHON_LIB_PATH=/home/ubuntu/anaconda3/envs/tf_build/lib/python3.10/site-packages --python_path=/home/ubuntu/anaconda3/envs/tf_build/bin/python
INFO: Reading rc options for 'build' from /home/ubuntu/builds/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug
INFO: Found applicable config definition build:short_logs in file /home/ubuntu/builds/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/ubuntu/builds/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:mkl in file /home/ubuntu/builds/tensorflow/.bazelrc: --define=build_with_mkl=true --define=enable_mkl=true --define=tensorflow_mkldnn_contraction_kernel=0 --define=build_with_openmp=true -c opt
INFO: Found applicable config definition build:dbg in file /home/ubuntu/builds/tensorflow/.bazelrc: -c dbg --per_file_copt=+.*,-tensorflow.*@-g0 --per_file_copt=+tensorflow/core/kernels.*@-g0 --cxxopt -DTF_LITE_DISABLE_X86_NEON --copt -DDEBUG_BUILD
INFO: Found applicable config definition build:linux in file /home/ubuntu/builds/tensorflow/.bazelrc: --define=build_with_onednn_v3=true --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/ubuntu/builds/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (614 packages loaded, 38324 targets configured).
INFO: Found 1 target...
ERROR: /home/ubuntu/builds/tensorflow/tensorflow/BUILD:1134:21: declared output 'tensorflow/libtensorflow_framework.so.2' is a dangling symbolic link
ERROR: /home/ubuntu/builds/tensorflow/tensorflow/BUILD:1134:21: Executing genrule //tensorflow:libtensorflow_framework.so.2_sym [for host] failed: not all outputs were created or valid
1690366816.573675378: src/main/tools/linux-sandbox.cc:152: calling pipe(2)...
1690366816.573706039: src/main/tools/linux-sandbox.cc:171: calling clone(2)...
1690366816.573936717: src/main/tools/linux-sandbox.cc:180: linux-sandbox-pid1 has PID 52399
1690366816.573986443: src/main/tools/linux-sandbox-pid1.cc:650: Pid1Main started
1690366816.574047951: src/main/tools/linux-sandbox.cc:197: done manipulating pipes
1690366816.574187327: src/main/tools/linux-sandbox-pid1.cc:269: working dir: /home/ubuntu/.cache/bazel/_bazel_ubuntu/e773aae8e1619280c7c65ec2bcc4c4c5/sandbox/linux-sandbox/2934/execroot/org_tensorflow
1690366816.574204103: src/main/tools/linux-sandbox-pid1.cc:301: writable: /home/ubuntu/.cache/bazel/_bazel_ubuntu/e773aae8e1619280c7c65ec2bcc4c4c5/sandbox/linux-sandbox/2934/execroot/org_tensorflow
1690366816.574210697: src/main/tools/linux-sandbox-pid1.cc:301: writable: /tmp
1690366816.574216986: src/main/tools/linux-sandbox-pid1.cc:301: writable: /dev/shm
1690366816.574277494: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /
1690366816.574284725: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /dev
1690366816.574289479: src/main/tools/linux-sandbox-pid1.cc:371: remount rw: /dev/shm
1690366816.574294383: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /dev/pts
1690366816.574298736: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /dev/hugepages
1690366816.574303348: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /dev/mqueue
1690366816.574307631: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys
1690366816.574312050: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/kernel/security
1690366816.574318166: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup
1690366816.574323186: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/unified
1690366816.574328198: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/systemd
1690366816.574333290: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/hugetlb
1690366816.574337803: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/freezer
1690366816.574342520: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/misc
1690366816.574370614: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/cpu,cpuacct
1690366816.574376364: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/perf_event
1690366816.574380879: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/blkio
1690366816.574385348: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/devices
1690366816.574389735: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/memory
1690366816.574394055: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/cpuset
1690366816.574398740: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/net_cls,net_prio
1690366816.574403141: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/pids
1690366816.574407626: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/cgroup/rdma
1690366816.574412563: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/pstore
1690366816.574417748: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/bpf
1690366816.574422147: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/kernel/debug
1690366816.574427574: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/kernel/tracing
1690366816.574453517: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/fs/fuse/connections
1690366816.574459675: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /sys/kernel/config
1690366816.574464740: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /proc
1690366816.574469467: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /proc/sys/fs/binfmt_misc
1690366816.574478344: src/main/tools/linux-sandbox-pid1.cc:391: remount(nullptr, /proc/sys/fs/binfmt_misc, nullptr, 2101281, nullptr) failure (Operation not permitted) ignored
1690366816.574487890: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /proc/sys/fs/binfmt_misc
1690366816.574499308: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /run
1690366816.574503860: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /run/lock
1690366816.574508265: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /run/snapd/ns
1690366816.574513396: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /run/user/1000
1690366816.574518358: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/amazon-ssm-agent/6563
1690366816.574523566: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /boot/efi
1690366816.574528274: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/amazon-ssm-agent/7497
1690366816.574547267: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/snapd/19361
1690366816.574553038: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/core18/2785
1690366816.574557467: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/core20/1950
1690366816.574562111: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/snapd/19457
1690366816.574566323: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/core18/2751
1690366816.574570473: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/core20/1974
1690366816.574574374: src/main/tools/linux-sandbox-pid1.cc:371: remount ro: /snap/lxd/24061
1690366816.574579365: src/main/tools/linux-sandbox-pid1.cc:371: remount rw: /home/ubuntu/.cache/bazel/_bazel_ubuntu/e773aae8e1619280c7c65ec2bcc4c4c5/sandbox/linux-sandbox/2934/execroot/org_tensorflow
1690366816.574585138: src/main/tools/linux-sandbox-pid1.cc:371: remount rw: /home/ubuntu/.cache/bazel/_bazel_ubuntu/e773aae8e1619280c7c65ec2bcc4c4c5/sandbox/linux-sandbox/2934/execroot/org_tensorflow
1690366816.574589762: src/main/tools/linux-sandbox-pid1.cc:371: remount rw: /tmp
1690366816.574594138: src/main/tools/linux-sandbox-pid1.cc:371: remount rw: /dev/shm
1690366816.574636966: src/main/tools/linux-sandbox-pid1.cc:460: calling fork...
1690366816.574760311: src/main/tools/linux-sandbox-pid1.cc:490: child started with PID 2
1690366816.583691390: src/main/tools/linux-sandbox-pid1.cc:507: wait returned pid=2, status=0x00
1690366816.583704145: src/main/tools/linux-sandbox-pid1.cc:525: child exited normally with code 0
1690366816.583952036: src/main/tools/linux-sandbox.cc:233: child exited normally with code 0
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 849.711s, Critical Path: 79.72s
INFO: 4142 processes: 1215 internal, 2927 linux-sandbox.
FAILED: Build did NOT complete successfully",False,"[-0.39517888 -0.24534614 -0.31298932 -0.14283395  0.06285989 -0.30214924
 -0.3346988   0.27276015 -0.2454975  -0.09775403 -0.13602614 -0.1625507
  0.14099321 -0.10407732 -0.14552388  0.24654353 -0.19057833 -0.30016828
  0.3306134   0.18949383 -0.29110736  0.20503835 -0.10218388  0.2965899
  0.39216986 -0.02486232 -0.31600296  0.34725958  0.04738981  0.16047063
  0.42059964  0.22486804 -0.28526187  0.05335276  0.2307572   0.52684736
 -0.1803829  -0.1345091  -0.21328324 -0.11671269  0.03038687  0.09062359
 -0.00179653 -0.03002737 -0.08490016  0.14307313  0.04662844  0.11398499
 -0.25933883 -0.2604431  -0.21782055  0.23906565 -0.18081442 -0.24950777
  0.14761321  0.16086468 -0.03393753 -0.04406511  0.03975868  0.19641724
  0.35780585 -0.02863001 -0.04189502  0.03840044  0.2493493   0.5129937
  0.2509999  -0.17360763  0.31014746 -0.17064965  0.08089246 -0.2853882
 -0.22094846 -0.02239203 -0.0929997   0.0918109  -0.2050761   0.19708753
  0.21872397 -0.22820601 -0.19684672 -0.15458357  0.09393269 -0.06089947
  0.00193505  0.15047577  0.37154266  0.18310547  0.19430321 -0.10192168
  0.37173945  0.38763046  0.03846582  0.30663848  0.22392702  0.12241902
  0.0870468   0.36542788 -0.30356932 -0.02556039 -0.16038115  0.06795852
  0.21842554 -0.11473676 -0.17178665 -0.13474247  0.30433196  0.14697894
  0.02049416 -0.12986979  0.05486172  0.08113204 -0.15205872 -0.22185756
  0.01842401  0.18840635 -0.01712982 -0.14070491 -0.02088102  0.5796706
  0.13969615  0.05964059 -0.05540277  0.15043113  0.19293529  0.11646356
 -0.13186574 -0.18977575  0.22458632 -0.0374525   0.01010576  0.16784735
 -0.2375837   0.06530739  0.11565253 -0.01828443 -0.02002793  0.18610783
  0.19540688 -0.10899828 -0.23941228  0.20424974  0.10385036 -0.16626598
 -0.01264766 -0.10172932 -0.09296906 -0.06332278 -0.11555466  0.24029803
 -0.07568996  0.03035384  0.02113506  0.25207144  0.26836276  0.00108749
  0.19728991 -0.01023488  0.06135552 -0.5720578  -0.19775274  0.3638215
  0.15649095 -0.12766504 -0.04568335  0.05689726 -0.505864   -0.15054178
  0.06183733  0.06906206 -0.13762584 -0.01338097  0.30449662  0.16543293
  0.2819884  -0.31507105  0.30033034 -0.18602523  0.0612067   0.4532464
  0.16714454  0.28467566 -0.10859826  0.03403307 -0.19212884 -0.06241864
 -0.11604755  0.0062092  -0.20655903 -0.12575108 -0.3566513   0.10072278
  0.12365995 -0.2604841  -0.3599829  -0.15855452  0.4045357   0.14984137
  0.4226715   0.10644995 -0.01422127  0.13369542 -0.03341711 -0.08964144
  0.06261254 -0.06453082 -0.30831745 -0.01256451 -0.08668679  0.05191587
 -0.18781762 -0.14123113  0.25333643 -0.19030643  0.01699565 -0.0665223
 -0.093224   -0.21957201 -0.01255854  0.26298654  0.14573182 -0.13130185
  0.0808783  -0.27242714  0.10569069 -0.0280045  -0.09568845  0.04464315
 -0.04602997 -0.09789969 -0.08201694 -0.05828058  0.11487914  0.17688033
  0.05812212 -0.11247972  0.26427177 -0.23561779 -0.23041493 -0.00796419
 -0.20824966 -0.47615775  0.19112627  0.06753871  0.23507746 -0.03635253
 -0.15526782  0.05182321 -0.03896531  0.19144759 -0.20056531 -0.01760066
  0.09487793  0.07329851  0.16080615  0.19825456 -0.10674487  0.03508437
  0.10662705 -0.21770829 -0.02574808  0.39393228 -0.1152733  -0.0169168
  0.21246731  0.02005645 -0.31857002  0.15309563 -0.16304275 -0.12434277
  0.28045735 -0.1857579   0.2907421  -0.2822469   0.29722518  0.12227459
  0.4873165  -0.11219273  0.10509516  0.15362892  0.10544269  0.22867605
 -0.11082992  0.29495478  0.18174256 -0.18526071 -0.09011024 -0.30132052
 -0.27035126  0.10152642 -0.20563966 -0.0265307  -0.2423308   0.08669679
 -0.16761622 -0.02047639 -0.17048478 -0.04447727  0.10579591  0.06938317
 -0.13217656 -0.13921389  0.33206022  0.00705154  0.0668098  -0.01479692
  0.06576239  0.2295213   0.37905994 -0.48269448  0.35159376  0.23420349
 -0.22236627  0.17631987  0.0646546   0.08273528 -0.4193271   0.5910684
  0.0747497  -0.08885542  0.21036088  0.0768352  -0.32490385  0.08953242
  0.06711718  0.1128668  -0.12876754 -0.21276808 -0.22117047  0.21928878
  0.00604315 -0.15620103 -0.3163276   0.19846565 -0.31812623 -0.27657223
 -0.4186738   0.27887672 -0.12595987 -0.2804528   0.1697798  -0.2936814
 -0.04067911 -0.21356061 -0.40952456 -0.20897321  0.19883601  0.10338056
 -0.01928312  0.28619963  0.11388887  0.13570318 -0.30310097 -0.13609047
 -0.43645957  0.13888623 -0.24154732  0.11832997  0.19748679  0.22330791
 -0.09643362  0.11149246 -0.12714763 -0.0983669   0.01941842 -0.2728737
 -0.13595004 -0.00457938  0.19851324  0.12890196  0.12585652  0.23179084
 -0.32462782  0.23996013  0.2563051  -0.09521808 -0.22668529  0.10597879
 -0.11171061 -0.29002258 -0.19872214 -0.1337769  -0.04012229  0.00378632]"
How to use the estimator interface to achieve cross-node training without using the strategy of tf itself stat:awaiting response stale comp:dist-strat type:docs-feature TF 2.7,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf2.7

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

python3.10

### Bazel version

5.1.1

### GCC/compiler version

9.4

### CUDA/cuDNN version

_No response_

### GPU model and memory

Tesla P100 12GB

### Current behavior?

![image](https://github.com/tensorflow/tensorflow/assets/69454138/f7e1159d-cd51-4dca-8890-2b9aebe58e57)

Cross-node training can be achieved in this way in tf1. If I want to use the estimator interface for training, can I put server.target in a certain config (similar to tf.estimator.RunConfig)

### Standalone code to reproduce the issue

```shell
cluster_spec = tf.train.ClusterSpec({
      'chief': ['172.20.21.189:1234'],
      'worker': ['172.20.21.197:1234'],
    })
simple_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(cluster_spec, task_type=""chief"",task_id=0)
is_per_host = tf.estimator.tpu.InputPipelineConfig.PER_HOST_V2
run_config = tf.estimator.tpu.RunConfig(
      cluster=simple_resolver,
      master=FLAGS.master,
      model_dir=FLAGS.output_dir,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
      tpu_config=tf.estimator.tpu.TPUConfig(
          iterations_per_loop=FLAGS.iterations_per_loop,
          num_shards=FLAGS.num_tpu_cores,
          per_host_input_for_training=is_per_host))
I do this configuration in the estimator interface, but he doesn't seem to be training across nodes (multiple servers),It seems to only recognize local devices.
```


### Relevant log output

_No response_",False,"[-0.38756603 -0.33036324 -0.2372587  -0.17704426  0.2660215  -0.18931638
 -0.07011125  0.03667698 -0.5403381  -0.26816404  0.080933   -0.01378113
 -0.1476604   0.13998373 -0.02717486  0.07021149  0.00155531  0.05664533
  0.3303343  -0.11583376 -0.13614252 -0.00454909 -0.09335984  0.2029614
  0.16636425  0.03948454 -0.17789564  0.04313554 -0.00425042  0.17203218
  0.0694246   0.15726295 -0.2990173  -0.10071838 -0.2578092   0.18158685
 -0.3112679  -0.09365096 -0.33488512  0.23718376 -0.27664453 -0.03606141
 -0.02543787 -0.11813776 -0.10937802 -0.18334325 -0.28715873 -0.07781571
 -0.12107591 -0.09673369  0.07561913 -0.05063896 -0.28302133 -0.24197647
 -0.00158177  0.10228418  0.1541036   0.19046856 -0.09213521  0.11129875
 -0.02416335  0.07776689 -0.0349088   0.00522763  0.07799679  0.38514745
  0.0641816   0.00818908  0.33957118 -0.19827929 -0.10464997 -0.28400815
 -0.273119   -0.02361797 -0.07843302 -0.05458676  0.13881844  0.3935811
  0.35183805 -0.12453626 -0.01139242 -0.16904245 -0.31913865  0.03283546
  0.15078932 -0.17086494  0.12555249  0.2934597   0.4494209  -0.20862481
  0.64767826  0.37318152 -0.09769718  0.0939777   0.50525606  0.2552491
 -0.17474234  0.04487449 -0.07175818  0.13766026 -0.14627281 -0.25598973
 -0.10499957  0.04203696 -0.16526179 -0.1014261   0.0710513  -0.22840875
  0.13875228  0.06281032  0.10408207  0.15307815 -0.07776725  0.01615546
 -0.08125157  0.09318236  0.01412817  0.06337999  0.4945069   0.41914296
  0.18428853 -0.26303163  0.04298074  0.23187949  0.6457993   0.04163352
 -0.06928597  0.06052792  0.06141461  0.0787316   0.21980348  0.00543581
 -0.17356402  0.08601598 -0.11710397  0.00839285 -0.1700815  -0.27169505
 -0.3901551   0.05580977 -0.35532904  0.01533317 -0.00624059 -0.5023211
  0.07252964  0.31360644 -0.30630755  0.42542174 -0.0725284   0.03027397
  0.10797291 -0.00282928  0.01060563  0.4784797   0.09164193  0.0970684
  0.24661219  0.02252143  0.00178708 -0.46617717 -0.08593643  0.2873604
  0.09321753 -0.22047165  0.14233351  0.18881822 -0.46764356 -0.10356501
 -0.01459641  0.27694687  0.06230246 -0.21071975  0.04475854  0.1727542
  0.45652038 -0.13031776  0.15889373 -0.50314265  0.02980182  0.11088784
 -0.03700112 -0.06775956  0.18120706  0.32771754  0.02289413  0.12398562
  0.28037518  0.12771873 -0.37806958 -0.04041285 -0.1560097  -0.25254136
  0.30537316 -0.24647851 -0.29702783 -0.0484621   0.31489348  0.07161567
 -0.08470684  0.08384376 -0.04221035 -0.10076781  0.03333903 -0.03331723
  0.14653498 -0.08574677  0.04170993 -0.66009736 -0.15597275 -0.04198557
  0.1455642  -0.37989265  0.35268575 -0.14165181 -0.43141603  0.00800207
 -0.14245108 -0.01692554 -0.02241519  0.22712554 -0.10665465 -0.05309197
  0.09109736 -0.4159212  -0.18682599  0.16762723 -0.19990595  0.04007694
  0.00259357 -0.01900557  0.20485558  0.26858443  0.27718508  0.3567372
  0.3135402  -0.14366564 -0.26431462 -0.2308834  -0.23173377 -0.02509753
 -0.17173533 -0.02201244 -0.01409268 -0.07122783  0.5142498   0.42598525
 -0.10651673 -0.10517526 -0.16344184  0.37303984 -0.2926703  -0.09950887
  0.15981129 -0.11714015  0.34483016  0.02616153  0.17508297  0.26250893
  0.31848836 -0.08323891  0.32161966  0.2742536   0.2283019   0.53402007
  0.3329249   0.38459247 -0.3558832   0.49841267 -0.2560083  -0.16201422
 -0.15245426 -0.20685947  0.49915498 -0.24296924  0.15382019 -0.14484139
  0.47281808 -0.22497529 -0.04057582  0.0455322  -0.13269107  0.00545061
 -0.2715932   0.35131648  0.04828691 -0.23487644 -0.12027731 -0.34444785
 -0.10363393 -0.01322425 -0.3981915   0.10815129 -0.25324395  0.01976138
 -0.18855232  0.13203879  0.17946397 -0.12645754  0.1354427  -0.04482905
 -0.16159977 -0.08090904  0.22997035 -0.27512813 -0.07059063 -0.24240312
  0.32233894  0.10423638  0.24520487 -0.31285825  0.11007458  0.04698133
  0.06826045  0.28679547 -0.03994638  0.25997823 -0.3789131   0.5846337
  0.05484042 -0.11441752  0.05790985 -0.0089282  -0.28657913  0.00538085
  0.1833595   0.03814869 -0.06724475 -0.2387406   0.01393921  0.08030573
 -0.15689664 -0.0476401  -0.10158052  0.08888765 -0.00258401 -0.07147235
 -0.21828038  0.21478546 -0.08596654 -0.15680845 -0.12055809 -0.22254634
 -0.29973418  0.01324853  0.03435607 -0.20743397  0.29572412  0.7046623
  0.1199574   0.0541022   0.03409662 -0.01942568 -0.34354717  0.13834631
 -0.1498879   0.4804519  -0.06711255 -0.07737833  0.17664623  0.2785745
 -0.24052265  0.18375877 -0.36622742  0.12105964  0.22222826 -0.14945419
 -0.13061476 -0.3123302  -0.00586121  0.09646776  0.06190474  0.12090041
 -0.30945146  0.13086744  0.54005325 -0.34616423  0.06385617  0.06620336
  0.05460642  0.01199378 -0.12887828 -0.25787246  0.17279357  0.20293333]"
Cannot subclass dataset_ops.DatasetV2 stat:awaiting response type:feature stale comp:ops comp:data,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.x

### Custom code

Yes

### OS platform and distribution

Mac OS 13.0

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi, I'm from LanceDB team and we're trying to build native support for tf.data. See WIP PR here https://github.com/lancedb/lance/pull/1087 .
Ideally, we'd like to simply subclass `tf.dataset_ops.DatasetV2` so that all the metadata needed to recreate the dataset can be pushed down to our file format that enabled parallelism elegantly.
So, it'd be something like this
```
class LanceTfDataset(dataset_ops.DatasetV2)
    def __init__(self):
       ...
       variant_tensor = tf.Tensor(self, (), dtype=tf.Variant)
       super().__init__(variant_tensor)
 ```
The above code complains that can not create LanceTfDataset to tf.Tensor/variant.

Issue - what exactly is variant_tensor and how do we go about creating one? I read through the docs but couldn't find anything concrete. There was a mention that variant_tensor is a special tensor that tell about the type of the dataset and that it's equivalent to tf.Variant, but the above code doesn't work.

Having a version of tf.dataset that we can use to capture extra metadata would allow us to improve the interface as well:
so instead of lance.tf.data.from_dataset(uri, columns, filter, batch_size) we can just have from_lance(uri).filter(..).batch_size(...).shuffle().

So what's the way to go about subclassing tf Dataset?

### Standalone code to reproduce the issue

```shell
class LanceTfDataset(dataset_ops.DatasetV2)
    def __init__(self):
       ...
       variant_tensor = tf.Tensor(self, (), dtype=tf.Variant)
       super().__init__(variant_tensor)
```


### Relevant log output

_No response_",False,"[-0.43689066 -0.2579461  -0.04433912 -0.00307564  0.20462868 -0.47298658
 -0.20294927  0.09457664 -0.49116123 -0.34764925  0.2948438  -0.18629816
 -0.17602992  0.07532957 -0.15709533  0.24259911 -0.11606251 -0.06998452
  0.41748878  0.10355742 -0.26038742 -0.01139631 -0.40223742  0.03998905
  0.14822063  0.45184427 -0.50122213  0.18830164 -0.10215017  0.39925253
  0.4651208   0.01809987 -0.02867384  0.3178683   0.20806965  0.20112441
 -0.22473077 -0.23022157 -0.32169908  0.00821814 -0.15079361  0.1254166
  0.1615752  -0.10571124  0.00991417 -0.16951181 -0.13986157 -0.29530722
 -0.07788336 -0.37361553 -0.09717063  0.21063957 -0.5382018  -0.27248335
 -0.01898514 -0.03197934 -0.01574759  0.10541409 -0.0619326   0.16887209
  0.10313959 -0.1177406   0.13222817 -0.03919075  0.1661653   0.07216475
  0.49652225 -0.02098351  0.6056035  -0.18489364  0.15462722  0.01273074
 -0.50738144  0.06420764 -0.09098545  0.22202209 -0.1777038  -0.05400709
  0.33769974 -0.39327842  0.03615425  0.00684404 -0.14787637 -0.14790398
  0.07214874 -0.302515    0.3540237   0.04571739  0.38171637 -0.14199218
  0.4953555   0.40398842  0.02043313  0.18327478  0.34172195  0.11067885
  0.27254298  0.35212225 -0.00748876 -0.05026942 -0.13854307 -0.14978418
 -0.21045089  0.07582849 -0.28495535 -0.25836962  0.12886736 -0.09687532
  0.0469014   0.17012092  0.0571043   0.11972513 -0.05297309 -0.21988964
 -0.07852796 -0.11239978 -0.2814635   0.0353921  -0.0669079   0.5417917
  0.11995086 -0.19028139 -0.116082    0.31904468  0.68356967  0.09687907
 -0.13163835  0.09898608  0.13134068 -0.1584704   0.23078063 -0.08189726
  0.10725911  0.19598226 -0.11380005  0.2137645  -0.13850622  0.02396537
 -0.3251086  -0.16832162 -0.25223696  0.18321626 -0.09028623 -0.6251323
  0.36652556  0.24718374 -0.22073843  0.30457717 -0.15940586  0.04649593
  0.13573062  0.24970177 -0.26356125  0.5656293   0.04008434  0.13237664
  0.5485829   0.00131045  0.01978658 -0.5460812   0.1429462   0.40184155
 -0.11811345 -0.1572389   0.35317713  0.05119648 -0.450725   -0.43466043
  0.01862593  0.44446388 -0.31099206 -0.10671568  0.08468132 -0.16477978
  0.3380949  -0.14900568 -0.00371404 -0.3625929   0.0827674   0.36616316
  0.15169138  0.10887034 -0.00987229  0.32180756  0.17646861 -0.0330128
  0.07855066  0.21154313 -0.4455089  -0.0987727  -0.62835085 -0.09093996
  0.51312125  0.01753248 -0.05605491 -0.1406262   0.26352394  0.26414967
  0.01152409  0.08566191 -0.30912116 -0.05886425 -0.17981987  0.04544016
  0.06816433 -0.21410277 -0.19388488 -0.4806112  -0.48738927  0.11570501
 -0.11270126 -0.4681567   0.11642604  0.00169006 -0.2863691   0.35091436
  0.21065983  0.14041924 -0.31091872  0.0589545   0.02844272 -0.29872215
  0.11940284 -0.36973774 -0.11838977  0.24601457 -0.50217754  0.01861822
 -0.01903723  0.13152903  0.03236029  0.03191838  0.34459525  0.18001722
  0.2834394  -0.16198397  0.05438683 -0.23241279 -0.15915099  0.13169259
 -0.6982435  -0.2310715  -0.10955477  0.01528843  0.52150965  0.5100706
 -0.02662258  0.03451956 -0.41428006  0.05881136 -0.08012129  0.0637155
  0.4866932  -0.01160624  0.3013487   0.3227458   0.19972552  0.09009501
  0.22524826 -0.2967196   0.29769838  0.1075995   0.0452414   0.5241296
  0.06320669  0.44224226 -0.43938363  0.50595415  0.12450036 -0.22994936
  0.1986211  -0.21817024  0.68423074 -0.29967707 -0.13360657 -0.02239266
  0.2360761  -0.0908964   0.03538878  0.03674009 -0.05302276  0.22629055
 -0.40306202  0.13487558  0.08457153 -0.3745559  -0.18690649 -0.610451
 -0.18765295  0.10701083 -0.2349011  -0.0982702  -0.10415939  0.12288192
 -0.2515084   0.00445111  0.20578258 -0.296229    0.14531766 -0.04392605
 -0.3112759   0.11580776  0.3162185  -0.3573497  -0.12641999 -0.12336721
  0.5392989   0.305995    0.52629584 -0.37285298  0.01739741  0.01201752
 -0.2792746   0.56017447 -0.09206519 -0.10521759 -0.33635104  0.5895103
  0.05987187 -0.09769519  0.1933341  -0.16828671 -0.12567551  0.15860286
  0.45266834  0.031331   -0.24890472 -0.21545666  0.11344308  0.23933375
  0.03570735 -0.01602199 -0.16855708  0.03986536 -0.19071908 -0.0807998
 -0.24012163  0.3013323   0.00920493 -0.12871812 -0.25135517 -0.14224246
 -0.14727966 -0.1246343  -0.0658969  -0.24134189  0.35083848  0.5682019
 -0.25605467  0.28238076  0.01072099  0.05993608 -0.1992844   0.03857992
  0.08185632  0.3112441   0.17567655 -0.05200879  0.28841048  0.08980052
 -0.4234056   0.19108549 -0.40961522  0.01647281 -0.04676428 -0.01933227
 -0.20742804 -0.2233888   0.13525358  0.1441978  -0.22460079  0.0417925
 -0.4438881   0.26508445  0.4785684  -0.29645818 -0.09710789  0.07065693
  0.19447574  0.05087891  0.14694607 -0.1801374   0.13031085  0.03057456]"
Issues running Transformer model example with estimator api stat:awaiting response type:support type:others comp:model,"Hello everyone! I am trying to run an Image classification training example with Vision Transformer from keras examples (https://keras.io/examples/vision/image_classification_with_vision_transformer/). Everything ran perfectly when I ran it as it is but I started facing issues when i switched training from `model.fit()` to `tf.estimator.train_and_evaluate()` (ofcourse I made the appropriate changes to first convert model to estimator). From what I understand ... the problem lies with saving and reloading the model which is done by the estimator api. The model has custom classes:
```
class Patches(layers.Layer):
    def __init__(self, patch_size, **kwargs):
        super().__init__(**kwargs)
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding=""VALID"",
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches

    ## personal addition
    def get_config(self):
        base_config = super().get_config()
        base_config.update({
            'patch_size': self.patch_size,
        })
        return base_config

class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim, **kwargs):
        super().__init__(**kwargs)
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        encoded = self.projection(patch) + self.position_embedding(positions)
        return encoded

    ## personal addition
    def get_config(self):
        base_config = super().get_config()
        base_config.update({
            'num_patches': self.num_patches,
            'projection': self.projection,
            'position_embedding': self.position_embedding
        })
        return base_config
```

From looking at some related issues, I found how we need to provide a `get_config()` method to save and reload the model with custom classes so I made small personal modifications but now its sort of giving me a different issue I am unable to understand. 

Error Log:
```
  warnings.warn(
x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 1)
x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 1)
WARNING:tensorflow:From train.py:225: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras instead.
/home/nearchus/.local/lib/python3.8/site-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.
  warnings.warn(
Traceback (most recent call last):
  File ""train.py"", line 257, in <module>
    history = run_experiment(vit_classifier)
  File ""train.py"", line 231, in run_experiment
    model_est = keras.estimator.model_to_estimator(keras_model=model, model_dir='.', config=run_config)
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/estimator/__init__.py"", line 376, in model_to_estimator_v2
    return keras_lib.model_to_estimator(
  File ""/home/nearchus/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/keras_lib.py"", line 725, in model_to_estimator
    warm_start_path = _save_first_checkpoint(keras_model, custom_objects,
  File ""/home/nearchus/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/keras_lib.py"", line 457, in _save_first_checkpoint
    model = _clone_and_build_model(ModeKeys.TRAIN, keras_model,
  File ""/home/nearchus/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/keras_lib.py"", line 230, in _clone_and_build_model
    clone = tf.compat.v2.keras.__internal__.models.clone_and_build_model(
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 806, in clone_and_build_model
    clone = clone_model(model, input_tensors=input_tensors)
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 539, in clone_model
    return _clone_functional_model(
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 222, in _clone_functional_model
    model_configs, created_layers = _clone_layers_and_model_config(
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 298, in _clone_layers_and_model_config
    config = functional.get_network_config(
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/engine/functional.py"", line 1590, in get_network_config
    layer_config = serialize_layer_fn(layer)
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 295, in _copy_layer
    created_layers[layer.name] = layer_fn(layer)
  File ""/home/nearchus/.local/lib/python3.8/site-packages/keras/src/models/cloning.py"", line 52, in _clone_layer
    return layer.__class__.from_config(layer.get_config())
  File ""train.py"", line 108, in from_config
    return cls(**config)
TypeError: __init__() missing 1 required positional argument: 'projection_dim'
```
I thought it might be because of `PatchEncoder` class constructor has custom objects as argument - so i tried to do serialization/deserialization but to no vail. In any case, I would highly appreciate if someone can guide me as to where I am going wrong in this!  


  ",False,"[-1.54547572e-01 -2.64148265e-01 -2.41339400e-01 -2.31266528e-01
  1.92850336e-01 -1.69360079e-02  7.28080422e-03 -3.95897329e-02
 -3.44471395e-01  5.73889464e-02  2.11004302e-01 -8.35795701e-02
  2.51525175e-02  1.84005082e-01 -8.53559226e-02  2.49655783e-01
 -3.02580819e-02  4.41550538e-02  4.41288799e-02  1.27238734e-02
  7.86523446e-02 -7.02790841e-02 -3.16800885e-02  2.58156825e-02
 -4.26685158e-03  1.38181239e-01  1.32105067e-01 -5.08049950e-02
  1.85255304e-01 -8.45856518e-02 -6.29051328e-02 -1.48176983e-01
  3.07122506e-02  4.09369469e-02 -9.20382887e-02 -3.88013422e-02
 -1.31373554e-01 -3.58138233e-02 -5.96849062e-02 -4.72742729e-02
  5.62919751e-02 -1.06784455e-01 -8.08225572e-02 -2.91290641e-01
  5.62712960e-02 -1.20524593e-01 -2.95493305e-01 -1.23449937e-01
 -2.41464332e-01 -1.80520400e-01  8.67559537e-02 -1.91563159e-01
 -2.62440532e-01 -4.18307781e-01  1.18785523e-01  1.24136023e-01
  1.02992371e-01  1.32912949e-01  9.56292450e-03 -2.44921833e-01
 -1.96007758e-01  9.39918309e-02 -9.23690647e-02 -2.30894573e-02
  6.61747977e-02  1.84055090e-01 -1.86737515e-02 -3.23356092e-01
  4.28706110e-01 -3.41434419e-01  1.53014604e-02 -8.98942947e-02
  5.00548929e-02 -5.46648763e-02  1.62131656e-02 -2.44121134e-01
 -1.26092061e-01  3.67754817e-01  2.22519502e-01 -2.04558998e-01
  2.20143765e-01  3.98553815e-03 -3.59569430e-01  3.30913812e-04
  3.52835894e-01 -1.58894449e-01  1.24068707e-01 -3.03412318e-01
  4.80671227e-01  1.91258550e-01  2.83125460e-01 -8.49869400e-02
 -8.53484273e-02  1.38096675e-01  4.97084945e-01  1.05926089e-01
 -1.18471943e-01 -5.42163439e-02  7.56260231e-02  8.90505910e-02
 -2.20233738e-01 -3.99116307e-01  6.13639690e-02  2.84909815e-01
  2.70642154e-03 -1.72984183e-01 -3.31585668e-02 -4.33530025e-02
  1.51106000e-01  1.35162979e-01  1.31104320e-01 -3.49384435e-02
 -2.99005628e-01  8.34897086e-02 -1.85397714e-02 -1.77291527e-01
 -1.99065916e-02  1.95317253e-01 -6.85057491e-02  2.67220676e-01
  1.25236243e-01 -3.81910443e-01  3.35109174e-01  2.74192929e-01
  3.05168152e-01  1.63956061e-01 -3.68898034e-01 -2.19072811e-02
 -1.95277065e-01  2.20253259e-01  2.60947466e-01  2.96400487e-03
 -2.60476291e-01 -1.48362309e-01  8.40657055e-02  5.27021885e-02
 -5.27154654e-03 -9.71775129e-02 -2.86002904e-01  1.66522294e-01
 -1.69264019e-01  3.12614501e-01  4.83238511e-03 -1.83699951e-02
 -7.26200640e-02  2.37629488e-01 -2.88071960e-01  2.81195223e-01
 -3.73291075e-02  8.82884637e-02  7.26514030e-03 -1.45170391e-01
 -2.60988027e-01  1.92411274e-01 -4.23079915e-02 -6.11241311e-02
  1.33304834e-01  2.08895355e-02  2.73378789e-02 -7.86048844e-02
  1.22939005e-01  8.79517477e-03  9.72836912e-02 -2.32785985e-01
  4.49183166e-01  1.61952108e-01 -1.89325541e-01 -1.88424051e-01
 -8.31400156e-02  2.26331651e-01  2.09977657e-01 -1.61953419e-02
  1.06424779e-01 -1.52374074e-01  6.51731253e-01  1.25876516e-01
  2.08391324e-01 -8.84405896e-03 -4.28931832e-01 -3.12637687e-02
  3.26018035e-01 -1.19638778e-01  7.65827075e-02 -1.14269644e-01
  1.12201966e-01 -1.07685491e-01  2.26893395e-01 -1.17947228e-01
 -3.38210762e-01  1.01231389e-01 -2.08368808e-01 -2.06598043e-01
  1.29473239e-01  1.78026836e-02 -9.00971293e-02 -3.58177245e-01
  1.29321560e-01 -1.88349672e-02  1.48795053e-01  2.07490269e-02
 -3.89854401e-01  8.78131688e-02  9.76008326e-02 -4.10100877e-01
 -6.09023161e-02  2.22862855e-01 -3.55069637e-01 -4.72344100e-01
 -1.61313891e-01  2.70134687e-01 -7.23438114e-02 -4.24439579e-01
  2.99426429e-02  4.56129648e-02 -1.19876124e-01 -9.99319181e-02
 -1.18280910e-02  7.95185417e-02  9.83578116e-02  3.35023463e-01
 -1.94014221e-01  1.15937382e-01  5.19503169e-02 -2.16668010e-01
 -2.18165860e-01  3.25161844e-01  1.24186985e-01 -1.01453066e-01
 -6.54528961e-02 -1.78920448e-01  3.08299094e-01 -1.95955336e-01
  3.28716487e-01 -7.77745843e-02  8.16402659e-02 -5.67218475e-02
 -7.45101720e-02 -9.98860449e-02 -1.89701378e-01 -2.10762262e-01
 -2.33605832e-01 -3.14394057e-01 -1.36164337e-01 -1.69474542e-01
  9.27060619e-02  2.66184419e-01 -2.30146334e-01 -1.54603183e-01
 -1.84355587e-01  5.00929892e-01 -8.44440907e-02  6.44256547e-02
  1.45962551e-01  3.82513329e-02 -3.31882946e-03  1.28508747e-01
  6.70400858e-02  1.35687038e-01  1.44868225e-01  2.14621294e-02
  1.31523967e-01  2.56287128e-01  5.32114953e-02  4.98875022e-01
  1.13734931e-01  2.34970957e-01 -1.39432818e-01  9.54404995e-02
 -1.70785636e-01 -4.48758602e-01  2.80205011e-01 -2.18272135e-01
  2.90424943e-01 -1.97101459e-01  1.88078046e-01  8.08321685e-02
  1.39625639e-01  1.06880754e-01 -1.70674264e-01 -1.34215564e-01
  1.88448697e-01  1.79278910e-01 -4.03912663e-01  2.05313429e-01
  6.92804754e-02  4.42086793e-02  1.54339045e-01 -2.32013687e-01
 -9.86246467e-02 -2.29565889e-01  2.00805932e-01 -1.68304592e-01
  3.66633475e-01 -8.77098292e-02 -1.54694557e-01 -1.67897418e-01
  4.18614037e-02  2.67894194e-03  6.40288442e-02 -3.19043905e-01
 -2.79142886e-01  2.23812193e-01  3.97759706e-01 -7.99530149e-02
 -2.67518401e-01 -2.08994523e-01  2.51977801e-01  2.54087728e-02
  3.46565276e-01  3.95312682e-02  4.66958284e-02  4.93713282e-02
  6.06967136e-02  3.99771184e-01 -7.83760101e-02  2.95606285e-01
 -1.46830112e-01  2.51662046e-01 -2.94219375e-01  8.38035345e-02
 -1.52241349e-01 -7.82732740e-02 -4.29677963e-02  1.03899419e-01
  1.62904531e-01 -1.19850874e-01 -1.52057037e-01 -8.13469961e-02
 -3.04987788e-01  1.25842839e-02 -4.41054553e-02  3.75234932e-02
  7.26300552e-02  2.95720756e-01  9.76226851e-02 -3.58443737e-01
 -2.05966413e-01  7.35397220e-01  1.00375004e-01 -1.22001633e-01
  1.03232495e-01 -3.00344437e-01 -4.75613654e-01 -3.65333021e-01
  3.27982530e-02 -2.26551637e-01 -8.53623077e-03  5.21226168e-01
  1.85683090e-03  1.80141434e-01  2.71188855e-01  1.12209782e-01
 -4.55546081e-02  1.28185570e-01  1.81724876e-02  2.71257669e-01
  2.12788492e-01  1.28556013e-01  4.67828885e-02  5.14132977e-01
 -1.15176216e-01  9.06747505e-02 -2.64419377e-01 -1.68713167e-01
  1.14874572e-01 -5.33003882e-02  2.58618981e-01 -4.21411276e-01
  1.29595697e-01  3.51077557e-01 -3.42248023e-01  1.24754816e-01
 -3.49916250e-01 -2.02307366e-02  2.83108056e-01 -2.14716852e-01
  3.21993262e-01 -1.49327204e-01  2.90975034e-01  3.19439501e-01
  2.42562905e-01 -2.78045714e-01 -3.16852391e-01  8.51885378e-02]"
`tensorflow-macos` 2.13.0 missing x86 wheels stat:awaiting response type:build/install subtype:macOS TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS 13 x86

### Mobile device

N/A

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tensorflow-macos` 2.13.0 release is [missing x86 wheels for macOS](https://pypi.org/project/tensorflow-macos/2.13.0/#files). This means that using tools like [Poetry](https://python-poetry.org/) or [`pip-tools`](https://github.com/jazzband/pip-tools) to lock versions, it's not possible to use 2.13.0 because if it's locked to that version, it will fail to install on x86 Macs.

[x86 wheels were provided for 2.12.0](https://pypi.org/project/tensorflow-macos/2.12.0/#files).

### Standalone code to reproduce the issue

From an x86 macOS machine:

```shell
python -m pip install tensorflow-macos==2.13.0
```


### Relevant log output

```shell
ERROR: Could not find a version that satisfies the requirement tensorflow-macos==2.13.0 (from versions: 2.9.0, 2.9.1, 2.9.2, 2.10.0, 2.11.0, 2.12.0)
ERROR: No matching distribution found for tensorflow-macos==2.13.0
```",False,"[-0.34249374 -0.27614433  0.05277104  0.0823989   0.17692745 -0.5009201
 -0.0914093   0.03258165 -0.305314   -0.3895287   0.16156688  0.00176918
 -0.22344838  0.1198274  -0.20030555  0.30788314 -0.31008098 -0.29632932
  0.32349396  0.09861253 -0.16663542 -0.06513082 -0.20060217  0.14087947
  0.11490199  0.26044446 -0.16370654  0.19258834 -0.10875356  0.20240235
  0.3981124   0.17101511  0.02698562 -0.01562835  0.29809088  0.21220143
 -0.17729513 -0.32245395 -0.5408299  -0.03002632 -0.15440752  0.07622784
  0.27289006 -0.10659537 -0.08754618 -0.29504496 -0.03628081 -0.24318773
 -0.02618455 -0.3733887   0.1433543  -0.11717034 -0.23101394 -0.3799473
 -0.18515015  0.06357527  0.02838598 -0.02826637  0.26384497  0.29563814
  0.26153848  0.03339945 -0.06668195 -0.07038994  0.23618157  0.11822173
  0.23401994 -0.21940526  0.4419679  -0.15616038  0.13327068 -0.1477492
 -0.32876724  0.08965188 -0.06733619  0.1846295   0.16917685 -0.01150774
 -0.00807577 -0.29290262 -0.27104163 -0.17651582 -0.00956696 -0.09412088
  0.19575937 -0.1628946   0.22872472  0.21692145  0.3371688  -0.19636215
  0.3262906   0.25583148 -0.01264428 -0.18271318  0.4344582  -0.00360507
  0.1661348   0.20453358 -0.02093472 -0.01195255 -0.03299865 -0.12560198
  0.15330853 -0.05575458 -0.12309404 -0.07927497  0.2460599  -0.2355769
  0.06437159 -0.11551839  0.09755652  0.06233724  0.30525303  0.06903216
 -0.13403377 -0.15293482 -0.47874779  0.02614503  0.06914269  0.9818089
 -0.07614458 -0.22472157  0.00885157  0.23471734  0.4172863   0.04944546
 -0.36747614 -0.06686298 -0.04841313 -0.1029161   0.02805145  0.02105817
  0.15580687  0.31102124 -0.07911216  0.04108283 -0.14358667 -0.19493404
 -0.17007561 -0.09815145 -0.17752692  0.0413626   0.02476286 -0.630596
  0.17726594  0.01050308 -0.40714     0.24660796 -0.21216792  0.02307983
 -0.09742387  0.07168035  0.03467084  0.3337358   0.2311788   0.02013426
  0.5632542  -0.00794615 -0.24563274 -0.5029441   0.05996685  0.3276559
 -0.18387844 -0.01426345  0.14364381  0.06811258 -0.445303   -0.29362544
  0.01473322  0.34595767 -0.0511554  -0.15988083  0.07784197  0.157449
  0.14358485 -0.15674213  0.43087232 -0.53268254 -0.13011011  0.3590349
  0.04986066  0.02812687  0.18110302  0.26344752  0.13504137  0.11613244
  0.02807839  0.28937238 -0.27246916  0.1934607  -0.40909815 -0.00419586
  0.42629772  0.00640836 -0.06486456  0.10468338  0.09184008  0.01766229
 -0.10929325  0.11318332 -0.1843027  -0.02484269 -0.13976447 -0.1302633
 -0.01909598 -0.53144425 -0.09764507 -0.2419901  -0.47327846  0.02541998
  0.21498936 -0.5115806   0.2880068   0.03144653 -0.3413126   0.1617838
  0.12904656  0.07547878 -0.12836592  0.20754087 -0.13746284 -0.24229005
 -0.23603398 -0.23706634 -0.29846993  0.17935562 -0.33242714 -0.06906623
 -0.04911309  0.18423691  0.09002254 -0.03895757  0.31973025 -0.02216918
  0.39178866 -0.14608762 -0.07834508 -0.24277356 -0.08578141  0.16912535
 -0.47372353 -0.15007512  0.14161743  0.02812481  0.451863    0.31136167
 -0.10893458 -0.09789996 -0.3622144   0.08737823 -0.2203457   0.41416997
  0.2590649   0.22595407  0.26844043  0.22334051  0.15025641  0.23698297
  0.2162844  -0.16079351  0.24102995  0.11988293  0.06756292  0.3958341
  0.18619868  0.30774388 -0.2498583   0.47446117  0.13174748  0.00668497
  0.08263423 -0.29288247  0.8003628  -0.5377926  -0.00655171  0.06027582
  0.29848284  0.06559725  0.03869234 -0.0058714  -0.00602553  0.4877597
 -0.39503545  0.15642172  0.07666377 -0.24500252 -0.01022342 -0.7670676
 -0.23676175  0.06055712 -0.30710775  0.09048508 -0.25713205  0.12632278
 -0.20662402  0.2731632   0.11014733 -0.12455461  0.13109693  0.28804502
  0.0255705   0.16785905  0.39676207 -0.5808847  -0.01087031 -0.05116717
  0.29334754  0.31688684  0.37388352 -0.3963365   0.13747512 -0.15312755
 -0.1265797   0.4997646   0.18642715  0.13955434 -0.38311905  0.65975076
  0.13345903 -0.03916664  0.19025418 -0.11818705 -0.37786335  0.02429009
  0.43889362 -0.02445312  0.05751394 -0.50358015  0.10158571  0.11701034
  0.03760025 -0.13823439 -0.00424947  0.31700653 -0.2635735   0.02124331
 -0.2961655   0.159993   -0.1216295  -0.30880678 -0.195147   -0.10254815
 -0.01215008 -0.04057879  0.04647861 -0.2720567   0.35989118  0.5031376
 -0.1394571   0.08826017 -0.13795611  0.22919305 -0.5484049  -0.02530269
  0.02359307  0.3329517  -0.04454258 -0.0209208   0.4108335   0.19580735
 -0.2592339   0.14306831 -0.556779    0.16167885  0.1315425  -0.23606595
 -0.34500903 -0.10028782 -0.05385232  0.20756307 -0.13778165  0.03366365
 -0.26376295  0.23744044  0.58541703 -0.32152715 -0.31789902  0.02927546
  0.0164018  -0.19105464  0.04126154 -0.10280696  0.09757681  0.02324804]"
"Cannot type ""I Accept"" to extract from hexagon_nn_skel stat:awaiting response stale comp:lite","Workflow:
1. download hexagon_nn_skel.run from [this link](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.1.run)
2. adb push to /data/local/tmp
3. run cmd: chmod +x tflite_hexagon_nn_skel_v1.20.0.1.run
4. run cmd: .\tflite_hexagon_nn_skel_v1.20.0.1.run
5. **Extraction aborted**: 
![Screenshot 2023-07-25 182129](https://github.com/tensorflow/tensorflow/assets/68681893/09484ae3-7156-4c61-9612-31c57fba8aa3)

Problem: 
After the program shows ""**_Type ""I ACCEPT"" if you agree to the terms of the license:_**"", it didn't give me time to type ""I ACCEPT"", and hence extraction aborted.
",False,"[-5.79593241e-01  6.08125702e-03 -3.51078361e-01  2.65415728e-01
  1.94045365e-01 -2.24042147e-01  5.16265258e-03  4.82974984e-02
 -1.25237495e-01 -1.77578866e-01  5.70513122e-03 -1.72254108e-02
  8.64048488e-03 -1.75010294e-01 -1.51465029e-01 -9.24140513e-02
 -2.27575660e-01 -1.64846912e-01  3.47930372e-01  1.42535537e-01
  2.17209503e-01 -2.08265334e-03 -1.26694530e-01 -3.09717208e-02
  1.88089818e-01  7.74395168e-02 -2.28166074e-01 -8.42502415e-02
  3.29810493e-02  1.15354121e-01  5.02327144e-01 -1.59389284e-02
 -7.23026022e-02 -1.38214529e-01  4.68312949e-03 -2.02895969e-01
 -1.95097297e-01 -5.59537783e-02 -1.03194952e-01 -5.47378361e-02
  2.62835979e-01  3.57290916e-03 -6.71459883e-02  1.71008885e-01
 -1.32508084e-01 -1.18635073e-01  2.43454710e-01 -4.44334522e-02
  1.50717258e-01 -8.15877467e-02 -3.15096080e-01  1.05612770e-01
 -1.84218600e-01 -1.84912086e-01 -1.45098239e-01 -1.02430671e-01
 -1.09930329e-01  2.02756062e-01  8.05980042e-02  1.16121493e-01
  3.01334232e-01  9.14969668e-03 -1.78969830e-01  6.40953481e-02
  2.60216370e-02 -9.06617045e-02 -1.91969514e-01 -2.74163336e-01
  4.24079984e-01 -2.94933379e-01  1.32178720e-02  2.97076434e-01
 -8.45008492e-02  5.54005206e-02 -1.05350658e-01  8.03596824e-02
 -3.68803620e-01  1.44222945e-01 -6.86119497e-02 -3.98808599e-01
 -2.47444175e-02 -1.50344759e-01  1.39914572e-01  1.05013609e-01
  6.83190823e-02  2.20010564e-01  1.29388034e-01  1.75918743e-01
  3.04011941e-01  2.34559961e-02  2.85028756e-01  2.55995065e-01
 -2.13581827e-02 -1.89654101e-02  7.79302716e-02 -5.30913696e-02
  2.39756837e-01  3.03506523e-01 -2.47021958e-01 -4.97860275e-02
 -2.14037765e-03 -1.81558669e-01 -1.85912564e-01  2.58884970e-02
  1.37315691e-01 -3.38629559e-02  5.03055491e-02  3.44990641e-01
  6.58904687e-02  1.54913127e-01  2.39247859e-01  1.26359239e-01
 -8.91120285e-02  1.97511822e-01  1.52517557e-01  9.34630483e-02
 -2.77598530e-01 -2.77243286e-01 -1.76431313e-01  3.80824387e-01
 -2.37106696e-01 -1.25622988e-01 -6.78590089e-02  2.58251190e-01
  4.86035943e-01 -6.27869666e-02 -2.60525316e-01 -1.64351668e-02
 -3.02555859e-02  3.57326448e-01  6.58302456e-02  9.00010392e-02
 -3.13826382e-01 -2.80983038e-02  4.26334292e-02 -1.03240907e-01
 -3.24257612e-01 -2.48333663e-01 -3.14465165e-01  3.34465265e-01
  5.80706671e-02  2.36365050e-01 -1.07975274e-01 -2.47877091e-01
 -1.28590316e-01 -2.94653594e-01  6.93499111e-03 -8.37753713e-02
  7.74572492e-02  1.56241283e-02 -2.56105304e-01  3.63414362e-02
 -4.22793299e-01  3.80387127e-01 -3.75744700e-02  1.13292903e-01
  1.13626808e-01  5.73975593e-02  4.57457975e-02 -5.41659355e-01
 -2.49888226e-02  3.16815078e-01  1.06800541e-01  4.52339768e-01
  2.05473974e-01 -1.31499451e-02 -4.62688744e-01 -6.20864406e-02
  2.43821573e-02  2.14548141e-01 -1.56839818e-01  2.85198409e-02
 -4.72649559e-03  1.46932472e-02  1.74254537e-01  8.67767334e-02
  5.51973343e-01 -1.92740440e-01 -1.24097161e-01  2.54238963e-01
  7.01769367e-02 -1.66074842e-01 -5.62011376e-02  1.01229019e-01
 -6.68345690e-02  2.73755759e-01  7.72044212e-02  1.61965385e-01
 -8.55086744e-02  5.00058755e-03 -2.03108758e-01  3.60173821e-01
  1.12082332e-01 -1.08355932e-01 -2.97882855e-01  1.75243989e-02
 -1.40565604e-01 -3.24894711e-02  2.79486954e-01 -4.57827039e-02
  2.87854001e-02  5.57361990e-02 -2.83444207e-02 -1.06836416e-01
  2.07003236e-01  3.82186845e-02 -2.21956179e-01 -3.60974073e-01
 -1.32611737e-01 -1.45284638e-01  4.88993227e-02 -5.76653302e-01
 -2.69639254e-01 -1.45163149e-01 -1.75657019e-01  1.94742665e-01
  1.59797013e-01  5.48709333e-02  7.26957992e-03  1.29233092e-01
 -3.28682810e-02 -4.60343547e-02 -1.29559562e-01 -7.80218691e-02
 -4.01623487e-01 -2.26516068e-01  1.71542317e-01  1.66176170e-01
  1.85897085e-03  4.27962959e-01 -5.06615937e-02 -4.42313030e-02
  4.96101618e-01 -2.52656154e-02  2.57177413e-01  2.01868057e-01
  7.45621696e-02 -2.54665971e-01  2.59091686e-02  7.36696124e-02
 -1.03316627e-01 -2.24307284e-01 -1.68745562e-01 -2.83814549e-01
 -1.20847188e-01 -2.45129630e-01 -1.22575372e-01  8.40525776e-02
 -1.63756102e-01  2.84424901e-01  1.01451121e-01 -4.09769267e-03
  1.25476569e-01  2.80748785e-01  1.56934053e-01  2.03556001e-01
 -2.82278419e-01 -2.47481689e-01  2.26246566e-01 -7.58375451e-02
 -7.02231005e-02  5.62081337e-01 -4.77352589e-02  4.39425498e-01
  6.79042563e-02 -1.11010678e-01 -3.69803309e-01  1.55214667e-02
 -5.31537384e-02  1.99547321e-01  1.75665528e-01 -2.35176131e-01
  1.50395244e-01 -3.50148790e-02  1.39875382e-01 -1.27871394e-01
  2.72208333e-01 -2.96852231e-01  1.99094415e-01  2.06561953e-01
  3.32505047e-01  4.08514470e-01 -1.68156669e-01 -6.30074441e-02
  2.82793343e-01 -2.68473048e-02  5.47116958e-02 -4.99819010e-01
 -3.40507746e-01  2.80505456e-02 -1.49361461e-01  1.98773444e-02
  2.45827492e-02  1.21231683e-01 -1.37547910e-01 -1.41711831e-01
  4.21995103e-01 -2.79153049e-01  9.54758227e-02  5.92080839e-02
  1.03733083e-02  1.95485055e-01  4.10172809e-03 -4.37740415e-01
  1.33792266e-01  6.91806003e-02  1.17680237e-01  3.74551187e-03
  3.78346503e-01  8.38050395e-02  1.87515616e-01 -9.85205024e-02
 -8.43120143e-02 -4.34720889e-04  7.38179982e-02  2.41710767e-01
 -1.48056149e-01  2.54330665e-01  4.99222577e-02 -3.19814682e-02
  2.55454004e-01 -5.21061480e-01 -1.05300307e-01 -4.70756516e-02
  1.55164897e-01  1.92270219e-01 -2.80977488e-01 -3.90911728e-01
 -6.07642382e-02  1.80994626e-03  1.56306833e-01 -2.08822250e-01
  7.33057559e-02  3.06531526e-02 -1.41383693e-01  5.84703982e-02
  1.53525949e-01  1.64292544e-01 -8.20289329e-02 -4.06143665e-01
 -2.98373818e-01 -3.53315800e-01  2.01271430e-01 -1.64491445e-01
  2.95764089e-01 -9.69318300e-02  1.08690426e-01  1.68337375e-01
  2.56971002e-01  1.27866119e-01 -1.48460686e-01  7.69705027e-02
 -2.68152654e-01 -1.84275210e-02  1.49709314e-01  2.51173347e-01
  4.05860692e-03  1.76884197e-02  1.20624952e-01  2.44928598e-01
  1.15637250e-01  9.42090601e-02 -4.45141420e-02 -2.30384469e-01
 -1.51126981e-01  9.42865312e-02 -2.93634646e-02  1.38703380e-02
  3.15140218e-01 -2.18245164e-02 -2.15701342e-01 -1.41956449e-01
 -2.54641443e-01  3.31389084e-02  2.93823898e-01 -1.32010221e-01
 -4.65983078e-02 -2.51989037e-01  1.43085197e-01 -7.83176199e-02
 -1.74147516e-01  7.36879110e-02 -6.00135028e-02  1.08947545e-01]"
float8 support for array ops stat:awaiting tensorflower type:feature comp:ops TF 2.12,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12.0

### Custom code

No

### OS platform and distribution

macOS-13.2.1-arm64-arm-64bit

### Mobile device

_No response_

### Python version

3.9.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Please add FP8 datatype support for array ops (like Reshape, Transpose, GatherV2, ExpandDims, Squeeze, ConcatV2, Split, Pack, Unpack, and StridedSlice).

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.framework import dtypes

a = tf.constant([[1.2345678, 2.3456789, 3.4567891], [4.5678912, 5.6789123, 6.7891234]], dtype=dtypes.float16)
print(a)

a_fp8 = tf.cast(a, dtypes.float8_e4m3fn)
print(a_fp8)

b = a_fp8[1:2] # tensorflow.python.framework.errors_impl.NotFoundError
b = tf.transpose(a_fp8, [1, 0]) # tensorflow.python.framework.errors_impl.NotFoundError
```


### Relevant log output

_No response_",False,"[-3.74273956e-01 -3.00024539e-01 -1.44609690e-01 -7.82916099e-02
  1.64610803e-01 -6.10721648e-01 -7.44749531e-02 -4.56738696e-02
 -4.33604330e-01 -3.61232579e-01  8.32244195e-03  5.16764149e-02
 -3.02779019e-01  1.20422974e-01 -1.86407715e-01  2.58076727e-01
 -6.99143261e-02  1.64065108e-01  2.55729914e-01  1.89165577e-01
 -1.13330476e-01 -1.33026838e-01 -1.48334295e-01  9.72780585e-02
  9.61050391e-02  4.13931429e-01 -3.10571671e-01  1.32015631e-01
 -9.04217064e-02  2.18036115e-01  4.25496936e-01  1.36626884e-02
  1.81586295e-01  1.78575724e-01 -2.18595088e-01  2.08371416e-01
 -2.53482580e-01 -1.85761556e-01 -2.75018483e-01  8.69489312e-02
 -2.22622156e-02  1.75599515e-01  2.58202016e-01 -1.92166045e-01
  1.33786395e-01  4.55182716e-02 -2.35209197e-01  5.55669256e-02
  3.33846062e-02 -1.16478756e-01 -7.28343949e-02  1.15702916e-02
 -2.82736242e-01 -3.13385963e-01 -4.65038344e-02 -8.29752088e-02
 -4.06269208e-02 -1.99434772e-01 -1.46040201e-01  4.36561443e-02
  7.59745538e-02 -6.56766295e-02  2.37089515e-01 -1.27395540e-02
  1.29107594e-01  1.50975838e-01  3.88459563e-01 -3.90035659e-02
  4.20408100e-01 -3.26490223e-01  9.10129696e-02  1.08951062e-01
 -3.38278711e-01  8.75204057e-02 -9.37212780e-02  3.26254994e-01
 -7.91705847e-02  2.66672913e-02  3.38440537e-01 -1.96926042e-01
  6.92045763e-02 -4.38909866e-02  2.90243458e-02  5.00384755e-02
  1.34530097e-01 -1.10752545e-01  3.49629492e-01  2.45121464e-01
  3.21627676e-01 -1.30931109e-01  4.83229518e-01  5.92409134e-01
 -1.59473941e-01 -5.43909259e-02  4.25312400e-01  1.67679787e-01
  3.65654320e-01 -1.21956328e-02 -2.91146208e-02 -1.18316717e-01
  1.03965938e-01 -3.83469284e-01 -1.36736529e-02 -2.78312080e-02
 -5.92618436e-02 -1.55205920e-01  8.40113461e-02 -5.96176162e-02
  4.84295115e-02  1.07725300e-01  3.63421679e-01  6.25894666e-02
  2.01487467e-02 -1.21506281e-01 -3.80364686e-01 -1.06857017e-01
 -2.12070823e-01 -7.82371983e-02 -6.99204057e-02  5.70135117e-01
  2.84666363e-02 -1.41990185e-01 -1.36420310e-01  3.85912001e-01
  5.28700113e-01  1.51745752e-01 -3.46816152e-01 -1.09953806e-02
  2.80952919e-03  9.34061557e-02  2.38086805e-01 -2.15987623e-01
 -1.13435760e-01  6.06772900e-02  7.54671767e-02  2.52115488e-01
 -6.72677383e-02 -2.24802285e-01 -5.29913902e-01 -1.70703337e-01
 -5.60890734e-02  3.32255512e-02 -7.89720267e-02 -6.97662354e-01
  3.86989474e-01  3.47749650e-01 -5.03257103e-02  4.85533848e-02
 -1.03861503e-01  6.90288842e-02 -1.43705785e-01 -1.07390592e-02
 -1.95933372e-01  5.28957784e-01  2.96996888e-02  1.09889336e-01
  3.33308220e-01  8.20139647e-02  5.53885251e-02 -5.25146723e-01
 -1.08406976e-01  3.98458779e-01  8.82404447e-02 -1.98594540e-01
 -5.27352840e-02  1.06833383e-01 -3.67332757e-01 -1.50372162e-01
 -3.27649772e-01  6.15799069e-01 -1.89401209e-01 -2.89867818e-02
 -9.46975127e-02 -2.33554784e-02  2.58203149e-01 -4.16979268e-02
  2.31494516e-01 -3.77679318e-01  8.48504677e-02  2.19923794e-01
  1.23781711e-01  1.07358657e-01  3.96731317e-01  1.22319251e-01
  7.72525817e-02  5.28357774e-02  1.29766151e-01  2.00548947e-01
 -2.86390811e-01  1.80496909e-02 -5.27971387e-01 -2.72788256e-01
  3.87773097e-01 -5.13262115e-02 -1.05627641e-01  7.10482001e-02
  3.60661894e-02  5.44206277e-02  1.50732875e-01  7.80045241e-02
 -2.16229856e-01 -5.50465845e-03 -2.46644080e-01  8.25178530e-03
  1.12962104e-01 -1.42944202e-01 -2.26677686e-01 -6.73371851e-01
 -4.08652663e-01  2.11286783e-01 -6.00896962e-03 -4.00270820e-01
 -1.26787290e-01  1.38280809e-01 -2.13298649e-01  3.84901255e-01
 -4.07420844e-03  8.24998319e-02 -1.26322225e-01 -5.33113405e-02
 -2.07947008e-02 -2.98032075e-01 -5.96087314e-02 -3.54915679e-01
 -3.39907229e-01  1.92940265e-01 -6.26753092e-01  8.10254812e-02
 -7.07572252e-02  1.82576746e-01  1.08100913e-01 -1.22028947e-01
  4.56763208e-01  4.81944755e-02  1.75720990e-01  1.88804306e-02
 -9.98490900e-02 -1.02329031e-01 -2.38485128e-01  1.18221715e-02
 -6.31897688e-01 -1.00380167e-01  2.70212293e-02 -7.70645216e-03
  3.69642556e-01  4.62780982e-01  1.68324471e-01 -1.81430541e-02
 -3.28603804e-01  2.15314209e-01 -1.68693751e-01 -7.15758279e-03
  3.89924914e-01  1.19517736e-01  4.39606607e-01  8.30140710e-02
  2.58383840e-01  1.64982229e-01  2.04300135e-01 -4.73502487e-01
  2.59722769e-01  1.30965620e-01  1.78683609e-01  4.45306003e-01
  1.73583850e-01  4.91834655e-02 -2.71304488e-01  6.43347383e-01
  1.45744726e-01 -2.49510467e-01  1.29912436e-01  1.31064326e-01
  8.11797976e-01 -3.80155087e-01 -2.36745834e-01 -1.90277204e-01
  2.68738210e-01  1.59082226e-02 -1.31348789e-01  5.89648560e-02
 -6.20558858e-05  2.26381004e-01 -3.26515615e-01  3.37757617e-02
 -8.86172205e-02 -4.90430653e-01  1.80600762e-01 -3.85490417e-01
 -3.50429118e-02  9.05867592e-02 -3.50325495e-01 -1.81248039e-01
 -2.46267065e-01  1.87625036e-01 -3.18578809e-01 -4.92094010e-02
  1.01576626e-01 -1.19671479e-01 -2.53792685e-02  2.76648670e-01
 -8.25711638e-02  1.43426999e-01  3.83499801e-01 -3.12701702e-01
 -2.25640953e-01  5.07876500e-02  3.74259800e-01  1.03813902e-01
  5.10496855e-01 -3.81953120e-01  1.21024705e-01 -1.92998976e-01
 -3.53959143e-01  3.46744895e-01  6.91413879e-02  1.52893513e-01
 -2.85912156e-01  5.93415737e-01  5.85087854e-03 -1.05817467e-01
  9.62845385e-02 -2.58741915e-01 -2.06859246e-01  7.13186935e-02
  1.76340297e-01 -3.29720639e-02 -1.77317560e-01 -5.05075753e-01
  2.12768823e-01  3.61004137e-02 -3.46214660e-02  1.80763099e-02
 -2.58516036e-02 -1.82180524e-01 -1.09730344e-02 -3.58560443e-01
 -2.17347160e-01  6.35598898e-02 -1.59679174e-01 -1.86437294e-01
 -2.00822562e-01 -2.44767100e-01 -2.51348734e-01 -3.80854517e-01
  8.35290104e-02 -2.70898640e-01  4.75385129e-01  3.88029635e-01
 -1.46330744e-01  1.81160539e-01  2.25457326e-01  4.88824472e-02
 -1.29807219e-01  1.98197782e-01  1.02001600e-01  3.75755310e-01
 -1.37236714e-03 -7.51535445e-02  8.71066228e-02  1.35631293e-01
 -2.66730249e-01  3.47310185e-01 -4.97080326e-01  3.86566743e-02
  2.70547003e-01 -2.91966647e-01  4.42299619e-02 -2.87779033e-01
  1.81660533e-01  3.00719380e-01 -2.07006514e-01  1.44660145e-01
 -2.84701526e-01  5.26833594e-01  5.51374495e-01 -3.38261306e-01
 -3.22882645e-02  2.08068360e-03  4.11446214e-01  9.18784812e-02
  2.33231962e-01 -1.43808991e-01  4.99573760e-02  1.21589795e-01]"
Performance drop with tensorflow 2.13 stat:awaiting response stale comp:keras type:performance TF 2.13,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

Debian GNU/Linux 12 (bookworm)

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I notice a big performance drop between tensorflow 2.12 (CPU) and tensorflow 2.13 (CPU). With the last release (and also with tf-nightly '2.14.0-dev20230724') it takes *4 times* longer to perform a simple sum. It is between keras inputs though some I am not sure if this is directly related to tensorflow or if it comes from keras.

See code below for a very simple example. Note that this is with tensorflow CPU only.
The timings are:
- for tensorflow 2.12.1 + keras 2.12.0: **5.3 s**
- for tensorflow 2.13.0 + keras 2.13.1: **24 s**
- for tensorflow 2.14.0-dev20230724 + keras  2.14.0.dev2023072407: 26 s

### Standalone code to reproduce the issue

```shell
import time

from tensorflow.keras import Input


number_of_executions = 3000

x = Input((1,))
y = Input((1,))

start = time.time()
for i in range(number_of_executions):
    x + y
duration = time.time() - start
print(f""Duration: {duration:.1f}"")
```


### Relevant log output

_No response_",False,"[-3.27705175e-01 -4.53171015e-01 -2.27686942e-01 -9.48772393e-03
 -4.14140187e-02 -4.74406928e-01 -4.23620008e-02  2.57759430e-02
 -3.95218521e-01 -2.67454475e-01  2.35462338e-01  2.45425805e-01
 -1.53490886e-01  3.85722779e-02 -2.03831464e-01  1.02827147e-01
  1.69180691e-01  2.62170546e-02  2.43209124e-01 -1.80343930e-02
 -2.33008593e-01 -1.52592123e-01 -7.81420842e-02  2.10105717e-01
  1.77561194e-01  2.71321297e-01 -1.80123508e-01 -2.81799696e-02
 -1.89225972e-01 -4.40552533e-02  2.97787011e-01  1.87318712e-01
  2.80794725e-02 -1.22502193e-01 -3.88619065e-01  4.85372514e-01
 -1.16296858e-01 -3.33382785e-01 -2.37971663e-01  1.15118012e-01
 -8.51002559e-02  1.06466778e-01  8.18294436e-02 -1.41593486e-01
 -2.12576464e-02 -1.50966197e-01 -4.39163521e-02 -2.52568275e-02
 -1.31992370e-01 -3.62102270e-01 -5.94832450e-02  1.16308153e-01
 -2.12999225e-01 -3.07895243e-01 -1.91470325e-01 -8.78454447e-02
 -1.58986449e-02 -1.30010456e-01 -8.14508051e-02 -2.37333551e-02
  1.37146562e-01  6.97356276e-03  5.34835830e-02 -1.11472473e-01
  2.47245789e-01  1.46942675e-01  3.62854898e-01 -1.34334743e-01
  4.35813129e-01 -6.83677942e-03 -1.18486919e-01 -4.72438484e-02
 -5.29575229e-01  1.43381685e-01 -8.57289955e-02  3.20171505e-01
  1.97296306e-01 -1.06019646e-01  4.55494583e-01 -1.03883028e-01
  7.46821687e-02 -4.37074244e-01 -7.55615458e-02 -3.18389356e-01
  1.15303829e-01 -1.35705054e-01  5.61964333e-01  1.43400222e-01
  3.85968268e-01 -3.60323459e-01  4.59391117e-01  7.22542703e-01
 -2.78017014e-01 -9.11653340e-02  3.95972013e-01  1.21414751e-01
  1.54618938e-02  1.30966119e-02  1.61056966e-03 -1.91356428e-02
  9.72984359e-04 -1.31689027e-01  1.61041170e-02  6.26002401e-02
 -1.38856858e-01 -3.54278445e-01  3.03035200e-01 -1.89221442e-01
  2.59200931e-01  1.98152781e-01  1.89168379e-01 -1.47970483e-01
  1.39438704e-01  1.17387362e-01  1.11731634e-01  2.28607327e-01
 -1.59157574e-01  4.55297939e-02  5.29300869e-02  8.61120582e-01
 -3.39786820e-02 -1.29085362e-01  2.04553336e-01  1.26764238e-01
  3.46039593e-01  1.34415299e-01  3.58140990e-02  3.96924205e-02
 -1.31166086e-01 -1.71500862e-01  1.78651549e-02 -1.49520636e-02
 -6.42729551e-02  2.42920071e-01 -1.99407816e-01  1.53365582e-01
  1.28541782e-01 -2.59429932e-01 -3.85224372e-01 -2.42791742e-01
 -2.83307165e-01  3.29441130e-01  1.00217806e-02 -3.86756837e-01
  3.43529910e-01  1.32008746e-01 -2.15882182e-01  1.14875793e-01
 -1.96341500e-02  1.64461687e-01 -1.23645827e-01  1.51361421e-01
  2.55280316e-01  2.08890110e-01  4.02343392e-01  1.78242147e-01
  1.75823033e-01 -1.65536284e-01  1.74935367e-02 -5.85039496e-01
 -3.74229588e-02  3.37260246e-01 -1.24669835e-01 -2.62932539e-01
  6.32581562e-02  1.97411686e-01 -2.53797948e-01 -3.35472882e-01
 -1.38430148e-01  4.85387266e-01 -2.85409182e-01 -1.14237301e-01
 -1.19907930e-01  7.08585009e-02  2.37386495e-01  1.06009468e-03
  1.13444813e-01 -6.89426482e-01  1.13605298e-01  2.41721705e-01
 -5.58118038e-02  3.39911580e-01  2.87624955e-01  4.37114954e-01
  1.38371093e-02  1.31625691e-02  1.64744914e-01  1.18962519e-01
 -3.16780180e-01  1.89480577e-02 -3.68881732e-01 -4.09318864e-01
  3.60862136e-01 -1.91678718e-01  2.45708227e-02 -1.57724954e-02
  2.62371421e-01 -6.39382601e-02 -8.03466737e-02 -1.07494853e-01
 -1.26972757e-02 -3.33078414e-01 -1.63800299e-01 -6.89797252e-02
  1.26332805e-01 -5.09597957e-01 -1.68190941e-01 -2.00979918e-01
 -1.85528010e-01 -3.68979163e-02  9.22581404e-02 -7.17871428e-01
  1.70711651e-02  1.20341927e-01 -3.01591337e-01 -3.68835405e-03
  1.50596097e-01  3.37544322e-01 -1.78266123e-01  4.01602417e-01
  4.86949459e-02 -2.14994639e-01 -5.68953007e-02 -3.99118543e-01
 -2.83163548e-01  5.99213317e-03 -4.06752765e-01  2.89785087e-01
  5.88935241e-02  3.22825670e-01 -1.14464173e-02  8.02065507e-02
  3.12629342e-01  2.07564116e-01  1.38841867e-01 -2.65794694e-01
 -1.24429643e-01 -2.72485297e-02 -1.68583080e-01 -4.93833087e-02
 -3.80486190e-01 -1.30085833e-02  2.15078071e-01 -2.12957263e-01
  3.77518207e-01  4.47849512e-01 -1.49857104e-01 -3.20862904e-02
 -3.44163984e-01  1.89518169e-01 -2.74601132e-01  1.67552218e-01
  1.08401708e-01  6.77437410e-02  5.52881360e-01  1.89016070e-02
  1.04437694e-01  2.58843787e-02  4.01398420e-01 -1.38544187e-01
  3.49862695e-01  2.32018590e-01 -6.42307475e-02  6.09533072e-01
  3.59083772e-01  3.55715930e-01 -3.66658509e-01  6.10380054e-01
  5.28740548e-02 -2.48152018e-01 -1.22918531e-01 -4.35832113e-01
  5.38533211e-01 -4.24632967e-01  2.08880067e-01 -2.26741359e-01
  2.84664035e-01  1.86694622e-01 -3.28330435e-02  7.12991282e-02
  8.57725739e-02  1.92679763e-01 -4.15620983e-01 -2.29852460e-03
 -2.06940934e-01 -3.52200508e-01 -9.29691717e-02 -5.74779451e-01
  3.79149988e-02 -5.58505058e-02 -1.25388145e-01  2.25074947e-01
  2.20797032e-01  1.39486074e-01 -1.25574291e-01  1.67662799e-01
  2.21169323e-01  1.03732817e-01  2.11330906e-01  7.52858520e-02
 -4.05540943e-01 -6.74449056e-02  4.83066082e-01 -4.44479942e-01
 -2.72366881e-01  7.03858733e-02  2.42276907e-01  2.01664656e-01
  2.28927135e-01 -2.66887367e-01  2.83657521e-01  1.62685066e-01
 -5.07197715e-02  3.66021454e-01  8.75449032e-02  8.73888433e-02
 -3.06926578e-01  6.75654590e-01 -1.47382729e-02 -7.66808689e-02
  1.38779834e-01 -8.73634219e-02 -2.13544652e-01  2.96593130e-01
  2.06280529e-01 -9.03563872e-02  1.74256086e-01 -3.54087621e-01
  7.12301731e-02 -5.38308397e-02  2.52087042e-03 -2.59153366e-01
 -2.52418339e-01  2.38651365e-01  9.97567698e-02  1.04023125e-02
 -3.45440388e-01  2.65617907e-01 -9.78042781e-02 -5.11305749e-01
 -7.15578422e-02 -1.96866691e-04 -9.99834314e-02 -2.42622077e-01
  1.22310162e-01 -2.32062936e-01  5.20134032e-01  4.30819094e-01
 -2.51421154e-01  7.56185204e-02 -3.57846111e-01  2.11656705e-01
 -2.69307137e-01 -2.34504774e-01 -1.48320764e-01  4.63132024e-01
 -2.05667410e-03 -2.05570728e-01  4.03726697e-01  4.10071880e-01
 -1.08120129e-01  4.69212607e-02 -3.87439042e-01  9.57339108e-02
  8.42750221e-02 -2.69638956e-01 -9.12052467e-02 -2.40115851e-01
  2.20874920e-01  4.61684555e-01  6.83218241e-05  1.82756633e-01
 -2.41390035e-01  3.35973918e-01  4.78273839e-01 -5.61149955e-01
 -4.30441976e-01 -1.93987601e-02  7.64218867e-02 -7.13436529e-02
  4.94968295e-02 -1.04682252e-01 -7.66003430e-02  3.57410219e-03]"
converter issue  stat:awaiting response stale TFLiteConverter,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",False,"[-4.95820373e-01 -5.55413246e-01 -1.11618623e-01  6.97739571e-02
  1.42564714e-01 -1.12284891e-01 -5.34416921e-02 -6.64936081e-02
 -1.59273848e-01 -1.66428894e-01 -4.03464846e-02  6.96865842e-04
 -9.34556723e-02  1.59634918e-01 -2.89669186e-01  1.83578849e-01
 -1.46028966e-01 -1.45149946e-01  3.36968631e-01  6.28317446e-02
 -5.55406846e-02  7.32736103e-03 -3.54908794e-01  2.87237793e-01
  2.91421413e-01  2.43511349e-01 -2.22388238e-01  8.36145692e-03
  3.73113081e-02  1.69351235e-01  2.98916668e-01  7.48381019e-02
  3.14918533e-02  3.60075086e-02  2.51519550e-02  1.59470677e-01
 -1.36052787e-01 -7.22356364e-02 -2.53325701e-01 -2.57117242e-01
  2.45260969e-01 -5.94952181e-02 -4.56521064e-02  9.25214663e-02
  1.20152477e-02  5.76253012e-02  1.48367643e-01 -1.44885987e-01
 -1.66084468e-01 -3.54150161e-02 -1.78760350e-01 -4.86762859e-02
 -4.07161921e-01 -9.81203765e-02 -5.87034971e-02  5.25278524e-02
  2.54085153e-01  2.80174389e-02 -5.81309982e-02  4.91318777e-02
 -1.99949071e-02 -1.02067208e-02 -2.00075656e-01 -6.54210895e-02
  8.84734243e-02  2.90006250e-01  1.91941902e-01 -5.83168902e-02
  3.30482870e-01 -3.05452377e-01 -6.12225085e-02 -9.88683999e-02
 -1.89600781e-01 -1.70680601e-02  5.17590577e-03  6.60157204e-02
 -1.96404189e-01  2.48466864e-01  1.91782624e-01 -1.02845011e-02
 -1.06515326e-01 -1.93653017e-01 -3.85110406e-03  1.00619510e-01
  8.56358334e-02  2.16236226e-02  1.52595311e-01  1.17308483e-01
  2.49680609e-01 -1.94940746e-01  3.53129894e-01  4.19637263e-01
  1.40908748e-01 -1.77533105e-02  4.31474805e-01  1.31893203e-01
 -5.02888039e-02  2.91377664e-01  3.35225761e-01 -8.41979384e-02
 -1.17986664e-01 -2.25413084e-01 -2.52091885e-01 -1.16287850e-01
  1.21605195e-01 -1.97561026e-01  1.27211303e-01  1.36876345e-01
  2.05560401e-02 -1.16798997e-01  1.64414361e-01 -3.12148072e-02
  1.39731541e-01 -3.52621078e-02  1.47404149e-01  2.04603411e-02
  2.93100923e-02  1.04680508e-01  6.43587783e-02  6.26788497e-01
 -7.62345344e-02 -5.07274121e-02  2.00216562e-01  2.90393919e-01
  2.50940382e-01  1.34615764e-01 -1.75688326e-01 -5.56202158e-02
  3.63264084e-02  2.42301315e-01  2.27135718e-01  2.33494684e-01
 -2.17025399e-01  2.16527015e-01 -4.15218174e-02 -1.50413781e-01
 -1.48498500e-02 -3.72987874e-02 -3.06608498e-01  9.18016583e-02
 -9.78630558e-02  1.54142156e-01 -6.60644919e-02 -2.95866698e-01
 -1.32896930e-01  1.22885834e-02 -2.58633941e-01  1.88894838e-01
 -1.58062540e-02  2.44000107e-01 -6.70290217e-02  6.14313371e-02
 -5.17279543e-02  4.44581628e-01  1.09392613e-01  9.38687474e-02
  2.09238648e-01 -4.29951362e-02  5.08498475e-02 -4.80881333e-01
 -1.22876547e-01  2.46176228e-01 -2.39015192e-01 -1.35694325e-01
  4.42636907e-02  1.54899359e-01 -5.90287805e-01 -1.72990933e-01
  1.38963029e-01  2.27457494e-01 -1.04748383e-01 -1.33825451e-01
  1.18452385e-01  9.47934687e-02  1.80431396e-01  1.68411294e-03
  7.16700912e-01 -4.91523623e-01 -1.73926681e-01 -2.40840614e-02
  1.42376214e-01  3.60248052e-02 -1.71158947e-02 -6.14233017e-02
  7.45630357e-04  8.01471919e-02  1.80393845e-01  8.28326419e-02
 -4.42463875e-01  4.88416106e-02 -3.02639604e-01 -3.51649560e-02
  2.56307691e-01  5.26287109e-02 -2.87702560e-01 -2.87531409e-02
  1.76394936e-02  7.04686157e-03 -7.94097036e-02  1.04918934e-01
 -1.89247638e-01 -2.96172053e-02 -3.07000056e-02  7.23840762e-03
 -3.77786485e-03 -1.73280627e-01 -1.82928741e-01 -4.24153030e-01
 -6.81825399e-01  6.07484654e-02  1.50182441e-01 -3.73231590e-01
  6.99394420e-02 -7.49495551e-02 -1.86809644e-01  6.67126402e-02
 -1.27411515e-01 -1.42855980e-02 -2.34559625e-01  1.03325993e-01
 -9.78645831e-02  2.91685835e-02  1.67306289e-01 -1.75011814e-01
 -4.03779596e-01 -2.31484566e-02 -2.14569628e-01  3.10720384e-01
  6.71268031e-02  1.95803404e-01 -1.27153784e-01 -1.03542358e-02
  5.18305898e-01  1.87175274e-01  4.72028442e-02 -1.88716531e-01
  8.87070000e-02 -1.22638531e-01 -3.34698558e-01  8.36009532e-02
 -2.82177150e-01 -1.53106838e-01 -2.93096751e-02  4.71413322e-02
 -2.16917634e-01  2.95443296e-01  6.58566691e-03 -1.28663123e-01
 -2.92774856e-01  1.98175207e-01  1.89364403e-02 -1.90385878e-01
  4.43187177e-01  2.76314080e-01  2.57652730e-01  1.21389195e-01
 -1.36933085e-02 -2.28050929e-02  1.45626932e-01 -2.42350698e-02
  1.62149578e-01  2.29365826e-01  1.12134784e-01  5.76820016e-01
  3.19589794e-01  1.78325772e-01 -2.29820549e-01  2.19372153e-01
 -2.63303518e-01 -1.80052623e-01  9.20509249e-02 -2.62851954e-01
  2.30308652e-01 -2.35357225e-01  1.18106514e-01 -1.10653877e-01
  2.65675902e-01  1.92733034e-02 -7.78273912e-03  1.46326005e-01
  1.34007141e-01  3.94178122e-01 -5.64433455e-01  1.15042053e-01
  1.83410626e-02 -1.31833315e-01  1.33156180e-01 -4.83001173e-01
 -1.54010326e-01  1.46840632e-01 -1.52391121e-01  1.60197213e-01
  2.19710916e-02  1.02036253e-01  2.62580942e-02  2.48352848e-02
  2.53119618e-01 -5.62370270e-02  1.45413086e-01  8.97824764e-02
 -2.14764476e-01  3.20801675e-01  3.03716779e-01 -2.86400795e-01
 -1.83411673e-01  4.80400547e-02  3.49648118e-01  1.30057812e-01
  3.85468006e-01 -4.64291215e-01  2.33042479e-01  4.18512970e-02
  9.94084403e-02  1.96762487e-01  7.00729936e-02 -1.69717669e-02
 -3.39341164e-01  4.38700080e-01 -2.41163149e-02  8.22749585e-02
  2.37840235e-01 -2.26592366e-02 -3.53610069e-01  1.40552819e-01
 -5.64584974e-03  1.58640727e-01 -1.65999345e-02 -9.41833705e-02
 -2.25745708e-01  1.73838019e-01 -6.06722608e-02 -1.77394271e-01
 -8.60470086e-02  9.38739330e-02 -1.62320912e-01 -2.22361177e-01
 -1.76856160e-01  2.41583973e-01 -9.77303609e-02 -3.25056285e-01
 -1.08491734e-01 -2.81952113e-01 -1.28500722e-02 -4.50537324e-01
  8.15018639e-02 -3.29246342e-01  1.73916638e-01  3.19265366e-01
  1.80553049e-02  1.05047464e-01 -1.81122273e-02 -7.34580532e-02
 -2.85099804e-01 -3.58722210e-02 -1.11685172e-01  3.02898347e-01
 -1.31173626e-01 -2.61930861e-02  2.36462414e-01  4.60582703e-01
 -1.33421481e-01 -1.36555254e-01 -3.52440655e-01 -1.70818076e-01
  7.01259524e-02 -1.75762117e-01 -2.76872396e-01 -2.00264305e-01
  1.29752487e-01  5.20775676e-01 -1.19290441e-01  2.63541251e-01
 -2.52478063e-01  1.16994463e-01  4.90713000e-01 -2.50518143e-01
 -1.88480243e-01  1.03886366e-01  1.15137823e-01 -2.66378701e-01
 -1.02168500e-01  4.78416681e-02  7.67833591e-02 -1.08253196e-01]"
ColumnReduceKernel: min() type casting error and improvement awaiting review comp:ops type:performance TF 2.13,"### Issue type

Performance

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10 22H2

### Mobile device

_No response_

### Python version

Anaconda 2023.07-1

### Bazel version

6.2.1

### GCC/compiler version

Visual Studio 2022 (build tools 14.36) + msys2-x86_64-20230718

### CUDA/cuDNN version

CUDA 11.8 + CUDNN 8.6.0 + TensorRT 8.5.3

### GPU model and memory

GTX 750 Ti 2GB

### Current behavior?

There are two type casting errors in reduction_gpu_kernels.cu.h under MSVC. One of them is fixed in https://github.com/tensorflow/tensorflow/pull/61339. Another is related to a TODO.

in [ColumnReduceKernel()](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_gpu_kernels.cu.h#L341), the TODO said the followings:

> 1D array necessary due to bug in CUDA 9 compiler.
> TODO(nluehr) revert to 2D array when compiler is ready.
> This is to mimic the following, but without constructors:
> __shared__ storage_type<value_type> partial_sums[TF_RED_WARPSIZE *
> (TF_RED_WARPSIZE + 1)];

Since latest version required CUDA 11, it's time to address the TODO and apply bug fix together.

### Standalone code to reproduce the issue

```shell
1. download https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.13.0.zip and extract
2. comment out Windows CUDA build rejection code in configure.py
3. run `python configure.py` to configure Windows CUDA build
4. run `bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`
```


### Relevant log output

```shell
external/com_google_absl\absl/status/status.h(796): warning #2810-D: ignoring return value type with ""nodiscard"" attribute

.\tensorflow/tsl/platform/file_system.h(571): warning #611-D: overloaded virtual function ""tsl::FileSystem::FilesExist"" is only partially overridden in class ""tsl::WrappedFileSyste
m""

.\tensorflow/tsl/platform/file_system.h(571): warning #611-D: overloaded virtual function ""tsl::FileSystem::CreateDir"" is only partially overridden in class ""tsl::WrappedFileSystem
""

.\tensorflow/tsl/platform/env.h(500): warning #611-D: overloaded virtual function ""tsl::Env::RegisterFileSystem"" is only partially overridden in class ""tsl::EnvWrapper""

.\tensorflow/tsl/platform/float8.h(936): warning #177-D: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To tsl::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=tsl::float8_i
nternal::float8_e4m3b11, To=tsl::float8_internal::float8_e4m3fn, kSaturate=false, kTruncate=false]""
(1018): here
            instantiation of ""Derived tsl::float8_internal::float8_base<Derived>::ConvertFrom(const From &) [with Derived=tsl::float8_internal::float8_e4m3fn, kSaturate=false, kTru
ncate=false, From=tsl::float8_internal::float8_e4m3b11]""
(277): here

.\tensorflow/tsl/platform/float8.h(936): warning #177-D: variable ""aligned_highest"" was declared but never referenced
          detected during:
            instantiation of ""To tsl::float8_internal::ConvertImpl<From, To, kSaturate, kTruncate, std::enable_if_t<<expression>, void>>::run(const From &) [with From=tsl::float8_i
nternal::float8_e4m3b11, To=float, kSaturate=false, kTruncate=false]""
(1024): here
            instantiation of ""To tsl::float8_internal::float8_base<Derived>::ConvertTo<To,kSaturate,kTruncate>(const Derived &) [with Derived=tsl::float8_internal::float8_e4m3b11,
To=float, kSaturate=false, kTruncate=false]""
(75): here
            instantiation of ""tsl::float8_internal::float8_base<Derived>::operator float() const [with Derived=tsl::float8_internal::float8_e4m3b11]""
(116): here
            instantiation of ""Derived tsl::float8_internal::float8_base<Derived>::operator-(const Derived &) const [with Derived=tsl::float8_internal::float8_e4m3b11]""
(302): here

.\tensorflow/core/kernels/reduction_gpu_kernels.cu.h(392): error: no instance of overloaded function ""tensorflow::min"" matches the argument list
            argument types are: (int, unsigned int)
          detected during:
            instantiation of ""void tensorflow::functor::ColumnReduceKernel(T, OUT_T, int, int, Op, std::iterator_traits<T>::value_type) [with T=const float *, OUT_T=float *, Op=cub
::Max]""
(828): here
            instantiation of ""void tensorflow::functor::LaunchColumnReduction_LTE4096Cols(tensorflow::OpKernelContext *, OUT_T, IN_T, int, int, Op, T, const gpuStream_t &) [with T=
float, Op=cub::Max, OUT_T=float *, IN_T=const float *]""
(862): here
            instantiation of ""void tensorflow::functor::LaunchColumnReduction(tensorflow::OpKernelContext *, OUT_T, IN_T, int, int, Op, T, const gpuStream_t &) [with T=float, Op=cu
b::Max, OUT_T=float *, IN_T=const float *]""
(1088): here
            instantiation of ""void tensorflow::functor::ReduceImpl<T,Op,OUT_T,IN_T,ReductionAxes>(tensorflow::OpKernelContext *, OUT_T, IN_T, int, int, int, int, int, const Reducti
onAxes &, Op) [with T=float, Op=cub::Max, OUT_T=float *, IN_T=const float *, ReductionAxes=const Eigen::array<Eigen::DenseIndex, 1ULL> &]""
E:\_bazel_tensorflow\4zvk5ci6\execroot\org_tensorflow\tensorflow\core\kernels\multinomial_op_gpu.cu.cc(111): here
            instantiation of ""void tensorflow::functor::MultinomialFunctor<tensorflow::functor::GPUDevice, T, OutputType>::operator()(tensorflow::OpKernelContext *, const tensorflo
w::functor::GPUDevice &, tensorflow::TTypes<T, 1, Eigen::DenseIndex>::ConstMatrix, tensorflow::TTypes<float, 1, Eigen::DenseIndex>::Flat, tensorflow::TTypes<float, 1, Eigen::DenseI
ndex>::Flat, tensorflow::TTypes<float, 1, Eigen::DenseIndex>::Flat, int, int, int, const tsl::random::PhiloxRandom &, tensorflow::TTypes<OutputType, 1, Eigen::DenseIndex>::Matrix)
[with T=Eigen::half, OutputType=tsl::int32]""
E:\_bazel_tensorflow\4zvk5ci6\execroot\org_tensorflow\tensorflow\core\kernels\multinomial_op_gpu.cu.cc(126): here

1 error detected in the compilation of ""tensorflow/core/kernels/multinomial_op_gpu.cu.cc"".
nvcc warning : The 'compute_35', 'compute_37', 'sm_35', and 'sm_37' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppres
s warning).
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1996.828s, Critical Path: 480.15s
INFO: 441 processes: 7 internal, 434 local.
FAILED: Build did NOT complete successfully
```
",False,"[-4.01831359e-01 -3.82641792e-01 -1.76459610e-01 -1.44850295e-02
 -1.27474189e-01 -4.72686291e-01 -2.42550269e-01  4.58603054e-02
 -5.23072898e-01 -2.60955691e-01 -3.09128255e-01  5.09651303e-02
 -3.14681947e-01 -1.09218106e-01 -7.25628287e-02  3.17537099e-01
  1.03055239e-01  6.52578548e-02  2.19349861e-01  1.79320306e-01
 -1.80393055e-01 -1.59743398e-01 -4.06788766e-01  1.20222084e-01
  3.22589874e-01  1.14116110e-01 -4.03867275e-01 -7.08238930e-02
  2.49860659e-02  5.83236814e-02 -4.46091667e-02  1.77336484e-03
  1.22565381e-01 -4.06026095e-02 -9.28124115e-02  1.81521744e-01
 -2.63399065e-01 -3.57619435e-01 -3.68999064e-01 -1.58323143e-02
 -1.06854215e-01  1.50845915e-01 -5.20626828e-02 -2.52388775e-01
 -4.71012257e-02 -1.98707432e-02 -1.53651506e-01 -3.31310071e-02
 -1.61154583e-01 -5.21064252e-02 -2.56210446e-01  1.03409134e-01
 -3.63202244e-01 -2.72230119e-01 -9.61132497e-02 -3.30683924e-02
  1.75506100e-02 -1.23410299e-01 -9.64364223e-03 -3.46907899e-02
  1.56763628e-01 -7.91485235e-02  8.19363296e-02 -1.31594494e-01
  7.94374347e-02  1.05253533e-01  4.51971471e-01 -4.63711545e-02
  4.30359244e-01 -3.12478185e-01  5.92435226e-02 -8.73639882e-02
 -5.92561007e-01  1.91311896e-01  1.49675995e-01  1.99499577e-01
  1.36687398e-01 -4.95499521e-02  4.12725538e-01 -3.63379836e-01
  7.84560665e-02 -2.80827135e-01 -5.83094172e-02  4.04389799e-02
  4.04483438e-01 -1.49668887e-01  4.84659255e-01  2.10822359e-01
  3.23420793e-01 -4.25376417e-03  3.31010163e-01  4.75248992e-01
 -2.59396672e-01  1.61456868e-01  3.40394497e-01  1.15544379e-01
  2.06621721e-01  5.80391102e-02  4.42237966e-02 -4.34431434e-03
  1.61276609e-01 -1.71040237e-01  6.10475689e-02  1.48151457e-01
 -1.92070380e-01 -2.77403116e-01  2.30200201e-01 -4.74549830e-02
  8.79255682e-02  6.29349798e-02  3.18808615e-01  1.49198979e-01
  7.91945606e-02 -2.65099555e-01 -1.84051782e-01  1.78584397e-01
 -2.84200072e-01 -9.02727991e-02  1.67904854e-01  6.98616803e-01
  1.14558905e-01 -1.58314317e-01 -3.71047035e-02  4.32876885e-01
  5.12931824e-01  1.23226002e-01 -2.66812555e-03  1.04798637e-01
  6.79616332e-02 -1.63890332e-01  1.18940428e-01  3.28706875e-02
  8.87772292e-02  1.91239253e-01  1.09241977e-01  2.92805970e-01
 -1.08201057e-01 -3.75814259e-01 -2.21197784e-01 -4.50459719e-01
 -2.12772131e-01  3.96872014e-01 -2.52422262e-02 -5.70865512e-01
  3.91685963e-01  1.46823138e-01 -2.18213707e-01  2.37345874e-01
 -2.14998081e-01  7.11471066e-02  9.31306928e-02  2.04258010e-01
 -1.82027102e-01  3.18965018e-01  1.58257887e-01  9.65578705e-02
  1.09781951e-01 -3.10726054e-02  1.81218892e-01 -7.01124072e-01
 -1.79625094e-01  2.95136631e-01 -2.12389454e-02 -2.02847227e-01
  2.10522175e-01  1.77021325e-03 -4.17755723e-01 -2.12256163e-01
 -1.55971766e-01  7.09898412e-01 -2.61648715e-01 -2.33606875e-01
  7.32667968e-02  1.28479693e-02  2.51111597e-01  8.27270895e-02
 -2.55311094e-02 -3.74292403e-01  1.49466127e-01  4.28685516e-01
  3.39610159e-01  3.16920578e-01  4.14480507e-01  2.99429148e-01
  1.33469880e-01  6.24428578e-02  2.96380281e-01  4.12537634e-01
 -2.52468050e-01  1.41293749e-01 -4.38263953e-01  1.00710317e-02
  3.00651938e-01 -9.70667377e-02 -1.14708737e-01  2.16721632e-02
  2.00566053e-01 -2.16410846e-01  7.19508007e-02 -2.37679034e-02
 -1.56060115e-01 -5.12572415e-02 -2.11169362e-01  4.67902236e-02
  1.60796255e-01 -1.85562640e-01 -1.28353521e-01 -3.78986865e-01
 -2.00227693e-01  3.61898810e-01  4.59395647e-02 -7.41834819e-01
 -1.25910163e-01 -1.78877592e-01 -1.98494866e-01  1.36900499e-01
  2.47381389e-01  3.61669004e-01 -2.31515884e-01  1.12907127e-01
  2.91632488e-04 -1.90933093e-01 -6.87246323e-02 -3.57161045e-01
 -2.35655874e-01 -5.63719496e-02 -5.04157066e-01  1.13595560e-01
 -2.55231500e-01  1.47271216e-01 -3.39227989e-02 -8.85229409e-02
  3.52772087e-01  1.48766875e-01  4.09487009e-01 -1.50228918e-01
  5.95811903e-02 -2.20812201e-01 -3.26417625e-01 -1.72172442e-01
 -3.78516972e-01 -1.79759830e-01 -7.55325481e-02 -1.48782879e-01
  5.64414263e-01  5.86030841e-01  1.40463382e-01  6.13847896e-02
 -2.90929228e-01  2.45507360e-01 -3.01622033e-01  1.93109840e-01
  2.81202555e-01 -1.01742923e-01  4.32504386e-01  2.22697258e-01
  1.86804220e-01  8.44916850e-02  3.02711338e-01 -2.72307903e-01
  4.90348577e-01  2.34202147e-01  1.64315224e-01  5.08778334e-01
  4.00351375e-01  3.20672274e-01 -1.89873993e-01  4.35157448e-01
  4.63657593e-03 -1.35064051e-01  1.23716354e-01 -1.23322174e-01
  6.25373006e-01 -2.67186344e-01  1.16017081e-01 -2.20427126e-01
  2.93209374e-01 -1.12153806e-01 -3.93392257e-02 -1.21679708e-01
  2.18297631e-01  2.15371192e-01 -2.47953758e-01  2.40259603e-01
 -9.55546945e-02 -2.39853397e-01 -7.84343183e-02 -6.31908953e-01
 -8.90191048e-02  1.26579851e-01 -1.53425872e-01 -1.41602620e-01
 -3.66081484e-02 -1.63333286e-02 -1.82012975e-01  1.54256463e-01
  1.65670127e-01 -7.12399557e-02  1.01862326e-02 -2.96277516e-02
 -2.97279842e-02  6.34443387e-02  4.24611837e-01 -3.30411792e-01
 -2.41178870e-01 -8.70917588e-02  5.76845288e-01  1.92647755e-01
  3.00079435e-01 -2.25955784e-01  9.27355811e-02 -1.10899851e-01
 -3.06794345e-01  2.50528753e-01 -1.21806458e-01  1.20370343e-01
 -3.14236492e-01  5.79706490e-01  1.93315908e-01 -1.70375973e-01
  9.36933421e-03 -2.97570914e-01 -3.08958352e-01  1.51970997e-01
  2.23546885e-02 -1.91225380e-01 -5.48697263e-02 -3.87385607e-01
  9.07203257e-02  1.29630491e-02  1.66227929e-02 -1.37322262e-01
 -8.28897581e-02 -4.06015813e-02  8.64163786e-02 -2.81478584e-01
 -1.20822966e-01  2.49005198e-01 -1.70654833e-01 -3.37152898e-01
 -7.76641816e-02 -6.83303028e-02  9.24220607e-02 -3.14647019e-01
 -1.36013836e-01 -3.27272803e-01  3.08760703e-01  2.52223730e-01
 -3.75368148e-02  1.17767140e-01 -2.01106369e-02  2.37513199e-01
 -2.31429741e-01 -1.32020384e-01 -9.58537087e-02  1.77103877e-01
  3.76611389e-02 -9.17802826e-02  2.37617150e-01  1.30955994e-01
 -1.30575418e-01  2.96536446e-01 -4.82539803e-01  3.41067985e-02
  1.98405802e-01 -2.07232744e-01 -1.61019370e-01 -2.68340968e-02
  1.90292895e-01  2.77645975e-01 -5.05123958e-02  2.61037529e-01
 -2.18117595e-01  2.31154352e-01  4.50070560e-01 -3.10314476e-01
 -2.09109597e-02 -1.02786422e-01  2.43306041e-01 -8.40780511e-03
  1.11390024e-01 -1.26069427e-01 -1.36028156e-02  1.34906784e-01]"
rocm_helpers missing dependency declarations type:build/install subtype: ubuntu/linux,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

master/nightly

### Custom code

Yes

### OS platform and distribution

`Arch Linux (Linux 6.4.4-arch1-1 #1 SMP PREEMPT_DYNAMIC x86_64 GNU/Linux)`

### Mobile device

N/A

### Python version

3.10

### Bazel version

6.1.0

### GCC/compiler version

gcc (GCC) 13.1.1 20230714

### CUDA/cuDNN version

None

### GPU model and memory

AMD Radeon RX 7900 XT

### Current behavior?

After adding `#include <stdint.h>` to line 16 of `tensorflow/tsl/lib/io/cache.cc` to fix a different error, and using the installation method described in the reproduce field.

Bazel gives the error described in the attached log.

This persists through different Bazel versions, and full cleans.

I am using the following archlinux packages for ROCm:
```
local/opencl-amd 1:5.6.0-2
    ROCr OpenCL stack
local/opencl-amd-dev 1:5.6.0-1
    OpenCL SDK / HIP SDK / ROCM Compiler.
```

### Standalone code to reproduce the issue

```shell
./configure
You have bazel 6.1.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:

Found possible Python library paths:
  /usr/lib/python3.11/site-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3.11/site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: y
ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Do you want to use Clang to build TensorFlow? [Y/n]:
Clang will be used to compile TensorFlow.

Please specify the path to clang executable. [Default is /usr/bin/clang]:

You have Clang 17.0.0 installed.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]:

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.



bazel build --config=opt --verbose_failures //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
ERROR: /home/user/Repos/tensorflow/tensorflow/compiler/xla/stream_executor/rocm/BUILD:527:11: Compiling tensorflow/compiler/xla/stream_executor/rocm/rocm_helpers.cu.cc [for tool] failed: undeclared inclusion(s) in rule '//tensorflow/compiler/xla/stream_executor/rocm:rocm_helpers':
this rule is missing dependency declarations for the following files included by 'tensorflow/compiler/xla/stream_executor/rocm/rocm_helpers.cu.cc':
  '/opt/rocm-5.6.0/include/hip/hip_version.h'
  '/opt/rocm-5.6.0/include/hip/hip_runtime.h'
  '/opt/rocm-5.6.0/include/hip/hip_common.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_runtime.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_common.h'
  '/opt/rocm-5.6.0/include/hip/hip_runtime_api.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/host_defines.h'
  '/opt/rocm-5.6.0/include/hip/driver_types.h'
  '/opt/rocm-5.6.0/include/hip/texture_types.h'
  '/opt/rocm-5.6.0/include/hip/channel_descriptor.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_channel_descriptor.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_vector_types.h'
  '/opt/rocm-5.6.0/include/hip/surface_types.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_runtime_pt_api.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/hip_ldg.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_atomic.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_device_functions.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/math_fwd.h'
  '/opt/rocm-5.6.0/include/hip/hip_vector_types.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/device_library_decls.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_warp_functions.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_unsafe_atomics.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_surface_functions.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/ockl_image.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/texture_fetch_functions.h'
  '/opt/rocm-5.6.0/include/hip/hip_texture_types.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/texture_indirect_functions.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_math_functions.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/hip_fp16_math_fwd.h'
  '/opt/rocm-5.6.0/include/hip/library_types.h'
  '/opt/rocm-5.6.0/include/hip/hip_bfloat16.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_bfloat16.h'
  '/opt/rocm-5.6.0/include/hip/hip_fp16.h'
  '/opt/rocm-5.6.0/include/hip/amd_detail/amd_hip_fp16.h'
clang-16: warning: argument unused during compilation: '-fcuda-flush-denormals-to-zero' [-Wunused-command-line-argument]
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3.270s, Critical Path: 3.10s
INFO: 77 processes: 53 internal, 24 local.
FAILED: Build did NOT complete successfully
```
",False,"[-4.02701378e-01 -4.93154675e-01 -4.19612303e-02 -8.25904310e-02
  1.57338440e-01 -5.31410038e-01 -9.29473266e-02  1.72884874e-02
 -4.60987389e-01 -2.34806895e-01  1.47632241e-01 -1.18171670e-01
 -3.69123489e-01  2.14711964e-01 -4.57054675e-02  2.69046843e-01
 -7.30682760e-02 -4.53972995e-01  4.31561559e-01  4.16525185e-01
 -1.32266730e-02 -2.11541414e-01 -3.42613935e-01  9.75074321e-02
 -7.10568577e-03  4.82280999e-01 -1.28900558e-01  6.27871305e-02
  2.00758070e-01  1.66493859e-02  6.01588786e-01  1.37295797e-01
  3.06234896e-01  1.07190236e-01  3.02249163e-01  4.95049685e-01
 -6.02191314e-02 -2.72296906e-01 -9.72772837e-02 -3.73143852e-02
 -1.82783425e-01 -9.25145205e-03  1.26953498e-01  2.25544721e-03
 -3.87207955e-01 -2.65812367e-01  2.46785030e-01 -3.54862601e-01
  1.30533576e-01 -6.13491893e-01 -1.90878101e-02 -2.40473673e-02
 -5.80441117e-01 -3.04867625e-01 -1.47514671e-01 -9.95053202e-02
  2.37789497e-01  8.54859203e-02  5.87401986e-02  2.13252872e-01
  3.08094323e-01  1.21452324e-01  4.20391709e-02  8.17694068e-02
  1.88376129e-01  2.48526260e-01  2.53544033e-01 -1.78921983e-01
  5.93002319e-01 -1.38694525e-01 -1.17404900e-01 -3.56342364e-03
 -1.29163593e-01  1.46033585e-01  2.29162365e-01  3.35510045e-01
  2.21959531e-01  1.65789187e-01  2.28469938e-01 -2.89533466e-01
 -3.24364841e-01 -7.04215169e-02 -5.35597876e-02 -1.18522746e-02
 -9.89753287e-03 -2.20719844e-01  1.15654752e-01  6.24682680e-02
  4.71722513e-01 -2.00335503e-01  2.76915789e-01  3.07283789e-01
  2.14362741e-01  4.36313450e-02  1.54098868e-01 -4.58492115e-02
 -5.63589483e-02  2.15742454e-01 -1.05967119e-01 -1.35456175e-01
 -7.14868009e-02  1.72783613e-01 -1.44853499e-02 -1.07066140e-01
 -2.55064487e-01 -9.01764631e-02  2.15035141e-01 -1.58569187e-01
 -8.92016441e-02  1.01659447e-01  3.47375572e-02  6.36548027e-02
 -2.66814884e-02 -1.08890489e-01 -1.13425009e-01  3.59233953e-02
 -3.54780197e-01 -2.44466335e-01 -5.80646507e-02  5.11917114e-01
 -1.56754389e-01 -2.46693626e-01  8.15255418e-02  3.79539788e-01
  4.46750969e-01  1.01135775e-01 -3.47297311e-01 -7.14637861e-02
  1.57760471e-01  1.11549437e-01 -8.06576684e-02  1.51299655e-01
  4.48825181e-01  3.25186253e-01  2.96313822e-01  1.45497605e-01
 -2.82486558e-01 -1.66914403e-01  1.92681193e-01 -1.43108815e-01
 -1.89585090e-01  2.70408869e-01 -1.07765183e-01 -5.87148666e-01
  6.24243766e-02  8.91384855e-02 -3.36223990e-01  1.35624141e-01
 -1.52306810e-01  7.75522888e-02 -6.42309487e-02  3.08187082e-02
  2.02695802e-01  2.58762240e-01  1.85838521e-01  2.77478039e-01
  5.57835221e-01 -1.03405207e-01 -1.79314435e-01 -4.00607884e-01
 -1.57158256e-01  3.77271354e-01 -9.19246376e-02 -1.33332312e-01
  2.21332997e-01  1.50833698e-02 -6.70035839e-01 -4.60680962e-01
  1.72373965e-01  4.87054199e-01  1.41709954e-01  2.34496556e-02
  2.91238546e-01 -2.41494793e-02  5.39679408e-01 -1.17857195e-01
  6.37841940e-01 -5.39276004e-01 -1.59306660e-01  4.48406696e-01
  1.94472671e-01 -2.95783728e-02  5.79131469e-02  3.63517329e-02
 -7.11826310e-02  1.37522006e-02  2.25241318e-01  1.07781209e-01
 -2.44627252e-01 -2.51663215e-02 -4.30728108e-01  4.29955423e-01
  2.16528878e-01 -4.25658256e-01 -4.29043233e-01  2.72159632e-02
  3.62946630e-01  2.58525312e-01 -7.43837729e-02 -7.72761693e-03
 -4.96343151e-02  8.44530165e-02 -4.37174439e-01  9.58755016e-02
  5.53461425e-02 -1.70657113e-01 -3.39105316e-02 -2.18977034e-01
 -5.97883940e-01  4.55669835e-02  1.24871343e-01 -2.36228704e-01
  5.54919709e-03 -2.13616602e-02 -1.40774414e-01  6.07929565e-02
 -1.46832079e-01  5.41462339e-02 -2.29414374e-01  4.10399735e-02
  1.15958929e-01 -4.20067608e-01 -3.03753614e-02 -2.70667702e-01
 -3.78330722e-02 -1.29371285e-01 -3.20344269e-01 -2.93545306e-01
  1.01037264e-01  1.15345970e-01 -8.57668743e-02 -8.51696879e-02
  2.83517003e-01  1.04734190e-02  3.11452150e-01  4.23112512e-02
  9.64485854e-02 -2.58828729e-01 -3.42306614e-01  9.33523029e-02
 -3.67027968e-01 -3.62613499e-01  1.07340455e-01  2.71859523e-02
  2.84979939e-01  4.97908771e-01  7.43323565e-02  1.67879641e-01
 -2.91892529e-01  1.47758797e-01 -2.37694681e-01  3.63390565e-01
  2.31424958e-01  2.91617274e-01  1.31239951e-01  3.71230602e-01
 -9.25271586e-02  5.34205884e-02  7.43924379e-02 -2.83088714e-01
 -2.89247483e-02  1.31162629e-01 -1.29191160e-01  2.59780824e-01
  2.91998744e-01  3.85688722e-01 -4.91178483e-01  3.77013475e-01
 -4.26590815e-02 -1.59691259e-01  2.13116780e-01 -1.15096979e-01
  6.05236411e-01 -3.77897024e-01 -6.65460676e-02 -7.24613667e-02
  3.25261593e-01 -1.84557840e-01  3.95082869e-02 -9.20161698e-03
 -1.24255955e-01  2.88066626e-01 -3.69904846e-01  1.47132814e-01
  3.02367240e-01 -2.06188023e-01  5.49152084e-02 -7.09749341e-01
 -3.66187096e-01 -1.20346539e-01 -1.30801260e-01  1.05914392e-01
 -2.48725340e-01  2.24424481e-01 -2.86861300e-01 -2.18269721e-01
  5.00289071e-03  1.38395399e-01  2.09084749e-01  2.39679277e-01
 -4.19736803e-02 -2.42165282e-01  2.40547463e-01 -3.19586873e-01
 -1.56584859e-01 -1.49830848e-01  1.12188175e-01  3.45711708e-01
  5.10224044e-01 -4.66966152e-01  3.52337897e-01  1.76619455e-01
 -1.23935066e-01  5.25629163e-01  6.87887520e-03  1.66335762e-01
 -3.80377978e-01  5.39184690e-01  8.01555887e-02 -8.68202448e-02
  3.03824276e-01 -2.35593408e-01 -2.37106562e-01 -3.45367342e-02
  6.39552027e-02 -1.71396255e-01 -2.27761924e-01 -5.35560191e-01
 -1.25896811e-01  2.14868933e-01 -8.51522014e-02 -8.49793777e-02
 -7.68095702e-02  4.06189889e-01 -4.23249900e-01 -9.57293361e-02
 -2.16887996e-01  5.32826245e-01 -2.46113211e-01 -2.01893866e-01
 -3.21529001e-01 -1.16410680e-01 -1.55234665e-01 -3.55293989e-01
  7.00199045e-04 -8.02582130e-02  4.63014543e-01  6.72615990e-02
 -3.60893369e-01  3.67786795e-01  9.14070010e-02  2.84414470e-01
 -2.89775193e-01 -3.37947309e-02  1.29427627e-01  2.11412489e-01
  9.64290425e-02 -4.74770032e-02  1.65738434e-01  1.38728604e-01
 -8.84207189e-02  2.90773183e-01 -4.18628395e-01  1.29559878e-02
 -1.12824999e-02 -8.43766928e-02 -2.93869495e-01 -1.03558293e-02
  5.80250565e-03  4.11185503e-01  5.42890094e-03  1.09585747e-01
 -3.32076997e-01  1.27636999e-01  5.92955828e-01 -3.58669937e-01
 -2.54922390e-01  2.32816741e-01  1.54632241e-01 -6.46548644e-02
  1.86961040e-01 -1.19370051e-01  2.05802485e-01  7.82044418e-03]"
Need Help with TensorFlow Lite Model Running on GPU - Output Interpretation Issue (Android Studio Kotlin) stat:awaiting response type:support stale comp:lite TFLiteGpuDelegate Android,"Hello. I have created an Android application in Android Studio that uses a tflite model. Its implementation works without any issues and looks as follows:

val model = Ssd.newInstance(context)

// Creates inputs for reference.
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 320, 320, 3), DataType.UINT8)
inputFeature0.loadBuffer(byteBuffer)

// Runs model inference and gets result.
val outputs = model.process(inputFeature0)
val outputFeature0 = outputs.outputFeature0AsTensorBuffer
val outputFeature1 = outputs.outputFeature1AsTensorBuffer
val outputFeature2 = outputs.outputFeature2AsTensorBuffer
val outputFeature3 = outputs.outputFeature3AsTensorBuffer

// Releases model resources if no longer used.
model.close()

However, the application is running slowly, and I would like to perform the model computations on the GPU.

I am facing an issue with the input and output parts.

I couldn't find any information about it anywhere. The current code looks like this:

val options = Interpreter.Options().apply {
    if(compatList.isDelegateSupportedOnThisDevice) {
        val delegateOptions = compatList.bestOptionsForThisDevice
        this.addDelegate(GpuDelegate(delegateOptions))
    } else {
        this.setNumThreads(4)
    }
}
interpreter = Interpreter(loadModelFile(assets,""Ssd.tflite""), options)
val inputFeature0 = TensorBuffer.createFixedSize(intArrayOf(1, 320, 320, 3), DataType.FLOAT32)
inputFeature0.loadBuffer(byteBuffer)

Then, I should create the input.buffer for the main line:

interpreter.run(inputFeature0.buffer, outputs.buffer)

I tried doing some adjustments, but the outputs.buffer I got as a result was something I couldn't interpret. Has anyone encountered a similar problem? If so, please, I would appreciate your help.

",False,"[-3.04192036e-01 -3.28645051e-01 -4.31865543e-01 -3.39042127e-01
 -2.08203159e-02 -1.43959746e-01 -5.69311790e-02  1.65166855e-01
 -2.69017637e-01 -1.29221171e-01 -9.44726616e-02 -1.33002371e-01
  9.19209272e-02  3.20385039e-01 -2.13260263e-01  1.59418099e-02
 -1.38058037e-01 -7.47474730e-02 -1.24032676e-01  1.56121910e-01
  1.65472180e-01  7.23994821e-02 -2.53523346e-02  1.61196858e-01
  4.13956761e-01  4.01375920e-01  1.02809101e-01 -2.22060412e-01
  3.16674978e-01  1.26476198e-01 -1.05133221e-01  2.26814032e-01
 -7.17418492e-02  2.28297129e-01 -2.46669978e-01 -1.50830429e-02
 -5.43556988e-01 -7.07085729e-02 -3.97086889e-01  2.41705447e-01
  9.94113982e-02  2.96228796e-01 -2.60270983e-02  7.85233155e-02
 -2.27812305e-01  2.82261670e-02  3.27043310e-02  1.70498699e-01
 -1.64749727e-01  1.47898734e-01 -6.00968227e-02 -1.70055032e-03
 -2.92616129e-01 -4.08606172e-01 -2.98029594e-02  8.15718621e-02
  5.89057133e-02  3.17116305e-02  6.68795556e-02 -2.27772996e-01
  1.48517549e-01 -5.51384464e-02  1.20957658e-01 -4.47645970e-02
  1.46784782e-01  2.95763016e-01  2.97789881e-03 -1.67883530e-01
  5.58306336e-01 -4.35183764e-01 -1.46091253e-01  4.68282178e-02
 -3.88792604e-02 -1.40248939e-01  3.78874913e-02  7.46545494e-02
 -4.61459830e-02  3.49331617e-01  2.89135128e-02 -7.77689293e-02
  2.69590229e-01 -1.37389153e-01 -1.76783398e-01  3.45065951e-01
  4.31716383e-01 -6.68089762e-02  2.60648608e-01  9.20423344e-02
  3.76141131e-01  3.67201030e-01  1.04425907e-01  7.29959130e-01
 -2.64366060e-01  1.20795488e-01  1.69700235e-01  3.03368300e-01
  3.05169895e-02  6.28342330e-02 -9.65693891e-02 -2.02072933e-02
  6.30124751e-03 -2.25907937e-01 -6.35006949e-02  2.47048870e-01
  4.04036283e-01 -3.47173035e-01  4.90880385e-02 -3.56684402e-02
  7.61742443e-02  1.40887305e-01  3.33698690e-01 -9.80791748e-02
  5.17303199e-02  4.68791202e-02 -1.45442318e-04  1.17303804e-01
  1.26867697e-01  1.20334201e-01 -7.97646716e-02  2.89318323e-01
 -2.91367294e-03 -2.86020756e-01 -6.79510534e-02 -1.26449302e-01
  3.01819623e-01  5.50467558e-02 -3.50237727e-01 -1.76842883e-01
  2.48494092e-02 -1.46042053e-02  2.34127164e-01  2.61769742e-02
 -4.43779349e-01 -3.86965692e-01  5.94477840e-02  2.45205462e-01
 -5.93549572e-02 -3.24905902e-01 -4.32296604e-01  1.75740510e-01
 -2.12033421e-01  8.88670981e-02 -2.02793181e-01 -2.22188815e-01
  1.88199095e-02  3.78115833e-01 -1.49108693e-01  2.20378518e-01
 -8.06565210e-02 -1.73769444e-02 -1.39719546e-01 -9.37976688e-02
 -1.59381568e-01  2.22956538e-01  4.40673620e-01 -1.05624542e-01
 -6.44379202e-03 -3.28531116e-02  1.75981641e-01 -4.24782395e-01
 -2.46095449e-01  1.23109914e-01 -3.25910486e-02 -9.27727669e-03
  3.40076983e-01  3.24506402e-01 -2.49420851e-01  5.12666181e-02
 -3.85941640e-02  1.03382096e-01  1.73187360e-01  2.05275323e-02
  1.57538980e-01 -3.38822789e-03  2.25589097e-01 -1.87739469e-02
  8.37650150e-05 -3.77932370e-01  1.55895472e-01 -2.57016659e-01
  4.35221255e-01  1.64130419e-01 -1.19255364e-01 -3.48338857e-04
 -3.85322332e-01  3.26422006e-02  1.11654829e-02  1.60633177e-01
 -3.88560534e-01  2.27194987e-02 -3.06511253e-01  5.08714765e-02
 -6.31672442e-02  1.04394749e-01 -3.21819454e-01 -2.07711726e-01
  6.70084432e-02 -1.14963233e-01  1.02125242e-01  1.95014119e-01
  3.12884562e-02  2.50319354e-02 -1.59845486e-01 -1.74550116e-01
  1.66716754e-01 -1.65844917e-01 -1.68787271e-01 -1.97882101e-01
 -1.32337987e-01  1.66814059e-01  2.76259005e-01 -3.20760995e-01
 -2.34154463e-01 -2.80404627e-01  1.02044024e-01 -2.90604770e-01
 -2.03061119e-01 -9.13953781e-02 -5.09483337e-01  3.50451112e-01
  1.07480563e-01 -1.26474977e-01  1.43541411e-01 -3.25608224e-01
 -3.85229647e-01 -8.12591612e-03 -1.67293996e-01  4.76667881e-02
 -6.45272806e-02  1.52880624e-01 -1.30871460e-01  2.30419114e-02
  3.93218189e-01 -4.82934117e-02  2.43596941e-01 -3.30329537e-01
  8.04889388e-03 -3.99775028e-01 -4.93431807e-01 -3.51699859e-01
 -2.30104513e-02 -2.73058653e-01 -4.14002780e-03 -3.32113057e-01
 -4.54123132e-02 -2.31426954e-02  1.65682971e-01  1.07816786e-01
 -2.24101245e-01  3.89555216e-01  1.65351331e-02 -1.67565495e-01
  2.96469629e-01  1.28807575e-01  1.97307512e-01  2.34261546e-02
 -1.06088400e-01 -7.49078244e-02 -6.61737695e-02  2.62030870e-01
  2.18844905e-01  2.25262314e-01  6.81069195e-02  7.04607487e-01
  4.07577574e-01  6.87528178e-02 -5.94419278e-02  1.41440719e-01
 -1.78109452e-01 -1.81269661e-01  1.45554572e-01 -2.02527232e-02
  2.60881692e-01 -3.54810953e-01  2.21488148e-01 -1.16937414e-01
  4.77477998e-01 -7.98289664e-03 -8.76431614e-02  3.05082109e-02
  3.23079973e-01  1.08474031e-01 -3.41441005e-01 -1.73044145e-01
  2.53765166e-01 -3.09331179e-01 -2.35964805e-01  1.42493565e-02
 -3.34802538e-01 -6.24062791e-02 -1.13887697e-01 -1.73175529e-01
  1.51355401e-01  2.02874154e-01 -2.40387127e-01 -8.11640173e-02
  2.20097005e-01  2.67743487e-02  3.41739774e-01 -1.33561999e-01
 -8.15675110e-02 -2.52442099e-02  1.55701637e-01 -3.00452799e-01
 -1.03673652e-01 -8.70504901e-02  2.07944050e-01 -1.41738102e-01
  4.33229744e-01 -1.59430593e-01  1.42313302e-01  1.74694017e-01
  5.87784499e-03  5.38208127e-01 -2.88032651e-01  1.64684266e-01
 -5.04156828e-01  3.78436327e-01 -5.87958135e-02  2.08443999e-02
 -6.98018298e-02 -2.10938185e-01 -2.19210744e-01 -2.38091052e-02
  8.54855403e-02  2.38889873e-01  5.36437258e-02  6.03406429e-02
  8.00239444e-02 -5.87101839e-02 -1.92641318e-01 -1.48530066e-01
 -5.57880476e-02  2.00558618e-01 -3.68308090e-02 -1.42971188e-01
 -1.22417800e-01  2.86507249e-01 -1.38186365e-01 -2.36026067e-02
 -1.01466641e-01 -2.56167322e-01 -4.09225345e-01 -4.70084190e-01
  1.21779412e-01 -9.68203917e-02  1.59347653e-01  3.97275031e-01
  2.19628930e-01  7.27342442e-02 -2.79739127e-02  1.88859105e-01
  2.68076397e-02 -1.21956989e-01 -1.75409719e-01  4.34616625e-01
  1.80550173e-01 -1.39172792e-01  8.14477801e-02  6.19151473e-01
  7.55980164e-02  2.42667168e-01 -3.65645826e-01 -3.79254818e-01
 -4.98086140e-02 -9.77815092e-02  1.07723698e-01  9.78112295e-02
  3.86746526e-01  6.92044735e-01 -2.34165221e-01  3.25265408e-01
 -2.22025976e-01 -1.45359263e-01  1.73291773e-01 -1.24958701e-01
  2.49414772e-01 -1.70392066e-01  1.53882906e-01 -7.36432374e-02
 -1.68837667e-01  2.21240163e-01 -5.87729178e-02  1.97399348e-01]"
esrgan re-convert to tflite fail stat:awaiting response stale type:performance TFLiteConverter TF 2.13,"### 1. System information

- OS Platform and Distribution: macOS 12.2.1; Apple M1; MacBook Pro
- TensorFlow installation : pip3 install tensorflow
- TensorFlow library: 2.13.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
convert tflite:  https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/ml/super_resolution.ipynb
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
test demo:  https://github.com/tensorflow/examples/tree/master/lite/examples/super_resolution
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces some errors

### 4. (optional) RNN conversion support
model is esrgan

### 5. (optional) Any other info / logs
case1.  enable optimize like ""tf.lite.Optimize.DEFAULT"",  load model fail: Didn't find op for builtin opcode 'DEQUANTIZE' version '5'
case2. disable optimize, set_shape 50x50, run fail msg: Something went wrong when copying input buffer to input tensor
case3. disable optimize, set_shape 640x360, run fail msg: signal 11 (SIGSEGV): stack pointer is in a non-existent map; likely due to stack overflow.  function crash: SuperResolution.cpp->DoSuperResolution() line at: TfLiteInterpreterAllocateTensors
",False,"[-3.85689020e-01 -5.09523451e-01 -2.21537352e-02  1.55332133e-01
  2.00857490e-01 -5.85660599e-02 -3.82108949e-02 -1.23595893e-02
 -2.53852844e-01 -1.38744801e-01 -6.84713200e-02  7.89615326e-03
 -1.96150765e-01  1.90901488e-01 -4.16596383e-01  2.25054443e-01
 -1.74956098e-01 -2.46445864e-01  3.55376869e-01 -4.03040200e-02
  1.63678117e-02  2.26260461e-02 -2.90367723e-01  2.02096775e-01
  1.53003365e-01  1.57746553e-01 -1.31126523e-01 -5.23672253e-02
 -7.41336122e-02 -5.34815900e-03  2.45665982e-01  7.50323609e-02
 -7.02169165e-02  6.18231297e-02  6.22391589e-02  7.62212202e-02
 -7.02129453e-02 -1.34053394e-01 -3.09697807e-01 -2.75244951e-01
  1.09438077e-01 -2.61340849e-03  4.50653136e-02  1.42606556e-01
 -3.83324549e-02  4.01206054e-02  1.50591731e-01 -6.27722368e-02
 -7.22259134e-02  2.51577981e-02 -1.72167569e-01 -4.53118384e-02
 -3.07154179e-01 -1.80248827e-01 -5.92960678e-02  2.33157709e-01
  9.84896123e-02  1.95448041e-01  5.43681607e-02  4.69216844e-03
  2.40927204e-01  2.88805794e-02 -2.49742314e-01 -1.42253444e-01
  2.08843574e-01  4.30848181e-01  1.68901786e-01 -1.15947850e-01
  3.00164551e-01 -5.70498556e-02 -3.71556357e-02  1.77737586e-02
 -3.62831414e-01  4.73237336e-02  8.51316452e-02  2.35101253e-01
 -1.53006157e-02  1.23542145e-01  2.53675699e-01  1.21923819e-01
 -2.07246453e-01 -2.11320043e-01 -5.79900295e-02 -2.43958198e-02
  2.33676463e-01 -9.82486382e-02  1.72667857e-03 -8.17637742e-02
  1.51158199e-01 -1.34555668e-01  2.68532038e-01  2.57357180e-01
  1.48488238e-01 -4.16111648e-02  3.44582677e-01  1.41823977e-01
  6.44304231e-02  2.45710880e-01  2.17227131e-01  4.92939241e-02
 -9.57409963e-02 -9.20551568e-02 -1.53200895e-01 -5.82601093e-02
  1.86461098e-02 -3.77413213e-01  3.36263061e-01  1.55870661e-01
  4.91943210e-05  2.64686365e-02  1.99056461e-01 -2.63711084e-02
  2.31522813e-01  8.66552964e-02  2.72264004e-01  9.18333232e-02
 -9.48393196e-02  1.52296185e-01  2.26589039e-01  6.03921294e-01
 -2.06887573e-01  6.37179911e-02  1.16616055e-01  8.34378824e-02
  1.34903431e-01  9.98906493e-02 -1.80573747e-01 -7.14313090e-02
 -2.00390052e-02  2.65546679e-01  1.25569582e-01  3.58597428e-01
 -2.00561285e-01  2.04267904e-01 -8.46001729e-02  5.22766933e-02
 -7.46921636e-04 -1.67239934e-01 -4.14500356e-01  1.00345716e-01
 -1.49335906e-01  1.73097312e-01 -9.58977640e-02 -3.70823294e-01
 -2.16165930e-03  7.34036863e-02 -2.47714520e-01  2.04698980e-01
 -6.11688010e-02  2.14131922e-03 -6.24888986e-02 -9.03576016e-02
  2.62963958e-02  2.83374369e-01  2.19252169e-01  1.24166772e-01
  2.83001006e-01 -1.91821642e-02 -3.78306918e-02 -2.95981884e-01
 -1.41882688e-01  1.81601182e-01 -3.33898902e-01 -1.14451721e-01
  2.77817175e-02  1.32486552e-01 -3.45423311e-01 -1.51584432e-01
  6.12432249e-02  4.04628575e-01 -1.02200545e-01 -1.76962823e-01
 -7.27685094e-02  1.84036478e-01  2.69198537e-01 -1.63350582e-01
  6.02233946e-01 -5.71706772e-01 -5.98597750e-02 -1.96582079e-01
  6.57820031e-02  1.90282241e-02  5.98073751e-02 -2.38302462e-02
  2.14241110e-02  5.11847287e-02  3.07780802e-01  1.52698010e-01
 -2.80394346e-01  4.89032790e-02 -3.08762550e-01  5.28695667e-03
  2.53177136e-01  2.07932234e-01 -1.57533944e-01 -1.13750644e-01
 -4.98761013e-02 -9.47706252e-02 -7.10668564e-02  7.72621483e-02
  4.84329648e-03  1.21162832e-03 -1.34007543e-01 -2.85066012e-02
 -7.03326315e-02 -2.86440641e-01 -2.01642320e-01 -3.44809711e-01
 -5.51807106e-01 -2.35699564e-01  1.58258244e-01 -2.64793873e-01
  5.00172563e-02 -5.84617741e-02 -1.92222908e-01 -9.73271281e-02
  4.42673564e-02  6.65948018e-02 -3.40403557e-01  1.63004488e-01
 -1.17293531e-02  8.74647722e-02  1.01493619e-01 -2.34987095e-01
 -4.13793594e-01 -6.10764325e-02 -1.19228289e-01  9.01326910e-02
 -2.83544566e-02  3.06491494e-01 -1.31834885e-02 -3.13133560e-02
  4.65664566e-01 -8.67952853e-02  1.00961775e-01 -2.27228239e-01
  1.23209981e-02 -1.71102017e-01 -4.51675832e-01 -5.30963913e-02
 -2.20661253e-01 -5.90309873e-02  5.54848649e-02 -2.32716426e-02
 -4.37716171e-02  1.41537324e-01 -5.54334037e-02 -1.50164992e-01
 -2.55146354e-01  3.97790134e-01 -5.47127351e-02 -1.63244367e-01
  3.64358753e-01  2.15135306e-01  1.15306780e-01  1.42120510e-01
  1.26413852e-01 -1.98244676e-02  5.18707223e-02  6.55035824e-02
  2.51300037e-01  1.74340308e-01  1.54976502e-01  4.69233572e-01
  3.00487876e-01  3.97574574e-01 -5.90378307e-02  9.59556401e-02
 -2.59167589e-02  5.23556545e-02  2.41791699e-02 -3.21393788e-01
  3.23470622e-01 -1.32212907e-01 -3.51932459e-03 -1.40362561e-01
  9.75556672e-02 -6.16495684e-02 -3.92532684e-02  1.82254493e-01
  1.20732151e-01  3.03858280e-01 -5.73495030e-01  4.45809290e-02
 -1.73078209e-01 -2.01623976e-01  1.98141187e-01 -4.22611415e-01
 -1.05331823e-01  8.28195065e-02 -1.96680322e-01  2.80714929e-02
  3.46113518e-02  6.40284941e-02 -4.65448461e-02  1.58342838e-01
  2.89243281e-01 -6.24018684e-02  1.21637605e-01 -2.58379988e-02
 -2.19362020e-01  2.21761748e-01  2.72062123e-01 -4.19323325e-01
 -2.46179365e-02  9.39513147e-02  2.76807278e-01  6.19825423e-02
  4.13470268e-01 -3.41931134e-01  3.00908625e-01  3.93787846e-02
  9.56535786e-02  3.57130527e-01 -8.89350846e-02 -4.76565063e-02
 -3.16576660e-01  4.27635580e-01  2.06076115e-01 -2.88774259e-02
  1.62778944e-01 -6.20467849e-02 -3.72861594e-01  4.67049554e-02
  1.24943666e-02  1.47856742e-01 -1.76040716e-02 -6.56785741e-02
 -6.77158684e-02  7.00916909e-03 -2.46122554e-02 -7.89495781e-02
 -1.97402298e-01  2.76596308e-01 -3.22034776e-01  3.30067985e-02
 -1.87654883e-01  1.60296053e-01 -7.66401440e-02 -3.38607460e-01
 -2.87891459e-03 -1.62234753e-01 -2.99988016e-02 -3.21079850e-01
  1.65449709e-01 -2.30350330e-01  1.61736161e-01  3.12634051e-01
 -1.58531204e-01  3.81136462e-02 -2.99059987e-01 -3.84496748e-02
 -2.02761322e-01 -4.51679975e-02 -5.15765920e-02  2.11203098e-01
 -1.43757582e-01  4.23958004e-02  2.48508453e-01  3.81978035e-01
 -1.28303975e-01 -4.91569526e-02 -4.61990535e-01 -1.36609584e-01
  1.08657181e-01 -6.29007295e-02 -3.74873996e-01 -3.94101441e-03
  9.26382840e-02  6.03042543e-01 -7.93965980e-02  2.10140347e-01
 -3.17589790e-02  1.06516249e-01  4.20217693e-01 -2.39809304e-01
 -1.79284632e-01  1.04296014e-01  3.65310535e-02 -1.56652153e-01
  5.73602132e-02 -1.02886140e-01  5.41459620e-02 -1.04880817e-01]"
Cannot build tensorflow2.7-gpu version with bazel3.7.2 stat:awaiting response type:build/install stale subtype: ubuntu/linux TF 2.7,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.7

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

Linux Ubuntu 22.04

### Python version

3.8

### Bazel version

3.7.2

### GCC/compiler version

gcc 11

### CUDA/cuDNN version

12.1/8.9

### GPU model and memory

Quadro RTX 8000, 48GB

### Current behavior?

Context: I can use the exact same source codes to successfully build the CPU-only version. I hope to get a gpu version with the same code.

So I ran `bazel clean`, it looks fine.

Then I run `bazel build --config=cuda //tensorflow/tools/pip_package:build_pip_package`. I have tried for several times, but everytime got the following error: (I only paste the main part here, before this there were some regular bazel output, e.g., INFO:...)

```
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1076, column 27, in _create_local_cuda_repository
                cuda_libs = _find_libs(repository_ctx, check_cuda_libs_script, cuda_config)
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 606, column 21, in _find_libs
                _check_cuda_libs(repository_ctx, check_cuda_libs_script, check_cuda_libs_params.values())
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, column 28, in _check_cuda_libs
                checked_paths = execute(repository_ctx, [python_bin, ""-c"", cmd]).stdout.splitlines()
        File ""/home/xiaxia/tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Repository command failed
Expected even number of arguments
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//c
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//c
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: Repository command failed
Expected even number of arguments
```

Note that, I do not have root/admin role of the remote ubuntu machine, will it be a problem? I also tried to implement cuda and cudnn in my /home/ directory, but it still not help.

Please provide me with any advice, and thanks a lot!

### Standalone code to reproduce the issue

```shell
Sorry I don't know how to share my problem as reproducible, it happens in the process of building tensorflow.
```


### Relevant log output

```shell
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=157
INFO: Reading rc options for 'build' from /home/xiaxia/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/xiaxia/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /home/xiaxia/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/xiaxia/anaconda3/envs/tf-cus-gpu/bin/python3 --action_env PYTHON_LIB_PATH=/home/xiaxia/anaconda3/envs/tf-cus-gpu/lib/python3.8/site-packages --python_path=/home/xiaxia/anaconda3/envs/tf-cus-gpu/bin/python3 --action_env TF_CUDA_VERSION=12.2 --action_env TF_CUDNN_VERSION=8 --action_env TF_NCCL_VERSION= --action_env TF_CUDA_PATHS=/home/xiaxia/cuda-12.2 --action_env CUDA_TOOLKIT_PATH=/home/xiaxia/cuda-12.2 --action_env TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0 --action_env LD_LIBRARY_PATH=/usr/local/cuda-12.1/targets/x86_64-linux/lib:/usr/local/cuda-12.1/targets/x86_64-linux/lib:/usr/local/cuda-12.1/targets/x86_64-linux/lib::/home/xiaxia/cuda-12.2/lib64:/home/xiaxia/cuda-12.2/lib64 --action_env GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-11 --config=cuda
INFO: Reading rc options for 'build' from /home/xiaxia/tensorflow/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils
INFO: Found applicable config definition build:short_logs in file /home/xiaxia/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/xiaxia/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:linux in file /home/xiaxia/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/xiaxia/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Repository local_config_cuda instantiated at:
  /home/xiaxia/tensorflow/WORKSPACE:15:14: in <toplevel>
  /home/xiaxia/tensorflow/tensorflow/workspace2.bzl:1080:19: in workspace
  /home/xiaxia/tensorflow/tensorflow/workspace2.bzl:94:19: in _tf_toolchains
Repository rule cuda_configure defined at:
  /home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl:1448:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1401, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1076, column 27, in _create_local_cuda_repository
                cuda_libs = _find_libs(repository_ctx, check_cuda_libs_script, cuda_config)
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 606, column 21, in _find_libs
                _check_cuda_libs(repository_ctx, check_cuda_libs_script, check_cuda_libs_params.values())
        File ""/home/xiaxia/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, column 28, in _check_cuda_libs
                checked_paths = execute(repository_ctx, [python_bin, ""-c"", cmd]).stdout.splitlines()
        File ""/home/xiaxia/tensorflow/third_party/remote_config/common.bzl"", line 230, column 13, in execute
                fail(
Error in fail: Repository command failed
Expected even number of arguments
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
INFO: Found applicable config definition build:cuda in file /home/xiaxia/tensorflow/.bazelrc: --repo_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --@local_config_cuda//:enable_cuda
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: @local_config_cuda//:enable_cuda :: Error loading option @local_config_cuda//:enable_cuda: Repository command failed
Expected even number of arguments
```
",False,"[-0.5202509  -0.38573825 -0.25567824  0.18958335  0.150504   -0.44688216
 -0.3323371   0.11559948 -0.20679584 -0.39409006 -0.00538236  0.16408092
 -0.28268844  0.09666193 -0.16909783  0.5434477  -0.21372986 -0.04406771
  0.32348877  0.23490825 -0.28233477 -0.18521181 -0.1633302   0.2679531
  0.24544384  0.21735457 -0.10194987  0.05861493  0.06543748  0.10980178
  0.46126384  0.1961443  -0.02897014  0.12612341  0.06730904  0.24080752
 -0.26138407 -0.15922883 -0.22053388 -0.14846362  0.06162199  0.1295438
  0.21590704 -0.12199302  0.02901698 -0.2459234   0.12651812 -0.04087401
 -0.05931314 -0.2969731  -0.09121674 -0.20438845 -0.30529845 -0.42534465
 -0.11639187 -0.05098291  0.23367308 -0.04089547  0.01111322  0.2149112
  0.23771429  0.13546821  0.20028138  0.02802847  0.07703403  0.18633929
  0.1597037  -0.22671251  0.5040719  -0.1363398   0.2438319  -0.26981035
 -0.36535364  0.08761299 -0.06176346  0.16169585  0.2701645   0.30857992
  0.2669937  -0.06180903 -0.05447668 -0.34648943 -0.09046082 -0.25374746
  0.06057002 -0.00404579  0.40614542  0.16010892  0.4142235  -0.3739016
  0.56189156  0.54144675  0.03472936  0.00913504  0.27069932  0.18277007
  0.29499933  0.39462823 -0.06728946 -0.26377708 -0.03930961 -0.38535184
  0.11492524  0.0665859  -0.16715194 -0.16483635  0.30739403 -0.06593396
 -0.01786503 -0.13178185  0.15997094 -0.08265655  0.27610236  0.13533428
  0.0415896  -0.05232839 -0.37009695  0.1889689  -0.07503471  0.86925316
 -0.13268918 -0.01382567 -0.02972705 -0.08482446  0.5490011  -0.00782131
 -0.20452178 -0.09281394  0.4478722   0.28482282 -0.00598338  0.2896135
 -0.13173692  0.18699066 -0.09573667  0.1744813  -0.22276676 -0.0671591
 -0.03949321 -0.25833893 -0.16055954  0.20235537  0.00259467 -0.618401
  0.03712422 -0.01142959 -0.18670446  0.22750524 -0.35155058  0.1952207
 -0.26824933 -0.04999609 -0.11972759  0.33718812  0.19948082 -0.0388332
  0.21044119 -0.09033699 -0.03675764 -0.46334738 -0.0047407   0.53792787
 -0.10487778 -0.20187026 -0.22783034  0.17918432 -0.45491254 -0.30765504
  0.25628436  0.49199817 -0.13645741 -0.10910656  0.26327717  0.27565104
  0.18004794 -0.2060031   0.43214053 -0.52145076 -0.02676592  0.41050202
  0.07934632  0.21170837  0.02739435  0.0845712   0.18770662 -0.16344544
  0.11654817  0.01596278 -0.26246136 -0.08509797 -0.36821234 -0.13924089
  0.4620649  -0.3154031  -0.31600085  0.19693293  0.31467134  0.00228428
  0.11044065 -0.00486045 -0.06815975  0.01525031 -0.06320842  0.02103215
  0.0157249  -0.21884751 -0.0894696  -0.18751276 -0.47907254 -0.09808165
  0.12674357 -0.39440954  0.23901972 -0.09388682 -0.2284832   0.14336567
  0.26589233  0.02147321 -0.24931559  0.34158224  0.13836883 -0.31565297
 -0.02744987 -0.36433676 -0.15717363 -0.05899203 -0.12575859  0.1060835
 -0.04875132  0.14229348 -0.07631063  0.05182733  0.40200964  0.3501665
  0.31750906 -0.1486849   0.05831638 -0.18937129 -0.25485212  0.08432873
 -0.28979108 -0.11218102  0.12397774 -0.04950846  0.31904668  0.2747054
 -0.15317261 -0.14652847 -0.3569038   0.38999966 -0.24011596  0.10927719
  0.25039005  0.2435549   0.48998186  0.237295   -0.0998508   0.18862483
  0.17476386 -0.13464761  0.30967963  0.2488401  -0.12366098  0.21368998
  0.03614264  0.2669643  -0.45114174  0.4655243   0.21388683 -0.19019794
  0.21853872 -0.24541457  0.59194654 -0.45554584  0.03487317 -0.02719473
  0.23835318  0.13004409 -0.05050538 -0.08489057  0.14152496  0.23526706
 -0.30487502 -0.10271896  0.05700602 -0.08694261  0.05390532 -0.7829661
 -0.34145004  0.24658996 -0.24961276  0.24736041 -0.2626209  -0.12820902
 -0.37938464 -0.18434967  0.00821797  0.10594004  0.09143542  0.22226074
 -0.09826668  0.02606532  0.3756425  -0.46183684 -0.18768619 -0.1611287
  0.07403435  0.1304849   0.55219066 -0.39550447  0.00801616 -0.17318976
  0.02636996  0.35729784  0.09875274  0.2705453  -0.45249623  0.5666739
  0.39763132 -0.18360858  0.1807456  -0.13660537 -0.37898943  0.16478893
  0.27185783 -0.25779477 -0.01664091 -0.50018084  0.06569792  0.3179121
 -0.2648844  -0.028418   -0.07051324  0.18930341 -0.28005892  0.12966202
 -0.4034454  -0.00890577 -0.17777553 -0.3784017  -0.18243286 -0.24861386
 -0.09456997 -0.31912196 -0.19030422 -0.31599107  0.36806813  0.29510084
 -0.22600847  0.03902233 -0.03108334  0.31776297 -0.4674106   0.03555642
  0.02867988  0.05055105  0.059031   -0.07767956  0.5639546   0.278193
 -0.0789098   0.1406903  -0.31373584  0.1477344   0.32660204 -0.34286857
 -0.251351   -0.14918849 -0.19400415  0.4212654   0.03441495  0.32522604
 -0.21829782  0.28183374  0.5724678  -0.59697974 -0.46970192  0.24401432
 -0.04993374 -0.23107074 -0.04413505 -0.11096795  0.32972842 -0.06266853]"
 RuntimeError: Quantization to 16x8-bit not yet supported for op: 'FLOOR_MOD'    stat:awaiting response stale type:performance TFLiteConverter TF 2.12,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
google colab
- TensorFlow installation (pip package or built from source):
- pip
- TensorFlow library (version, if pip package or github SHA, if built from source):
- 2.12.0

### 2. Code
I have following model.
```python
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

train_labels = train_labels[:1000]
test_labels = test_labels[:1000]

train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0


# Normalize the input image so that each pixel value is between 0 to 1.
train_images = train_images.astype(np.float32) / 255.0
test_images = test_images.astype(np.float32) / 255.0


# Define a simple sequential model

model_infrence = tf.keras.Sequential([
    MyDense(512, activation='relu', input_shape=(784,)),
    keras.layers.Dense(512, activation='relu', input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10)
  ])

model_infrence.compile(optimizer='adam',
                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])



# Create a basic model instance


# Display the model's architecture
model_infrence.summary()

# Create a basic model instance


# Display the model's architecture
model_infrence.summary()
```

Then I custom one of the dense layers like so 
```python
# Copyright 2015 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
""""""Contains the Dense layer.""""""


import tensorflow.compat.v2 as tf

from keras import activations
from keras import backend
from keras import constraints
from keras import initializers
from keras import regularizers
from keras.dtensor import utils
from keras.engine.base_layer import Layer
from keras.engine.input_spec import InputSpec

# isort: off
from tensorflow.python.util.tf_export import keras_export


@keras_export(""keras.layers.Dense"")
class MyDense(Layer):
    """"""Just your regular densely-connected NN layer.

    `Dense` implements the operation:
    `output = activation(dot(input, kernel) + bias)`
    where `activation` is the element-wise activation function
    passed as the `activation` argument, `kernel` is a weights matrix
    created by the layer, and `bias` is a bias vector created by the layer
    (only applicable if `use_bias` is `True`). These are all attributes of
    `Dense`.

    Note: If the input to the layer has a rank greater than 2, then `Dense`
    computes the dot product between the `inputs` and the `kernel` along the
    last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).
    For example, if input has dimensions `(batch_size, d0, d1)`, then we create
    a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2
    of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are
    `batch_size * d0` such sub-tensors).  The output in this case will have
    shape `(batch_size, d0, units)`.

    Besides, layer attributes cannot be modified after the layer has been called
    once (except the `trainable` attribute).
    When a popular kwarg `input_shape` is passed, then keras will create
    an input layer to insert before the current layer. This can be treated
    equivalent to explicitly defining an `InputLayer`.

    Example:

    >>> # Create a `Sequential` model and add a Dense layer as the first layer.
    >>> model = tf.keras.models.Sequential()
    >>> model.add(tf.keras.Input(shape=(16,)))
    >>> model.add(tf.keras.layers.Dense(32, activation='relu'))
    >>> # Now the model will take as input arrays of shape (None, 16)
    >>> # and output arrays of shape (None, 32).
    >>> # Note that after the first layer, you don't need to specify
    >>> # the size of the input anymore:
    >>> model.add(tf.keras.layers.Dense(32))
    >>> model.output_shape
    (None, 32)

    Args:
        units: Positive integer, dimensionality of the output space.
        activation: Activation function to use.
            If you don't specify anything, no activation is applied
            (ie. ""linear"" activation: `a(x) = x`).
        use_bias: Boolean, whether the layer uses a bias vector.
        kernel_initializer: Initializer for the `kernel` weights matrix.
        bias_initializer: Initializer for the bias vector.
        kernel_regularizer: Regularizer function applied to
            the `kernel` weights matrix.
        bias_regularizer: Regularizer function applied to the bias vector.
        activity_regularizer: Regularizer function applied to
            the output of the layer (its ""activation"").
        kernel_constraint: Constraint function applied to
            the `kernel` weights matrix.
        bias_constraint: Constraint function applied to the bias vector.

    Input shape:
        N-D tensor with shape: `(batch_size, ..., input_dim)`.
        The most common situation would be
        a 2D input with shape `(batch_size, input_dim)`.

    Output shape:
        N-D tensor with shape: `(batch_size, ..., units)`.
        For instance, for a 2D input with shape `(batch_size, input_dim)`,
        the output would have shape `(batch_size, units)`.
    """"""

    @utils.allow_initializer_layout
    def __init__(
        self,
        units,
        activation=None,
        use_bias=True,
        kernel_initializer=""glorot_uniform"",
        bias_initializer=""zeros"",
        kernel_regularizer=None,
        bias_regularizer=None,
        activity_regularizer=None,
        kernel_constraint=None,
        bias_constraint=None,
        **kwargs,
    ):
        super().__init__(activity_regularizer=activity_regularizer, **kwargs)

        self.units = int(units) if not isinstance(units, int) else units
        if self.units < 0:
            raise ValueError(
                ""Received an invalid value for `units`, expected ""
                f""a positive integer. Received: units={units}""
            )
        self.activation = activations.get(activation)
        self.use_bias = use_bias
        self.kernel_initializer = initializers.get(kernel_initializer)
        self.bias_initializer = initializers.get(bias_initializer)
        self.kernel_regularizer = regularizers.get(kernel_regularizer)
        self.bias_regularizer = regularizers.get(bias_regularizer)
        self.kernel_constraint = constraints.get(kernel_constraint)
        self.bias_constraint = constraints.get(bias_constraint)

        self.input_spec = InputSpec(min_ndim=2)
        self.supports_masking = True

    def build(self, input_shape):
        dtype = tf.as_dtype(self.dtype or backend.floatx())
        if not (dtype.is_floating or dtype.is_complex):
            raise TypeError(
                ""A Dense layer can only be built with a floating-point ""
                f""dtype. Received: dtype={dtype}""
            )

        input_shape = tf.TensorShape(input_shape)
        last_dim = tf.compat.dimension_value(input_shape[-1])
        if last_dim is None:
            raise ValueError(
                ""The last dimension of the inputs to a Dense layer ""
                ""should be defined. Found None. ""
                f""Full input shape received: {input_shape}""
            )
        self.input_spec = InputSpec(min_ndim=2, axes={-1: last_dim})
        self.kernel = self.add_weight(
            ""kernel"",
            shape=[last_dim, self.units],
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            constraint=self.kernel_constraint,
            dtype=self.dtype,
            trainable=True,
        )
        if self.use_bias:
            self.bias = self.add_weight(
                ""bias"",
                shape=[
                    self.units,
                ],
                initializer=self.bias_initializer,
                regularizer=self.bias_regularizer,
                constraint=self.bias_constraint,
                dtype=self.dtype,
                trainable=True,
            )
        else:
            self.bias = None
        self.built = True

    def call(self, inputs):
        if inputs.dtype.base_dtype != self._compute_dtype_object.base_dtype:
            inputs = tf.cast(inputs, dtype=self._compute_dtype_object)

        is_ragged = isinstance(inputs, tf.RaggedTensor)
        if is_ragged:
            # In case we encounter a RaggedTensor with a fixed last dimension
            # (last dimension not ragged), we can flatten the input and restore
            # the ragged dimensions at the end.
            if tf.compat.dimension_value(inputs.shape[-1]) is None:
                raise ValueError(
                    ""Dense layer only supports RaggedTensors when the ""
                    ""innermost dimension is non-ragged. Received: ""
                    f""inputs.shape={inputs.shape}.""
                )
            original_inputs = inputs
            if inputs.flat_values.shape.rank > 1:
                inputs = inputs.flat_values
            else:
                # Innermost partition is encoded using uniform_row_length.
                # (This is unusual, but we can handle it.)
                if inputs.shape.rank == 2:
                    inputs = inputs.to_tensor()
                    is_ragged = False
                else:
                    for _ in range(original_inputs.ragged_rank - 1):
                        inputs = inputs.values
                    inputs = inputs.to_tensor()
                    original_inputs = tf.RaggedTensor.from_nested_row_splits(
                        inputs, original_inputs.nested_row_splits[:-1]
                    )

        rank = inputs.shape.rank
        if rank == 2 or rank is None:
            # We use embedding_lookup_sparse as a more efficient matmul
            # operation for large sparse input tensors. The op will result in a
            # sparse gradient, as opposed to
            # sparse_ops.sparse_tensor_dense_matmul which results in dense
            # gradients. This can lead to sigfinicant speedups, see b/171762937.
            if isinstance(inputs, tf.SparseTensor):
                # We need to fill empty rows, as the op assumes at least one id
                # per row.
                inputs, _ = tf.sparse.fill_empty_rows(inputs, 0)
                # We need to do some munging of our input to use the embedding
                # lookup as a matrix multiply. We split our input matrix into
                # separate ids and weights tensors. The values of the ids tensor
                # should be the column indices of our input matrix and the
                # values of the weights tensor can continue to the actual matrix
                # weights.  The column arrangement of ids and weights will be
                # summed over and does not matter. See the documentation for
                # sparse_ops.sparse_tensor_dense_matmul a more detailed
                # explanation of the inputs to both ops.
                ids = tf.SparseTensor(
                    indices=inputs.indices,
                    values=inputs.indices[:, 1],
                    dense_shape=inputs.dense_shape,
                )
                weights = inputs
                outputs = tf.nn.embedding_lookup_sparse(
                    self.kernel, ids, weights, combiner=""sum""
                )
            else:
                

                  print(inputs)
                  quotient, x = divmod(inputs, (2**n))
                  #x = inputs % (2**n);
                  quotient1, x1 = divmod(inputs, (2**n - 1))
                  #x1 = inputs % (2**n - 1);
                  quotient2, x2 = divmod(inputs, (2**n + 1))
                  #x2 = inputs % (2**n + 1);
                  # w =  self.w % (2**n);
                  quotient3, w = divmod(self.w, (2**n))
                  # w1 = self.w % (2**n - 1);
                  quotient4, w1 = divmod(self.w, (2**n - 1))
                  # w2 = self.w % (2**n + 1)
                  quotient5, w2 = divmod(self.w, (2**n + 1))
                  quotient6, z = divmod((tf.matmul(x, w) + self.b), (2**n))
                  # z = (tf.matmul(x, w) + self.b) % (2**n)
                  quotient7, z1 = divmod((tf.matmul(x, w) + self.b), (2**n - 1))
                  # z1 = (tf.matmul(x1, w1) + self.b) % (2**n - 1)
                  quotient8, z2 = divmod((tf.matmul(x, w) + self.b), (2**n + 1))
                  # z2 = tf.matmul(x2, w2) + self.b % (2**n + 1)

                  Dm = (2**n) * (2**n - 1) * (2**n + 1);
                  m1 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n));
                  m2 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n - 1));
                  m3 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n + 1));

                  outputs = rns_to_decimal(Dm, z, z1, z2, m1, m2, m3, n)
                  print(outputs)
        # Broadcast kernel to inputs.
        else:
            outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])
            # Reshape the output back to the original ndim of the input.
            if not tf.executing_eagerly():
                shape = inputs.shape.as_list()
                output_shape = shape[:-1] + [self.kernel.shape[-1]]
                outputs.set_shape(output_shape)

        # if self.use_bias:
        #     outputs = tf.nn.bias_add(outputs, self.bias)

        if self.activation is not None:
            outputs = self.activation(outputs)

        if is_ragged:
            outputs = original_inputs.with_flat_values(outputs)

        return outputs

    def compute_output_shape(self, input_shape):
        input_shape = tf.TensorShape(input_shape)
        input_shape = input_shape.with_rank_at_least(2)
        if tf.compat.dimension_value(input_shape[-1]) is None:
            raise ValueError(
                ""The last dimension of the input shape of a Dense layer ""
                ""should be defined. Found None. ""
                f""Received: input_shape={input_shape}""
            )
        return input_shape[:-1].concatenate(self.units)

    def get_config(self):
        config = super().get_config()
        config.update(
            {
                ""units"": self.units,
                ""activation"": activations.serialize(self.activation),
                ""use_bias"": self.use_bias,
                ""kernel_initializer"": initializers.serialize(
                    self.kernel_initializer
                ),
                ""bias_initializer"": initializers.serialize(
                    self.bias_initializer
                ),
                ""kernel_regularizer"": regularizers.serialize(
                    self.kernel_regularizer
                ),
                ""bias_regularizer"": regularizers.serialize(
                    self.bias_regularizer
                ),
                ""activity_regularizer"": regularizers.serialize(
                    self.activity_regularizer
                ),
                ""kernel_constraint"": constraints.serialize(
                    self.kernel_constraint
                ),
                ""bias_constraint"": constraints.serialize(self.bias_constraint),
            }
        )
        return config
    def rns_to_decimal(dm, z1, z2, z3, m1, m2, m3, n = 6):
        M1 = 2**n
        M2 = 2**n - 1
        M3 = 2**n + 1
        x1 = 1
        x2 = 1
        x3 = 1
        quotient, mm1 = divmod(m1, M1)
       # mm1 = m1 % M1
        for i in range(1, M1):
            quotient1, X = divmod((i * mm1), M1)
            if X == 1:
                x1 = i
        quotient2, mm2 = divmod(m2, M2)
       # mm2 = m2 % M2
        for i in range(1, M2):
            quotient3, X1 = divmod((i * mm2), M2)
            if X1 == 1:
                x2 = i
       # mm3 = m3 % M3
        quotient4, mm3 = divmod(m3, M3)
        for i in range(1, M3):
            quotient5, X2 = divmod((i * mm3), M3)
            if X2 == 1:
                x3 = i
        quotient5, num = divmod((z1 * m1 * x1 + z2 * m2 * x2 + z3 * m3 * x3), dm)
       # num = (z1 * m1 * x1 + z2 * m2 * x2 + z3 * m3 * x3) % dm
        return num
```

The only change to original layer is following code:

```python
                  quotient, x = divmod(inputs, (2**n))
                  #x = inputs % (2**n);
                  quotient1, x1 = divmod(inputs, (2**n - 1))
                  #x1 = inputs % (2**n - 1);
                  quotient2, x2 = divmod(inputs, (2**n + 1))
                  #x2 = inputs % (2**n + 1);
                  # w =  self.w % (2**n);
                  quotient3, w = divmod(self.w, (2**n))
                  # w1 = self.w % (2**n - 1);
                  quotient4, w1 = divmod(self.w, (2**n - 1))
                  # w2 = self.w % (2**n + 1)
                  quotient5, w2 = divmod(self.w, (2**n + 1))
                  quotient6, z = divmod((tf.matmul(x, w) + self.b), (2**n))
                  # z = (tf.matmul(x, w) + self.b) % (2**n)
                  quotient7, z1 = divmod((tf.matmul(x, w) + self.b), (2**n - 1))
                  # z1 = (tf.matmul(x1, w1) + self.b) % (2**n - 1)
                  quotient8, z2 = divmod((tf.matmul(x, w) + self.b), (2**n + 1))
                  # z2 = tf.matmul(x2, w2) + self.b % (2**n + 1)

                  Dm = (2**n) * (2**n - 1) * (2**n + 1);
                  m1 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n));
                  m2 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n - 1));
                  m3 = math.floor(((2**n) * (2**n - 1) * (2**n + 1))/(2**n + 1));

                  outputs = rns_to_decimal(Dm, z, z1, z2, m1, m2, m3, n)
```

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
",False,"[-0.41737223 -0.5303365  -0.07264842  0.06334235  0.14575678 -0.13189742
 -0.12024682  0.03656602 -0.31256303 -0.19814327 -0.04343388 -0.02305762
 -0.12181114  0.1920397  -0.36271426  0.15996654  0.12230895 -0.3240401
  0.33586857 -0.11990742 -0.09381412 -0.07773381 -0.20643905  0.2931919
  0.38113758  0.16301537 -0.19601513 -0.08438496  0.07114046  0.22135916
  0.28662172 -0.1149325  -0.06126023 -0.05483933  0.02449883  0.16652632
  0.05399956 -0.03923782 -0.28858837 -0.11180677  0.11146469  0.06128997
 -0.12573259  0.09119825  0.03840888  0.03341255  0.2799606  -0.0758042
 -0.33319914 -0.05837122 -0.08473003  0.15758055 -0.2576174  -0.01938802
  0.02721507 -0.05891015  0.11892109  0.16584659  0.11955298  0.13219762
  0.01200105 -0.04445965 -0.0846976   0.01756576  0.07756169  0.31747162
  0.20961991 -0.301467    0.24357885 -0.28914735 -0.06320417 -0.11423857
 -0.19554791  0.06415467  0.03845903  0.11619221 -0.10984046  0.21361187
  0.21587805 -0.01040239 -0.00197335 -0.2911861   0.18781793  0.19421126
  0.03186217 -0.11117245  0.02736954  0.25765708  0.230202   -0.18148583
  0.23548932  0.5012324  -0.12615189  0.13551094  0.30209482  0.00439027
  0.00979919  0.37829143  0.23563641 -0.0193767  -0.02401751 -0.26873022
 -0.45813054 -0.06307031  0.08732231 -0.11133449  0.06392606  0.06382419
  0.20387271  0.03845985  0.24333766  0.09076248  0.09370508 -0.13348743
  0.17901334  0.06353026 -0.1262546   0.17352752  0.11663617  0.5139849
 -0.01386834 -0.01895818 -0.0212501   0.0795878   0.35946852  0.07380515
 -0.0275482  -0.05955863  0.18308842  0.16183425  0.12561536  0.21865985
 -0.29430297  0.06071331  0.06271379 -0.05785208 -0.05234062 -0.01070145
 -0.18285213  0.03593588 -0.39151067  0.11743218  0.06524207 -0.2523686
 -0.11541939 -0.03986011 -0.13314515  0.22989205  0.02670537  0.00192147
 -0.22505942  0.08627236 -0.03944833  0.31169552  0.1717765   0.16673619
  0.24404436 -0.15169646 -0.04284279 -0.40711954 -0.03696957  0.2261146
 -0.22903092 -0.14828001 -0.00338952  0.14765315 -0.496172   -0.2158542
  0.1072087   0.16952154 -0.15097678 -0.19976875  0.05939013 -0.04618809
  0.1252284  -0.07902554  0.38433278 -0.30960488 -0.04272032  0.05671759
  0.27640927  0.03213885  0.11635731  0.1486716   0.0410596   0.00621938
  0.13493     0.18368559 -0.28984416 -0.04242115 -0.08273276 -0.02307254
  0.2421      0.03795155 -0.29281467 -0.06328616  0.13259926 -0.06397165
 -0.04508325  0.28567517 -0.02407964 -0.07297972 -0.00721262 -0.00604345
  0.00819661 -0.22292519 -0.25514236 -0.29096833 -0.4670515  -0.13295296
 -0.03653102 -0.25250274  0.24625456 -0.20256007 -0.17548606  0.08652607
 -0.2098737  -0.17261285 -0.17091487  0.04915647 -0.09678687 -0.02710363
 -0.04238784 -0.17326206 -0.18085197 -0.1869005  -0.2551173   0.28889388
  0.03687404  0.2592008  -0.17573264 -0.00123638  0.39617267  0.24048373
  0.09118635 -0.14506051  0.06308089 -0.10208526 -0.2975649   0.12107047
 -0.25483268 -0.04826901  0.04194901 -0.10740659 -0.07403667  0.08289734
 -0.04474818 -0.1699147  -0.1870653   0.31864762 -0.17446873 -0.1722643
  0.34590402  0.18541756  0.28615084  0.05343267 -0.01226226 -0.08605186
  0.2206561  -0.08561333  0.09665674  0.27099025  0.03183118  0.25938714
  0.29891402  0.13176    -0.23772648  0.17399803 -0.12922123 -0.00841235
  0.17990695 -0.2221356   0.21917391 -0.20297584  0.00927082 -0.00265422
  0.37311316  0.09884066 -0.09733428  0.14306477 -0.01786307  0.48763883
 -0.3917804   0.01131154  0.02507594 -0.25818703 -0.04133155 -0.3265404
 -0.24062401  0.10977916 -0.22816287  0.06425156 -0.02131475  0.16982342
 -0.07017474 -0.02586714  0.27967656  0.04002922  0.0816651   0.04410878
 -0.11117564  0.26194233  0.23030168  0.03842761 -0.11317568  0.1155533
  0.27302653  0.04198056  0.4315149  -0.36090136  0.12940183  0.16793038
  0.07162822  0.32388645  0.05490987  0.17817652 -0.28603262  0.3829883
  0.1412198  -0.0796968   0.22931781  0.01512934 -0.4288394  -0.06050114
 -0.01525486  0.1395048   0.11932293 -0.1505342  -0.13824138  0.08064893
 -0.01140806 -0.14634147 -0.20312878  0.11184595 -0.13314888 -0.17013162
 -0.03322165  0.227413   -0.00739828 -0.20610777 -0.05183771 -0.21571149
  0.04457483 -0.28638718 -0.13359372 -0.11892421  0.24618223  0.10670636
 -0.0768941   0.12483872 -0.11559135 -0.05355482 -0.27199918 -0.07545486
 -0.24794075  0.21072946 -0.09434194  0.04929366  0.18324777  0.30449432
 -0.18014613 -0.12485458 -0.3872472  -0.24030006  0.23732    -0.07915106
 -0.31262287 -0.10269047  0.05774863  0.5025619  -0.02890776  0.24476683
 -0.3351758   0.0178172   0.49023417 -0.08797252 -0.15760821 -0.0040187
  0.03399926 -0.22009432 -0.14554676  0.01010198  0.11055028 -0.08784994]"
Failed to build tensorflow on Apple silicon. stat:awaiting tensorflower type:build/install subtype:macOS TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13

### Custom code

No

### OS platform and distribution

macOS 13.4.1

### Mobile device

None

### Python version

3.11

### Bazel version

5.3.0-homebrew

### GCC/compiler version

Apple clang version 14.0.3 (clang-1403.0.22.14.1)

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current behavior?

After failing to build tensorflow using the default options, I attempted the solution suggested in this issue (https://github.com/tensorflow/tensorflow/issues/60179). However, I found that installing `coreutils` directly still resulted in the same problem.

### Standalone code to reproduce the issue

Default settings used for all options.
```shell
bazel build //tensorflow/tools/pip_package:build_pip_package
```
 

### Relevant log output

```shell
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=80
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.13.0/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/sunruiqi/miniconda3/envs/tensorflow-macos/bin/python3 --action_env PYTHON_LIB_PATH=/Users/sunruiqi/miniconda3/envs/tensorflow-macos/lib/python3.11/site-packages --python_path=/Users/sunruiqi/miniconda3/envs/tensorflow-macos/bin/python3
INFO: Reading rc options for 'build' from /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc:
  'build' options: --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_tfrt,tensorflow/compiler/mlir/tfrt/tests/lhlo_to_jitrt,tensorflow/compiler/mlir/tfrt/tests/tf_to_corert,tensorflow/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils,tensorflow/core/tfrt/utils/debug
INFO: Found applicable config definition build:short_logs in file /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:macos in file /Users/sunruiqi/Desktop/tensorflow-2.13.0/.bazelrc: --apple_platform_type=macos --copt=-DGRPC_BAZEL_BUILD --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (611 packages loaded, 37637 targets configured).
INFO: Found 1 target...
ERROR: /Users/sunruiqi/Desktop/tensorflow-2.13.0/tensorflow/BUILD:1134:21: declared output 'tensorflow/libtensorflow_framework.2.dylib' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely)
ERROR: /Users/sunruiqi/Desktop/tensorflow-2.13.0/tensorflow/BUILD:1134:21: Executing genrule //tensorflow:libtensorflow_framework.2.dylib_sym failed: not all outputs were created or valid
realpath: illegal option -- -
usage: realpath [-q] [path ...]
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 438.472s, Critical Path: 37.63s
INFO: 5437 processes: 1278 internal, 4159 local.
FAILED: Build did NOT complete successfully
```
",False,"[-8.68081152e-02 -4.51321244e-01  2.47677937e-01  8.15134197e-02
  2.14225382e-01 -3.69473308e-01 -2.88528830e-01 -1.55113906e-01
 -3.49000394e-01 -3.45696270e-01  1.32999837e-01 -2.68061578e-01
 -3.58161569e-01  1.38917848e-01 -3.15781385e-01  4.50560421e-01
 -4.22313899e-01 -1.74013391e-01  2.29093224e-01  2.43799329e-01
 -9.42666456e-02 -1.70193940e-01 -1.75741643e-01  1.50011256e-01
  1.40608519e-01  2.44468510e-01 -1.28712684e-01  6.31650016e-02
  1.92265771e-03  2.28897333e-01  3.54102135e-01  3.01314313e-02
  1.14106946e-02  1.39166266e-01  1.60457209e-01  2.54385591e-01
 -6.63390532e-02 -2.72368073e-01 -3.44486237e-01  8.66611954e-03
 -5.12643754e-02  1.83797821e-01  2.66648054e-01  7.11751878e-02
 -7.06210732e-02 -8.58342797e-02  1.37867332e-01 -3.83594364e-01
 -1.02388829e-01 -2.90955007e-01 -3.56729254e-02 -9.86845046e-02
 -1.30297363e-01 -4.50421125e-01 -1.56605840e-01  1.73471361e-01
  4.20968831e-02  2.57166982e-01  3.05514783e-01  3.68693709e-01
  1.65249497e-01  1.84246507e-02  6.63879067e-02 -1.21001303e-01
  1.42277017e-01  2.61263669e-01  3.37783024e-02 -1.35980889e-01
  3.22714508e-01 -3.68002295e-01  2.89134949e-01 -3.71019728e-02
 -2.77049780e-01  2.22039759e-01  1.32480383e-01  3.58886600e-01
  5.46446145e-02  8.23044628e-02  1.15183599e-01 -1.76267594e-01
 -2.94328600e-01 -9.92669724e-04 -1.31288290e-01  1.18446894e-01
  2.79882371e-01 -2.48953313e-01  1.11739993e-01 -8.74526426e-03
  2.45549113e-01 -1.18453451e-01  4.72269535e-01  1.70962438e-01
 -1.12772426e-02 -1.31605878e-01  5.47173023e-01 -2.95427609e-02
  7.40845054e-02  2.63238817e-01  1.03294939e-01 -1.39583305e-01
 -7.10798874e-02 -1.98906176e-02 -7.21710250e-02  7.39584267e-02
 -5.80653884e-02 -4.90180433e-01  2.09217846e-01 -1.36689305e-01
  7.99405202e-02 -3.53432074e-02  2.26892471e-01  2.39014328e-02
  2.44028419e-01  7.36553408e-03 -1.23210251e-04 -1.66717947e-01
 -3.13805640e-01  1.05912440e-01  1.26693785e-01  8.02350938e-01
  5.22960052e-02 -2.94599473e-01 -7.34559596e-02  1.37076620e-02
  2.67886043e-01  6.19649934e-03 -3.68099630e-01 -8.41956213e-03
  3.41938324e-02  3.61082070e-02 -2.73320526e-02  1.10853299e-01
  3.08651537e-01 -1.28691614e-01  7.55241737e-02  3.91051546e-02
 -1.87909558e-01 -3.46663207e-01 -2.43641198e-01 -1.91735014e-01
 -2.57240325e-01  2.89064765e-01 -3.88399065e-02 -5.72467148e-01
  1.89055622e-01 -9.92164537e-02 -2.71506995e-01  2.65997231e-01
 -5.22320867e-02 -2.68556118e-01 -1.65877312e-01 -1.20702863e-01
  8.83354619e-02  1.95335686e-01  3.35169435e-01  1.19794063e-01
  3.61996293e-01 -9.06115770e-02 -3.04744601e-01 -4.09081042e-01
  1.46470085e-01  4.22479510e-01 -7.90673643e-02 -1.13490120e-01
  1.29187524e-01  1.34382173e-01 -3.27874124e-01 -6.58721328e-02
  1.27480417e-01  5.24784446e-01  1.42880797e-01 -1.75681919e-01
  1.29857242e-01  1.67894498e-01  8.75455514e-02 -2.89821804e-01
  4.86664981e-01 -5.81610084e-01  1.02696039e-01  1.14899293e-01
  2.95532942e-01 -4.72064354e-02  2.79248774e-01  1.86673313e-01
  5.54351322e-02  2.99779419e-02 -5.79371601e-02  4.58858252e-01
 -1.87184453e-01  2.28552669e-01 -4.29070950e-01  1.50831640e-01
  2.68319368e-01  3.63275222e-02 -1.67839974e-01  1.13226935e-01
 -1.85498390e-02 -8.37202892e-02 -9.96383652e-02  1.62510693e-01
 -6.27167821e-02 -1.56387970e-01 -2.66496539e-01  7.92373419e-02
 -1.54335722e-01 -6.18733227e-01 -2.29852647e-01 -3.01808000e-01
 -6.09944999e-01  3.48013714e-02 -3.00431959e-02 -4.84572083e-01
  1.10122807e-01  1.24424212e-02 -1.21270113e-01  1.56149238e-01
  2.54685760e-01  8.01003352e-02 -1.90314472e-01  1.91203132e-02
 -1.57299608e-01 -4.55179036e-01 -4.02021706e-01 -2.43127912e-01
 -2.24484846e-01 -9.29002687e-02 -2.90595740e-01 -1.08887002e-01
  2.95385718e-03  3.41093272e-01 -7.74708837e-02  2.02877074e-03
  3.35570753e-01 -1.14712164e-01  4.19526070e-01 -2.01290622e-01
 -3.18649001e-02 -3.49480987e-01 -3.70226324e-01  1.37279779e-01
 -4.87488538e-01  1.32405668e-01 -2.65503824e-02 -4.24845368e-02
  3.95014882e-01  2.69001245e-01  2.66094878e-02 -1.15423247e-01
 -2.49749199e-01  1.40982956e-01 -1.46531403e-01  4.19299603e-01
  3.06078970e-01  6.02567643e-02  2.32579589e-01  1.19590692e-01
  2.08406001e-01  1.62008852e-01  1.29422277e-01 -5.92241138e-02
  2.82552570e-01  1.57552764e-01  2.83698022e-01  3.82934153e-01
  6.81476295e-02  2.95815796e-01 -3.76962483e-01  3.87686074e-01
  7.63644874e-02  5.29753864e-02  1.75272137e-01 -1.25770420e-01
  7.33216763e-01 -4.76645827e-01 -1.13057218e-01  6.75928146e-02
  2.14103252e-01 -2.05412470e-02 -2.04109922e-01 -8.18755180e-02
  1.96816623e-02  5.39256215e-01 -4.69244838e-01 -4.10935022e-02
 -1.53951094e-01 -2.82484829e-01  1.14571370e-01 -7.65033424e-01
 -2.76340485e-01  8.19387883e-02 -2.50672936e-01  1.76790997e-01
 -2.24801868e-01  8.96607712e-03 -2.80332834e-01  2.22123995e-01
  1.62434280e-02 -3.37444618e-02 -5.70380837e-02  1.32403612e-01
 -3.74053195e-02  9.89840478e-02  2.80060709e-01 -3.61054510e-01
  9.16412398e-02  1.28953993e-01  2.67618388e-01  2.23321468e-02
  6.19461119e-01 -4.14922237e-01  2.59080380e-01 -5.51651046e-03
 -1.95273012e-01  5.50853193e-01  9.40153301e-02  6.35070503e-02
 -3.66784751e-01  6.48464441e-01  1.59978271e-01 -1.11031272e-01
  1.81621477e-01 -1.52295545e-01 -4.42260623e-01 -3.54062766e-02
  4.59865153e-01 -1.01331413e-01 -7.79739022e-02 -3.35282564e-01
  1.33956552e-01  1.20011754e-01 -5.29942475e-02 -8.58280715e-03
 -3.62298906e-01  1.81329519e-01 -2.41071329e-01 -8.10944587e-02
 -1.77237898e-01  2.96356261e-01 -5.08307591e-02 -2.87180841e-01
 -1.33485988e-01 -4.31139246e-02 -1.33724421e-01 -1.53693363e-01
  7.96047598e-02 -1.41736180e-01  2.12700039e-01  3.08662027e-01
 -3.96964937e-01  2.24563554e-01 -1.17492631e-01  2.52880007e-01
 -4.70966876e-01 -1.07667893e-01 -2.09462926e-01  3.12719464e-01
 -6.42753169e-02 -6.49502724e-02  3.75310838e-01  3.39259595e-01
 -1.36941791e-01  4.29424942e-02 -5.08293390e-01 -1.90542042e-02
  2.09004015e-01 -1.20451879e-02 -4.90267217e-01  9.93518010e-02
  1.35604382e-01  5.07426500e-01 -3.74297798e-02  1.91001981e-01
 -1.83017105e-01  1.66521296e-01  5.45081496e-01 -2.27338523e-01
 -3.37449729e-01  8.71632844e-02  1.48398757e-01  5.93186915e-02
  2.34643757e-01 -2.69045252e-02  3.08099300e-01 -2.49729529e-02]"
`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function. stale comp:data comp:tf.function,"The first is the map function:
    def map_function(example):

        feature_map = {""wav_raw"": tf.io.FixedLenFeature([], tf.string)}
        parsed_example = tf.io.parse_single_example(example, features=feature_map)

        wav_slice = tf.io.decode_raw(parsed_example[""wav_raw""], out_type=tf.float64)
        wav_slice = tf.cast(wav_slice, tf.float32) / 2 ** 15

        return wav_slice

The second one is the training process:

     for epoch in range(args.num_epochs):
              
        trainset = tf.data.TFRecordDataset(args.trainset_tfrecords_path)
        trainset = trainset.map(map_func=map_function,
                                num_parallel_calls=num_cpus)  # num_parallel_calls should be number of cpu cores

        #trainset = trainset.shuffle(buffer_size=args.batch_size * 200, reshuffle_each_iteration=True)
        trainset = trainset.batch(batch_size=args.batch_size)
        trainset = trainset.prefetch(buffer_size=args.batch_size)


        # train_loss for each epoch
        train_loss_epoch = []
        train_loss = 0.0

        # record the train time for each epoch
        start = time.time()
        # MASK

        #EMA_MODELmaskindex

        binary_mask = RandomMaskingGenerator(input_size,frame_length,mask_ratio)
        # bmr:01
        # bm_T:1,0
        # bm,bm_T = binary_mask()
        

        for step, _input in enumerate(trainset):
            bm, bm_T, _ = binary_mask.random_mask(_input.shape[0],alpha_e_max)#.totally_random_mask(_input.shape[0])
            print(""_input"",_input)
            # print(""bm_shape"", bm.shape)
            # print(""bm_T_shape"", bm_T.shape)
            loss_value = train_step(_input,_input*bm,bm_T)
            loss_float = float(loss_value)

            train_loss_epoch.append(loss_float)

            # Calculate the accumulated train loss value
            train_loss += loss_float



        # average train loss for each epoch
        train_loss /= (step + 1)
        train_loss_all.append(train_loss)

        # print log
        log = ""train epoch {}/{}, train_loss = {:.06f}, time = {:.06f}""
The third one is the train_step function:

    @tf.function
    def train_step(_input,_input_mask,bm_T):
        with tf.GradientTape() as tape:
            enc_output, batch_mean, batch_var = sem_enc(_input_mask)
            #semantic decoder
            print(""main:"",enc_output)
            _output = sem_dec([enc_output, batch_mean, batch_var])
            loss_value = mse_loss(tf.multiply(_input, bm_T), _output)
            tf.print(loss_value)
            loss_whole = loss_value

        grads = tape.gradient(loss_whole, weights_all)  # compute gradients
        optimizer.apply_gradients(zip(grads, weights_all))  # update parameters

        return loss_whole



The _ouput generated by sem_dec() is the value I wanted change it from kerasTensor to TF.tensor; I could send you the whole codes, if you need, this is my email: limingxiao@link.cuhk.edu.cn. Thank you for your reply!

_Originally posted by @lmx666-gif in https://github.com/tensorflow/tensorflow/issues/61307#issuecomment-1643164557_
            ",False,"[-3.53098273e-01 -1.31496370e-01 -1.02355361e-01 -2.37206072e-01
  7.81951398e-02 -7.99803808e-02  2.56128311e-01  1.10638719e-02
 -6.38242841e-01  1.14982463e-02 -1.16448617e-03 -4.08160180e-01
  2.40244642e-01  2.10123092e-01 -1.09462760e-01  1.81214899e-01
 -1.37185436e-02  4.36118469e-02  2.73911178e-01 -1.60173014e-01
  1.60805374e-01  9.35618579e-02 -1.54726565e-01  1.58299312e-01
  1.11285642e-01  4.04393151e-02  1.11329585e-01  1.34793833e-01
 -8.74214172e-02  1.16543971e-01  6.81733564e-02 -1.14166461e-01
 -3.26318443e-01  2.03984231e-01  6.26355112e-02  3.18441033e-01
 -3.48316461e-01 -7.39916861e-02 -6.91799819e-02 -1.36427552e-01
 -9.07259807e-03  5.96809611e-02  1.25033379e-01 -6.01582043e-03
 -1.07071355e-01 -9.28793997e-02  8.96068662e-02  2.39936724e-01
 -4.59748983e-01  4.26877104e-02  3.45291570e-04  1.76888078e-01
 -5.65295577e-01 -3.98613185e-01  2.09013879e-01  8.25833827e-02
 -3.07890493e-02  2.17821926e-01 -1.10027470e-01 -3.02403212e-01
 -9.08829924e-03 -1.40638519e-02  1.23962283e-01  1.22664697e-01
  1.30466849e-01  2.45094717e-01  6.52514324e-02  8.19166191e-04
  5.77328086e-01 -1.72283337e-01 -6.49484769e-02  1.02475420e-01
 -1.66522950e-01  6.04571402e-02 -1.75383478e-01  8.18270743e-02
 -2.31855959e-01  1.85767114e-01 -1.56020671e-01 -3.35947461e-02
 -3.10457498e-02 -2.60844082e-03  5.57321981e-02 -4.50024158e-02
  3.63347679e-03 -3.33619684e-01  2.94916034e-01  1.71219464e-02
  3.32399189e-01  1.70194894e-01  5.28541267e-01  2.52046049e-01
 -1.88569665e-01  8.28559995e-02  1.48026332e-01  6.41052723e-02
  8.77818614e-02  3.05215746e-01 -5.23808450e-02 -5.34063727e-02
 -2.27204680e-01 -1.21588305e-01 -8.69329274e-02  1.81878686e-01
  3.65864664e-01 -1.23152748e-01 -2.73954868e-01 -1.48881733e-01
  1.92265347e-01  1.17865928e-01 -5.18228821e-02  1.29239410e-01
  8.36802945e-02 -1.83353037e-01 -9.38582271e-02 -2.67877541e-02
  5.02292328e-02  1.70776486e-01  6.71934187e-02  2.99491528e-02
  8.32080320e-02 -3.39757204e-01  1.12685926e-01  1.62053019e-01
  1.65079147e-01  2.41975054e-01 -1.70743734e-01 -1.90682873e-01
  1.40336931e-01 -4.48902138e-02  5.93224466e-02  5.05122915e-02
 -1.74151748e-01 -4.06461023e-02  7.36045539e-02 -1.41967699e-01
 -4.94990051e-01  6.09086677e-02 -3.83200884e-01 -1.84860583e-02
 -2.13498577e-01 -1.76216021e-01 -1.31969109e-01 -2.40821928e-01
  2.97841318e-02  2.87459850e-01 -3.45955610e-01  1.99668944e-01
 -2.99151808e-01 -8.16702247e-02  1.51220590e-01  7.15367198e-02
 -5.32331429e-02  4.21881750e-02 -6.11734353e-02  2.73972631e-01
  3.62328887e-01 -2.35715330e-01  1.67605355e-01 -2.88840353e-01
 -8.16295668e-02  4.08424854e-01 -6.88347444e-02 -1.41944617e-01
  1.71267331e-01  1.39545575e-01 -2.12027758e-01  1.35444745e-01
  1.57341272e-01  5.05442433e-02  9.47632343e-02  5.51464735e-03
 -7.03552067e-02 -6.85168952e-02  2.77162820e-01  1.01038396e-01
 -5.03359400e-02 -4.67285961e-01  2.18694091e-01 -5.04807085e-02
  1.28257126e-01 -4.53927405e-02 -2.05679461e-01 -3.48620862e-02
 -6.68002665e-02 -1.91391587e-01  2.85713315e-01  2.17651963e-01
 -7.30140880e-02 -1.86818063e-01 -5.49051128e-02 -3.33216816e-01
  4.14389670e-01  2.25835629e-02  2.11353488e-02 -2.36110002e-01
  3.66979063e-01  1.36705160e-01  2.55176961e-01  2.04883888e-01
  1.78870976e-01 -3.25034857e-01 -7.78835863e-02 -1.02828071e-03
  1.96858525e-01 -9.80043709e-02 -2.53229558e-01 -3.76216412e-01
  8.64840485e-03  1.10525280e-01 -4.11746353e-02 -2.21928116e-02
 -1.07133143e-01 -2.94649839e-01 -1.29348814e-01 -6.55502081e-02
  4.67864051e-02 -1.36489093e-01 -2.75893271e-01  5.12819365e-02
  8.27081650e-02 -2.76194423e-01  1.66685551e-01 -2.74997652e-01
  1.02324136e-01 -1.44836441e-01 -1.18277997e-01  1.84015408e-01
  1.19888313e-01 -2.04132736e-01  1.10568702e-02 -2.44229376e-01
  5.05296290e-01  8.11434984e-02  4.02624756e-02 -2.14590698e-01
  2.77511235e-02 -3.92206967e-01 -7.76501521e-02  1.47018969e-01
 -1.63600415e-01 -9.75191891e-02 -1.26531154e-01 -2.35032946e-01
 -6.97126463e-02 -1.54727310e-01 -1.52155131e-01  2.07389332e-02
 -2.24867105e-01  1.13938354e-01 -1.39512643e-01 -1.74790621e-01
  1.09548077e-01  1.22608863e-01  1.14769742e-01  1.48386925e-01
 -6.46924376e-02 -7.50359744e-02  1.23998523e-03  2.23560631e-01
  1.34876072e-01  3.10711235e-01  1.74267665e-01  7.84881711e-01
  1.65059149e-01  2.20595330e-01 -1.37007833e-01  1.71673521e-01
  1.42677605e-01 -1.69914335e-01  6.88667893e-02 -8.65391567e-02
  1.72134772e-01 -1.92342758e-01  4.04590219e-01  1.90962553e-02
  2.78089464e-01 -2.57163376e-01 -2.06000451e-02  2.32758716e-01
 -1.56672701e-01  2.37759322e-01 -1.88842177e-01  1.46991283e-01
 -7.71824121e-02 -1.53767169e-01 -1.44226924e-01 -2.08430871e-01
 -3.58334810e-01  1.61160842e-01 -3.15663904e-01 -1.99060425e-01
  9.10890326e-02 -1.61695600e-01 -1.46006733e-01 -2.92610526e-02
 -5.58833033e-03 -7.57932514e-02  8.59853029e-02 -2.25103647e-01
 -3.66209894e-02  3.76037419e-01  2.22095370e-01  1.22887269e-01
  1.93677485e-01  4.14067060e-02  3.42853129e-01  1.94246881e-02
  3.33177030e-01 -1.40988559e-01 -8.44056085e-02  2.19343722e-01
 -1.41809791e-01  3.67912620e-01 -8.87061059e-02 -4.57998030e-02
 -7.29339123e-02  5.26215553e-01  9.87493247e-02  1.26047432e-01
 -1.78687751e-01 -4.72059965e-01 -4.25410926e-01 -3.64402458e-02
  3.39603305e-01  3.95251572e-01  6.35297894e-02 -1.00658067e-01
  4.42521200e-02 -1.66203916e-01 -7.20315203e-02 -5.75609729e-02
 -2.85825491e-01  9.01712105e-03 -1.76789254e-01 -1.71091765e-01
 -1.78772971e-01  1.96845487e-01 -4.02304605e-02  1.29373502e-02
 -9.89875644e-02 -3.08284163e-01 -2.42454588e-01 -4.17280257e-01
  6.98764697e-02 -1.83545321e-01  3.41070704e-02  4.29335803e-01
  1.28999669e-02  7.70442635e-02  1.07979082e-01 -4.10076305e-02
  5.55432439e-02 -3.46660167e-02  4.57978129e-01  3.02125722e-01
  1.30027801e-01 -2.33771689e-02 -9.98658687e-02  4.28668886e-01
 -2.35259742e-01  1.36674970e-01 -2.00961828e-01 -9.11645666e-02
 -2.60401070e-01 -9.37398523e-02 -1.42435968e-01  7.24711344e-02
  3.08879495e-01  2.63064086e-01 -1.41657293e-01  1.94355756e-01
 -2.53994823e-01  1.57010704e-01  1.43825203e-01 -2.29553841e-02
  2.04920813e-01  1.58148974e-01  4.05845910e-01  1.22438163e-01
 -2.26337984e-01  1.03777349e-01  3.48199397e-01 -4.89091314e-02]"
How to get raw buffer pointer from python tf.Tensor stat:awaiting response type:feature stale comp:core TF 2.13,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In C++ api, tensorflow::Tensor has `data()` method which returns pointer to memory array. While in python api tf.Tensor does not allow to get raw data pointer. Is there any solution or workaround for this?

### Standalone code to reproduce the issue

```shell
tensorflow::Tensor.data()
tf.Tensor
```


### Relevant log output

_No response_",False,"[-4.81889397e-01 -4.28780884e-01 -3.87314886e-01  1.33675873e-01
  1.71373323e-01 -4.04373646e-01 -1.42263412e-01  1.57594621e-01
 -4.30309117e-01 -3.21610242e-01  1.07871726e-01 -3.79066706e-01
 -2.62252450e-01  1.11213818e-01 -1.29215956e-01  1.88424140e-01
 -9.49933827e-02  3.65412049e-02  4.17333424e-01  1.66497827e-01
 -1.12552017e-01  7.10935667e-02 -3.13594162e-01  3.91478121e-01
  2.37745196e-01  5.18166274e-02 -1.13916278e-01 -2.38302335e-01
 -4.67076339e-02  1.68679237e-01  3.19418192e-01 -1.64559767e-01
 -1.41472042e-01  1.27454713e-01  1.00464471e-01  3.57877612e-01
 -2.36345947e-01 -1.81876048e-01 -2.66170144e-01 -8.03565532e-02
  1.94641650e-01  1.92399211e-02  1.01667307e-01 -1.96053341e-01
  1.86301053e-01 -6.28401637e-02  1.66999940e-02 -7.52871931e-02
 -6.14260659e-02 -3.21472168e-01  1.11807398e-02  1.15207985e-01
 -4.16137815e-01 -5.12485743e-01 -2.33856797e-01 -2.52429962e-01
  9.85371694e-02 -2.35050302e-02 -7.78634772e-02  1.64479017e-01
 -4.30839136e-03 -3.33287343e-02  9.55075473e-02 -2.31184110e-01
  3.76974583e-01  2.34572858e-01  3.36833477e-01  2.00395703e-01
  5.02326488e-01 -3.78907025e-01  2.04616543e-02 -9.87296999e-02
 -5.71754336e-01 -8.90978649e-02 -8.73622596e-02  1.75276190e-01
  9.13091749e-02  2.16332614e-01  1.98404133e-01 -2.76258767e-01
  7.40717575e-02 -2.11416855e-01 -2.34881774e-01 -2.95390904e-01
  1.41172007e-01  3.14408839e-02  6.04080677e-01  2.85229057e-01
  3.85099530e-01 -2.90459871e-01  6.01803482e-01  4.85476166e-01
 -9.74668413e-02  1.09892964e-01  4.08449709e-01  4.04353589e-02
  1.67173460e-01  1.66828781e-01  1.32502820e-02 -1.49128437e-01
 -3.86131927e-04 -2.61998892e-01  5.84147871e-02 -1.48068191e-02
 -1.73617750e-02 -6.67569935e-02  1.15866587e-01 -1.79037191e-02
  1.69092044e-01 -1.64117545e-01  1.52376518e-01  2.30070241e-02
  2.71753997e-01 -1.39709026e-01  1.56139238e-02  1.52639681e-02
 -9.75602567e-02  1.83254257e-02  1.82560518e-01  6.66151285e-01
  8.97307619e-02 -8.67304131e-02  1.01690017e-01  3.57858956e-01
  3.49270165e-01  4.85803485e-02 -1.79368518e-02  2.41024755e-02
  1.26923040e-01 -8.28705356e-03  3.06132466e-01  3.75305377e-02
 -3.27912629e-01  2.20653027e-01 -2.93586373e-01  5.52228317e-02
 -1.63568228e-01 -2.23385185e-01 -3.49953502e-01 -4.95827764e-01
 -4.15381007e-02  1.07620776e-01 -3.59344214e-01 -6.45571351e-01
  1.35410070e-01  1.40879959e-01 -1.26309156e-01  3.45404804e-01
 -4.88958836e-01  4.95245814e-01  4.95527461e-02  1.25889003e-01
 -2.07382768e-01  3.07920784e-01  7.03225657e-02  2.98430882e-02
  1.60184711e-01 -1.62992328e-01  6.10644221e-02 -5.99776745e-01
  4.81233969e-02  6.69871092e-01  9.66134388e-03 -1.26975209e-01
 -1.26355607e-02  2.50586510e-01 -4.53091919e-01 -3.01314473e-01
  2.36419708e-01  4.48511392e-01 -2.03498691e-01 -4.86453474e-02
  3.38550769e-02 -1.22346595e-01  1.41322210e-01 -1.19369701e-01
  1.08999416e-01 -5.07518291e-01  7.06798583e-02  2.70328641e-01
 -1.83581412e-01  2.26178795e-01 -1.93589225e-01 -9.19211935e-03
  1.08794861e-01  4.42440622e-02  6.12885840e-02  1.27697572e-01
 -2.27307737e-01  1.62911296e-01 -3.80285352e-01 -3.89000058e-01
  3.94972920e-01 -1.34706840e-01 -5.72538637e-02  8.93682092e-02
  1.36505648e-01  3.80576029e-03 -3.40160988e-02  1.17028035e-01
 -4.57212925e-02 -3.98522228e-01 -1.20349087e-01  1.71039253e-01
  7.83710182e-02 -3.65842164e-01 -1.18355937e-01 -3.46389741e-01
 -5.96295446e-02  8.45533013e-02  8.05798694e-02 -7.23948300e-01
 -6.06746003e-02 -8.05955827e-02 -3.49572957e-01  7.68649355e-02
  2.88992643e-01  1.55625790e-01 -1.32158786e-01  1.09363139e-01
  2.71263123e-01 -6.91628158e-02  2.26918042e-01 -4.16925132e-01
  4.03896272e-02  4.18054052e-02 -3.10383916e-01  2.35617995e-01
 -1.23733357e-01  1.30862474e-01  2.68759519e-01  2.53275633e-01
  1.51051506e-01  3.69594574e-01  4.60465550e-01 -3.20007920e-01
 -9.16932821e-02 -8.91052559e-02  2.94617750e-02  2.41542041e-01
 -4.55616057e-01 -1.62175447e-01 -1.46797895e-01 -5.96953519e-02
  3.03905785e-01  4.46661830e-01  1.01491541e-01 -1.19649144e-02
 -5.60706019e-01  3.02402377e-01 -1.85193479e-01  1.43878251e-01
  4.69093800e-01  6.28625751e-02  4.80847985e-01  2.54514694e-01
  9.28235948e-02  3.34610641e-01  1.34680450e-01 -3.22247356e-01
  4.23481286e-01  3.42217505e-01  7.08225965e-02  4.29512382e-01
  1.99357212e-01  3.01650465e-01 -4.09599125e-01  7.18699753e-01
  2.10749388e-01 -1.27561390e-01  1.32487580e-01 -3.75720084e-01
  4.52943861e-01 -3.71700585e-01  7.55985081e-02 -1.73928007e-01
  5.12100816e-01  1.00605614e-01  1.22289404e-01  3.28911424e-01
  1.99490398e-01  3.96202385e-01 -2.43209481e-01  1.24167576e-01
  8.36421177e-02 -2.87762672e-01 -1.49341881e-01 -7.00492978e-01
 -4.00259718e-02  1.32122114e-01 -3.81915808e-01  7.33974352e-02
 -5.41936643e-02 -5.48002496e-02 -7.13607892e-02  1.02403492e-01
  1.59512628e-02 -2.97411680e-01 -5.08489739e-03  2.49072149e-01
 -3.14680815e-01 -6.42170310e-02  1.91122085e-01 -2.64447391e-01
 -1.48020297e-01 -1.66568920e-01  5.06920099e-01  4.03052807e-01
  5.45326293e-01 -3.55934203e-01 -1.06403530e-02 -2.71336704e-01
 -4.98358682e-02  2.70445585e-01 -1.34483576e-01 -9.06942561e-02
 -2.97317147e-01  8.31285238e-01  3.05514991e-01 -1.08567804e-01
  2.02933460e-01 -3.52274179e-01 -4.21410382e-01  6.40637428e-02
  1.64855063e-01 -7.75328130e-02 -4.60002497e-02 -4.81896043e-01
  1.21107198e-01  2.76724577e-01 -2.76772559e-01 -2.16998130e-01
 -2.52869278e-01 -1.19974107e-01 -1.15436062e-01 -8.37340355e-02
 -4.46678162e-01  4.11779508e-02  3.01084705e-02 -3.60095531e-01
 -1.85451031e-01 -2.53989518e-01 -1.25337496e-01 -1.57537863e-01
  4.22470123e-02 -4.71604288e-01  4.97693717e-01  8.24676156e-01
 -9.85707790e-02 -1.46916240e-01  1.50915846e-01  1.40841026e-02
 -3.79662275e-01 -4.60706614e-02  5.93581274e-02  4.53506768e-01
  4.91562933e-02 -3.10006052e-01  3.93105388e-01  4.64640856e-01
 -2.74781495e-01  2.61074770e-02 -2.58437037e-01  3.68382297e-02
  2.29868859e-01 -3.04572642e-01 -1.86383545e-01 -2.10622817e-01
  1.81019694e-01  2.00352192e-01 -1.33621961e-01  3.41518641e-01
 -3.37679982e-01  5.04572511e-01  5.61833143e-01 -4.85686719e-01
 -2.50533670e-01  1.38266176e-01  2.71328956e-01 -1.05866626e-01
  4.80407253e-02 -6.88263625e-02  1.80393323e-01 -1.08697355e-01]"
Tesla v100 Tensorflow CUDA Support stat:awaiting response type:support stale comp:gpu TF 2.4,"### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.4

### Custom code

Yes

### OS platform and distribution

RHEL 7.9

### Mobile device

_No response_

### Python version

3.6

### Bazel version

_No response_

### GCC/compiler version

4.3

### CUDA/cuDNN version

11/8, 10.1/7.6

### GPU model and memory

Tesla V100 2GB Vram

### Current behavior?

Attempting to fetch value instead of handling error Internal: failed to get device attribute 13 for device 0: CUDA_ERROR_UNKNOWN: unknown error.

NVIDIA-SMI give the following output:| NVIDIA-SMI 450.51.05 Driver Version: 450.51.05 CUDA Version: 11.0

nvcc-V the following output:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Fri_Feb__8_19:08:17_PST_2019
Cuda compilation tools, release 10.1, V10.1.105


### Standalone code to reproduce the issue

```shell
Doesnt happen with Windows or Ubuntu systems.
```


### Relevant log output

_No response_",False,"[-0.58647954 -0.49093357 -0.25424412  0.22720107  0.17344068 -0.3407776
 -0.33658957  0.1431839  -0.27078483 -0.547963    0.15074879 -0.20278364
 -0.14890209 -0.01114679 -0.272975    0.13927089 -0.17766042  0.12931813
  0.11635888  0.16540577 -0.27891627 -0.08221403 -0.14959219  0.11912689
  0.33266255  0.17072296 -0.10799119 -0.15621674  0.02611517  0.01738555
  0.45441645  0.03304848  0.12504023  0.11411089  0.10799719  0.13864511
 -0.22645284 -0.25760835 -0.34690866 -0.12198003 -0.24052714 -0.0532749
  0.4676433   0.11802503  0.12311499 -0.09124293 -0.01848305 -0.08401859
 -0.09268586 -0.12444137 -0.04910321 -0.0041191  -0.21278329 -0.37680244
 -0.2538448  -0.09370224  0.1564869  -0.16082536 -0.12854101  0.34810135
  0.14101546  0.00303699  0.09940806 -0.24619293  0.1236843   0.17130603
  0.14851081 -0.07436382  0.62215304 -0.06564857  0.24772055 -0.0210278
 -0.19002658 -0.170086    0.00810573  0.05229354  0.06435929  0.25649
  0.27267933 -0.02303142  0.29740667 -0.17461836  0.03419073 -0.4602934
  0.21977739  0.10863985  0.32108426  0.18411615  0.36802685 -0.31410578
  0.173891    0.5502712   0.1950326   0.20166862  0.3247435   0.05377652
  0.17185721  0.08993967 -0.02455104 -0.34395644 -0.12053034 -0.05323122
 -0.18357863  0.27916417 -0.13734624 -0.34825855  0.20701939  0.18880826
 -0.24975649 -0.14070337  0.283547   -0.01124735  0.14444529 -0.20048591
  0.03805189  0.07567544  0.00186411 -0.22116981 -0.0186931   0.66639644
  0.04792458 -0.00437407 -0.06505527  0.12599924  0.46613616 -0.05003597
 -0.10398312  0.00383477 -0.0280186  -0.00884904  0.16488549  0.01829215
  0.21690473  0.36983454 -0.2805525  -0.01992946 -0.09320425 -0.18553185
 -0.20733455 -0.11704886 -0.33812982  0.22673365 -0.19828266 -0.49628723
  0.0587308   0.16957602 -0.17016059  0.33455646  0.1869194   0.09930263
  0.01103095  0.06344309 -0.07667667  0.49105918  0.31998676  0.20732334
  0.24609214 -0.11849859 -0.1117581  -0.6245953   0.01261234  0.30795652
 -0.0111317  -0.01628014 -0.05622088  0.32762122 -0.33406043 -0.12059533
 -0.04217041  0.5022601  -0.3871119  -0.42820656  0.20665798 -0.0106243
  0.1006236  -0.19209974 -0.0623977  -0.5438901  -0.12461676  0.36578584
 -0.03808981  0.06822803  0.31861582  0.01157389  0.25233236  0.34617105
 -0.01296715  0.21029685 -0.19797626 -0.07250219 -0.37092876 -0.21600515
 -0.01849184 -0.1125724  -0.00306896 -0.12149805  0.01808033  0.20244603
 -0.128997    0.15820993 -0.25799936 -0.04523681 -0.09296659  0.00388571
 -0.0228385  -0.26583207 -0.2288566  -0.35700572 -0.34139064  0.06131001
 -0.16970545 -0.4239554   0.22753775  0.00615731 -0.45236862  0.27951214
  0.00388957 -0.0107391  -0.21737745  0.11204273  0.3296293  -0.30271727
  0.17257962 -0.29327214 -0.09186155  0.2139973   0.00922062  0.30809623
 -0.07077659  0.14376381  0.20029278  0.10365328  0.37973917  0.2968667
  0.4647882  -0.10374909 -0.25587147 -0.06104465 -0.02303609  0.23188055
 -0.57847345 -0.1361646   0.00138941  0.15672603  0.45343608  0.40887022
 -0.06751315 -0.05126223 -0.468444   -0.01475662 -0.00443259 -0.21974078
  0.4957077  -0.17056353  0.19764371 -0.1353516   0.12858549  0.15846777
  0.16322407 -0.3496083   0.48640513  0.39394093  0.0182424   0.43315354
  0.26780075  0.4000464  -0.456733    0.5519134  -0.151153    0.03811513
  0.07795589 -0.33035576  0.66339636 -0.40515614  0.1649893  -0.13135204
  0.3438431   0.08895047 -0.09769065  0.13914964  0.10008828  0.2054023
 -0.28831774  0.13934454 -0.11195603 -0.51879704 -0.2341523  -0.444982
 -0.21739891 -0.1660847  -0.49738106  0.26374787 -0.01610341  0.07688235
 -0.11871435 -0.00492113  0.13906643 -0.0117863   0.1214323  -0.04215281
 -0.30043238 -0.06680352  0.36822805 -0.5720769  -0.42222196 -0.17667359
  0.3860882   0.3323633   0.61906207 -0.31665123  0.22374189 -0.21537635
 -0.02901115  0.42229655 -0.12429969  0.07396926 -0.20086846  0.44129825
  0.21720879 -0.00830287  0.05558421 -0.39586493 -0.3247186   0.04911542
  0.19179496 -0.03219398 -0.08963159 -0.3438386   0.05592816  0.6091125
  0.09077248  0.2734478  -0.07810027 -0.00611685 -0.30864054  0.06610199
 -0.15261897  0.1883861   0.12246974 -0.18872191 -0.37765896 -0.06576227
 -0.22218658 -0.24144602 -0.10335538 -0.2418401   0.33714217  0.6525228
 -0.05243421 -0.01379743 -0.13731748  0.15694743 -0.3017462  -0.1762114
 -0.20593068  0.50003594  0.44100368 -0.21625811  0.32540065  0.17308748
 -0.07972743  0.08403061 -0.39928874  0.06919657 -0.01554094 -0.31521314
 -0.1290582  -0.50363636  0.05336592  0.21508706 -0.26215994 -0.01260774
 -0.06419399  0.23788953  0.6328187  -0.3610726  -0.25493574  0.06574208
  0.21823259  0.09158105  0.23514724 -0.18527555  0.2631459  -0.00777285]"
LSTM support for quantisation aware training. stat:awaiting response stale TFLiteConverter,I wonder if quantization-aware training has the support for lstm? ,False,"[-1.91022009e-01 -1.76465824e-01 -8.18567425e-02 -2.69816350e-02
  2.63907790e-01  3.69694978e-01  1.69582829e-01  6.83304667e-02
 -5.13501823e-01 -6.97908178e-02  1.01314917e-01  3.71836573e-01
  3.40297222e-02  2.46919021e-01 -2.75273949e-01 -7.41728842e-02
  4.41857986e-02  9.35513824e-02 -1.49865508e-01 -2.50243813e-01
 -4.21428949e-01 -1.32563710e-01  2.40712747e-01  1.71085358e-01
  2.25800917e-01  1.19405156e-02 -1.08598977e-01 -3.38150471e-01
  8.40501934e-02  2.88061410e-01 -4.34357561e-02  1.19343281e-01
  1.99676275e-01 -6.10792004e-02 -3.76921535e-01  4.10170481e-02
  3.16389948e-01  1.13425732e-01 -1.21540323e-01  1.98972240e-01
  9.76095572e-02 -5.83037883e-02 -6.71328530e-02  1.71142206e-01
 -1.34730451e-02  1.65797174e-02 -4.81520146e-02  7.16583133e-02
 -4.30252343e-01  9.92053673e-02  8.23233798e-02 -2.00091690e-01
  5.56812398e-02 -1.33874610e-01 -6.55752346e-02  2.87760735e-01
  1.13663919e-01  3.06050450e-01 -1.31084576e-01  1.51846111e-01
  2.08286285e-01 -3.55746955e-01 -2.06015036e-01 -1.39675602e-01
 -4.37119231e-03  3.76438200e-01 -1.40251160e-01 -2.01852635e-01
  4.51926827e-01  1.55485079e-01 -1.00552432e-01 -2.33455487e-02
 -4.83475804e-01  1.93387181e-01  2.10954063e-02  4.72128466e-02
  9.36681330e-02  1.48949146e-01  6.23514593e-01 -3.10869694e-01
  1.19985349e-01 -1.55906439e-01  8.32946971e-02  2.66823769e-02
  1.20375305e-01 -8.07939470e-02 -2.10319221e-01  2.12378129e-02
 -4.07175161e-02  2.03959420e-02  1.28942624e-01 -1.15851961e-01
 -4.07840550e-01 -9.23785791e-02  2.54857123e-01  8.20353553e-02
 -9.47929025e-02  4.60656792e-01  4.66109850e-02  4.46333773e-02
 -5.07308692e-02 -4.26604062e-01 -3.17319036e-01  4.08460855e-01
 -3.06913018e-01 -4.90212142e-01 -6.55488595e-02 -5.15732132e-02
 -2.17625331e-02 -7.37875700e-02  3.00321013e-01  9.43297595e-02
  1.97331667e-01  5.09629175e-02 -1.21415444e-01  3.90947275e-02
  3.05730999e-01  2.36835763e-01  5.76185295e-03  1.22749306e-01
  1.50363579e-01 -1.05568200e-01 -4.42918167e-02 -9.10590068e-02
  2.91471332e-01 -2.82136410e-01 -3.53318453e-01  1.00387119e-01
  8.33921954e-02  6.12126350e-01  3.26558918e-01  1.06542982e-01
 -4.72457141e-01 -4.43732664e-02 -1.86286211e-01 -1.70204848e-01
  1.91118404e-01 -3.09235513e-01 -4.71357852e-01  4.64858443e-01
 -3.69747162e-01  1.01403303e-01 -7.32747018e-02 -2.14499593e-01
 -2.03723088e-01  1.79348335e-01 -2.14832485e-01 -1.19571753e-01
  8.56463239e-02 -1.36952400e-01 -2.55825281e-01 -6.20882958e-02
  1.21789627e-01 -3.56469513e-03  2.71079630e-01  1.53599814e-01
  2.65857503e-02 -8.12033750e-03 -3.14718932e-01 -1.50271520e-01
 -2.84505576e-01  2.74159700e-01  8.44021365e-02 -1.53372422e-01
  1.06560007e-01 -1.24612581e-02  2.44220573e-04 -2.04275012e-01
  2.22898826e-01  1.71951056e-01  2.20950935e-02 -1.45108551e-01
  9.03208405e-02 -1.98627263e-01  2.39150494e-01 -1.74639225e-01
  4.81307924e-01 -3.40299517e-01  1.07476793e-01 -1.92186654e-01
  9.79114398e-02 -3.65157485e-01  3.35229307e-01  1.86949238e-01
  4.94613536e-02  1.13992751e-01 -1.49726123e-01 -1.06613534e-02
 -2.53784180e-01 -1.63470283e-01  2.10006952e-01  3.53991836e-02
 -5.22458628e-02  3.27045202e-01 -1.50465086e-01 -6.23739958e-01
 -2.95995474e-02  1.90610196e-02  5.23416884e-02  9.29796472e-02
 -1.46865100e-01  1.28755629e-01 -5.26949018e-02 -6.94801435e-02
  1.93208143e-01  1.54645756e-01 -3.46556365e-01 -5.03296614e-01
 -7.36213028e-02 -4.61264312e-01 -8.48020613e-02  1.14381313e-01
  3.11650962e-01 -2.32031748e-01 -5.20841658e-01 -5.68522364e-02
 -1.34144068e-01  1.97559610e-01 -6.59324974e-02  1.19985826e-01
 -3.49270344e-01  3.53619844e-01 -2.27471858e-01 -2.07682028e-02
 -4.47726309e-01  3.61030884e-02 -7.33126178e-02  5.47065616e-01
 -1.01869352e-01  5.18352091e-01  6.27931133e-02  1.43161282e-01
  8.51768777e-02  5.48606962e-02  2.68008918e-01 -3.13006848e-01
 -1.62271529e-01  2.17738658e-01 -5.23318887e-01 -2.50337929e-01
 -2.43099302e-01 -2.51587480e-02 -3.40004951e-01 -4.49255072e-02
  6.85617179e-02 -5.15002869e-02  1.05260499e-01 -2.17828512e-01
  2.36073777e-01  3.64825368e-01 -2.13271216e-01 -1.55445132e-02
  1.71832681e-01  6.96970299e-02  9.34349000e-02 -9.92097408e-02
 -1.22322410e-01 -2.93639302e-01  4.72632330e-03 -2.68897891e-01
  5.45513965e-02  4.04876888e-01 -2.41460100e-01  3.28713059e-01
  2.90596098e-01 -2.08666734e-02 -2.30749369e-01  1.26519248e-01
 -1.12996802e-01 -9.88183692e-02  1.61656484e-01 -5.21642268e-01
 -5.77882715e-02  2.42380857e-01  6.38625100e-02 -5.90015501e-02
  4.20219511e-01 -5.02909496e-02 -4.02934104e-01  3.41789246e-01
  4.44483347e-02 -1.64663494e-01 -3.07887942e-01  1.65742829e-01
  2.27054968e-01 -1.08470768e-01  1.79897491e-02 -3.70887488e-01
 -1.27488106e-01 -1.17621541e-01 -1.63001120e-01  1.54078811e-01
  2.12964147e-01  1.18544646e-01  1.46661535e-01  1.02524109e-01
  5.20608604e-01 -2.50453919e-01  1.81999832e-01  4.54128943e-02
 -4.51988280e-01  2.68094335e-02 -1.70953035e-01  1.34228496e-02
  1.10770211e-01 -9.35287923e-02  1.53424889e-01 -1.28412813e-01
  2.01262936e-01  5.85208125e-02 -7.55853765e-03  2.99785644e-01
  3.14572483e-01  4.31572139e-01 -1.94423601e-01  1.45659402e-01
 -1.84259832e-01  1.37455491e-02 -2.52830654e-01  1.09708928e-01
  1.16490768e-02  3.85107212e-02 -6.28055707e-02 -1.96359128e-01
  1.12386398e-01  2.04379290e-01 -8.54751021e-02 -3.68173331e-01
 -9.48651209e-02  7.46358708e-02 -9.85931978e-02 -2.20795557e-01
 -8.01131502e-02 -8.05317760e-02  5.11270821e-01  1.52584687e-01
  9.44886059e-02  3.09442878e-01 -3.24531704e-01 -3.96652296e-02
 -2.99756124e-04  1.76328316e-01  2.95149349e-02  3.27071667e-01
  2.32674927e-01  3.19778323e-02 -2.00629845e-01  5.00495613e-01
  2.81717721e-03  4.20989513e-01 -1.17041834e-01  3.25659811e-01
  2.35576168e-01 -2.26174444e-01 -2.10060164e-01  1.61776498e-01
  1.07253090e-01  3.48107629e-02  1.30332755e-02  4.06177610e-01
 -4.42241251e-01  3.76645952e-01 -3.17592263e-01  5.45123100e-01
 -1.22663938e-01 -1.69171885e-01 -4.76970412e-02 -3.57092917e-01
 -2.34777480e-01  2.71214932e-01  6.04690500e-02  2.36040398e-01
 -2.13967323e-01  1.34583771e-01  1.59275979e-01  1.59133419e-01
  1.73363313e-01 -1.91758111e-01 -2.62268066e-01  1.57339692e-01
 -5.16523838e-01 -4.10285473e-01 -1.59454554e-01  7.89485127e-02]"
"How can I profile ""Inference"" by Profiler, and view performance profile by tensorboard comp:tensorboard stat:awaiting response type:support stale TF 2.9","### Issue type

Support

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.9.3

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I want to profiling ""Inference"" by Profiler. 
However, the test of profiling training is success. But, when I try to profiling inference, the profiling log generated is empty and there are no active dashboards for the current data set.
How can I find the tutorial about analyzing the performance of inference?


### Standalone code to reproduce the issue

```shell
saved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING])
infer = saved_model_loaded.signatures['serving_default']
batch_data = tf.constant(images_data)
options = tf.profiler.experimental.ProfilerOptions(host_tracer_level = 3, python_tracer_level = 1, device_tracer_level = 1)
tf.profiler.experimental.start('logdir', options)
pred_bbox = infer(batch_data)
tf.profiler.experimental.stop()
```


### Relevant log output

_No response_",False,"[-4.80823040e-01 -5.22633314e-01 -2.80623019e-01  2.85128281e-02
  4.37594116e-01 -2.89425313e-01 -3.38228792e-02 -4.05090377e-02
 -3.78230661e-01 -4.83768642e-01  2.10328922e-01 -1.27533928e-01
 -2.19198465e-01  8.41733292e-02 -3.18913221e-01  2.80315906e-01
  4.86848876e-04  1.45596378e-02  1.70261160e-01 -8.33063275e-02
 -2.75330842e-01 -1.37528405e-02  3.39837745e-02  3.49652827e-01
 -3.56420316e-02  1.56721830e-01 -1.96181104e-01 -1.36360213e-01
 -7.50835799e-03 -3.43593284e-02  4.08856660e-01  1.22153327e-01
 -1.41824573e-01 -4.30417508e-02 -3.53918701e-01  4.21514750e-01
 -3.16878438e-01 -1.12373091e-01 -2.80298352e-01  1.03270747e-02
  9.88672972e-02  1.61163598e-01  1.30734146e-01 -5.80347236e-03
  1.17264017e-02 -1.44282103e-01  6.09535128e-02 -1.96155608e-01
 -1.25090212e-01 -3.27334367e-02 -4.63836007e-02 -2.78332174e-01
 -4.83274668e-01 -3.56204391e-01 -1.14200756e-01  1.46899253e-01
  1.64035842e-01 -2.60877788e-01 -2.16917276e-01  4.27381843e-02
 -8.81118476e-02  7.61238411e-02  6.22259453e-02 -5.74426576e-02
  3.84836555e-01  4.00200963e-01  3.29913616e-01 -2.15766821e-02
  4.81235951e-01  3.28224041e-02  1.32214308e-01 -2.09065765e-01
 -6.72798574e-01  1.45780727e-01 -4.60592434e-02  1.51214033e-01
  2.61794865e-01  2.15968385e-01  4.47867483e-01 -2.32577264e-01
 -4.13034968e-02 -4.08496112e-01 -8.13790709e-02 -2.17736736e-01
  1.28774196e-01 -1.54742092e-01  4.29616094e-01  2.38304257e-01
  3.82468134e-01 -2.72375464e-01  5.74490428e-01  3.81396890e-01
 -2.09700778e-01 -3.60425860e-02  4.64156210e-01  2.65301824e-01
 -7.66147375e-02  2.96863377e-01  2.22662851e-01  3.78200561e-02
 -3.01529944e-01 -1.88002676e-01  1.34433016e-01  1.03916571e-01
 -2.35133424e-01 -2.09824085e-01  7.84356445e-02 -1.03289835e-01
  1.88744307e-01  2.81609427e-02 -6.81438111e-03 -8.37319866e-02
  1.88322991e-01  9.15675461e-02  1.92136139e-01 -2.11147338e-01
  8.68999809e-02  1.49238914e-01  2.02476963e-01  6.38064444e-01
  1.25309438e-01 -1.32818252e-01  3.26399267e-01  2.32583225e-01
  4.12473559e-01  1.21351033e-01 -2.44170576e-01  5.29141314e-02
  2.28568226e-01  8.25357735e-02  1.91151351e-01  1.05366938e-01
 -1.10520110e-01  2.21669301e-01 -2.57299960e-01  1.12964481e-01
  2.46489625e-02 -7.40653127e-02 -2.50608057e-01 -1.46246731e-01
 -2.56234318e-01  1.61660925e-01 -2.07557440e-01 -3.08246225e-01
  2.15996221e-01  1.92141712e-01 -3.50009322e-01  5.10730967e-02
 -3.49938393e-01  2.23131493e-01 -1.31123647e-01  1.91641059e-02
  1.78714201e-01  3.06072086e-01  3.43991101e-01  1.88380316e-01
  1.21377170e-01 -2.65951693e-01  5.42750992e-02 -6.17553115e-01
  1.06032565e-03  3.90415192e-01  3.12640667e-02 -1.74174473e-01
  3.73639576e-02  3.19857061e-01 -3.26785386e-01 -2.91020215e-01
  1.93284065e-01  4.88719106e-01 -2.85080895e-02 -3.17781538e-01
 -1.63580477e-01  2.18668252e-01  2.24095464e-01 -3.40538383e-01
  1.66627884e-01 -5.32620192e-01 -9.54909995e-03  3.02149635e-02
 -6.96969256e-02  4.28653136e-02 -3.02234907e-02  2.94242829e-01
 -1.60689190e-01  3.85564081e-02  1.02868378e-01  1.12883776e-01
 -3.73358101e-01  3.79267447e-02 -3.72185022e-01 -2.64119864e-01
  4.39548433e-01 -1.43460691e-01 -6.55966699e-02 -1.78422332e-02
  4.11144674e-01 -9.31947529e-02 -9.31666717e-02  1.32342875e-01
 -2.78041810e-02 -2.46779397e-01 -8.10756236e-02 -9.20270830e-02
  3.83750647e-02 -3.52109760e-01 -8.89495686e-02 -2.71220684e-01
 -2.41323650e-01  7.02843443e-03  2.27936149e-01 -5.53183496e-01
 -2.45877840e-02  8.19258243e-02 -2.02142984e-01 -3.32239121e-02
 -1.03737041e-01  2.53169715e-01 -1.45058289e-01  4.97944474e-01
  4.90781479e-02 -1.56000108e-01 -7.27438778e-02 -3.32423270e-01
 -4.20185149e-01  9.46503580e-02 -1.16034120e-01  8.94872919e-02
 -1.59084409e-01  3.27631712e-01  1.39114454e-01  4.39894199e-01
  3.08409154e-01  2.02906594e-01  2.97319591e-01 -1.99529380e-01
 -1.35908231e-01 -2.96895474e-01 -1.99879497e-01  2.20367089e-02
 -3.86429310e-01 -9.96839255e-02 -1.24238491e-01 -2.27716081e-02
  3.15302670e-01  5.48536420e-01 -1.39076740e-01 -1.05852306e-01
 -2.98246205e-01  2.91873872e-01 -1.84914798e-01  1.04795218e-01
  3.73037726e-01  7.63600320e-02  3.41685772e-01  2.76735574e-01
  1.27556771e-01  1.91798940e-01  2.71329284e-01 -8.61790925e-02
  4.51861471e-01  2.73411065e-01  5.11251017e-02  5.70800543e-01
  2.45243907e-01  3.61075193e-01 -5.00589848e-01  4.18283284e-01
  2.35303342e-02 -2.74954230e-01 -3.38284373e-02 -4.07073557e-01
  5.64926445e-01 -4.21504021e-01  8.94388109e-02  7.74741396e-02
  3.33882749e-01  7.30780363e-02 -8.47352594e-02  2.01032132e-01
  2.04772651e-01  6.58070520e-02 -2.68574774e-01  3.16578932e-02
 -1.49028480e-01 -2.17791408e-01 -1.77144706e-01 -6.00119591e-01
 -1.40255839e-01  5.08806333e-02 -2.64042735e-01  1.50680244e-01
 -1.42736778e-01 -6.67604059e-02 -1.30979821e-01 -5.76001499e-03
  7.71127939e-02  5.81869856e-02  1.12186611e-01  3.26469541e-02
 -1.16933852e-01 -2.94007063e-01  4.02720988e-01 -3.56262565e-01
 -2.04264715e-01 -1.42022192e-01  4.67608452e-01  1.97010070e-01
  2.97406912e-01 -2.99992979e-01  2.58440487e-02 -9.63297486e-02
  4.86556329e-02  2.79883087e-01  8.18692707e-03 -5.33580035e-03
 -5.25440633e-01  7.79347122e-01  8.25553685e-02  4.37864661e-03
  1.38698429e-01 -2.81749606e-01 -1.80780202e-01  1.92337602e-01
  1.77444488e-01 -2.18822397e-02  1.56860456e-01 -5.06605506e-01
 -1.93694085e-01  9.70616192e-02 -2.50931442e-01 -1.87340528e-01
 -2.18790740e-01  1.13968283e-01 -1.86620146e-01  2.23244168e-02
 -3.56724143e-01  2.65377223e-01 -3.74860726e-02 -4.08143580e-01
 -2.10106343e-01 -1.85277075e-01 -1.25292748e-01 -2.92776287e-01
  5.21342717e-02 -3.61091316e-01  3.69659722e-01  6.25842810e-01
 -9.42803323e-02  1.45467132e-01  1.55327655e-03  3.27273011e-01
 -3.43135655e-01 -2.10937098e-01  1.43078983e-01  3.86340499e-01
  2.48613283e-02 -2.50391483e-01  4.40987647e-01  5.87981701e-01
 -1.23363510e-01  7.86014087e-03 -9.07503814e-02  1.85399517e-01
  3.86612922e-01 -2.72771657e-01  4.44628000e-02 -5.42320728e-01
  2.00824961e-01  2.91798234e-01  9.14987773e-02  7.58842975e-02
 -1.01150244e-01  3.23865473e-01  2.67100245e-01 -4.89111722e-01
 -1.23071507e-01  1.08996201e-02  8.24867487e-02 -2.26015329e-01
  2.36633182e-01 -2.93107182e-01 -1.02982894e-01 -1.05022736e-01]"
Android GPU Delegate Error while using groups in Conv2d stat:awaiting response stale comp:lite TFLiteGpuDelegate,"**System information**
- Android 13:
- TensorFlow installed from binary:
 
**use GPU Delegate**

**use model**

```python
        self.g_a0 = tf.keras.layers.Conv2D(
            N,
            kernel_size=5,
            strides=2,
            padding=""same"",
            # data_format=""channels_first"",
        )
        self.g_a1_test1 = tf.keras.layers.Conv2D(
            N,
            3,
            padding=""same"",
            groups=N,
            # data_format=""channels_first"",
        )
```
**error on android**
```
java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Can not open OpenCL library on this device - undefined symbol: clGetCommandBufferInfoKHR
Falling back to OpenGL
TfLiteGpuDelegate Init: No shader implementation for split
TfLiteGpuDelegate Prepare: delegate is not initialized
Node number 2 (TfLiteGpuDelegateV2) failed to prepare.
Restored original execution plan after delegate application failure.
```


**use anothor model**

```python
        self.g_a0 = tf.keras.layers.Conv2D(
            N,
            kernel_size=5,
            strides=2,
            padding=""same"",
            # data_format=""channels_first"",
        )
        self.g_a1_test1 = tf.keras.layers.Conv2D(
            N,
            3,
            padding=""same"",
            # groups=N,
            # data_format=""channels_first"",
        )
```
No errors on Android",False,"[-0.11572415 -0.29605976 -0.4613989  -0.22721052 -0.15575056 -0.10243924
  0.13680921  0.01646488 -0.1217317   0.03173582  0.16598572 -0.3414011
  0.23296759  0.42479086 -0.1579892   0.13613269 -0.27050602 -0.07144691
  0.0860002   0.159581    0.07367134 -0.23502845 -0.2577485   0.14693314
  0.3589597   0.38113397 -0.02659035 -0.0918075   0.24022618  0.17482576
  0.23775911  0.10228556 -0.07386543  0.12704584 -0.06012552  0.0509064
 -0.46500725 -0.4565583  -0.20996647  0.22344813  0.20738298  0.24587299
  0.14971972  0.05257822 -0.3727975  -0.03594383 -0.14801702  0.20474856
 -0.24846663 -0.19960728  0.1071478   0.34204748 -0.49518707 -0.46906188
 -0.11097207  0.10425165  0.04107188  0.06092715  0.06354129 -0.04981665
  0.24261723  0.02457294 -0.14124003  0.02564849 -0.26658243  0.19527403
 -0.07460341 -0.03280563  0.5882705  -0.26477873  0.01604651  0.12383542
 -0.04653725 -0.15836053  0.2288787   0.22633162 -0.07368428  0.38990712
 -0.03297647 -0.48644286 -0.0054563   0.06333869  0.07070865  0.20831046
  0.31398875 -0.28924835  0.15500566  0.09649393  0.3291341   0.29800996
  0.38303402  0.5267948   0.0960118   0.12979291  0.00609396  0.30613646
  0.01957362  0.07968942 -0.08578137 -0.04223127  0.13125911  0.05339728
 -0.05104837  0.38224882  0.07449802 -0.35931838  0.0759769  -0.07139073
  0.13696747  0.25075585  0.20574717 -0.12722379  0.18725762  0.03623258
  0.12091257  0.30783293 -0.2894709   0.3446085  -0.17837167  0.02770011
 -0.12471668 -0.21513191  0.15348148 -0.02675147  0.33774233  0.05687918
 -0.22023565 -0.23830809  0.09278467 -0.04563168  0.3996837   0.22105052
 -0.36091214 -0.12667193 -0.1391103   0.22922094 -0.08112513 -0.27859712
 -0.4377276   0.03644757  0.04471002  0.03497672 -0.03379245 -0.14465103
  0.0751006   0.314658   -0.3099441   0.2839797   0.03880099  0.0629022
 -0.14373127 -0.05125695 -0.3755036   0.11911944  0.08747484 -0.05196349
  0.26976332  0.0537522  -0.04313589 -0.4354258  -0.05298052  0.17973001
 -0.10362807  0.13595279  0.45196092  0.19969493 -0.19519624 -0.17898361
  0.01105687  0.05727134  0.09563695 -0.12043554 -0.12381144  0.01692155
  0.1445676  -0.41236562 -0.19456436 -0.33442518 -0.00711113 -0.16608298
  0.19868782  0.22694483  0.005426   -0.13832763 -0.23619288  0.13172302
 -0.09068774 -0.0011131  -0.19553687  0.03692704 -0.32895494  0.07047546
 -0.04876993  0.10405165 -0.23958041 -0.30786425  0.50123036  0.16685581
  0.0943599   0.31009257 -0.13645642 -0.17973125 -0.00098693 -0.14175163
  0.28279495 -0.20431936 -0.54643977 -0.25007692 -0.23273459  0.1903202
  0.12633798 -0.36766842 -0.2281482  -0.1925344   0.06217614 -0.23669063
 -0.07657517  0.18386763 -0.42172536  0.06224935  0.19643441  0.0087916
  0.13849871 -0.35983846 -0.42351487 -0.04611512  0.13779564 -0.19274488
  0.0749041   0.19551131  0.0962559  -0.23770091  0.31033987  0.0085778
 -0.05338559 -0.12672132  0.38484716 -0.30449554  0.14098169 -0.35175285
 -0.03966803  0.01848971  0.08291899 -0.20606452  0.23026226  0.09744466
  0.07596085  0.15470552 -0.1555441   0.35609037 -0.00669891 -0.16754517
  0.42236388  0.00799636  0.274956   -0.10055075 -0.2013336  -0.26471424
  0.03642456 -0.08034842 -0.05283504  0.50751275 -0.29198915  0.51425606
  0.2486242  -0.0647948  -0.22632995  0.3687355   0.16068476 -0.38255018
  0.3450576  -0.28765044  0.21412198 -0.29912627 -0.1398668  -0.03059546
  0.2539547   0.06704444 -0.03284989  0.08112692  0.2262978   0.55046225
 -0.02000138 -0.20883183  0.2948468  -0.5728545  -0.15339583 -0.22578394
 -0.37351978 -0.15584466  0.04149921  0.1327922   0.31930548  0.23350692
 -0.10058224  0.03504715  0.17496012  0.14470828  0.4344242   0.08496127
 -0.30114004  0.15025876  0.06181543 -0.16785963 -0.21548882  0.02826865
  0.01218976  0.00999501  0.5575017  -0.3163568   0.67652977 -0.11903995
 -0.04255315  0.44206747 -0.20118326  0.15045437  0.04704905  0.2596419
  0.02537706  0.03915916  0.08138647 -0.24900377 -0.18597728  0.27310127
  0.10250506 -0.00889857 -0.21012476 -0.01847461 -0.07439992  0.16178653
  0.18593977 -0.12852807  0.06521688  0.13975553  0.06746508 -0.04743977
 -0.28672275  0.34755743 -0.04338942 -0.11356027 -0.29285142 -0.31428856
 -0.31097606 -0.21221069 -0.24755004 -0.15510431  0.2985098   0.47139105
  0.14057882  0.11424653 -0.03769172 -0.05129153 -0.10528313 -0.00907039
 -0.24448305  0.4397211   0.12269916 -0.12914383  0.32433444  0.38719147
 -0.21729542 -0.18908557 -0.3035962  -0.21909966 -0.10617834 -0.10254161
 -0.04213142 -0.02860003  0.18886563  0.34934077 -0.3427239   0.10073806
 -0.28991473 -0.2481935   0.3309414  -0.08798318  0.06372634 -0.12428008
  0.0149869  -0.05553702 -0.2498878   0.04318918 -0.19594246 -0.03669352]"
Converted(quantized) model of simple dense neural network returns same repeated output stat:awaiting response type:performance TFLiteConverter TF 2.7,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04.2 LTS
- TensorFlow installation (pip package or built from source):
```bash
export condaPip=`which -a pip | grep $condaEnvName`


$condaPip install tensorflow==2.7.0
$condaPip install numpy
$condaPip install pandas
$condaPip install scikit-learn
$condaPip install statsmodels 
$condaPip install matplotlib
$condaPip install future
$condaPip install onnx
$condaPip install torchviz
$condaPip install mpi
$condaPip install torch
$condaPip install tqdm
$condaPip install pydot
$condaPip install ipympl
$condaPip install seaborn
$condaPip install tabulate
$condaPip install xgboost
$condaPip install catboost
$condaPip install bokeh
```
Installed using conda
- TensorFlow library (version, if pip package or github SHA, if built from source):
Listed above

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
```python
    dnn_model = keras.Sequential([
        normalizer,
        layers.Dense(512, activation='relu', input_dim=x_train.shape[1]),
        layers.Dense(512, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
```
https://www.tensorflow.org/lite/performance/post_training_quantization
My model code is simple as above
And I used **Post Training Quantization using TF.Lite.Converter** (32-float to 8-int)
Integer only post training quantization (https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only)

(Input dim: 8)


### 3. Failure after conversion
#### Reset input shape
After the conversion, the model has been distorted and i solved the problem with resetting input format
```python
interpreter.resize_tensor_input(input_details['index'], (1,8))
```
And it works well but it returns same output all the time from different inputs
Like this,
```
[array([[76]], dtype=uint8), array([[76]], dtype=uint8), array([[76]], dtype=uint8), array([[76]], ...
```

I googled the cases same as mine, but some pointed out learning epochs, or Model structure.
Nothing was problematic with learning epochs and I suppose there are some model compatibility for quantization
I tried other tensorflow versions (2.13.0, or nightly: 2.14.0-dev20230706) but still same as before.

#### Quantization w/ floating value
https://www.tensorflow.org/lite/performance/post_training_quantization#integer_with_float_fallback_using_default_float_inputoutput
I tried quantization using float32 input/output, but it's same with float value
```
[array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), array([[0.36328125]], dtype=float32), ...
```

#### Quantized Model Weights
I printed out model weights, but not sure why it happens

I'm afriad the most of model weights are going 0 during conversion, and it became useless.
So the identical output is returned due to Model Bias.
(You can see the weights of quantized model below)

Do you have any idea why it happens or any tips ? 

### 5. (optional) Any other info / logs

#### The predicted output of original model (Expected)
```
[0.04892164 0.05425507 0.34756148 0.12509596 0.05663526 0.05041704
 0.01185179 0.33582878 0.0572255  0.07754967 0.20722428 0.208343
 0.48504433 0.10513872 0.05141798 0.03368005 0.09833255 0.2034252
 0.109099   0.48486084 0.23009542 0.08055633 0.06971437 0.14443058
 0.12913615 0.03327829 0.06535947 0.4671367  0.20984942 0.35980904
...
 0.23956934 0.23102319 0.10548833 0.06846321 0.16444126 0.04730341
 0.47125074 0.1931791  0.16075167 0.06618178 0.06408405 0.50926745
 0.12504464 0.73206306 0.22954968 0.00386357 0.19742006 0.09496576
 0.04637471 0.06708863 0.37460512 0.17932612 0.10157627 0.04863232
 0.04412654 0.50302374 0.06885636 0.21528003 0.09171575 0.0635072
 0.08805934 0.3634451  0.57593703 0.09678486]
```

#### Quantized model weights

<details> 
<summary> Click to watch </summary>

```
Quantized weights
[[  0 161   1 199 181 127   0   0]]
[[-128  127 -126 -126 -126 -127 -127 -127]]
[[ 127 -128 -128 -128 -128 -128 -128 -128]]
[[  80   23  -77 ...    0    7  115]
 [  74   55  -27 ...   70   56  103]
 [ 112  -89   43 ...  -18 -112  -91]
 ...
 [  45  104   -4 ...  -57   91   47]
 [  43   97  -94 ...   19  113  -43]
 [ 110   50  110 ...   34   18    3]]
[  57  -20  195  271  152  140  164  230  186  199 -129   -9  -25  204
  192  -11  183  -83  210  137  166   13  -21   36  248  243 -118  128
 -114  224  221   85  221  134 -142  -60  108  194  118  222  183   39
  201  104 -130   51    7  -39   16  246  224  103 -103  -43  -36  198
  156  -87  224  175   76 -105  -54   97  -69   62  -26 -169  238   63
   99  235   32 -130  -98  152  247  -56  145  191 -122  161   88 -119
   -2  -17  104  158  147  123  221  189  236  170  266  191  210  162
  -43  193  -94  208  -94   71 -130   97  -95  -29  181  244  183 -120
   72  -40 -106 -151   41  125  210  -92   84  -14  187    1  141  -94
   43  141  195  201   -7  227  178    8   67  243   97  229  200   38
  195  197  203   26  186   94  170 -138  210  209   49 -150 -146  202
  187  199  -91  196   95  216  222  203  213  237  167  -39  187  180
  183  -80  -28  170  -62  243   69   48  -57  178  241  213  247   42
  207  131 -111 -117  149 -114 -131  152  143  -36 -124  -81 -141  126
  259  127   89   -4  124  -63  229  192  191   -4  -79  -46   35  -99
  195   77  -62  185  194   30  -75  192  130   35  179 -155  -96   -7
  178   29  177 -105  197   39  237  176  171  201   24  199  -70 -149
  236  220  211 -165  165  248 -100  -71 -152 -123   77  183  159  -72
  186   37 -127  159  183  211  171 -120   38   -7  183  208  -28 -101
  174  176  234  204  182  165  238  145  232  159  256   50  -59   77
  -31  109   -3 -117   41   48  140  131  203  235  117  106   80  197
   32  166  141  183  203  -62   59  183  -36  -54  188   28  221 -108
 -136  -79  190  161  199  111   62  153  -12  139  205  247  233  154
  209  222 -130   27  209 -103  -83  240  130  210  184  138 -104   72
  173   28   14  206  -80  119  177  124  158  200  242  230   83  156
  242    7  190  210  -94  114  210  224  225  -42  232  -34  -67  -39
  230  165  199  224 -134  163  156   -7  136  134  176    2  209   73
   40  192 -109  216  227  201   -6  157  250   81  142   19  218  187
  -52   13  236   29  -24  243   90  135  143  174  189  218  146  239
  173  230  211  211  139  248  178  217  210  206  -86  136  -65  198
  201  218  238  192  223 -125 -178  203   16 -146 -162 -116  198  220
  232  179  105  -51  137 -111  -35 -154  266  -38  -88 -123  189  -95
  -98  237  159  -63  186   72   71  -25  182  168  239  216  171  168
  253  -33  223 -148  109  186  198 -157  229  198  210  100  155 -130
  112  210  111 -114   59  190  224   50  177 -119  144  -95 -110  182
  225  183   56  223  119  171  155  200  220  156  -73  178   44  200
  194   -6  -65  220   18   81  -20  -43]
[[  23   24  -26 ...   36  -24   37]
 [  48   40  -87 ...   44  -32  -21]
 [  -2   16  -38 ... -101  -28  104]
 ...
 [  64   89    2 ...  -37   95  -48]
 [  47  -14  107 ...  -85  -23 -106]
 [  12   31  -60 ...   63  -71  -85]]
[ 2110  2151 -1352  1999  1340  -833 -1006  -657 -1454  -641  2383  2034
  2358  -577 -1184  2455  2270  2831  -626 -1301  2957  2355  -665 -1340
  2138  -948  1411  -747  -690 -1268  2206  -575  2340  2376    70  2465
  -530  -668 -1109   951  2015  2042  -825  -976  2430  -682 -1268  -174
 -1176  1994  -440  -954  -727 -1087 -1017 -1290  -968  2100  1938  -783
  1915  -175  2582  -340  1964 -1330  2090  1963  -688 -1356   -22  1897
  2157  2447  1994  2207  -593 -1213 -1291  2120  2198 -1067  -918  2017
  1983  2399  2494  2201  1857 -1121  2406  2163   -48  -895 -1257 -1288
  2175  2481  2058  2170  3646  -714 -1171  1959  2715  2042  2605  -928
  -473  2099  2286  2298  1995  1968  2472  -356  -901   971  2610  -796
  2199  2135  1871  2212  1927  -972  2225  -975  -820  -932 -1070  2459
  -755  -721  1895  2021  -967  1987  -689 -1477  2076 -1138  2072 -1029
  -413  1856  -649  2216  2087  2194  1955  1996  2144  1895  3004  -780
  1739 -1404  -586 -1151  1895  -940 -1537  3030  -282   832  2043  1985
  2074  1936  2167 -1049  2218  -531 -1049  -843 -1189  -541  2679  -911
  2163  1518  -842 -1254  -966  1975 -1307  -983 -1360  -761  2085 -1137
  2005 -1255  2499  2207  -797 -1105  2200  1923  1895  -885  -633  1901
  1926 -1021  2195  1965 -1151  -952  -502 -1146  2204 -1306  2188  1859
 -1117  1888 -1144  1930  2082  -522 -1029   326  2102 -1007  2078  2298
  2038  1992 -1017  1646 -1298  1881 -1000  1916  2775  -558  2738 -1283
  -715 -1176 -1282  2161  2398  1922  -746 -1193  2281 -1019  2200  2021
  2053  1871 -1253  -621 -1009 -1205  2219  -925 -1342     0  2216  -885
 -1417 -1225 -1295  2147  -984  2133  1986  2767 -1077  1961  2159 -1142
 -1275   259  2182 -1005  -987 -1213  2103  2014  2043 -1087 -1509 -1196
 -1394  -303 -1050  -995  2462  2664  2005 -1157  -931  2737 -1013 -1521
  -751  -934  2335  2780 -1111  2000  1935  2166  2061  2373  2191  -682
 -1478  1882  -505  2093  1916 -1253  -304  2630  1891 -1145 -1167  1923
  1887  1956  2147  2966  -332  -270   336 -1002 -1138  -676  2056  -930
  -520  2277  1894  -970 -1346  -768  1858  2171  2364  2105  -795  2191
  -999  2027 -1141  -381 -1358  -732  1929  -469 -1206 -1283  1889  2446
 -1072 -1082  2227  2127 -1278  2183 -1148  1875  2052  2225 -1003  -741
 -1108 -1066 -1231  2373  1970  2116  2023  -234  1028  2289  1832 -1344
  -649 -1129  1993   505   625  -932 -1135 -1160  -355 -1130  2307 -1242
 -1087  2216  1848  -836  -150  2590  1926  2043  -896  -810  1922 -1031
  -869  2524  -476  -228    76 -1179  -469  2511  2012 -1650  1855  2989
  2400  2430  -155 -1171  -961 -1158  1823 -1036 -1244  2838  2554  2139
  1925  -972  -403   364 -1263  2226  2255  2024  2200  2205 -1223  1982
  1809  -921  2116  2142  -843  2195  -212 -1501  2415  -676  -607  2274
  2172  2157 -1212 -1000  2007  2753 -1065 -1048 -1231  1937  2695 -1240
  -953 -1246  -608  1932  1148 -1232  2039  2225 -1069  2009  2291  2003
  2003 -1369  2226 -1174  2264  -890  2128  1888  3043  -734  -719 -1385
 -1140  2348  2233  2103  2280  2138  2450  2377 -1008  2026  1958  1892
 -1075 -1108  1946 -1078  2522  1899  1416  1882]
[[ -67  -52   50  -61  -66   54  107   16   83   32  -22  -59  -30  113
    95  -30 -123  -93    1   55   -5  -24   28   58  -77   60  -67   22
    59   21  -99   15  -18  -33   60  -62  111   22   10  -51  -74  -54
    36    1  -27   16  103  108   59  -37   53   80    9  100   62   78
   112  -94 -100   93  -96   23  -87   19  -86   88  -91 -102   97   20
    32  -59  -88  -98  -11 -106   90   18    5  -41  -67   16    9  -54
   -38  -70 -119 -102  -89  110  -16  -75   88    5    5    8  -36  -55
  -108  -49  -12   31   45 -115  -70 -120  -71   39   43 -124  -44  -27
   -21  -62 -127   77  105  -52  -30   46  -79  -46  -83  -64  -28   91
   -80  116   99   52   99   -9   38  116  -87  -63   64  -93   71   82
  -108   70  -48  111   31 -109   40  -62  -45  -92 -115  -73 -112 -118
    -7   48  -11   27   85   86  -59   11   40  -31   25  -88  -83 -116
   -99  -39  -37   36  -12  112   65   79   60    1  -51   73  -34  -55
    28   53   98 -122   16   85   59  111  -60   40 -125   19  -78  -35
    55   12  -26  -87  -96   85   16 -100  -45   94 -109   -9   18    5
    61   64  -93   17   -5 -124  110  -41   77  -67  -74   69  105  -68
   -28   32  -20  -18  -81  -57   38  -84   14  -53   48 -105  -52   54
   -92   78   40   89   24  -49 -106  -90   33   34 -111   74   -9  -72
   -93  -87   55   49   10   23  -93   31    4  -60  -41   57   18   34
    93  -96    3  -79  -47  -21  114  -36  -13   97   66   72   -7   89
   103   61  -36 -120  -57  111    7   83   38   10   37   17  -96  -12
   -57   48    7  -75    8   31   16   34 -122  -12   62 -118  -71  -85
   -47 -121 -108   34   78  -86   79  -56  -90   60   73  -10  -31   30
    92  -99  -80 -119  -34  -55   63   81   69   19   78   70  -47  113
    16  -97  -61   16   26   66 -103 -110  -31  -43  108  -78   81 -123
    62   61   66   71 -113   34   29   57 -117  -22   93   21  -59 -118
   102  -81   35  -61  -98  -58   74   44   16   38   70  -36  -66  -49
   -34   18   -2  -48 -103   23   23   32  -35  -94  -68   64   24  114
    58   83  -30   22    6  -72 -116   58   44  -32  -31  -89   69   62
   -70   47   48 -106   66   73  117   16  108 -101 -114   97  -75  -15
   -27  -95   39   58   23   44  -74   77   79  -12   -8  -30  -65   76
    49  -98   68  -36  -31  -26 -120  -30   17 -100  -56   89  -43  -49
    59  -99   83   36  -37   10  100  -18 -119  -64  111   42  -96   -6
    30   87   20  -86  -48    5   92   13   96  -31  -36  112 -112 -112
    71  -81 -109 -101  -36   72  -37   54  -89   69  -55  -67  -14    7
    89  113   52  -24  -25  -51  -57  -13  -24  -73   91 -104  -49  -51
   112  110  -60   25  -46  -77  -72   -9]]
[-960]
[[   0 -110   -2   74  -75  127    0    0]]
[[  80 -116    4   33  -75  127    0    0]]
[[-64  -8  50   9   0   0   0   0]]
[[   0 -110   -2   74  -75  127    0    0   96   33   50    9    0    0
     0    0  105   98   47  112  121  116  104  111  110   51   46   56
    47  115  105  116  -80   -2   96    9    0    0    0    0  112   40
    55    9    0    0    0    0    0    0    0    0    0    0    0    0
    98   55   -4   74  -75  127    0    0   80 -116    4   33  -75  127
     0    0    0 -110  124   40   -2  127    0    0    0    0    0    0
     0    0    0    0  -30   27   -5   74  -75  127    0    0  -64   -8
    50    9    0    0    0    0  -64   -8   50    9    0    0    0    0
     0    0    0    0    0    0    0    0  -14   81   -4   74  -75  127
     0    0  -64  -87   58   45  -75  127    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0  -14   27
    -5   74  -75  127    0    0    0  -34  106   42  -75  127    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0  106   55   -4   74  -75  127    0    0  -80   35   19   41
   -75  127    0    0  120  126   -2   74  -75  127    0    0    0    0
     0    0    0    0    0    0  -22   27   -5   74  -75  127    0    0
   -32   73 -109    8    0    0    0    0    2    0    0    0    0    0
     0    0  116  102   46   69  110  113  117  101  117  101   84   80
    85   69  109   98  101  100  100  105  110  103   73  110  116  101
   103  101  114   66   97  116   99  104    0  103  101  115   47  107
  -127    3    0    0    0    0    0    0   48  -86   50    9    0    0
     0    0  -48   54   29    9    0    0    0    0    0    0    0    0
     0    0    0    0   97    3    0    0    0    0    0    0  -48   48
    50    9    0    0    0    0  -32 -100    1  -57  -75  127    0    0
     0    0    0    0    0    0    0    0   65    3    0    0    0    0
     0    0 -128   88   62    9    0    0    0    0    0   87   27    9
     0    0    0    0   16 -120 -109    8    0    0    0    0   88    1
     1   75  -75  127    0    0   80   50  -73    7    0    0    0    0
     2    0    0    0    3    0    0    0  120  126   -2   74  -75  127
     0    0  -32   49  -73    7    0    0    0    0    0 -110   -2   74
   -75  127    0    0    0   50  -73    7    0    0    0    0  109  101
    47   99   99   47   97  110   97   99  111  110  100   97   51   47
   -80   -2   96    9    0    0    0    0  112   40   55    9    0    0
     0    0    0    0    0    0    0    0    0    0   82   55   -4   74
   -75  127    0    0   80 -116    4   33  -75  127    0    0    0 -110
   124   40   -2  127    0    0    0    0    0    0    0    0    0    0
   -30   27   -5   74  -75  127    0    0]]
[[ -64   -8   50    9    0    0    0    0  -64   -8   50    9    0    0
     0    0    0    0    0    0    0    0    0    0  -22   81   -4   74
   -75  127    0    0  -64  -87   58   45  -75  127    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
   -14   27   -5   74  -75  127    0    0    0  -34  106   42  -75  127
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
     0    0    0    0   90   55   -4   74  -75  127    0    0   16   36
    19   41  -75  127    0    0  120  126   -2   74  -75  127    0    0
     0    0    0    0    0    0    0    0  -22   27   -5   74  -75  127
     0    0   40   74 -109    8    0    0    0    0    9    0    0    0
     0    0    0    0  116  102   46   69  110  113  117  101  117  101
    84   80   85   69  109   98  101  100  100  105  110  103   82   97
   103  103  101  100   84  101  110  115  111  114   66   97  116   99
   104    0  -31    1    0    0    0    0    0    0  -64   36   33    9
     0    0    0    0  -64   17   48    9    0    0    0    0    0    0
     0    0    0    0    0    0 -127    1    0    0    0    0    0    0
  -128   25   50    9    0    0    0    0  -32 -100    1  -57  -75  127
     0    0    0    0    0    0    0    0    0    0   97    1    0    0
     0    0    0    0   96    6   31    9    0    0    0    0  -80  -49
    86    9    0    0    0    0   16 -120 -109    8    0    0    0    0
    80    1    1   75  -75  127    0    0  -16   51  -73    7    0    0
     0    0    2    0    0    0    3    0    0    0  120  126   -2   74
   -75  127    0    0 -128   51  -73    7    0    0    0    0    0 -110
    -2   74  -75  127    0    0  -96   51  -73    7    0    0    0    0
    98   47  112  121  116  104  111  110   51   46   56   47  115  105
   116  101  -80   -2   96    9    0    0    0    0  112   40   55    9
     0    0    0    0    0    0    0    0    0    0    0    0   66   55
    -4   74  -75  127    0    0   80 -116    4   33  -75  127    0    0
     0 -110  124   40   -2  127    0    0    0    0    0    0    0    0
     0    0  -30   27   -5   74  -75  127    0    0  -64   -8   50    9
     0    0    0    0  -64   -8   50    9    0    0    0    0    0    0
     0    0    0    0    0    0  -30   81   -4   74  -75  127    0    0
   -64  -87   58   45  -75  127    0    0    0    0    0    0    0    0
     0    0    0    0    0    0    0    0    0    0  -14   27   -5   74
   -75  127    0    0    0  -34  106   42  -75  127    0    0    0    0
     0    0    0    0    0    0    0    0    0    0    0    0    0    0
    74   55   -4   74  -75  127    0    0]]
[[0]]
[[80]]
```

</details>


#### Debugger
- I used debugger
  - https://www.tensorflow.org/lite/performance/quantization_debugger

![image](https://github.com/tensorflow/tensorflow/assets/15901475/a67571a4-e017-4bcd-a6a0-80fdfbc8a62f)
",False,"[-4.01745468e-01 -5.97768664e-01 -2.29027346e-01 -1.92477368e-02
  8.70092362e-02 -1.46307230e-01 -6.09723814e-02 -3.31398696e-02
 -2.21061811e-01 -1.11474179e-01  3.19848955e-03  1.05460942e-01
 -1.35940194e-01  2.11921290e-01 -5.11578262e-01  9.98537689e-02
  1.50395706e-02 -2.61344612e-01  2.36932889e-01  3.88794066e-03
 -1.88249111e-01 -2.44681001e-01 -2.10953549e-01  2.47074425e-01
  3.83869946e-01  2.21408933e-01 -1.76747590e-01 -8.94882381e-02
  1.76672146e-01  4.92259152e-02  7.97797516e-02  7.73772970e-02
 -1.26675516e-01  3.71421501e-02 -1.86407551e-01  1.52768567e-01
 -1.29386961e-01 -3.58851030e-02 -2.75532544e-01 -8.55602995e-02
  1.19288191e-01 -6.88596815e-02 -7.58678317e-02  2.04006042e-02
 -3.30830552e-02 -6.21909946e-02  4.12323959e-02 -2.48098299e-02
 -1.40640140e-01 -1.68420076e-01 -1.22126266e-01  9.68486071e-02
 -1.32956192e-01 -2.17960417e-01  2.47792378e-02  3.42329293e-02
  2.63663769e-01  1.29764944e-01  9.31447372e-03  2.82515455e-02
  1.02294162e-01 -7.30387028e-03 -1.34952426e-01  1.60324872e-01
  2.64455557e-01  1.53859630e-01  8.80928412e-02 -1.59223437e-01
  2.79361010e-01 -2.34106660e-01 -4.74237241e-02 -1.45151556e-01
 -2.00838268e-01 -1.16488919e-01  8.18800703e-02  1.62761182e-01
  7.51164854e-02  3.13183665e-01  1.72970444e-01 -8.65433514e-02
  1.94174051e-01 -2.11051241e-01 -6.75216541e-02 -1.02620073e-01
  2.33360659e-02  8.90791714e-02  6.10809773e-02  1.08565139e-02
  2.54339248e-01 -2.75782883e-01  3.11595947e-01  3.60309541e-01
 -2.67200947e-01  2.11486705e-02  3.25389922e-01  1.82137549e-01
 -8.33973587e-02  2.57378101e-01  2.73116231e-01  3.84350531e-02
 -6.14105985e-02 -2.01511279e-01 -1.14100076e-01  4.90889922e-02
  1.91577822e-02 -1.48082048e-01  1.46714851e-01  5.40796928e-02
  1.96372960e-02  6.91970252e-03  1.72710016e-01 -4.93537541e-03
  2.13548720e-01  1.80166632e-01  2.66748726e-01 -6.52858168e-02
 -6.47263378e-02  1.38012648e-01  6.92840070e-02  5.80764174e-01
 -1.73775524e-01 -3.33114237e-01  1.42115682e-01 -1.97593719e-02
  3.87286186e-01  1.46092862e-01  1.37143955e-01 -1.48165703e-01
  1.30586565e-01  7.46538937e-02  1.68300457e-02  1.84050202e-01
 -2.25911230e-01  1.21775389e-01 -9.63554382e-02  4.49915826e-02
 -4.59155161e-03 -4.44723330e-02 -3.73813570e-01  6.59968406e-02
 -3.15724194e-01  3.04943621e-01 -1.27966776e-01 -2.39772364e-01
 -1.40052170e-01  5.60945719e-02 -1.92608818e-01  1.19590864e-01
  3.43205184e-02  2.31616169e-01 -2.55459070e-01 -2.55882949e-01
  1.65612474e-02  1.66508913e-01  2.94994652e-01  9.39480662e-02
  6.19371012e-02  4.14079577e-02 -1.63399987e-03 -3.44309330e-01
  3.00315991e-02  1.47312194e-01 -4.02719140e-01 -1.70564517e-01
  1.01540230e-01  2.42233664e-01 -3.74864936e-01 -1.48349702e-01
  7.93831795e-02  2.25710571e-01 -1.33711874e-01 -3.43508303e-01
  5.15668392e-02  1.27042100e-01  2.89570332e-01 -1.93694055e-01
  2.79315412e-01 -5.12040138e-01  2.87067089e-02  1.45848040e-02
  2.08056346e-01 -1.48168057e-02  1.41621470e-01  5.85558638e-02
 -8.89437832e-03 -4.70674522e-02  3.22373003e-01  2.80937552e-01
 -4.58096504e-01  8.79149958e-02 -3.32307726e-01 -5.95711246e-02
  3.15929979e-01 -1.84329078e-02 -1.53155044e-01  9.82204974e-02
  2.36178339e-01  2.25940704e-01 -4.60899174e-02  1.57486886e-01
 -1.68777198e-01 -2.68530428e-01  1.63497955e-01  2.27483362e-02
  9.68596041e-02 -1.33080572e-01 -2.33035877e-01 -2.15319350e-01
 -5.62344015e-01  9.42696109e-02 -4.89728199e-03 -1.82208359e-01
  1.50766373e-01  1.41269028e-01 -4.15147357e-02  9.35909301e-02
 -2.91605979e-01 -1.33138478e-01 -3.49595100e-01  2.18201727e-01
  1.09864548e-01 -1.52340997e-03  2.85259068e-01 -1.82454094e-01
 -2.81782538e-01  7.53471162e-03 -2.67833233e-01  1.15777567e-01
  1.22622512e-02  2.19819784e-01 -7.87534118e-02  1.31307691e-01
  2.15266302e-01  1.63878843e-01  1.96003914e-01 -2.47898698e-01
  1.90329701e-01 -1.22268394e-01 -3.58498812e-01  2.10014489e-02
 -4.05446649e-01 -1.61502868e-01  1.54933974e-01 -4.79397476e-02
  4.74046879e-02  1.76292807e-01 -8.97908136e-02 -1.25820450e-02
 -1.56360567e-01  1.69833288e-01 -4.52462062e-02 -1.32302959e-02
 -7.16712046e-03  1.21681958e-01  1.72581822e-01  3.14230509e-02
 -1.30693257e-01  1.75286025e-01  8.69380832e-02  9.40190479e-02
  1.97828531e-01  3.26212317e-01  1.86052192e-02  4.25695717e-01
  1.55006438e-01  2.74881065e-01 -3.07177216e-01  2.59334713e-01
 -1.75049067e-01 -4.66968715e-02  1.51676768e-02 -3.25273156e-01
  1.24122947e-01 -2.73768991e-01  1.00311041e-01 -4.87519382e-03
  1.90614760e-01  1.17226571e-01 -1.64610028e-01  6.42995164e-02
  1.07192039e-01  3.59379113e-01 -5.38466513e-01  6.83293641e-02
  4.44404082e-03 -1.57534838e-01 -1.76445544e-01 -2.95781851e-01
 -1.98039919e-01  8.66128206e-02  2.07323171e-02  1.12705134e-01
 -4.55204919e-02  6.97376877e-02  8.05349052e-02  1.61157757e-01
  3.35363448e-01  1.45715922e-01  3.45043540e-01  1.13757476e-02
 -9.83028933e-02  1.24067970e-01  3.47750902e-01  1.91064067e-02
 -2.39545390e-01 -9.75486189e-02  1.94130570e-01 -2.33657882e-02
  3.71883571e-01 -3.26885492e-01  2.85891593e-01  1.10946216e-01
  1.23190358e-01  2.67274350e-01  5.60098514e-03  2.03629866e-01
 -3.06951463e-01  3.12887132e-01  1.25442326e-01  5.74044883e-04
  2.17995733e-01 -5.77997975e-02 -4.06215906e-01 -4.12732325e-02
  5.20697609e-02 -4.65538017e-02 -1.15969079e-03 -3.31223235e-02
 -2.33555064e-01  3.90304551e-02  3.02116144e-02 -2.40587324e-01
 -2.10170120e-01  2.83217132e-01 -8.87310728e-02 -3.12260501e-02
 -2.17005312e-01  2.29419217e-01 -1.47764944e-02 -3.25015038e-01
  8.48938301e-02 -2.87597060e-01 -7.47015839e-03 -3.99805218e-01
 -1.54797450e-01 -2.22869575e-01 -3.35577168e-02  2.18678683e-01
 -3.18351351e-02  7.21639022e-02 -7.62905329e-02 -1.57976337e-02
 -1.85878277e-01  5.93417734e-02 -1.40381642e-02  3.36162388e-01
 -1.19314104e-01 -2.25902870e-02  5.72811291e-02  5.03860831e-01
  2.39528976e-02 -1.00369260e-01 -1.16035014e-01 -2.60533154e-01
  1.44233108e-02 -4.32583615e-02 -2.43471786e-01 -1.30820498e-01
  1.10396475e-01  4.97743577e-01  1.24671701e-02  2.70100623e-01
 -1.27528816e-01  9.49493125e-02  4.41731572e-01 -3.61767650e-01
 -2.66852975e-01  7.76483342e-02  7.35477805e-02 -2.04117984e-01
 -6.59935102e-02  2.15818897e-01 -4.15677167e-02  2.21537333e-02]"
import tensorflow error stat:awaiting response type:build/install stale subtype:windows TF 2.12,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12.0

### Custom code

Yes

### OS platform and distribution

windows 10

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

not able to import tensorflow

### Standalone code to reproduce the issue

```shell
import tensorflow
```


### Relevant log output

```shell
TypeError                                 Traceback (most recent call last)
Cell In [69], line 1
----> 1 import tensorflow

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\__init__.py:37
     34 import sys as _sys
     35 import typing as _typing
---> 37 from tensorflow.python.tools import module_util as _module_util
     38 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     40 # Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\__init__.py:42
     37 from tensorflow.python.eager import context
     39 # pylint: enable=wildcard-import
     40 
     41 # Bring in subpackages.
---> 42 from tensorflow.python import data
     43 from tensorflow.python import distribute
     44 # from tensorflow.python import keras

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\__init__.py:21
     15 """"""`tf.data.Dataset` API for input pipelines.
     16 
     17 See [Importing Data](https://tensorflow.org/guide/data) for an overview.
     18 """"""
     20 # pylint: disable=unused-import
---> 21 from tensorflow.python.data import experimental
     22 from tensorflow.python.data.ops.dataset_ops import AUTOTUNE
     23 from tensorflow.python.data.ops.dataset_ops import Dataset

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\experimental\__init__.py:97
     15 """"""Experimental API for building input pipelines.
     16 
     17 This module contains experimental `Dataset` sources and transformations that can
   (...)
     93 @@UNKNOWN_CARDINALITY
     94 """"""
     96 # pylint: disable=unused-import
---> 97 from tensorflow.python.data.experimental import service
     98 from tensorflow.python.data.experimental.ops.batching import dense_to_ragged_batch
     99 from tensorflow.python.data.experimental.ops.batching import dense_to_sparse_batch

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\experimental\service\__init__.py:419
      1 # Copyright 2020 The TensorFlow Authors. All Rights Reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     13 # limitations under the License.
     14 # ==============================================================================
     15 """"""API for using the tf.data service.
     16 
     17 This module contains:
   (...)
    416   job of ParameterServerStrategy).
    417 """"""
--> 419 from tensorflow.python.data.experimental.ops.data_service_ops import distribute
    420 from tensorflow.python.data.experimental.ops.data_service_ops import from_dataset_id
    421 from tensorflow.python.data.experimental.ops.data_service_ops import register_dataset

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\experimental\ops\data_service_ops.py:22
     20 from tensorflow.core.protobuf import data_service_pb2
     21 from tensorflow.python import tf2
---> 22 from tensorflow.python.data.experimental.ops import compression_ops
     23 from tensorflow.python.data.experimental.service import _pywrap_server_lib
     24 from tensorflow.python.data.experimental.service import _pywrap_utils

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\experimental\ops\compression_ops.py:16
      1 # Copyright 2020 The TensorFlow Authors. All Rights Reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     13 # limitations under the License.
     14 # ==============================================================================
     15 """"""Ops for compressing and uncompressing dataset elements.""""""
---> 16 from tensorflow.python.data.util import structure
     17 from tensorflow.python.ops import gen_experimental_dataset_ops as ged_ops
     20 def compress(element):

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\util\structure.py:22
     18 import itertools
     20 import wrapt
---> 22 from tensorflow.python.data.util import nest
     23 from tensorflow.python.framework import composite_tensor
     24 from tensorflow.python.framework import ops

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\data\util\nest.py:34
      1 # Copyright 2017 The TensorFlow Authors. All Rights Reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the ""License"");
   (...)
     13 # limitations under the License.
     14 # ==============================================================================
     16 """"""## Functions for working with arbitrarily nested sequences of elements.
     17 
     18 NOTE(mrry): This fork of the `tensorflow.python.util.nest` module
   (...)
     31    arrays.
     32 """"""
---> 34 from tensorflow.python.framework import sparse_tensor as _sparse_tensor
     35 from tensorflow.python.util import _pywrap_utils
     36 from tensorflow.python.util import nest

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\sparse_tensor.py:25
     23 from tensorflow.python import tf2
     24 from tensorflow.python.framework import composite_tensor
---> 25 from tensorflow.python.framework import constant_op
     26 from tensorflow.python.framework import dtypes
     27 from tensorflow.python.framework import ops

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\constant_op.py:25
     23 from tensorflow.core.framework import types_pb2
     24 from tensorflow.python.eager import context
---> 25 from tensorflow.python.eager import execute
     26 from tensorflow.python.framework import dtypes
     27 from tensorflow.python.framework import op_callbacks

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\eager\execute.py:21
     19 from tensorflow.python import pywrap_tfe
     20 from tensorflow.python.eager import core
---> 21 from tensorflow.python.framework import dtypes
     22 from tensorflow.python.framework import ops
     23 from tensorflow.python.framework import tensor_shape

File ~\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\dtypes.py:37
     34 from tensorflow.core.function import trace_type
     35 from tensorflow.tools.docs import doc_controls
---> 37 _np_bfloat16 = _pywrap_bfloat16.TF_bfloat16_type()
     38 _np_float8_e4m3fn = _pywrap_float8.TF_float8_e4m3fn_type()
     39 _np_float8_e5m2 = _pywrap_float8.TF_float8_e5m2_type()

TypeError: Unable to convert function return value to a Python type! The signature was
	() -> handle
```
",False,"[-4.73985612e-01 -4.05085027e-01 -6.32306933e-02  2.08900392e-01
  2.38688976e-01 -3.54349494e-01 -3.83045733e-01  6.16595745e-02
 -2.97864348e-01 -4.49318290e-01 -3.33377719e-02 -1.55334562e-01
 -2.16273546e-01 -8.95863585e-03 -3.04427862e-01  3.62626582e-01
 -3.56299639e-01  2.10062545e-02  2.35232532e-01  1.30604371e-01
 -2.69221008e-01 -1.58235207e-01 -3.37094724e-01  1.98991656e-01
  2.71771997e-01  1.19252786e-01 -2.55367160e-01 -9.21928510e-02
  8.10601860e-02  2.24036366e-01  4.25293475e-01 -2.25453004e-02
 -1.18050948e-02  9.59563926e-02  1.83703914e-01  2.44275540e-01
 -1.78626657e-01 -1.38671994e-01 -3.74786019e-01 -1.20834529e-01
  1.18086770e-01  1.53388068e-01  1.22768745e-01 -1.43275365e-01
  9.79646742e-02 -1.73027068e-01  8.43457431e-02 -2.95461506e-01
 -1.29867211e-01 -2.02147126e-01 -1.80590600e-01  5.19754887e-02
 -4.59279954e-01 -3.91924679e-01 -3.50962341e-01 -3.00973117e-01
  1.43838003e-01 -9.26867574e-02 -7.39758536e-02  3.26569736e-01
 -2.97026671e-02 -8.93870294e-02  2.46025994e-01 -1.79661274e-01
  4.03181426e-02  8.27936605e-02  1.63042203e-01 -5.77068515e-02
  5.02888381e-01 -4.59693402e-01  1.60104021e-01 -7.33668730e-02
 -2.97454536e-01  1.77205279e-01  4.73468602e-02  5.09964488e-02
  7.35751539e-02  1.54347345e-01  4.48820651e-01 -2.95305312e-01
  1.56831387e-02 -2.63331473e-01  1.44584971e-02 -2.70045102e-01
  2.18566269e-01 -1.21276468e-01  3.57210338e-01  1.42678648e-01
  5.29153228e-01 -3.01525176e-01  5.90967178e-01  2.56271541e-01
  1.16408251e-01  1.53414905e-01  5.98713040e-01  1.35138571e-01
  1.24706082e-01  4.02743995e-01  2.48568915e-02 -1.66075945e-01
 -1.67088985e-01 -2.40421981e-01  6.01773337e-03 -1.39490105e-02
  8.80376250e-02 -2.58760810e-01  2.68765271e-01 -5.71204387e-02
  1.79154158e-01 -1.48550957e-01  1.63848966e-01 -8.55172276e-02
  2.97797978e-01 -1.74118519e-01 -2.61886597e-01  6.04341254e-02
 -2.57888883e-01  5.11941202e-02 -6.51365742e-02  8.63244057e-01
 -5.03736734e-02 -5.71654066e-02  7.39839375e-02  2.07143426e-02
  3.13970923e-01 -6.42211735e-02 -3.28293443e-01 -3.41956727e-02
  4.18746546e-02  4.13690694e-02  1.81145549e-01  1.53147876e-01
  1.75460309e-01  1.74110189e-01  1.02730975e-01  3.65416110e-02
 -2.04004318e-01 -1.96121261e-01 -3.02536190e-01 -4.05042589e-01
 -2.85195798e-01  2.43695825e-01 -1.94258392e-01 -6.96235716e-01
  6.80647492e-02 -1.23639964e-03 -1.80703312e-01  2.67192483e-01
 -2.49701574e-01  1.81574702e-01 -3.41682471e-02  3.23555917e-01
 -1.31210223e-01  2.64562905e-01  1.71543926e-01  4.55598161e-02
  4.21023101e-01 -8.88300613e-02 -6.79491907e-02 -7.80062556e-01
  7.86893219e-02  4.99038309e-01  5.42968698e-03 -1.99395150e-01
  2.86675572e-01  2.58138359e-01 -4.84431773e-01 -2.28846610e-01
  2.02228293e-01  4.50942874e-01 -3.60851586e-02 -2.17572913e-01
  8.74238536e-02  1.15531698e-01  3.23781908e-01  1.03557035e-02
  2.65378714e-01 -5.55761576e-01 -1.11972347e-01  4.62191075e-01
  2.41060615e-01  7.34522492e-02  1.25677690e-01  1.01466969e-01
  4.45925929e-02  1.11648664e-01  1.18970543e-01  2.21584767e-01
 -2.22096175e-01  1.58689916e-01 -3.54103297e-01 -5.49777299e-02
  3.97860408e-01 -1.29257917e-01 -2.62357891e-01  1.87942475e-01
  1.54928833e-01 -1.10686347e-01  8.21491852e-02 -5.27820252e-02
 -1.93148211e-01 -7.85386860e-02 -1.54556818e-02 -1.63195767e-02
  7.33475164e-02 -4.24009383e-01  2.90665142e-02 -4.47677165e-01
 -4.76858556e-01  9.64165032e-02  4.12926860e-02 -5.86856961e-01
  8.05252045e-02  2.43553445e-02 -3.86735380e-01  3.01013082e-01
  2.16183305e-01  1.85101219e-02 -2.42178470e-01  1.65652826e-01
  5.98358139e-02 -1.89765424e-01 -8.77928287e-02 -3.20566416e-01
 -2.06515461e-01  2.38842025e-01 -3.19099367e-01  1.52389944e-01
 -2.32068449e-02  3.12748730e-01  1.60741895e-01  4.24998924e-02
  3.28990638e-01  2.37007931e-01  5.95423222e-01 -3.52010071e-01
 -2.77837552e-02 -5.91608025e-02 -9.23241451e-02  1.02143288e-01
 -3.56467485e-01 -1.29955590e-01 -1.51042297e-01 -4.57935929e-02
  1.45307288e-01  3.87016535e-01 -2.69601613e-01 -1.68793947e-01
 -4.66239899e-01  2.27506191e-01 -1.32140979e-01  2.78258383e-01
  4.11485374e-01  2.64740884e-01  4.94116396e-01  3.62266660e-01
  1.14032343e-01  3.41606855e-01  2.17194140e-01 -1.69561863e-01
  5.73166251e-01  1.14694320e-01 -2.07496025e-02  2.54766345e-01
  2.10446104e-01  1.71224266e-01 -4.84527856e-01  5.88609278e-01
  1.04608178e-01 -8.23342875e-02  1.17951512e-01 -4.15602207e-01
  5.52371383e-01 -5.85883021e-01 -1.24846354e-01 -2.30986662e-02
  3.76957715e-01  1.04621366e-01 -2.05479592e-01  1.22046992e-02
  2.17669457e-03  4.50653166e-01 -3.74925375e-01  6.18885569e-02
  4.44741063e-02 -2.98928656e-02 -5.38319349e-02 -7.21386731e-01
 -1.73762813e-01  1.31082326e-01 -2.69703805e-01  1.98495388e-02
 -1.12313583e-01 -1.32519687e-02 -2.82837689e-01 -4.10351902e-04
 -1.88717917e-02 -1.26023173e-01 -3.27283964e-02  1.61042631e-01
 -5.26918434e-02  2.18000457e-01  5.08599162e-01 -5.40103674e-01
 -1.38420656e-01 -2.80547589e-01  4.74132597e-01  3.75018209e-01
  4.33904707e-01 -4.61673737e-01  2.66252458e-01 -1.77754238e-01
 -9.43929255e-02  5.60244918e-01 -9.30068940e-02 -4.30561602e-02
 -4.27510500e-01  7.11599946e-01  2.34369487e-01 -1.23022310e-01
  2.92308271e-01 -1.82395279e-01 -3.08512777e-01  8.56348723e-02
  3.48288953e-01  8.20017979e-03 -1.25896573e-01 -4.55421954e-01
 -2.91689578e-02  2.78093815e-01 -1.32849336e-01 -6.67607561e-02
 -5.64926714e-02 -1.32959932e-02 -2.14412019e-01 -4.17272076e-02
 -5.16479015e-01  3.43424618e-01  1.05328955e-01 -3.57380390e-01
 -9.26202834e-02 -1.45639598e-01  2.74744093e-01 -1.48566365e-01
  9.25757289e-02 -4.17901903e-01  4.28869545e-01  3.84897470e-01
 -5.18000275e-02  2.17832327e-01 -5.93281984e-02  2.06181481e-01
 -5.04708648e-01  2.07954831e-03  1.40803307e-03  1.44892097e-01
  7.32315332e-02 -2.49497384e-01  3.62861365e-01  3.61010134e-01
  2.02300977e-02  2.59033859e-01 -3.55717570e-01 -6.12235740e-02
  2.58559465e-01 -2.75478750e-01 -3.57920706e-01 -2.91957259e-01
  1.46361351e-01  3.65284175e-01 -9.32506025e-02  3.30426246e-01
 -3.94152105e-01  4.55878347e-01  5.45060635e-01 -5.15989244e-01
 -4.05345559e-01  1.72798365e-01  7.52211958e-02 -1.09345078e-01
  1.16228104e-01 -1.42898917e-01  1.12512715e-01 -6.19263202e-03]"
ImportError: undefined symbol after install stat:awaiting response type:build/install stale subtype: ubuntu/linux,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

1.7.0

### Custom code

No

### OS platform and distribution

Linux-4.14.0-xilinx-v2018.3-armv7l-with-pynqlinux-v2.6-WFH

### Mobile device

no

### Python version

3.6.5

### Bazel version

0.10.0- (@non-git)

### GCC/compiler version

GCC 7.3.0

### CUDA/cuDNN version

no

### GPU model and memory

_No response_

### Current behavior?

I'm facing with the ImportError - Undefined symbol when trying to import tensorflow after successfully compiling from source and installed Tensorflow 1.7.0 on a 32 bit architecture. 

### Standalone code to reproduce the issue

```shell
> Build configurations for Tensorflow:

build --action_env PYTHON_BIN_PATH=""/usr/bin/python3""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python3.6/dist-packages""
build --force_python=py3
build --host_force_python=py3
build --python_path=""/usr/bin/python3""
build:gcp --define with_gcp_support=true
build:hdfs --define with_hdfs_support=true
build:s3 --define with_s3_support=true
build:kafka --define with_kafka_support=true
build:xla --define with_xla_support=true
build:gdr --define with_gdr_support=true
build:verbs --define with_verbs_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""0""
build --define grpc_no_ares=true
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK
build --host_copt=-DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK


> After bazel build:

sudo python3 -m pip install /tmp/tensorflow_pkg/tensorflow-1.7.0-cp36-cp36m-linux_armv7l.whl

> Importing in python3 

import tensorflow as tf
```


### Relevant log output

```shell
xilinx@pynq:/tmp/tensorflow_pkg$ python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *  # pylint: disable=redefined-builtin
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```
",False,"[-0.3923691  -0.28113407 -0.1718642   0.12301075  0.35118484 -0.32818156
  0.02363252  0.07444468 -0.30978408 -0.28519753  0.12468153  0.24227782
  0.09546082 -0.05726535 -0.27015358  0.38855666 -0.2764942  -0.18925238
  0.1531307   0.10332265 -0.12983158 -0.2490408  -0.08600346 -0.08048026
  0.17375398 -0.00139821 -0.33824033  0.07706328  0.12238245  0.22326392
  0.6894727   0.08441688 -0.12305042  0.26148984  0.13428968  0.28813446
 -0.25555146  0.00104115 -0.11166885  0.081983   -0.00842631 -0.01399958
  0.04451437 -0.03488692 -0.01262159  0.01226331  0.03838596 -0.20863545
  0.074812   -0.15437508 -0.12851077  0.06713904 -0.43895823 -0.35620993
 -0.11522447 -0.13103412  0.14388815  0.20614664 -0.01042086  0.17250481
  0.10767271 -0.15810674  0.16803864  0.00111007 -0.2704364   0.09367691
  0.06920162  0.08484174  0.5160703  -0.02392562  0.25810817  0.1135596
 -0.4233026   0.04776592 -0.0320721   0.19374987 -0.20856822  0.0251753
  0.22699162 -0.16728452 -0.22509769 -0.10925382  0.10720336  0.10752009
  0.26475853  0.09294887  0.20577845  0.06915326  0.33901733 -0.1227603
  0.6320132   0.19774042 -0.02625859  0.00321709  0.53334004  0.14976896
  0.31506234  0.43238586 -0.16846314 -0.22197168 -0.111862   -0.19649896
 -0.08792474  0.08465876  0.06323744 -0.08896388 -0.03516955 -0.08645268
  0.17218328 -0.18872876  0.22876409 -0.05610652  0.11317895  0.08084194
 -0.15363917 -0.05365118 -0.27371687  0.00621957 -0.1623156   0.5965035
  0.11204566  0.06804337 -0.09764513  0.09805144  0.30984157  0.11160734
 -0.11166647 -0.07075727 -0.09141304  0.22383499  0.06401829  0.10978101
 -0.05762485  0.14113598 -0.05724876 -0.24802217 -0.19118504 -0.04914672
 -0.19149065 -0.18855822 -0.30238312  0.12234664 -0.07840081 -0.45906505
  0.07488256 -0.14573501 -0.20924605  0.22810295  0.00934394 -0.05742142
 -0.02017994 -0.01083124 -0.3540641   0.5252677   0.30166245  0.35450155
  0.26243693 -0.0030871  -0.14129902 -0.7669815   0.13573557  0.5477227
 -0.04129428 -0.11014702 -0.02482324  0.06582992 -0.37301543 -0.04215907
  0.05406491  0.25946152 -0.01429756  0.03653789  0.00137465  0.17928714
  0.26015553 -0.05408611  0.36496568 -0.59407604 -0.22509031  0.3342919
  0.4630373  -0.05156254 -0.07840161  0.11798135 -0.06278297  0.16362105
  0.02151761 -0.10572831 -0.1485702   0.03582851 -0.25328687  0.17201185
  0.4036227  -0.33450365 -0.27583674 -0.14748302  0.06818719 -0.05994079
  0.21072982  0.1343733   0.0909787   0.11126512  0.00195312  0.13369098
  0.03629112 -0.3212508  -0.16258323 -0.3078639  -0.4417548  -0.03998082
 -0.31351176 -0.271645    0.36090434 -0.09554715 -0.13102351  0.1112351
 -0.08799633  0.1277924  -0.18817988 -0.02976725 -0.11535785 -0.38034004
 -0.11290607 -0.22444855  0.03414455  0.15869302 -0.02716979 -0.12831143
  0.16772306  0.307737   -0.10778131  0.02096172  0.31849998  0.21895154
  0.30832607 -0.07648631 -0.11879075 -0.01231093  0.08073242  0.36606875
 -0.43516442  0.14655086 -0.13539748 -0.06691758  0.16592892  0.08801249
 -0.07617117  0.13687566 -0.28967118  0.10901956 -0.16233492 -0.0014863
  0.2503423   0.04637151  0.39579958  0.17399469  0.06500001  0.22938898
  0.14230657 -0.2027002   0.22129251 -0.01088639 -0.01471298  0.04137652
  0.13963325  0.05140904 -0.40514615  0.5151439   0.10259357 -0.06256735
  0.16340728 -0.25172764  0.4856953  -0.32692742 -0.10909659  0.00712778
  0.35233396 -0.02440759 -0.05871622 -0.00929241 -0.06510737  0.34105384
 -0.44179448  0.06244627  0.09004498 -0.37475413  0.12020823 -0.45225728
 -0.3925231   0.16944435 -0.19906011  0.18884875 -0.16426474 -0.03734778
 -0.29986614  0.02755218 -0.03933979 -0.20425339 -0.00369051  0.44513798
 -0.19387238  0.19569096  0.28943998 -0.25280142 -0.07513082 -0.2558842
  0.22958376 -0.0318451   0.48606455 -0.5691401   0.2646103   0.21113433
 -0.07517028  0.33215457  0.0545692   0.10894424 -0.3995271   0.64153916
  0.08873034 -0.03261553  0.10425738 -0.32657725 -0.2954511  -0.28527343
  0.16662306  0.05905855 -0.10262524 -0.50758326 -0.09245253  0.27238086
 -0.19187333 -0.09569305 -0.15632756  0.06479418 -0.20425013 -0.08867742
 -0.25260833  0.31438804 -0.00262166 -0.25944167 -0.01456207 -0.14640248
  0.06941423 -0.1896129  -0.1373961  -0.34801415  0.34831044  0.24964207
 -0.12803641  0.1473842   0.16564299  0.07546029 -0.34321308  0.21415895
 -0.16666701  0.02407496  0.07973793 -0.13036059  0.32493225  0.29956278
 -0.27808672  0.17319116 -0.33201012 -0.01945749  0.12707426 -0.25108063
 -0.2077362  -0.19276854 -0.02170309  0.30138803 -0.12109396  0.20119956
 -0.4902926   0.35626146  0.5333084  -0.3123819  -0.37535778  0.40086007
  0.14680082 -0.13593234 -0.2362881   0.06247806  0.26982647 -0.06029215]"
xla_cpu_gpu_device: MSVC compile errors stat:awaiting response type:build/install stale subtype:windows TF 2.13,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Windows 10 22H2

### Mobile device

_No response_

### Python version

Anaconda 2023.07-1

### Bazel version

6.2.1

### GCC/compiler version

Visual Studio 2022 (build tools 14.36) + msys2-x86_64-20230718

### CUDA/cuDNN version

CUDA 11.8 + CUDNN 8.6.0 + TensorRT 8.5.3

### GPU model and memory

GTX 750 Ti 2GB

### Current behavior?

This issue required two fixes.

First, `external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py` adds too many unrelated include path to compilation command. Related scripts or header dependencies need to be fixed. Minimum command is just like:
`nvcc -v -c -x=c++ -std=c++17 tensorflow/compiler/jit/xla_cpu_device.cc -I .,bazel-out/x64_windows-opt/bin,external/eigen_archive,
external/com_google_absl,external/com_google_protobuf/src,external/farmhash_archive/src,external/llvm-project/llvm/include,
external/llvm-raw/llvm/include,external/llvm-project/mlir/include,bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include,
external/tf_runtime/include -o bazel-out/x64_windows-opt/bin/tensorflow/compiler/jit/_objs/xla_cpu_device/xla_cpu_device.obj`

Second, `nvcc` will pass the above command to `cl.exe`. Then MSVC will throw some syntax errors and stop. If I use `clang-cl` provided by Visual Studio, some warnings may appear but the `.obj` compilation is successful. Following linux build migration to clang, I think the compiler path of `msvc_wrapper_for_nvcc.py` can change to `clang-cl` to avoid syntax errors.

### Standalone code to reproduce the issue

```shell
1. download https://github.com/tensorflow/tensorflow/archive/refs/tags/v2.13.0.zip and extract
2. comment out Windows CUDA build rejection code in configure.py
3. run `python configure.py` to configure Windows CUDA build
4. run `bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package`
```


### Relevant log output

```shell
ERROR: E:/tensorflow-2.13.0-createprocessw/tensorflow/compiler/jit/BUILD:113:11: Compiling tensorflow/compiler/jit/xla_cpu_device.cc failed: (Exit -1): python.exe failed: error exe
cuting command (from target //tensorflow/compiler/jit:xla_cpu_device)
  cd /d E:/_bazel_tensorflow/4zvk5ci6/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8
    SET INCLUDE=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.36.32532\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.3
6.32532\ATLMFC\include;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Auxiliary\VS\include;C:\Program Files (x86)\Windows Kits\10\include\10.0.22621.0\ucrt;C:\Program F
iles (x86)\Windows Kits\10\\include\10.0.22621.0\\um;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\shared;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621
.0\\winrt;C:\Program Files (x86)\Windows Kits\10\\include\10.0.22621.0\\cppwinrt
    SET LIB=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.36.32532\ATLMFC\lib\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\1
4.36.32532\lib\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.22621.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\\lib\10.0.22621.0\\um\x64
    SET PATH=C:\Program Files\Microsoft Visual Studio\2022\Community\VC\Tools\MSVC\14.36.32532\bin\HostX64\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\V
C\VCPackages;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files\Microsoft Visual Studio\2022\Community\Commo
n7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files\Microsoft Visual Studio\2022\Community\MSBuild\Current\bin\Roslyn;C:\Program Files\Microsoft Visual
Studio\2022\Community\Team Tools\Performance Tools\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\b
in\10.0.22621.0\\x64;C:\Program Files (x86)\Windows Kits\10\bin\\x64;C:\Program Files\Microsoft Visual Studio\2022\Community\\MSBuild\Current\Bin\amd64;C:\Windows\Microsoft.NET\Fra
mework64\v4.0.30319;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\Tools\;;C:\Windows\system32
;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\
CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files\Microsoft Visual Studio\2022\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/tensorflow/anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Users/tensorflow/anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\msys64\tmp
    SET TF2_BEHAVIOR=1
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,5.0,7.0
    SET TMP=C:\msys64\tmp
  C:\Users\tensorflow\anaconda3\python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_
SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_window
s-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexte
rnal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/com_google_protobuf /Ib
azel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/llvm-project /Ibazel-out/x64_windows-opt/bin/external/llvm-project /Iexternal/llvm_terminfo /Ibazel-out/x64_win
dows-opt/bin/external/llvm_terminfo /Iexternal/llvm_zlib /Ibazel-out/x64_windows-opt/bin/external/llvm_zlib /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/l
ibjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iextern
al/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64
_windows-opt/bin/external/highwayhash /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/loc
al_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm /Iexternal/local_c
onfig_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Iexternal/cudnn_frontend_archive /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive /Iex
ternal/curl /Ibazel-out/x64_windows-opt/bin/external/curl /Iexternal/boringssl /Ibazel-out/x64_windows-opt/bin/external/boringssl /Iexternal/jsoncpp_git /Ibazel-out/x64_windows-opt
/bin/external/jsoncpp_git /Iexternal/com_github_grpc_grpc /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc /Iexternal/upb /Ibazel-out/x64_windows-opt/bin/external/upb
/Iexternal/mkl_dnn_v1 /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1 /Iexternal/stablehlo /Ibazel-out/x64_windows-opt/bin/external/stablehlo /Iexternal/tf_runtime /Ibazel-out/
x64_windows-opt/bin/external/tf_runtime /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributeInterfacesIncGen /Ibazel-out/x64_windows-opt/bi
n/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectBytecodeGen /Ibaze
l-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLoca
tionAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtu
al_includes/BuiltinTypeInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llv
m-project/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen /Ibazel-out/x64_windows-
opt/bin/external/llvm-project/mlir/_virtual_includes/FunctionInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGe
n /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Reg
ionKindInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project
/mlir/_virtual_includes/SymbolInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorEncodingIncGen /Ibazel-out/x64_windows-opt/bin/ext
ernal/llvm-project/mlir/_virtual_includes/ArithBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithCanonicalizationIncGen /Ibazel-out/x64_w
indows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithOpsInterfacesIncGen /Ib
azel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferIntRangeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/
VectorInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-projec
t/mlir/_virtual_includes/ControlFlowOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/FuncIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-
project/mlir/_virtual_includes/AsmParserTokenKinds /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/QuantOpsIncGen /Ibazel-out/x64_windows-opt/bin/exter
nal/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Mem2RegInterfacesIncGen /Ibazel-out/x64
_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/DialectUtilsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGe
n /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesInc
Gen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Con
versionPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/
_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Ibazel-out/x64_windows-opt/bin/external/lo
cal_config_cuda/cuda/_virtual_includes/cudnn_header /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/DerivedAttributeOpInterfaceIncGen /Ibazel-out/x64_w
indows-opt/bin/external/llvm-project/mlir/_virtual_includes/MLProgramAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MLProgramOpsIncGe
n /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MLProgramTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Run
timeVerifiableOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/cudnn_frontend_archive/_virtual_includes/cudnn_frontend /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler
/xla/mlir_hlo/_virtual_includes/mlir_hlo /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/canonicalize_inc_gen /Ibazel-out/x64_windows-opt/bin/ten
sorflow/compiler/xla/mlir_hlo/_virtual_includes/convert_op_folder /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_attrs_inc_gen /Ibazel-o
ut/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_common /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_
enums_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_v
irtual_includes/hlo_ops_pattern_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_ops_typedefs_inc_gen /Ibazel-out/x64_windows-opt/bin/
external/llvm-project/mlir/_virtual_includes/ComplexAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ComplexBaseIncGen /Ibazel-out/x64_
windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ComplexOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMDialectInterfaceIncGe
n /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMIntrinsicOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/L
LVMOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includ
es/CopyOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_v
irtual_includes/MemRefOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapedOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-
project/mlir/_virtual_includes/DestinationStyleOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ValueBoundsOpInterfaceIncGen /Ibazel-o
ut/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Sha
peOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_
virtual_includes/AffineOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ParallelCombiningOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/ext
ernal/llvm-project/mlir/_virtual_includes/TensorOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TilingInterfaceIncGen /Ibazel-out/x64_windows
-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorAttrDefsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorOpsIncGen
/Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/base /Ibaz
el-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/base_attr_interfaces_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/broadcast_utils /I
bazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/chlo_ops /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/chlo_attrs_inc_gen /Ibazel-out/x64_
windows-opt/bin/external/stablehlo/_virtual_includes/chlo_enums_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/chlo_ops_inc_gen /Ibazel-out/x64_window
s-opt/bin/external/stablehlo/_virtual_includes/stablehlo_assembly_format /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/stablehlo_type_inference /Ibazel-out/x
64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mhlo_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/chlo_legalize_t
o_hlo /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/chlo_legalize_to_hlo_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_h
lo/_virtual_includes/map_chlo_to_hlo_op /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AllocationOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/ext
ernal/llvm-project/mlir/_virtual_includes/BufferizableOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BufferizationBaseIncGen /Ibazel
-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BufferizationEnumsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Bufferiz
ationOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFDeviceMappingInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/m
lir/_virtual_includes/SCFIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/m
lir_hlo/_virtual_includes/hlo_legalize_to_stablehlo /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_stablehlo_to_hlo_op /Ibazel-out/x64_windo
ws-opt/bin/external/stablehlo/_virtual_includes/stablehlo_ops /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/stablehlo_attrs_inc_gen /Ibazel-out/x64_windows-o
pt/bin/external/stablehlo/_virtual_includes/stablehlo_enums_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/stablehlo_ops_inc_gen /Ibazel-out/x64_windo
ws-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/legalize_to_linalg_utils /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_mhlo_t
o_scalar_op /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MathBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes
/MathOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MaskableOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_vi
rtual_includes/MaskingOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-p
roject/mlir/_virtual_includes/LinalgEnumsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/
external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen /Ibazel-out/x64_
windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/legalize_to_standard_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/l
hlo /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_in
cludes/lhlo_ops_structs_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_structured_interface /Ibazel-out/x64_windows-opt/bin/tensorf
low/compiler/xla/mlir_hlo/_virtual_includes/lhlo_structured_interface_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lower_complex_inc_g
en /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_hlo_to_lhlo_op /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_i
ncludes/mhlo_pass_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/mhlo_rng_utils /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/
mlir_hlo/_virtual_includes/mhlo_scatter_gather_utils /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/shape_component_analysis /Ibazel-out/x64_win
dows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/stablehlo_legalize_to_hlo /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo /I
bazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_op
s_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virt
ual_includes/thlo_bufferizable_op_interface /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/type_conversion /Ibazel-out/x64_windows-opt/bin/exter
nal/llvm-project/mlir/_virtual_includes/BufferizationPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/FuncTransformsPassIncGen /Ibazel-out/x6
4_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/unfuse_batch_norm /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffinePassIncGen
 /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/DLTIBaseI
ncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/GPUOps
IncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Me
mRefPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/NVGPUIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes
/NVGPUPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_i
ncludes/VectorEnumsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_
virtual_includes/X86VectorIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeTransformsPassIncGen /Ibazel-out/x64_windows-opt/bin/tensorflow/c
ompiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_attrdefs_inc_gen /Ibazel-out/x64_win
dows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_dialect_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo
_gpu_ops_enums_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lhlo_gpu_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/x
la/mlir_hlo/_virtual_includes/lhlo_gpu_ops_ops /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/stablehlo_passes /Ibazel-out/x64_windows-opt/bin/external/stable
hlo/_virtual_includes/stablehlo_pass_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/version /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtua
l_includes/vhlo_ops /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/vhlo_attr_interfaces_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_in
cludes/vhlo_attrs_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/vhlo_enums_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includ
es/vhlo_op_interfaces_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/vhlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_incl
udes/vhlo_types /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includes/vhlo_type_interfaces_inc_gen /Ibazel-out/x64_windows-opt/bin/external/stablehlo/_virtual_includ
es/vhlo_types_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/hlo_dialect_registration /Ibazel-out/x64_windows-opt/bin/external/stablehlo
/_virtual_includes/register /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXCodeGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_vi
rtual_includes/NVPTXCommonTableGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtual_includes/NVPTXInfo /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm
/_virtual_includes/NVPTXUtilsAndDesc /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AsyncOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-proje
ct/mlir/_virtual_includes/GPUPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LLVMConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/l
lvm-project/mlir/_virtual_includes/LLVMPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/NVVMOpsIncGen /Ibazel-out/x64_windows-opt/bin/externa
l/llvm-project/mlir/_virtual_includes/LLVMIntrinsicConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenMPInterfacesIncGen /Ibazel-out/
x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenMPOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenMPTypeInterfacesIn
cGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ROCDLConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes
/ROCDLOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AMXIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/
ArmNeonIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmSVEIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes
/NVVMConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeToStandardGen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_h
lo/_virtual_includes/transforms_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation /Ibazel-out/x64_windows-opt/bin/tensorflow/co
mpiler/xla/mlir_hlo/_virtual_includes/deallocation_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_utils /Ibazel-out/x64
_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocation_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/deallocat
ion_passes_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_bufferizable_op_interface /Ibazel-out/x64_windows-opt/bin/tensorflow/co
mpiler/xla/mlir_hlo/_virtual_includes/gml_st_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_passes_inc_gen /Ibazel-out/x64_windows
-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gml_st_transforms /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/transforms_passes_i
nc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/userange_analysis /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_incl
udes/TransformDialectEnumsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformDialectIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-p
roject/mlir/_virtual_includes/TransformDialectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformDialectMatchInterfacesIncGen /I
bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Transform
TypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformDialectTransformsIncGen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/ml
ir_hlo/_virtual_includes/all_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/lmhlo_pass_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow
/compiler/xla/mlir_hlo/_virtual_includes/lmhlo_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_lmhlo_to_scalar_op /Ibazel-out/x64_wind
ows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/map_lhlo_to_hlo_op /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_passes /Ib
azel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/thlo_passes_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includ
es/transforms_gpu_passes /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/mlir_hlo/_virtual_includes/gpu_transforms_passes_inc_gen /Ibazel-out/x64_windows-opt/bin/external/l
lvm-project/mlir/_virtual_includes/GPUToNVVMGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AMDGPUIncGen /Ibazel-out/x64_windows-opt/bin/external/l
lvm-project/mlir/_virtual_includes/GPUToROCDLTGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AMXConversionIncGen /Ibazel-out/x64_windows-opt/bin/e
xternal/llvm-project/mlir/_virtual_includes/ArmNeonConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ArmSVEConversionIncGen /Ibazel-out/
x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenACCOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenACCTypeInterfaces
IncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpenACCTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/
X86VectorConversionIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtual_includes/X86CodeGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtua
l_includes/X86CommonTableGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtual_includes/X86Info /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtua
l_includes/X86UtilsAndDesc /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_virtual_includes/JITLinkTableGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/_
virtual_includes/X86DisassemblerInternalHeaders /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAttrUtilsGen /Ibazel-out/x64_windows-opt/bin/exter
nal/llvm-project/mlir/_virtual_includes/SPIRVAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVAvailabilityIncGen /Ibazel-out/x64_w
indows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVCanonicalizationIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVOpsIncGen
/Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SPIRVSerializationGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/Inde
xEnumsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/IndexOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_include
s/TosaDialectIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TosaInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_vi
rtual_includes/TosaPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SparseTensorPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-proj
ect/mlir/_virtual_includes/AsyncPassIncGen /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_
windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/
bin/external/com_google_protobuf/src /Iexternal/llvm-project/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include /Iexternal/llvm-project/mlir/include /I
bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include /Itensorflow/compiler/mlir/tensorflow/include /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/tensorflow/i
nclude /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/farmhash_archive/src /Ibaz
el-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt
/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_co
nfig_rocm/rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazel-out/x64_windows-opt/bin/external/local_config_
rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_
config_rocm/rocm/rocm/include/roctracer /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /Iexternal/curl/include /Ibazel-out/x64_windows-opt/b
in/external/curl/include /Iexternal/boringssl/src/include /Ibazel-out/x64_windows-opt/bin/external/boringssl/src/include /Iexternal/jsoncpp_git/include /Ibazel-out/x64_windows-opt/
bin/external/jsoncpp_git/include /Iexternal/com_github_grpc_grpc/include /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/include /Iexternal/com_github_grpc_grpc/src/c
ore/ext/upb-generated /Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/src/core/ext/upb-generated /Iexternal/com_github_grpc_grpc/third_party/address_sorting/include /
Ibazel-out/x64_windows-opt/bin/external/com_github_grpc_grpc/third_party/address_sorting/include /Iexternal/mkl_dnn_v1/include /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/i
nclude /Iexternal/mkl_dnn_v1/src /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src /Iexternal/mkl_dnn_v1/src/common /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/co
mmon /Iexternal/mkl_dnn_v1/src/common/ittnotify /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/common/ittnotify /Iexternal/mkl_dnn_v1/src/cpu /Ibazel-out/x64_windows-opt/b
in/external/mkl_dnn_v1/src/cpu /Iexternal/mkl_dnn_v1/src/cpu/gemm /Ibazel-out/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/gemm /Iexternal/mkl_dnn_v1/src/cpu/x64/xbyak /Ibazel-o
ut/x64_windows-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak /Itensorflow/compiler/xla/translate/hlo_to_mhlo/include /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/transla
te/hlo_to_mhlo/include /Iexternal/tf_runtime/include /Ibazel-out/x64_windows-opt/bin/external/tf_runtime/include /Iexternal/tf_runtime/third_party/llvm_derived/include /Ibazel-out/
x64_windows-opt/bin/external/tf_runtime/third_party/llvm_derived/include /Iexternal/llvm-project/llvm/lib/Target/NVPTX /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/li
b/Target/NVPTX /Iexternal/llvm-project/llvm/lib/Target/X86 /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/lib/Target/X86 /Iexternal/llvm-project/mlir/lib/Conversion/Fun
cToSPIRV /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/FuncToSPIRV /Iexternal/llvm-project/mlir/lib/Conversion/MathToSPIRV /Ibazel-out/x64_windows-opt/b
in/external/llvm-project/mlir/lib/Conversion/MathToSPIRV /Iexternal/llvm-project/mlir/lib/Conversions/GPUToSPIRV /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conv
ersions/GPUToSPIRV /Iexternal/llvm-project/mlir/lib/Conversion/MemRefToSPIRV /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/MemRefToSPIRV /Iexternal/llvm
-project/mlir/lib/Conversion/TensorToLinalg /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/TensorToLinalg /Iexternal/llvm-project/mlir/lib/Conversion/Ten
sorToSPIRV /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/TensorToSPIRV /Iexternal/llvm-project/mlir/lib/Conversion/TosaToArith /Ibazel-out/x64_windows-o
pt/bin/external/llvm-project/mlir/lib/Conversion/TosaToArith /Iexternal/llvm-project/mlir/lib/Conversion/TosaToLinalg /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib
/Conversion/TosaToLinalg /Iexternal/llvm-project/mlir/lib/Conversion/TosaToSCF /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/TosaToSCF /Iexternal/llvm-p
roject/mlir/lib/Conversion/TosaToTensor /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/lib/Conversion/TosaToTensor /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /D_CRT_S
ECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLTDL
_SHLIB_EXT="".dll"" /DLLVM_PLUGIN_EXT="".dll"" /DLLVM_NATIVE_ARCH=""X86"" /DLLVM_NATIVE_ASMPARSER=LLVMInitializeX86AsmParser /DLLVM_NATIVE_ASMPRINTER=LLVMInitializeX86AsmPrinter /DLLVM_N
ATIVE_DISASSEMBLER=LLVMInitializeX86Disassembler /DLLVM_NATIVE_TARGET=LLVMInitializeX86Target /DLLVM_NATIVE_TARGETINFO=LLVMInitializeX86TargetInfo /DLLVM_NATIVE_TARGETMC=LLVMInitia
lizeX86TargetMC /DLLVM_NATIVE_TARGETMCA=LLVMInitializeX86TargetMCA /DLLVM_HOST_TRIPLE=""x86_64-pc-win32"" /DLLVM_DEFAULT_TARGET_TRIPLE=""x86_64-pc-win32"" /DLLVM_VERSION_MAJOR=17 /DLLV
M_VERSION_MINOR=0 /DLLVM_VERSION_PATCH=0 /DLLVM_VERSION_STRING=""17.0.0git"" /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DBLAKE3_USE_NEON=0 /DBLAKE3_NO_AVX
2 /DBLAKE3_NO_AVX512 /DBLAKE3_NO_SSE2 /DBLAKE3_NO_SSE41 /DTF_USE_SNAPPY /DTF_ENABLE_ACTIVITY_WATCHER /DCURL_STATICLIB /DGRPC_ARES=0 /DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL /DTEN
SORFLOW_USE_MKLDNN_CONTRACTION_KERNEL /DEIGEN_USE_AVX512_GEMM_KERNELS=0 /DGOOGLE_CUDA=1 /DEIGEN_ALTIVEC_USE_CUSTOM_PACK=0 /DEIGEN_NEON_GEBP_NR=4 /DTF_LLVM_X86_AVAILABLE=1 /DBAZEL_C
URRENT_REPOSITORY="""" /showIncludes /O2 /DNDEBUG /W0 /Zc:__cplusplus /D_USE_MATH_DEFINES /d2ReducedOptimizeHugeFunctions -DWIN32_LEAN_AND_MEAN -DNOGDI /Zc:preprocessor /d2ReducedOpt
imizeHugeFunctions /arch:AVX2 /std:c++17 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/jit/_objs/xla_cpu_device/xla_cpu_device.obj /c tensorflow/compiler/jit/xla_cpu_device.
cc
# Configuration: 65bceb0453d201701ee7f6753c2bb4140d61507260ca636610d54b27f4c27251
# Execution platform: @local_execution_config_platform//:platform
Action failed to execute: java.io.IOException: ERROR: src/main/native/windows/process.cc(165): CreateProcessWithExplicitHandles(""C:\Users\tensorflow\anaconda3\python.exe"" -B extern
al/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SIL
ENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_w
indows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_wi(...)): command is longer than CreateProcessW's limit (32767 characters)
Target //tensorflow/compiler/jit:jit failed to build
INFO: Elapsed time: 768.468s, Critical Path: 94.60s
INFO: 5 processes: 5 internal.
FAILED: Build did NOT complete successfully
```
",False,"[-3.32484722e-01 -2.70985126e-01 -5.90382628e-02  8.34722742e-02
  1.58658013e-01 -3.95178258e-01 -3.17151040e-01  1.64228499e-01
 -3.84407997e-01 -2.95773029e-01 -1.80482000e-01 -1.73380136e-01
 -2.50061274e-01 -9.17623192e-02 -3.69357318e-01  3.74262989e-01
 -1.68876201e-01 -1.95232332e-01  2.70383656e-01  3.24105769e-01
 -2.26339951e-01 -1.52900606e-01 -3.76362026e-01  2.41499037e-01
  3.70015711e-01  1.72905490e-01 -1.96824446e-01  8.75348598e-02
  1.11614011e-01  3.31613958e-01  2.39395052e-01  1.52486367e-02
  9.98032689e-02  1.44170105e-01  2.43738219e-01  2.65790932e-02
  6.12083822e-04 -3.48444670e-01 -4.62784469e-01 -1.56438023e-01
 -9.27775279e-02  9.06380415e-02  7.73780197e-02 -2.11730808e-01
  1.55985117e-01 -2.33086318e-01  3.16828191e-02 -2.07312137e-01
 -1.25578031e-01 -8.75977874e-02 -1.59318030e-01  1.04283467e-01
 -2.28948653e-01 -5.12297630e-01 -2.26921797e-01  1.10401381e-02
  5.70354871e-02  1.69728622e-01 -3.63148041e-02  3.04499894e-01
  2.49495655e-01 -2.32090488e-01  1.48706675e-01 -6.99335188e-02
  5.29591367e-02  2.53120661e-01  2.10423291e-01 -5.96622527e-02
  4.23917353e-01 -3.35281819e-01  6.45024329e-02 -1.42482847e-01
 -3.44258577e-01 -2.10843086e-02  1.86753035e-01  1.69419646e-01
  2.57572770e-01 -1.43672600e-01  2.97178060e-01 -1.60144776e-01
  6.86868727e-02 -1.60854235e-01 -3.26242484e-02 -8.38503242e-04
  3.22682232e-01 -3.59684378e-02  2.34800279e-01  1.61855519e-01
  4.44189191e-01 -3.48938346e-01  4.03397352e-01  3.51744592e-01
  1.51187360e-01  2.17376634e-01  5.77626944e-01  6.26478940e-02
  1.71139061e-01  1.54298186e-01 -2.51206476e-02 -1.03577524e-01
 -1.84744596e-01 -2.20560476e-01  5.53805828e-02  1.26793832e-02
 -9.95548144e-02 -2.66151428e-01  4.66425270e-01  8.08912888e-02
 -4.05107625e-03 -1.01665914e-01  2.63579309e-01  1.92217305e-01
  6.70360774e-02 -7.44353309e-02 -1.50804162e-01  4.63005416e-02
 -3.35985988e-01  1.07486740e-01 -1.71724670e-02  5.94507933e-01
 -9.85035598e-02 -3.18519413e-01 -9.09472536e-03  4.11166251e-02
  4.37628180e-01 -6.91193119e-02 -1.69621035e-01 -3.00869197e-02
  7.51758367e-02 -5.38450181e-02  1.53202683e-01  1.01081833e-01
  2.64140546e-01  1.01032324e-01  1.46449924e-01  2.15120047e-01
 -2.29590088e-01 -2.04672709e-01 -2.06489801e-01 -2.89662659e-01
 -2.96426713e-01  3.35589051e-01  5.35929166e-02 -5.39906979e-01
 -2.44755410e-02  1.25883833e-01 -2.05853194e-01  2.10166305e-01
 -4.85687405e-02  9.13283601e-02  1.73864275e-01  1.24826819e-01
 -1.91010594e-01  3.07159066e-01  1.93678647e-01  4.74151932e-02
  2.61798263e-01 -1.98714472e-02 -5.20615354e-02 -6.03745043e-01
  4.00512945e-03  3.25764745e-01 -6.89103305e-02 -5.58040403e-02
  4.73289490e-02  2.77441084e-01 -4.65337515e-01 -1.38988853e-01
  1.14362255e-01  5.06189466e-01 -4.00191620e-02 -2.36408234e-01
  2.43989125e-01  8.59445706e-02  3.11037660e-01 -1.98305041e-01
  1.38071612e-01 -1.19701162e-01 -9.96977538e-02  2.05892712e-01
  9.15391669e-02  1.61264971e-01  2.44224191e-01  6.27329499e-02
  1.28335088e-01  2.84442127e-01  1.95781186e-01  2.55090594e-01
 -2.03337371e-01  5.01548313e-02 -4.31372076e-01  1.15424082e-01
  3.13542575e-01 -1.73749626e-02 -1.28635287e-01  1.57692477e-01
  1.48258239e-01 -6.85171559e-02  6.77962601e-03  1.31693155e-01
 -1.73047602e-01  7.25365430e-03 -1.22616068e-04  5.33917844e-02
  1.55780196e-01 -3.00845832e-01 -1.43081009e-01 -3.15551937e-01
 -3.59182745e-01 -7.58060664e-02  1.18062906e-01 -3.14370453e-01
 -1.36920691e-01 -8.83303024e-03 -5.75254373e-02  2.59388447e-01
  3.41142602e-02  3.32066193e-02 -1.07742772e-01 -3.82487616e-03
  2.80766003e-03 -1.31743103e-01 -1.38286084e-01 -4.22664076e-01
 -4.09783721e-02 -6.04067091e-03 -1.60977110e-01 -1.27571315e-01
 -1.74870566e-01  5.77708557e-02  1.21542677e-01  8.30917731e-02
  3.48446637e-01  8.24438035e-02  5.93005300e-01 -1.71635851e-01
 -4.75103967e-02 -1.90157264e-01 -1.92615792e-01  5.36840744e-02
 -3.66099834e-01 -2.40591973e-01 -1.49294764e-01 -2.58277096e-02
  4.74570334e-01  4.32252139e-01 -1.53439436e-02 -2.28961349e-01
 -1.55392095e-01  2.31036305e-01 -1.62776917e-01  1.28254920e-01
  3.34325373e-01  5.08182906e-02  9.78098959e-02  4.21440750e-01
  1.70769900e-01  1.08337119e-01  2.19720930e-01 -5.72841391e-02
  3.87207001e-01  1.51557714e-01  3.20306644e-02  1.30040854e-01
  1.37291282e-01  3.39818001e-01 -3.29030722e-01  4.55390573e-01
  7.92794377e-02  8.79551023e-02  2.88865387e-01 -4.39463615e-01
  5.40777683e-01 -4.66211319e-01 -1.88802928e-02  1.32669985e-01
  4.08023238e-01  1.29624709e-01  2.78743282e-02  3.72475013e-04
  1.45024627e-01  4.45837200e-01 -4.26677227e-01 -2.36467347e-02
  1.07675746e-01 -1.88952371e-01 -2.83308744e-01 -5.86386919e-01
 -1.38134286e-01  1.30403653e-01 -1.69246897e-01 -1.12091608e-01
 -2.30816931e-01  7.13179633e-02 -3.35579276e-01  1.01241972e-02
  2.05194168e-02 -1.43193781e-01 -1.03982024e-01 -1.22866750e-01
 -3.85397196e-01 -1.69659182e-02  2.75526047e-01 -3.78484279e-01
 -1.29430771e-01 -3.64517212e-01  3.69077533e-01  3.29830408e-01
  5.28543055e-01 -4.38551724e-01  2.65105247e-01 -1.60343558e-01
 -2.75058120e-01  5.39442897e-01 -7.84380138e-02 -4.38606292e-02
 -4.09027308e-01  5.56886911e-01  1.33838207e-01 -1.67556792e-01
  2.03594625e-01 -3.72480959e-01 -3.00435483e-01  1.32738292e-01
  2.85323501e-01  4.61798906e-02 -4.44543421e-01 -2.00441867e-01
  1.14262357e-01  2.32605904e-01 -7.38942623e-03 -1.55630382e-02
 -1.34831458e-01  5.87899834e-02 -3.54910910e-01 -1.23314425e-01
 -3.50909501e-01  2.30783284e-01  4.05351408e-02 -3.22467327e-01
 -2.94634759e-01 -1.75959751e-01 -6.61810786e-02 -3.06930453e-01
  8.01027268e-02 -1.92045495e-01  3.17070335e-01  2.38919258e-01
  2.00013220e-01  1.56032726e-01 -4.65906784e-02  1.31098658e-01
 -3.08536887e-01 -4.87834811e-02 -2.87961304e-01  2.86207676e-01
  2.54018247e-01  4.23033796e-02  2.39214167e-01  1.02556899e-01
  2.33102627e-02  1.59482211e-01 -4.66185659e-01  1.78234316e-02
  3.03363681e-01 -1.76143289e-01 -3.10555279e-01 -6.32006228e-02
  1.03622209e-02  8.55151042e-02  6.10358529e-02  1.23214737e-01
 -2.68817812e-01  3.17891836e-01  3.40547323e-01 -2.83494800e-01
 -2.98351824e-01  2.42190044e-02  9.48051661e-02  1.11141980e-01
  1.23655900e-01 -5.70926256e-02  2.48991549e-01 -6.41836375e-02]"
