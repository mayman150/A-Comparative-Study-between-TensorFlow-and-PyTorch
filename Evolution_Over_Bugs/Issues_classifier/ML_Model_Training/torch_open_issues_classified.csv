Issue Number,Issue Title,Time created,Time closed,Number of Assignees,Number of Comments,Tags,Final_Is_Bug
114968,Missing `.so` files when installing PyTorch 1.11.0 for PyTorch3D with Python 3.8  oncall: pt2,2023-12-01 18:20:37+00:00,,0,0,oncall: pt2,False
114967,Inplace update to buffers doesn't work with `aot_compile` ,2023-12-01 18:10:19+00:00,,0,0,,False
114966,[dynamo] dynamo does not support dataclasses with `frozen=True` triaged module: dynamo,2023-12-01 18:04:35+00:00,,0,0,triaged module: dynamo,False
114964,"[dynamo] missing support for function `object.__setattr__(obj, name, value)` triaged module: dynamo",2023-12-01 18:00:39+00:00,,0,0,triaged module: dynamo,False
114963,"[dynamo] missing support for builtin function `dict.fromkeys(iterable, value=None, /)` triaged module: dynamo",2023-12-01 17:58:57+00:00,,0,0,triaged module: dynamo,False
114962,`CUDAExtension` no longer works with `ccache` ,2023-12-01 17:48:02+00:00,,0,0,,False
114954,Add dynamic shape tests for important models to guard against regression module: onnx triaged onnx-triaged,2023-12-01 16:18:29+00:00,,0,0,module: onnx triaged onnx-triaged,True
114951,Unexpected poor performance of C++ extension / wish for a fast `operator[]` module: cpp triaged,2023-12-01 14:08:18+00:00,,0,1,module: cpp triaged,True
114949,"Nvidia P100, where to disable upcasting? Plus kernel image missing. module: binaries module: cuda triaged",2023-12-01 12:18:13+00:00,,0,2,module: binaries module: cuda triaged,True
114948,conda package _openmp_mutex  makes pytorch dataloader slower when set num_workers > 0 module: binaries triaged,2023-12-01 12:08:53+00:00,,0,2,module: binaries triaged,True
114945,torch.multinomial - Unexpected (incorrect) results when replacement=True in version 2.1.1+cpu triaged module: random,2023-12-01 10:50:33+00:00,,0,1,triaged module: random,True
114943,torch.einsum may choose a strategy for which there is not enough memory module: cuda module: memory usage triaged,2023-12-01 10:24:55+00:00,,0,0,module: cuda module: memory usage triaged,True
114942,[MPS only] failed assertion `New volume: xxx should match old volume: xxx [reshapeWithCommandBuffer] MPSNDArrayIdentity.' module: crash triaged module: mps,2023-12-01 10:10:57+00:00,,0,0,module: crash triaged module: mps,True
114938,torch.compile is not working with torchaudio.functional.lfilter oncall: pt2,2023-12-01 08:42:07+00:00,,0,0,oncall: pt2,False
114936,[PT2.1][torch.compile] empty shape in tracing oncall: pt2,2023-12-01 07:04:13+00:00,,0,0,oncall: pt2,False
114935,print statements used in torch/utils/cpp_extension.py module: cpp-extensions triaged better-engineering actionable,2023-12-01 06:51:51+00:00,,0,1,module: cpp-extensions triaged better-engineering actionable,True
114934,get an approximate value to the memory usage for both inferance and training   module: memory usage oncall: pt2,2023-12-01 06:38:20+00:00,,0,1,module: memory usage oncall: pt2,False
114933,duplicate code in python_arg_parser.cpp  module: internals triaged,2023-12-01 06:27:09+00:00,,0,1,module: internals triaged,True
114928,`torch.nn.utils.rnn.pack_padded_sequence` heap-buffer-overflow module: nn triaged,2023-12-01 03:45:39+00:00,,0,0,module: nn triaged,True
114925,DISABLED test_make_fx_symbolic_exhaustive_out_special_entr_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-12-01 00:57:10+00:00,,0,1,triaged module: flaky-tests skipped module: ProxyTensor,False
114913,[CI] Lintrunner takes 60+ min module: ci triaged module: flaky-tests,2023-12-01 00:16:29+00:00,,0,2,module: ci triaged module: flaky-tests,True
114911,single-batch `torch.bmm` is significantly slower with cuBLAS>12.1.0  high priority triage review module: cuda module: third_party topic: performance,2023-11-30 23:56:42+00:00,,0,5,high priority triage review module: cuda module: third_party topic: performance,True
114904,Validations for 2.1.2 release oncall: releng triaged,2023-11-30 22:05:53+00:00,,0,0,oncall: releng triaged,True
114903,[inductor] `torch.var` unexpectedly returns NaN values on A16 and A2  module: cuda oncall: pt2 module: inductor,2023-11-30 21:57:41+00:00,,0,3,module: cuda oncall: pt2 module: inductor,False
114899,aot_export_module doesn't work with torch.cond export-triage-review oncall: export,2023-11-30 20:57:30+00:00,,1,2,export-triage-review oncall: export,False
114892,ExportedProgram.run_decompistion errors when specifying dim=-1 in torch.ops.aten.scatter_add oncall: export,2023-11-30 19:44:48+00:00,,0,0,oncall: export,False
114888,"PyTorch is super bloated, even the data loader has too many lines of code. needs reproduction triaged",2023-11-30 18:50:05+00:00,,0,2,needs reproduction triaged,False
114884,Undefined reference to ncclCommSplit oncall: distributed module: nccl,2023-11-30 17:31:37+00:00,,0,0,oncall: distributed module: nccl,False
114879,[inductor][cpu]basic_gnn_gin and basic_gnn_sage AMP performance regression module: cpu oncall: pt2 module: inductor,2023-11-30 16:09:17+00:00,,1,2,module: cpu oncall: pt2 module: inductor,False
114877,Sparse CSC | CSR | Tensor Serialization Load Issue module: sparse module: serialization triaged,2023-11-30 15:23:35+00:00,,0,0,module: sparse module: serialization triaged,True
114876,Add support for torch.cat on nested_tensor triaged module: nestedtensor,2023-11-30 15:23:18+00:00,,0,1,triaged module: nestedtensor,False
114875,[inductor][cpu] few models show performance regression on CPP Wrapper module: cpu oncall: pt2 module: inductor module: cpu inductor,2023-11-30 14:54:06+00:00,,0,3,module: cpu oncall: pt2 module: inductor module: cpu inductor,False
114874,nn.LSTM tolerates wrong input shape when hidden state isn't provided. module: nn triaged,2023-11-30 14:44:17+00:00,,0,0,module: nn triaged,True
114873,[inductor][cpu] hf_T5_generate and doctr_det_predictor failed on CPP Wrapper Dynamic Shape module: cpu oncall: pt2 module: inductor,2023-11-30 14:42:05+00:00,,1,3,module: cpu oncall: pt2 module: inductor,False
114871,Sparse block tensors (`torch.sparse`) module: sparse feature triaged,2023-11-30 14:14:19+00:00,,0,0,module: sparse feature triaged,True
114869,[inductor][cpu] llama and pytorch_stargan failed on CPP Wrapper module: cpu oncall: pt2 module: inductor,2023-11-30 13:59:42+00:00,,1,3,module: cpu oncall: pt2 module: inductor,False
114868,test_vmapvjpvjp_linalg_tensorsolve_cpu_float32 fails with precision issue triaged module: functorch,2023-11-30 12:59:01+00:00,,0,0,triaged module: functorch,True
114864,`torch.Tensor.index_add` segfault by negative-size-param triaged actionable module: python frontend module: edge cases,2023-11-30 11:34:57+00:00,,1,1,triaged actionable module: python frontend module: edge cases,True
114861,Torchrun threads with different computing speed. oncall: distributed,2023-11-30 10:05:50+00:00,,0,2,oncall: distributed,False
114859,PT2 QAT flow fails to get reasonable accuracy with  mobilenet_v3_large oncall: quantization oncall: pt2,2023-11-30 09:11:12+00:00,,0,0,oncall: quantization oncall: pt2,False
114857,recursive search for DDP ignored parameters oncall: distributed,2023-11-30 08:34:56+00:00,,0,0,oncall: distributed,False
114856,[RFC] Intel GPU Inductor backend upstreaming module: intel oncall: pt2 module: inductor,2023-11-30 08:25:11+00:00,,3,0,module: intel oncall: pt2 module: inductor,False
114855,SparseTensor index select uses more CUDA memory than Torch index select module: sparse module: memory usage triaged,2023-11-30 08:19:42+00:00,,0,0,module: sparse module: memory usage triaged,True
114850,[RFC] Add Intel GPU support into PyTorch CI/CD  oncall: releng module: intel,2023-11-30 07:15:58+00:00,,0,0,oncall: releng module: intel,False
114848,[RFC]Intel GPU oneDNN Upstreaming triaged module: intel,2023-11-30 06:47:36+00:00,,0,0,triaged module: intel,True
114844,CUDA graph doesn't update tensor on replay triaged module: cuda graphs,2023-11-30 05:33:46+00:00,,0,0,triaged module: cuda graphs,True
114842,[RFC] Intel GPU Runtime Upstreaming triaged module: intel,2023-11-30 05:05:21+00:00,,0,0,triaged module: intel,True
114840,MacOS tests randomly fail after https://github.com/pytorch/pytorch/commit/165f4f6ccf7522d75df99c30821d583dfc58ad62 high priority triage review module: ci module: macos,2023-11-30 05:01:36+00:00,,0,1,high priority triage review module: ci module: macos,True
114835,[RFC] Building system for SYCL and limited number of SYCL kernels for ATen fallbacks of TorchInductor feature module: intel oncall: pt2 module: inductor,2023-11-30 03:44:16+00:00,,0,0,feature module: intel oncall: pt2 module: inductor,False
114834,DISABLED test_grad_nn_functional_conv3d_cuda_float32 (__main__.TestOperatorsCUDA) module: autograd module: nn triaged module: flaky-tests skipped,2023-11-30 03:40:18+00:00,,0,1,module: autograd module: nn triaged module: flaky-tests skipped,False
114833,DISABLED test_fn_gradgrad_linalg_lu_cuda_complex128 (__main__.TestBwdGradientsCUDA) module: autograd triaged module: flaky-tests module: linear algebra skipped,2023-11-30 03:40:16+00:00,,0,1,module: autograd triaged module: flaky-tests module: linear algebra skipped,False
114832,DISABLED test_fn_gradgrad_linalg_lu_factor_ex_cuda_float64 (__main__.TestBwdGradientsCUDA) module: autograd triaged module: flaky-tests module: linear algebra skipped,2023-11-30 03:40:12+00:00,,0,1,module: autograd triaged module: flaky-tests module: linear algebra skipped,False
114831,DISABLED test_torch_name_rule_map_updated (__main__.TraceRuleTests) triaged module: flaky-tests skipped module: dynamo,2023-11-30 03:40:12+00:00,,0,3,triaged module: flaky-tests skipped module: dynamo,False
114808,Efficient Cholesky and QR updates module: sparse triaged,2023-11-29 23:26:11+00:00,,0,0,module: sparse triaged,True
114807,Addition of Sparse and Dense Tensors Triggers Internal Assertion Failure module: sparse triaged,2023-11-29 23:19:28+00:00,,0,0,module: sparse triaged,True
114805,Unsupported: call_method UserDefinedObjectVariable(_profiler_enabled) __call__ [] {} triaged module: graph breaks,2023-11-29 22:51:45+00:00,,0,0,triaged module: graph breaks,True
114801,torch.onnx.export to support opset 20.  module: onnx triaged onnx-triaged,2023-11-29 22:09:51+00:00,,0,0,module: onnx triaged onnx-triaged,False
114796,Add head_mask for transformers module: nn triaged oncall: transformer/mha,2023-11-29 21:09:10+00:00,,0,0,module: nn triaged oncall: transformer/mha,True
114786,[ONNX] Refactor op level_debug to catch mismatches between ONNX models and ExportedProgram and nn.Module module: onnx triaged onnx-triaged,2023-11-29 20:00:15+00:00,,2,0,module: onnx triaged onnx-triaged,True
114783,[ONNX] Extend `test_fx_op_conistency.py` to take `ExportedProgram` converter module: onnx triaged onnx-triaged,2023-11-29 19:55:21+00:00,,0,0,module: onnx triaged onnx-triaged,True
114777,torch.compile x autograd.Function x enum inputs graph breaks triage review triaged oncall: pt2 module: dynamo module: graph breaks,2023-11-29 19:20:04+00:00,,0,1,triage review triaged oncall: pt2 module: dynamo module: graph breaks,True
114765,DistributedDataParallel _verify_param_shape_across_processes causes reserved memory on GPU oncall: distributed,2023-11-29 17:59:33+00:00,,0,0,oncall: distributed,False
114760,[AOTAutograd] nit `assert_functional_graph` is misnamed triaged module: aotdispatch,2023-11-29 15:41:44+00:00,,0,0,triaged module: aotdispatch,True
114752,Issues when trying to compile nn.functional.interpolate() layer with dynamic input and output size triaged oncall: pt2 module: dynamic shapes module: dynamo,2023-11-29 12:40:36+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes module: dynamo,False
114750,"[aarch64] `nn.Linear(20, 1)` inference fails high priority triage review oncall: jit module: nn module: mkldnn module: regression module: arm",2023-11-29 12:27:31+00:00,,0,12,high priority triage review oncall: jit module: nn module: mkldnn module: regression module: arm,True
114743,aot_module fails to trace modules whose parameters are tied (models with shared params) triaged oncall: pt2 module: pt2-dispatcher,2023-11-29 08:18:14+00:00,,0,1,triaged oncall: pt2 module: pt2-dispatcher,True
114742,[Add][compile] Tensor with dtype torch.int64 is not the expected dtype of torch.int32! needs reproduction triaged oncall: pt2 module: dynamo,2023-11-29 07:33:27+00:00,,0,2,needs reproduction triaged oncall: pt2 module: dynamo,False
114740,torch.cuda.amp.common.amp_definitely_not_available() failed and needs to raise RuntimeError triaged module: amp (automated mixed precision),2023-11-29 06:27:51+00:00,,0,1,triaged module: amp (automated mixed precision),True
114723,[RFC] Intel GPU Upstreaming  triage review module: intel module: inductor,2023-11-29 01:48:29+00:00,,4,1,triage review module: intel module: inductor,True
114721,AppleSilicon binaries are build without OpenMP support high priority triage review oncall: releng,2023-11-29 01:32:16+00:00,,1,5,high priority triage review oncall: releng,True
114719,DISABLED test_make_fx_symbolic_exhaustive_out_special_log_ndtr_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-11-29 00:56:53+00:00,,0,1,triaged module: flaky-tests skipped module: ProxyTensor,False
114691,`torch.from_numpy` does not support `set_default_device` module: docs triaged actionable module: tensor creation,2023-11-28 19:29:02+00:00,,0,4,module: docs triaged actionable module: tensor creation,True
114681,DISABLED test_sync_batch_norm_empty_input (__main__.DistributedDataParallelTest) oncall: distributed module: flaky-tests skipped,2023-11-28 15:39:39+00:00,,0,1,oncall: distributed module: flaky-tests skipped,False
114680,pytorch cpuinfo submodule to be updated to the latest module: build triaged,2023-11-28 14:07:46+00:00,,0,2,module: build triaged,True
114679,Unexpected Results in PyTorch Tensor Operations with Python Scalars triaged module: numpy,2023-11-28 13:59:12+00:00,,0,0,triaged module: numpy,True
114677,"Upgrading from 1.8.1 to 1.13.0 causes exported ONNX file enlarged greatly, printable graph changed, and crashed when converting to TensorRT module: onnx triaged",2023-11-28 09:22:00+00:00,,0,0,module: onnx triaged,True
114676,'SymFloat' object has no attribute 'is_integer' good first issue triaged oncall: pt2 module: dynamic shapes,2023-11-28 09:19:34+00:00,,1,0,good first issue triaged oncall: pt2 module: dynamic shapes,True
114674,Dynamo guards key error for `guarded_backend_cache.cached_backends` triaged oncall: pt2 module: dynamo,2023-11-28 08:57:21+00:00,,1,4,triaged oncall: pt2 module: dynamo,True
114672,torch.fft.ifft crashes for empty input triaged module: fft,2023-11-28 07:52:42+00:00,,0,0,triaged module: fft,True
114670,DISABLED test_backward_nn_functional_conv3d_cuda_float32 (__main__.TestCompositeComplianceCUDA) module: autograd module: nn triaged module: flaky-tests skipped,2023-11-28 06:40:17+00:00,,0,1,module: autograd module: nn triaged module: flaky-tests skipped,False
114665,DISABLED test_cuda_stream_method_dynamic_shapes (__main__.DynamicShapesCtxManagerTests) triaged skipped module: dynamo,2023-11-28 05:00:25+00:00,,0,1,triaged skipped module: dynamo,False
114656,DISABLED test_conj_view_nn_functional_conv3d_cuda_complex64 (__main__.TestMathBitsCUDA) module: nn triaged module: flaky-tests skipped,2023-11-28 00:57:42+00:00,,0,1,module: nn triaged module: flaky-tests skipped,False
114655,DISABLED test_noncontiguous_samples_nn_functional_conv3d_cuda_float32 (__main__.TestCommonCUDA) module: rocm triaged module: flaky-tests skipped module: unknown,2023-11-28 00:57:12+00:00,,0,1,module: rocm triaged module: flaky-tests skipped module: unknown,False
114654,DISABLED test_jvpvjp_nn_functional_conv3d_cuda_float32 (__main__.TestOperatorsCUDA) triaged module: flaky-tests skipped module: functorch,2023-11-28 00:57:09+00:00,,0,2,triaged module: flaky-tests skipped module: functorch,False
114652,__torch_dispatch__ fails on functionalization when lazy device and `out=` arg is used triaged module: __torch_dispatch__ module: lazy module: functionalization,2023-11-28 00:55:28+00:00,,0,1,triaged module: __torch_dispatch__ module: lazy module: functionalization,True
114650,Error when calculating the Jacobian of torch.conj using forward-mode differentiation module: autograd triaged module: complex has workaround needs design,2023-11-28 00:35:47+00:00,,0,2,module: autograd triaged module: complex has workaround needs design,True
114635,[export] AssertionError: expected FunctionType found method <bound method QuantLinearConvBase.quantized_forward of <class '__main__.QuantConv2d'>> oncall: export,2023-11-27 22:32:00+00:00,,0,0,oncall: export,False
114632,[DeviceMesh] back DeviceMesh initialization by custom_pg oncall: distributed triaged,2023-11-27 22:02:09+00:00,,0,0,oncall: distributed triaged,True
114631,DISABLED test_vjpvmap_nn_functional_conv3d_cuda_float32 (__main__.TestOperatorsCUDA) triaged module: flaky-tests skipped,2023-11-27 21:42:45+00:00,,0,5,triaged module: flaky-tests skipped,False
114629,[DeviceMesh] `init_device_mesh` does not support CPU-only oncall: distributed triaged,2023-11-27 21:16:45+00:00,,0,0,oncall: distributed triaged,False
114628,`run_decompositions` in `ExportedProgram` doesn't keep the same model state_dict oncall: export,2023-11-27 21:14:06+00:00,,0,2,oncall: export,False
114617,RNN argument order module: nn triaged actionable,2023-11-27 18:53:24+00:00,,1,5,module: nn triaged actionable,True
114602,[RFC] macOS x86 builds / test deprecation module: binaries module: ci triaged module: macos,2023-11-27 17:19:21+00:00,,0,4,module: binaries module: ci triaged module: macos,True
114592,DISABLED test_variant_consistency_eager_nn_functional_conv3d_cuda_complex64 (__main__.TestCommonCUDA) triaged module: flaky-tests skipped module: unknown,2023-11-27 09:39:36+00:00,,0,1,triaged module: flaky-tests skipped module: unknown,False
114591,Missing packaging dependency in torch 2.1.x oncall: releng triaged module: tensorboard,2023-11-27 09:20:17+00:00,,0,3,oncall: releng triaged module: tensorboard,True
114590,`torch.compiler.disable` causes a guard failure triaged oncall: pt2 module: dynamo,2023-11-27 09:12:26+00:00,,0,0,triaged oncall: pt2 module: dynamo,True
114581,Flawed testing of onesidedness in istft triaged module: fft,2023-11-27 05:12:17+00:00,,0,0,triaged module: fft,True
114577,torch.norm_except_dim() operator: Cannot call sizes() on tensor with symbolic sizes/strides  triaged actionable module: norms and normalization oncall: pt2,2023-11-27 04:03:15+00:00,,0,1,triaged actionable module: norms and normalization oncall: pt2,False
114574,DISABLED test_schema_correctness_nn_functional_conv3d_cuda_complex64 (__main__.TestSchemaCheckModeOpInfoCUDA) oncall: jit module: flaky-tests skipped,2023-11-27 03:39:27+00:00,,0,2,oncall: jit module: flaky-tests skipped,False
114573,DISABLED test_schema_correctness_nn_functional_conv3d_cuda_complex128 (__main__.TestSchemaCheckModeOpInfoCUDA) oncall: jit module: flaky-tests skipped,2023-11-27 03:39:23+00:00,,0,2,oncall: jit module: flaky-tests skipped,False
114572,"In version 2.1, libtorch needs to be woken up every time it is called after the model is initialized, which means that every time the model is called, it is very slow to predict the first picture. needs reproduction module: cpp triaged",2023-11-27 02:58:12+00:00,,0,1,needs reproduction module: cpp triaged,True
114571,inconsistency between nan cast to int32 on CPU and GPU module: cuda triaged module: NaNs and Infs,2023-11-27 02:29:03+00:00,,0,1,module: cuda triaged module: NaNs and Infs,True
114569,inconsistency on torch.clamp module: cuda triaged module: NaNs and Infs,2023-11-27 02:04:29+00:00,,0,1,module: cuda triaged module: NaNs and Infs,True
114567,Internal CI for libTorch module: windows triaged,2023-11-27 01:01:48+00:00,,1,3,module: windows triaged,True
114566,Build LibTorch for Windows ARM64 module: windows triaged,2023-11-27 00:56:51+00:00,,1,2,module: windows triaged,True
114565,Create a reproducible build for LibTorch x64 on VS2022 module: windows triaged,2023-11-27 00:50:02+00:00,,1,2,module: windows triaged,True
114548,[BE / AOTAutograd] AOTAutograd should be refactored into smaller files with clear responsibilities triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-11-26 10:41:58+00:00,,0,8,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
114543,[torch.jit.script] Torchscript produces incorrect result when argmax result is used in indexing oncall: jit,2023-11-26 08:12:49+00:00,,0,1,oncall: jit,True
114542,size error when using bits-level ops + broadcasting + view triaged oncall: pt2,2023-11-26 03:29:45+00:00,,0,1,triaged oncall: pt2,True
114535,[profiling] CUDA memory viz does not compose with `torch.compile` oncall: profiler,2023-11-25 14:02:07+00:00,,0,1,oncall: profiler,False
114534,`PYTORCH_NO_CUDA_MEMORY_CACHING=1` with `torch.multiprocessing` shared tensors seems to perform use-after-free triage review module: multiprocessing module: cuda,2023-11-25 13:47:37+00:00,,0,0,triage review module: multiprocessing module: cuda,True
114533,Tensor copied over to multiple GPUs on its own needs reproduction module: cuda module: memory usage triaged,2023-11-25 08:45:32+00:00,,0,4,needs reproduction module: cuda module: memory usage triaged,True
114529,Bug in element-wise multiplication of `torch.sparse_csr_tensor`s on GPU - 0's in result considered significant - PyTorch 2.1.1 module: sparse triaged module: regression,2023-11-25 05:43:02+00:00,,0,7,module: sparse triaged module: regression,True
114527,"[debugging] Given a flag, `FakeTensor`s should store metadata about their creation stacktrace  triage review triaged enhancement oncall: pt2 module: fakeTensor module: pt2-dispatcher",2023-11-25 04:10:08+00:00,,0,0,triage review triaged enhancement oncall: pt2 module: fakeTensor module: pt2-dispatcher,True
114525,[Inductor] StableDiffusion unet with `cudagraphs` backend raises fake tensor mismatch error triaged module: cuda graphs oncall: pt2 module: inductor,2023-11-25 03:13:36+00:00,,1,3,triaged module: cuda graphs oncall: pt2 module: inductor,True
114517,Update error message on cache size exceeded to mention whether it's from `cache_size_limit` or `accumulated_cache_size_limit` triaged oncall: pt2,2023-11-24 19:18:18+00:00,,0,1,triaged oncall: pt2,True
114516,Change `automatic_dynamic_shapes` to trigger on `cache_size_limit` recompiles but not `accumulated_cache_size_limit` recompiles. triage review triaged oncall: pt2 module: dynamic shapes module: dynamo,2023-11-24 19:16:13+00:00,,0,1,triage review triaged oncall: pt2 module: dynamic shapes module: dynamo,True
114511,"Raise `torch._dynamo.config.accumulated_cache_size_limit` higher, or potentially just remove it altogether. triage review triaged oncall: pt2 module: dynamo",2023-11-24 17:18:04+00:00,,0,3,triage review triaged oncall: pt2 module: dynamo,True
114502,_foreach_supported_types is a list type while it is used for `in` check of _default_to_fused_or_foreach method in optimizer.py module: optimizer triaged,2023-11-24 09:15:16+00:00,,0,1,module: optimizer triaged,True
114495,[inductor][cpu]basic_gnn_edgecnn performance regression module: performance oncall: pt2 module: cpu inductor,2023-11-24 03:54:46+00:00,,1,6,module: performance oncall: pt2 module: cpu inductor,False
114494,[inductor][cpu]llama fp32 dynamic single thread performance regression module: performance oncall: pt2 module: cpu inductor,2023-11-24 03:22:59+00:00,,1,3,module: performance oncall: pt2 module: cpu inductor,False
114491,[inductor][cpu]pyhpc_isoneutral_mixing performance regression module: performance oncall: pt2 module: cpu inductor,2023-11-24 02:40:15+00:00,,1,2,module: performance oncall: pt2 module: cpu inductor,False
114489,[inductor][cpu]pyhpc_equation_of_state performance regression module: performance oncall: pt2 module: cpu inductor,2023-11-24 01:56:29+00:00,,1,2,module: performance oncall: pt2 module: cpu inductor,False
114488,[inductor][cpu]swin_base_patch4_window7_224 AMP single thread performance regression module: performance oncall: pt2 module: cpu inductor,2023-11-24 01:30:08+00:00,,2,2,module: performance oncall: pt2 module: cpu inductor,False
114485,Parameters between models don't copy in the C++ Pytroch Frontend under windows module: windows module: cpp triaged,2023-11-23 22:51:15+00:00,,0,0,module: windows module: cpp triaged,True
114483,dynamo supports Tensor.tolist but not Tensor.item oncall: pt2 module: dynamo,2023-11-23 20:55:35+00:00,,0,0,oncall: pt2 module: dynamo,False
114473,[inductor][cpu]cspdarknet53 and tf_mixnet_l AMP single thread performance regression module: performance oncall: pt2 module: cpu inductor,2023-11-23 18:24:43+00:00,,1,2,module: performance oncall: pt2 module: cpu inductor,False
114468,[inductor][cpu]phi_1_5 accuracy failure and AMP single thread performance regression module: performance oncall: pt2 module: cpu inductor,2023-11-23 16:10:38+00:00,,1,10,module: performance oncall: pt2 module: cpu inductor,True
114466,[inductor][cpu]detectron2_fcos_r_50_fpn accuracy and performance failure module: performance oncall: pt2 module: cpu inductor,2023-11-23 15:41:38+00:00,,1,3,module: performance oncall: pt2 module: cpu inductor,True
114465,Installation error: 'CMake Error at third_party/benchmark/CMakeLists.txt:304 (message):   Failed to determine the source files for the regular expression backend' needs reproduction module: build triaged,2023-11-23 15:16:10+00:00,,0,11,needs reproduction module: build triaged,True
114464,inductor silently ignores clone followed by a reshape oncall: pt2,2023-11-23 14:08:17+00:00,,0,4,oncall: pt2,False
114462,Transformer with convolutional position wise feed forward network  module: nn triaged needs research,2023-11-23 13:14:53+00:00,,0,1,module: nn triaged needs research,True
114461,DISABLED test_make_fx_symbolic_exhaustive_special_laguerre_polynomial_l_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-11-23 12:45:46+00:00,,0,1,triaged module: flaky-tests skipped module: ProxyTensor,False
114458,The behavior of Using different device in Autocast context  triaged module: amp (automated mixed precision),2023-11-23 11:28:34+00:00,,0,2,triaged module: amp (automated mixed precision),True
114455,Torch Cpu Memory Leak with FastApi uvicorn. needs reproduction module: memory usage triaged,2023-11-23 09:36:16+00:00,,0,5,needs reproduction module: memory usage triaged,True
114450,[AOTInductor] Need support to export freezing model oncall: quantization feature oncall: pt2,2023-11-23 07:38:23+00:00,,1,2,oncall: quantization feature oncall: pt2,False
114440,There seems to be redundant calling of findOp in findSchemaOrThrow method of Dispatcher.cpp triaged module: dispatch,2023-11-23 04:09:16+00:00,,0,1,triaged module: dispatch,True
114439,DISABLED test_variant_consistency_eager_linalg_lu_factor_cuda_float32 (__main__.TestCommonCUDA) triaged module: flaky-tests skipped module: unknown,2023-11-23 03:39:29+00:00,,0,1,triaged module: flaky-tests skipped module: unknown,False
114415,AOTAutograd should detect aliasing of inputs that happens *below* subclasses module: __torch_dispatch__ oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-11-22 22:36:03+00:00,,0,0,module: __torch_dispatch__ oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
114414,[BE] remove SmallVector optimization in PyInterpreter.cpp when storing custom sizes triaged module: __torch_dispatch__ module: dynamic shapes,2023-11-22 22:28:48+00:00,,0,0,triaged module: __torch_dispatch__ module: dynamic shapes,True
114413,Support sending __torch_dispatch__ subclasses in torch.compile that do not desugar module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,2023-11-22 22:22:49+00:00,,0,2,module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,False
114412,torch.compile <> __torch_dispatch__ support tracker issue module: __torch_dispatch__ oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-11-22 22:18:44+00:00,,0,1,module: __torch_dispatch__ oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
114411,higher order ops + torch.compile + __torch_dispatch__ subclasses module: __torch_dispatch__ oncall: pt2 module: higher order operators module: pt2-dispatcher,2023-11-22 22:18:17+00:00,,0,0,module: __torch_dispatch__ oncall: pt2 module: higher order operators module: pt2-dispatcher,False
114410,__torch_dispatch__ + compile: backward guards module: __torch_dispatch__ oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-11-22 22:08:43+00:00,,0,3,module: __torch_dispatch__ oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
114408,DISABLED test_make_fx_symbolic_exhaustive_special_i1_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-11-22 21:39:21+00:00,,0,2,triaged module: flaky-tests skipped module: ProxyTensor,False
114406,torch.onnx.dynamo_export functionalization does not support aten.add_.Tensor module: onnx triaged onnx-triaged release notes: onnx,2023-11-22 21:36:22+00:00,,0,0,module: onnx triaged onnx-triaged release notes: onnx,False
114405,__torch_dispatch__ + compile: extra guards high priority triaged module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,2023-11-22 21:34:37+00:00,,0,4,high priority triaged module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,True
114403,refactor TracingContext to take a more limited subset of ViewAndMutationMeta good first issue oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-11-22 21:23:34+00:00,,0,5,good first issue oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
114401,torch.compile + subclasses: input is duplicate of another input triaged enhancement module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,2023-11-22 21:18:36+00:00,,0,0,triaged enhancement module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,False
114400,torch.compile + subclasses: input aliases another input triaged module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,2023-11-22 21:17:15+00:00,,0,0,triaged module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,False
114399,"[BE] AOTAutograd, refactor `run_functionalized_fw_and_collect_metadata` to not take in `is_training` flag. triaged better-engineering oncall: pt2 module: aotdispatch module: pt2-dispatcher",2023-11-22 21:15:47+00:00,,1,0,triaged better-engineering oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
114398,[BE] better testing for subclasses + compile triaged better-engineering module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,2023-11-22 21:10:29+00:00,,0,2,triaged better-engineering module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,True
114397,__torch_dispatch__ + compile: run functionalization before the subclass module: __torch_dispatch__ module: functionalization oncall: pt2 module: pt2-dispatcher,2023-11-22 21:01:50+00:00,,0,2,module: __torch_dispatch__ module: functionalization oncall: pt2 module: pt2-dispatcher,False
114396,is PGNCCL abortCommsFromMap working correctly? oncall: distributed module: nccl,2023-11-22 20:56:02+00:00,,0,0,oncall: distributed module: nccl,False
114392,[BUG][pytree] equal `dict`s do not imply equal leaves and equal treespecs triaged module: pytree,2023-11-22 19:53:50+00:00,,1,0,triaged module: pytree,True
114389,__torch_dispatch__ + torch.compile: support custom methods/constructor calls module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,2023-11-22 18:58:36+00:00,,0,1,module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,False
114388,Respect user-specified USE_ROCM/USE_CUDA module: build module: rocm triaged enhancement,2023-11-22 18:53:35+00:00,,0,1,module: build module: rocm triaged enhancement,True
114374,dynamic shapes support for __torch_dispatch__ subclasses + torch.compile module: __torch_dispatch__ oncall: pt2 module: dynamic shapes module: pt2-dispatcher,2023-11-22 17:49:56+00:00,,0,7,module: __torch_dispatch__ oncall: pt2 module: dynamic shapes module: pt2-dispatcher,False
114373,torch.compile support for SeamlessExpressivity/SeamlessM4T in fairseq2 oncall: pt2,2023-11-22 17:37:51+00:00,,0,0,oncall: pt2,False
114369,__torch_function_ subclasses do not work with dynamic shapes module: __torch_function__ oncall: pt2 module: dynamic shapes,2023-11-22 16:51:02+00:00,,1,1,module: __torch_function__ oncall: pt2 module: dynamic shapes,False
114366,TypeError: unhashable type: non-singleton SymInt in AOTAutograd merge_view_inputs triaged oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,2023-11-22 16:17:30+00:00,,0,0,triaged oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,True
114364,Improve usability of CUDA package by adding description of CUDA module: docs triaged enhancement,2023-11-22 15:54:13+00:00,,0,0,module: docs triaged enhancement,True
114363,Register Meta func for aten::_cslt_sparse_mm module: sparse triaged,2023-11-22 15:53:35+00:00,,1,1,module: sparse triaged,True
114361,Custom Process Group for Each Module in FSDP triaged module: fsdp,2023-11-22 15:12:38+00:00,,0,4,triaged module: fsdp,True
114357,Pytorch DDP across nodes: self._store = TCPStore( # type: ignore[call-arg] RuntimeError: Stop_waiting response is expected oncall: distributed triaged,2023-11-22 14:01:39+00:00,,0,1,oncall: distributed triaged,True
114356,"[WARNING] could not load None, generating random data instead is too spammy triaged module: minifier",2023-11-22 13:58:29+00:00,,0,0,triaged module: minifier,True
114353,add support for linalg lstsq in dynamo triaged oncall: pt2 release notes: dynamo,2023-11-22 13:02:49+00:00,,0,3,triaged oncall: pt2 release notes: dynamo,False
114352,DISABLED test_make_fx_symbolic_exhaustive_out_special_laguerre_polynomial_l_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: unknown,2023-11-22 12:46:30+00:00,,0,1,triaged module: flaky-tests skipped module: unknown,False
114350,The 'out'-parameter in torch.matmul() works for 'cuda' device but not for 'cpu' module: cpu module: error checking triaged module: correctness (silent) module: intel,2023-11-22 10:58:34+00:00,,0,14,module: cpu module: error checking triaged module: correctness (silent) module: intel,True
114346,[PT2] Compile Cold Start - Async JIT compile with Eager fallback triaged oncall: pt2 module: dynamo,2023-11-22 09:30:58+00:00,,0,7,triaged oncall: pt2 module: dynamo,False
114345,Incorrect line in description of torch.frombuffer() method module: docs triaged actionable module: python frontend,2023-11-22 09:12:20+00:00,,0,2,module: docs triaged actionable module: python frontend,True
114344,[AOTAutograd] Incorrect CSE aliasing while `requires_grad` meta differs oncall: pt2,2023-11-22 08:19:37+00:00,,0,2,oncall: pt2,True
114340,DISABLED test_use_orig_params (__main__.TestFSDPOptimState) triaged module: flaky-tests skipped module: unknown,2023-11-22 06:40:27+00:00,,0,1,triaged module: flaky-tests skipped module: unknown,False
114339,DISABLED test_make_fx_symbolic_exhaustive_out_special_i1_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: unknown,2023-11-22 06:40:24+00:00,,0,1,triaged module: flaky-tests skipped module: unknown,False
114338,[AOTAutograd] torch.compile under ambient `no_grad` is broken high priority triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-11-22 05:00:24+00:00,,1,10,high priority triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
114334,`AdaptiveLogSoftmaxWithLoss` division by zero module: cpp module: nn module: error checking triaged actionable,2023-11-22 04:27:06+00:00,,1,1,module: cpp module: nn module: error checking triaged actionable,True
114332,DISABLED test_make_fx_symbolic_exhaustive_special_hermite_polynomial_h_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: unknown,2023-11-22 03:40:37+00:00,,0,2,triaged module: flaky-tests skipped module: unknown,False
114320,"When cuda graph captures multi-stream function, allocations are not serviced from the correct pool triaged module: cuda graphs",2023-11-22 00:01:36+00:00,,1,3,triaged module: cuda graphs,True
114310,Support builtin round in torch.compile with dynamic shapes triaged oncall: pt2 module: dynamic shapes,2023-11-21 22:58:56+00:00,,1,3,triaged oncall: pt2 module: dynamic shapes,False
114306,Support calling torch.vmap from inside torch.compile triaged oncall: pt2 module: functorch module: pt2-dispatcher,2023-11-21 22:06:22+00:00,,1,0,triaged oncall: pt2 module: functorch module: pt2-dispatcher,False
114302,Mutation after `tensor.expand` returns wrong result. high priority triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-11-21 20:25:11+00:00,,1,10,high priority triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
114301,Timeout of the coalesced operation cannot be detected oncall: distributed,2023-11-21 20:14:11+00:00,,0,0,oncall: distributed,False
114299,Per-Parameter-Sharding FSDP Tracker triaged module: fsdp,2023-11-21 20:01:48+00:00,,2,9,triaged module: fsdp,True
114298,Compile pytorch for ppc64 redhat8 module: build triaged module: POWER,2023-11-21 19:51:55+00:00,,0,13,module: build triaged module: POWER,True
114296,Minifier doesn't work with dynamic shapes triaged oncall: pt2 module: dynamic shapes module: minifier,2023-11-21 19:47:58+00:00,,0,0,triaged oncall: pt2 module: dynamic shapes module: minifier,True
114293,Recompilation triggered at each step of the loop involving array indexing triaged oncall: pt2 module: dynamic shapes,2023-11-21 19:08:11+00:00,,0,12,triaged oncall: pt2 module: dynamic shapes,True
114292,"Tracing per-param sharding FSDP: User Defined Objects as inputs to HOPs (autograd.Function, specifically)  triaged module: fsdp oncall: pt2 module: higher order operators module: distributed module: compiled autograd module: pt2-dispatcher",2023-11-21 18:49:02+00:00,,0,5,triaged module: fsdp oncall: pt2 module: higher order operators module: distributed module: compiled autograd module: pt2-dispatcher,True
114291,"Tracing per-param sharding FSDP: Tensor keys in dicts over-rely on `specialized_value`, potentially soundness bug triaged module: fsdp oncall: pt2 module: dynamo module: distributed",2023-11-21 18:46:15+00:00,,0,0,triaged module: fsdp oncall: pt2 module: dynamo module: distributed,True
114290,Tracing per-param sharding FSDP: RemovableHandle -> RemovableHandleVariable  module: nn triaged module: fsdp oncall: pt2 module: distributed,2023-11-21 18:43:58+00:00,,0,1,module: nn triaged module: fsdp oncall: pt2 module: distributed,True
114289,"Tracing per-param sharding FSDP: Streams, Stream reconstruction  triaged module: fsdp oncall: pt2 module: dynamo module: distributed",2023-11-21 18:38:51+00:00,,0,0,triaged module: fsdp oncall: pt2 module: dynamo module: distributed,True
114288,Tracing per-param sharding FSDP: Dynamo tracing weakrefs oncall: distributed triaged oncall: pt2,2023-11-21 18:36:19+00:00,,0,0,oncall: distributed triaged oncall: pt2,False
114287,Align on the minimum supported Linux version (CentOS 7 is EOL in july 2024) module: cuda oncall: releng triaged,2023-11-21 18:36:02+00:00,,0,0,module: cuda oncall: releng triaged,True
114286,Tracing per-param sharding FSDP oncall: distributed triaged module: fsdp oncall: pt2 module: dynamo module: distributed,2023-11-21 18:35:58+00:00,,0,0,oncall: distributed triaged module: fsdp oncall: pt2 module: dynamo module: distributed,True
114285,torch.linalg.matrix_rank fails on mps triaged module: mps,2023-11-21 18:04:11+00:00,,0,0,triaged module: mps,True
114283,CppException in Android Studio with faster_rcnn_fbnetv3g_fpn.yaml Configuration oncall: mobile,2023-11-21 17:09:12+00:00,,0,0,oncall: mobile,False
114232,manylinux_2_28 support module: build triaged enhancement,2023-11-21 13:09:43+00:00,,0,3,module: build triaged enhancement,True
114231,inductor with dynamic shapes fails on float tensor creation from a tuple of ints high priority triaged module: correctness (silent) oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,2023-11-21 13:02:08+00:00,,1,4,high priority triaged module: correctness (silent) oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,True
114230,DISABLED test_make_fx_symbolic_exhaustive_special_chebyshev_polynomial_u_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-11-21 12:46:30+00:00,,0,1,triaged module: flaky-tests skipped module: ProxyTensor,False
114226,[Inductor][gpu][miscompile] Outputs of torch.interpolate abnormally change when swapping output sequence module: cuda triaged module: correctness (silent) bug oncall: pt2 module: inductor topic: fuzzer,2023-11-21 12:28:02+00:00,,0,0,module: cuda triaged module: correctness (silent) bug oncall: pt2 module: inductor topic: fuzzer,True
114220,boolean masking creates a graph break due to aten.nonzero triaged oncall: pt2 module: dynamic shapes,2023-11-21 10:42:47+00:00,,1,1,triaged oncall: pt2 module: dynamic shapes,True
114216,torch.nn.functional.max_pool2d outputs inf module: nn module: error checking triaged module: edge cases,2023-11-21 08:11:06+00:00,,1,1,module: nn module: error checking triaged module: edge cases,True
114215,Outputs of torch.mul abnormally change when swapping the input args of torch.mul triaged oncall: pt2 topic: fuzzer,2023-11-21 08:08:08+00:00,,0,0,triaged oncall: pt2 topic: fuzzer,False
114212,DISABLED test_make_fx_symbolic_exhaustive_special_hermite_polynomial_he_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-11-21 06:40:18+00:00,,0,3,triaged module: flaky-tests skipped module: ProxyTensor,False
114211,DISABLED test_make_fx_symbolic_exhaustive_special_chebyshev_polynomial_t_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-11-21 06:40:18+00:00,,0,3,triaged module: flaky-tests skipped module: ProxyTensor,False
114207,`torch.gradient` heap buffer overflow high priority triaged actionable module: python frontend,2023-11-21 06:06:25+00:00,,0,2,high priority triaged actionable module: python frontend,True
114206,[PT2] Compile Cold Start (Persistent Cacheing) - AOTAutograd may be bottleneck when `TORCHINDUCTOR_FX_GRAPH_CACHE=1` module: performance triaged topic: performance oncall: pt2 module: aotdispatch module: dynamo module: startup-tracing-compile time module: pt2-dispatcher,2023-11-21 06:00:24+00:00,,0,8,module: performance triaged topic: performance oncall: pt2 module: aotdispatch module: dynamo module: startup-tracing-compile time module: pt2-dispatcher,False
114203,[inductor] huggingface diffusers randn not replaced triaged bug oncall: pt2 module: decompositions module: inductor,2023-11-21 04:13:59+00:00,,0,2,triaged bug oncall: pt2 module: decompositions module: inductor,True
114202,torch._dynamo.exc.Unsupported: call_method UserDefinedObjectVariable(FrozenDict) __contains__ [ConstantVariable(str)] {} triaged oncall: pt2 module: dynamo,2023-11-21 04:00:41+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
114195,Remove temp variable to improve efficiency in `get_data_ptrs` for `TensorIterator` module: performance triaged module: TensorIterator,2023-11-21 01:41:24+00:00,,0,0,module: performance triaged module: TensorIterator,True
114191,DISABLED test_closure_out_of_scope_cell_with_cond (__main__.MiscTests) module: rocm triaged module: flaky-tests skipped module: dynamo,2023-11-21 00:57:08+00:00,,0,1,module: rocm triaged module: flaky-tests skipped module: dynamo,False
114189,Custom backend not called for compiling backward graph triaged module: backend oncall: pt2,2023-11-20 23:58:25+00:00,,0,5,triaged module: backend oncall: pt2,True
114188,Unsupported operator error: `aten::to_mkldnn` export to ONNX not supported module: onnx triaged module: mkldnn onnx-triaged onnx-needs-info,2023-11-20 23:38:35+00:00,,0,2,module: onnx triaged module: mkldnn onnx-triaged onnx-needs-info,True
114179,funccol collectives rewrite in dynamo does not work w/ kwargs oncall: distributed good first issue triaged oncall: pt2 module: dynamo,2023-11-20 22:26:27+00:00,,0,5,oncall: distributed good first issue triaged oncall: pt2 module: dynamo,True
114175,Unable to build on CUDA 11.8 due to cutlass incompatibility module: build module: cuda triaged,2023-11-20 21:51:58+00:00,,0,1,module: build module: cuda triaged,True
114171,"Indexing into torch.nn.Modulelist with indices >= 2 returns ""torch._dynamo.exc.InternalTorchDynamoError: SymNodeVariable() is not a constant"" triaged oncall: pt2 module: dynamic shapes",2023-11-20 21:28:24+00:00,,1,2,triaged oncall: pt2 module: dynamic shapes,True
114165,Consider adding y/x -> y * 1/x optimization for `_foreach_div_.ScalarList` and other div Scalar overloads module: optimizer triaged actionable module: mta,2023-11-20 20:12:01+00:00,,0,1,module: optimizer triaged actionable module: mta,True
114155,Lintrunner on all files fails locally even though it passes in CI module: lint triaged,2023-11-20 19:13:03+00:00,,0,1,module: lint triaged,True
114153,[Tracker] Inconsistencies between CPU and GPU computation module: numerical-stability module: cuda triaged,2023-11-20 19:01:37+00:00,,0,1,module: numerical-stability module: cuda triaged,True
114142,Mutating graph inputs on torch.export fails triaged export-triage-review oncall: export,2023-11-20 17:43:54+00:00,,0,19,triaged export-triage-review oncall: export,True
114139,test_arange_dynamic doesn't work with capture scalar outputs triaged bug oncall: pt2 module: dynamic shapes module: inductor,2023-11-20 17:18:03+00:00,,0,0,triaged bug oncall: pt2 module: dynamic shapes module: inductor,True
114136,torch.vmap doesn't compose with torch.cond triaged module: vmap oncall: pt2 module: functorch module: higher order operators module: pt2-dispatcher,2023-11-20 17:14:14+00:00,,1,2,triaged module: vmap oncall: pt2 module: functorch module: higher order operators module: pt2-dispatcher,True
114131,Inconsistency of state_dict loading across devices module: nn triaged,2023-11-20 15:54:02+00:00,,0,3,module: nn triaged,True
114123,Cannot fullgraph differentiate through boolean masking triaged module: boolean tensor oncall: pt2 module: dynamic shapes,2023-11-20 14:38:36+00:00,,0,0,triaged module: boolean tensor oncall: pt2 module: dynamic shapes,True
114119,Performance of `torch.compile` is significantly slowed down under `torch.inference_mode` module: performance triaged has workaround ezyang's list inference mode oncall: pt2,2023-11-20 13:33:35+00:00,,0,3,module: performance triaged has workaround ezyang's list inference mode oncall: pt2,False
114114,Why there is no nn.LPPool3d? module: nn triaged enhancement actionable,2023-11-20 11:38:34+00:00,,0,3,module: nn triaged enhancement actionable,True
114113,Typo in example of torch.linalg.solve_triangular module: docs triaged module: linear algebra actionable,2023-11-20 11:23:52+00:00,,0,2,module: docs triaged module: linear algebra actionable,True
114112,[JIT] - torch.script - 'Optional[Tensor]' object has no attribute or method 'size' oncall: jit,2023-11-20 10:15:43+00:00,,0,0,oncall: jit,True
114109,`torch.compile()` makes loss `nan` triaged oncall: pt2 module: pt2 accuracy,2023-11-20 09:15:40+00:00,,0,6,triaged oncall: pt2 module: pt2 accuracy,False
114107,Add the possibility to pass a `Generator` to `gumbel_softmax` module: nn triaged enhancement module: random,2023-11-20 08:45:17+00:00,,0,1,module: nn triaged enhancement module: random,True
114105,Unexpected `None` value for stream with dynamo module: cuda triaged oncall: pt2 module: dynamo,2023-11-20 07:52:40+00:00,,0,2,module: cuda triaged oncall: pt2 module: dynamo,True
114102,"""aten::prelu"" can not be used in the metal backend oncall: mobile",2023-11-20 07:25:32+00:00,,0,1,oncall: mobile,False
114097,[RFC] Support for Redundant Hosts in TorchElastic oncall: distributed module: elastic,2023-11-20 06:40:11+00:00,,4,1,oncall: distributed module: elastic,False
114093,Found nn.LazyBatchNorm1d(0) has inconsistency bug between GPU and CPU testing module: nn triaged,2023-11-20 02:35:26+00:00,,0,2,module: nn triaged,True
114090,Nested tensors fail on Conv2D triaged module: nestedtensor,2023-11-20 02:26:16+00:00,,0,1,triaged module: nestedtensor,True
114086,exit code -1073740791 (0xC0000409) when torch.package.PackageExporter module: windows oncall: package/deploy,2023-11-20 01:11:43+00:00,,0,0,module: windows oncall: package/deploy,False
114085,inconsistency in torch.Tensor.scatter on GPU and CPU module: numerical-stability triaged,2023-11-20 01:05:07+00:00,,1,4,module: numerical-stability triaged,True
114080, torch.matrix_exp(x) get inf and nan module: cuda module: cpu triaged,2023-11-20 00:23:04+00:00,,0,1,module: cuda module: cpu triaged,True
114072,Dynamic shapes not properly supported for nested tensor / tensor subclasses triaged module: nestedtensor tensor subclass module: dynamic shapes,2023-11-19 21:09:58+00:00,,0,1,triaged module: nestedtensor tensor subclass module: dynamic shapes,True
114070,[inductor] Assert that Inductor preserves output strides if `TORCHINDUCTOR_LAYOUT_OPTIMIZATION=0` feature triaged oncall: pt2 module: inductor,2023-11-19 20:39:00+00:00,,0,0,feature triaged oncall: pt2 module: inductor,False
114064,New Optimizer  module: optimizer triaged,2023-11-19 16:08:50+00:00,,0,4,module: optimizer triaged,False
114048,Undocumented CUDA graphs requirement that kernels must use stream triaged module: custom-operators module: cuda graphs,2023-11-19 03:21:20+00:00,,0,3,triaged module: custom-operators module: cuda graphs,True
114044,Nan on torch.corrcoef(x.t()) triaged,2023-11-19 02:48:24+00:00,,0,1,triaged,True
114042,Poetry Torch 2.1.1+cu121 problem on Windows module: binaries module: windows triaged,2023-11-19 01:58:07+00:00,,1,12,module: binaries module: windows triaged,True
114040,IndexError: map::at with MPI CUDA collectives module: multiprocessing module: cuda triaged,2023-11-19 00:50:05+00:00,,0,0,module: multiprocessing module: cuda triaged,True
114035,Load model from jit script format. Repeating inference several times can lead to errors. oncall: jit,2023-11-19 00:13:39+00:00,,0,2,oncall: jit,True
114005,[pt2] Make error message clearer for torch.compile when running on windows module: windows triaged oncall: pt2,2023-11-17 23:49:18+00:00,,0,4,module: windows triaged oncall: pt2,True
113985,[export] Helper function for specifying dynamic batch size triaged export-triage-review oncall: export,2023-11-17 21:09:29+00:00,,0,2,triaged export-triage-review oncall: export,False
113962,[v2.1.2] Release Tracker triaged,2023-11-17 17:46:03+00:00,,0,15,triaged,True
113956,iSTFT gives wrong results for some batched input module: cuda triaged module: correctness (silent) module: fft,2023-11-17 16:20:30+00:00,,0,3,module: cuda triaged module: correctness (silent) module: fft,True
113953,DISABLED test_lazy_clone_cuda_bfloat16 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-11-17 15:41:17+00:00,,0,2,module: tests triaged module: flaky-tests skipped oncall: pt2,False
113952,AOTAutograd silently drops required runtime assertions triaged module: dynamic shapes,2023-11-17 15:12:01+00:00,,0,0,triaged module: dynamic shapes,True
113948,Unknown CUDA Architecture Name 9.0a in CUDA_SELECT_NVCC_ARCH_FLAGS (compiling from source) module: build module: cuda triaged,2023-11-17 12:57:22+00:00,,0,0,module: build module: cuda triaged,True
113946,DISABLED test_dynamic_kwarg (__main__.SubGraphTests) triaged module: flaky-tests skipped oncall: pt2,2023-11-17 12:46:18+00:00,,0,3,triaged module: flaky-tests skipped oncall: pt2,False
113945,torch.jit.trace has incorrect execution for += operation during compilation oncall: jit,2023-11-17 12:25:52+00:00,,0,9,oncall: jit,True
113942,Tensorboard list of tensors as input triaged module: tensorboard,2023-11-17 10:26:02+00:00,,0,1,triaged module: tensorboard,True
113941,"libtorch_cpu.so: Undefined symbol ""__assert_fail"" module: binaries triaged",2023-11-17 10:02:42+00:00,,0,0,module: binaries triaged,True
113938,deep copy 'HigherOrderOperator' bug triaged module: higher order operators,2023-11-17 09:44:18+00:00,,0,1,triaged module: higher order operators,True
113937,DISABLED test_distributed_checkpoint_state_dict_type0 (__main__.TestDistributedCheckpoint) oncall: distributed module: flaky-tests skipped,2023-11-17 09:39:56+00:00,,0,2,oncall: distributed module: flaky-tests skipped,False
113936,DISABLED test_distributed_checkpoint_state_dict_type1 (__main__.TestDistributedCheckpoint) oncall: distributed module: flaky-tests skipped,2023-11-17 09:39:53+00:00,,0,2,oncall: distributed module: flaky-tests skipped,False
113933,How to  re-use torch.compile results in different python processes? high priority triaged oncall: pt2 module: dynamic shapes module: dynamo,2023-11-17 08:22:11+00:00,,1,11,high priority triaged oncall: pt2 module: dynamic shapes module: dynamo,True
113914,Register a torch op for `functorch.experimental.control_flow.map` to lower triaged module: xla oncall: pt2 module: functorch module: pt2-dispatcher,2023-11-17 02:37:50+00:00,,0,1,triaged module: xla oncall: pt2 module: functorch module: pt2-dispatcher,True
113909,DISABLED test_cuda_stream_method (__main__.CtxManagerTests) triaged module: flaky-tests skipped module: dynamo,2023-11-17 00:57:16+00:00,,0,1,triaged module: flaky-tests skipped module: dynamo,False
113908,DISABLED test_fn_fwgrad_bwgrad_linalg_lu_factor_ex_cuda_complex128 (__main__.TestFwdGradientsCUDA) module: autograd triaged module: flaky-tests skipped,2023-11-17 00:57:08+00:00,,0,1,module: autograd triaged module: flaky-tests skipped,False
113905,Sympy reasoning with true division is broken module: dependency bug triaged,2023-11-17 00:26:07+00:00,,0,6,module: dependency bug triaged,True
113899,Tensor.requires_grad_() does not trace with make_fx triaged oncall: pt2 module: ProxyTensor pre_dispatch tracing module: pt2-dispatcher,2023-11-16 22:28:20+00:00,,0,5,triaged oncall: pt2 module: ProxyTensor pre_dispatch tracing module: pt2-dispatcher,True
113895,Native c10d_functional collectives on inductor + CUDAGraphTrees generate wrong results triaged module: c10d bug oncall: pt2 module: inductor,2023-11-16 21:24:14+00:00,,1,6,triaged module: c10d bug oncall: pt2 module: inductor,True
113879,Tracking: Improve SymNode/Dynamic Shapes Logging triaged oncall: pt2 module: dynamic shapes,2023-11-16 17:00:05+00:00,,0,0,triaged oncall: pt2 module: dynamic shapes,False
113876,Fine grained SymNode logging good first issue triaged oncall: pt2 module: dynamic shapes,2023-11-16 16:39:10+00:00,,0,1,good first issue triaged oncall: pt2 module: dynamic shapes,True
113868,Torchrun distribute training on Windows WSL oncall: distributed,2023-11-16 14:49:12+00:00,,0,0,oncall: distributed,False
113860,torch.fx export graph error  triaged oncall: pt2 oncall: export,2023-11-16 12:00:31+00:00,,0,2,triaged oncall: pt2 oncall: export,True
113858,Why the cuda peak memory usage increase after dist.reuduce_scatter operation ?  oncall: distributed,2023-11-16 09:41:28+00:00,,0,0,oncall: distributed,False
113857,DISABLED test_make_fx_symbolic_exhaustive_out_special_chebyshev_polynomial_u_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-11-16 09:39:47+00:00,,0,4,triaged module: flaky-tests skipped module: ProxyTensor,False
113854,Timer: Add Standard Deviation for Single Measurement oncall: profiler module: benchmark topic: new features,2023-11-16 08:51:20+00:00,,0,0,oncall: profiler module: benchmark topic: new features,False
113850,DISABLED test_vmapvjpvjp_linalg_lu_factor_cuda_float32 (__main__.TestOperatorsCUDA) triaged module: flaky-tests skipped module: functorch,2023-11-16 06:40:30+00:00,,0,1,triaged module: flaky-tests skipped module: functorch,False
113847,RuntimeError in use torch 2.1.0 cuda 11.8  module: cuda triaged,2023-11-16 06:20:20+00:00,,0,0,module: cuda triaged,True
113840,torch.einsum is stuck in mp.Process needs reproduction module: multiprocessing triaged,2023-11-16 04:06:21+00:00,,0,2,needs reproduction module: multiprocessing triaged,True
113836,Can reducer provide a copy_hook? oncall: distributed,2023-11-16 03:39:05+00:00,,0,0,oncall: distributed,False
113833,cannot backprop a cnn when intermediate output has size larger than 2**31 module: cuda triaged module: 64-bit actionable module: edge cases,2023-11-16 03:11:58+00:00,,0,6,module: cuda triaged module: 64-bit actionable module: edge cases,True
113831,Update _scaled_mm to support addmm with bias.size == out.size triaged module: float8,2023-11-16 02:31:52+00:00,,1,0,triaged module: float8,True
113824,DISABLED test_vjpvmap_nn_functional_conv_transpose3d_cuda_float32 (__main__.TestOperatorsCUDA) module: rocm triaged module: flaky-tests skipped module: functorch,2023-11-16 00:57:38+00:00,,0,1,module: rocm triaged module: flaky-tests skipped module: functorch,False
113817,[ONNX] ONNX export of simple quantized model fails module: onnx low priority triaged,2023-11-15 23:05:50+00:00,,1,1,module: onnx low priority triaged,True
113809,"Error during DDP, torch.compile, and cudagraph_trees oncall: distributed triaged oncall: pt2 module: inductor",2023-11-15 22:07:42+00:00,,1,3,oncall: distributed triaged oncall: pt2 module: inductor,True
113808,[ONNX][dynamo_export] ONNX::Celu Half unsupported but export passed w/ invalid model when opmath disabled module: onnx triaged onnx-triaged,2023-11-15 21:53:39+00:00,,0,6,module: onnx triaged onnx-triaged,False
113806,DISABLED test_make_fx_symbolic_exhaustive_out_special_chebyshev_polynomial_t_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-11-15 21:39:53+00:00,,0,4,triaged module: flaky-tests skipped module: ProxyTensor,False
113800,InternalTorchDynamoError rewrapping loses exception chaining triaged oncall: pt2 module: dynamo,2023-11-15 20:51:20+00:00,,0,0,triaged oncall: pt2 module: dynamo,True
113798,consistency checks for across minor version builds module: binaries module: build triaged,2023-11-15 20:04:39+00:00,,0,7,module: binaries module: build triaged,True
113794,[FSDP] Raise error when applying FSDP to `nn.ModuleList` or `nn.ModuleDict` triaged module: fsdp,2023-11-15 19:26:30+00:00,,0,0,triaged module: fsdp,True
113793,`torch.compile` fails when applied to tensor views that have been modified by in-place operators triaged oncall: pt2,2023-11-15 19:13:33+00:00,,0,1,triaged oncall: pt2,True
113792,[export] Export warnings as no-ops triaged oncall: pt2 module: dynamo,2023-11-15 19:12:14+00:00,,1,5,triaged oncall: pt2 module: dynamo,False
113788,torch.compile CUBLAS_STATUS_EXECUTION_FAILED high priority triaged module: ddp oncall: pt2 module: inductor module: distributed,2023-11-15 19:03:14+00:00,,1,3,high priority triaged module: ddp oncall: pt2 module: inductor module: distributed,True
113786,torch.compile CUDNN_STATUS_MAPPING_ERROR high priority triaged module: ddp oncall: pt2 module: inductor module: distributed,2023-11-15 19:00:43+00:00,,1,3,high priority triaged module: ddp oncall: pt2 module: inductor module: distributed,True
113778,Unsupported builtin operators for torch.export.export high priority triaged export-triaged oncall: export,2023-11-15 17:46:44+00:00,,1,4,high priority triaged export-triaged oncall: export,True
113776,[cusparseLt] CUDA error: internal error when calling `cusparseLtStructuredDescriptorInit` module: cuda triaged,2023-11-15 17:44:50+00:00,,1,3,module: cuda triaged,True
113773,RoCm support loop unrolling for `at::native::gpu_kernel_multiple_outputs` module: rocm triaged ciflow/rocm,2023-11-15 16:44:20+00:00,,0,1,module: rocm triaged ciflow/rocm,False
113761,[DTensor][TP] 1-way TP raises index out of range triaged better-engineering module: dtensor,2023-11-15 13:55:46+00:00,,1,1,triaged better-engineering module: dtensor,True
113760,torch.profiler Trace view in Tensorboard is displayed as empty on RoCm version of PyTorch module: rocm low priority triaged module: tensorboard oncall: profiler,2023-11-15 13:55:16+00:00,,0,8,module: rocm low priority triaged module: tensorboard oncall: profiler,True
113755,Upsample trilinear onnx  module: onnx triaged,2023-11-15 12:35:57+00:00,,0,4,module: onnx triaged,False
113752,device_mesh documentation in FSDP ctor oncall: distributed module: fsdp,2023-11-15 11:53:18+00:00,,1,1,oncall: distributed module: fsdp,False
113751,"Cannot cast tensor to numpy array inside vmap due to ""Access data pointer of tensor that doesn't have storage"" triaged module: numpy module: vmap",2023-11-15 10:55:05+00:00,,0,4,triaged module: numpy module: vmap,True
113750,[docs] Add reference/decomp impl snippets to functions in online docs for hackability and educational purposes / compensate for some unclear language in existing docs module: docs triaged better-engineering module: norms and normalization,2023-11-15 10:15:08+00:00,,0,3,module: docs triaged better-engineering module: norms and normalization,False
113748,New feature: Balanced Sampler module: dataloader triaged module: data,2023-11-15 09:32:32+00:00,,0,0,module: dataloader triaged module: data,True
113744,Regression: `capture_pre_autograd_graph` does not support empty args and kwargs only anymore triaged oncall: pt2 oncall: export,2023-11-15 08:49:07+00:00,,1,1,triaged oncall: pt2 oncall: export,False
113743,The cuda batched GEMM has a poor performance for bigger batch size with smaller matrix size module: performance module: cuda triaged matrix multiplication,2023-11-15 07:05:49+00:00,,0,1,module: performance module: cuda triaged matrix multiplication,True
113741,Plan for transformer module based ROCm module: rocm triaged ciflow/rocm,2023-11-15 06:14:54+00:00,,0,0,module: rocm triaged ciflow/rocm,True
113737,torch.compile + SAC: mutations in backward are not preserved module: checkpoint triaged oncall: pt2 module: aotdispatch module: distributed module: pt2-dispatcher,2023-11-15 04:29:45+00:00,,2,7,module: checkpoint triaged oncall: pt2 module: aotdispatch module: distributed module: pt2-dispatcher,True
113736,DISABLED test_make_fx_symbolic_exhaustive_out_special_airy_ai_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-11-15 03:39:39+00:00,,0,4,triaged module: flaky-tests skipped module: ProxyTensor,False
114604,"lax.cond, lax.switch alternatives triaged module: functorch",2023-11-15 02:22:41+00:00,,0,1,triaged module: functorch,True
113730,Can the official provide cpu offload method for large model inference? feature triaged needs design,2023-11-15 01:40:39+00:00,,0,1,feature triaged needs design,False
113724,[ONNX] Refactor xfail API to handle conditional failure scenarios module: onnx triaged,2023-11-15 00:28:33+00:00,,0,0,module: onnx triaged,True
113717,`preserve_rng_state=True` in torch.utils.checkpoint causes error when used with torch.compile + selective checkpointing + CUDA triaged oncall: pt2,2023-11-15 00:13:36+00:00,,1,0,triaged oncall: pt2,True
113713,[cuDNN][cuDNN V8 API] cuDNN Flash-Attention Upstreaming RFC/tracking issue module: cudnn module: cuda oncall: transformer/mha,2023-11-14 23:21:24+00:00,,1,2,module: cudnn module: cuda oncall: transformer/mha,True
113708,`log_softmax` could be `2**124` to `2**1021` times more accurate on small outputs module: nn triaged module: multi-headed-attention,2023-11-14 22:48:19+00:00,,0,1,module: nn triaged module: multi-headed-attention,True
113707,torch.compile doesnt respect use_determistic_algorithms during the backward() triaged module: determinism oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-11-14 22:47:14+00:00,,1,6,triaged module: determinism oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
113705,[ONNX] Track dynamic shapes integration for torch.onnx.dynamo_export module: onnx triaged onnx-triaged release notes: onnx,2023-11-14 22:38:57+00:00,,2,4,module: onnx triaged onnx-triaged release notes: onnx,False
113700,C++ backtrace logging for dynamic shapes specialization triaged ezyang's list module: dynamic shapes,2023-11-14 22:29:56+00:00,,0,2,triaged ezyang's list module: dynamic shapes,True
113698,verify ROCM profiler behavior listed in https://github.com/pytorch/tutorials/issues/2014 module: rocm triaged ciflow/rocm,2023-11-14 22:24:27+00:00,,1,1,module: rocm triaged ciflow/rocm,False
113692,Removing index_put fallback results in bad C++ code triaged oncall: pt2 module: cpu inductor,2023-11-14 21:41:00+00:00,,0,1,triaged oncall: pt2 module: cpu inductor,False
113691,inductor/test_aot_inductor.py gets segfault but test ultimately passes module: ci triaged oncall: pt2,2023-11-14 21:19:21+00:00,,1,3,module: ci triaged oncall: pt2,False
113687,[ONNX] Execute ONNX Runtime with IOBindings through ONNXProgram.__call__ module: onnx triaged onnx-triaged release notes: onnx,2023-11-14 20:52:16+00:00,,1,0,module: onnx triaged onnx-triaged release notes: onnx,False
113682,[export] Support capturing of non-tensor inputs triaged export-triaged oncall: export,2023-11-14 20:35:49+00:00,,1,3,triaged export-triaged oncall: export,True
113673,Failure to resume from a normal (non-FSDP) checkpoint due to the optimizer state dict rekey module: optimizer triaged module: fsdp,2023-11-14 19:14:44+00:00,,0,2,module: optimizer triaged module: fsdp,True
113670,Implement Variable Tracker for Dataclasses triaged topic: new features module: dynamo,2023-11-14 18:49:03+00:00,,0,0,triaged topic: new features module: dynamo,False
113669,document functional_collectives oncall: distributed module: docs triaged oncall: pt2,2023-11-14 18:40:14+00:00,,0,2,oncall: distributed module: docs triaged oncall: pt2,False
113663,FP8 types should not participate in type promotion and should have no math ops defined on them high priority triaged module: float8,2023-11-14 18:03:48+00:00,,0,6,high priority triaged module: float8,True
113652,[ONNX] ONNX export fails when combining tracing and scripting in the presence of symbolic functions module: onnx triaged module: regression,2023-11-14 16:47:42+00:00,,0,2,module: onnx triaged module: regression,False
113646,Error in torch.set_default_tensor_type() documentation online (depreciated) module: docs triaged actionable,2023-11-14 14:54:59+00:00,,0,3,module: docs triaged actionable,True
113643,Distinguish immutable/mutable fake tensor mode triaged module: fakeTensor module: dynamo,2023-11-14 13:51:14+00:00,,0,2,triaged module: fakeTensor module: dynamo,True
113642,interpolate::trilinear returns wrong gradients on CUDA module: autograd module: cuda triaged,2023-11-14 13:08:05+00:00,,0,0,module: autograd module: cuda triaged,True
113637,Output mismatch of torch.Tensor.to with gpu inductor compiled when swapping output sequence triaged oncall: pt2 topic: fuzzer,2023-11-14 10:07:03+00:00,,0,0,triaged oncall: pt2 topic: fuzzer,False
113623,torch.compile crash on sdxl unet compile with AMD 7900XTX needs reproduction module: rocm triaged oncall: pt2,2023-11-14 06:17:15+00:00,,1,1,needs reproduction module: rocm triaged oncall: pt2,True
113621,App crashes when I attempt to run it with an iOS xcframework that relies on the LibTorch-Lite CocoaPod module: binaries triaged module: ios,2023-11-14 05:10:25+00:00,,0,1,module: binaries triaged module: ios,True
113612,Graph breaks in APEX `FusedRMSNorm` causes bad interaction between NCCL allreduce and cudagraph tree triaged module: ddp module: cuda graphs oncall: pt2 module: dynamo module: distributed,2023-11-14 01:49:24+00:00,,0,0,triaged module: ddp module: cuda graphs oncall: pt2 module: dynamo module: distributed,True
113606,`torch._dynamo.config.suppress_errors` may not be properly reset module: tests triaged oncall: pt2 module: dynamo,2023-11-14 01:02:42+00:00,,1,0,module: tests triaged oncall: pt2 module: dynamo,False
113600,Segmentation fault in RPC worker when DataLoader has num_workers > 0 module: dataloader triaged module: distributed,2023-11-14 00:43:20+00:00,,0,0,module: dataloader triaged module: distributed,True
113590,Implement FlashFFTConv algorithm triaged oncall: transformer/mha module: multi-headed-attention,2023-11-13 22:31:39+00:00,,0,3,triaged oncall: transformer/mha module: multi-headed-attention,True
113589,Fake tensor produces incorrect values w/ is_coalesced and sparse_coo module: sparse triaged module: fakeTensor,2023-11-13 22:27:17+00:00,,0,5,module: sparse triaged module: fakeTensor,True
113587,bsr_dense_mm may produce incorrect results depending on triton kernel num_stages parameter module: sparse triaged module: correctness (silent) bug,2023-11-13 21:37:00+00:00,,1,3,module: sparse triaged module: correctness (silent) bug,True
113586,mps bug: failed assertion `[MPSNDArrayDescriptor sliceDimension:withSubrange:] error: subRange.start (8192) is not less than length of dimension[1] (512)' triaged module: mps,2023-11-13 21:23:33+00:00,,0,0,triaged module: mps,True
113583,"OSS Test Failures surfaced internally, but not detected externally module: ci triaged",2023-11-13 20:25:35+00:00,,1,4,module: ci triaged,True
113579,Creating Gaussian Mixture Models with MultivariateNormal module: distributions module: nn triaged,2023-11-13 19:46:03+00:00,,0,1,module: distributions module: nn triaged,True
113572,Deepcopying an exported model changes numerics for MobileNetV2 triaged export-triage-review oncall: export,2023-11-13 18:24:22+00:00,,0,0,triaged export-triage-review oncall: export,True
113560,t.contiguous() ~10 slower in eager mode compared to torch.compile module: performance module: cuda triaged,2023-11-13 15:03:00+00:00,,0,5,module: performance module: cuda triaged,True
113552,2.1.0 export module：Why can't export() capture reverse graphs? triaged export-triage-review oncall: export,2023-11-13 08:52:38+00:00,,0,1,triaged export-triage-review oncall: export,True
113550,CUDA extension error message doesn't look correct module: cpp-extensions module: cuda triaged,2023-11-13 07:55:52+00:00,,0,0,module: cpp-extensions module: cuda triaged,True
113546,"P0: Improve failure trace back when crashed to identify the cause of a crash and the ranks that the crash, output the real traceback at last. triaged module: c10d module: elastic",2023-11-13 04:32:09+00:00,,0,0,triaged module: c10d module: elastic,True
113545,P0: Logging Granularity checks fixes across torch.distributed + torchelastic launcher triaged module: c10d,2023-11-13 04:32:06+00:00,,0,0,triaged module: c10d,True
113544,"P0: Integrate distributed logger with TORCH_LOGS, make sure upper level library can't override distributed LOG level. triaged module: c10d module: distributed",2023-11-13 04:29:51+00:00,,0,7,triaged module: c10d module: distributed,True
113543,[PTD] Logging Improvements Main Task triaged module: c10d module: distributed,2023-11-13 04:29:38+00:00,,0,0,triaged module: c10d module: distributed,True
113541,Enhanced RNG State Management with Index-Based Control for Graph-Safe Tensor Parallelism module: cuda triaged,2023-11-13 04:04:55+00:00,,0,1,module: cuda triaged,True
113537,torch.compile error triaged oncall: pt2,2023-11-12 16:33:22+00:00,,0,3,triaged oncall: pt2,True
113530,"Make ""torch.load"" multi threaded process module: serialization triaged needs research",2023-11-12 11:44:42+00:00,,0,2,module: serialization triaged needs research,True
113525,[Feature request] `stft` doesn't have `pad_value` argument triaged module: fft,2023-11-11 21:14:30+00:00,,0,0,triaged module: fft,True
113524,_fused_sdp_choice returns wrong backend in pt2 triaged oncall: transformer/mha module: multi-headed-attention,2023-11-11 20:37:35+00:00,,0,3,triaged oncall: transformer/mha module: multi-headed-attention,True
113522,SDPA Tutorial - fails for CPU runs on Google Colab module: docs triaged docathon-h2-2023,2023-11-11 17:11:10+00:00,,1,1,module: docs triaged docathon-h2-2023,False
113521,SDPA Tutorial - libcuda.so not found error for torch compile on Google Colab triaged bug oncall: pt2 upstream triton,2023-11-11 16:55:00+00:00,,0,2,triaged bug oncall: pt2 upstream triton,True
113515,torchrun discarding --rdzv-endpoint when it should not triaged module: elastic,2023-11-11 11:51:06+00:00,,0,0,triaged module: elastic,True
113507,Revisit test_edge_op_registration on PyTorch CI triaged oncall: export,2023-11-11 01:51:00+00:00,,0,0,triaged oncall: export,False
113506,DISABLED test_vit_aten_export (__main__.TestQuantizePT2EModels) triaged oncall: mobile skipped,2023-11-11 01:39:30+00:00,,0,1,triaged oncall: mobile skipped,False
113504,[ONNX] STFT ExportProgram error module: onnx triaged,2023-11-11 00:58:16+00:00,,1,4,module: onnx triaged,True
113496,"FSDP.forward() fails ""_is_root should not have been set"" error after saving a distributed checkpoint triaged module: fsdp module: distributed",2023-11-10 23:17:35+00:00,,0,2,triaged module: fsdp module: distributed,True
113490,/nodefaultlib:vcomp doesn't seem to be set when compiling with Intel OpenMP on Windows module: build module: windows triaged,2023-11-10 21:59:22+00:00,,0,4,module: build module: windows triaged,True
113489,DISABLED test_make_fx_symbolic_exhaustive_round_decimals_3_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-11-10 21:39:45+00:00,,0,16,triaged module: flaky-tests skipped module: ProxyTensor,False
113486,Tensors Can't be  Overwritten in Visual Studio Windows module: crash module: windows module: cpp triaged,2023-11-10 20:31:45+00:00,,0,0,module: crash module: windows module: cpp triaged,True
113483,Should `_native_batch_norm_legit_functional` be in native_functions.yaml? module: nn triaged,2023-11-10 19:26:21+00:00,,0,2,module: nn triaged,True
113479,DISABLED test_make_fx_symbolic_exhaustive_round_decimals_neg_3_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: unknown,2023-11-10 18:40:35+00:00,,0,16,triaged module: flaky-tests skipped module: unknown,False
113478,DISABLED test_make_fx_symbolic_exhaustive_round_decimals_0_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: unknown,2023-11-10 18:39:34+00:00,,0,2,triaged module: flaky-tests skipped module: unknown,False
113456,Expose FakeTensor and FakeTensorMode as public APIs triaged module: fakeTensor,2023-11-10 17:18:00+00:00,,0,3,triaged module: fakeTensor,True
113453,Remove approvals when reverting a pr triaged module: devx,2023-11-10 16:36:39+00:00,,1,4,triaged module: devx,True
113449,cuDNN error: CUDNN_STATUS_MAPPING_ERROR on gtx_1080/A10 when conv1d is called module: cudnn module: cuda triaged,2023-11-10 15:50:39+00:00,,0,0,module: cudnn module: cuda triaged,True
113444,ONNX Export - miscompilation for complex-valued operators module: onnx triaged,2023-11-10 13:53:18+00:00,,1,15,module: onnx triaged,True
113443,`CompileProfiler` reports graph breaks while `dynamo.explain` reports no graph breaks triaged bug oncall: pt2 module: dynamo,2023-11-10 12:32:32+00:00,,0,2,triaged bug oncall: pt2 module: dynamo,True
113440,[inductor][cpu] freezing caused a lot of CV model crashed triaged bug oncall: pt2 module: cpu inductor,2023-11-10 09:48:59+00:00,,1,17,triaged bug oncall: pt2 module: cpu inductor,True
113422,Implement pass-through `state_dict` and `load_state_dict` for dynamo `OptimizedModule` module: serialization triaged oncall: pt2,2023-11-10 03:36:49+00:00,,0,5,module: serialization triaged oncall: pt2,False
113419,code crashes on CI when using keep-going flag on PR triaged module: devx,2023-11-10 02:56:59+00:00,,1,6,triaged module: devx,True
113415,Torch compile with DDP errors on parameterized modules triage review oncall: distributed triaged module: ddp module: nn.utils.parametrize bug oncall: pt2 module: dynamo module: distributed,2023-11-10 02:05:54+00:00,,0,7,triage review oncall: distributed triaged module: ddp module: nn.utils.parametrize bug oncall: pt2 module: dynamo module: distributed,True
113414,Potential Precision Issue in SumKernel CPU Implementation Due to acc_type Selection module: cuda triaged,2023-11-10 02:04:39+00:00,,0,2,module: cuda triaged,True
113408,Early testing stop logic for CUDA error looks wrong for instantiated_test with pytest module: cuda triaged module: testing,2023-11-10 01:59:44+00:00,,0,7,module: cuda triaged module: testing,True
113382,TorchXLA - owner @JackCaoG triaged module: xla,2023-11-09 19:48:17+00:00,,0,0,triaged module: xla,True
113379,Print sources for free variables in dynamic shape logs triaged oncall: pt2 module: dynamic shapes module: dynamo module: guards,2023-11-09 18:43:43+00:00,,0,0,triaged oncall: pt2 module: dynamic shapes module: dynamo module: guards,False
113378,[pytree] implement key path API triaged module: pytree,2023-11-09 18:40:34+00:00,,1,6,triaged module: pytree,True
113376,Add validation for send/recv sizes oncall: distributed,2023-11-09 18:24:12+00:00,,0,6,oncall: distributed,False
113370,Incorrect stride when permuting shapes where a zero dimension is present. triaged module: edge cases module: empty tensor,2023-11-09 17:16:14+00:00,,0,0,triaged module: edge cases module: empty tensor,True
113368,Gradient of image rotation triaged enhancement module: forward ad,2023-11-09 16:40:40+00:00,,0,0,triaged enhancement module: forward ad,True
113342,Torch2.1 returned  by calling new_group() or _get_default_group() is incorrect oncall: distributed,2023-11-09 10:52:49+00:00,,0,0,oncall: distributed,True
113329,Mark aten.normal as a core aten op triaged oncall: pt2,2023-11-09 05:10:39+00:00,,1,4,triaged oncall: pt2,False
113326,`torch._C._cuda_getDeviceCount` inflates system memory usage module: cuda triaged,2023-11-09 02:32:21+00:00,,0,2,module: cuda triaged,True
113291,[aot_autograd] Handle `requires_grad` mutation in AOTAutograd feature module: autograd triaged module: aotdispatch module: pt2-dispatcher,2023-11-08 19:41:34+00:00,,0,0,feature module: autograd triaged module: aotdispatch module: pt2-dispatcher,True
113290,[dynamo / aot_autograd] AOTAutograd does not guard on input `requires_grad` setting changing for backwards module: autograd triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-11-08 19:33:40+00:00,,0,2,module: autograd triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
113287,Provide a way to AOT torch.compile and serialize a model triage review feature triaged oncall: pt2 module: dynamo module: startup-tracing-compile time module: distributed,2023-11-08 18:55:37+00:00,,0,14,triage review feature triaged oncall: pt2 module: dynamo module: startup-tracing-compile time module: distributed,True
113281,NCCL p2p ops hung after communicator aborts oncall: distributed module: c10d,2023-11-08 18:04:13+00:00,,0,3,oncall: distributed module: c10d,False
113271,[dynamo] Assigning result of Tensor in-place op destroys mutation tracking triaged bug oncall: pt2 module: dynamo,2023-11-08 16:37:35+00:00,,0,2,triaged bug oncall: pt2 module: dynamo,True
113263,PT2 improperly executes ambient saved_tensors_hooks triage review module: autograd triaged bug oncall: pt2 module: pt2-dispatcher,2023-11-08 14:59:13+00:00,,0,7,triage review module: autograd triaged bug oncall: pt2 module: pt2-dispatcher,True
113252,DISABLED test_function_returns_input_inner_requires_grad_True_save_for_jvp_save_tensors_neither_mark_dirty_True_cpu (__main__.TestAutogradFunctionCPU) needs reproduction triaged skipped oncall: pt2,2023-11-08 08:00:26+00:00,,0,5,needs reproduction triaged skipped oncall: pt2,False
113249,DISABLED test_reorder_compute_for_overlap (__main__.TestComputeCommReorderingMultiProc) module: rocm triaged skipped,2023-11-08 07:44:02+00:00,,0,1,module: rocm triaged skipped,False
113245,NCCL error of PyTorch 2.1.0 when using multiple gpus oncall: distributed,2023-11-08 06:28:03+00:00,,0,0,oncall: distributed,True
113235,[dynamo] Remove usage of copy_graphstate/restore_graphstate in higher_order_ops.py triaged oncall: pt2 module: dynamo,2023-11-08 03:25:39+00:00,,1,8,triaged oncall: pt2 module: dynamo,False
113232,[Torch Inductor] Torch Inductor Better Support for GNN workload and Inductor Sparse Compiler triage review feature triaged oncall: pt2 module: dynamic shapes module: inductor,2023-11-08 03:02:06+00:00,,0,1,triage review feature triaged oncall: pt2 module: dynamic shapes module: inductor,False
113228,dataloader use lmdb stuck triaged module: deadlock release notes: dataloader,2023-11-08 01:51:22+00:00,,0,0,triaged module: deadlock release notes: dataloader,True
113222,EPOCH_DEPRECATION_WARNING in ChainedScheduler.step triaged module: LrScheduler topic: deprecation,2023-11-08 01:30:09+00:00,,0,0,triaged module: LrScheduler topic: deprecation,True
113221,DISABLED test_disable_fwd_grad_mixed_cpu (__main__.TestJvpCPU) triaged module: flaky-tests skipped oncall: pt2 module: functorch module: pt2-dispatcher,2023-11-08 01:00:40+00:00,,0,3,triaged module: flaky-tests skipped oncall: pt2 module: functorch module: pt2-dispatcher,False
113220,DISABLED test_disable_fwd_grad_inside_cpu (__main__.TestJvpCPU) triaged module: flaky-tests skipped oncall: pt2 module: functorch module: pt2-dispatcher,2023-11-08 00:56:55+00:00,,0,3,triaged module: flaky-tests skipped oncall: pt2 module: functorch module: pt2-dispatcher,False
113203,`inference_mode` before training results in FSDP AssertionError triaged module: fsdp,2023-11-07 21:00:23+00:00,,0,5,triaged module: fsdp,True
113192,"Fix docstring errors in checkpoint_example.py, basic_strategy.py, op_schema.py, contract.py, redistribute.py, __init__.py, api.py, device_mesh.py, _utils.py, sharding_prop.py, random.py, checkpoint_activation.py, placement_types.py, fully_shard.py, parallel_mode.py, replicate.py module: docs triaged medium docathon-h2-2023",2023-11-07 19:46:45+00:00,,1,1,module: docs triaged medium docathon-h2-2023,False
113190,"Fix docstring errors in _state_dict_utils.py, _runtime_utils.py, _shard_utils.py module: docs triaged medium docathon-h2-2023",2023-11-07 19:46:41+00:00,,1,1,module: docs triaged medium docathon-h2-2023,False
113188,"Fix docstring errors in _common_utils.py, _optim_utils.py, _wrap_utils.py, _unshard_param_utils.py, _fsdp_extensions.py, api.py, _debug_utils.py, _utils.py, wrap.py, sharded_grad_scaler.py module: docs triaged medium docathon-h2-2023",2023-11-07 19:46:37+00:00,,1,3,module: docs triaged medium docathon-h2-2023,False
113187,"Fix docstring errors in embedding.py, _limiter_utils.py, _dynamo_utils.py, embedding_bag.py, tensor_ops.py, api.py, _internals.py, _common.py, init.py, _exec_order_utils.py, _traversal_utils.py, chunk_sharding_spec.py, _trace_utils.py module: docs triaged medium docathon-h2-2023",2023-11-07 19:46:35+00:00,,1,6,module: docs triaged medium docathon-h2-2023,False
113186,"Fix docstring errors in shard.py, op_registry_utils.py, local_timer.py, file_based_local_timer.py, api.py, distributed.py, cycling_iterator.py, __init__.py, _utils.py, reshard.py, common_op_utils.py, elastic_distributed_sampler.py, utils.py, log_level.py, store.py, logging.py, sharder.py, metadata.py module: docs triaged medium docathon-h2-2023",2023-11-07 19:46:33+00:00,,1,1,module: docs triaged medium docathon-h2-2023,False
113180,Higher train loss and worse evaluation metrics when using `torch.compile()` high priority triage review needs reproduction triaged oncall: pt2 module: pt2 accuracy,2023-11-07 18:35:47+00:00,,0,5,high priority triage review needs reproduction triaged oncall: pt2 module: pt2 accuracy,True
113161,[Distributed][DTENSOR] Some operators registered under compositeimplicitautograd key can not work if I registered them under privateuse1 oncall: distributed module: dispatch module: dtensor,2023-11-07 15:25:17+00:00,,0,1,oncall: distributed module: dispatch module: dtensor,True
113160,"[dynamo] self-assigning operation causes `TensorVariable` to lose `mutable_local`, thus causing its attribute mutations to be untracked triaged oncall: pt2 module: dynamo",2023-11-07 15:18:45+00:00,,0,5,triaged oncall: pt2 module: dynamo,False
113156,torch._foreach_mul_ segmentation fault module: crash module: error checking triaged module: numpy module: edge cases,2023-11-07 13:11:31+00:00,,1,5,module: crash module: error checking triaged module: numpy module: edge cases,True
113151,different outputs on `torch.asinh` between eager mode and torch.compile triage review triaged bug oncall: pt2,2023-11-07 10:57:36+00:00,,0,4,triage review triaged bug oncall: pt2,True
113150,"Aborted (core dumped) after Run Pytorch2.0.0, which I compiled by myself needs reproduction module: build triaged",2023-11-07 10:44:24+00:00,,0,6,needs reproduction module: build triaged,True
113149,Compilation error on loongarch64 module: build triaged module: third_party,2023-11-07 10:17:00+00:00,,0,1,module: build triaged module: third_party,True
113148,DISABLED test_lgamma_cpu (__main__.CpuTests) triaged module: flaky-tests skipped module: inductor,2023-11-07 09:44:43+00:00,,0,1,triaged module: flaky-tests skipped module: inductor,False
113146,DISABLED test_large_strided_reduction_dynamic_shapes_cpu (__main__.DynamicShapesCodegenCpuTests) triaged module: flaky-tests skipped module: inductor,2023-11-07 09:43:51+00:00,,0,1,triaged module: flaky-tests skipped module: inductor,False
113141,Output mismatch of torch.sum with torch.compile when swapping the input parameters of torch.mul on CPU triaged bug oncall: pt2 module: cpu inductor,2023-11-07 08:06:22+00:00,,1,0,triaged bug oncall: pt2 module: cpu inductor,True
113140,DISABLED test_large_block_sizes_cuda (__main__.CudaTests) triaged module: flaky-tests skipped module: inductor,2023-11-07 06:40:54+00:00,,0,2,triaged module: flaky-tests skipped module: inductor,False
113139,DISABLED test_large_block_sizes_cpu (__main__.CpuTests) triaged module: flaky-tests skipped module: inductor,2023-11-07 06:40:01+00:00,,0,2,triaged module: flaky-tests skipped module: inductor,False
113137,DISABLED test_large_offset_pointwise_dynamic_shapes_cpu (__main__.DynamicShapesCodegenCpuTests) triaged module: flaky-tests skipped module: inductor,2023-11-07 06:39:55+00:00,,0,2,triaged module: flaky-tests skipped module: inductor,False
113136,DISABLED test_large_block_sizes_dynamic_shapes_cuda (__main__.DynamicShapesCodegenCudaTests) triaged module: flaky-tests skipped module: inductor,2023-11-07 06:39:52+00:00,,0,2,triaged module: flaky-tests skipped module: inductor,False
113135,DISABLED test_large_block_sizes_dynamic_shapes_cpu (__main__.DynamicShapesCodegenCpuTests) triaged module: flaky-tests skipped module: inductor,2023-11-07 06:39:49+00:00,,0,2,triaged module: flaky-tests skipped module: inductor,False
113134,DISABLED test_large_offset_pointwise_dynamic_shapes_cpu (__main__.DynamicShapesCpuTests) triaged module: flaky-tests skipped module: inductor,2023-11-07 06:39:46+00:00,,0,2,triaged module: flaky-tests skipped module: inductor,False
113133,DISABLED test_large_block_sizes_dynamic_shapes_cpu (__main__.DynamicShapesCpuTests) triaged module: flaky-tests skipped module: inductor,2023-11-07 06:39:43+00:00,,0,2,triaged module: flaky-tests skipped module: inductor,False
113132,DISABLED test_large_block_sizes_dynamic_shapes_cuda (__main__.DynamicShapesCudaTests) triaged module: flaky-tests skipped module: inductor,2023-11-07 06:39:40+00:00,,0,2,triaged module: flaky-tests skipped module: inductor,False
113130,get_unbacked_symbol_defs in inductor uses a set and iterates over it low priority triaged oncall: pt2 module: inductor internal ramp-up task,2023-11-07 05:59:20+00:00,,0,1,low priority triaged oncall: pt2 module: inductor internal ramp-up task,False
113129,[torch.compile] Dynamic shape behavior is different between using torch.compile with and without compiled_autograd.enable triaged oncall: pt2 module: dynamic shapes module: compiled autograd module: pt2-dispatcher,2023-11-07 05:45:59+00:00,,0,2,triaged oncall: pt2 module: dynamic shapes module: compiled autograd module: pt2-dispatcher,False
113128,NCCL watchdog thread terminated with exception  oncall: distributed module: nccl,2023-11-07 05:37:43+00:00,,0,5,oncall: distributed module: nccl,True
113124,Request to add system requirements to doc module: docs module: cpp triaged docathon-h2-2023,2023-11-07 03:48:26+00:00,,1,1,module: docs module: cpp triaged docathon-h2-2023,False
113122,Request to create a tutorial for loading a model dumped by `torch.export` triaged docathon-h2-2023 oncall: export,2023-11-07 03:45:53+00:00,,1,3,triaged docathon-h2-2023 oncall: export,False
113118,RuntimeError on `torch.unqiue_consecutive` with torch.compile( fullgraph = true) triaged bug oncall: pt2 module: dynamic shapes,2023-11-07 02:51:41+00:00,,0,2,triaged bug oncall: pt2 module: dynamic shapes,True
113114,Allow to construct distributed Work object from Future oncall: distributed,2023-11-07 01:30:27+00:00,,0,0,oncall: distributed,False
113113,FSDP does not move modules without parameters to device triaged module: fsdp,2023-11-07 01:22:43+00:00,,0,6,triaged module: fsdp,True
113112,Add tests for ProcessGroupGloo::reduce_scatter_base and allgather_base oncall: distributed,2023-11-07 01:19:26+00:00,,0,0,oncall: distributed,False
113089,Add docs for __tensor_flatten__ / __tensor_unflatten__ triage review module: docs triaged module: __torch_dispatch__ tensor subclass oncall: pt2 module: pt2-dispatcher,2023-11-06 22:37:32+00:00,,0,3,triage review module: docs triaged module: __torch_dispatch__ tensor subclass oncall: pt2 module: pt2-dispatcher,True
113079,"Automatically run ""lintrunner -a"" when needed and create review comments with autofixes triaged",2023-11-06 22:16:21+00:00,,0,3,triaged,True
113067,[ONNX] stft export fails with dynamo_export module: onnx triaged,2023-11-06 20:50:10+00:00,,1,22,module: onnx triaged,False
113063,Rework Dynamic Benchmarks To Actually Vary Shapes high priority triage review triaged oncall: pt2 module: dynamic shapes,2023-11-06 20:25:06+00:00,,0,1,high priority triage review triaged oncall: pt2 module: dynamic shapes,True
113062,MPS device: Sample from MultivariateNormal distribution triaged module: mps,2023-11-06 20:22:30+00:00,,0,3,triaged module: mps,True
113053,[CUDA-12.2] cuSPARSE deprecated support for sparse BSR module: sparse module: cuda triaged,2023-11-06 19:09:00+00:00,,0,5,module: sparse module: cuda triaged,True
113045,[Feature][DTensor] Manage additional `_padded_local_tensor` attribute feature triaged module: dtensor,2023-11-06 18:25:42+00:00,,0,0,feature triaged module: dtensor,False
113040,[PT2] [Hardening] Track recompiles alongside graph breaks in our actual/expected comparison CI runs triage review good first issue triaged oncall: pt2 module: dynamo,2023-11-06 17:50:09+00:00,,0,0,triage review good first issue triaged oncall: pt2 module: dynamo,True
113037,`torch.div` on empty tensors causes segmentation fault high priority module: crash module: cpu triaged module: regression,2023-11-06 17:40:49+00:00,,1,4,high priority module: crash module: cpu triaged module: regression,True
113035,Training a network SUPER slow with Pytorch 2.1 needs reproduction module: performance module: cuda triaged,2023-11-06 17:32:43+00:00,,0,4,needs reproduction module: performance module: cuda triaged,True
113034,make_fx produces incorrect graph when used under FunctionalTensorMode triaged module: fx oncall: fx,2023-11-06 17:28:59+00:00,,0,2,triaged module: fx oncall: fx,True
113030,"Mismatching behaviour of tensor assignment ""a.data = b"" between torch.compile and eager execution triage review triaged oncall: pt2",2023-11-06 16:40:39+00:00,,0,5,triage review triaged oncall: pt2,True
113024,Upsample bilinear 2d decomposition does not match native implementation for uint8 triaged module: decompositions,2023-11-06 13:49:39+00:00,,1,2,triaged module: decompositions,True
113022,Operator with only `Tensor[][]` args unsupported by dispatcher triaged module: dispatch,2023-11-06 11:56:13+00:00,,0,0,triaged module: dispatch,True
113020,Output mismatch of torch.to with torch.compile when swapping the input parameters of torch.mul on CPU triaged oncall: pt2 oncall: cpu inductor,2023-11-06 10:00:35+00:00,,1,2,triaged oncall: pt2 oncall: cpu inductor,False
113017,Output mismatch of torch.max with torch.compile when swapping output sequence on CPU triaged oncall: pt2 oncall: cpu inductor,2023-11-06 09:38:14+00:00,,0,3,triaged oncall: pt2 oncall: cpu inductor,False
113015,Output mismatch of torch.sum with torch.compile when swapping output sequence on CPU triaged oncall: pt2 oncall: cpu inductor,2023-11-06 09:23:05+00:00,,0,11,triaged oncall: pt2 oncall: cpu inductor,False
113007,`make_fx` failing for `_scaled_dot_product_flash_attention` decomposition triaged module: fx module: decompositions,2023-11-06 05:22:43+00:00,,1,8,triaged module: fx module: decompositions,True
113002,AOTInductor data dependents error when using max().item() triage review triaged oncall: pt2 module: dynamic shapes oncall: export,2023-11-06 02:51:05+00:00,,0,3,triage review triaged oncall: pt2 module: dynamic shapes oncall: export,True
113001,the format of exported json from profiler is wrong! oncall: profiler,2023-11-06 02:25:41+00:00,,1,1,oncall: profiler,False
112997,Add support for Flash Attention for AMD/ROCm module: rocm triaged ciflow/rocm,2023-11-05 21:07:53+00:00,,0,1,module: rocm triaged ciflow/rocm,False
112956,Optim.step() is significantly SLOW on MPS module: performance triaged module: mps,2023-11-04 15:52:50+00:00,,0,4,module: performance triaged module: mps,True
112955," Error loading ""AppData\\Local\\Temp\\_MEI136882\\torch\\lib\\cufft64_10.dll"" or one of its dependencies. module: windows triaged module: third_party",2023-11-04 15:34:49+00:00,,0,2,module: windows triaged module: third_party,True
112944,Getting an Error when loading a checkpoint :  AttributeError: Can't get attribute 'base_args_dict' on <module '__main__'> needs reproduction module: windows module: serialization triaged module: python frontend,2023-11-04 06:36:42+00:00,,0,2,needs reproduction module: windows module: serialization triaged module: python frontend,True
112942,Can't build PyTorch 2.1 from source by GCC 13.2 on M1 MacOS module: build module: docs triaged module: macos,2023-11-04 03:47:16+00:00,,0,2,module: build module: docs triaged module: macos,True
112919,Export + assume_constant_result does not work for top-level annotated function triage review good first issue triaged oncall: pt2 oncall: export,2023-11-03 22:01:44+00:00,,0,1,triage review good first issue triaged oncall: pt2 oncall: export,True
112903,Support for Bazel workspace function or Bazel module module: build triaged module: bazel,2023-11-03 21:01:24+00:00,,0,3,module: build triaged module: bazel,True
112901,Tracking: Dynamo Tracing Improvements triaged oncall: pt2,2023-11-03 20:36:56+00:00,,0,0,triaged oncall: pt2,False
112887,DISABLED test_do_not_skip_side_effects (__main__.SkipNonTensorTests) triaged module: flaky-tests skipped,2023-11-03 18:39:59+00:00,,0,4,triaged module: flaky-tests skipped,False
112883,Torchbench inference failures triaged oncall: pt2,2023-11-03 17:59:25+00:00,,0,1,triaged oncall: pt2,False
112881,Torchbench Training Failing Models Tracker triaged oncall: pt2,2023-11-03 17:58:39+00:00,,0,2,triaged oncall: pt2,False
112876,How to handle CVE vulnerabilities in underlying operating system? triaged module: docker security,2023-11-03 17:32:14+00:00,,0,4,triaged module: docker security,True
112872,libtorch exports miniz symbols module: binaries module: cpp triaged,2023-11-03 16:53:26+00:00,,0,2,module: binaries module: cpp triaged,True
112865,[dynamo] Unable to continue tracing graph after try/except graph break triaged oncall: pt2 module: dynamo,2023-11-03 16:28:36+00:00,,0,6,triaged oncall: pt2 module: dynamo,False
112857,Vendored FindCUDAToolkit.cmake deviates from upstream in splayed installation support module: build triaged,2023-11-03 15:31:48+00:00,,0,2,module: build triaged,True
112853,Vmap: There is a performance drop because we have not yet implemented the batching rule for aten::max_pool3d. triaged module: functorch,2023-11-03 15:07:53+00:00,,0,0,triaged module: functorch,True
112849,CSR matrix on MPS module: sparse triaged module: mps,2023-11-03 13:41:15+00:00,,0,1,module: sparse triaged module: mps,True
112848,[dynamo] nit - add `@torch.autocast` function decoration to the testing path good first issue triaged oncall: pt2 module: dynamo,2023-11-03 13:37:36+00:00,,0,2,good first issue triaged oncall: pt2 module: dynamo,True
112847,High dimensional grid sample module: cuda triaged enhancement,2023-11-03 13:05:23+00:00,,0,0,module: cuda triaged enhancement,True
112844,torch.export does not support torchaudio.transforms.Spectrogram module: onnx module: dynamo oncall: export module: export,2023-11-03 11:06:39+00:00,,0,7,module: onnx module: dynamo oncall: export module: export,False
112834,[MPS]Apple MPS produce different Loss value vs. NVIDIA CUDA after couple of digits triaged module: mps,2023-11-03 08:04:13+00:00,,0,0,triaged module: mps,True
112833,Failed to remove output annotation in Quantizer for PT2 QAT quantization oncall: quantization triaged,2023-11-03 07:47:58+00:00,,1,3,oncall: quantization triaged,True
112831,dynamo_export successfully export model but fails at onnx.checker.check_model module: onnx triaged onnx-needs-info oncall: pt2 oncall: export,2023-11-03 07:31:45+00:00,,0,15,module: onnx triaged onnx-needs-info oncall: pt2 oncall: export,False
112827,Multi Scale Deformable Attention Support feature module: nn triaged oncall: transformer/mha module: multi-headed-attention,2023-11-03 05:53:07+00:00,,0,1,feature module: nn triaged oncall: transformer/mha module: multi-headed-attention,True
112826,sparse allreduce not support  CSR format oncall: distributed module: sparse,2023-11-03 04:39:56+00:00,,0,1,oncall: distributed module: sparse,False
112824,[dynamo] Skipping the entire frame when graph breaking in for/while loop is excessive triaged oncall: pt2 module: dynamo,2023-11-03 04:27:51+00:00,,0,1,triaged oncall: pt2 module: dynamo,False
112815,DISABLED test_trace_while_active (__main__.NCCLTraceTest) oncall: distributed module: rocm triaged module: flaky-tests skipped,2023-11-03 00:57:51+00:00,,0,1,oncall: distributed module: rocm triaged module: flaky-tests skipped,False
112814,DISABLED test_get_parent_mesh_dim_not_exist (__main__.TestDeviceMeshGetItem) oncall: distributed module: flaky-tests skipped,2023-11-03 00:56:56+00:00,,0,1,oncall: distributed module: flaky-tests skipped,False
112794,[dynamo] Implement enumerate fallback as polyfill triaged oncall: pt2 module: dynamo,2023-11-02 22:05:49+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
112791,DISABLED test_cuda_event_method_dynamic_shapes (__main__.DynamicShapesCtxManagerTests) triaged module: flaky-tests skipped module: dynamo,2023-11-02 21:39:31+00:00,,0,1,triaged module: flaky-tests skipped module: dynamo,False
112789,[export] Make serialized pytree type name required  triaged export-triaged oncall: export,2023-11-02 21:18:43+00:00,,1,0,triaged export-triaged oncall: export,True
112788,async_compile.triton is unable to cache generated triton kernels due to meta containing kernel names triaged oncall: pt2 module: inductor,2023-11-02 21:18:13+00:00,,1,1,triaged oncall: pt2 module: inductor,False
112775,Multi-Threaded GraphModule / torch.fx inference raises an exception module: multiprocessing triaged module: fx,2023-11-02 19:35:40+00:00,,0,1,module: multiprocessing triaged module: fx,True
112771,PT2: don't always require inputs to be aligned triaged oncall: pt2,2023-11-02 19:25:46+00:00,,1,1,triaged oncall: pt2,False
112749,Tracing mode for unbacked SymInts using real data triaged oncall: pt2 module: dynamic shapes,2023-11-02 15:56:36+00:00,,0,2,triaged oncall: pt2 module: dynamic shapes,True
112745,torch._dynamo.export raises Unexpected type in sourceless builder <class 'nemo.core.neural_types.elements.VoidType'> for torchaudio model module: onnx triaged module: dynamo,2023-11-02 15:24:15+00:00,,0,3,module: onnx triaged module: dynamo,True
112744,"When torch.compile is used, some APIs are decomposed even if decomposition is empty. triaged oncall: pt2",2023-11-02 15:24:00+00:00,,0,2,triaged oncall: pt2,False
112731,"[dynamo + optim] complex, sparse are not on tracing testing path module: optimizer triaged oncall: pt2",2023-11-02 14:13:51+00:00,,1,0,module: optimizer triaged oncall: pt2,False
112727,[dynamo] Implement iter fallback (and possibly all iters/generators) as polyfill triaged oncall: pt2,2023-11-02 13:34:59+00:00,,1,6,triaged oncall: pt2,False
112719,DISABLED test_meta_outplace_nn_functional_margin_ranking_loss_cpu_uint8 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-11-02 12:45:37+00:00,,0,5,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112718,'aten::unique_consecutive' to ONNX opset version 14 is not supported module: onnx triaged,2023-11-02 12:31:39+00:00,,0,5,module: onnx triaged,False
112716,Quantized model's size gets doubled on optimizing and saving it for mobile oncall: quantization triaged oncall: mobile,2023-11-02 10:53:52+00:00,,1,0,oncall: quantization triaged oncall: mobile,False
112715,Wrong with code_coverage/readme.md module: docs triaged,2023-11-02 10:50:37+00:00,,0,1,module: docs triaged,True
112714,log_softmax() on CPU and GPU has expected numerical error when used with low-precision bfloat16 module: numerical-stability module: cuda module: cpu triaged,2023-11-02 09:43:06+00:00,,0,2,module: numerical-stability module: cuda module: cpu triaged,True
112713,Discrepancy in Behavior of torch.diag_embed Between Eager Execution and 'torch.compiled' Optimized Mode triaged oncall: pt2 module: decompositions,2023-11-02 09:39:45+00:00,,0,1,triaged oncall: pt2 module: decompositions,False
112702,DISABLED test_meta_outplace_std_mean_cpu_float64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-11-02 06:39:37+00:00,,0,7,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112694,DISABLED test_cuda_stream_context_manager1_dynamic_shapes (__main__.DynamicShapesCtxManagerTests) triaged module: flaky-tests skipped module: dynamo,2023-11-02 03:40:38+00:00,,0,20,triaged module: flaky-tests skipped module: dynamo,False
112691,when huawei NPU triaged module: backend,2023-11-02 03:02:11+00:00,,0,0,triaged module: backend,True
113369,When will Huawei Shengteng atlas be supported triaged module: backend,2023-11-02 02:56:14+00:00,,0,1,triaged module: backend,True
112681,DISABLED test_meta_outplace_std_mean_cpu_float32 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-11-02 00:57:40+00:00,,0,8,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112678,Catalogue of flaky tests under `test/dynamo` triaged oncall: pt2,2023-11-02 00:10:56+00:00,,0,4,triaged oncall: pt2,False
112670,[dynamo] direct invocation of bound method introduces graph break triaged oncall: pt2 module: dynamo module: graph breaks,2023-11-01 22:52:46+00:00,,0,2,triaged oncall: pt2 module: dynamo module: graph breaks,False
112666,dispatcher cannot determine dispatch key on tuple input triaged module: dispatch,2023-11-01 22:05:26+00:00,,0,3,triaged module: dispatch,True
112658,FSDP requires global device context oncall: distributed triaged module: fsdp,2023-11-01 21:11:34+00:00,,0,4,oncall: distributed triaged module: fsdp,True
112646,"Fix docstring errors in stream.py, pipe.py, blockpartition.py, microbatch.py, namespace.py, profile.py, tracker.py, portal.py, layout.py, __init__.py module: docs triaged medium docathon-h2-2023",2023-11-01 19:43:13+00:00,,1,3,module: docs triaged medium docathon-h2-2023,False
112644,"Fix docstring errors in default_hooks.py, post_localSGD_hook.py, debugging_hooks.py, utils.py, hierarchical_model_averager.py, optimizer_overlap_hooks.py, mixed_precision_hooks.py, quantization_hooks.py, ddp_zero_hook.py, __init__.py, powerSGD_hook.py, averagers.py module: docs triaged medium docathon-h2-2023",2023-11-01 19:43:10+00:00,,1,6,module: docs triaged medium docathon-h2-2023,False
112641,"Fix docstring errors in post_localSGD_optimizer.py, functional_sgd.py, _functional_collectives.py, optimizer.py, utils.py, api.py, server_process_global_profiler.py, functions.py, backend_registry.py, __init__.py, options.py, internal.py, functional_adam.py module: docs triaged medium docathon-h2-2023",2023-11-01 19:43:05+00:00,,1,4,module: docs triaged medium docathon-h2-2023,False
112638,"Fix docstring errors in spectral_ops_fuzz_test.py, simple_timeit.py, timer_interface.py, op_benchmark.py, _stubs.py, fuzzer.py, compare.py, compile.py, interp.py, hipify_python.py, common.py, end_to_end.py, timer.py, __init__.py, sparse_fuzzer.py, blas_compare_setup.py module: docs triaged medium docathon-h2-2023",2023-11-01 19:43:00+00:00,,1,5,module: docs triaged medium docathon-h2-2023,False
112622,[ONNX] Assertion in models is not supported by fx exporter  module: onnx triaged onnx-triaged,2023-11-01 18:53:44+00:00,,0,0,module: onnx triaged onnx-triaged,True
112615,DISABLED test_resnet18_cpu (__main__.BenchmarkFusionCpuTest) triaged module: macos skipped module: inductor module: cpu inductor,2023-11-01 17:42:59+00:00,,0,1,triaged module: macos skipped module: inductor module: cpu inductor,False
112608,`grad_fn` is not defined in `__torch_dispatch__` arguments when lazy device is used triaged lazy,2023-11-01 16:48:41+00:00,,0,8,triaged lazy,True
112602,"Fix docstring errors in batchnorm.py, activation.py module: docs triaged medium docathon-h2-2023",2023-11-01 15:29:41+00:00,,1,7,module: docs triaged medium docathon-h2-2023,False
112600,Fix docstring errors in loss.py module: docs triaged medium docathon-h2-2023,2023-11-01 15:29:38+00:00,,1,3,module: docs triaged medium docathon-h2-2023,False
112593,"Fix docstring errors in nadam.py, radam.py, sgd.py, anomaly_mode.py, rprop.py, __init__.py, swa_utils.py, rmsprop.py, optimizer.py, lr_scheduler.py module: docs triaged medium docathon-h2-2023",2023-11-01 15:29:26+00:00,,1,3,module: docs triaged medium docathon-h2-2023,False
112588,"Fix docstring errors in _torch_docs.py, serialization.py, overrides.py, _utils.py module: docs triaged medium docathon-h2-2023",2023-11-01 15:29:18+00:00,,1,1,module: docs triaged medium docathon-h2-2023,False
112587,"Fix docstring errors in __init__.py, _tensor_docs.py, _meta_registrations.py, _tensor.py module: docs triaged medium docathon-h2-2023",2023-11-01 15:29:17+00:00,,1,2,module: docs triaged medium docathon-h2-2023,False
112586,"Fix docstring errors in _guards.py, _ops.py, _jit_internal.py, functional.py, _tensor_str.py, library.py module: docs triaged medium topic: not user facing docathon-h2-2023",2023-11-01 15:29:15+00:00,,1,1,module: docs triaged medium topic: not user facing docathon-h2-2023,False
112585,"Fix docstring errors in _VF.py, _appdirs.py, hub.py, _classes.py, _storage_docs.py, _linalg_utils.py, torch_version.py, quasirandom.py, random.py, __future__.py, _lowrank.py, _vmap_internals.py, _sources.py, __config__.py, _lobpcg.py, _namedtensor_internals.py module: docs triaged medium docathon-h2-2023",2023-11-01 15:29:14+00:00,,1,2,module: docs triaged medium docathon-h2-2023,False
112583,Nesting no_grad in autocast causes backwards graph to be (partially) lost outside of no_grad high priority module: autograd triaged actionable module: amp (automated mixed precision),2023-11-01 15:12:57+00:00,,0,9,high priority module: autograd triaged actionable module: amp (automated mixed precision),True
112580,USE_SYSTEM_ONNX: undefined references module: onnx triaged,2023-11-01 14:20:36+00:00,,0,10,module: onnx triaged,True
112579,Bugs in `torch.ao.quantization.fuse_modules`  cause operator fusion failure (`Conv2d` and `BatchNorm2d`) oncall: quantization triaged,2023-11-01 11:48:32+00:00,,1,7,oncall: quantization triaged,True
112577,[Breaking change 2.1] Passing non-contiguous inputs to SDPA on CUDA device with the mem-efficient attention backend returns garbage high priority triaged module: regression module: correctness (silent) module: multi-headed-attention,2023-11-01 11:00:42+00:00,,1,9,high priority triaged module: regression module: correctness (silent) module: multi-headed-attention,True
112575,"RuntimeError: ""grid_sampler_2d_cuda"" not implemented for 'BFloat16' module: cuda triaged module: bfloat16",2023-11-01 09:51:57+00:00,,0,0,module: cuda triaged module: bfloat16,True
112574,DISABLED test_meta_outplace_std_mean_cpu_float16 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-11-01 09:44:21+00:00,,0,8,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112569,"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch needs reproduction module: cuda module: error checking triaged",2023-11-01 06:51:05+00:00,,0,1,needs reproduction module: cuda module: error checking triaged,True
112567,DISABLED test_numpy_non_writeable_cpu (__main__.TestNumPyInteropCPU) triaged module: flaky-tests module: numpy skipped oncall: pt2,2023-11-01 06:40:02+00:00,,0,2,triaged module: flaky-tests module: numpy skipped oncall: pt2,False
112566,[inductor][cpu] detectron2 fasterrcnn accuracy failure high priority triage review triaged oncall: pt2 module: cpu inductor,2023-11-01 06:02:59+00:00,,1,4,high priority triage review triaged oncall: pt2 module: cpu inductor,True
112558,DISABLED test_meta_outplace_pinverse_cpu_complex64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-11-01 03:39:32+00:00,,0,6,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112556,"when convert to onnx ,the jit will merge th outputs, it results to we can't distinguish  what the outputs represents module: onnx triaged",2023-11-01 03:25:33+00:00,,0,0,module: onnx triaged,True
112552,Inductor cpp wrapper: clean up the hard-coded schema for fusion OPs triaged oncall: pt2,2023-11-01 02:21:49+00:00,,1,0,triaged oncall: pt2,False
112548,Show warning when compiling the optimizer and grads are going to be copied to cudagraph-owned memory triaged oncall: pt2,2023-11-01 01:25:26+00:00,,1,0,triaged oncall: pt2,False
112532,[Tracking] Follow ups for itertools infinite iterators good first issue triaged oncall: pt2 module: dynamo,2023-10-31 21:11:07+00:00,,0,5,good first issue triaged oncall: pt2 module: dynamo,True
112515,[Bug Report]FSDP: An error raises when loading FSDP distributed checkpoint with ignoring modules. oncall: distributed triaged module: fsdp,2023-10-31 17:00:49+00:00,,1,2,oncall: distributed triaged module: fsdp,True
112512,Constant output from exported ONNX module: onnx triaged,2023-10-31 16:32:01+00:00,,0,1,module: onnx triaged,False
112509,Add a function to torch.nested to create nested tensors from a buffer and sizes module: serialization triaged module: nestedtensor,2023-10-31 16:15:59+00:00,,0,2,module: serialization triaged module: nestedtensor,True
112495,Compilation Failure of torch.special.exp2 in torch.compile Optimized Mode triaged module: cuda graphs oncall: pt2,2023-10-31 12:22:25+00:00,,0,0,triaged module: cuda graphs oncall: pt2,True
112492,Compilation Failure of torch.cumsum in torch.compile Optimized Mode triaged module: cuda graphs oncall: pt2 module: decompositions,2023-10-31 11:21:55+00:00,,0,0,triaged module: cuda graphs oncall: pt2 module: decompositions,True
112491,[ONNX] Result from export_onnx in pytorch returns different result from pytorch module: onnx triaged,2023-10-31 11:16:34+00:00,,0,1,module: onnx triaged,False
112487,[ONNX] Exporting the operator 'aten::sparse_coo_tensor' to ONNX opset version 17 is not supported module: onnx triaged,2023-10-31 08:09:31+00:00,,0,1,module: onnx triaged,False
112479,DISABLED test_meta_outplace_pinverse_cpu_complex128 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-31 06:39:49+00:00,,0,8,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112463,DISABLED test_meta_outplace_nn_functional_margin_ranking_loss_cpu_int16 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-31 00:56:57+00:00,,0,8,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112462,DISABLED test_cuda_event_method (__main__.CtxManagerTests) triaged module: flaky-tests skipped module: dynamo,2023-10-31 00:56:55+00:00,,0,1,triaged module: flaky-tests skipped module: dynamo,False
112459,GRUCell batching rule for vmap triaged module: functorch,2023-10-31 00:43:41+00:00,,0,0,triaged module: functorch,True
112443,torch.export emits node outside of Core ATen IR triaged export-triaged export-triage-review oncall: export,2023-10-30 21:21:38+00:00,,1,4,triaged export-triaged export-triage-review oncall: export,True
112425,torch.jit.trace output changes when I add comments to the file oncall: jit,2023-10-30 18:44:12+00:00,,0,0,oncall: jit,False
112424,[torch.compile] Unit test failures after we always trace all module.forward method triaged oncall: pt2 module: aotdispatch module: dynamo module: pt2-dispatcher,2023-10-30 18:40:02+00:00,,0,0,triaged oncall: pt2 module: aotdispatch module: dynamo module: pt2-dispatcher,False
112422,DISABLED test_meta_outplace_nn_functional_margin_ranking_loss_cpu_float32 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-30 18:39:21+00:00,,0,7,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112402,[torch.jit.script] Expected a value of type 'Tensor' for argument 'b' but instead found type 'bool'. oncall: jit,2023-10-30 16:29:51+00:00,,0,2,oncall: jit,True
112398,[Tracker] Move nested tensors to beta triaged module: nestedtensor,2023-10-30 15:45:07+00:00,,0,7,triaged module: nestedtensor,True
112395,"KeyError during model export while using ""newer"" data types module: serialization triaged",2023-10-30 13:51:06+00:00,,0,0,module: serialization triaged,True
112389,KINETO_USE_DAEMON not work oncall: profiler,2023-10-30 13:08:41+00:00,,0,1,oncall: profiler,False
112386,DISABLED test_meta_outplace_nn_functional_hinge_embedding_loss_cpu_float32 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-30 12:45:57+00:00,,0,10,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112383,Possible use-after-free of Tensor in JIT generated code module: crash module: cpp-extensions triaged,2023-10-30 11:33:03+00:00,,0,1,module: crash module: cpp-extensions triaged,True
112380,Need support CPU flash attention with mask oncall: transformer/mha,2023-10-30 09:40:13+00:00,,0,0,oncall: transformer/mha,False
112377,"RuntimeError: NVML_SUCCESS == r INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1695392020201/work/c10/cuda/CUDACachingAllocator.cpp"":1154, please report a bug to PyTorch. triaged module: CUDACachingAllocator",2023-10-30 08:47:11+00:00,,0,7,triaged module: CUDACachingAllocator,True
112376,Contradictory Error Message for stride Argument in torch.conv_transpose3d() module: error checking module: convolution triaged,2023-10-30 08:43:46+00:00,,0,1,module: error checking module: convolution triaged,True
112372,[Tracer] RuntimeError: _Map_base::at when tracing using autograd.Function oncall: jit,2023-10-30 07:00:30+00:00,,0,0,oncall: jit,False
112371,Why is flake8 F821 disabled good first issue triaged better-engineering,2023-10-30 06:59:07+00:00,,1,3,good first issue triaged better-engineering,True
112369,"In the func Tensor.to, how can I make privateuse lazy init module: internals triaged",2023-10-30 06:39:18+00:00,,0,3,module: internals triaged,True
112354,DISABLED test_torch_name_rule_map (__main__.TraceRuleTests) triaged module: flaky-tests skipped module: dynamo,2023-10-30 00:57:43+00:00,,0,47,triaged module: flaky-tests skipped module: dynamo,False
112353,DISABLED test_meta_outplace_nn_functional_alpha_dropout_cpu_float16 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-30 00:56:53+00:00,,0,9,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112352,RFC: [pytree] node registration namespaces triaged needs design module: pytree,2023-10-29 21:53:22+00:00,,0,6,triaged needs design module: pytree,True
112347,"[dynamo] `.view([..., -1, ...])` fails on Tensors with unbacked SymInts in the shape triaged oncall: pt2 module: dynamic shapes",2023-10-29 17:45:28+00:00,,0,3,triaged oncall: pt2 module: dynamic shapes,False
112344,[test/dynamo] BE: cleanup `test_misc.py` triaged better-engineering oncall: pt2 module: dynamo,2023-10-29 14:43:59+00:00,,0,2,triaged better-engineering oncall: pt2 module: dynamo,False
112342,[pytree] Pytree node registration hygeine: deprecate global _register_pytree_node; only allow enabling registered pytree extensions locally triaged module: pytree,2023-10-29 14:06:09+00:00,,0,11,triaged module: pytree,True
112340,Unexpected behaviour with shared modules in multiprocessing on WSL2 module: multiprocessing triaged module: wsl,2023-10-29 12:42:20+00:00,,0,3,module: multiprocessing triaged module: wsl,True
112339,Requesting to add a section to the Installing C++ Distributions of PyTorch documentation for Apple M1/M2 Processors module: binaries module: docs module: cpp triaged module: macos module: arm,2023-10-29 11:22:10+00:00,,0,3,module: binaries module: docs module: cpp triaged module: macos module: arm,True
112338,`set` of enums produces a graph break (no repro) triaged oncall: pt2 module: dynamo module: graph breaks,2023-10-29 11:11:52+00:00,,1,0,triaged oncall: pt2 module: dynamo module: graph breaks,False
112330,Library is included twice QNNPACK module: build triaged oncall: mobile actionable,2023-10-28 23:56:20+00:00,,0,1,module: build triaged oncall: mobile actionable,True
112318,IBM AfroHacks at AfroTech Participation triaged,2023-10-28 13:02:11+00:00,,0,4,triaged,True
112303,"[dynamo]RFC/Feature request - cache, guard, and reuse InliningInstructionTranslator inline_call feature triaged oncall: pt2 module: dynamo",2023-10-27 23:24:58+00:00,,0,10,feature triaged oncall: pt2 module: dynamo,False
112285,PyTorch is shipped with different versions on NCCL high priority module: binaries oncall: releng triaged module: nccl,2023-10-27 20:15:02+00:00,,0,2,high priority module: binaries oncall: releng triaged module: nccl,True
112277,test_DistributedDataParallel fails with static_graph=True oncall: distributed,2023-10-27 18:54:46+00:00,,0,3,oncall: distributed,False
112273,DISABLED test_meta_outplace_nn_functional_alpha_dropout_cpu_bfloat16 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-27 18:39:28+00:00,,0,8,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112264,Can't access attribute of wrapper tensor subclass under torch.compile triaged tensor subclass oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-10-27 17:36:38+00:00,,0,3,triaged tensor subclass oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
112260,[dynamo] we do not instantiate guards for ambient autocast mode triaged module: amp (automated mixed precision) oncall: pt2 module: dynamo module: guards,2023-10-27 16:49:08+00:00,,1,4,triaged module: amp (automated mixed precision) oncall: pt2 module: dynamo module: guards,False
112256,[ONNX] In-place additon not being functionalized by torch.onnx.dynamo_export module: onnx triaged onnx-triaged release notes: onnx,2023-10-27 16:21:01+00:00,,0,1,module: onnx triaged onnx-triaged release notes: onnx,False
112251,Optest is unable to xfail on parametrized tests triaged module: opcheck,2023-10-27 13:57:05+00:00,,0,1,triaged module: opcheck,True
112247,DISABLED test_meta_outplace_masked_var_cpu_complex64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-27 12:44:46+00:00,,0,6,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112236,DISABLED test_meta_outplace_linalg_pinv_cpu_complex128 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-27 06:39:46+00:00,,0,10,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112220,DISABLED test_meta_outplace_linalg_lu_solve_cpu_complex64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped oncall: pt2,2023-10-27 00:58:41+00:00,,0,3,triaged module: flaky-tests skipped oncall: pt2,False
112218,[ONNX] Expose the graph module in torch.onnx ExportOutput module: onnx triaged,2023-10-27 00:42:15+00:00,,0,0,module: onnx triaged,False
112186,"Using the ""mps"" device on x86 Mac with AMD gpu, torch.argmax returns incorrect results. triaged module: correctness (silent) module: mps",2023-10-26 19:20:38+00:00,,0,0,triaged module: correctness (silent) module: mps,True
112182,DISABLED test_meta_outplace_fft_ihfftn_cpu_uint8 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-26 18:39:23+00:00,,0,9,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112180,Validations for 2.1.1 release triaged,2023-10-26 18:37:30+00:00,,2,6,triaged,True
112176,~ Docathon H2 2023 ~ module: docs triaged docathon-h2-2023,2023-10-26 17:35:23+00:00,,1,5,module: docs triaged docathon-h2-2023,False
112171,Implement clip grad value on FSDP feature triaged module: fsdp,2023-10-26 16:56:19+00:00,,0,0,feature triaged module: fsdp,True
112164,[Tracking] torch.compile + torch.distributed + set_grad_enabled/autocast oncall: distributed,2023-10-26 16:06:42+00:00,,1,1,oncall: distributed,False
112152,DDP backpropagated gradients not the same across all gpus when forward inference not using all declared modules oncall: distributed,2023-10-26 14:13:20+00:00,,0,0,oncall: distributed,True
112148,Export List/Tuple type inputs with dynamic size module: onnx triaged,2023-10-26 10:42:13+00:00,,0,0,module: onnx triaged,True
112142,DISABLED test_out_warning__refs_clamp_cpu (__main__.TestCommonCPU) triaged module: flaky-tests skipped module: unknown oncall: pt2,2023-10-26 09:39:31+00:00,,0,2,triaged module: flaky-tests skipped module: unknown oncall: pt2,False
112139,[inductor][cpu] performance regression triaged oncall: pt2 module: cpu inductor,2023-10-26 08:32:16+00:00,,1,6,triaged oncall: pt2 module: cpu inductor,False
112137,FSDP load sharded state dict + multi-backend init + bf16 + gloo (?) crashes high priority triage review oncall: distributed module: fsdp,2023-10-26 07:24:26+00:00,,0,2,high priority triage review oncall: distributed module: fsdp,True
112136,pyTorch 2.1 3x slower than 2.0 module: performance module: windows module: cuda triaged,2023-10-26 07:16:55+00:00,,0,1,module: performance module: windows module: cuda triaged,True
112133,DISABLED test_meta_outplace_fft_ihfftn_cpu_int64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-26 06:39:41+00:00,,0,9,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112117,DISABLED test_cuda_stream_context_manager1 (__main__.CtxManagerTests) triaged module: flaky-tests skipped module: dynamo,2023-10-26 03:39:37+00:00,,0,1,triaged module: flaky-tests skipped module: dynamo,False
112104,DISABLED test_meta_outplace_fft_ihfftn_cpu_int32 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-26 00:56:32+00:00,,0,9,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112095,torch.export fails on a model with optional parameter triaged export-triage-review oncall: export,2023-10-25 23:17:57+00:00,,0,3,triaged export-triage-review oncall: export,True
112090,Memory leak in torch.compile after aborting backward triaged oncall: pt2 module: dynamo,2023-10-25 22:37:36+00:00,,1,7,triaged oncall: pt2 module: dynamo,False
112089,"Failed to compile: null in call to `__builtin_memmove(__result, __first, sizeof(_Tp) * _Num);` Debian 12, ppc64le, gcc 12.2 module: build triaged",2023-10-25 22:37:26+00:00,,0,3,module: build triaged,True
112079,The error type `UserError` in dynamo is confusing. triaged export-triage-review oncall: export,2023-10-25 20:47:55+00:00,,0,0,triaged export-triage-review oncall: export,True
112054,torch.distributed.distributed_c10d._get_default_group() is not Dynamo traceable triaged oncall: pt2 module: dynamo,2023-10-25 20:02:30+00:00,,0,1,triaged oncall: pt2 module: dynamo,False
112044,pack_padded_sequence/pad_packed_sequence support in dynamo module: rnn triaged module: fx oncall: pt2 module: dynamic shapes,2023-10-25 17:43:00+00:00,,0,7,module: rnn triaged module: fx oncall: pt2 module: dynamic shapes,False
112043,"AssertionError: graph-captured input #1, of type <class 'torch.Tensor'>, is not among original inputs of types… high priority triaged export-triage-review oncall: export",2023-10-25 17:42:58+00:00,,1,4,high priority triaged export-triage-review oncall: export,True
112029,DDPWrapper oncall: distributed,2023-10-25 16:07:54+00:00,,0,2,oncall: distributed,False
112028,"dynamo rewrite for allreduce, default group oncall: distributed module: dynamo",2023-10-25 16:07:08+00:00,,1,2,oncall: distributed module: dynamo,False
112024,torch.inference_mode and tensor subclass: RuntimeError: Cannot set version_counter for inference tensor module: autograd triaged actionable tensor subclass,2023-10-25 15:12:20+00:00,,0,2,module: autograd triaged actionable tensor subclass,True
112015,op scaled_dot_product_attention case different results oncall: transformer/mha,2023-10-25 10:23:04+00:00,,0,3,oncall: transformer/mha,False
112013,RuntimeError for hessian vector product with jvp triaged module: functorch,2023-10-25 09:06:39+00:00,,0,0,triaged module: functorch,True
112009,_functional_collectives.all_gather_into_tensor cannot compile in aot_module_simplified triaged oncall: pt2 module: dynamo,2023-10-25 08:00:10+00:00,,0,4,triaged oncall: pt2 module: dynamo,False
112006,Strided tensor in backward cause uninitialized output module: nn triaged,2023-10-25 06:43:04+00:00,,1,3,module: nn triaged,True
112004,DISABLED test_meta_outplace_fft_ihfftn_cpu_bool (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-25 06:39:31+00:00,,0,8,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112003,DISABLED test_meta_outplace_fft_ihfft_cpu_uint8 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-25 06:39:29+00:00,,0,2,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
112002,[v2.1.0 torch.compile]  triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-10-25 06:30:48+00:00,,0,1,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
111987,DISABLED test_meta_outplace_fft_ihfft_cpu_int8 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-25 00:56:57+00:00,,0,3,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111986,DISABLED test_meta_outplace_fft_ifft_cpu_int16 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-25 00:56:57+00:00,,0,8,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111968,DISABLED test_meta_outplace_fft_ihfft_cpu_int32 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-24 21:39:55+00:00,,0,2,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111967,DISABLED test_meta_outplace_addmm_cpu_complex64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-24 21:39:28+00:00,,0,8,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111966,[PT2] Graph break in forward pre-hook skips compiling forward for `nn.Transformer` triaged oncall: pt2 module: dynamo,2023-10-24 21:39:06+00:00,,1,3,triaged oncall: pt2 module: dynamo,False
111962,[pytree] `pytree.tree_map` does not respect type of `torch.Size` triaged module: pytree bug,2023-10-24 20:45:31+00:00,,0,5,triaged module: pytree bug,True
111958,torch 2.1 FSDP only some layers might not be working with training only a couple of layers triaged module: fsdp,2023-10-24 20:13:41+00:00,,0,7,triaged module: fsdp,True
111953,Missing a vectorized version of TORCH_CHECK triaged oncall: pt2 module: inductor,2023-10-24 19:35:51+00:00,,1,2,triaged oncall: pt2 module: inductor,False
111952,Inductor with cpu lowering fails to raise exception on invalid getitem triaged oncall: pt2 module: inductor,2023-10-24 19:18:36+00:00,,1,2,triaged oncall: pt2 module: inductor,True
111950,Operators that return dynamic-shape outputs that require_grad choke in AOTAutograd triaged oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,2023-10-24 18:48:07+00:00,,0,9,triaged oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,True
111943,Implement scatter and broadcast in functional collectives oncall: distributed,2023-10-24 17:58:02+00:00,,0,1,oncall: distributed,False
111931,[opcheck] Faster gradcheck execution triaged module: opcheck,2023-10-24 15:45:30+00:00,,0,5,triaged module: opcheck,True
111930,[opcheck] Way to reduce Hypothesis sampling when running opcheck triaged module: opcheck,2023-10-24 15:43:16+00:00,,0,1,triaged module: opcheck,True
111928,DISABLED test_meta_inplace_addmm_cpu_complex64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-24 15:39:43+00:00,,0,7,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111925,Resize warning in two argument torch.logical_* with broadcasting triaged module: boolean tensor module: safe resize,2023-10-24 15:37:51+00:00,,0,0,triaged module: boolean tensor module: safe resize,True
111924,[opcheck] Cannot share failures_dict between multiple tests with differing sets of tests they run  triaged module: opcheck,2023-10-24 15:37:26+00:00,,0,0,triaged module: opcheck,True
111918,Graph break doesn't result in continuation function when break happens in if-statement expression without inline function call triaged oncall: pt2 module: dynamic shapes release notes: dynamo,2023-10-24 14:47:52+00:00,,0,2,triaged oncall: pt2 module: dynamic shapes release notes: dynamo,False
111909,DISABLED test_meta_outplace_fft_ihfft_cpu_float64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped oncall: pt2,2023-10-24 12:46:00+00:00,,0,2,triaged module: flaky-tests skipped oncall: pt2,False
111908,Inconsistent Keyword Arguments behaviors in torch.triangular_solve() triaged topic: deprecation topic: docs module: python frontend,2023-10-24 11:42:42+00:00,,0,2,triaged topic: deprecation topic: docs module: python frontend,True
111907,Migration from c10::variant to std::variant causes undefined symbols when linking against older pytorch module: build triaged,2023-10-24 11:01:25+00:00,,0,7,module: build triaged,True
111905,Cannot build static windows libraries  module: build module: windows triaged module: static linking topic: binaries,2023-10-24 10:10:25+00:00,,0,6,module: build module: windows triaged module: static linking topic: binaries,True
111903,DISABLED test_meta_inplace_addmm_cpu_complex128 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-24 09:39:42+00:00,,0,11,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111902,torchscript file can not be loaded if its saved form the export model produced by torch.export.export oncall: quantization low priority triaged export-triage-review oncall: export,2023-10-24 09:00:32+00:00,,1,3,oncall: quantization low priority triaged export-triage-review oncall: export,True
111901,Multiprocess. DataLoader worker  is killed by signal: Segmentation fault. needs reproduction module: crash module: dataloader triaged,2023-10-24 08:11:27+00:00,,0,4,needs reproduction module: crash module: dataloader triaged,True
111900,"OOM when saving model(lora adapter), seems the clause ""FullyShardedDataParallel(model,...)"" will directly cause the OOM. oncall: distributed module: memory usage triaged module: fsdp",2023-10-24 07:47:04+00:00,,0,2,oncall: distributed module: memory usage triaged module: fsdp,True
111897,DISABLED test_meta_outplace_fft_ihfft2_cpu_int8 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-24 06:39:45+00:00,,0,2,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111896,ImportError: cannot import name 'external_utils' from partially initialized module 'torch._dynamo' needs reproduction module: binaries triaged module: docker topic: binaries,2023-10-24 06:28:07+00:00,,0,5,needs reproduction module: binaries triaged module: docker topic: binaries,True
111890,DISABLED test_cuda_stream_context_manager2 (__main__.CtxManagerTests) triaged module: flaky-tests skipped module: dynamo,2023-10-24 03:41:35+00:00,,0,1,triaged module: flaky-tests skipped module: dynamo,False
111884,Custom FFT implementation returns unexpected results when using torch.compile triaged module: complex module: correctness (silent) oncall: pt2,2023-10-24 01:35:25+00:00,,0,3,triaged module: complex module: correctness (silent) oncall: pt2,True
111874,Coalescing manager does not work w/device from torch.cuda.current_device() oncall: distributed,2023-10-24 00:37:27+00:00,,0,0,oncall: distributed,False
111863,Jit scripting support for `|` and mixing `typing`. oncall: jit,2023-10-23 23:47:20+00:00,,0,0,oncall: jit,True
111850,DISABLED test_cond_side_effects (__main__.MiscTests) triaged module: flaky-tests skipped module: dynamo,2023-10-23 21:39:55+00:00,,0,3,triaged module: flaky-tests skipped module: dynamo,False
111849,DISABLED test_meta_outplace_fft_ihfft2_cpu_int64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-23 21:39:52+00:00,,0,2,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111848,DISABLED test_cuda_stream_context_manager2_dynamic_shapes (__main__.DynamicShapesCtxManagerTests) triaged module: flaky-tests skipped module: dynamo,2023-10-23 21:39:49+00:00,,0,1,triaged module: flaky-tests skipped module: dynamo,False
111837,[dynamo] UnspecializedNNModuleVariable does not implement object identity triaged oncall: pt2 module: dynamo,2023-10-23 20:01:35+00:00,,0,3,triaged oncall: pt2 module: dynamo,False
111834,"Numerical inaccuracies in ""ddp_apply_optim_in_backward"" unit tests for gloo backend oncall: distributed",2023-10-23 19:34:58+00:00,,0,1,oncall: distributed,False
111830,[aotinductor]14k models: AttributeError: 'int' object has no attribute 'device' triaged oncall: pt2,2023-10-23 19:11:28+00:00,,1,3,triaged oncall: pt2,False
111825,DISABLED test_meta_outplace_fft_ihfft2_cpu_int16 (__main__.TestMetaCPU) triaged module: flaky-tests skipped oncall: pt2,2023-10-23 18:42:45+00:00,,0,3,triaged module: flaky-tests skipped oncall: pt2,False
111824,GroupNorm & InstanceNorm does not handle channels_last correctly module: nn module: cuda triaged module: memory format actionable,2023-10-23 18:22:50+00:00,,0,5,module: nn module: cuda triaged module: memory format actionable,True
111819,__slots__ + inheriting from torch.Tensor triaged tensor subclass module: python frontend,2023-10-23 18:05:17+00:00,,0,1,triaged tensor subclass module: python frontend,True
111818,[aotinductor] 14k models: Function input to foward() triaged oncall: pt2,2023-10-23 17:59:47+00:00,,1,5,triaged oncall: pt2,False
111813,[aot_inductor]14k models: RuntimeError: t == DeviceType::CUDA INTERNAL triaged oncall: pt2,2023-10-23 17:52:00+00:00,,1,4,triaged oncall: pt2,True
111806,Revisit security implications of #31875 triaged better-engineering topic: security security,2023-10-23 16:35:27+00:00,,0,4,triaged better-engineering topic: security security,True
111804,Dynamo - more closely tracker class type in UserDefinedObjectVariable triaged oncall: pt2 module: dynamo,2023-10-23 15:36:43+00:00,,1,0,triaged oncall: pt2 module: dynamo,False
111799,DISABLED test_meta_outplace_fft_ihfft2_cpu_float64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped oncall: pt2,2023-10-23 12:46:09+00:00,,1,2,triaged module: flaky-tests skipped oncall: pt2,False
111792,torch.jit.trace is not able to trace torch extension oncall: jit,2023-10-23 09:44:12+00:00,,0,2,oncall: jit,False
111789,Precisely monitor the collective communication tasks oncall: distributed triaged enhancement oncall: profiler,2023-10-23 09:25:44+00:00,,0,0,oncall: distributed triaged enhancement oncall: profiler,True
111786,pytorch support for cuda 12.2 ? module: cuda oncall: releng triaged needs design,2023-10-23 06:55:02+00:00,,0,6,module: cuda oncall: releng triaged needs design,True
111785,torch.compile precision bug when the attr object changes high priority triage review triaged oncall: pt2 module: dynamo,2023-10-23 06:40:09+00:00,,1,6,high priority triage review triaged oncall: pt2 module: dynamo,True
111784,DISABLED test_meta_outplace_fft_ihfft2_cpu_float32 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-23 06:40:02+00:00,,0,2,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111768,DISABLED test_meta_outplace_fft_ihfft2_cpu_bool (__main__.TestMetaCPU) triaged module: flaky-tests skipped oncall: pt2,2023-10-23 00:57:00+00:00,,1,7,triaged module: flaky-tests skipped oncall: pt2,False
111754,[dynamo] Better determinism of `ConfigModule` by walking using pytree triaged oncall: pt2 module: dynamo,2023-10-22 01:43:09+00:00,,1,0,triaged oncall: pt2 module: dynamo,False
111753,[dynamo] AutogradFunctionMethodHigherOrderVariable check for new guards is broken triaged module: dynamo,2023-10-22 01:28:12+00:00,,0,0,triaged module: dynamo,False
111752,Is it a good time to switch to CXX11_ABI? module: binaries triaged,2023-10-22 01:23:38+00:00,,0,2,module: binaries triaged,True
111739,grad is inf/nan when using torch.amp triaged module: amp (automated mixed precision),2023-10-21 08:04:24+00:00,,0,1,triaged module: amp (automated mixed precision),True
111733,Bug: torch.compile fails to compile torch.func.vmap with reduction functions and raw python numbers triaged oncall: pt2 module: functorch module: pt2-dispatcher,2023-10-21 05:56:06+00:00,,1,1,triaged oncall: pt2 module: functorch module: pt2-dispatcher,False
111713,[dynamo] generic `is_` type shortcut is not appropriately guarded triaged bug oncall: pt2 module: dynamo,2023-10-20 22:31:04+00:00,,1,1,triaged bug oncall: pt2 module: dynamo,True
111709,lintrunner job time keeps growing triaged better-engineering module: devx,2023-10-20 22:09:40+00:00,,0,6,triaged better-engineering module: devx,True
111706,DISABLED test_meta_outplace_fft_ifft_cpu_uint8 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-20 21:39:22+00:00,,0,8,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111704,Add more flexibility on print / output console module: cpp triaged enhancement,2023-10-20 21:00:57+00:00,,0,1,module: cpp triaged enhancement,True
111693,"[export] 14k models: AssertionError: graph-captured input # 2, of type <class 'torch.nn.parameter.Parameter'>, is not among original inputs of types triaged oncall: pt2 export-triage-review oncall: export",2023-10-20 19:37:57+00:00,,1,2,triaged oncall: pt2 export-triage-review oncall: export,False
111678,AOT Inductor Does not Work with minifier feature triaged oncall: pt2 module: inductor,2023-10-20 18:14:06+00:00,,0,0,feature triaged oncall: pt2 module: inductor,False
111676,[export] self.buffer += 1 raises error triaged export-triage-review oncall: export,2023-10-20 17:57:21+00:00,,0,0,triaged export-triage-review oncall: export,True
111674,Dynamo Compile samples should record file/line that raised exception triaged oncall: pt2 module: dynamo,2023-10-20 17:54:50+00:00,,1,0,triaged oncall: pt2 module: dynamo,True
111669,Buffer overflow not prevented on MPS devices module: error checking triaged module: advanced indexing module: mps,2023-10-20 17:05:00+00:00,,0,1,module: error checking triaged module: advanced indexing module: mps,True
111666,torch.onnx.errors.UnsupportedOperatorError: Exporting the operator 'aten::binary_cross_entropy' to ONNX opset version 14 is not supported. module: onnx triaged,2023-10-20 16:35:56+00:00,,0,2,module: onnx triaged,False
111663,[dynamo] Tracking: object identity triaged oncall: pt2 module: dynamo,2023-10-20 16:11:42+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
111654,"Static Linking C++, Op not available at runtime module: build module: cpp triaged enhancement module: vision has workaround",2023-10-20 14:36:53+00:00,,0,3,module: build module: cpp triaged enhancement module: vision has workaround,True
111651,DISABLED test_meta_outplace_fft_ifft_cpu_int64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-20 12:45:30+00:00,,0,6,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111646,torchrun: elastic training not restarted on missing keep-alive heartbeat/scale-down event oncall: distributed,2023-10-20 10:33:28+00:00,,0,0,oncall: distributed,False
111641,Can't export a pth model to onnx (RuntimeError: Couldn't lower all tuples) module: onnx triaged onnx-needs-info,2023-10-20 09:16:15+00:00,,0,1,module: onnx triaged onnx-needs-info,False
111640,[RFC] Enable Int8-Mixed-BF16 PT2E PTQ Quantization with Inductor oncall: quantization triaged,2023-10-20 07:44:11+00:00,,1,17,oncall: quantization triaged,False
111638,DISABLED test_meta_outplace_fft_ifft_cpu_float64 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-20 06:39:44+00:00,,0,7,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111636,torch2.1.0 DDP+compile+dynamic_shape cause error triaged oncall: pt2 module: dynamic shapes,2023-10-20 05:44:12+00:00,,0,7,triaged oncall: pt2 module: dynamic shapes,True
111634,Batched matmul gives incorrect result on MPS devices high priority triaged module: correctness (silent) module: mps,2023-10-20 05:21:17+00:00,,0,1,high priority triaged module: correctness (silent) module: mps,True
111632,"[dynamo][profiler] console spew of ...""torch._dynamo.variables.torch: [WARNING] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored"" for pages... module: logging triaged oncall: pt2",2023-10-20 04:51:51+00:00,,1,1,module: logging triaged oncall: pt2,False
111626,DISABLED test_meta_outplace_fft_hfft_cpu_uint8 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-20 00:56:46+00:00,,0,6,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111623,Missing `ignored_param` when calling wrapper_cls (FSDP) recursively oncall: distributed triaged,2023-10-20 00:17:23+00:00,,0,5,oncall: distributed triaged,True
111621,maximum Python version supported is not indicated module: docs triaged,2023-10-19 23:46:22+00:00,,0,0,module: docs triaged,True
111619,DISABLED test_cat_nhwc (__main__.TestQuantizedOps) triaged module: macos skipped,2023-10-19 23:17:29+00:00,,0,2,triaged module: macos skipped,False
111583,DISABLED test_vmapjvpall_linalg_det_singular_cpu_float32 (__main__.TestOperatorsCPU) triaged module: macos skipped,2023-10-19 17:21:09+00:00,,0,2,triaged module: macos skipped,False
111580,Dynamic shapes doesn't work for torch.diff / resize__symint in some cases triaged oncall: pt2 module: dynamic shapes,2023-10-19 16:52:33+00:00,,0,3,triaged oncall: pt2 module: dynamic shapes,True
111577,Prolonged network hiccup preventing retrieval of workflow job id triaged module: devx,2023-10-19 16:33:36+00:00,,0,0,triaged module: devx,True
111570,Tensor `.cuda()` very slow with specific array sizes  module: performance module: cuda triaged,2023-10-19 15:24:58+00:00,,0,3,module: performance module: cuda triaged,True
111569,"[dynamo] so-called global state guard is installed on global, when in fact values are thread-local triaged oncall: pt2",2023-10-19 13:30:28+00:00,,1,1,triaged oncall: pt2,False
111566,build: failure when building pytorch with TBB module: build triaged module: tbb,2023-10-19 12:42:25+00:00,,0,2,module: build triaged module: tbb,True
111564,misusing percision value in test_cuda function in torch/testing/_internal/common_nn.py. triaged module: testing,2023-10-19 11:56:06+00:00,,0,0,triaged module: testing,True
111563,"Higher-order derivatives extremely slow, increasing exponentially module: autograd triaged needs research",2023-10-19 11:46:56+00:00,,0,33,module: autograd triaged needs research,True
111562,[dynamo] `not aliased -> aliased` Guard only implemented for Tensors triaged oncall: pt2,2023-10-19 11:40:44+00:00,,1,4,triaged oncall: pt2,False
111560,Building docs fails triaged oncall: export,2023-10-19 09:05:15+00:00,,0,0,triaged oncall: export,True
111559,[RFC] Add GradScaler on CPU triaged module: half,2023-10-19 09:03:22+00:00,,0,0,triaged module: half,True
111552,[Bug]: some parameters' grad is None when using FSDP with torch2.1.0 oncall: distributed triaged module: fsdp,2023-10-19 07:02:00+00:00,,0,2,oncall: distributed triaged module: fsdp,True
111551,Custom `ModuleDict.__getitem__(key: tuple)` produces a graph break triaged oncall: pt2 module: dynamo,2023-10-19 06:55:18+00:00,,1,1,triaged oncall: pt2 module: dynamo,False
111550,[dynamo] Implement full `is_` checking triaged oncall: pt2 module: dynamo,2023-10-19 06:46:47+00:00,,1,3,triaged oncall: pt2 module: dynamo,False
111547,Bug with as_strided_tensorimpl for MPS devices triaged module: mps,2023-10-19 06:20:02+00:00,,0,1,triaged module: mps,True
111538,Propose to add constant padding mode to the `torch.nn.functional.grid_sample` function module: nn triaged has workaround needs design,2023-10-19 03:51:38+00:00,,0,2,module: nn triaged has workaround needs design,False
111525,Functorch FCD breaks with tensor subclasses triaged module: functorch module: first class dims,2023-10-19 01:19:27+00:00,,0,2,triaged module: functorch module: first class dims,True
111522,Insufficient hasattr guards on user defined objects triaged oncall: pt2 module: dynamo,2023-10-18 23:55:44+00:00,,0,3,triaged oncall: pt2 module: dynamo,False
111519,"[pt2+profiler] attach aot_id to CompiledFunction, _compiled_fn number without aot_autograd triaged oncall: pt2",2023-10-18 22:56:00+00:00,,1,0,triaged oncall: pt2,False
111517,MPS Performance regressions on Sonoma 14.0  high priority triaged module: mps,2023-10-18 22:13:03+00:00,,1,1,high priority triaged module: mps,True
111509,Sparse Tensor Sum Still Does Not Work for PyTorch Geometric module: sparse triaged,2023-10-18 21:16:16+00:00,,0,14,module: sparse triaged,True
111508,LBFGS accuracy difference between CPU and GPU needs reproduction module: optimizer triaged,2023-10-18 21:13:04+00:00,,0,4,needs reproduction module: optimizer triaged,True
111495,[ONNX][dynamo] Parameter to export flat graphs module: onnx triaged,2023-10-18 19:02:21+00:00,,3,2,module: onnx triaged,False
111482,"When keep_inference_input_mutations=True is set, one dynamic shape test fails triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher",2023-10-18 16:40:43+00:00,,1,3,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
111480,torch.jit.script persistently changes default from utf-8 to ascii oncall: jit,2023-10-18 16:11:12+00:00,,0,0,oncall: jit,True
111479,multi_head_attention_forward generates different values on MPS compared to CPU triaged module: mps,2023-10-18 14:34:59+00:00,,0,0,triaged module: mps,True
111473,"Rephrase sentence in ""Why and when to use sparsity"" for better understanding. module: docs triaged",2023-10-18 12:21:27+00:00,,0,0,module: docs triaged,True
111471,test_learnable_forward_per_channel fails due to integer overflow oncall: quantization triaged,2023-10-18 11:41:30+00:00,,1,2,oncall: quantization triaged,True
111466,yolov5_train feature module: cuda triaged module: determinism,2023-10-18 09:00:11+00:00,,0,1,feature module: cuda triaged module: determinism,True
111462,DISABLED test_meta_inplace_addmm_decomposed_cpu_complex128 (__main__.TestMetaCPU) triaged module: flaky-tests skipped module: primTorch oncall: pt2,2023-10-18 06:39:47+00:00,,0,11,triaged module: flaky-tests skipped module: primTorch oncall: pt2,False
111456,torch.autocast() hangs on CPUs module: cpu triaged module: amp (automated mixed precision),2023-10-18 03:46:51+00:00,,1,10,module: cpu triaged module: amp (automated mixed precision),True
111454,[ONNX][dynamo] Failed to export cumsum with dtype=float16 module: onnx triaged module: half,2023-10-18 02:47:20+00:00,,1,0,module: onnx triaged module: half,False
111450,[FX Quant] operator.matmul (@ operator ) is not converted to torch.ops.quantized.matmul oncall: quantization triaged,2023-10-18 01:46:56+00:00,,1,4,oncall: quantization triaged,True
111448,DISABLED test_compile_dtensor_redistribute_backward (__main__.TestDTensorCompileE2E) oncall: distributed triaged skipped,2023-10-18 01:22:14+00:00,,0,1,oncall: distributed triaged skipped,False
111441,torch.compile of simple loop takes 34 seconds triaged oncall: pt2 module: startup-tracing-compile time module: higher order operators module: pt2-dispatcher,2023-10-17 23:51:30+00:00,,1,8,triaged oncall: pt2 module: startup-tracing-compile time module: higher order operators module: pt2-dispatcher,True
111424,Multi-node torchrun  training job does not use IB Network oncall: distributed,2023-10-17 16:54:31+00:00,,0,1,oncall: distributed,False
111423,torch.compile x autograd.Function: Make the backward strict mode less srict module: autograd triaged oncall: pt2 module: dynamo module: pt2-dispatcher,2023-10-17 16:26:56+00:00,,1,0,module: autograd triaged oncall: pt2 module: dynamo module: pt2-dispatcher,False
111419,nonnull error needs reproduction module: build triaged,2023-10-17 12:34:54+00:00,,0,5,needs reproduction module: build triaged,True
111417,AOTAutograd generates wrong strides for view+inplace op triaged ezyang's list bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-10-17 08:30:40+00:00,,0,3,triaged ezyang's list bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
111416,The results of masked.log_softmax on MPS are inconsistent with those on CPU triaged oncall: transformer/mha module: correctness (silent) module: mps,2023-10-17 04:45:58+00:00,,0,2,triaged oncall: transformer/mha module: correctness (silent) module: mps,True
111414,Minifier doesn't transfer execution states like @torch.no_grad to repro triaged oncall: pt2,2023-10-17 03:11:57+00:00,,2,6,triaged oncall: pt2,False
111410,"Use of -Wl,--as-needed in cmake config files can leak into third-party users' code and modify their own private libraries module: build triaged topic: build",2023-10-17 01:39:07+00:00,,0,0,module: build triaged topic: build,True
111385,Torch Compile Dynamic fails on sample on diffusers VAE triaged oncall: pt2 module: dynamic shapes,2023-10-16 20:00:41+00:00,,1,2,triaged oncall: pt2 module: dynamic shapes,True
111384,[quant][pt2] Default batchnorm aten op has poor numerics during QAT high priority oncall: quantization triaged,2023-10-16 19:51:20+00:00,,1,0,high priority oncall: quantization triaged,True
111374,Tensor.lerp inconsistent when using -Infinity between MPS and CPU triaged module: mps,2023-10-16 15:28:51+00:00,,0,1,triaged module: mps,True
111370,Tracker for torch._numpy errors under dynamo triaged module: numpy module: dynamo,2023-10-16 12:01:26+00:00,,0,1,triaged module: numpy module: dynamo,False
111368,Implement device parameter in Dropout2d feature module: nn triaged,2023-10-16 11:50:49+00:00,,0,1,feature module: nn triaged,False
111366,RuntimeError: CUDAPluggableAllocator does not yet support cacheInfo feature triaged module: CUDACachingAllocator,2023-10-16 11:01:17+00:00,,0,0,feature triaged module: CUDACachingAllocator,True
111365,Failed to import transformer. high priority needs reproduction module: binaries module: windows triaged,2023-10-16 10:59:59+00:00,,0,2,high priority needs reproduction module: binaries module: windows triaged,True
111363,Simulating lower memory on GPU does not indicate simulated memory in error message module: cuda module: memory usage triaged module: CUDACachingAllocator,2023-10-16 09:41:01+00:00,,0,0,module: cuda module: memory usage triaged module: CUDACachingAllocator,True
111359,I have a trouble with to_symmetric needs reproduction module: sparse triaged,2023-10-16 07:40:09+00:00,,0,3,needs reproduction module: sparse triaged,True
111357,Couldn't export yolov7 quantized model to onnx module: onnx triaged onnx-triaged,2023-10-16 07:37:15+00:00,,0,2,module: onnx triaged onnx-triaged,True
111351,_foreach_copy_ supports fast copy between cpu and cuda devices. feature triaged module: mta,2023-10-16 03:58:57+00:00,,0,0,feature triaged module: mta,False
111349,CUDA version 12.2 has differential accuracy when executing CPU and GPU module: cuda triaged,2023-10-16 02:47:22+00:00,,0,0,module: cuda triaged,True
111348,Custom Tensor Instances Do Not Work With DDP oncall: distributed,2023-10-16 02:34:21+00:00,,0,0,oncall: distributed,False
111331,[dynamo] Proposal: `@init_values_once` API for initializing tensors and constants - without tracing the function in Dynamo feature module: optimizer triaged module: dynamo,2023-10-15 17:11:19+00:00,,0,9,feature module: optimizer triaged module: dynamo,False
111320,test_max_pool1d reliably OOMs after https://github.com/pytorch/pytorch/pull/111216/ module: memory usage triaged oncall: pt2,2023-10-15 02:58:38+00:00,,0,0,module: memory usage triaged oncall: pt2,False
111317,Torch 2.1 compile + FSDP (mixed precision) + LlamaForCausalLM: `RuntimeError: attempting to assign a gradient with dtype 'c10::BFloat16' to a tensor with dtype 'float'.` triage review oncall: distributed triaged module: fsdp oncall: pt2 module: distributed,2023-10-15 02:20:28+00:00,,1,10,triage review oncall: distributed triaged module: fsdp oncall: pt2 module: distributed,True
111279,Cannot use compiled model together with the ddp strategy oncall: distributed,2023-10-14 04:22:34+00:00,,0,5,oncall: distributed,False
111255,[AOTInductor] 14k models: AssertionError: Dynamo attempts to add additional input during export triaged oncall: pt2,2023-10-13 22:20:47+00:00,,0,0,triaged oncall: pt2,False
111254,[AOTInductor] 14k models: AssertionError: Failed to produce a graph during tracing. Tracing through 'f' must produce a single graph. triaged oncall: pt2,2023-10-13 22:16:49+00:00,,0,0,triaged oncall: pt2,False
111252,[AOTInductor] 14k models: UserError: Tried to use data-dependent value in the subsequent computation triaged oncall: pt2,2023-10-13 22:10:57+00:00,,0,0,triaged oncall: pt2,False
111250,"[AOTInductor] 14k models: AssertionError: original output #2 is None, but only the following types are supported triaged oncall: pt2",2023-10-13 21:58:37+00:00,,1,1,triaged oncall: pt2,False
111247,pytorch index_select is too slow module: performance module: cuda triaged,2023-10-13 21:53:59+00:00,,0,2,module: performance module: cuda triaged,True
111235,[dynamo] `ConfigModule`: Implement mechanism to hash non-`compile_ignored` configs quickly triaged oncall: pt2,2023-10-13 20:36:20+00:00,,0,0,triaged oncall: pt2,False
111227,"nanogpt_generate: C++ compile times out, because the generated .cpp file is too large. triaged oncall: pt2",2023-10-13 19:31:33+00:00,,1,1,triaged oncall: pt2,False
111224,[AOTInductor] 14K models: TypeError: make_boxed_func..g() missing 1 required positional argument: 'args' triaged oncall: pt2,2023-10-13 19:17:44+00:00,,1,1,triaged oncall: pt2,False
111223,[dynamo] Investigate interop issues with torch_scatter/torch_sparse/pyg_lib triaged oncall: pt2 module: dynamic shapes module: dynamo,2023-10-13 18:53:33+00:00,,0,2,triaged oncall: pt2 module: dynamic shapes module: dynamo,False
111220,[dynamo] Tracking: improve `ConfigModule` triaged oncall: pt2,2023-10-13 18:30:28+00:00,,0,0,triaged oncall: pt2,False
111210,"RuntimeError: Expected is_sm80 || is_sm90 to be true, but got false. (Using Google Colab) triaged",2023-10-13 16:42:11+00:00,,0,5,triaged,True
111209,[dynamo] annotate `allow_in_graph` with soft constraints module: optimizer triaged oncall: pt2 module: dynamo,2023-10-13 15:18:15+00:00,,0,0,module: optimizer triaged oncall: pt2 module: dynamo,False
111206,`torch.utils.checkpoint` drops custom Tensor attributes module: checkpoint triaged,2023-10-13 14:05:58+00:00,,0,0,module: checkpoint triaged,True
111312,Trace dynamic batch size with make_fx triaged actionable module: fx oncall: pt2 oncall: fx module: functorch module: dynamic shapes module: pt2-dispatcher,2023-10-13 10:00:30+00:00,,1,4,triaged actionable module: fx oncall: pt2 oncall: fx module: functorch module: dynamic shapes module: pt2-dispatcher,True
111194,Guards elimination for unused variables triaged oncall: pt2,2023-10-13 09:06:07+00:00,,1,1,triaged oncall: pt2,False
111190,[inductor][dynamic] fused_attention pattern could not be matched due to sym_size triaged oncall: transformer/mha module: dynamic shapes module: inductor module: multi-headed-attention inductor_pattern_match,2023-10-13 06:42:33+00:00,,1,3,triaged oncall: transformer/mha module: dynamic shapes module: inductor module: multi-headed-attention inductor_pattern_match,True
111187,"torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1331, unhandled cuda error (run with NCCL_DEBUG=INFO for details) oncall: distributed triaged module: wsl",2023-10-13 05:23:31+00:00,,0,4,oncall: distributed triaged module: wsl,True
111186,[ONNX][Exporter] Maintain support for exporter arguments export_params and keep_initializers_as_inputs module: onnx triaged,2023-10-13 04:59:16+00:00,,1,1,module: onnx triaged,False
111182,Training iresnet with torch.compile is slower than eager mode for torch 2.1.0 triaged oncall: pt2,2023-10-13 03:24:50+00:00,,1,3,triaged oncall: pt2,False
111173,unique(return_counts=True) fails on MPS for unsorted tensors with 1M+ elements triaged module: mps,2023-10-13 00:30:15+00:00,,0,1,triaged module: mps,True
111169,Result of adding noise is very different in mps vs cuda or cpu triaged module: mps,2023-10-12 23:18:27+00:00,,0,0,triaged module: mps,True
111168,Regression on CUDA 12.1 for vanilla transformer layer needs reproduction module: performance module: cuda triaged,2023-10-12 23:07:56+00:00,,0,10,needs reproduction module: performance module: cuda triaged,True
111159,Wrong onnx model from `torch.onnx.export` when using `index_add_` function with duplicate `index` values. module: onnx triaged module: advanced indexing,2023-10-12 20:41:19+00:00,,0,2,module: onnx triaged module: advanced indexing,False
111158,RuntimeError in run_streaming_llama.py When Using Accelerate with Streaming LLMa Model on A4500 GPU needs reproduction module: error checking triaged,2023-10-12 20:18:05+00:00,,0,3,needs reproduction module: error checking triaged,True
111157,overloads can perhaps be more performant? module: optimizer triaged module: dispatch module: codegen,2023-10-12 19:55:46+00:00,,0,7,module: optimizer triaged module: dispatch module: codegen,True
111150,[dynamo] `ConfigModule` and `config.patch` are not thread safe triaged oncall: pt2,2023-10-12 18:00:41+00:00,,0,0,triaged oncall: pt2,False
111142,'torch._C.Node' object has no attribute 'cs' oncall: jit,2023-10-12 16:30:09+00:00,,0,0,oncall: jit,False
111138,Module states cannot be fully synchronized due to the DDP broadcast_buffers breaking change oncall: distributed,2023-10-12 14:55:22+00:00,,0,0,oncall: distributed,False
111135,"No op for aten::where with argument types: Tensor, Tensor, bool. oncall: jit",2023-10-12 13:22:34+00:00,,0,0,oncall: jit,True
111133,Mismatch results of index_add_ between torch.compile Inductor backend and eager mode triaged ezyang's list module: functionalization bug oncall: pt2 module: pt2-dispatcher,2023-10-12 12:06:53+00:00,,1,7,triaged ezyang's list module: functionalization bug oncall: pt2 module: pt2-dispatcher,True
111131,"""Invalid Scalar type"" when using bf16 allreduce with Gloo backend oncall: distributed",2023-10-12 10:52:36+00:00,,0,0,oncall: distributed,False
111126,type promotion test for torch.div variants is broken triaged module: testing,2023-10-12 08:42:41+00:00,,0,0,triaged module: testing,True
111121,torch::serialize::OutputArchive::save_to crash if save on C:\\ module: windows module: serialization triaged,2023-10-12 06:16:31+00:00,,0,2,module: windows module: serialization triaged,True
111086,Build failure with Xcode 15 linker module: build triaged module: macos,2023-10-11 22:13:19+00:00,,0,1,module: build triaged module: macos,True
111085,"Getting ""master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified"" warning when using rdzv. oncall: distributed oncall: r2p",2023-10-11 22:08:40+00:00,,0,0,oncall: distributed oncall: r2p,False
111082,There is a performance drop because we have not yet implemented the batching rule for aten::mkldnn_rnn_layer_backward. triaged module: vmap module: functorch,2023-10-11 20:35:15+00:00,,1,4,triaged module: vmap module: functorch,True
111311,There is a performance drop because we have not yet implemented the batching rule for aten::mkldnn_rnn_layer_backward. triaged module: functorch,2023-10-11 20:33:10+00:00,,0,0,triaged module: functorch,True
111081,AOTAutograd perf: avoid as_strided() calls when we have intermediate bases module: performance triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-10-11 20:23:00+00:00,,0,7,module: performance triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
111077,dump_operator_names.cc uses std::cout but dose not include iostream triaged better-engineering actionable,2023-10-11 20:02:09+00:00,,0,1,triaged better-engineering actionable,True
111075,Export swallows exception good first issue triaged oncall: pt2 export-triaged oncall: export,2023-10-11 19:20:14+00:00,,1,3,good first issue triaged oncall: pt2 export-triaged oncall: export,True
111070,Option to disable fastpath in MHA needs design oncall: transformer/mha,2023-10-11 18:59:14+00:00,,0,0,needs design oncall: transformer/mha,False
111050,DISABLED test__int_mm (__main__.TestSelectAlgorithm) module: rocm triaged skipped,2023-10-11 18:06:22+00:00,,0,1,module: rocm triaged skipped,False
111047,[HigherOrderOp] cond  should accept pytree inputs triaged oncall: pt2,2023-10-11 17:32:34+00:00,,0,0,triaged oncall: pt2,False
111033,`CapabilityBasedPartitioner` returns invalid partitions. triaged module: xla oncall: pt2,2023-10-11 14:34:15+00:00,,1,0,triaged module: xla oncall: pt2,False
111029,"[JIT] Error when scripting wrapper of `matrix_norm` using `p: Union[str, int]`  oncall: jit",2023-10-11 13:13:54+00:00,,0,0,oncall: jit,True
111027,[PT2.1] SIGSEGV seen with view + sgn operator inside torch.compile triage review triaged ZeroTensor oncall: pt2,2023-10-11 10:09:20+00:00,,1,7,triage review triaged ZeroTensor oncall: pt2,True
111025,Unprompted UserWarning module: optimizer triaged,2023-10-11 08:36:50+00:00,,0,0,module: optimizer triaged,True
111024,ncu python conv2d.py runs indefinitely after activating cudnn.benchmark module: cudnn module: cuda triaged,2023-10-11 07:19:17+00:00,,0,0,module: cudnn module: cuda triaged,True
111020,[Dynamo] Error in speculate_subgraph doesn't report inner user stack trace triaged oncall: pt2 module: dynamo,2023-10-11 03:59:58+00:00,,1,5,triaged oncall: pt2 module: dynamo,True
111019,[Dynamo] Support more argument types for autograd Function speculate: HigherOrderOperator with body that accepts non-Tensors as input triaged module: dynamo,2023-10-11 03:57:54+00:00,,1,0,triaged module: dynamo,False
111003,Dynamo inlining should compile partial subgraphs triaged module: dynamo,2023-10-10 23:18:13+00:00,,0,5,triaged module: dynamo,False
110980,DISABLED test_jvp_linalg_det_singular_cpu_float32 (__main__.TestOperatorsCPU) triaged module: macos module: regression skipped module: functorch,2023-10-10 20:19:17+00:00,,0,6,triaged module: macos module: regression skipped module: functorch,False
110971,`pip install deepspeed` fails if number of GPUs greater than a certain small number? needs reproduction module: binaries triaged module: third_party,2023-10-10 18:57:38+00:00,,0,4,needs reproduction module: binaries triaged module: third_party,True
110966,`torch.is_autocast_enabled()` always False on CPU triaged module: amp (automated mixed precision),2023-10-10 18:35:06+00:00,,0,8,triaged module: amp (automated mixed precision),True
110959,`model.named_buffers()` fails if module not hashable. module: nn triaged,2023-10-10 17:37:02+00:00,,0,5,module: nn triaged,True
110957,kBackendDefaultTimeout is causing a timeout exception when rank 0 process exceeds 30 minutes preparing a dataset. oncall: distributed,2023-10-10 17:07:15+00:00,,0,2,oncall: distributed,True
110950,Enable more flake8-pyi ruff checks good first issue module: lint triaged actionable,2023-10-10 14:57:36+00:00,,0,2,good first issue module: lint triaged actionable,True
110946,RuntimeError: out_ptr == out_accessor[thread_count_nonzero[tid + 1]].data() INTERNAL ASSERT FAILED needs reproduction triaged module: advanced indexing,2023-10-10 12:55:19+00:00,,0,7,needs reproduction triaged module: advanced indexing,True
110944,gdb core dump when enable DEBUG mode to compile cpu torch in centos!!! needs reproduction triaged,2023-10-10 12:18:29+00:00,,0,1,needs reproduction triaged,False
110937,Issue with torch.distributed.launch oncall: distributed oncall: r2p,2023-10-10 09:37:58+00:00,,0,0,oncall: distributed oncall: r2p,True
110936,[inductor][cpu] [dynamic shapes][cppwrapper] performance regression module: cpu triaged oncall: pt2,2023-10-10 09:01:27+00:00,,1,1,module: cpu triaged oncall: pt2,False
110921,The NCCL kernel did not start as expected oncall: distributed,2023-10-10 04:03:18+00:00,,0,1,oncall: distributed,False
110905,"RuntimeError: !needs_dynamic_casting<func_t>::check(iter) INTERNAL ASSERT FAILED at ""../aten/src/ATen/native/cpu/Loops.h"":349, ... please report a bug to PyTorch. module: optimizer triaged",2023-10-09 22:52:38+00:00,,1,3,module: optimizer triaged,True
110904,[Inductor] `ConstantFolder` Utility Breaking in Recent Nightly triaged module: inductor,2023-10-09 22:44:38+00:00,,1,0,triaged module: inductor,True
110885,Depthwise conv3d slower than normal conv3d module: performance module: nn triaged needs research,2023-10-09 19:29:22+00:00,,0,1,module: performance module: nn triaged needs research,True
110883,MAX_JOBS ignored when compiling pytorch from source needs reproduction module: build triaged,2023-10-09 19:28:11+00:00,,0,2,needs reproduction module: build triaged,False
110871,[dynamo] Add asserts to prevent user defined objects/classes from going into ConstantVariable triaged module: dynamo,2023-10-09 16:40:48+00:00,,0,0,triaged module: dynamo,False
110866,Gradients (Jacobian) in inference module: onnx triaged onnx-triaged,2023-10-09 14:58:33+00:00,,0,5,module: onnx triaged onnx-triaged,True
110865,BCEWithLogitsLoss: Check if labels / targets are within zero and one module: nn triaged actionable,2023-10-09 14:48:49+00:00,,0,1,module: nn triaged actionable,True
110858,Broadcasting matmul is much slower than corresponding einsum module: performance module: cpu triaged,2023-10-09 10:03:41+00:00,,1,2,module: performance module: cpu triaged,True
110855,Matmul failure after dtype change on mixed AMD setup module: rocm triaged,2023-10-09 08:48:24+00:00,,0,4,module: rocm triaged,True
110843,"AOTAutograd: set_ under no_grad still triggers ""a view of a leaf Variable that requires grad is being used in an in-place operation"" triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher",2023-10-09 03:54:39+00:00,,0,2,triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
110842,AOTAutograd: set_ on input that ultimately no-ops fails in runtime_wrapper copy_ triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-10-09 03:37:10+00:00,,0,1,triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
110841,[Inductor] [cpu][amp]  Eager model failed to run for some torchbench models triaged module: cpu inductor,2023-10-09 02:48:39+00:00,,1,5,triaged module: cpu inductor,True
110819,Segmentation fault on aarch64 (Rpi4) using Pytorch 2.1.0 & torchaudio  module: binaries triaged,2023-10-08 15:04:07+00:00,,0,4,module: binaries triaged,True
110815,The derivation of swish activation function is wrong. caffe2 triaged,2023-10-08 07:54:03+00:00,,0,0,caffe2 triaged,True
110810,M2 Failing to build example-app in c++ needs reproduction module: binaries module: build module: cpp triaged module: macos,2023-10-08 05:08:13+00:00,,0,7,needs reproduction module: binaries module: build module: cpp triaged module: macos,True
110803,Toggling model.train() causes guard failures every time triage review triaged bug oncall: pt2 module: dynamo,2023-10-08 02:14:41+00:00,,0,7,triage review triaged bug oncall: pt2 module: dynamo,True
110801,ONNX converter does not properly trace dynamic axis through graph module: onnx triaged,2023-10-07 22:47:30+00:00,,0,0,module: onnx triaged,False
110795,Build process failure with torch_shm_manager module: build triaged,2023-10-07 16:31:56+00:00,,0,3,module: build triaged,True
110786,.lldbinit formatters only work when building with clang triaged release notes: devx topic: devs,2023-10-07 05:54:53+00:00,,0,0,triaged release notes: devx topic: devs,True
110780,backward and grad behave inconsistently w.r.t. set_ on leaf variable module: autograd triaged needs research has workaround,2023-10-07 03:30:06+00:00,,0,3,module: autograd triaged needs research has workaround,True
110779,"Repro for non-deterministic ""operation not permitted when stream is capturing"" crash oncall: distributed",2023-10-07 01:27:45+00:00,,0,2,oncall: distributed,True
110775,torch.distributed.pipeline skip module throws assert error that portal.grad is not None oncall: distributed,2023-10-07 00:04:43+00:00,,0,0,oncall: distributed,True
110761,Using ChainedScheduler with ReduceLROnPlateau leads to unexpected keyword argument error module: optimizer triaged needs design module: LrScheduler,2023-10-06 20:43:25+00:00,,0,7,module: optimizer triaged needs design module: LrScheduler,True
110758,"Fused Adamw RuntimeError: params, grads, exp_avgs, and exp_avg_sqs must have same dtype, device, and layout module: optimizer triaged",2023-10-06 20:22:03+00:00,,0,2,module: optimizer triaged,False
110738,Support using SymBool in arithmetics good first issue triaged bug oncall: pt2 module: dynamic shapes,2023-10-06 18:31:54+00:00,,1,7,good first issue triaged bug oncall: pt2 module: dynamic shapes,True
110737,pytorch consuming all cpu cores 100% on ARM module: binaries triaged module: openmp module: arm,2023-10-06 18:30:47+00:00,,0,6,module: binaries triaged module: openmp module: arm,True
110730,[dynamo]: `assert counter.frame_count == 1` is a bad practice when checking for no graph breaks triaged oncall: pt2 module: dynamo,2023-10-06 18:05:42+00:00,,0,8,triaged oncall: pt2 module: dynamo,False
110702,sliding_window attention in scaled_dot_product oncall: transformer/mha,2023-10-06 13:18:48+00:00,,0,0,oncall: transformer/mha,False
110701,Error with monai SwinUNETR and checkpointing needs reproduction module: cuda triaged,2023-10-06 12:32:03+00:00,,0,8,needs reproduction module: cuda triaged,True
110700,More informative variable names in AOTAutograd triaged module: fx module: aotdispatch module: pt2-dispatcher,2023-10-06 12:19:42+00:00,,0,0,triaged module: fx module: aotdispatch module: pt2-dispatcher,True
110698,max_pool3d_with_indices_backward_cuda and avg_pool3d_backward_cuda does not have a deterministic implementation feature module: nn triaged module: determinism,2023-10-06 11:30:11+00:00,,0,1,feature module: nn triaged module: determinism,True
110693,"onnx export jit.script ShapeInferenceError Unexpected axis value: 1. Expected range [-1, 1) module: onnx triaged",2023-10-06 10:16:40+00:00,,0,0,module: onnx triaged,True
110681,[RFC] Scaled Dot Product Attention  API Changes feature module: nn triaged module: python frontend module: multi-headed-attention,2023-10-06 02:38:07+00:00,,0,4,feature module: nn triaged module: python frontend module: multi-headed-attention,True
110669,Backward pass for Nested Tensors using flash attention in sdpa fails triaged module: nestedtensor oncall: transformer/mha module: multi-headed-attention,2023-10-06 00:32:43+00:00,,0,1,triaged module: nestedtensor oncall: transformer/mha module: multi-headed-attention,True
110649,opinfo split is confusing triaged module: testing,2023-10-05 21:10:15+00:00,,0,1,triaged module: testing,True
110641,`pytest test/dynamo -v ` fails locally high priority triaged oncall: pt2,2023-10-05 20:45:23+00:00,,1,6,high priority triaged oncall: pt2,True
110636,[discussion] Have PyTorch functions support python scalars (like NumPy) + introduce convenience constants like `torch.pi` and `torch.e` and maybe analogue of `scipy.constants` namespace triaged module: numpy module: python frontend,2023-10-05 18:58:21+00:00,,0,14,triaged module: numpy module: python frontend,True
110630,Memory efficient attention for tensors where the last dimension is not divisible by 8 triaged oncall: transformer/mha module: multi-headed-attention,2023-10-05 18:23:58+00:00,,0,0,triaged oncall: transformer/mha module: multi-headed-attention,True
110611,torch.compile CPU backend is slower than eager for several transcendental functions triaged module: inductor module: cpu inductor,2023-10-05 16:13:33+00:00,,2,3,triaged module: inductor module: cpu inductor,True
110610,DISABLED test_type_promotion__foreach_sub (__main__.ForeachTests) triaged module: flaky-tests skipped module: inductor,2023-10-05 15:39:56+00:00,,0,2,triaged module: flaky-tests skipped module: inductor,False
110605,ValueError issued instead of TypeError when tensor is cast to a scalar module: error checking triaged module: numpy,2023-10-05 14:41:21+00:00,,1,4,module: error checking triaged module: numpy,True
110602,AOTAutograd logging: log autograd graphs module: logging triaged module: aotdispatch module: pt2-dispatcher,2023-10-05 14:29:22+00:00,,1,3,module: logging triaged module: aotdispatch module: pt2-dispatcher,True
110599,[torch.compile] Multiple set operations don't work good first issue triaged oncall: pt2 module: dynamo release notes: dynamo,2023-10-05 13:54:42+00:00,,1,4,good first issue triaged oncall: pt2 module: dynamo release notes: dynamo,True
110595,Incorrect docstring / documentation for torch.nn.functional.scaled_dot_product_attention in 2.1 module: docs triaged module: multi-headed-attention,2023-10-05 12:03:26+00:00,,0,3,module: docs triaged module: multi-headed-attention,True
110594,Multiprocessing takes forever after on .get()  with mp.Queue() (Possible Deadlock) needs reproduction module: multiprocessing triaged,2023-10-05 09:43:28+00:00,,0,7,needs reproduction module: multiprocessing triaged,True
110588,libtorch.so: error adding symbols: file in wrong format needs reproduction module: build triaged,2023-10-05 06:54:49+00:00,,0,1,needs reproduction module: build triaged,True
110543,Clean way to distinguish python subclass NT vs. C++ NT triaged module: nestedtensor,2023-10-04 19:13:10+00:00,,0,4,triaged module: nestedtensor,True
110541,On the correctness of torch.signal.windows.cosine triaged module: numpy topic: bc breaking topic: docs,2023-10-04 18:55:16+00:00,,0,2,triaged module: numpy topic: bc breaking topic: docs,True
110525,performance drop because batching rule for aten::_scaled_dot_product_attention_math is not yet implemented module: vmap oncall: transformer/mha module: functorch,2023-10-04 15:27:09+00:00,,0,5,module: vmap oncall: transformer/mha module: functorch,False
110516,Torch Nested Issue With Backward Pass In Transpose triaged has workaround module: nestedtensor,2023-10-04 14:38:10+00:00,,0,2,triaged has workaround module: nestedtensor,True
110515,DynamicQuantizedLinear shows incorrect qscheme after applying eager mode dynamic quantization oncall: quantization triaged,2023-10-04 14:35:59+00:00,,1,6,oncall: quantization triaged,True
110507,doc modification of torch.nn.softshrink api module: docs module: nn triaged,2023-10-04 12:30:47+00:00,,0,0,module: docs module: nn triaged,True
110506,[dynamo] Slow compile times for optimizers due to for loops module: optimizer triaged module: dynamo,2023-10-04 12:16:09+00:00,,0,11,module: optimizer triaged module: dynamo,False
110505,scaled_dot_product returns NaN arrays with eval() oncall: transformer/mha,2023-10-04 07:43:21+00:00,,0,2,oncall: transformer/mha,False
110485,[export] `torch.tensor(0)` should not get burned in as a constant triaged oncall: pt2 module: aotdispatch module: dynamo oncall: export module: pt2-dispatcher,2023-10-03 23:21:10+00:00,,0,3,triaged oncall: pt2 module: aotdispatch module: dynamo oncall: export module: pt2-dispatcher,False
110479,[FSDP] [Checkpointing] Loading optimizer state dict with use_orig_params True causes OOM triaged module: fsdp,2023-10-03 22:11:55+00:00,,0,8,triaged module: fsdp,True
110476,[ONNX] Figure out aot inline strategy for Dort / onnxrt backend module: onnx triaged,2023-10-03 21:45:33+00:00,,0,0,module: onnx triaged,False
110461,Custom tensor attributes not preserved with registered functions triaged module: custom-operators module: library,2023-10-03 19:32:02+00:00,,0,4,triaged module: custom-operators module: library,True
110455,Local build breakage on AWS cluster module: build triaged,2023-10-03 18:01:02+00:00,,0,5,module: build triaged,True
110450,`test_pytorch_onnx_onnxruntime_cuda.py` is not run in CI module: onnx module: cuda module: tests triaged,2023-10-03 15:42:01+00:00,,0,0,module: onnx module: cuda module: tests triaged,True
110448,Explore Hybrid (CPU+GPU) Graphs in Scalar parameters feature triaged oncall: pt2 module: inductor,2023-10-03 14:51:14+00:00,,0,3,feature triaged oncall: pt2 module: inductor,False
110447,Using `torch.onnx.export` from file named `onnx.py` results in cryptic error message module: onnx triaged,2023-10-03 13:41:09+00:00,,0,1,module: onnx triaged,True
110439,Torch.onnx.export of module used positional and keyword arguments module: onnx triaged,2023-10-03 08:43:17+00:00,,0,2,module: onnx triaged,True
110436,Pytorch for Python 3.12 not available high priority module: build triaged module: python frontend,2023-10-03 05:22:03+00:00,,0,15,high priority module: build triaged module: python frontend,True
110422,jacrev Issue when Using Cuda triaged module: functorch,2023-10-02 23:53:57+00:00,,1,4,triaged module: functorch,True
110387,Different results for forward pass of two equal tensors through Conv2d triaged module: numerical-reproducibility module: memory format,2023-10-02 13:34:23+00:00,,0,13,triaged module: numerical-reproducibility module: memory format,True
110379,Pytorch LoadNativeLibrary issue oncall: mobile,2023-10-02 09:12:51+00:00,,0,0,oncall: mobile,True
110366,Categorical Simplex constraint throws error for valid values module: distributions triaged,2023-10-01 20:44:04+00:00,,0,0,module: distributions triaged,True
110363,"nn.BatchNorm2d (track_running_stats = True) causes ""modified by an in-place operation"" error when in torch.nn.parallel.DistributedDataParallel oncall: distributed triaged",2023-10-01 19:28:47+00:00,,0,1,oncall: distributed triaged,True
110356,"Dropout signature inconsistent between `torch.dropout`, `torch.nn.Dropout` and `torch.nn.functional.dropout` module: nn triaged module: python frontend",2023-10-01 13:37:59+00:00,,0,0,module: nn triaged module: python frontend,True
110347,ONNX export: TransformerEncoder is exported with fixed input dims module: onnx triaged,2023-10-01 04:05:35+00:00,,0,1,module: onnx triaged,False
110342,[inductor]: Not handling `ConcatKernel/NopKernel` fusions leads to suboptimal fusions triaged oncall: pt2 module: inductor,2023-09-30 23:50:00+00:00,,1,4,triaged oncall: pt2 module: inductor,False
110334,Perf-Drop (factor=2) Ubuntu-vs-Windows on same PC (dual-boot) module: windows module: cuda triaged,2023-09-30 11:16:01+00:00,,0,7,module: windows module: cuda triaged,True
110332,`torch.jit.load()` might unresponsive in IBM s390x when loading some certain torchscript saved by x86 machine. oncall: jit,2023-09-30 08:29:43+00:00,,0,0,oncall: jit,False
110331,"torch.Tensor.__repr__ causes torch.compile to error: ""got an unexpected keyword argument 'tensor_contents'"" triaged oncall: pt2",2023-09-30 07:53:42+00:00,,0,5,triaged oncall: pt2,True
110315,"torch._dynamo.exc.Unsupported: unexpected sourceless type bases: (<class 'torchrec.streamable.Pipelineable'>,) good first issue triaged ezyang's list oncall: pt2 module: dynamo oncall: export",2023-09-29 21:08:35+00:00,,1,4,good first issue triaged ezyang's list oncall: pt2 module: dynamo oncall: export,True
110304,"pytorch_stargan: ""_inductor/fx_passes/joint_graph.py"", line 166, in constant_fold_uniform_value KeyError ""val"" triaged oncall: pt2",2023-09-29 19:30:39+00:00,,1,0,triaged oncall: pt2,False
110295,Tests modify global state cause later tests to fail module: ci triaged module: devx,2023-09-29 18:24:09+00:00,,0,0,module: ci triaged module: devx,True
110291,AOT Autograd Neg View Support triaged module: functionalization oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-09-29 17:34:59+00:00,,0,5,triaged module: functionalization oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
110285,TypeError: Got unsupported ScalarType BFloat16 triaged module: numpy,2023-09-29 15:52:26+00:00,,0,4,triaged module: numpy,True
110281,feat(Pipeline Parallelism): use mincut optimization for local communication optimization oncall: distributed,2023-09-29 13:21:35+00:00,,0,0,oncall: distributed,False
114605,vmap: Transform single-element tensor to integer triaged module: functorch,2023-09-29 10:28:46+00:00,,0,1,triaged module: functorch,True
110259,Cannot avoid kineto_LIBRARY-NOTFOUND error when using pre-built pytorch module: binaries oncall: profiler,2023-09-29 00:31:26+00:00,,0,1,module: binaries oncall: profiler,True
110255,ONNX export of torch.nn.Transformer still fails module: onnx triaged oncall: transformer/mha,2023-09-28 23:38:03+00:00,,0,1,module: onnx triaged oncall: transformer/mha,False
110252,cuda/tf32 docs are outdated module: docs module: cuda triaged module: tf32,2023-09-28 22:49:27+00:00,,1,4,module: docs module: cuda triaged module: tf32,True
110250,Accessing Particular Nightly Builds Don't Work module: binaries triaged,2023-09-28 21:54:06+00:00,,0,14,module: binaries triaged,True
110249,`torch.func.functional_call` does not work with `__torch_function__ ` Tensor-like objects triaged enhancement module: __torch_function__ module: functorch,2023-09-28 21:49:11+00:00,,0,3,triaged enhancement module: __torch_function__ module: functorch,True
110238,PyTorch with non-shared build (building a single shared lib) is unsupported module: build triaged module: static linking,2023-09-28 18:40:26+00:00,,0,7,module: build triaged module: static linking,True
110210,DISABLED test_noncontiguous_samples__native_batch_norm_legit_cuda_float32 (__main__.TestCommonCUDA) triaged module: flaky-tests skipped oncall: pt2,2023-09-28 09:40:03+00:00,,0,2,triaged module: flaky-tests skipped oncall: pt2,False
110205,RuntimeError: Expected packed scalar Tensor to be of dimension 1. Got 0 instead. module: optimizer triaged module: regression actionable,2023-09-28 08:02:20+00:00,,0,4,module: optimizer triaged module: regression actionable,True
110198,DISABLED test_activations_abs__cpu (__main__.TestNestedTensorDeviceTypeCPU) triaged module: flaky-tests skipped oncall: pt2,2023-09-28 03:40:01+00:00,,0,5,triaged module: flaky-tests skipped oncall: pt2,False
110194,cudaMallocAsync cause too much fragmentation. module: cuda module: memory usage triaged module: CUDACachingAllocator,2023-09-28 02:32:05+00:00,,0,1,module: cuda module: memory usage triaged module: CUDACachingAllocator,True
110175,Module-level bufferization for torch dynamo module spanning multiple `fx.GraphModule` feature triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-09-27 18:04:36+00:00,,0,1,feature triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
110156,Add _worker_end_fn_t to the DataLoader module: dataloader triaged module: data,2023-09-27 13:13:56+00:00,,0,2,module: dataloader triaged module: data,True
110154,"ValueError: args contained 2 None's after flattening. When exporting a ScriptModule or ScriptFunction, no args may be None because that breaks type propagation. module: onnx triaged module: multi-headed-attention",2023-09-27 12:54:55+00:00,,0,0,module: onnx triaged module: multi-headed-attention,True
110148,Torch.onnx.dynamo_export stuck at reshape module: onnx triaged module: dynamo oncall: export,2023-09-27 09:33:35+00:00,,0,9,module: onnx triaged module: dynamo oncall: export,False
110137," custom_ops._destroy(""test::foo"") doesn't remove abstract_impl triaged oncall: pt2",2023-09-27 03:58:05+00:00,,1,0,triaged oncall: pt2,False
110136,Unbacked SymInts get reallocated whenever you repropagate fake tensors triaged oncall: pt2 module: fakeTensor module: dynamic shapes module: pt2-dispatcher,2023-09-27 03:55:20+00:00,,0,0,triaged oncall: pt2 module: fakeTensor module: dynamic shapes module: pt2-dispatcher,True
110135,logging stack_info doesn't do anything module: logging triaged oncall: pt2 module: ProxyTensor module: pt2-dispatcher,2023-09-27 03:53:10+00:00,,0,0,module: logging triaged oncall: pt2 module: ProxyTensor module: pt2-dispatcher,True
110131,Some ONNX tests have been disabled because of new tensor.split signature module: onnx triaged onnx-triaged,2023-09-27 02:02:00+00:00,,0,4,module: onnx triaged onnx-triaged,False
110130,Create a new heuristic TD rule for failures coming from base commit of the pull requests triaged module: devx,2023-09-27 01:35:06+00:00,,0,1,triaged module: devx,True
110116,Skip cuda kernel launch with torch.sum when dimension length is 0 module: performance module: cuda triaged,2023-09-26 21:38:24+00:00,,0,0,module: performance module: cuda triaged,True
110098,Dynamo tests in CI seem to not run at times high priority triaged oncall: pt2,2023-09-26 19:43:26+00:00,,1,8,high priority triaged oncall: pt2,True
110096,"GPT2ForSequenceClassification, LayoutLMForSequenceClassification: ""torch._dynamo.exc.Unsupported: call_function BuiltinVariable(setattr) [HFPretrainedConfigVariable(), ConstantVariable(str), ConstantVariable(str)] {}"" triaged oncall: pt2",2023-09-26 19:28:41+00:00,,1,0,triaged oncall: pt2,False
110089,"sam: AssertionError at torch/_inductor/graph.py `assert isinstance(value, (TensorBox, sympy.Expr))` triaged oncall: pt2 module: inductor module: export",2023-09-26 17:32:38+00:00,,1,9,triaged oncall: pt2 module: inductor module: export,False
110084,scatter_add: Mixing 0-dim and 1-dim tensors module: docs triaged module: python frontend module: edge cases,2023-09-26 16:24:02+00:00,,0,0,module: docs triaged module: python frontend module: edge cases,True
110080,Devices API feature triaged needs design topic: new features module: python frontend,2023-09-26 15:31:55+00:00,,0,3,feature triaged needs design topic: new features module: python frontend,False
110074,ImportError: libc10_cuda.so: cannot open shared object file: No such file or directory module: binaries module: build triaged,2023-09-26 12:23:52+00:00,,0,1,module: binaries module: build triaged,True
110065,[BUG] Elastic cannot kill all subprocesses after sending sigterm. oncall: distributed module: multiprocessing triaged module: elastic,2023-09-26 08:32:41+00:00,,0,2,oncall: distributed module: multiprocessing triaged module: elastic,True
110056,torch.onnx.export causes floating point exception with core dump for empty slice assignment module: onnx triaged,2023-09-26 04:47:26+00:00,,0,0,module: onnx triaged,True
110032,Race condition on shutdown involving PThreadPool and autograd triaged module: multithreading module: sanitizers,2023-09-25 20:46:41+00:00,,0,1,triaged module: multithreading module: sanitizers,True
110029,Dataloader resetting with num_workers=1 and persistent_workers=True module: dataloader triaged,2023-09-25 20:23:34+00:00,,0,0,module: dataloader triaged,True
110026,DISABLED test_raises_mesh_dim_less_than_2 (__main__.TestDeviceMeshGetItem) oncall: distributed module: flaky-tests skipped,2023-09-25 18:39:53+00:00,,0,1,oncall: distributed module: flaky-tests skipped,False
110014,tan/tanh discrepancies with complex due to jiterator module: cuda triaged module: jiterator,2023-09-25 15:52:03+00:00,,0,2,module: cuda triaged module: jiterator,True
110004,Please offer packages with local version `torch==2.1.0+cpu` for macOS module: build oncall: releng triaged,2023-09-25 09:52:02+00:00,,0,0,module: build oncall: releng triaged,True
110000,RuntimeError: device >= 0 && device < num_gpus INTERNAL ASSERT FAILED module: cuda triaged,2023-09-25 07:48:46+00:00,,0,1,module: cuda triaged,True
109991,Implmenet kthvalue for bfloat16 on CUDA module: cuda triaged module: bfloat16,2023-09-25 04:32:57+00:00,,0,0,module: cuda triaged module: bfloat16,True
109987,Static quantization for Transformer block : AttributeError 'function' object has no attribute 'is_cuda' triaged oncall: transformer/mha,2023-09-25 03:08:08+00:00,,0,3,triaged oncall: transformer/mha,True
109982,DISABLED test_cat_addmm (__main__.TestMaxAutotune) module: rocm triaged module: flaky-tests skipped module: inductor,2023-09-25 00:57:00+00:00,,0,3,module: rocm triaged module: flaky-tests skipped module: inductor,False
109970,torch-<version>.dist-info WHEEL file contains incorrect metadata for M1/M2 macOS platform module: binaries oncall: releng triaged module: m1,2023-09-24 10:46:36+00:00,,0,0,module: binaries oncall: releng triaged module: m1,True
109968,Dtype hard-coded in DataLoader (for python floats). triaged module: data,2023-09-24 10:33:55+00:00,,0,1,triaged module: data,True
109963,WelfordReduction seems to have invalid/dead code when reduction_numel <= 1 triaged module: inductor,2023-09-24 05:27:26+00:00,,1,0,triaged module: inductor,True
109958,How to compile torch 2.0.1 version from source? oncall: releng triaged,2023-09-24 00:53:04+00:00,,0,1,oncall: releng triaged,False
109948,Simple script segfaulting when grad is enabled module: autograd triaged needs design module: edge cases,2023-09-23 20:37:43+00:00,,0,2,module: autograd triaged needs design module: edge cases,True
109946,Indexed batch matrix multiplication to support MoEs and FFFs module: sparse triaged,2023-09-23 18:18:05+00:00,,0,1,module: sparse triaged,True
109943,Problems when loading PT files und Linux - Duda which are created under Mac Apple Silicon MPS triaged module: mps,2023-09-23 15:49:33+00:00,,0,1,triaged module: mps,True
109942,pytorch XLA document error module: docs triaged module: xla,2023-09-23 15:24:49+00:00,,0,0,module: docs triaged module: xla,True
109941,Need latest NCCL support to reduce GPU HBM consumption oncall: distributed module: nccl,2023-09-23 09:27:54+00:00,,0,0,oncall: distributed module: nccl,False
109938,Batching for is_in triaged module: functorch,2023-09-23 05:22:05+00:00,,0,0,triaged module: functorch,True
109934,test test_2d_fsdp_integration_fsdp_nested_param_groups failed oncall: distributed triaged,2023-09-23 01:15:45+00:00,,1,0,oncall: distributed triaged,True
109929,Memory access fault with AMD Rocm needs reproduction module: rocm triaged,2023-09-22 23:48:40+00:00,,0,4,needs reproduction module: rocm triaged,True
109926,cm3leon_generate failing compilation triaged oncall: pt2,2023-09-22 23:12:42+00:00,,1,8,triaged oncall: pt2,False
109923,Import order issue with torch and pybind11 Library Statically Linked to libstdc++ module: abi triaged module: static linking,2023-09-22 22:35:25+00:00,,0,0,module: abi triaged module: static linking,True
109909,Large Discrepancies between PyTorch and ONNXRuntime Inference  module: onnx triaged,2023-09-22 20:12:16+00:00,,0,0,module: onnx triaged,False
109895,moco: torch._dynamo.exc.Unsupported: hasattr: TensorVariable() triaged oncall: pt2,2023-09-22 17:47:20+00:00,,1,0,triaged oncall: pt2,False
109885,"DALLE2_pytorch: ""torch._dynamo.exc.Unsupported: call_method NNModuleVariable() eval [] {}"" triaged oncall: pt2 module: inductor",2023-09-22 14:44:30+00:00,,1,0,triaged oncall: pt2 module: inductor,False
109884,basic_gnn_gcn: ERROR:common:TypeError: object of type 'GreaterThan' has no len() triaged ezyang's list oncall: pt2 module: dynamic shapes module: inductor,2023-09-22 14:32:00+00:00,,1,4,triaged ezyang's list oncall: pt2 module: dynamic shapes module: inductor,True
109880,[FSDP ]How to convert sharded_state_dict files into full_state_dict offline without distributed process oncall: distributed triaged module: fsdp module: distributed_checkpoint,2023-09-22 13:44:11+00:00,,2,2,oncall: distributed triaged module: fsdp module: distributed_checkpoint,True
109874,[inductor][cpu] performance regression triaged oncall: pt2 module: inductor module: cpu inductor,2023-09-22 09:53:50+00:00,,1,3,triaged oncall: pt2 module: inductor module: cpu inductor,False
109873,Allow try except check for numpy bfloat16 representation triaged module: numpy,2023-09-22 09:49:47+00:00,,0,1,triaged module: numpy,True
109870,Wrongly returns nan for vectorized complex numbers division on PPC/ZArch triaged module: POWER,2023-09-22 09:03:25+00:00,,0,0,triaged module: POWER,True
109863,[BUG?] Why Allocator use stream to manage Block? module: cuda triaged,2023-09-22 04:21:18+00:00,,0,0,module: cuda triaged,True
109861,Cannot use constrain_as_size from fake tensor implementations: RuntimeError: tried to get Int out of SymInt triaged oncall: pt2 module: dynamic shapes oncall: export,2023-09-22 03:36:52+00:00,,2,0,triaged oncall: pt2 module: dynamic shapes oncall: export,True
109856,Severe performance regression on deterministic algorithm in torch 2.0 module: performance triaged module: cublas module: determinism,2023-09-22 03:23:53+00:00,,0,11,module: performance triaged module: cublas module: determinism,True
109854,"Directly support assert on Scalar, instead of forcing Tensor triaged oncall: pt2 oncall: export",2023-09-22 03:10:54+00:00,,2,2,triaged oncall: pt2 oncall: export,False
109850,torch._export has no logging module: logging triaged oncall: pt2 export-triaged oncall: export,2023-09-22 02:56:55+00:00,,1,0,module: logging triaged oncall: pt2 export-triaged oncall: export,True
109848,[dynamo][stream] Stream runtime operation in FX graph is ignored by remaining compiler oncall: distributed triaged ezyang's list oncall: pt2 module: aotdispatch module: dynamo module: pt2-dispatcher,2023-09-22 02:52:14+00:00,,1,4,oncall: distributed triaged ezyang's list oncall: pt2 module: aotdispatch module: dynamo module: pt2-dispatcher,False
109833,Implement Copy-on-write (COW) tensors module: internals triaged module: viewing and reshaping,2023-09-21 23:23:09+00:00,,1,3,module: internals triaged module: viewing and reshaping,True
109827,PIN disabled tests for the release module: binaries oncall: releng triaged topic: binaries,2023-09-21 21:19:34+00:00,,0,0,module: binaries oncall: releng triaged topic: binaries,False
109819,ValueError: only one element tensors can be converted to Python scalars needs reproduction triaged module: regression,2023-09-21 19:39:51+00:00,,0,8,needs reproduction triaged module: regression,True
109806,Incompatible dimensions error for FusedMatMul module: onnx triaged,2023-09-21 17:39:40+00:00,,0,0,module: onnx triaged,True
109802,Bits types cannot be used under deterministic mode triaged module: determinism,2023-09-21 17:22:51+00:00,,1,18,triaged module: determinism,True
109791,Heap-buffer-overflow during tensor unpickling module: serialization triaged,2023-09-21 15:14:34+00:00,,0,0,module: serialization triaged,True
109781,"`torch.embedding`, `weight[indices]`, `torch.index_select` returns random data with indices on meta device module: bc-breaking triaged module: embedding ezyang's list module: meta tensors topic: bc breaking",2023-09-21 10:42:12+00:00,,0,1,module: bc-breaking triaged module: embedding ezyang's list module: meta tensors topic: bc breaking,True
109777,Wrong vector shift results on PowerPC triaged module: vectorization module: POWER,2023-09-21 08:19:13+00:00,,0,2,triaged module: vectorization module: POWER,True
109774,[DDP + Dynamo] Tracing DDP AllReduce oncall: distributed feature triaged module: ddp oncall: pt2 module: dynamo module: distributed,2023-09-21 07:19:33+00:00,,1,3,oncall: distributed feature triaged module: ddp oncall: pt2 module: dynamo module: distributed,False
109770,Slow performance when running torch.jit traced model with Flash Attention using libtorch on Windows module: windows triaged oncall: transformer/mha,2023-09-21 04:47:05+00:00,,0,1,module: windows triaged oncall: transformer/mha,True
109768,LLaMA-2 70b model convert from PyTorch to ONNX format  module: onnx triaged,2023-09-21 03:50:22+00:00,,0,1,module: onnx triaged,False
109762,DTensor: summon full tensor API? oncall: distributed,2023-09-21 01:23:07+00:00,,0,3,oncall: distributed,False
109753,fp16 parity issue with traced code on GPU oncall: jit triaged module: half oncall: export,2023-09-20 23:55:15+00:00,,0,10,oncall: jit triaged module: half oncall: export,True
109747,[RFC][TorchElastic] topology info in training apps/ranks oncall: distributed triaged module: elastic,2023-09-20 22:37:30+00:00,,0,0,oncall: distributed triaged module: elastic,True
109725,Profiler should implicitly synchronize gpu devices  oncall: profiler,2023-09-20 17:25:25+00:00,,0,6,oncall: profiler,False
109724,assert_is_valid_input_type is too weak triaged module: dispatch,2023-09-20 17:22:06+00:00,,0,0,triaged module: dispatch,True
109719,Make torch.cuda.graphs.is_current_stream_capturing() available in TorchScript oncall: jit module: cuda graphs,2023-09-20 14:39:07+00:00,,0,0,oncall: jit module: cuda graphs,True
109706,Make standard container classes satisfy container Protocols. module: nn triaged needs research,2023-09-20 09:58:12+00:00,,0,2,module: nn triaged needs research,True
109700,[inductor][cpu] performance regression triaged oncall: pt2 module: inductor module: cpu inductor,2023-09-20 09:24:16+00:00,,3,5,triaged oncall: pt2 module: inductor module: cpu inductor,False
109699,[TorchScript] Support ScriptFunction arguments in torch.jit.script calls. oncall: jit,2023-09-20 09:15:45+00:00,,0,0,oncall: jit,False
109697,[DDP + Dynamo] Traceable DDP hooks oncall: distributed triaged oncall: pt2,2023-09-20 07:15:33+00:00,,1,3,oncall: distributed triaged oncall: pt2,False
109691,Extends the functionality of  `nn.BatchNorm1d`. oncall: distributed module: nn triaged needs research,2023-09-20 06:22:41+00:00,,0,9,oncall: distributed module: nn triaged needs research,True
109687,[RFC]: Moving most torch.compile backends out of core triaged topic: bc breaking dependency issue oncall: pt2 module: dynamo,2023-09-20 04:43:18+00:00,,0,19,triaged topic: bc breaking dependency issue oncall: pt2 module: dynamo,True
109675,[FSDP] UnpicklingError when calling save_state_dict in distributed run oncall: distributed triaged module: fsdp,2023-09-20 01:51:40+00:00,,0,6,oncall: distributed triaged module: fsdp,True
109666,FSDP: ShardedStateDict support for world_size = 1 oncall: distributed module: fsdp,2023-09-20 00:39:21+00:00,,0,1,oncall: distributed module: fsdp,False
109652,InstanceNorm does not catch dim mismatch module: nn triaged actionable,2023-09-19 21:49:44+00:00,,0,1,module: nn triaged actionable,True
109610,AsyncCompile loses useful exception backtrace in __get_result good first issue module: logging triaged oncall: pt2 module: inductor,2023-09-19 15:31:58+00:00,,0,9,good first issue module: logging triaged oncall: pt2 module: inductor,True
109592,test_memory_timeline fails on PPC due to extra temopraries triaged module: POWER oncall: profiler,2023-09-19 12:23:32+00:00,,0,0,triaged module: POWER oncall: profiler,True
109586,Max pool with negative integer inputs and channels_last memory layout gives the wrong values module: nn triaged,2023-09-19 08:21:03+00:00,,0,0,module: nn triaged,True
109585,[Torch-Onnx] Exporting the operator 'quantized::conv_transpose2d' to ONNX opset version 13 is not supported. module: onnx triaged,2023-09-19 07:48:04+00:00,,0,0,module: onnx triaged,False
109583,[dynamo][jagged tensor] Slow compilation time for a helper function of jagged tensor triaged oncall: pt2 module: dynamic shapes,2023-09-19 05:54:54+00:00,,0,2,triaged oncall: pt2 module: dynamic shapes,False
109582,Make Dropout take a dim=... argument module: nn triaged needs research has workaround,2023-09-19 05:23:50+00:00,,0,4,module: nn triaged needs research has workaround,True
109581,torch.optim.Adafactor feature module: optimizer triaged needs research,2023-09-19 05:20:14+00:00,,1,3,feature module: optimizer triaged needs research,False
109579,[Android: React Native] couldn't find DSO to load: libtorch-code-gen.so when loading model  oncall: mobile,2023-09-19 03:50:57+00:00,,0,0,oncall: mobile,False
109577,ONNX Export error module: onnx triaged,2023-09-19 03:40:57+00:00,,0,1,module: onnx triaged,True
109552,[fake/meta] Bad meta kernel for conv1d triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher,2023-09-18 22:49:15+00:00,,1,2,triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher,False
109539,Torch FX SubgraphMatcher Any / Oneof Patterns triaged module: fx module: fx.passes,2023-09-18 20:54:59+00:00,,0,1,triaged module: fx module: fx.passes,True
109528,attn_output_weights sometimes rerurn `None` triaged module: multi-headed-attention,2023-09-18 18:11:10+00:00,,0,0,triaged module: multi-headed-attention,True
109514,_assert_bound_is_rational can fail triaged module: dynamic shapes,2023-09-18 15:49:19+00:00,,0,1,triaged module: dynamic shapes,True
109504,[dynamo] torch._dynamo.exc.Unsupported: comparison SymNodeVariable() <built-in function is_> ListVariable() good first issue triaged oncall: pt2 module: dynamo,2023-09-18 15:25:02+00:00,,0,5,good first issue triaged oncall: pt2 module: dynamo,True
109497,Very big differences in output of `torch.lobpcg` (values and run-time) compared to SciPy on a very ill-conditioned Laplacian matrix triaged module: linear algebra,2023-09-18 13:41:02+00:00,,0,17,triaged module: linear algebra,True
109494,Performance degradation on AMD + A800 when computation is small module: performance module: cuda triaged,2023-09-18 12:37:01+00:00,,0,2,module: performance module: cuda triaged,True
109489,Investigate Strictness of torch.compile `is_big_gpu` feature triaged oncall: pt2 module: inductor,2023-09-18 10:44:49+00:00,,0,8,feature triaged oncall: pt2 module: inductor,False
109488,[bug] FALLBACK path has been taken inside: runCudaFusionGroup oncall: jit,2023-09-18 09:24:28+00:00,,0,1,oncall: jit,False
109484,[dynamo][symbolic shapes] Long compilation time for KJT helper function triaged oncall: pt2 module: dynamic shapes,2023-09-18 08:32:47+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes,False
109478,ProcessGroup is not automatically destroyed when the process exits oncall: distributed,2023-09-18 06:29:44+00:00,,0,4,oncall: distributed,False
109477,[DTensor] optimizer step performance is still too bad oncall: distributed module: dtensor,2023-09-18 05:39:51+00:00,,0,5,oncall: distributed module: dtensor,False
109462,Inconsistent behavior for in-place operations on coalesced sparse tensors module: sparse triaged,2023-09-17 16:45:33+00:00,,0,9,module: sparse triaged,True
109460,[BUG][pytree] treespec serialization for locally defined classes and namedtuple types triaged module: pytree,2023-09-17 13:48:15+00:00,,0,1,triaged module: pytree,True
109457,Training results from using MPS backend are poor compared to CPU and CUDA needs reproduction triaged module: mps,2023-09-17 11:34:28+00:00,,0,1,needs reproduction triaged module: mps,True
109453,Inconsistent Behavior of `ConvTranspose2d` on CPU and CUDA needs reproduction module: nn triaged,2023-09-17 06:11:15+00:00,,0,2,needs reproduction module: nn triaged,True
109446,torch pollutes libgomp symbols when import _C module: binaries triaged module: openmp module: third_party,2023-09-16 21:22:06+00:00,,0,1,module: binaries triaged module: openmp module: third_party,True
109445,Memory usage steadily increasing when using back propagation with sparse CSR parameter matrices on CPU module: sparse module: memory usage triaged,2023-09-16 20:47:59+00:00,,0,3,module: sparse module: memory usage triaged,True
109443,RNN Documentation is Confusing / Wrong module: docs module: nn module: rnn triaged actionable,2023-09-16 18:31:53+00:00,,0,4,module: docs module: nn module: rnn triaged actionable,True
109442,CPU memory cannot get released after `torch.compile` (caused by importing `AsyncCompile`) triaged oncall: pt2 module: inductor,2023-09-16 18:13:09+00:00,,1,1,triaged oncall: pt2 module: inductor,False
109440,[FSDP] supports QLora finetuning feature triaged module: fsdp,2023-09-16 17:15:17+00:00,,0,1,feature triaged module: fsdp,False
109425,Cannot export a quantized model that permutes a quantized tensor to ONNX module: onnx oncall: quantization low priority triaged,2023-09-16 04:08:35+00:00,,1,3,module: onnx oncall: quantization low priority triaged,True
109420,Supporting Block_Ptrs in inductor code gen feature triaged oncall: pt2 module: inductor,2023-09-16 01:38:12+00:00,,0,10,feature triaged oncall: pt2 module: inductor,False
109401,Interleaved isend and irecv causes hang oncall: distributed triaged,2023-09-15 21:39:55+00:00,,0,2,oncall: distributed triaged,True
109392,[FSDP] Implement additional check for turn on 2D TP + FSDP extension triaged module: fsdp,2023-09-15 17:50:34+00:00,,1,0,triaged module: fsdp,True
109386,Make Fx Generating Incorrect Graph For GPTQ model triaged module: fx module: ProxyTensor,2023-09-15 16:48:43+00:00,,0,4,triaged module: fx module: ProxyTensor,True
109385,FSDP crashes when submodule calls method that isn't `forward()` triaged module: fsdp,2023-09-15 16:31:52+00:00,,0,2,triaged module: fsdp,True
109383,cuda rng state for 2.0.1 cannot be used for 2.1.0 module: cuda triaged module: random,2023-09-15 16:06:26+00:00,,0,4,module: cuda triaged module: random,True
109379,DISABLED test_compute_local_shape_and_global_offset_1D (__main__.UtilTest) oncall: distributed module: flaky-tests skipped,2023-09-15 15:39:50+00:00,,1,3,oncall: distributed module: flaky-tests skipped,False
109362,DISABLED test_nondeterministic_alert_median_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-15 06:39:32+00:00,,0,2,module: tests triaged module: flaky-tests skipped oncall: pt2,False
109341,DISABLED test_nondeterministic_alert_kthvalue_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-15 00:56:47+00:00,,0,3,module: tests triaged module: flaky-tests skipped oncall: pt2,False
109321,DISABLED test_backend_match_guard_multi_threads (__main__.MiscTests) triaged module: flaky-tests skipped module: dynamo,2023-09-14 21:39:25+00:00,,0,27,triaged module: flaky-tests skipped module: dynamo,False
109310,DISABLED test_nondeterministic_alert_histc_cuda (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-14 18:39:23+00:00,,0,2,module: tests triaged module: flaky-tests skipped oncall: pt2,False
109309,Support the `ExitStack` context manager (or a simplified version) triaged oncall: pt2 module: dynamo,2023-09-14 18:36:01+00:00,,1,1,triaged oncall: pt2 module: dynamo,False
109303,Create static analysis tool to improve ONNX export success module: onnx triaged onnx-triaged release notes: onnx,2023-09-14 17:42:45+00:00,,1,0,module: onnx triaged onnx-triaged release notes: onnx,False
109294,Attribute 'kernel_shape' is expected to have field 'ints' when exporting a module with `List[Tensor]` inputs/outputs module: onnx triaged,2023-09-14 13:46:53+00:00,,0,0,module: onnx triaged,True
109292,aten::squeeze exported to ONNX as an `If` node module: onnx triaged,2023-09-14 12:56:02+00:00,,0,2,module: onnx triaged,False
109290,DISABLED test_nondeterministic_alert_bincount_cuda (__main__.TestTorchDeviceTypeCUDA) triaged module: flaky-tests skipped,2023-09-14 12:45:31+00:00,,0,2,triaged module: flaky-tests skipped,False
109289,PyTorch 2.1 smoke test requirements  triaged,2023-09-14 12:38:13+00:00,,3,2,triaged,False
109285,[inductor][cpu] perf regression triaged oncall: pt2 module: cpu inductor,2023-09-14 11:08:06+00:00,,2,1,triaged oncall: pt2 module: cpu inductor,False
109276,DISABLED test_nondeterministic_alert_MaxUnpool3d_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-14 06:39:42+00:00,,0,2,module: tests triaged module: flaky-tests skipped oncall: pt2,False
109268,DISABLED test_nondeterministic_alert_MaxUnpool3d_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-14 00:56:32+00:00,,0,2,module: tests triaged module: flaky-tests skipped oncall: pt2,False
109266,Add a unittest for ModuleWrapPolicy callable oncall: distributed good first issue triaged pt_distributed_rampup,2023-09-14 00:35:19+00:00,,0,4,oncall: distributed good first issue triaged pt_distributed_rampup,True
109263,[profiler] Show shapes for lists of tensors in chrome traces oncall: profiler,2023-09-14 00:11:45+00:00,,1,0,oncall: profiler,False
109261,[dynamo] Disable DDPOptimizer or error out if DDPOptimizer + static_graph is detected triaged module: ddp oncall: pt2 module: distributed,2023-09-13 23:37:29+00:00,,1,3,triaged module: ddp oncall: pt2 module: distributed,True
109260,[FSDP] Simplify `_fully_sharded_module_to_handle` triaged module: fsdp,2023-09-13 23:23:45+00:00,,0,0,triaged module: fsdp,True
109240,AOTAutograd should put keep mutations in the graph during training triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-09-13 19:17:32+00:00,,0,21,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
109237,"AOTAutograd should track view chains so it can replay them, instead of using as_strided. triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher",2023-09-13 19:11:13+00:00,,0,0,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
109236,Report name of defining class along side function name in Dynamo logs module: logging triaged oncall: pt2 module: dynamo,2023-09-13 19:07:05+00:00,,1,0,module: logging triaged oncall: pt2 module: dynamo,False
109235,ONNX exporter issue: fails to add conversions exporting T5 Transformer model module: onnx triaged,2023-09-13 19:06:32+00:00,,0,1,module: onnx triaged,False
109224,DISABLED test_nondeterministic_alert_MaxUnpool3d_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-13 18:39:32+00:00,,0,2,module: tests triaged module: flaky-tests skipped oncall: pt2,False
109213,inductor/test_max_autotune having timeout issues module: tests triaged oncall: pt2 module: inductor,2023-09-13 17:22:44+00:00,,1,2,module: tests triaged oncall: pt2 module: inductor,False
109204,Pytorch ROCM windows builds module: windows module: rocm triaged,2023-09-13 16:07:55+00:00,,0,0,module: windows module: rocm triaged,True
109195,DISABLED test_nondeterministic_alert_MaxUnpool2d_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-13 12:45:54+00:00,,0,1,module: tests triaged module: flaky-tests skipped oncall: pt2,False
109194,"`torch.mean` not supported for `torch.sparse_coo_tensor`, but `torch.sum` is supported (`scipy.sparse.coo_matrix` does support both `mean` and `sum`) module: sparse triaged enhancement",2023-09-13 11:14:17+00:00,,0,5,module: sparse triaged enhancement,False
109193,"F.conv2d(input, weight, bias, self.stride, RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR needs reproduction module: cudnn module: nn module: cuda triaged",2023-09-13 11:10:21+00:00,,0,2,needs reproduction module: cudnn module: nn module: cuda triaged,True
109191,Gradients across different ranks are not synchronized when using DDP oncall: distributed triaged,2023-09-13 10:27:51+00:00,,1,8,oncall: distributed triaged,True
109181,DISABLED test_nondeterministic_alert_MaxUnpool2d_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) triaged module: flaky-tests skipped,2023-09-13 06:39:39+00:00,,0,4,triaged module: flaky-tests skipped,False
109180,FSDP vs. MiCS oncall: distributed module: docs triaged module: fsdp,2023-09-13 06:14:27+00:00,,1,4,oncall: distributed module: docs triaged module: fsdp,True
109175,SparseSemiStructuredTensors are constructed differently from the original dense ones module: sparse triaged,2023-09-13 05:04:13+00:00,,1,9,module: sparse triaged,True
109166,NAN appears in the backward results of masked.cumprod on macos needs reproduction module: autograd triaged module: NaNs and Infs module: half module: mps,2023-09-13 03:06:49+00:00,,0,1,needs reproduction module: autograd triaged module: NaNs and Infs module: half module: mps,True
109162,DISABLED test_nondeterministic_alert_MaxUnpool2d_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-13 00:56:52+00:00,,0,2,module: tests triaged module: flaky-tests skipped oncall: pt2,False
109137,[ONNX] Provide an option to not generate `report_dynamo_export.sarif` module: onnx triaged,2023-09-12 19:29:01+00:00,,0,0,module: onnx triaged,False
109134,FSDP should have tests for partial state_dict and optim state_dict loading triaged module: fsdp module: distributed_checkpoint,2023-09-12 18:56:57+00:00,,0,0,triaged module: fsdp module: distributed_checkpoint,True
109131,Introduce 'backend' concept to torch.export.export API module: onnx feature triaged topic: new features oncall: export,2023-09-12 18:41:58+00:00,,1,3,module: onnx feature triaged topic: new features oncall: export,False
109130,DISABLED test_nondeterministic_alert_MaxUnpool1d_cuda_float64 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-12 18:39:22+00:00,,0,1,module: tests triaged module: flaky-tests skipped oncall: pt2,False
109113,"The API ""torch::jit::_load_for_mobile"" is limited to create an object living on the stack. oncall: jit",2023-09-12 14:52:30+00:00,,0,0,oncall: jit,True
109112,Unable to install the latest version of PyTorch using mamba. module: binaries triaged,2023-09-12 14:39:21+00:00,,0,2,module: binaries triaged,True
109108,Cannot construct `torch.sparse_coo_tensor` (but `scipy.sparse.coo_matrix` works fine): `TypeError: only integer tensors of a single element can be converted to an index` module: sparse triaged module: scipy compatibility,2023-09-12 13:27:01+00:00,,0,10,module: sparse triaged module: scipy compatibility,True
109103,"DDP - ""No backend type associated with device type cpu"" with new Model Phi 1.5 despite everything loaded on GPUs oncall: distributed",2023-09-12 12:14:58+00:00,,0,2,oncall: distributed,False
109100,FSDP do not support `ignored_parameters` when `auto_wrap_policy` is specified oncall: distributed triaged module: fsdp,2023-09-12 07:29:16+00:00,,0,4,oncall: distributed triaged module: fsdp,True
109098,Can't initializa NVML triaged,2023-09-12 06:28:46+00:00,,0,2,triaged,True
109094,Parameters of cuda module zero out when used in multiprocessing module: docs module: multiprocessing module: cuda triaged,2023-09-12 02:56:24+00:00,,0,2,module: docs module: multiprocessing module: cuda triaged,True
109074,torch.compile/triton holding GIL during compilation and CompiledKernel call results in deadlocks during distributed training high priority triaged oncall: pt2 module: inductor upstream triton,2023-09-11 22:32:08+00:00,,1,13,high priority triaged oncall: pt2 module: inductor upstream triton,True
109067,torch.argmax fails for device='mps:0' triaged module: mps,2023-09-11 21:33:55+00:00,,0,9,triaged module: mps,True
109062,CollectiveFunctionRewriteVariable for all_to_all_single triaged oncall: pt2 module: dynamo,2023-09-11 20:35:22+00:00,,1,0,triaged oncall: pt2 module: dynamo,False
109052,Allow reductions to write into pinned memory module: cuda module: memory usage triaged needs research module: reductions,2023-09-11 19:25:44+00:00,,0,7,module: cuda module: memory usage triaged needs research module: reductions,True
109017,torch.sparse_coo_tensor argname quirks + [feature request] `.numpy()`/`from_numpy` method for sparse_coo_tensor/sparse_csr_tensor (or maybe name them as `.scipy()`/`.from_scipy()` or at least under some `torch.utils.*` namespace module: sparse triaged module: scipy compatibility,2023-09-11 10:22:50+00:00,,0,5,module: sparse triaged module: scipy compatibility,True
109016,pr build failures in inductor dynamic shape test for operation tests with simple tensors. Side effect of current test framework triaged oncall: pt2 module: dynamic shapes module: cpu inductor,2023-09-11 09:42:43+00:00,,0,3,triaged oncall: pt2 module: dynamic shapes module: cpu inductor,True
109014,Cannot install torchmetrics - ERROR 403 high priority module: binaries triaged,2023-09-11 09:09:25+00:00,,0,20,high priority module: binaries triaged,True
109010,The following will always fail on NixOS module: cudnn triaged topic: build,2023-09-11 08:46:40+00:00,,0,0,module: cudnn triaged topic: build,True
109009,TypeError: mask must have dtype bool triaged module: masked operators,2023-09-11 08:13:31+00:00,,0,3,triaged module: masked operators,True
109002,[FSDP] How can I wrap a model that has both nn.Parameter and nn.Module oncall: distributed triaged module: fsdp,2023-09-11 04:13:14+00:00,,1,0,oncall: distributed triaged module: fsdp,True
109001,"Incorrect strides and accuracy when combining `torch.compile` with `op(out=out)` having complex number outputs, `test_ops::test_out` is bugged triaged module: complex bug oncall: pt2",2023-09-11 03:44:45+00:00,,0,2,triaged module: complex bug oncall: pt2,True
108999,[inductor][cpu] perf regression triaged oncall: pt2 module: inductor module: cpu inductor,2023-09-11 01:55:23+00:00,,1,2,triaged oncall: pt2 module: inductor module: cpu inductor,False
108993,DISABLED test_complex_half_reference_testing_pow_cuda_complex32 (__main__.TestCommonCUDA) triaged module: flaky-tests skipped module: nvfuser oncall: pt2,2023-09-11 00:56:36+00:00,,0,6,triaged module: flaky-tests skipped module: nvfuser oncall: pt2,False
108990,"`PYTORCH_TEST_WITH_INDUCTOR=1 python test/test_ops.py -k test_out_{warnings_, *}{_refs_, *}randn_cuda_float32` fails on main triaged module: random oncall: pt2 module: dynamo",2023-09-10 23:43:26+00:00,,0,4,triaged module: random oncall: pt2 module: dynamo,False
108984,PPC64le: GCC 11.2.1 Linker Error in bin/torch_shm_manager triaged topic: build bug,2023-09-10 19:39:25+00:00,,0,8,triaged topic: build bug,True
108983,ZeroTensor (and probably neg/conj) doesn't play well with wrapper tensor subclasses triaged module: __torch_dispatch__ ZeroTensor,2023-09-10 16:54:24+00:00,,0,7,triaged module: __torch_dispatch__ ZeroTensor,True
108977,[feature request] Provide some sparse eigen solver(s) for PyTorch (maybe via `ARPACK` as in scipy) + SPD sparse / laplace linear system solver - maybe NVidia AMGx library? module: sparse triaged module: linear algebra,2023-09-10 12:14:34+00:00,,0,10,module: sparse triaged module: linear algebra,True
108976,About FSDP oncall: distributed triaged,2023-09-10 10:04:24+00:00,,0,6,oncall: distributed triaged,True
108975, Exporting the operator 'aten::_convolution_mode' to ONNX opset version 14 is not supported. module: onnx triaged onnx-triaged onnx-needs-info release notes: onnx,2023-09-10 09:54:24+00:00,,0,5,module: onnx triaged onnx-triaged onnx-needs-info release notes: onnx,False
108971,Really slow compilation times for torch.compile causing distributed training errors triaged module: ddp oncall: pt2 module: startup-tracing-compile time module: distributed,2023-09-10 06:45:33+00:00,,0,3,triaged module: ddp oncall: pt2 module: startup-tracing-compile time module: distributed,True
108968,Unnecessary cuda synchronizations that we should remove in PyTorch module: performance module: cuda triaged,2023-09-10 04:51:03+00:00,,0,6,module: performance module: cuda triaged,True
108966,torch.compile graph breaks should be independent of DDP buckets triaged module: ddp oncall: pt2 module: distributed,2023-09-10 03:34:32+00:00,,0,4,triaged module: ddp oncall: pt2 module: distributed,True
108948,Add Lambert W function as torch.special.lambertw triaged module: special topic: new features,2023-09-09 15:26:22+00:00,,0,4,triaged module: special topic: new features,False
108942,Dynamo's eval_frame.c is not thread/subinterpreter safe triaged module: multithreading oncall: pt2 module: dynamo,2023-09-09 10:09:48+00:00,,1,3,triaged module: multithreading oncall: pt2 module: dynamo,False
108934,PPC64le: vsx_helpers.h errors module: build triaged,2023-09-09 03:36:25+00:00,,0,27,module: build triaged,True
108926,Support ONNX export for aten::select_backward and aten::slice_backward module: onnx triaged onnx-triaged,2023-09-08 23:10:42+00:00,,0,2,module: onnx triaged onnx-triaged,True
108909,DISABLED test_nondeterministic_alert_MaxUnpool1d_cuda_float32 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-08 21:40:51+00:00,,0,1,module: tests triaged module: flaky-tests skipped oncall: pt2,False
108900,[c10d] fix functional collective reduce op naming convention oncall: distributed triaged module: c10d,2023-09-08 20:44:18+00:00,,0,0,oncall: distributed triaged module: c10d,True
108898,[dynamo] Missing guard on global function triaged oncall: pt2 module: dynamo module: guards,2023-09-08 20:36:28+00:00,,0,0,triaged oncall: pt2 module: dynamo module: guards,False
108888,RuntimeError: Unrecognized tensor type ID: ZeroTensor triaged ZeroTensor,2023-09-08 19:02:33+00:00,,0,1,triaged ZeroTensor,True
108882,Inconsistent any( ) between cuda and cpu - Incorrect complex to bool conversion triaged module: complex module: correctness (silent),2023-09-08 18:02:03+00:00,,0,1,triaged module: complex module: correctness (silent),True
108877,[optimize_ddp] moco - NameError: name 's2' is not defined  oncall: distributed triaged module: ddp oncall: pt2 module: dynamic shapes module: distributed,2023-09-08 17:30:15+00:00,,0,8,oncall: distributed triaged module: ddp oncall: pt2 module: dynamic shapes module: distributed,True
108870,"Inconsistent, platform-dependent torch.ones_like behavior on metatensors triaged module: meta tensors",2023-09-08 15:29:22+00:00,,0,2,triaged module: meta tensors,True
108862,"A100 runners down: apt-get install nvidia-docker2, Could not get lock /var/lib/dpkg/lock-frontend triaged module: infra",2023-09-08 12:57:26+00:00,,0,7,triaged module: infra,True
108861,RuntimeError: DataLoader worker (pid 11011) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit module: dataloader triaged module: data,2023-09-08 11:07:31+00:00,,0,0,module: dataloader triaged module: data,True
108859,Certain torch functions are not handled by torch func wrapper triaged module: nestedtensor,2023-09-08 10:36:53+00:00,,0,0,triaged module: nestedtensor,True
108858,Ubuntu vs Windows: torch.cuda.OutOfMemoryError only happens on Ubuntu module: memory usage triaged,2023-09-08 09:54:41+00:00,,0,0,module: memory usage triaged,True
108857,DISABLED test_nondeterministic_alert_MaxUnpool1d_cuda_float16 (__main__.TestTorchDeviceTypeCUDA) module: tests triaged module: flaky-tests skipped oncall: pt2,2023-09-08 09:39:40+00:00,,0,2,module: tests triaged module: flaky-tests skipped oncall: pt2,False
108851,about nccl not work module: build triaged module: nccl,2023-09-08 07:30:57+00:00,,0,0,module: build triaged module: nccl,True
108840,Tensor Parallel doesn't work with torch.compile oncall: distributed triaged oncall: pt2 module: dtensor module: distributed,2023-09-08 05:00:14+00:00,,0,3,oncall: distributed triaged oncall: pt2 module: dtensor module: distributed,True
108835,CompileId in Dynamo log messages should include restart analysis count module: logging triaged oncall: pt2 module: dynamo,2023-09-08 03:44:33+00:00,,0,0,module: logging triaged oncall: pt2 module: dynamo,False
108833,Export should never unspec NN module triaged oncall: pt2 module: dynamo export-triage-review oncall: export,2023-09-08 03:41:43+00:00,,0,1,triaged oncall: pt2 module: dynamo export-triage-review oncall: export,False
108830,torch._export.pass_base.ExportPassBaseError: Unsupported target type: <function sym_min at 0x7ff25dd10670> triaged oncall: pt2 module: dynamic shapes export-triage-review oncall: export,2023-09-08 03:25:12+00:00,,1,1,triaged oncall: pt2 module: dynamic shapes export-triage-review oncall: export,False
108829,torch._dynamo.export produces object that is not pickleable module: serialization triaged oncall: pt2 oncall: export,2023-09-08 03:23:11+00:00,,0,0,module: serialization triaged oncall: pt2 oncall: export,False
108824,DISABLED test_lstm_packed (__main__.CPUReproTests) triaged skipped module: inductor module: cpu inductor,2023-09-08 01:45:14+00:00,,0,1,triaged skipped module: inductor module: cpu inductor,False
108805,Export torchvision detection model retinanet_resnet50_fpn triaged oncall: pt2 module: dynamic shapes oncall: export,2023-09-07 21:23:44+00:00,,0,2,triaged oncall: pt2 module: dynamic shapes oncall: export,True
108798,Dynamo Swallowing Exception In Lambda triaged oncall: pt2 module: dynamo,2023-09-07 19:51:51+00:00,,0,9,triaged oncall: pt2 module: dynamo,True
108794,[C++ Frontend] Simple Changes for Cleaner Options module: cpp triaged,2023-09-07 18:56:43+00:00,,0,0,module: cpp triaged,True
108784,Collect more env variables in `collect_env.py` triaged needs research,2023-09-07 16:16:32+00:00,,0,2,triaged needs research,False
108779,Tracing interpolate with tensor scale_factor is cursed triaged oncall: pt2 module: dynamic shapes oncall: export,2023-09-07 15:59:42+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes oncall: export,True
108777,RuntimeError: tried to get Double out of SymFloat triaged oncall: pt2 module: dynamic shapes,2023-09-07 15:38:45+00:00,,0,0,triaged oncall: pt2 module: dynamic shapes,True
108773,Sourceforge outage causing multiple CI failures module: ci triaged,2023-09-07 15:13:13+00:00,,1,2,module: ci triaged,True
108770,SDPA with nested backend: expose a way to avoid recomputing data layout information triaged oncall: transformer/mha,2023-09-07 14:41:59+00:00,,0,1,triaged oncall: transformer/mha,True
108752,[inductor][cpu] performance regression triaged oncall: pt2 module: cpu inductor,2023-09-07 06:36:53+00:00,,0,0,triaged oncall: pt2 module: cpu inductor,False
108747,Unable to compile the function which contains dict of types triaged oncall: pt2 module: dynamo,2023-09-07 04:57:12+00:00,,0,3,triaged oncall: pt2 module: dynamo,False
108746,[dtensor] Add debug tool to visualize sharding triaged module: dtensor,2023-09-07 03:56:57+00:00,,0,1,triaged module: dtensor,False
108745,[PT2.0] [.Compile] [Dynamic] Pytorch FX/JIT graph's inputs/nodes ordering is changed when FX recompiles even though the graph operations are same triaged oncall: pt2 module: dynamic shapes module: dynamo,2023-09-07 03:55:34+00:00,,0,4,triaged oncall: pt2 module: dynamic shapes module: dynamo,False
108744,switch more test cases to use MultithreadTestCase good first issue triaged module: dtensor,2023-09-07 03:40:20+00:00,,0,3,good first issue triaged module: dtensor,True
108743,DISABLED test_complex_half_reference_testing_fft_hfft2_cuda_complex32 (__main__.TestCommonCUDA) triaged module: flaky-tests skipped module: primTorch,2023-09-07 03:39:23+00:00,,0,5,triaged module: flaky-tests skipped module: primTorch,False
108742,[dtensor] enable tensor metadata check across ranks when run_check=True triaged module: dtensor,2023-09-07 03:34:56+00:00,,0,1,triaged module: dtensor,True
108739,"DDP Elastic ""master_addr"" resolution error in environment variables. oncall: distributed triaged module: elastic",2023-09-07 03:14:46+00:00,,1,1,oncall: distributed triaged module: elastic,True
108716,Support benchmark fusion for TemplateKernel triaged module: inductor module: dynamo,2023-09-06 23:43:53+00:00,,1,0,triaged module: inductor module: dynamo,False
108692,Adding Maximal Update Parametrization (µP) to torch.nn.init module: nn triaged needs research,2023-09-06 19:56:38+00:00,,0,1,module: nn triaged needs research,True
108676,RuntimeError when calling conv_transpose2d with groups needs reproduction module: nn triaged,2023-09-06 17:45:45+00:00,,0,2,needs reproduction module: nn triaged,True
108671,avg_pool3d_backward fails on meta with grad_input parameter triaged module: meta tensors,2023-09-06 17:19:40+00:00,,0,0,triaged module: meta tensors,True
108670,torch.jit.script produces incorrect gradients oncall: jit,2023-09-06 17:19:07+00:00,,0,3,oncall: jit,True
108665,INTERNAL ASSERT FAILED in `shape_type_inference.cpp` oncall: jit,2023-09-06 16:26:59+00:00,,0,0,oncall: jit,False
108651,libtorch: runtime error when iterating batch of dataloader module: cpp triaged,2023-09-06 13:10:06+00:00,,0,0,module: cpp triaged,True
108650,Unsupported: inline in skipfiles: Logger.info triaged oncall: pt2 module: dynamo module: graph breaks,2023-09-06 13:08:01+00:00,,0,0,triaged oncall: pt2 module: dynamo module: graph breaks,False
108865,Heap buffer overflow with `torch::load` on fuzzy data oncall: jit,2023-09-06 11:51:05+00:00,,1,14,oncall: jit,False
108645,uninformative OOM error module: cuda triaged,2023-09-06 09:56:49+00:00,,0,2,module: cuda triaged,True
108642,torch.topk returned values and indices are reordered if sorted=False triaged module: sorting and selection,2023-09-06 09:45:04+00:00,,0,4,triaged module: sorting and selection,True
108640,torch.onnx.export does not trace all outputs for the HF BLOOM model module: onnx triaged,2023-09-06 09:05:20+00:00,,0,0,module: onnx triaged,False
108636,torch.compile operation benchmark result is poor module: convolution triaged oncall: pt2 module: inductor module: cpu inductor,2023-09-06 08:28:19+00:00,,0,6,module: convolution triaged oncall: pt2 module: inductor module: cpu inductor,True
108627,autocast not consistent across different GPUs (A100 and RTX A6000) triaged module: amp (automated mixed precision),2023-09-06 06:21:49+00:00,,0,1,triaged module: amp (automated mixed precision),True
108621,[inductor] Triton matmul templates should use reduced_precision_reduction flags feature good first issue triaged module: half oncall: pt2 module: inductor matrix multiplication,2023-09-06 03:14:18+00:00,,1,1,feature good first issue triaged module: half oncall: pt2 module: inductor matrix multiplication,True
108602,torchrun fails to run on Windows 11 module: windows triaged,2023-09-05 22:44:12+00:00,,1,0,module: windows triaged,True
108569,Call for a deterministic implementation of scatter_add_cuda_kernel triaged module: scatter & gather ops,2023-09-05 15:02:12+00:00,,0,0,triaged module: scatter & gather ops,True
108567,Allow slicing of Nested Tensors along constant dimensions triaged module: nestedtensor,2023-09-05 14:50:12+00:00,,0,2,triaged module: nestedtensor,True
108565,`bytes(...)` support of torch tensor does not match numpy + it would be nice to support tensor.tobytes() as alias feature triaged module: numpy,2023-09-05 12:38:40+00:00,,0,17,feature triaged module: numpy,True
108532,"Breaking incompatibility with Cuda 12.2, pytorch stable, torchvision module: binaries module: crash module: cuda triaged",2023-09-04 20:14:33+00:00,,0,2,module: binaries module: crash module: cuda triaged,True
108522,nn.Transformer has dropout layers that BERT / GPT-2 do not have module: docs triaged oncall: transformer/mha,2023-09-04 16:01:31+00:00,,0,0,module: docs triaged oncall: transformer/mha,True
108521,"resutl of (torch.mm(a,b) does not match result of (a[:part,:], b) module: numerical-stability triaged matrix multiplication",2023-09-04 15:54:12+00:00,,0,2,module: numerical-stability triaged matrix multiplication,True
108520,[inductor] CPU int32 overflow behavior differs between clang and gcc triaged bug oncall: pt2 module: m1 module: inductor module: cpu inductor,2023-09-04 15:11:28+00:00,,0,10,triaged bug oncall: pt2 module: m1 module: inductor module: cpu inductor,True
108519,Pytorch profiler with Tensorboard example not working triaged module: tensorboard,2023-09-04 14:12:45+00:00,,0,1,triaged module: tensorboard,True
108514,torch model to onnx conversion success but failed when inference module: onnx triaged,2023-09-04 10:50:35+00:00,,0,0,module: onnx triaged,True
108500,[cond] cache size limit exceeeded triaged oncall: pt2 pre_dispatch tracing module: higher order operators module: pt2-dispatcher,2023-09-03 20:41:20+00:00,,1,7,triaged oncall: pt2 pre_dispatch tracing module: higher order operators module: pt2-dispatcher,False
108496,The CPU version of `torch.cummax` is slow module: performance module: cpu triaged,2023-09-03 17:42:17+00:00,,0,0,module: performance module: cpu triaged,True
108494,backend-friendly distributions module: distributions feature module: cuda triaged,2023-09-03 13:00:20+00:00,,0,1,module: distributions feature module: cuda triaged,True
108493,RWKV + Adam exp_avg_sq will change from positive to negative after loss.backward() needs reproduction module: optimizer triaged,2023-09-03 12:39:25+00:00,,0,3,needs reproduction module: optimizer triaged,True
108491,Suppport Fused AdamW on CPU module: performance feature module: optimizer triaged needs research,2023-09-03 07:20:27+00:00,,0,2,module: performance feature module: optimizer triaged needs research,True
108484,DistributedDataParallel to support __getattr__ oncall: distributed triaged,2023-09-02 21:31:21+00:00,,1,2,oncall: distributed triaged,True
108483,Efficient and robust calculation of diag(sparse @ diag @ sparse) module: sparse feature triaged,2023-09-02 19:00:57+00:00,,0,0,module: sparse feature triaged,True
108474,CNN w variable sized input performance regression 1.10.2 cu113 -> 2.0.1 cu117 module: performance module: nn module: cuda triaged module: regression,2023-09-02 05:10:01+00:00,,0,12,module: performance module: nn module: cuda triaged module: regression,True
108446,`SymInt` input doesn't get optimized out from `torch.compiled()` graph even if unused triaged oncall: pt2 module: dynamic shapes,2023-09-01 21:06:26+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes,False
108445,_foreach_copy_ with scalar second arg feature module: optimizer triaged actionable module: mta,2023-09-01 20:54:54+00:00,,1,3,feature module: optimizer triaged actionable module: mta,True
108442,Torch compile generates incorrect graph on Llama model high priority triaged module: regression oncall: pt2 module: dynamic shapes,2023-09-01 20:04:58+00:00,,1,1,high priority triaged module: regression oncall: pt2 module: dynamic shapes,True
108432,Wrong result of first run with torch.compile() when backend is using torch.jit.trace() and model has inplace operators  oncall: jit triaged oncall: pt2,2023-09-01 17:53:14+00:00,,0,2,oncall: jit triaged oncall: pt2,True
108407,torch.einsum() computes different results on cpu and cuda on A100 GPU. module: cuda triaged module: linear algebra,2023-09-01 09:19:57+00:00,,0,5,module: cuda triaged module: linear algebra,True
108406,enhance documentation around the developer build triaged topic: docs,2023-09-01 08:57:32+00:00,,0,1,triaged topic: docs,False
108404,multiple AMD GPUs module: multi-gpu module: rocm triaged,2023-09-01 08:22:32+00:00,,1,16,module: multi-gpu module: rocm triaged,True
108401,Crash on converting circular padding  to onnx module: onnx triaged,2023-09-01 07:10:17+00:00,,0,0,module: onnx triaged,True
108399,Generalize weight prepacking during quantized model deserialization oncall: quantization low priority triaged,2023-09-01 05:58:58+00:00,,1,6,oncall: quantization low priority triaged,True
108381,"FSDP always puts parameters to fp32 when loading state_dict, even if state_dict has bf16 params oncall: distributed",2023-08-31 23:02:32+00:00,,0,2,oncall: distributed,False
108378,NCCL ISend is not asynchronous  oncall: distributed module: c10d,2023-08-31 22:00:54+00:00,,0,2,oncall: distributed module: c10d,False
108342,ONNX export constant folding messes up with shared weight deduplication module: onnx triaged,2023-08-31 09:43:58+00:00,,0,2,module: onnx triaged,False
108341,"Added attention mechanism error,Need to modify torch.use_deterministic_algorithms(True) triaged module: determinism",2023-08-31 09:36:13+00:00,,0,0,triaged module: determinism,True
108332,RuntimeError: dims.value().size() == self->getMaybeRFactorDomain().size() oncall: jit,2023-08-31 07:48:19+00:00,,0,0,oncall: jit,False
108324,[inductor][cpu] Perf regression triaged oncall: pt2 module: cpu inductor,2023-08-31 03:14:18+00:00,,1,4,triaged oncall: pt2 module: cpu inductor,False
108300,Can't run Test/Inductor test: test_compiled_optimizers.py module: windows triaged oncall: pt2,2023-08-30 23:33:21+00:00,,0,9,module: windows triaged oncall: pt2,False
108277,Transformer performance drop due to slow PyTorch GEMMs module: performance module: cuda triaged,2023-08-30 20:08:29+00:00,,0,2,module: performance module: cuda triaged,True
108268,ONNX-FX based exporter documentation/tutorial topics for PyTorch 2.1 module: onnx triaged release notes: onnx,2023-08-30 18:41:02+00:00,,1,0,module: onnx triaged release notes: onnx,False
108246,pack_padded_sequence on GPU device triaged oncall: pt2,2023-08-30 16:46:36+00:00,,0,1,triaged oncall: pt2,False
108245,Integrate cutlass headers and scripts in pytorch package oncall: releng triaged topic: binaries oncall: pt2,2023-08-30 16:17:47+00:00,,0,10,oncall: releng triaged topic: binaries oncall: pt2,False
108244,Pytorch versions without the abi3 flag module: binaries triaged module: python frontend,2023-08-30 16:15:20+00:00,,0,2,module: binaries triaged module: python frontend,True
108241,Unrecognized attribute: axes for operator ReduceMean during onnx model conversion module: onnx triaged,2023-08-30 15:44:00+00:00,,0,2,module: onnx triaged,True
108231, DistributedSampler class: Change total_size into num_samples oncall: distributed triaged,2023-08-30 13:47:56+00:00,,0,4,oncall: distributed triaged,True
108226,torch.nn.functional.pad() with value type bool oncall: jit,2023-08-30 10:24:22+00:00,,0,1,oncall: jit,False
108225,"[docs] F.interpolate(uint8_input, mode = 'bicubic', ...) overshoot behavior: adjust the note in docs to explain that for uint8 saturating store is done and no manual clamp is needed or mention that bicubic is not supported for uint8 inputs module: docs module: nn triaged actionable",2023-08-30 10:21:01+00:00,,0,7,module: docs module: nn triaged actionable,False
108224,qnnpack quantized model can not be traced oncall: jit oncall: quantization low priority triaged,2023-08-30 08:41:40+00:00,,2,4,oncall: jit oncall: quantization low priority triaged,True
108212,Is there a standard procedure to check the consistency of environment across all nodes in PyTorch DDP training? oncall: distributed,2023-08-30 03:57:55+00:00,,0,0,oncall: distributed,False
108211,[Compile] Running Llama2 with torch.compile and FSDP results in Type mismatch assert in LlamaRotaryEmbedding  high priority oncall: distributed triaged module: fsdp oncall: pt2 module: distributed,2023-08-30 03:15:21+00:00,,1,6,high priority oncall: distributed triaged module: fsdp oncall: pt2 module: distributed,True
108210,Using distributed RPC and DDP together triggers error. oncall: distributed,2023-08-30 03:12:34+00:00,,0,1,oncall: distributed,False
108209,ninja: build stopped: subcommand failed. needs reproduction module: cpp-extensions triaged,2023-08-30 02:54:36+00:00,,0,2,needs reproduction module: cpp-extensions triaged,True
108197,AdaptiveMaxPool documentation is not detailed module: docs module: nn triaged actionable module: pooling,2023-08-29 23:55:40+00:00,,0,1,module: docs module: nn triaged actionable module: pooling,True
108190,[FSDP] incorrect backward prefetch order when using BackwardPrefetch.BACKWARD_POST triaged module: fsdp,2023-08-29 22:22:30+00:00,,0,0,triaged module: fsdp,True
108183,[Performance] Pass in head_size_og to FlashAttentionV2  triaged module: multi-headed-attention,2023-08-29 20:45:13+00:00,,1,0,triaged module: multi-headed-attention,True
108175,Enable FlashAttentionV2 on Windows triaged module: multi-headed-attention,2023-08-29 18:31:59+00:00,,0,0,triaged module: multi-headed-attention,True
108174,FlashAttentionV2 will OOM when building on ci/cd with default settings module: cuda module: ci triaged,2023-08-29 18:29:55+00:00,,1,4,module: cuda module: ci triaged,True
108158,TORCHELASTIC_RESTART_COUNT doesn't seem to be broadcasted to all worker triaged module: elastic oncall: r2p,2023-08-29 15:51:21+00:00,,0,0,triaged module: elastic oncall: r2p,True
108155,Automate release only changes in https://github.com/pytorch/pytorch/pull/108053 oncall: releng module: ci triaged,2023-08-29 15:09:23+00:00,,1,1,oncall: releng module: ci triaged,False
108152,`Tensor.uniform_` uses illegal argument name `from` module: distributions triaged module: python frontend,2023-08-29 14:43:21+00:00,,0,2,module: distributions triaged module: python frontend,True
108145,Problems hit when upgrading the version of HF used in CI triaged oncall: pt2 module: dynamic shapes,2023-08-29 14:07:18+00:00,,1,1,triaged oncall: pt2 module: dynamic shapes,False
108128,nccl:all_reduce is not profiled correctly oncall: distributed oncall: profiler,2023-08-29 04:24:47+00:00,,0,2,oncall: distributed oncall: profiler,False
108110,[inductor] Minifier fails on resnet50_quantized_qat triaged oncall: pt2 module: inductor,2023-08-29 00:12:38+00:00,,1,0,triaged oncall: pt2 module: inductor,False
108108,[BC BREAKING] Change default behavior of scaled_dot_product_attention's causal masking alignment module: bc-breaking module: nn oncall: transformer/mha topic: bc breaking,2023-08-28 23:47:02+00:00,,0,0,module: bc-breaking module: nn oncall: transformer/mha topic: bc breaking,True
108107,[inductor] soft_actor_critic training is slower than eager triaged oncall: pt2 module: inductor internal ramp-up task,2023-08-28 23:43:13+00:00,,0,0,triaged oncall: pt2 module: inductor internal ramp-up task,False
108102,_sampled_addmm_kernel cause 'misaligned address' with new triton pin module: sparse triaged,2023-08-28 22:37:00+00:00,,1,4,module: sparse triaged,True
108099,DISABLED test_multilayer_var_cpu (__main__.CpuTests) triaged module: macos skipped,2023-08-28 21:41:02+00:00,,0,1,triaged module: macos skipped,False
108095,[inductor] minifier fails on moco triaged oncall: pt2 module: dynamic shapes module: inductor,2023-08-28 20:56:32+00:00,,1,3,triaged oncall: pt2 module: dynamic shapes module: inductor,False
108090,[Optimizer Perf] Improve speed of _init_group to c++ module: performance module: optimizer triaged needs research,2023-08-28 19:47:57+00:00,,0,4,module: performance module: optimizer triaged needs research,True
108079,Aliased Input/Output Requirement in `aot_export_joint_simple` triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-08-28 18:21:02+00:00,,1,6,triaged bug oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
108047,DDP training can not accept subnet address in IPV6 oncall: distributed triaged,2023-08-28 12:29:24+00:00,,0,6,oncall: distributed triaged,True
108046,Enhanced Available Backend Discovery and Selection in PyTorch 2 triaged enhancement module: python frontend,2023-08-28 09:11:22+00:00,,0,5,triaged enhancement module: python frontend,True
108041,"Undefined Symobl: pybind11::detail::type_caster<at::Tensor, void>::load(pybind11::handle, bool) needs reproduction triaged module: pybind",2023-08-28 07:09:30+00:00,,0,5,needs reproduction triaged module: pybind,True
108030,DISABLED test_autocast_flash_attention (__main__.ActivationCheckpointingViaTagsTests) module: rocm triaged module: flaky-tests skipped module: dynamo,2023-08-28 00:56:42+00:00,,0,1,module: rocm triaged module: flaky-tests skipped module: dynamo,False
108023,[FSDP] Ignored modules on meta device seem to be initialized on CUDA device oncall: distributed triaged actionable module: fsdp,2023-08-27 10:03:27+00:00,,0,3,oncall: distributed triaged actionable module: fsdp,True
108022,ShapeEnv produce_guards AssertionError Triggered when tensor is resized triaged has workaround oncall: pt2 module: dynamic shapes module: dynamo,2023-08-27 08:53:38+00:00,,0,5,triaged has workaround oncall: pt2 module: dynamic shapes module: dynamo,False
108016,Failure in Initiating Pyotch DDP-style code ( Multi-machine multi-card environment) oncall: distributed triaged,2023-08-27 06:03:24+00:00,,0,1,oncall: distributed triaged,True
108014,NameError: name 's1' is not defined needs reproduction triaged oncall: pt2 module: dynamic shapes module: inductor,2023-08-27 02:03:08+00:00,,1,5,needs reproduction triaged oncall: pt2 module: dynamic shapes module: inductor,False
108013,Installation with rocm5.6 results in error: assert len(weights) == expected_node_count AssertionError needs reproduction module: build module: rocm triaged,2023-08-27 01:04:12+00:00,,0,1,needs reproduction module: build module: rocm triaged,True
107999,`upsample_bilinear2d_backward_out_cuda` is nondeterministic triaged module: determinism,2023-08-26 04:30:27+00:00,,0,3,triaged module: determinism,True
107980,DISABLED test_predispatch_with_for_out_dtype_nested_dynamic_shapes (__main__.DynamicShapesExportTests) triaged module: flaky-tests skipped module: dynamo,2023-08-25 21:39:28+00:00,,0,2,triaged module: flaky-tests skipped module: dynamo,False
108065,Batching rule for aten::_scaled_dot_product_attention_math not yet implemented triaged actionable module: vmap oncall: transformer/mha module: functorch,2023-08-25 17:01:16+00:00,,0,0,triaged actionable module: vmap oncall: transformer/mha module: functorch,True
107961,"aten.lift throws error in dynamo backends -> RuntimeError: !at::functionalization::impl::isFunctionalTensor(self)  INTERNAL ASSERT FAILED at ""../aten/src/ATen/FunctionalizeFallbackKernel.cpp"":167 triaged module: functionalization",2023-08-25 15:17:24+00:00,,0,2,triaged module: functionalization,True
107960,Torch compile: libcuda.so cannot found triaged dependency issue oncall: pt2,2023-08-25 15:01:07+00:00,,0,10,triaged dependency issue oncall: pt2,True
107955,PyTorch profile issues summary triage review module: regression oncall: profiler,2023-08-25 11:26:59+00:00,,0,3,triage review module: regression oncall: profiler,True
107950,DISABLED test_redundant_clone_for_layout_convert_cuda (__main__.FreezingCudaTests) module: rocm triaged module: flaky-tests skipped module: inductor,2023-08-25 09:39:41+00:00,,0,2,module: rocm triaged module: flaky-tests skipped module: inductor,False
107948,Exporting the operator 'aten::linalg_inv' to ONNX opset version 18 is not supported. module: onnx triaged,2023-08-25 08:25:31+00:00,,0,19,module: onnx triaged,False
107945,Torch 1.13 Onnx Scope constant name not correct! module: onnx triaged,2023-08-25 08:00:50+00:00,,0,0,module: onnx triaged,False
107929,Export to onnx error: RuntimeError: ArrayRef: invalid index Index = 3; Length = 3 module: onnx triaged,2023-08-25 04:47:18+00:00,,0,0,module: onnx triaged,False
107925,DISABLED test_conv_weight_layout_convert_cuda (__main__.FreezingCudaTests) module: rocm triaged module: flaky-tests skipped module: inductor,2023-08-25 03:39:28+00:00,,0,2,module: rocm triaged module: flaky-tests skipped module: inductor,False
107909,Provide a `reset_parameters()` method for MultiheadAttention to support FSDP meta device initializtion module: nn triaged module: fsdp,2023-08-25 00:00:11+00:00,,0,3,module: nn triaged module: fsdp,True
107898,[FakeTensor] fake tensor mode not working with inference mode on Tensor.item() triaged oncall: pt2 module: fakeTensor module: dynamic shapes module: pt2-dispatcher,2023-08-24 20:27:31+00:00,,0,9,triaged oncall: pt2 module: fakeTensor module: dynamic shapes module: pt2-dispatcher,True
107896,[feature request] [ux proposal] Min-max linear normalization to be supported in F.normalize (or in a new function) module: nn triaged topic: new features,2023-08-24 20:13:45+00:00,,0,2,module: nn triaged topic: new features,False
107893,DISABLED test_conv_with_as_strided_cpu (__main__.FreezingCpuTests) module: rocm module: cpu triaged skipped,2023-08-24 19:35:35+00:00,,0,2,module: rocm module: cpu triaged skipped,False
107879,FakeMode should not fakify non persistent buffer triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher,2023-08-24 17:43:49+00:00,,1,4,triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher,False
107873,"[BE] Consolidation of SymNode methods constant_int, maybe_as_int, etc triaged oncall: pt2",2023-08-24 16:20:20+00:00,,0,0,triaged oncall: pt2,False
107865,Graph break: call_function partial in skip_files triaged oncall: pt2 module: dynamo,2023-08-24 13:01:12+00:00,,1,1,triaged oncall: pt2 module: dynamo,False
107864,`C10_HOST_DEVICE` for `std::isnan(c10::complex<T>)`? module: cuda triaged,2023-08-24 12:06:07+00:00,,0,0,module: cuda triaged,True
107855,About the multi-node example not working properly oncall: distributed triaged,2023-08-24 07:44:12+00:00,,0,0,oncall: distributed triaged,True
107854,"""file_descriptor"" multiprocessing sharing strategy works incorrectly in dataloading  module: dataloader triaged module: data",2023-08-24 07:41:26+00:00,,0,1,module: dataloader triaged module: data,True
107842,nn.AdaptiveMaxPool2d returns identical results within a batch high priority module: nn module: cuda triaged module: correctness (silent) bug,2023-08-24 03:25:25+00:00,,0,3,high priority module: nn module: cuda triaged module: correctness (silent) bug,True
107841,Got Expand nodes with static shape input when exporting onnx model with dynamic shape  module: onnx triaged onnx-needs-info,2023-08-24 02:57:56+00:00,,0,3,module: onnx triaged onnx-needs-info,False
107830,FSDP custom args per module triaged module: fsdp,2023-08-23 23:54:39+00:00,,0,2,triaged module: fsdp,True
107824,torch.compile() fails when an `autograd.Function` gets called and torch.no_grad() is *not* being used oncall: distributed triaged ezyang's list oncall: pt2 module: dynamo,2023-08-23 22:08:03+00:00,,0,6,oncall: distributed triaged ezyang's list oncall: pt2 module: dynamo,False
107821,`torch.distributions.Pareto.sample` sometimes gives `inf` module: distributions triaged module: NaNs and Infs,2023-08-23 21:36:32+00:00,,0,1,module: distributions triaged module: NaNs and Infs,True
107820,`add_image_with_boxes` method from `torch.utils.tensorboard.writer.SummaryWriter` is broken triaged module: tensorboard oncall: visualization,2023-08-23 21:32:53+00:00,,0,1,triaged module: tensorboard oncall: visualization,True
107800,[feature request] [discussion] Include basic `ctypes` bindings for `cudart`/`cublasLt`/`cublas`/`nvrtc`/`cudnn` with stock PyTorch feature module: cuda triaged module: cublas,2023-08-23 18:10:56+00:00,,0,2,feature module: cuda triaged module: cublas,True
107797,"Fake Tensor error 'lengths' argument should be a 1D CPU int64 tensor, but got 1D meta Long tensor triage review module: performance oncall: pt2 module: fakeTensor mlperf module: pt2-dispatcher",2023-08-23 17:56:40+00:00,,0,3,triage review module: performance oncall: pt2 module: fakeTensor mlperf module: pt2-dispatcher,True
107780,Add caffe2 ideep/onednn tests to OSS CI oncall: releng module: ci triaged module: mkldnn,2023-08-23 13:27:51+00:00,,0,0,oncall: releng module: ci triaged module: mkldnn,True
107774,DISABLED test_conv_stride_constraints (__main__.CPUReproTests) module: rocm triaged skipped,2023-08-23 09:59:31+00:00,,0,1,module: rocm triaged skipped,False
107771,libtorch infer error : CUDNN_STATUS_INTERNAL_ERROR oncall: jit,2023-08-23 08:24:37+00:00,,0,1,oncall: jit,True
107770,libtorch vs (onnx+tensorRT) show different object detection results module: onnx triaged,2023-08-23 08:12:45+00:00,,0,0,module: onnx triaged,False
107751,conv cudnn support integers module: cudnn triaged,2023-08-22 23:19:43+00:00,,0,0,module: cudnn triaged,True
107739,DISABLED test_make_fx_symbolic_exhaustive_special_airy_ai_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-08-22 21:40:01+00:00,,0,18,triaged module: flaky-tests skipped module: ProxyTensor,False
107714,[ONNX] Retire FXSymbolicTracer in FX exporter module: onnx triaged onnx-triaged,2023-08-22 17:12:08+00:00,,1,0,module: onnx triaged onnx-triaged,True
107705,DISABLED test_multilayer_var_dynamic_shapes_cpu (__main__.DynamicShapesCpuTests) triaged skipped module: dynamic shapes,2023-08-22 16:25:27+00:00,,0,8,triaged skipped module: dynamic shapes,False
107703,"Hardtanh docs are inaccurate/incomplete, since hardtanh behaves like clamp module: docs triaged",2023-08-22 16:03:51+00:00,,0,0,module: docs triaged,False
107702,Inconsistencies when handling scalars that are out of the range relative to the input tensor's dtype triaged module: int overflow,2023-08-22 15:43:54+00:00,,0,0,triaged module: int overflow,True
107701,arange.out produces incorrect output when out tensor has dtype long triaged module: python frontend,2023-08-22 15:35:09+00:00,,0,2,triaged module: python frontend,True
107700,where.self_out doesn't fail gracefully when inputs have different dtypes triaged module: type promotion module: advanced indexing module: edge cases,2023-08-22 15:31:21+00:00,,0,2,triaged module: type promotion module: advanced indexing module: edge cases,True
107699,index.Tensor_out & index_put.out errors or segfaults with indices list containing only null tensors  high priority triaged module: advanced indexing,2023-08-22 15:26:46+00:00,,0,1,high priority triaged module: advanced indexing,True
107695,New variables in torch._ops.py pollute the torch.ops namespace triaged module: library,2023-08-22 14:48:55+00:00,,0,0,triaged module: library,True
107694,masked_fill_ outputs incorrect results for 'mps' tensor after transpose triaged module: mps,2023-08-22 14:33:52+00:00,,0,1,triaged module: mps,True
107693,Inconsistencies when casting to integral types triaged module: type promotion module: arm module: int overflow,2023-08-22 13:37:37+00:00,,0,4,triaged module: type promotion module: arm module: int overflow,True
107691,"torch._dynamo.exc.Unsupported: call_function BuiltinVariable(zip) [ListVariable(), ListVariable(), ListVariable(), UserDefinedObjectVariable(KJTList)] {} triaged oncall: pt2 module: dynamo",2023-08-22 13:17:33+00:00,,0,2,triaged oncall: pt2 module: dynamo,False
107685,Error in ONNX during Export GLU with Opset 18 module: onnx triaged,2023-08-22 12:58:26+00:00,,0,7,module: onnx triaged,True
107684,[Dynamo] 'NoneType' object is not subscriptable from torchrec (bad error message) triaged oncall: pt2,2023-08-22 12:55:51+00:00,,0,0,triaged oncall: pt2,True
107680,torch.nn.functional.cross_entropy different loss when providing one_hot_target and class weights module: nn module: loss triaged,2023-08-22 12:04:15+00:00,,0,1,module: nn module: loss triaged,True
107678,[Torch.fx] Torch fx failed to trace torch extension library triaged module: fx,2023-08-22 10:29:26+00:00,,0,2,triaged module: fx,True
107668,torch.dot gives wrong result on Macos high priority triaged module: macos module: correctness (silent),2023-08-22 07:17:17+00:00,,0,4,high priority triaged module: macos module: correctness (silent),True
107663,`RuntimeError: expected scalar type BFloat16 but found Float` with `torch.nn.TransformerEncoder` module: nn triaged oncall: transformer/mha module: amp (automated mixed precision),2023-08-22 05:46:29+00:00,,0,2,module: nn triaged oncall: transformer/mha module: amp (automated mixed precision),True
107661,A backward bug of dtensor seems to be caused by new_empty_strided high priority oncall: distributed triaged module: dtensor,2023-08-22 04:12:13+00:00,,1,5,high priority oncall: distributed triaged module: dtensor,True
107639,fullgraph=True doesn't actually raise error when you don't manage full graph inside DDP triaged oncall: pt2 module: dynamo,2023-08-21 21:28:36+00:00,,0,1,triaged oncall: pt2 module: dynamo,True
107637,[DDP PT2] TypeError: convert_frame_assert.<locals>._convert_frame_assert() missing 2 required positional arguments: 'hooks' and 'frame_state' triage review module: performance module: ddp oncall: pt2 module: dynamo mlperf module: distributed,2023-08-21 21:18:25+00:00,,0,3,triage review module: performance module: ddp oncall: pt2 module: dynamo mlperf module: distributed,True
107631,torch.fx.Interpreter modules don't get compiled triaged module: dynamo,2023-08-21 20:48:23+00:00,,1,4,triaged module: dynamo,True
107630,torch._dynamo.exc.InternalTorchDynamoError: 'NoneType' object has no attribute 'guards' triaged oncall: pt2 module: dynamo,2023-08-21 20:46:10+00:00,,1,3,triaged oncall: pt2 module: dynamo,False
107627,ModuleNotFoundError: No module named 'torchgen.code_template' module: build triaged module: android oncall: mobile,2023-08-21 20:36:44+00:00,,0,2,module: build triaged module: android oncall: mobile,True
107611,Previous version not found oncall: releng triaged,2023-08-21 18:07:59+00:00,,0,3,oncall: releng triaged,True
107605,Support AMD Ryzen Unified Memory Architecture (UMA) module: rocm triaged,2023-08-21 16:41:25+00:00,,0,0,module: rocm triaged,True
107593,dynamo: don't graph break on ctx.mark_dirty triaged module: dynamo,2023-08-21 14:15:00+00:00,,0,0,triaged module: dynamo,False
107591,`repeat_interleave` does not support tensor indexes on different devices while `repeat` does module: onnx triaged,2023-08-21 14:03:25+00:00,,0,2,module: onnx triaged,False
107590,Select on a coalesced COO tensor returns COO tensor with coalesce flag set to False. module: sparse feature triaged,2023-08-21 13:50:13+00:00,,0,0,module: sparse feature triaged,True
107587,Run transformers.OPTForCausalLM(config=config) occurs 'GraphModule' object has no attribute 'compile_subgraph_reason' triaged oncall: pt2 oncall: export,2023-08-21 12:27:49+00:00,,0,4,triaged oncall: pt2 oncall: export,False
107582,[FakeTensor] `to` doesn't error with `allow_non_fake_inputs=False` triaged module: fakeTensor,2023-08-21 11:00:36+00:00,,0,0,triaged module: fakeTensor,True
107581,[LibTorch/iOS] Building with METAL support script is freezing triaged oncall: mobile module: ios,2023-08-21 10:26:59+00:00,,0,6,triaged oncall: mobile module: ios,True
107580,Doc is unclear on how to install pytorch with Cuda via pip triaged topic: docs,2023-08-21 09:57:56+00:00,,0,9,triaged topic: docs,False
107575,halo，I continue pretrain llama2-13B model ，but save state_dict is about 50GB file oncall: distributed triaged,2023-08-21 07:46:21+00:00,,0,2,oncall: distributed triaged,True
107573,caching keys+values in TransformerDecoderLayer for faster inference triaged oncall: transformer/mha,2023-08-21 07:25:47+00:00,,0,3,triaged oncall: transformer/mha,True
107568,RuntimeError: Unsupported value kind: Tensor while torch.jit.script nn.Module oncall: jit,2023-08-21 04:00:42+00:00,,0,0,oncall: jit,True
107561,Dynamo guards on unused Tensor variables triaged oncall: pt2 module: dynamo,2023-08-21 00:52:52+00:00,,1,3,triaged oncall: pt2 module: dynamo,False
107556,Integer multiplication overflow when running torch.nn.AdaptiveAvgPool2d triaged module: int overflow,2023-08-20 17:56:12+00:00,,0,0,triaged module: int overflow,True
107555,Integer multiplication overflow when running torch.nn.MaxUnpool3d triaged module: int overflow,2023-08-20 17:54:59+00:00,,0,0,triaged module: int overflow,True
107554,Integer multiplication overflow when running torch.diagflat triaged module: int overflow,2023-08-20 17:53:26+00:00,,0,0,triaged module: int overflow,True
107553,Storage size calculation overflowed when torch.nn.Upsample triaged module: int overflow,2023-08-20 17:52:10+00:00,,0,0,triaged module: int overflow,True
107552,Storage size calculation overflowed when running torch.nn.functional.interpolate triaged module: int overflow,2023-08-20 17:37:18+00:00,,0,0,triaged module: int overflow,True
107550,Integer multiplication overflow when running torch.eye triaged module: int overflow,2023-08-20 17:32:11+00:00,,0,0,triaged module: int overflow,True
107548,Integer calculation overflow when running torch.nn.functional.adaptive_avg_pool2d triaged module: int overflow,2023-08-20 16:09:15+00:00,,0,0,triaged module: int overflow,True
107546,Integer overflow when running torch.nn.functional.upsample_bilinear triaged module: int overflow,2023-08-20 13:15:52+00:00,,0,0,triaged module: int overflow,True
107545,Integer overflow when running torch.nn.functional.upsample triaged module: int overflow,2023-08-20 13:12:49+00:00,,0,0,triaged module: int overflow,True
107544,Integer overflow when running torch.nn.ReplicationPad3d triaged module: int overflow,2023-08-20 13:03:07+00:00,,0,0,triaged module: int overflow,True
107543,Integer overflow when running torch.nn.AdaptiveAvgPool2d triaged module: int overflow,2023-08-20 12:24:46+00:00,,0,0,triaged module: int overflow,True
107541,Integer overflow when running torch.nn.MaxUnpool2d triaged module: int overflow,2023-08-20 12:04:34+00:00,,0,0,triaged module: int overflow,True
107540,Index out of bound when running torch.gather triaged module: advanced indexing,2023-08-20 12:01:24+00:00,,0,0,triaged module: advanced indexing,True
107539,Integer overflow when running torch.nn.functional.max_unpool2d triaged module: int overflow,2023-08-20 11:56:07+00:00,,0,1,triaged module: int overflow,True
107538,[fx] tracing function with in-place mutation results in unexpected behaviour due to local vars becoming persisted in  `GraphModule(nn.Module)` triaged module: fx,2023-08-20 10:27:30+00:00,,0,1,triaged module: fx,True
107536,Appending new logs to existing tbevent files when using tensorboard triaged module: tensorboard,2023-08-20 09:10:41+00:00,,0,0,triaged module: tensorboard,True
107534,NNPACK slow down M1/M2 Mac CPU triaged module: nnpack,2023-08-20 06:36:34+00:00,,0,2,triaged module: nnpack,True
107528,Inconsistent results when running torch.arctanh module: cuda triaged,2023-08-19 23:16:26+00:00,,0,1,module: cuda triaged,True
107522,Export of `quantized::linear_relu` operator not supported with `torch.onnx.export` module: onnx triaged,2023-08-19 20:07:50+00:00,,0,0,module: onnx triaged,False
107514,conv2d wrong results on 3090/3090ti triaged module: half,2023-08-19 15:01:32+00:00,,0,9,triaged module: half,True
107503,[nightly][jit] bad constant exponent (e+38.f) in default_program fused_mul_div_add oncall: jit triaged,2023-08-19 02:37:47+00:00,,0,1,oncall: jit triaged,True
107494,Mismatch in type of error raised when reducing along empty slice between eager and primtorch triaged module: primTorch,2023-08-18 23:38:53+00:00,,0,0,triaged module: primTorch,True
107478,Adding batched CSR tensors with different sparsities produces an invalid tensor module: sparse triaged module: correctness (silent) bug,2023-08-18 19:45:41+00:00,,0,0,module: sparse triaged module: correctness (silent) bug,True
107469,Reference cycles involving code -> co_extra -> compiled output -> reference to code triaged oncall: pt2 module: dynamo,2023-08-18 16:40:11+00:00,,1,8,triaged oncall: pt2 module: dynamo,False
107455,Moving tensor to MPS using .to(torch.device('mps') deletes entries from tensor triaged module: mps,2023-08-18 14:12:51+00:00,,0,1,triaged module: mps,False
107451,Conversion from COO with two sparse dimensions to CSR with dense_dim specified fails module: sparse feature triaged bug,2023-08-18 11:32:03+00:00,,0,0,module: sparse feature triaged bug,True
107444,[testing] dynamo testing: we should call `dynamo.reset` before running each test with dynamo. module: tests triaged oncall: pt2 module: dynamo,2023-08-18 08:13:03+00:00,,0,2,module: tests triaged oncall: pt2 module: dynamo,False
107443,Determinism by using datapipes shuffle module: dataloader triaged module: determinism,2023-08-18 06:53:17+00:00,,1,1,module: dataloader triaged module: determinism,True
107441,The generated triton MaxPool2d kernel has poor performance on amd vega20/60 module: rocm triaged module: inductor rocm,2023-08-18 04:52:53+00:00,,0,0,module: rocm triaged module: inductor rocm,True
107436,[FSDP]coding to multi-node save optimizer error triaged module: fsdp,2023-08-18 03:39:26+00:00,,2,1,triaged module: fsdp,True
107435,make backward function explicit in a layer which is a combination of some ops module: nn triaged oncall: profiler,2023-08-18 03:00:37+00:00,,0,2,module: nn triaged oncall: profiler,True
107433,No checks when running torch.nn.functional.ctc_loss with bogus inputs module: loss module: cpu triaged,2023-08-18 02:34:31+00:00,,1,5,module: loss module: cpu triaged,True
107432,"Inconsistent results when running torch.nn.functional.embedding_bag on CPU (1.12.0, 1.13.0) module: cpu triaged module: embedding",2023-08-18 02:32:42+00:00,,1,2,module: cpu triaged module: embedding,True
107429,Abort when running torch.set_num_interop_threads module: crash triaged module: single threaded,2023-08-18 01:36:33+00:00,,0,2,module: crash triaged module: single threaded,True
107426,DISABLED test_memory_format_nn_ConvTranspose1d_cuda_complex32 (__main__.TestModuleCUDA) module: nn triaged module: flaky-tests skipped,2023-08-18 00:56:54+00:00,,0,10,module: nn triaged module: flaky-tests skipped,False
107425,DISABLED test_wait_i_6 (__main__.TestMultiThreadedWait) oncall: distributed module: flaky-tests skipped,2023-08-18 00:56:50+00:00,,0,5,oncall: distributed module: flaky-tests skipped,False
107424,DISABLED test_wait_i_5 (__main__.TestMultiThreadedWait) oncall: distributed module: flaky-tests skipped,2023-08-18 00:56:47+00:00,,0,4,oncall: distributed module: flaky-tests skipped,False
107412,"max_pool1d, max_pool2d, max_pool3d Integers for cpu and cuda triaged actionable module: pooling",2023-08-17 22:26:01+00:00,,0,4,triaged actionable module: pooling,True
107402,Multiple runners shutdown for an autoupdate while still running jobs triaged ci: sev-mitigated,2023-08-17 18:41:02+00:00,,1,0,triaged ci: sev-mitigated,True
107396,[regression] Not getting `CUDA error: device-side assert triggered` on main for CUDA_KERNEL_ASSERT2 module: cuda triaged module: regression,2023-08-17 17:06:18+00:00,,0,9,module: cuda triaged module: regression,True
107394,[LibTorch/iOS] Unknown custom class type quantized.Conv2dPackedParamsBase. Please ensure it is registered triaged oncall: mobile module: ios,2023-08-17 17:00:08+00:00,,0,3,triaged oncall: mobile module: ios,True
107392,Overly strict type hints for `torch.utils.data.random_split` module: dataloader module: typing triaged,2023-08-17 16:53:23+00:00,,0,2,module: dataloader module: typing triaged,True
107389,caffe does not respect CUDNN_LIB_DIR when building from source (cmake) module: build triaged module: nvfuser,2023-08-17 16:23:29+00:00,,0,7,module: build triaged module: nvfuser,True
107387,Incorrect type hint for `torch.library.Library.define` module: typing triaged module: library,2023-08-17 16:17:24+00:00,,0,1,module: typing triaged module: library,True
107381,sparse_mask method ignores masked-in elements of sparse compressed input tensors module: sparse triaged module: correctness (silent) bug,2023-08-17 15:09:50+00:00,,0,5,module: sparse triaged module: correctness (silent) bug,True
107374,DataParallel scatter method split tensor wrong module: cuda triaged module: data parallel,2023-08-17 10:31:31+00:00,,0,1,module: cuda triaged module: data parallel,True
107372,torch compile error with SyncBatchNorm good first issue triaged module: c10d oncall: pt2 module: dynamo,2023-08-17 09:57:13+00:00,,1,3,good first issue triaged module: c10d oncall: pt2 module: dynamo,True
107363,Regression in text encoding module: performance module: cuda triaged module: multithreading module: regression,2023-08-17 06:38:56+00:00,,0,4,module: performance module: cuda triaged module: multithreading module: regression,True
107355,Getting more human-readable input and output names in the onnx model exported by torch module: onnx triaged oncall: pt2 oncall: export,2023-08-17 03:44:49+00:00,,0,1,module: onnx triaged oncall: pt2 oncall: export,False
107352,dist.scatter is incompatible with transpose/permute operation oncall: distributed triaged,2023-08-17 03:02:53+00:00,,0,1,oncall: distributed triaged,True
107321,"using Union[str, Tensor] as an argument to a torch.jit.script function oncall: jit triaged",2023-08-16 19:14:50+00:00,,0,2,oncall: jit triaged,True
107302,NumPy 2.0 Support triaged module: numpy module: python frontend,2023-08-16 15:50:58+00:00,,0,1,triaged module: numpy module: python frontend,True
107298,dist.destroy_process_group did not destroy the process group well oncall: distributed triaged,2023-08-16 15:12:16+00:00,,0,3,oncall: distributed triaged,True
107294,"'MPS' training Issue(s) with NanoGPT: -Inf, NaN's triaged module: mps",2023-08-16 12:55:24+00:00,,0,0,triaged module: mps,True
107286,Sparse compressed tensor values autograd support is not implemented module: sparse module: autograd triaged bug,2023-08-16 11:20:51+00:00,,1,0,module: sparse module: autograd triaged bug,True
107278,DISABLED test_find_or_create_pg (__main__.TestPgTag) oncall: distributed module: flaky-tests skipped,2023-08-16 03:40:41+00:00,,0,1,oncall: distributed module: flaky-tests skipped,False
107277,Translation layer (similar to torch_np) that can reliably lift Python operations into Tensor operations feature triaged oncall: pt2,2023-08-16 03:32:33+00:00,,0,0,feature triaged oncall: pt2,False
107276,CUBLAS_STATUS_NOT_SUPPORTED needs reproduction module: cuda triaged module: cublas,2023-08-16 03:26:12+00:00,,0,2,needs reproduction module: cuda triaged module: cublas,True
107269,model.forward() get error with torch.compile() when using huggingface llama triaged oncall: pt2 module: dynamo,2023-08-15 23:28:14+00:00,,1,13,triaged oncall: pt2 module: dynamo,True
107256,`torch.float8_e4m3fn` does not support `torch.cat` triaged module: float8,2023-08-15 20:52:10+00:00,,0,3,triaged module: float8,True
107253,Conda configuration shouldn't pollute $PATH variable oncall: releng module: ci triaged,2023-08-15 20:21:51+00:00,,0,0,oncall: releng module: ci triaged,True
107239,torch.compile not tracing ops on tensor subclass triaged module: __torch_dispatch__ oncall: pt2 module: dynamo module: pt2-dispatcher,2023-08-15 15:54:28+00:00,,1,2,triaged module: __torch_dispatch__ oncall: pt2 module: dynamo module: pt2-dispatcher,False
107238,How to export GNN with dict inputs correctly? module: onnx triaged,2023-08-15 15:43:12+00:00,,0,0,module: onnx triaged,True
107224,"[CPP API] Add Adadelta, Adamax, ASGD, NAdam, RAdam and Rprop module: cpp module: optimizer triaged needs research",2023-08-15 12:20:41+00:00,,0,3,module: cpp module: optimizer triaged needs research,True
107219,DISABLED test_RNN_input_size_zero (__main__.TestNN) module: rocm triaged skipped,2023-08-15 07:47:19+00:00,,0,2,module: rocm triaged skipped,False
107218,Documenting `__getitems__` for slicing support in `torch.utils.data` triaged module: data,2023-08-15 07:25:47+00:00,,0,1,triaged module: data,True
107217,Documenting `IterableDataset`'s needing `StopIteration` for finite data triaged module: data,2023-08-15 07:09:02+00:00,,0,0,triaged module: data,True
107214,The difference between input grad computed by channels last backward and the input grad computed by channels first backward of Hardswish on MPS is too large module: nn triaged module: memory format module: correctness (silent) module: mps,2023-08-15 06:31:49+00:00,,0,1,module: nn triaged module: memory format module: correctness (silent) module: mps,True
107211,[ONNX] ONNX doesn't support exporting non-persistent buffer included models in FakeMode module: onnx triaged onnx-triaged,2023-08-15 05:12:10+00:00,,0,1,module: onnx triaged onnx-triaged,False
107201,The difference between channels last backward and channels first backward of AvgPool2d on CUDA is too large module: nn module: cuda triaged module: memory format,2023-08-15 02:15:38+00:00,,0,0,module: nn module: cuda triaged module: memory format,True
107200,[inductor] [dynamic shape] 5 HF models fails with `Constraints violated` using transformers v4.31.0 triaged oncall: pt2 module: dynamic shapes module: dynamo,2023-08-15 01:59:42+00:00,,1,10,triaged oncall: pt2 module: dynamic shapes module: dynamo,False
107188,Can't construct a tensor from List[SymFloat] triage review module: performance oncall: pt2 module: dynamic shapes module: dynamo mlperf,2023-08-14 23:33:27+00:00,,1,6,triage review module: performance oncall: pt2 module: dynamic shapes module: dynamo mlperf,True
107183,DISABLED test_RNN_dropout_state (__main__.TestNN) module: rocm triaged skipped,2023-08-14 22:24:35+00:00,,0,1,module: rocm triaged skipped,False
107177,Timeout during NCCL initialization due to store oncall: distributed better-engineering,2023-08-14 21:23:07+00:00,,0,2,oncall: distributed better-engineering,False
107175,"sdp_kernel causes dynamo error on torch.compile(model, fullgraph=True) good first issue triaged oncall: pt2 module: dynamo",2023-08-14 21:05:05+00:00,,1,1,good first issue triaged oncall: pt2 module: dynamo,True
107174,Surface NCCL and CUDA version incompatibility oncall: distributed better-engineering,2023-08-14 20:57:28+00:00,,0,5,oncall: distributed better-engineering,False
107173,Dynamo test_vmap failures on Python-3.8 triaged oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher,2023-08-14 20:39:33+00:00,,0,0,triaged oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher,False
107170,Torch randn cannot take symbol shapes as shape argument. triaged oncall: pt2 module: dynamic shapes,2023-08-14 20:09:04+00:00,,0,5,triaged oncall: pt2 module: dynamic shapes,True
107166,jit compilation returns an int rather than a bool when using math.isnan() oncall: jit,2023-08-14 19:36:55+00:00,,1,2,oncall: jit,True
107155,DISABLED test_learnable_forward_per_channel_cpu (quantization.core.test_workflow_ops.TestFakeQuantizeOps) triaged module: macos skipped,2023-08-14 17:32:07+00:00,,1,2,triaged module: macos skipped,False
107143,[dynamo] calling __torch_function__ with dynamically created subclass of torch.Tensor fails compilation triaged module: __torch_function__ module: dynamo,2023-08-14 14:16:25+00:00,,0,1,triaged module: __torch_function__ module: dynamo,True
107133,torch.inverse throws error when DP but not in DDP or single GPU needs reproduction triaged module: data parallel module: linear algebra module: edge cases,2023-08-14 12:14:26+00:00,,0,2,needs reproduction triaged module: data parallel module: linear algebra module: edge cases,True
107130,[docs] Document dtype conversions dtype.to_complex() dtype.to_real() module: docs triaged,2023-08-14 11:03:26+00:00,,0,1,module: docs triaged,False
107125,combining `vmap` with NN containing `MaxPool2d' leads to discrepancies in output triaged module: functorch,2023-08-14 09:18:31+00:00,,0,3,triaged module: functorch,True
107115,H100 works differently than rtx4090 on same model needs reproduction triaged module: numerical-reproducibility,2023-08-14 03:54:30+00:00,,0,4,needs reproduction triaged module: numerical-reproducibility,True
107114,DISABLED test_make_fx_symbolic_exhaustive_special_bessel_y1_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: fx module: unknown,2023-08-14 03:39:36+00:00,,0,15,triaged module: flaky-tests skipped module: fx module: unknown,False
107112,from_blob python api triaged enhancement module: python frontend,2023-08-14 03:21:14+00:00,,0,2,triaged enhancement module: python frontend,True
107102,Error when using sparse_coo tensor with optimizer module: sparse triaged,2023-08-13 18:00:25+00:00,,0,0,module: sparse triaged,True
107099,memoryview support for `torch._C.import_ir_module_from_buffer` oncall: jit,2023-08-13 14:00:02+00:00,,0,0,oncall: jit,False
107087,RuntimeError with operations on torch.float8_e5m2 and torch.float_e4m3fn data types triaged module: float8,2023-08-12 16:21:00+00:00,,1,6,triaged module: float8,True
107081,[FSDP] summon_full_params won't change parameters triaged module: fsdp,2023-08-12 08:02:07+00:00,,0,5,triaged module: fsdp,True
107076,[Dynamo] Unable to Trace AdamW Optimizer when there is LR Scheduler high priority module: optimizer triaged oncall: pt2 module: dynamic shapes,2023-08-12 02:59:47+00:00,,1,4,high priority module: optimizer triaged oncall: pt2 module: dynamic shapes,True
107055,"Build a check we can defer to runtime, potentially add to the graph triaged oncall: pt2 module: dynamo",2023-08-11 19:27:33+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
107054,"Extend dict  and by extension __dict__ modeling in dynamo to support `setdefault`, `get` triaged oncall: pt2 module: dynamo",2023-08-11 19:23:43+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
107053,Dynamo x FSDP - Issue Tracking Master Task triaged oncall: pt2 module: dynamo,2023-08-11 19:21:39+00:00,,1,0,triaged oncall: pt2 module: dynamo,True
107041,"I want to calculate the matrix multiplication of two Boolean matrices, but torch.mm will report an error. Is there any more efficient alternative? triaged module: boolean tensor",2023-08-11 16:34:13+00:00,,0,2,triaged module: boolean tensor,True
107040,Dynamo not handling a NamedTuple  triage review module: performance oncall: pt2 module: dynamo mlperf,2023-08-11 16:05:03+00:00,,1,2,triage review module: performance oncall: pt2 module: dynamo mlperf,False
107026,a bug about tensor stride triaged module: viewing and reshaping,2023-08-11 09:51:25+00:00,,0,0,triaged module: viewing and reshaping,True
107023,"[feature request] [onnx] Support QuantLinear/DequantLinear float16 inputs (opset19 and maybe ""backport""-support them for opset17) module: onnx triaged enhancement",2023-08-11 09:34:02+00:00,,0,5,module: onnx triaged enhancement,False
107021,torchrun： RendezvousConnectionError when use C10d on multi nodes oncall: distributed oncall: r2p,2023-08-11 09:25:17+00:00,,0,1,oncall: distributed oncall: r2p,False
107016,cov onnx error module: onnx triaged,2023-08-11 06:25:40+00:00,,0,2,module: onnx triaged,True
107011,max_pool3d_with_indices_backward_cuda does not have a deterministic implementation triaged module: determinism module: pooling,2023-08-11 03:55:44+00:00,,0,0,triaged module: determinism module: pooling,True
107006,Apply fusion more aggressively in NAdam and Adagrad compilation good first issue triaged oncall: pt2 module: inductor,2023-08-11 02:15:43+00:00,,0,7,good first issue triaged oncall: pt2 module: inductor,True
107005,Dynamic shapes support for inductor foreach codegen good first issue triaged oncall: pt2 module: dynamic shapes module: inductor,2023-08-11 02:05:48+00:00,,0,6,good first issue triaged oncall: pt2 module: dynamic shapes module: inductor,True
107002,"RuntimeError: 0 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/alias_analysis.cpp"":615, please report a bug to PyTorch. We don't have an op for aten::full but it isn't a special case.  Argument types: int[], bool, NoneType, NoneType, Device, bool,  module: onnx triaged",2023-08-11 01:20:55+00:00,,0,0,module: onnx triaged,True
106991,Add cutlass as an alternative backend of PT2 Inductor triaged oncall: pt2 module: inductor,2023-08-10 21:59:49+00:00,,1,6,triaged oncall: pt2 module: inductor,False
106989,`ray` multiprocessing interference by torch import module: multiprocessing triaged,2023-08-10 20:41:13+00:00,,0,2,module: multiprocessing triaged,True
106973,Facing error while using onnx from scatterelements module: onnx triaged,2023-08-10 15:31:38+00:00,,0,0,module: onnx triaged,True
106972,RuntimeError: _Map_base::at when exporting squeeze module: onnx module: autograd triaged,2023-08-10 15:13:58+00:00,,0,3,module: onnx module: autograd triaged,True
106971,Found two conflicting CUDA installs module: build triaged,2023-08-10 15:06:08+00:00,,0,8,module: build triaged,True
106967,ONNX Model Producing Different Results Compared to Original PyTorch and JIT Traced Model module: onnx triaged,2023-08-10 13:59:53+00:00,,0,2,module: onnx triaged,True
106959,"`tensor.repeat` quirks: has no `torch.` variant, no `out=` variant, no inplace variant | `torch.tile` also does not have `out=` variant and uses `dims=` instead of `dim=` triaged module: viewing and reshaping module: python frontend",2023-08-10 08:19:05+00:00,,0,6,triaged module: viewing and reshaping module: python frontend,True
106956,Readily available python wheels for windows ARM module: binaries module: windows feature triaged,2023-08-10 07:16:05+00:00,,0,2,module: binaries module: windows feature triaged,True
106951,stride of gradient is not same as the corresponding tensor module: autograd module: optimizer triaged actionable,2023-08-10 05:56:14+00:00,,1,2,module: autograd module: optimizer triaged actionable,True
106942,[Minor Bug] Should consume_prefix_in_state_dict_if_present change ordering of keys? module: docs module: nn triaged actionable,2023-08-10 03:45:44+00:00,,0,1,module: docs module: nn triaged actionable,True
106939,Cannot export MiVOLO model into `onnx` format using `torch.onnx.export` module: onnx triaged,2023-08-10 03:00:04+00:00,,0,0,module: onnx triaged,False
106931,Other overloads of `_foreach_clamp` module: optimizer triaged enhancement actionable module: mta,2023-08-10 00:55:01+00:00,,0,2,module: optimizer triaged enhancement actionable module: mta,True
106894,[autograd.Function] freevar lifting is too aggressive? triaged oncall: pt2 module: dynamo,2023-08-09 19:08:58+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
106893,[autograd.Function] torch.compile w/ once_differentiable leads to opaque graph break high priority triaged oncall: pt2 module: dynamo,2023-08-09 19:03:50+00:00,,1,9,high priority triaged oncall: pt2 module: dynamo,True
106885,Dynamo graph break when using pyton module `heapq` (manipulates with `list`s) low priority triaged module: dynamo,2023-08-09 18:18:11+00:00,,0,2,low priority triaged module: dynamo,True
106877,[ONNX] Float8 support module: onnx low priority triaged,2023-08-09 17:38:49+00:00,,1,3,module: onnx low priority triaged,False
106873,[Dynamo] Integration exporter's diagnostic system into ONNXRuntime backend module: onnx triaged,2023-08-09 16:52:56+00:00,,0,0,module: onnx triaged,False
106872,[Dynamo] revise ONNXRuntime backend's use of CapabilityBasedPartitioner module: onnx triaged,2023-08-09 16:49:22+00:00,,1,0,module: onnx triaged,False
106871,[Dynam] a graph pass in Dynamo-ONNXRuntime backend needs revision module: onnx triaged,2023-08-09 16:45:49+00:00,,1,0,module: onnx triaged,False
106869,[Dyanmo] Pre-allocate flag should be a ONNXRuntime inference session level attribute module: onnx triaged,2023-08-09 16:38:48+00:00,,0,0,module: onnx triaged,True
106868,[Dynamo] ONNXRuntime backend (DORT) requires some guards to re-partition extracted by Dynamo module: onnx triaged,2023-08-09 16:34:46+00:00,,1,0,module: onnx triaged,False
106867,[Dynamo] ONNXRuntime Backend Shold Allow External Allocator module: onnx triaged,2023-08-09 16:17:39+00:00,,0,0,module: onnx triaged,False
106851,RPC all_gather doesn't work with dynamic world size (world_size=None) oncall: distributed module: rpc,2023-08-09 08:05:50+00:00,,0,0,oncall: distributed module: rpc,False
106846,untimeError: The following operation failed in the TorchScript interpreter. Traceback of TorchScript (most recent call last): RuntimeError: nvrtc: error: invalid value for --gpu-architecture (-arch) needs reproduction oncall: jit,2023-08-09 06:29:54+00:00,,0,0,needs reproduction oncall: jit,True
106845,`1/torch.inf` produce inconsistent results module: numerical-stability triaged module: complex module: type promotion module: edge cases,2023-08-09 05:40:03+00:00,,1,4,module: numerical-stability triaged module: complex module: type promotion module: edge cases,True
106828,Use expect tests for error inputs triaged better-engineering module: testing,2023-08-08 23:48:06+00:00,,0,1,triaged better-engineering module: testing,True
106815,Please verify 1.14.1 ONNX release candidate on TestPyPI module: onnx triaged module: infra,2023-08-08 20:28:03+00:00,,0,1,module: onnx triaged module: infra,False
106802,Optimizers should use learning rates passed as tensors directly module: optimizer triaged actionable module: dynamic shapes,2023-08-08 16:14:52+00:00,,1,9,module: optimizer triaged actionable module: dynamic shapes,True
106801,"Timer benchmark stores only one time value, and therefore has broken mean/median/etc metrics triaged module: benchmark",2023-08-08 16:03:51+00:00,,0,4,triaged module: benchmark,True
106784,"[ux] Suppot torch.tensor(set([1,2,3])) triaged needs research topic: new features module: python frontend",2023-08-08 14:18:41+00:00,,0,4,triaged needs research topic: new features module: python frontend,True
106780,inf and nan are mapped to quant_min in torch.fake_quantize_per_tensor_affine oncall: quantization triaged,2023-08-08 13:32:31+00:00,,1,1,oncall: quantization triaged,True
106770,[Inductor][cpu] torchbench model doctr_det_predictor perf regression triaged oncall: pt2,2023-08-08 09:03:18+00:00,,1,1,triaged oncall: pt2,False
106764,[Feature request] Add new API Tensor.device_as  triage review oncall: jit feature module: python frontend,2023-08-08 07:43:37+00:00,,0,0,triage review oncall: jit feature module: python frontend,True
106748,[FX][ONNX][exporter] Failed to export traced fx graph to onnx model module: onnx triaged oncall: export,2023-08-08 02:34:24+00:00,,0,6,module: onnx triaged oncall: export,False
106732,Hugging Face safetensor does not work with FakeTensorMode triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher,2023-08-07 22:46:02+00:00,,0,6,triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher,False
106718,Add AMD image to the .devcontainer spec triaged better-engineering,2023-08-07 18:50:13+00:00,,0,0,triaged better-engineering,True
106717,Provide .devcontainer PyTorch - MPS environment triaged better-engineering,2023-08-07 18:48:05+00:00,,0,0,triaged better-engineering,True
106713,Dev Container Support for PyTorch triaged better-engineering,2023-08-07 17:51:14+00:00,,1,0,triaged better-engineering,True
106711,'CUDA out of memory' when using a GPU services for reinforcement learning in Torch rpc tutorial oncall: distributed module: rpc,2023-08-07 16:01:14+00:00,,0,4,oncall: distributed module: rpc,True
106704,Dataloader extremely slow on in-memory datasets module: dataloader triaged module: data,2023-08-07 13:46:00+00:00,,0,1,module: dataloader triaged module: data,True
106700,C++ API `torch::nn::MultiheadAttention` Crashes by division by zero module: crash module: cpp triaged oncall: transformer/mha,2023-08-07 11:36:32+00:00,,0,1,module: crash module: cpp triaged oncall: transformer/mha,True
106699,torch.jit.script: scripting doesn't work with wraps oncall: jit,2023-08-07 11:23:50+00:00,,0,0,oncall: jit,True
106692,torch.polygamma inconsistent with scipy.special.polygamma for n >= 1 triaged module: numpy module: special,2023-08-07 05:36:43+00:00,,0,0,triaged module: numpy module: special,True
106690,DDP grads not synced when static_graph=True and module output is a dict subclass? oncall: distributed module: pytree,2023-08-07 04:34:47+00:00,,0,6,oncall: distributed module: pytree,True
106667,[docs] Idea collection of examples of custom ops / inline torch extensions module: docs triaged module: custom-operators,2023-08-05 17:13:57+00:00,,0,6,module: docs triaged module: custom-operators,False
106665,Inconsistency between CPU and GPU for `Linear()` layer with input size 0 module: nn triaged actionable module: edge cases,2023-08-05 12:53:17+00:00,,0,5,module: nn triaged actionable module: edge cases,True
106664,[docs] URL and link format proposal to make function page URLs more concise module: docs triaged enhancement,2023-08-05 11:42:07+00:00,,0,1,module: docs triaged enhancement,False
106662,Will torch.sparse.mm support multiplying two boolean matrices? module: sparse triaged module: boolean tensor,2023-08-05 09:52:28+00:00,,0,0,module: sparse triaged module: boolean tensor,True
106660,Question about garbage collection without GPU sync  triaged module: CUDACachingAllocator,2023-08-05 06:28:26+00:00,,0,1,triaged module: CUDACachingAllocator,True
106649,Dynamo graph break on triplet_margin_with_distance_loss triaged module: dynamo,2023-08-04 23:39:35+00:00,,0,1,triaged module: dynamo,False
106637,Using retain_graph in backward() with FSDP oncall: distributed triaged module: fsdp,2023-08-04 21:44:48+00:00,,1,0,oncall: distributed triaged module: fsdp,True
106634,Confusing error message for DataLoader with num_workers=0 and non-zero timeout module: dataloader triaged,2023-08-04 21:04:36+00:00,,0,0,module: dataloader triaged,True
106633,Refcount problem for torch.distributed.Store objects defined in Python oncall: distributed module: c10d,2023-08-04 20:58:25+00:00,,0,5,oncall: distributed module: c10d,True
106630,no_grad() changes output of TransformerDecoder module during evaluation  oncall: transformer/mha,2023-08-04 18:47:08+00:00,,0,2,oncall: transformer/mha,False
106624,"[feature request] [ux] Frontend methods for fused  elementwise affine transform: mul+add+dtype convert + support  `integer_tensor.mul_(float_constant)` and `float_tensor.mul(some_constant, out = integer_tensor)` maybe via new args `rounding_mode=...` and `dtype=...` + maybe support OpenCV-style saturated dtype conversions (e.g. `clamp_` before conversion) triaged module: type promotion module: python frontend",2023-08-04 17:37:47+00:00,,0,12,triaged module: type promotion module: python frontend,True
106623,Meta implementations of FFT operators often have incorrect strides triaged module: fft oncall: pt2,2023-08-04 17:25:52+00:00,,0,1,triaged module: fft oncall: pt2,True
106622,FFT Samples Inputs with More than Three Dimensions module: tests triaged module: fft,2023-08-04 17:22:25+00:00,,0,2,module: tests triaged module: fft,True
106614,Case study of torch.compile / cpp inductor on CPU: min_sum / mul_sum with 1d / matmul-like with static / dynamic shapes triaged module: custom-operators oncall: pt2 module: dynamic shapes module: pt2-dispatcher,2023-08-04 14:19:47+00:00,,0,17,triaged module: custom-operators oncall: pt2 module: dynamic shapes module: pt2-dispatcher,True
106608,ROCm & Windows Support module: rocm triaged,2023-08-04 09:08:01+00:00,,0,9,module: rocm triaged,False
106606,More Performant CachingHostAllocator for Pinned Memory Allocation module: cuda triaged,2023-08-04 07:57:44+00:00,,0,0,module: cuda triaged,True
106604,Relu6 not able to process nan values triaged module: NaNs and Infs,2023-08-04 07:01:39+00:00,,0,3,triaged module: NaNs and Infs,True
106602, onednn ops supported in pytorch triaged module: intel docathon-h2-2023,2023-08-04 06:22:53+00:00,,1,4,triaged module: intel docathon-h2-2023,False
106601,[ONNX] Keep functional ops as functions in dynamo exported onnx module: onnx triaged,2023-08-04 06:12:15+00:00,,0,1,module: onnx triaged,False
106596,[discussion] move-semantics for tensors triaged module: python frontend,2023-08-04 02:52:42+00:00,,0,10,triaged module: python frontend,True
106584,Lacking commutativity of `tensor.expand` and `tensor.flatten` triaged module: viewing and reshaping,2023-08-03 23:23:15+00:00,,0,5,triaged module: viewing and reshaping,True
106580,[dynamo] Unsupported to trace through Boolean Tensor indexing triaged oncall: pt2 module: dynamic shapes,2023-08-03 22:23:20+00:00,,1,5,triaged oncall: pt2 module: dynamic shapes,False
106579,"Boolean valued images loaded from disk, when converted to torch int/float tensor, the True valued pixels gets converted to 255 instead of 1 triaged module: boolean tensor",2023-08-03 21:47:27+00:00,,0,2,triaged module: boolean tensor,True
106566,DTensor Sharding prop cache stats oncall: distributed triaged module: dtensor,2023-08-03 19:06:58+00:00,,0,0,oncall: distributed triaged module: dtensor,True
106565,install cuda version always get cpuonly oncall: releng triaged,2023-08-03 18:43:55+00:00,,0,1,oncall: releng triaged,True
106563,NotImplementedError: Could not run 'aten::multinomial' with arguments from the 'Meta' backend. triaged module: random,2023-08-03 18:19:04+00:00,,0,1,triaged module: random,True
106557,DISABLED test_cpp_wrapper_cpu (__main__.FreezingCpuTests) triaged skipped oncall: pt2,2023-08-03 17:06:02+00:00,,1,1,triaged skipped oncall: pt2,False
106556,Pytorch: torch.autograd.grad returns NoneType module: autograd triaged,2023-08-03 16:41:36+00:00,,0,3,module: autograd triaged,True
106549,Can't build PyTorch 1.13.1 with Vulkan support triaged module: vulkan ciflow/periodic,2023-08-03 15:25:32+00:00,,0,2,triaged module: vulkan ciflow/periodic,True
106546,Potential Issue with Pandas Dataframe needs reproduction triaged oncall: pt2,2023-08-03 12:44:33+00:00,,0,1,needs reproduction triaged oncall: pt2,True
106545,`softmax` to handle dimensions comprised of `-inf` module: nn triaged,2023-08-03 10:59:35+00:00,,0,1,module: nn triaged,True
106544,"Branch name in double quotes """" module: ci triaged",2023-08-03 09:48:51+00:00,,1,5,module: ci triaged,True
106540,Dataset  with Queue issue module: dataloader triaged module: data,2023-08-03 08:50:38+00:00,,0,1,module: dataloader triaged module: data,True
106533,CUDA device support does not register allocator to c10::GetAllocator(...) module: cpp module: cuda triaged,2023-08-03 07:41:47+00:00,,0,0,module: cpp module: cuda triaged,True
106529,Pytorch + ROCm+ Windows module: rocm triaged,2023-08-03 06:13:09+00:00,,0,2,module: rocm triaged,False
106520,Distributed torch.linalg.eigh (and other functions) on cuda using cuSOLVERMG module: cuda triaged module: linear algebra,2023-08-03 03:12:55+00:00,,0,0,module: cuda triaged module: linear algebra,True
106485,Increasing batch size makes network forward 1000 times slower module: cudnn module: nn module: cuda triaged,2023-08-02 21:48:59+00:00,,0,6,module: cudnn module: nn module: cuda triaged,True
106469,Extreme slowdown of torch.mm for certain sizes and strides with bfloat16 module: cuda triaged module: bfloat16,2023-08-02 17:48:04+00:00,,0,1,module: cuda triaged module: bfloat16,True
106467,nn.CrossEntropyLoss with invalid target generates corrups memory eventualy leading to CUDA error: an illegal memory access module: nn module: loss triaged,2023-08-02 16:24:15+00:00,,0,7,module: nn module: loss triaged,True
106457,AOTAutograd should detect false aliasing. triaged module: viewing and reshaping oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-08-02 13:54:19+00:00,,0,1,triaged module: viewing and reshaping oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
106455,"vmap, jacrev, jacfwd, hessian, etc., in libTorch module: cpp triaged module: vmap module: functorch",2023-08-02 12:54:42+00:00,,0,2,module: cpp triaged module: vmap module: functorch,True
106450,Check for output_padding <= stride/dilation in ConvTranspose1d module: convolution triaged module: padding,2023-08-02 11:56:29+00:00,,0,0,module: convolution triaged module: padding,True
106439,DISABLED test_aot_sequence_nr_dynamic_shapes (dynamo.test_aot_autograd.DynamicShapesAotAutogradFallbackTests) triaged module: flaky-tests skipped module: dynamo,2023-08-02 03:40:53+00:00,,0,122,triaged module: flaky-tests skipped module: dynamo,False
106435,"[JIT] .item() dict keys cause `RuntimeError: Cannot create dict for key type 'Scalar', only int, float, complex, Tensor, device and string keys are supported` oncall: jit",2023-08-02 00:33:29+00:00,,0,1,oncall: jit,True
106432,inconsistent dtype of scale and zero_point in observers high priority oncall: quantization triaged,2023-08-01 23:38:28+00:00,,1,0,high priority oncall: quantization triaged,True
106427,`torch.nn.utils.clip_grad_norm_()` causes H2D sync with foreach ops. module: nn triaged module: mta,2023-08-01 23:13:25+00:00,,0,15,module: nn triaged module: mta,True
106408,PyTorchMPS not showing up in Instruments for `torch.mps.profiler` triaged module: mps,2023-08-01 21:15:05+00:00,,1,8,triaged module: mps,True
106388,Better export story for autograd.Function? triaged oncall: pt2,2023-08-01 15:59:38+00:00,,0,3,triaged oncall: pt2,False
106387,[torch.compile] autograd.Function where we assign a Tensor directly to ctx triaged oncall: pt2 module: dynamo,2023-08-01 15:56:17+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
106382,Installing torchvision for CPU leads to unwanted upgrade of torch + pip would not install nightly as considers that release is the latest (?) module: binaries triaged,2023-08-01 15:07:53+00:00,,0,4,module: binaries triaged,True
106378,[dynamo] can't compile if tensor subclass implements __torch_function__ using super() triaged oncall: pt2 module: dynamo,2023-08-01 12:13:47+00:00,,1,6,triaged oncall: pt2 module: dynamo,False
106377,Command to reproduce error is incorrect good first issue module: tests triaged module: infra module: testing,2023-08-01 11:54:23+00:00,,0,2,good first issue module: tests triaged module: infra module: testing,True
106375,nll_loss reference shouldn't be registered as a decomposition. triaged module: primTorch module: decompositions,2023-08-01 11:19:14+00:00,,0,0,triaged module: primTorch module: decompositions,True
106373,[ONNX] scatter_reduce does not support `include_self=False` module: onnx triaged,2023-08-01 09:02:58+00:00,,0,2,module: onnx triaged,False
106362,Calling ops.aten.embedding_bag() function got silent crash module: crash module: nn triaged,2023-08-01 04:00:04+00:00,,1,6,module: crash module: nn triaged,True
106361,DISABLED test_ddp_apply_optim_in_backward_ignored_params (__main__.TestDistBackendWithSpawn) oncall: distributed module: flaky-tests skipped,2023-08-01 03:40:05+00:00,,0,5,oncall: distributed module: flaky-tests skipped,False
106360,Improving save_on_cpu's performance by overlapping memory transfers with compute oncall: distributed module: autograd triaged enhancement,2023-08-01 03:38:31+00:00,,0,8,oncall: distributed module: autograd triaged enhancement,True
106359,backwards compatibility about class _LRScheduler triaged module: LrScheduler,2023-08-01 03:00:21+00:00,,0,0,triaged module: LrScheduler,True
106341,Transformer.generate_square_subsequent_mask has nan values on MPS device triaged module: NaNs and Infs module: mps,2023-07-31 22:21:52+00:00,,0,0,triaged module: NaNs and Infs module: mps,True
106339,"ReduceLROnPlateau increases learning rate exponentially, causing training to diverge triaged module: LrScheduler",2023-07-31 22:07:35+00:00,,0,0,triaged module: LrScheduler,True
106338,Inefficient code generated - does not use 256b registers triaged oncall: pt2 module: cpu inductor,2023-07-31 21:44:42+00:00,,1,3,triaged oncall: pt2 module: cpu inductor,False
106308,DISABLED test_cuda_assert_should_not_stop_common_distributed_test_suite_cuda (__main__.TestTestingCUDA) oncall: distributed module: ci triaged skipped,2023-07-31 14:18:47+00:00,,1,1,oncall: distributed module: ci triaged skipped,False
106302,`torch.nn.modules.MultiheadAttention` yields different graph under pre_dispatch tracing triaged module: __torch_function__ pre_dispatch tracing oncall: export,2023-07-31 14:09:01+00:00,,0,4,triaged module: __torch_function__ pre_dispatch tracing oncall: export,True
106298,Torch.onnx.export a fp16 model but get the output tensor fp32 module: onnx triaged,2023-07-31 09:19:44+00:00,,0,0,module: onnx triaged,True
106297,Can't build with non-static protobuf module: build triaged,2023-07-31 08:59:59+00:00,,0,2,module: build triaged,True
106287,Many tests in test/dynamo fail if run in the context of just 'pytest test/dynamo' module: ci triaged module: dynamo,2023-07-31 01:15:31+00:00,,3,1,module: ci triaged module: dynamo,False
106271,RuntimeError: GlooDeviceFactory::makeUVDevice(): interface or hostname can't be empty oncall: distributed,2023-07-30 11:03:11+00:00,,0,11,oncall: distributed,False
106269,Make our source attribution debug prints more useful for Compiler Explorer triaged oncall: pt2,2023-07-30 03:27:21+00:00,,0,2,triaged oncall: pt2,False
106265,RuntimeError: Expected a proper Tensor but got None (or an undefined Tensor in C++) for argument #0 'grad_y' module: autograd triaged actionable module: mps,2023-07-30 01:20:59+00:00,,0,5,module: autograd triaged actionable module: mps,True
106256,Runtime Error: Empty tensor needs reproduction triaged module: mps,2023-07-29 09:16:35+00:00,,0,2,needs reproduction triaged module: mps,True
106251, Improve Error Message in MultiMarginLoss for Inconsistent Target Size module: nn good first issue module: error checking triaged actionable,2023-07-29 06:31:31+00:00,,1,4,module: nn good first issue module: error checking triaged actionable,True
106243,OneCycleLR's state_dict includes a full reference to the optimizer module: optimizer triaged needs design module: LrScheduler,2023-07-29 01:03:48+00:00,,0,1,module: optimizer triaged needs design module: LrScheduler,True
106237,ghstack + mergebot race condition module: ci triaged,2023-07-28 23:16:01+00:00,,1,0,module: ci triaged,True
106221,RProp improvement tracker module: optimizer triaged actionable,2023-07-28 20:24:53+00:00,,0,5,module: optimizer triaged actionable,True
106220,torch compile does not work with torch.nn.functional.softmax ? triaged oncall: pt2,2023-07-28 20:21:10+00:00,,0,5,triaged oncall: pt2,False
106217,[dynamo.export] Assertion Error: Mutating module attribute during export. triaged oncall: pt2 oncall: export,2023-07-28 18:45:52+00:00,,0,3,triaged oncall: pt2 oncall: export,False
106206,Build failure due to C++ version mismatch module: build triaged,2023-07-28 14:52:48+00:00,,0,0,module: build triaged,True
106197,Scalar Tensor lowering to Fake Tensor inside Inductor triaged bug module: inductor module: dynamo,2023-07-28 09:07:18+00:00,,1,4,triaged bug module: inductor module: dynamo,True
106183,[dynamo.export] symbolic_shapes.GuardOnDataDependentSymNode triaged oncall: pt2 module: dynamic shapes,2023-07-28 00:32:49+00:00,,0,4,triaged oncall: pt2 module: dynamic shapes,False
106173,`torch.ops.aten.split.Tensor._schema` return alias annotations are wrong triaged module: viewing and reshaping,2023-07-27 22:39:10+00:00,,0,0,triaged module: viewing and reshaping,True
106171,torch compile changes model output triaged oncall: pt2,2023-07-27 22:29:20+00:00,,0,3,triaged oncall: pt2,False
106164,distributed.batch_isend_irecv() crash when send/recv refers to itself oncall: distributed module: crash module: c10d,2023-07-27 21:00:27+00:00,,0,0,oncall: distributed module: crash module: c10d,True
106144,Pytorch nighlty and openAI/triton cuda needs reproduction triaged oncall: pt2,2023-07-27 17:16:16+00:00,,0,12,needs reproduction triaged oncall: pt2,False
106141,Sparse COO indices are torch.Int64 -- is this necessary? module: sparse triaged,2023-07-27 16:00:42+00:00,,0,1,module: sparse triaged,True
106137,"`export(..., pre_dispatch=True)` for model in eval mode still inserts autograd ops triaged pre_dispatch tracing oncall: export",2023-07-27 15:37:34+00:00,,0,1,triaged pre_dispatch tracing oncall: export,True
106136,bc-linter false positive with TypeAliases triaged module: devx,2023-07-27 15:34:59+00:00,,1,5,triaged module: devx,True
106135,Registering function that takes const std::vector<c10::SymInt>& to SymInt[] schema gives confusing error message triaged module: dynamic shapes,2023-07-27 15:13:34+00:00,,0,0,triaged module: dynamic shapes,True
106130,torch._subclasses.fake_tensor.DynamicOutputShapeException: aten.nonzero.default triaged module: fakeTensor,2023-07-27 12:50:36+00:00,,0,0,triaged module: fakeTensor,True
106129,Allow settingsetGraphExecutorOptimize default for all threads oncall: jit,2023-07-27 12:23:11+00:00,,0,0,oncall: jit,False
106128,Torchscript optimizer incorrectly applies constant propagation to convert prim::ListConstruct() into prim::Constant oncall: jit,2023-07-27 12:15:41+00:00,,0,0,oncall: jit,False
106126,Libtorch report C10 error when compiling on my own project needs reproduction module: binaries module: cpp triaged,2023-07-27 09:47:22+00:00,,0,2,needs reproduction module: binaries module: cpp triaged,True
106124,[feature request] Better argument checks and error messaging for `tensor.repeat` module: error checking triaged actionable release notes: python_frontend,2023-07-27 08:43:39+00:00,,0,3,module: error checking triaged actionable release notes: python_frontend,True
106121,Got error when train models with more than one param_group in torch2.0 module: optimizer triaged has workaround,2023-07-27 08:09:57+00:00,,0,5,module: optimizer triaged has workaround,True
106112,MPS cumprod gradient is broken even when using cpu fallback on macos 13.2.1 triaged module: mps,2023-07-27 04:55:31+00:00,,0,0,triaged module: mps,True
106110,llama model failed for dynamic shape path needs reproduction triaged oncall: pt2 module: dynamic shapes module: dynamo,2023-07-27 04:32:29+00:00,,2,10,needs reproduction triaged oncall: pt2 module: dynamic shapes module: dynamo,True
106101,EMFORMER_RNNT not compilable triaged oncall: pt2 module: dynamic shapes module: dynamo,2023-07-27 02:00:59+00:00,,1,7,triaged oncall: pt2 module: dynamic shapes module: dynamo,False
106073,Potential lack of CI testing on older NVIDIA GPU module: ci triaged,2023-07-26 20:58:08+00:00,,0,1,module: ci triaged,True
106067,"Tensors always get 0/1 specialization guards, even if they're not used triaged oncall: pt2 module: dynamic shapes module: guards",2023-07-26 19:35:42+00:00,,0,0,triaged oncall: pt2 module: dynamic shapes module: guards,True
106050,"""Graph break: inline in skipfiles:"" is a bad message triaged oncall: pt2 module: dynamo",2023-07-26 17:17:23+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
106046,Avoid incrementing refcount of `grad_fn` in `unpack_list` module: autograd triaged module: mta,2023-07-26 16:21:33+00:00,,0,0,module: autograd triaged module: mta,True
106031,torch._dynamo.export does not support symbolic int inputs triaged oncall: pt2 module: dynamic shapes oncall: export,2023-07-26 14:45:58+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes oncall: export,False
106029,[FSDP] Investigate sharded GPU gradient lifetime when CPU offloading triaged module: fsdp,2023-07-26 13:02:45+00:00,,1,0,triaged module: fsdp,True
106027,DISABLED test_profiler_cuda_sync_events (__main__.TestProfiler) module: rocm triaged skipped,2023-07-26 12:40:59+00:00,,0,1,module: rocm triaged skipped,False
106011,Misleading error message in multilabel_margin_loss when passing incompatible tensor dimensions module: nn triaged actionable,2023-07-26 05:38:37+00:00,,0,2,module: nn triaged actionable,True
106006,[torch.compile] assertion sometimes ignored with inductor backend triaged oncall: pt2,2023-07-26 03:43:07+00:00,,0,1,triaged oncall: pt2,False
105997,Flatbuffer torchscipt files don't load in PyTorch Android Lite 1.13.1 oncall: jit,2023-07-26 02:35:08+00:00,,0,0,oncall: jit,False
105982,"vmap and rnn/lstm ""accessing '.data' under vmap transform is not allowed"" triaged module: functorch",2023-07-25 23:03:30+00:00,,0,1,triaged module: functorch,True
105961,"Error in Profiler : RuntimeError: Expected !config.profile_memory to be true, but got false oncall: profiler",2023-07-25 20:11:05+00:00,,0,1,oncall: profiler,True
105954,Ensure PRs are rebased on top of a recent commit (CI check) triaged module: devx,2023-07-25 18:51:37+00:00,,1,2,triaged module: devx,True
105944,ReplayRecordTests.test_fn_call_args and others fail on my local devserver triaged oncall: pt2 module: dynamo,2023-07-25 17:22:24+00:00,,1,0,triaged oncall: pt2 module: dynamo,False
105943,PT2 is not thread safe triaged oncall: pt2,2023-07-25 17:07:49+00:00,,0,1,triaged oncall: pt2,False
105941,Differences in the results of conv2d calculations in PyTorch 1.8 needs reproduction module: cudnn triaged,2023-07-25 16:43:30+00:00,,0,4,needs reproduction module: cudnn triaged,True
105934,Flip default on `add_zero_attn` in `torch.nn.MultiheadAttention` to `True` module: nn triaged oncall: transformer/mha,2023-07-25 15:56:49+00:00,,0,4,module: nn triaged oncall: transformer/mha,True
105933,DISABLED test_cross_entropy_large_tensor_reduction_sum_cuda (__main__.TestNNDeviceTypeCUDA) module: nn module: rocm triaged module: flaky-tests skipped,2023-07-25 15:39:35+00:00,,0,1,module: nn module: rocm triaged module: flaky-tests skipped,False
105929,Dynamo silently ignores TorchDispatchMode triaged module: __torch_dispatch__ oncall: pt2 module: dynamo module: pt2-dispatcher,2023-07-25 14:48:28+00:00,,0,3,triaged module: __torch_dispatch__ oncall: pt2 module: dynamo module: pt2-dispatcher,False
105925,Possible speed up of nn.MultiheadAttention oncall: transformer/mha,2023-07-25 14:19:56+00:00,,0,1,oncall: transformer/mha,False
105918,Torch.jit : RuntimeError:  Unable to extract string literal index for ModuleDict oncall: jit,2023-07-25 13:01:29+00:00,,0,0,oncall: jit,True
105917,Torch.jit.frontend.NotSupportedError: not supporting functions with variable number of arguments. oncall: jit,2023-07-25 12:49:22+00:00,,0,0,oncall: jit,False
105916,Missing coalesced flag from `torch.autograd.Function.backward` module: sparse triaged,2023-07-25 12:41:36+00:00,,0,0,module: sparse triaged,True
105914,torch.compile(cpu) does not handle float16 properly triaged oncall: pt2 module: cpu inductor,2023-07-25 12:08:42+00:00,,0,4,triaged oncall: pt2 module: cpu inductor,False
105912,Batching rule for aten::bincount. triaged module: functorch,2023-07-25 09:54:04+00:00,,0,0,triaged module: functorch,True
105904,Libtorch linking Error:undefined reference module: build triaged,2023-07-25 08:07:17+00:00,,0,1,module: build triaged,True
105901,Default parameters missing of maxpool2d node generated by dynamo export triaged oncall: pt2 oncall: export,2023-07-25 06:57:17+00:00,,0,3,triaged oncall: pt2 oncall: export,False
105900,torch.jit.frontend.NotSupported when compiling stable-diffusion oncall: jit,2023-07-25 06:52:42+00:00,,0,0,oncall: jit,False
105878,FakeTensor detach() gives meta tensor other than FakeTensor under `torch._C._DisableTorchDispatch()` triaged tensor subclass module: fakeTensor,2023-07-24 22:55:32+00:00,,0,3,triaged tensor subclass module: fakeTensor,True
105872,PyTorch 2.0.x `CUDA error: operation not supported` when `Tensor.to` a different device needs reproduction module: cuda triaged module: regression,2023-07-24 21:16:19+00:00,,0,6,needs reproduction module: cuda triaged module: regression,True
105863,torch.compile does not respect branching in forward() triaged oncall: pt2 module: guards,2023-07-24 19:03:33+00:00,,1,1,triaged oncall: pt2 module: guards,False
105860,Programmation error enabling unlegal memory access on gpu module: error checking triaged module: mps,2023-07-24 18:21:24+00:00,,0,0,module: error checking triaged module: mps,True
105859,Report model flop utilization (mfu) in benchmark feature triaged oncall: pt2,2023-07-24 18:19:46+00:00,,0,1,feature triaged oncall: pt2,False
105853,Bug when dealing with fallbacks on CPU triaged module: complex module: dynamic shapes module: inductor module: cpu inductor,2023-07-24 17:10:14+00:00,,0,3,triaged module: complex module: dynamic shapes module: inductor module: cpu inductor,True
105846,Strange backward behavior with sparse tensors module: sparse module: autograd triaged,2023-07-24 15:49:50+00:00,,0,7,module: sparse module: autograd triaged,True
105840,[FSDP] FSDP doesn't work (random accuracy performance) when using `param_init_fn` and `sync_module_states=True` feature triaged module: fsdp,2023-07-24 15:00:59+00:00,,1,19,feature triaged module: fsdp,True
105839,"MPS memory issue,  MPS backend out of memory, but works if I empty the MPS cache module: memory usage triaged module: mps",2023-07-24 13:15:54+00:00,,0,4,module: memory usage triaged module: mps,True
105838,Exporting the operator 'aten::grad' to ONNX opset version 18 is not supported. module: onnx module: autograd triaged,2023-07-24 12:45:30+00:00,,0,0,module: onnx module: autograd triaged,True
105821,JIT input aliasing does not support aten::fill_ oncall: jit,2023-07-24 03:51:54+00:00,,0,6,oncall: jit,False
105804,Conversion Error to ComplexDouble on MPS triaged module: complex module: mps,2023-07-23 17:29:47+00:00,,0,2,triaged module: complex module: mps,True
105802,Errors while trying to finetune compiled transformers model needs reproduction triaged oncall: pt2 module: dynamo,2023-07-23 15:31:23+00:00,,1,11,needs reproduction triaged oncall: pt2 module: dynamo,False
105799,inconsistent signature for dataloader in docs/source/data.rst module: dataloader triaged,2023-07-23 10:54:35+00:00,,0,2,module: dataloader triaged,True
105790,torch.sparse.mm() with reduce operator for GPU support and COO matrices module: sparse feature triaged module: reductions,2023-07-22 11:01:24+00:00,,0,0,module: sparse feature triaged module: reductions,True
105782,"DDP , error . [c10d] The client socket has timed out after 900s while trying to connect to (XX.XX.XX.XX, 8514). oncall: distributed",2023-07-22 05:22:37+00:00,,0,0,oncall: distributed,True
105779,Mode to warm up PT2 with a regular eager mode execution triaged oncall: pt2,2023-07-22 04:07:21+00:00,,0,3,triaged oncall: pt2,False
105778,https://pytorch.org/docs/stable/backends.html does not describe torch.backends.cpu module: docs module: cpu triaged actionable,2023-07-22 02:55:01+00:00,,0,2,module: docs module: cpu triaged actionable,True
105768,torch.compile uses more memory when using less parameters module: memory usage module: convolution triaged oncall: pt2,2023-07-21 22:03:11+00:00,,0,0,module: memory usage module: convolution triaged oncall: pt2,True
105751,Revisit checkpoint naming mismatch with torch name (and ONNX initializer name as a consequence) module: onnx triaged,2023-07-21 16:58:05+00:00,,1,1,module: onnx triaged,False
105742,`torch.unique()` messes around with order even if `sorted=False` module: docs triaged actionable module: sorting and selection docathon-h2-2023,2023-07-21 14:35:11+00:00,,1,10,module: docs triaged actionable module: sorting and selection docathon-h2-2023,False
105731,Pypi is missing dependencies module: binaries triaged,2023-07-21 13:38:50+00:00,,0,3,module: binaries triaged,True
105729,[FSDP] using CPUOffload cannot make the code runing stop triaged module: fsdp,2023-07-21 08:50:36+00:00,,0,1,triaged module: fsdp,True
105728,Compile error PyTorch 2.0.1 / GCC 13.1.0 module: build triaged,2023-07-21 08:06:46+00:00,,1,6,module: build triaged,True
105726,There is a big precision error between A100 and 3090 when using torch.matmul with fp16 precision module: cuda triaged module: half,2023-07-21 06:56:04+00:00,,0,8,module: cuda triaged module: half,True
105723,Parameter ... has been marked as ready twice oncall: distributed,2023-07-21 05:23:06+00:00,,0,2,oncall: distributed,False
105716,Dynamo test pipeline failed on MaxPool2d test when changed to use f-string module: ci module: tests triaged module: dynamo,2023-07-21 03:24:29+00:00,,1,1,module: ci module: tests triaged module: dynamo,True
105706,Unable to build documents module: onnx triaged module: doc infra,2023-07-21 01:15:42+00:00,,0,0,module: onnx triaged module: doc infra,True
105700,Don't use weak ref finalization for freeing resources when code objects die triaged oncall: pt2 module: dynamo,2023-07-21 00:22:33+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
105697,Support symmetry in einsum triaged module: linear algebra,2023-07-20 23:34:54+00:00,,0,1,triaged module: linear algebra,True
105686,Error using torch.compile with HF transformers and model `mosaicml/mpt-7b` triaged oncall: pt2 module: dynamic shapes,2023-07-20 20:09:56+00:00,,0,4,triaged oncall: pt2 module: dynamic shapes,True
105682,`torch.autocast(bfloat16)` runs bwd matmuls in fp16 triaged module: amp (automated mixed precision),2023-07-20 19:29:30+00:00,,0,0,triaged module: amp (automated mixed precision),True
105665,Running Llama 2 on Apple Silicon GPUs - missing MPS types and operators triaged module: mps,2023-07-20 15:31:09+00:00,,0,9,triaged module: mps,True
105655,Pytorch -  cpu only & caffe2 build failing module: build caffe2 triaged,2023-07-20 11:52:16+00:00,,0,1,module: build caffe2 triaged,True
105644,Tensor subclass is not preserved during backward with gradient checkpointing module: checkpoint triaged module: __torch_function__ tensor subclass,2023-07-20 04:57:33+00:00,,0,3,module: checkpoint triaged module: __torch_function__ tensor subclass,True
105641,Turn indexing with a scalar tensor into an copy into a view and avoid a D2H synchronization. module: bc-breaking triaged module: numpy module: advanced indexing topic: bc breaking,2023-07-20 04:37:14+00:00,,0,10,module: bc-breaking triaged module: numpy module: advanced indexing topic: bc breaking,True
105636,Syntax error when compileing Megatron-LM models. triaged ezyang's list oncall: pt2,2023-07-20 03:58:50+00:00,,0,2,triaged ezyang's list oncall: pt2,True
105635,FSDP with gradient checkpointing lead to redundant allgathers during backward triaged module: fsdp,2023-07-20 03:48:09+00:00,,1,6,triaged module: fsdp,True
105634,[inductor] unexpected dynamic shape error encountered in TritonTemplate triaged ezyang's list oncall: pt2 module: dynamic shapes,2023-07-20 03:15:20+00:00,,0,8,triaged ezyang's list oncall: pt2 module: dynamic shapes,True
105632,torch.nn.TransformerDecoderLayer lacks parameter validation check oncall: transformer/mha,2023-07-20 02:41:58+00:00,,0,2,oncall: transformer/mha,False
105629,F.pad will accept 0 and negative values as parameter module: nn module: error checking triaged module: padding,2023-07-20 02:38:55+00:00,,0,0,module: nn module: error checking triaged module: padding,True
105623,[ONNX] fix `test_fx_op_consistency.py` test failure when running on torch built with cuda module: onnx triaged,2023-07-20 00:40:38+00:00,,0,3,module: onnx triaged,True
105597,Out of bounds error with `nn.MultiMarginLoss` low priority triaged hackathon oncall: pt2,2023-07-19 20:29:18+00:00,,0,1,low priority triaged hackathon oncall: pt2,True
105582,RFC: Integrating oneDNN Graph Compiler into Inductor C++/OpenMP Backend for Enhanced Graph Fusion and Performance feature triaged oncall: pt2 module: inductor module: cpu inductor,2023-07-19 18:53:52+00:00,,1,5,feature triaged oncall: pt2 module: inductor module: cpu inductor,False
105572,Add color-coding to fx graph readable printouts :) triaged hackathon oncall: pt2,2023-07-19 17:45:51+00:00,,0,0,triaged hackathon oncall: pt2,False
105570,Using scans triaged hackathon oncall: pt2,2023-07-19 17:43:39+00:00,,0,1,triaged hackathon oncall: pt2,False
105569,Lowering topk to reductions and pointwise when k is small triaged hackathon oncall: pt2,2023-07-19 17:43:33+00:00,,0,0,triaged hackathon oncall: pt2,False
105568,Move Inductor-specific decompositions to general decomposition registrations. triaged hackathon oncall: pt2,2023-07-19 17:43:31+00:00,,0,0,triaged hackathon oncall: pt2,False
105567,replication_pad1d triaged hackathon oncall: pt2,2023-07-19 17:43:28+00:00,,0,2,triaged hackathon oncall: pt2,False
105566,Reflection_pad1d triaged hackathon oncall: pt2,2023-07-19 17:43:26+00:00,,1,0,triaged hackathon oncall: pt2,False
105562,aten.multilabel_margin_loss_backward triaged hackathon oncall: pt2,2023-07-19 17:43:17+00:00,,0,0,triaged hackathon oncall: pt2,False
105561,aten._cdist_backward triaged hackathon oncall: pt2,2023-07-19 17:43:14+00:00,,0,0,triaged hackathon oncall: pt2,False
105560,aten._trilinear triaged oncall: pt2,2023-07-19 17:43:12+00:00,,0,0,triaged oncall: pt2,False
105556,aten._cdist_forward triaged hackathon oncall: pt2,2023-07-19 17:43:02+00:00,,0,0,triaged hackathon oncall: pt2,False
105555,"Avoid calling AOTAutograd from AOTInductor, since Export has already done that triaged hackathon oncall: pt2",2023-07-19 17:43:00+00:00,,1,4,triaged hackathon oncall: pt2,False
105554,[easy] Add an option to force recompilation triaged hackathon oncall: pt2,2023-07-19 17:42:57+00:00,,0,1,triaged hackathon oncall: pt2,False
105548,torch.sparse.sampled_addmm doesn't compute gradients for 3D tensors module: sparse triaged,2023-07-19 17:23:18+00:00,,0,1,module: sparse triaged,True
105534,test_torchinductor_opinfo tracker triaged hackathon oncall: pt2,2023-07-19 13:53:40+00:00,,0,1,triaged hackathon oncall: pt2,False
105532,"tts_angular: fail_to_run, torch._dynamo.exc.Unsupported: call_method NNModuleVariable() flatten_parameters [] {} triaged oncall: pt2",2023-07-19 13:46:01+00:00,,2,0,triaged oncall: pt2,False
105530,convit_base: AssertionError: Mutating module attribute rel_indices during export. triaged oncall: pt2,2023-07-19 13:37:46+00:00,,1,2,triaged oncall: pt2,False
105529,Efficient BMM for sparse-dense tensors module: sparse triaged topic: new features,2023-07-19 13:20:29+00:00,,0,2,module: sparse triaged topic: new features,True
105528,DISABLED test_conv_with_as_strided_dynamic_shapes_cuda (__main__.DynamicShapesCudaTests) module: rocm triaged module: flaky-tests skipped,2023-07-19 12:46:51+00:00,,0,5,module: rocm triaged module: flaky-tests skipped,False
105526,torch.onnx.export error module: onnx triaged,2023-07-19 10:32:19+00:00,,0,1,module: onnx triaged,True
105520,[ONNX] Exporting the operator 'aten::exponential' to opset version 13 is not supported module: onnx triaged,2023-07-19 06:34:23+00:00,,0,3,module: onnx triaged,False
105519,aten.bernoulli.p is missing in core aten IR opset but does not get decomposed triaged oncall: pt2,2023-07-19 06:18:20+00:00,,0,1,triaged oncall: pt2,False
105515,[ONNX] FX produce valid node names in models module: onnx triaged,2023-07-19 01:32:13+00:00,,0,4,module: onnx triaged,False
105499,[FSDP] Revisit mixed-precision casting logic triaged module: fsdp,2023-07-18 20:50:00+00:00,,0,0,triaged module: fsdp,True
105488,torch.save throws an error when the path uses mixed separators on Windows module: windows triaged,2023-07-18 18:41:12+00:00,,1,6,module: windows triaged,True
105485,Specifying `FakeTensorMode` for Custom Backends triaged oncall: pt2,2023-07-18 17:58:33+00:00,,0,15,triaged oncall: pt2,False
105483,[OpInfo] index.Tensor triaged module: testing,2023-07-18 17:55:03+00:00,,1,0,triaged module: testing,True
105471,[benchmark] Rename the count field FunctionCount oncall: profiler,2023-07-18 15:35:02+00:00,,0,0,oncall: profiler,False
105465,"[proposal] Bit ops: e.g. setbit/getbit/togglebit/byteswap + introduce well-standardized unsigned dtypes (uint16, uint32, uint64) feature triaged needs research module: python frontend",2023-07-18 14:37:19+00:00,,0,2,feature triaged needs research module: python frontend,True
105464,[ONNX] Support Fake Tensor Mode on new Dynamo based ONNX exporter module: onnx triaged enhancement release notes: onnx,2023-07-18 14:35:44+00:00,,1,0,module: onnx triaged enhancement release notes: onnx,False
105460,Specify version module: docs triaged,2023-07-18 13:24:09+00:00,,0,0,module: docs triaged,True
105457,Top level Glossary for users (not contributers) module: docs triaged,2023-07-18 10:00:10+00:00,,0,4,module: docs triaged,True
105454,torch.onnx.export failed: torch.onnx.errors.SymbolicValueError: Unsupported: ONNX export of convolution for kernel of unknown shape module: onnx triaged,2023-07-18 08:59:37+00:00,,0,0,module: onnx triaged,False
105448,Will nn.unfold support non-4D-tensor input in future version? module: nn triaged enhancement actionable,2023-07-18 07:13:59+00:00,,0,1,module: nn triaged enhancement actionable,True
105447,DISABLED test_cross_entropy_large_tensor_reduction_none_cuda (__main__.TestNNDeviceTypeCUDA) module: nn module: rocm triaged module: flaky-tests skipped,2023-07-18 06:40:37+00:00,,0,1,module: nn module: rocm triaged module: flaky-tests skipped,False
105445,Silent Error of torch.fx.symbolic_trace when forward hooks are registered triaged module: fx,2023-07-18 04:14:50+00:00,,0,1,triaged module: fx,True
105442,`vmap` causes unpredictable behavior when combined with `autocast` triaged module: vmap module: amp (automated mixed precision) module: functorch,2023-07-18 02:57:54+00:00,,0,1,triaged module: vmap module: amp (automated mixed precision) module: functorch,True
105382,Need support and testing for Adam optimizer for MPS high priority module: optimizer triaged enhancement module: mps,2023-07-18 00:59:00+00:00,,1,10,high priority module: optimizer triaged enhancement module: mps,True
105379,FSDP loading with a partial state triggers KeyError triaged module: fsdp,2023-07-18 00:32:00+00:00,,0,1,triaged module: fsdp,True
105365,Quadric Layer feature module: nn triaged needs research,2023-07-17 22:44:45+00:00,,0,6,feature module: nn triaged needs research,True
105349,torch.onnx.export does not support divisor_override in AvgPool2d module: onnx triaged,2023-07-17 18:28:57+00:00,,0,4,module: onnx triaged,False
105348,FSDP Full Shard compatibility with BF16 AMP oncall: distributed triaged module: fsdp,2023-07-17 18:22:36+00:00,,1,20,oncall: distributed triaged module: fsdp,True
105338,[ONNX] Refactor `test_fx_op_consistency.py` module: onnx triaged onnx-triaged,2023-07-17 16:49:23+00:00,,1,0,module: onnx triaged onnx-triaged,True
105335,Enable SLEEF on ARM module: build triaged module: sleef module: arm topic: improvements,2023-07-17 16:26:15+00:00,,0,4,module: build triaged module: sleef module: arm topic: improvements,True
105332,DISABLED test_super_resolution_cuda (__main__.TestModels) oncall: jit module: flaky-tests skipped,2023-07-17 15:39:37+00:00,,0,146,oncall: jit module: flaky-tests skipped,False
105329,Softmax doesn't support sparse tensors with the CSR layout module: sparse triaged topic: new features,2023-07-17 15:27:16+00:00,,0,17,module: sparse triaged topic: new features,False
105328,TorchInductor Hack-a-Day on July 19th triaged tracker,2023-07-17 15:13:51+00:00,,3,0,triaged tracker,True
105326,Can't vmap over torch.tensor constructor triaged module: functorch,2023-07-17 14:37:29+00:00,,0,0,triaged module: functorch,True
105325,Padded tensor subclass feature triaged module: nestedtensor tensor subclass,2023-07-17 14:32:16+00:00,,0,9,feature triaged module: nestedtensor tensor subclass,True
105322,DeadKernel when training GNN for Cora on MPS triaged module: mps,2023-07-17 12:55:46+00:00,,0,1,triaged module: mps,True
105319,Implementation of torch.sparse.sampled_baddmm module: sparse triaged topic: new features,2023-07-17 12:04:37+00:00,,0,13,module: sparse triaged topic: new features,False
105318,[docs] torch.sigmoid to make clear equivalence relations to other sigmoid functions module: docs triaged actionable,2023-07-17 11:50:31+00:00,,0,9,module: docs triaged actionable,False
105313,Failed to convert model that has LeakyReLU to ONNX module: onnx triaged,2023-07-17 08:38:49+00:00,,0,0,module: onnx triaged,True
105311,Batching rule not implemented for aten::unsafe_chunk triaged actionable module: vmap module: functorch,2023-07-17 07:08:04+00:00,,0,1,triaged actionable module: vmap module: functorch,True
105304,"Backward pass with sparse parameters results in error ""Sparse division requires a scalar or zero-dim dense tensor divisor"" module: sparse module: loss module: optimizer triaged",2023-07-17 05:03:46+00:00,,0,2,module: sparse module: loss module: optimizer triaged,True
105299,Support ONNX opset 20 to export GELU to one single op module: onnx triaged,2023-07-17 01:34:38+00:00,,0,0,module: onnx triaged,False
105290,Torch.compile Error: RuntimeError: aten::_conj() Expected a value of type 'Tensor' for argument 'self' but instead found type 'complex'. triaged module: complex module: functionalization oncall: pt2 module: pt2-dispatcher,2023-07-16 14:48:37+00:00,,0,7,triaged module: complex module: functionalization oncall: pt2 module: pt2-dispatcher,True
105281,Optimize PyTorch C++ part with Profile-Guided Optimization (PGO) module: performance module: internals triaged oncall: pt2,2023-07-16 00:21:02+00:00,,0,0,module: performance module: internals triaged oncall: pt2,False
105279,[Dynamo][Compile]Torch compile with dynamic shapes not working triaged oncall: pt2,2023-07-15 22:19:55+00:00,,0,25,triaged oncall: pt2,False
105264,Inductor generates incorrect CPU code for `uint8` operations triaged oncall: pt2 module: cpu inductor,2023-07-15 04:27:34+00:00,,1,20,triaged oncall: pt2 module: cpu inductor,True
105255,[discussion] Integrate widely used utilities from fvcore into the core repo oncall: distributed feature module: nn triaged needs research module: LrScheduler,2023-07-14 23:18:45+00:00,,0,0,oncall: distributed feature module: nn triaged needs research module: LrScheduler,True
105254,DISABLED test_fused_optimizers_with_large_tensors (optim.test_optim.TestOptim) module: rocm triaged skipped,2023-07-14 22:52:09+00:00,,0,11,module: rocm triaged skipped,False
105254,DISABLED test_fused_optimizers_with_large_tensors (optim.test_optim.TestOptim) module: rocm triaged skipped,2023-07-14 22:52:09+00:00,,0,11,module: rocm triaged skipped,False
105253,DISABLED test_cross_entropy_large_tensor_reduction_mean_cuda (__main__.TestNNDeviceTypeCUDA) module: rocm triaged skipped,2023-07-14 22:51:42+00:00,,0,1,module: rocm triaged skipped,False
105248,Multiple linux jobs are failing with version `GLIBCXX_3.4.30' not found  module: ci triaged,2023-07-14 22:07:13+00:00,,0,6,module: ci triaged,True
105220,Significant time difference of calculating Jacobian matrix using jacrev and oracle functions module: autograd triaged module: functorch,2023-07-14 14:33:21+00:00,,0,4,module: autograd triaged module: functorch,True
105217,Export+AOTInductor issue tracker triaged tracker,2023-07-14 13:21:55+00:00,,2,18,triaged tracker,True
105214,[DTensor] Dtensor API should report the correct device when GPU is used triaged module: dtensor,2023-07-14 09:45:40+00:00,,1,0,triaged module: dtensor,True
105213,[DTensor] Module parallelized with ColwiseParallel should return a sharded tensor triaged module: dtensor,2023-07-14 09:09:17+00:00,,1,11,triaged module: dtensor,True
105211,autocast + torch.no_grad inference cause backward graph nodes to be lost module: autograd triaged,2023-07-14 08:40:18+00:00,,0,5,module: autograd triaged,True
105203,Pytorch dataloader not loading first-available data with multiple workers module: dataloader triaged,2023-07-14 04:38:49+00:00,,0,0,module: dataloader triaged,True
105196,Error loading TorchScript model with torchvision::nms operation in libtorch oncall: jit,2023-07-14 02:34:46+00:00,,0,0,oncall: jit,True
105192,Repro str could be displayed with slightly wrong env vars module: docs module: cuda triaged actionable,2023-07-14 01:06:00+00:00,,0,1,module: docs module: cuda triaged actionable,True
105181,"torch.compile leaks memory after compiled object is deleted, no apparent way to clean needs reproduction module: memory usage triaged oncall: pt2 module: dynamo",2023-07-13 21:45:35+00:00,,1,6,needs reproduction module: memory usage triaged oncall: pt2 module: dynamo,False
105157,PT2 custom ops does not work with future annotations triaged module: custom-operators oncall: pt2 module: pt2-dispatcher,2023-07-13 17:55:49+00:00,,1,2,triaged module: custom-operators oncall: pt2 module: pt2-dispatcher,False
105134,TypeError: 'NoneType' object is not subscriptable (Occurred when translating col2im). Can't translate torch.nn.functional.fold in opset_version 18. module: onnx triaged,2023-07-13 08:08:47+00:00,,0,7,module: onnx triaged,True
105125,DISABLED test_conv (quantization.jit.test_quantize_jit.TestQuantizeJit) module: rocm triaged skipped,2023-07-13 06:11:39+00:00,,0,1,module: rocm triaged skipped,False
105124,DISABLED test_conv_transpose (quantization.jit.test_quantize_jit.TestQuantizeJit) module: rocm triaged skipped,2023-07-13 06:09:33+00:00,,0,1,module: rocm triaged skipped,False
105123,DISABLED test_observer_with_ignored_function (quantization.jit.test_quantize_jit.TestQuantizeJit) module: rocm triaged skipped,2023-07-13 06:06:54+00:00,,0,1,module: rocm triaged skipped,False
105121,DISABLED test_single_linear (quantization.jit.test_quantize_jit.TestQuantizeJit) module: rocm triaged skipped,2023-07-13 06:04:39+00:00,,0,1,module: rocm triaged skipped,False
105120,DISABLED test_nested (quantization.jit.test_quantize_jit.TestQuantizeJit) module: rocm triaged skipped,2023-07-13 06:00:30+00:00,,0,1,module: rocm triaged skipped,False
105119,DISABLED test_unary_ops (__main__.TestTensorExprFuser) module: rocm triaged skipped,2023-07-13 05:57:00+00:00,,0,1,module: rocm triaged skipped,False
105108,MacOS arm64 runners are not available in CI module: ci triaged,2023-07-13 01:42:24+00:00,,0,2,module: ci triaged,True
105105,Remaining functions without meta registrations triaged module: meta tensors,2023-07-13 00:52:18+00:00,,0,4,triaged module: meta tensors,True
105494,workaround for using vmap when .item() is being used internally triaged module: vmap module: functorch,2023-07-12 22:26:39+00:00,,0,6,triaged module: vmap module: functorch,True
105092,[RFC] Proposal to upgrade LLVM version triaged NNC module: cpu inductor,2023-07-12 21:32:57+00:00,,0,7,triaged NNC module: cpu inductor,True
105077,torch.load fails under FakeTensorMode for GPT2 model triaged ezyang's list oncall: pt2 module: fakeTensor module: dynamo release notes: dynamo module: pt2-dispatcher,2023-07-12 18:30:25+00:00,,1,4,triaged ezyang's list oncall: pt2 module: fakeTensor module: dynamo release notes: dynamo module: pt2-dispatcher,False
105073,[ONNX] Support aten::var_mean module: onnx triaged,2023-07-12 17:49:56+00:00,,0,0,module: onnx triaged,False
105068,[linalg] test_ops.py::test_python_ref_meta__refs_linalg_svd_cpu_complex failing triaged module: linear algebra module: meta tensors,2023-07-12 16:41:03+00:00,,0,10,triaged module: linear algebra module: meta tensors,True
105066,test_view_dynamic_zero_dim no longer testing zero input module: onnx triaged,2023-07-12 15:57:07+00:00,,1,2,module: onnx triaged,True
105062,[feature request] make the input k in rot90 a list of int to rotate tensors individually in a batch feature triaged module: python frontend,2023-07-12 14:39:31+00:00,,0,4,feature triaged module: python frontend,True
105062,[feature request] make the input k in rot90 a list of int to rotate tensors individually in a batch feature triaged module: python frontend,2023-07-12 14:39:31+00:00,,0,4,feature triaged module: python frontend,True
105060,extra information messages for mac in setup.py would help.  module: build module: docs triaged,2023-07-12 14:28:12+00:00,,0,1,module: build module: docs triaged,True
105058,Support Delay Loading of c10.dll in when using libtorch as a thirdparty library. module: windows module: abi triaged,2023-07-12 14:16:21+00:00,,0,0,module: windows module: abi triaged,True
105053,Multiple dimensions support for `torch.max` feature triaged module: numpy needs design module: python frontend,2023-07-12 11:05:11+00:00,,0,10,feature triaged module: numpy needs design module: python frontend,True
105042,"`assert has_same_metadata(inpt_new, inpt_old)` fails when capturing forwards + backwards in train_step with resnet18 triaged oncall: pt2 module: functorch module: pt2-dispatcher",2023-07-12 06:02:13+00:00,,0,1,triaged oncall: pt2 module: functorch module: pt2-dispatcher,True
105024,DISABLED test_homogeneous_attributes (__main__.TestFSDPMiscMultiThread) oncall: distributed module: flaky-tests skipped module: fsdp,2023-07-12 00:59:24+00:00,,2,7,oncall: distributed module: flaky-tests skipped module: fsdp,False
105013,DISABLED test_compile_vmap_hessian_cuda (__main__.TestCompileTransformsCUDA) module: rocm triaged skipped,2023-07-11 21:46:35+00:00,,0,3,module: rocm triaged skipped,False
104998,[export] tensor creation ops burn in device triaged oncall: pt2 oncall: export,2023-07-11 19:48:40+00:00,,0,5,triaged oncall: pt2 oncall: export,False
104981,NotImplementedError: Could not run 'aten::_spdiags' with arguments from the 'CUDA' backend. module: sparse module: cuda triaged,2023-07-11 15:15:08+00:00,,0,3,module: sparse module: cuda triaged,True
104962,Add a diagram showing the code structure to CONTRIBUTING.md  module: docs triaged,2023-07-11 13:08:07+00:00,,0,0,module: docs triaged,False
104959,Saving a LightningModule torch.jit.ScriptModule is incompatible with torch.amp.autocast oncall: jit has workaround module: amp (automated mixed precision),2023-07-11 12:33:15+00:00,,0,2,oncall: jit has workaround module: amp (automated mixed precision),True
104952,[Inductor] [CPU] performance regression with TORCHINDUCTOR_FREEZING=1 triaged oncall: pt2 module: cpu inductor,2023-07-11 09:14:54+00:00,,1,0,triaged oncall: pt2 module: cpu inductor,False
104949,ONNX export process  failed to keep consistence of input_names specified module: onnx triaged,2023-07-11 08:34:37+00:00,,0,0,module: onnx triaged,False
104943,[torch.compile] RuntimeError during Gradient Computation in torch.compile() triaged has workaround bug oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,2023-07-11 05:58:55+00:00,,1,3,triaged has workaround bug oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,True
104935,torch version compare triaged module: python frontend,2023-07-11 03:37:13+00:00,,0,1,triaged module: python frontend,True
104925,Unnecessary record_stream call for backend:cudaMallocAsync module: cuda module: logging triaged,2023-07-11 02:06:09+00:00,,0,3,module: cuda module: logging triaged,True
104913,StableDiffusion with dynamic=True still recompiles triaged oncall: pt2 module: dynamic shapes,2023-07-10 23:04:50+00:00,,0,3,triaged oncall: pt2 module: dynamic shapes,False
104906,torch.jit.frontend.NotSupportedError: keyword-arg expansion is not supported: for dgl.nn.HeteroGraphConv() oncall: jit,2023-07-10 21:01:49+00:00,,0,3,oncall: jit,False
104899,Refactor Adam and AdamW by abstracting out common code module: optimizer triaged better-engineering actionable,2023-07-10 19:26:26+00:00,,1,7,module: optimizer triaged better-engineering actionable,True
104884,[dynamo][higher_order_op] assert in check_kwargs leads to error instead of graph-break good first issue triaged oncall: pt2 module: dynamo,2023-07-10 17:19:36+00:00,,0,1,good first issue triaged oncall: pt2 module: dynamo,True
104880,torch.onnx.export does not respect nn.Module.forward API when using export_modules_as_functions=True module: onnx triaged,2023-07-10 16:28:56+00:00,,0,2,module: onnx triaged,False
104878,DISABLED test_custom_op_cuda_cuda_wrapper (__main__.TestCudaWrapper) triaged module: flaky-tests skipped module: inductor,2023-07-10 15:40:22+00:00,,0,2,triaged module: flaky-tests skipped module: inductor,False
104875,torch/testing/_comparison.py: If you are a user and see this message during normal operation please file an issue triaged module: testing,2023-07-10 14:28:32+00:00,,0,0,triaged module: testing,True
104872,errors in CONTRIBUTING.md module: docs triaged,2023-07-10 13:56:56+00:00,,0,2,module: docs triaged,True
104868,Conversion of a CSR tensor with batches to COO tensor fails module: sparse triaged,2023-07-10 13:09:41+00:00,,1,0,module: sparse triaged,True
104867,rfftn and irfftn operations in pt2 return different results compared to v1.12.1 module: cuda triaged module: third_party module: fft,2023-07-10 12:19:05+00:00,,0,8,module: cuda triaged module: third_party module: fft,True
104860,torch.nn.Conv2d's padding mode circular cannot accept 3-dim input module: nn triaged actionable,2023-07-10 07:06:15+00:00,,1,4,module: nn triaged actionable,True
104857,Torch's `LayerNorm` and Adam optimizer vs those in tensorflow needs reproduction module: numerical-stability module: nn module: optimizer triaged,2023-07-10 02:44:26+00:00,,0,3,needs reproduction module: numerical-stability module: nn module: optimizer triaged,True
104856,DISABLED test_custom_op_cuda (__main__.CudaTests) triaged module: flaky-tests skipped oncall: pt2 module: inductor,2023-07-10 00:58:14+00:00,,0,22,triaged module: flaky-tests skipped oncall: pt2 module: inductor,False
104854,DISABLED test_custom_op_cpu_dynamic_shapes_cpp_wrapper (__main__.DynamicShapesCppWrapperCpuTests) triaged module: flaky-tests skipped module: inductor,2023-07-10 00:58:09+00:00,,0,3,triaged module: flaky-tests skipped module: inductor,False
104853,torch.norm inconsistency? module: numerical-stability triaged module: norms and normalization,2023-07-10 00:44:49+00:00,,0,1,module: numerical-stability triaged module: norms and normalization,True
104849,[feature request] torch.mix function to generalize/symmetrize addcmul feature triaged needs research module: python frontend,2023-07-09 22:59:58+00:00,,0,6,feature triaged needs research module: python frontend,False
104845,Implement `diag` method for sparse COO tensors module: sparse triaged,2023-07-09 16:03:54+00:00,,0,2,module: sparse triaged,True
104832,"MPS matmul with sliced (strided) out argument produces wrong output, may corrupt memory triaged module: mps",2023-07-09 00:20:43+00:00,,0,0,triaged module: mps,True
104823,Unrelated error messages with torch.nn.AdaptiveAvgPool3d module: nn triaged module: pooling,2023-07-08 09:00:01+00:00,,1,0,module: nn triaged module: pooling,True
104933,torch.func.jvp fails with BERT training module: autograd triaged actionable module: forward ad module: functorch,2023-07-08 03:37:16+00:00,,0,1,module: autograd triaged actionable module: forward ad module: functorch,True
104817,[RFC] Let in-place foreach functions return a list of Tensors triaged module: mta,2023-07-08 01:12:07+00:00,,0,1,triaged module: mta,True
104814,[compile][dynamic] dsplit is seeing a list of mixed ints and symints triaged oncall: pt2 module: dynamic shapes,2023-07-08 00:38:10+00:00,,0,0,triaged oncall: pt2 module: dynamic shapes,True
104808,PyTorch built with CuDNN-8.8.1 crashes if CuDNN-8.9.2 is installed on the system module: cudnn module: ci triaged,2023-07-07 22:24:00+00:00,,2,0,module: cudnn module: ci triaged,True
104791,"inductor/triton fails on `view(..., dtype=torch.int16)` triaged bug oncall: pt2 module: inductor",2023-07-07 18:54:39+00:00,,0,7,triaged bug oncall: pt2 module: inductor,True
104789,[BE] Evaluate and improve eager for-loop optimizer memory perf module: optimizer triaged better-engineering actionable,2023-07-07 18:38:04+00:00,,1,2,module: optimizer triaged better-engineering actionable,True
104788,Use `isinstance` instead of `type` when checking for `torch.nn.Parameter` module: nn triaged,2023-07-07 18:35:54+00:00,,0,2,module: nn triaged,True
104776,torch.nn.CrossEntropyLoss: class weighting changes label_smoothing module: nn module: loss triaged,2023-07-07 15:41:47+00:00,,2,4,module: nn module: loss triaged,True
104775,Subgraph matcher returned a false match triaged module: fx oncall: pt2,2023-07-07 15:22:00+00:00,,1,1,triaged module: fx oncall: pt2,False
104768,Support for `eval` in functional_call module: nn triaged module: functional UX,2023-07-07 11:08:07+00:00,,0,1,module: nn triaged module: functional UX,True
104761,"Torch Filename Storage hangs on ""file_system"" sharing strategy after in-place fill module: multiprocessing triaged module: mps",2023-07-07 05:36:36+00:00,,0,5,module: multiprocessing triaged module: mps,True
104755,fsdp load model causing insufficient CPU memory triaged module: fsdp,2023-07-07 02:38:39+00:00,,2,3,triaged module: fsdp,True
104748,torch._dynamo.exc.InternalTorchDynamoError: Could not run 'aten::_local_scalar_dense' with arguments from the 'Meta' backend triaged oncall: pt2 oncall: export,2023-07-06 23:37:55+00:00,,0,1,triaged oncall: pt2 oncall: export,False
104739,Error reporting uses formal parameter names of downstream C++ function triaged module: multi-headed-attention,2023-07-06 22:09:37+00:00,,0,5,triaged module: multi-headed-attention,True
104737,DISABLED test_vmapvjpvjp_linalg_lu_factor_ex_cuda_float32 (__main__.TestOperatorsCUDA) triaged module: flaky-tests skipped module: functorch,2023-07-06 21:39:57+00:00,,0,3,triaged module: flaky-tests skipped module: functorch,False
104732,"torch.jit.trace says ""Arguments for call are invalid"" on torch.ops.aten.sub(3, x, alpha=3) oncall: jit",2023-07-06 20:08:35+00:00,,0,0,oncall: jit,True
104729,Add support for NEON ISA in the Inductor C++ backend feature triaged module: inductor,2023-07-06 19:33:55+00:00,,0,17,feature triaged module: inductor,False
104719,Nondeterministic segfault in test_content_store.py under Dynamo config module: tests triaged module: dynamo,2023-07-06 18:13:56+00:00,,0,2,module: tests triaged module: dynamo,False
104712,torch.jit slicing error (styleganv2) oncall: jit,2023-07-06 16:43:20+00:00,,0,2,oncall: jit,True
104711,New Loss Function Add In Pytorch feature module: loss triaged,2023-07-06 15:49:43+00:00,,0,2,feature module: loss triaged,False
104704,generate_vmap_rule=True sometimes gives batched grad_output triaged module: functorch,2023-07-06 14:02:02+00:00,,0,0,triaged module: functorch,True
104702,[feature request] Specialized memory layouts and wide blocked/tiled dtypes for cublasLt/onednn: e.g. torch.float16x32 / torch.int8x32 / torch.bits1x512 (akin to torch.quint2x4) feature triaged module: memory format,2023-07-06 12:26:26+00:00,,0,1,feature triaged module: memory format,False
104701,System memory leak when using different input size of torch.nn.Conv3d module: cudnn module: nn module: cuda module: memory usage triaged,2023-07-06 12:17:30+00:00,,0,3,module: cudnn module: nn module: cuda module: memory usage triaged,True
104698,Incorrect Error Message Ordering for nn.AdaptiveAvgPool2d with Incorrect output_size  module: nn triaged,2023-07-06 10:31:43+00:00,,0,2,module: nn triaged,True
104697,LSTM built-in dropout not reproducible on GPU module: cudnn module: nn triaged module: random,2023-07-06 10:23:34+00:00,,0,2,module: cudnn module: nn triaged module: random,True
104695,DISABLED test_cuda_memory_leak_detection (__main__.TestCudaMultiGPU) module: cuda triaged module: flaky-tests skipped,2023-07-06 09:39:51+00:00,,0,1,module: cuda triaged module: flaky-tests skipped,False
104678,torch._dynamo.export does not work with bert model triaged ezyang's list oncall: pt2 oncall: export,2023-07-06 00:25:53+00:00,,0,5,triaged ezyang's list oncall: pt2 oncall: export,False
104674,[compile] DDPOptimizer + activation checkpointing not supported module: checkpoint triaged module: ddp oncall: pt2 module: distributed,2023-07-05 23:12:24+00:00,,0,3,module: checkpoint triaged module: ddp oncall: pt2 module: distributed,True
104653,"vision_maskrcnn: AssertionError: expected size 368==368, stride 156==28 at dim=0 triaged oncall: pt2 module: dynamic shapes",2023-07-05 19:16:30+00:00,,0,5,triaged oncall: pt2 module: dynamic shapes,False
104623,I propose a new overview section in the documentation triaged topic: docs,2023-07-05 11:43:06+00:00,,0,8,triaged topic: docs,False
104620,`torch.distributed.rpc.backend_registry.register_backend` fails to update `BackendType` enum oncall: distributed triaged module: rpc,2023-07-05 08:36:54+00:00,,0,1,oncall: distributed triaged module: rpc,True
104610,"torch.compile fails with ""INTERNAL ASSERT FAILED"" when compiling GPT-2 module: onnx triaged oncall: pt2",2023-07-04 21:26:11+00:00,,1,10,module: onnx triaged oncall: pt2,False
104606,"Failure in optimize_for_mobile when using conv1d(..., padding='same') triaged oncall: mobile",2023-07-04 19:02:39+00:00,,0,2,triaged oncall: mobile,True
104602,"F.adaptive_avg_pool3d(input, 1) returns infinity in half precision module: numerical-stability module: nn module: cpu triaged",2023-07-04 14:16:11+00:00,,0,0,module: numerical-stability module: nn module: cpu triaged,True
104598,ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. needs reproduction module: binaries triaged,2023-07-04 12:40:42+00:00,,0,4,needs reproduction module: binaries triaged,True
104593,Bug in Conv/BN fuser with torch.fx triaged module: fx oncall: fx,2023-07-04 09:50:40+00:00,,0,1,triaged module: fx oncall: fx,True
104591,version libcudnn_ops_infer.so.8 not defined in file libcudnn_ops_infer.so.8 with link time reference module: cuda triaged,2023-07-04 08:43:35+00:00,,0,15,module: cuda triaged,True
104589,Issue with FSDP does not reduce memory footprint  when scaling up GPUs oncall: distributed triaged module: fsdp,2023-07-04 08:27:51+00:00,,0,0,oncall: distributed triaged module: fsdp,True
104587,Conv1d step-by-step numerical error  needs reproduction triaged module: macos module: determinism module: arm,2023-07-04 08:06:20+00:00,,1,12,needs reproduction triaged module: macos module: determinism module: arm,True
104568,Init_rpc() errors when running the test code in the TorchPRC document on two different machines  triaged module: rpc,2023-07-04 02:23:52+00:00,,0,1,triaged module: rpc,True
104536,torch compile for jacrev'ed function triaged oncall: pt2 module: functorch module: pt2-dispatcher,2023-07-03 20:02:34+00:00,,0,6,triaged oncall: pt2 module: functorch module: pt2-dispatcher,False
104531,[RFC] Optional Modular Representation for FX Graph from `torch.compile()` triaged oncall: pt2,2023-07-03 16:46:08+00:00,,0,3,triaged oncall: pt2,False
104516,vec_test_all_types_xxx with dtype c10::complex<float> and c10::complex<double> has failures on division module: cpu triaged module: complex module: vectorization,2023-07-03 05:41:28+00:00,,1,3,module: cpu triaged module: complex module: vectorization,True
104510,"Using the latest version of Torch, when the code executes tcpstore, there is no response oncall: distributed triaged",2023-07-02 17:33:42+00:00,,1,3,oncall: distributed triaged,True
104507,[error] while Implementation of pytorch DistributedParallel oncall: distributed triaged,2023-07-02 07:34:19+00:00,,0,2,oncall: distributed triaged,False
104506,TImeout in NCCL doesn't work oncall: distributed triaged,2023-07-02 07:33:57+00:00,,1,13,oncall: distributed triaged,True
104505,Wrong functionalization of as_strided leads to wrong results high priority module: docs triaged oncall: pt2,2023-07-02 06:25:03+00:00,,1,3,high priority module: docs triaged oncall: pt2,True
104502,Get errors after compiling and running PyTorch MINIMAL EXAMPLE for c++ Mac M1 with make module: binaries module: cpp triaged module: macos module: arm,2023-07-02 01:14:36+00:00,,0,10,module: binaries module: cpp triaged module: macos module: arm,True
104479,FSDP Optimizer Overlap - follow ups oncall: distributed triaged module: fsdp,2023-06-30 19:37:28+00:00,,1,2,oncall: distributed triaged module: fsdp,True
104472,Investigate numerical stability of forward-mode AD of some foreach functions triaged module: forward ad,2023-06-30 18:03:18+00:00,,0,0,triaged module: forward ad,True
104466,`torch.view_as_real(tensor)` should return `nn.identity(tensor)` if its not complex instead of raising an error triaged module: complex,2023-06-30 15:56:35+00:00,,0,2,triaged module: complex,True
104458,[Feature Request] Add a new overload of torch::jit::load to restore traced shape and type  oncall: jit,2023-06-30 03:33:16+00:00,,0,0,oncall: jit,False
104454,DISABLED test_nnc_correctness_frac_cpu_bfloat16 (__main__.TestNNCOpInfoCPU) skipped NNC,2023-06-30 01:15:08+00:00,,0,1,skipped NNC,False
104452,[ONNX][TypePromo] Automate codegen type promotion rules module: onnx triaged,2023-06-30 00:56:30+00:00,,0,1,module: onnx triaged,False
104450,"Numpy/scipy module works fine with Torch modules, but not TorchScript. How to torchscript a numpy/scipy module? oncall: jit",2023-06-30 00:29:43+00:00,,0,4,oncall: jit,True
104435,torch.compiled model output gets overwritten despite tensor.detach() triaged oncall: pt2,2023-06-29 19:35:52+00:00,,1,1,triaged oncall: pt2,False
104421,LibTorch 2.0.1 scripting in Debug mode on Windows module: windows module: cpp triaged,2023-06-29 16:29:46+00:00,,0,2,module: windows module: cpp triaged,True
104417,Support CUDA 12.2  module: cuda triaged,2023-06-29 15:58:24+00:00,,0,20,module: cuda triaged,True
104411,"RuntimeError: t == DeviceType::CUDA INTERNAL ASSERT FAILED at HIPGuardImplMasqueradingAsCUDA.h:60, please report a bug to PyTorch module: rocm triaged",2023-06-29 13:44:13+00:00,,0,0,module: rocm triaged,True
104405,Detailed error: Tensor-likes are not close! When use torch.jit.trace oncall: jit,2023-06-29 11:50:26+00:00,,0,5,oncall: jit,True
104403,Inconsistencies in ONNX exporting of operation `torch.full()` module: onnx low priority triaged OSS contribution wanted,2023-06-29 10:05:56+00:00,,0,3,module: onnx low priority triaged OSS contribution wanted,True
104389,distributed hooks want to support custom device oncall: distributed triaged,2023-06-29 01:29:39+00:00,,2,2,oncall: distributed triaged,False
104367,FakeTensor can't handle meta impls that perform device conversion triaged module: fakeTensor,2023-06-28 19:26:20+00:00,,0,4,triaged module: fakeTensor,True
104366,DISABLED test_conv3d_64bit_indexing_cuda (__main__.TestConvolutionNNDeviceTypeCUDA) module: rocm triaged skipped,2023-06-28 18:52:41+00:00,,0,1,module: rocm triaged skipped,False
104361,ReduceLROnPlateau will throw IndexError: list index out of range with modified optimizer's param_groups. module: optimizer triaged actionable module: LrScheduler,2023-06-28 18:25:10+00:00,,0,5,module: optimizer triaged actionable module: LrScheduler,True
104342,"Segmentation error while using F.cross_entropy with mps(for code that works fine with device= ""cpu"") triaged module: mps",2023-06-28 13:07:30+00:00,,0,1,triaged module: mps,True
104328,DISABLED test_backward_ddp_inside (__main__.TensorPipeDdpUnderDistAutogradTest) oncall: distributed module: flaky-tests skipped,2023-06-28 06:44:26+00:00,,0,1,oncall: distributed module: flaky-tests skipped,False
104322,Illegal Memory Access on H100 `TestSparseCompressedTritonKernelsCUDA.test_triton_sampled_addmm_block_size_16_cuda_bfloat16` module: sparse module: cuda triaged,2023-06-28 06:13:44+00:00,,0,0,module: sparse module: cuda triaged,True
104315,Torch randperm with device mps does not sample exactly uniformly from all possible permutations triaged module: random module: mps,2023-06-28 01:46:01+00:00,,0,1,triaged module: random module: mps,True
104301,Attempt to use minifier on sam model fails triaged oncall: pt2 module: dynamic shapes module: minifier,2023-06-27 21:58:34+00:00,,0,3,triaged oncall: pt2 module: dynamic shapes module: minifier,True
104297,"torch.distributed.all_to_all_single & alltoall_base, size limit INT_MAX oncall: distributed",2023-06-27 21:01:43+00:00,,1,1,oncall: distributed,False
104296,affine_grid and grid_sample operators merge/accelleration module: performance feature triaged,2023-06-27 21:01:03+00:00,,0,29,module: performance feature triaged,True
104289,getattr on `__slots__` object potentially suspicious triaged oncall: pt2,2023-06-27 20:25:07+00:00,,1,1,triaged oncall: pt2,False
104284,`F.conv1d` and `F.conv2d` propagate `nan`'s incorrectly when minibatch > 15 needs reproduction module: nn triaged module: macos module: NaNs and Infs module: arm,2023-06-27 19:26:33+00:00,,0,7,needs reproduction module: nn triaged module: macos module: NaNs and Infs module: arm,True
104282,Rename `topic: not user facing` to `release notes: not user facing` triaged,2023-06-27 19:10:53+00:00,,0,0,triaged,False
104265,torch._dynamo.exc.TorchRunTimeError in get_fake_value while performing quantization aware training oncall: quantization triaged oncall: pt2,2023-06-27 16:23:33+00:00,,1,7,oncall: quantization triaged oncall: pt2,False
104259,ImportError: libcudnn.so.8: cannot open shared object file: No such file or directory needs reproduction module: binaries module: cuda triaged,2023-06-27 14:32:48+00:00,,0,16,needs reproduction module: binaries module: cuda triaged,True
104258,[FSDP] `ignored_states` is broken with auto wrap triaged module: fsdp,2023-06-27 14:31:27+00:00,,1,0,triaged module: fsdp,True
104252,[RFC] Make `_HYBRID_SHARD_ZERO2` public as `HYBRID_SHARD_GRAD_OP` triaged module: fsdp,2023-06-27 12:18:05+00:00,,0,0,triaged module: fsdp,True
104247,"[proposal] ""Name"" string attribute for modules, parameters, buffers, tensors for more pleasant debugging (especially for graph printouts / export / studying compiled generated code) feature triaged needs design",2023-06-27 10:29:24+00:00,,0,11,feature triaged needs design,True
104244,DISABLED test_mem_get_info (__main__.TestCudaMultiGPU) module: cuda triaged module: flaky-tests skipped,2023-06-27 09:39:30+00:00,,0,31,module: cuda triaged module: flaky-tests skipped,False
104230,[ONNX] Investigate `nn.functional.nll_loss` skip/xfail reason module: onnx triaged,2023-06-27 03:19:10+00:00,,0,0,module: onnx triaged,False
104195,Torchscript with dynamic quantization produces inconsistent model outputs oncall: jit oncall: quantization low priority triaged,2023-06-26 16:52:51+00:00,,1,4,oncall: jit oncall: quantization low priority triaged,True
104194,View ops on fake tensors can dispatch `detach`es to backend kernels triaged module: fakeTensor,2023-06-26 16:37:04+00:00,,0,2,triaged module: fakeTensor,True
104193,Conversion from strided to batched sparse compressed tensor with a non-constant number of zeros in batches fails module: sparse triaged,2023-06-26 16:26:47+00:00,,1,2,module: sparse triaged,True
104191,torch.embedding: Trying to convert BFloat16 to the MPS backend but it does not have support for that dtype. triaged enhancement module: bfloat16 module: mps,2023-06-26 14:55:11+00:00,,0,2,triaged enhancement module: bfloat16 module: mps,True
104188,Add memory managemenet information for Apple silicon mps backend triaged enhancement module: mps,2023-06-26 13:55:04+00:00,,0,3,triaged enhancement module: mps,True
104175,No document for parameter `load_debug_files` in `torch::jit::load` in C++ API module: docs triaged actionable,2023-06-26 07:53:27+00:00,,0,0,module: docs triaged actionable,True
104174,distributed.scatter memory leak in source rank oncall: distributed module: memory usage,2023-06-26 07:44:44+00:00,,0,0,oncall: distributed module: memory usage,False
104168,Incorrect Reduce collective result with `_coalescing_manager`  oncall: distributed,2023-06-26 00:46:38+00:00,,1,2,oncall: distributed,True
104164,DDP enhancement  oncall: distributed,2023-06-25 19:37:54+00:00,,0,0,oncall: distributed,False
104163,Nested Tensor with PyG dataset custom class triaged module: nestedtensor actionable,2023-06-25 19:14:06+00:00,,1,2,triaged module: nestedtensor actionable,True
104162,"Network does not return any thing, not even None and breaks loops needs reproduction module: windows triaged module: third_party",2023-06-25 18:34:09+00:00,,0,5,needs reproduction module: windows triaged module: third_party,True
104154,Numbers bigger than the range should be inf while the implementation just keeps the original. module: docs triaged module: NaNs and Infs,2023-06-25 07:52:04+00:00,,0,6,module: docs triaged module: NaNs and Infs,True
104152,Error 101: invalid device ordinal (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.) module: cuda triaged module: third_party,2023-06-25 04:06:16+00:00,,0,2,module: cuda triaged module: third_party,True
104147,PyTorch2.0 ROCM LayerNorm HIP error: invalid configuration module: rocm triaged,2023-06-24 18:55:11+00:00,,0,6,module: rocm triaged,True
104146,make_fx: torch.where scalar promotion burns in device triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-06-24 17:24:36+00:00,,0,3,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
104144,[ONNX] Support symbolic tracing without using external `FakeTensorMode` on public API module: onnx triaged enhancement release notes: onnx,2023-06-24 15:28:06+00:00,,0,0,module: onnx triaged enhancement release notes: onnx,False
104128,Python Crashes When Importing Torch With C API needs reproduction module: windows triaged module: third_party,2023-06-23 21:40:09+00:00,,1,18,needs reproduction module: windows triaged module: third_party,True
104119,Re-enable `test_typing` module: typing module: ci triaged,2023-06-23 18:57:45+00:00,,0,2,module: typing module: ci triaged,True
104113,Documentation building fails due to torchgen module: build module: docs triaged actionable,2023-06-23 17:09:53+00:00,,0,7,module: build module: docs triaged actionable,True
104107,Tensor to_sparse fails on large matrices module: sparse module: cuda triaged,2023-06-23 16:21:58+00:00,,0,8,module: sparse module: cuda triaged,True
104106,batch size unexpectedly affects model inference on Mac M1 triaged module: mps,2023-06-23 15:32:46+00:00,,0,0,triaged module: mps,True
104102,Inductor does not check input SymInt invariant on GraphModules passed in good first issue triaged oncall: pt2 module: dynamic shapes module: inductor,2023-06-23 13:29:40+00:00,,0,12,good first issue triaged oncall: pt2 module: dynamic shapes module: inductor,True
104095,(Possible) Memory leak on deleting a compiled model high priority triaged has workaround module: cuda graphs oncall: pt2,2023-06-23 08:08:38+00:00,,1,8,high priority triaged has workaround module: cuda graphs oncall: pt2,True
104093,RuntimeError: _ivalue_ INTERNAL ASSERT FAILED oncall: jit,2023-06-23 08:01:43+00:00,,0,0,oncall: jit,True
104088,Regressions with torch.compile + amp + ddp with recent nightly builds needs reproduction oncall: distributed module: cuda triaged module: ddp oncall: pt2 module: distributed,2023-06-23 04:00:54+00:00,,0,1,needs reproduction oncall: distributed module: cuda triaged module: ddp oncall: pt2 module: distributed,True
104081,Distributing HSDP checkpoint writing for load balancing  oncall: distributed module: fsdp,2023-06-23 00:22:03+00:00,,1,3,oncall: distributed module: fsdp,False
104053,Tracking issue for optimizer graph not being an inference graph triaged tracker oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-06-22 18:23:52+00:00,,0,2,triaged tracker oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
104047,[torch.compile] torch._dynamo.exc.TorchRuntimeError: Failed running call_function <built-in method normal of type object at ***: got an unexpected keyword argument 'device' triaged module: random oncall: pt2 module: decompositions,2023-06-22 17:21:03+00:00,,0,1,triaged module: random oncall: pt2 module: decompositions,True
104045,"""TorchDynamo Deeper Dive"" doc is missing some part triaged topic: docs",2023-06-22 15:30:31+00:00,,1,1,triaged topic: docs,False
104043,Torch.compile tutorial shows incorrect triton kernel triaged topic: docs,2023-06-22 14:18:18+00:00,,1,1,triaged topic: docs,True
104041,Scripted model is loaded on GPU but the inference seems to utilize the CPU with zero GPU utilization oncall: jit,2023-06-22 13:47:39+00:00,,0,1,oncall: jit,True
104040,Improved error checking for custom Function when saving intermediates module: double backwards module: autograd triaged actionable,2023-06-22 13:35:38+00:00,,0,0,module: double backwards module: autograd triaged actionable,True
104033,Reproducibility documentation should be updated module: docs triaged module: numerical-reproducibility,2023-06-22 09:30:45+00:00,,0,0,module: docs triaged module: numerical-reproducibility,True
104029,SummaryWriter.add_embedding not working for RGBA images triaged enhancement module: tensorboard,2023-06-22 07:40:56+00:00,,0,0,triaged enhancement module: tensorboard,True
104026,Is memory-efficient FSDP initialization intended to be possible with torch.device('meta')? triaged module: fsdp,2023-06-22 06:37:44+00:00,,1,14,triaged module: fsdp,False
104022,[PT2.0][compile] torch._dynamo.config.log_level does not exist module: logging triaged oncall: pt2,2023-06-22 05:39:16+00:00,,1,0,module: logging triaged oncall: pt2,False
104020,[torch.compile] `permute_linear_fusion` ignores the inplace operation for the tensor high priority triaged module: correctness (silent) oncall: pt2 module: aotdispatch module: inductor module: pt2-dispatcher,2023-06-22 04:44:24+00:00,,1,4,high priority triaged module: correctness (silent) oncall: pt2 module: aotdispatch module: inductor module: pt2-dispatcher,True
104011,DISABLED test_backward_ddp_outside_uneven_inputs (__main__.TensorPipeDdpUnderDistAutogradTest) oncall: distributed module: flaky-tests skipped,2023-06-22 00:58:09+00:00,,0,1,oncall: distributed module: flaky-tests skipped,False
104010,Several Torchbench models don't run with float16 or bfloat16 in the inference eager mode triaged module: bfloat16 module: half module: benchmark oncall: pt2,2023-06-22 00:35:30+00:00,,0,3,triaged module: bfloat16 module: half module: benchmark oncall: pt2,True
103990,[Inductor] Freezing Add support for Caching Parameter Conversions feature triaged oncall: pt2 module: inductor,2023-06-21 20:18:47+00:00,,0,7,feature triaged oncall: pt2 module: inductor,False
103966,'MPS' Issue Running HuggingFace Transformer Pix2Struct Model triaged module: mps,2023-06-21 15:32:08+00:00,,0,1,triaged module: mps,True
103965,[ONNX] Isolate TorchScript-based code-base from Dynamo-based ONNX exporter for easier deprecation module: onnx triaged enhancement,2023-06-21 15:28:40+00:00,,1,0,module: onnx triaged enhancement,False
103962,How to unwrap after auto_wrap in FSDP? oncall: distributed triaged module: fsdp,2023-06-21 11:27:10+00:00,,0,3,oncall: distributed triaged module: fsdp,True
103960,CODEOWNERS file has errors due to non existent people being referred to module: docs triaged,2023-06-21 09:53:43+00:00,,0,3,module: docs triaged,True
103959,"Need the full ""Release Compatibility Matrix"" of torch module: binaries oncall: releng triaged",2023-06-21 09:47:49+00:00,,0,3,module: binaries oncall: releng triaged,True
103958,How to modify gradients of an FSDP model? oncall: distributed module: fsdp,2023-06-21 09:33:32+00:00,,1,3,oncall: distributed module: fsdp,False
103955,when train with multi GPUS oncall: distributed,2023-06-21 08:07:16+00:00,,0,3,oncall: distributed,False
103949,torch.save() fails if path contains multibyte characters module: serialization triaged,2023-06-21 06:25:05+00:00,,0,0,module: serialization triaged,True
103947,"[torch.fx] Deserialization Error - TypeError: ones() received an invalid combination of arguments - got (tuple, device=Attribute)  high priority triage review module: fx oncall: fx",2023-06-21 06:07:37+00:00,,0,2,high priority triage review module: fx oncall: fx,True
103913,[dynamo] functools.wraps : graph-break when wrapping nested functions. triaged oncall: pt2 module: dynamo,2023-06-20 19:20:52+00:00,,1,3,triaged oncall: pt2 module: dynamo,False
103895,Issue with loading similar checkpoints in a distributed fashion  module: serialization triaged,2023-06-20 16:35:31+00:00,,0,2,module: serialization triaged,True
103891,Docker images: faster linker for `torch.compile` triaged module: docker oncall: pt2,2023-06-20 15:40:51+00:00,,0,4,triaged module: docker oncall: pt2,False
103883,Runtime Error outerNode->outputs().size() == node->inputs().size() INTERNAL ASSERT FAILED when exporting custom operator module: onnx triaged,2023-06-20 13:12:57+00:00,,0,2,module: onnx triaged,True
103879,Can ``torch.vmap`` add ``grad_fn``= SelectBackward when maping over some dimension of the inputs? triaged actionable module: functorch,2023-06-20 11:57:06+00:00,,0,5,triaged actionable module: functorch,True
103868,"row.device().is_cpu() INTERNAL ASSERT FAILED at ""csrc/cpu/diag_cpu.cpp"":7 triaged module: macos",2023-06-19 23:16:10+00:00,,0,1,triaged module: macos,True
103863,DISABLED test_create_chunk_dtensor (__main__.TestShardUtilsDistributedDTensor) oncall: distributed triaged skipped,2023-06-19 20:43:44+00:00,,0,1,oncall: distributed triaged skipped,False
103860,"Long PR description leads to ""Argument list too long"" error from docker module: ci triaged module: devx",2023-06-19 19:01:35+00:00,,0,7,module: ci triaged module: devx,True
103857,[ONNX] FX exporter: replace `aten::copy_` with out-place version module: onnx triaged,2023-06-19 18:41:03+00:00,,1,10,module: onnx triaged,False
103852,Segmentation fault when tensorrt is imported before torch needs reproduction module: build triaged has workaround,2023-06-19 15:26:56+00:00,,0,1,needs reproduction module: build triaged has workaround,True
103848,torch compile aten::floor_divide error triaged oncall: pt2,2023-06-19 14:21:31+00:00,,0,1,triaged oncall: pt2,True
103847,Some parameters are missing type descriptions module: docs triaged actionable,2023-06-19 14:18:17+00:00,,0,0,module: docs triaged actionable,True
103846,"The document style is inconsistent with other documents, and the parameter type is not clearly highlight module: docs triaged actionable",2023-06-19 14:04:03+00:00,,0,0,module: docs triaged actionable,True
103844,Missing examples in some API docs module: docs triaged actionable,2023-06-19 13:50:29+00:00,,0,2,module: docs triaged actionable,False
103841,[question] [docs] Short/mid/long-term status of TorchScript / JIT / torch.jit.trace / FX / symbolic tracing and its replacement by Dynamo oncall: jit module: docs triaged,2023-06-19 13:13:34+00:00,,0,4,oncall: jit module: docs triaged,True
103840,Gradient operations (zero_grad and gradient accumulations) as graphs feature module: autograd triaged oncall: pt2 module: pt2-dispatcher,2023-06-19 13:12:38+00:00,,0,15,feature module: autograd triaged oncall: pt2 module: pt2-dispatcher,False
103837,type conflict module: cpp triaged,2023-06-19 11:38:41+00:00,,0,0,module: cpp triaged,True
103836,Please consider the SCFA/dynamic flash attention for your implementation of scaled dot product attention module: sparse triaged oncall: transformer/mha topic: new features,2023-06-19 11:06:27+00:00,,0,1,module: sparse triaged oncall: transformer/mha topic: new features,False
103832,Torch 1.13 for GPU breaks if libcublas is already present. triaged module: docker,2023-06-19 08:58:16+00:00,,0,0,triaged module: docker,True
103831,[dynamo] AssertionError for custom iterable nn.Module triaged oncall: pt2,2023-06-19 08:52:02+00:00,,1,1,triaged oncall: pt2,False
103829,RPC Framework support custom backend oncall: distributed triaged,2023-06-19 08:01:21+00:00,,0,0,oncall: distributed triaged,True
103820,Upgrading SpGEMM algorithm to resolve Cusparse SpGEMM insufficient resources problem module: sparse module: cuda triaged,2023-06-19 02:55:26+00:00,,0,1,module: sparse module: cuda triaged,True
103814,"abnormal behavior in function ""scatter"" triaged module: python frontend",2023-06-18 15:52:45+00:00,,0,1,triaged module: python frontend,True
103805,Error when building with USE_TENSORRT=1 module: onnx module: build triaged,2023-06-17 16:19:47+00:00,,0,2,module: onnx module: build triaged,True
103803,Support `Sequence` type in JIT oncall: jit module: typing triaged enhancement,2023-06-17 15:29:31+00:00,,0,4,oncall: jit module: typing triaged enhancement,True
103802,Eager PTDQ Performs Worse Than Non-Quantized Linear Layer on CPU(in Terms of Speed) triaged,2023-06-17 15:25:30+00:00,,1,4,triaged,True
103800,Mis-annotated return for `F._no_grad_embedding_renorm_` (also JIT related) oncall: jit module: typing triaged bug,2023-06-17 11:33:55+00:00,,0,0,oncall: jit module: typing triaged bug,True
103798,Type misalignments in `nn.functional` (also JIT related) oncall: jit module: typing triaged,2023-06-17 11:05:51+00:00,,0,0,oncall: jit module: typing triaged,True
103788,"[Torch Mlir] avg_pool1d function padding init value should be (0,) triage review oncall: jit",2023-06-16 23:39:41+00:00,,0,1,triage review oncall: jit,True
103787,Generate complete annotations for `torch._C._nn` module: typing triaged enhancement release notes: devx,2023-06-16 23:36:03+00:00,,0,0,module: typing triaged enhancement release notes: devx,True
103785,[PT2] Return int32 indices in max_pool2d_with_indices triaged oncall: pt2,2023-06-16 22:54:54+00:00,,1,2,triaged oncall: pt2,False
103764,[ONNX] Handle absence of `onnxscript` module in PyTorch requirements.txt module: onnx triaged enhancement,2023-06-16 19:36:53+00:00,,1,0,module: onnx triaged enhancement,False
103761,Merge type stubs for `torch.nn.functional` module: typing triaged,2023-06-16 18:21:59+00:00,,0,3,module: typing triaged,True
103760,dlrm and hf_T5_generate fails aot_eager with bfloat16+dynamic_shapes triaged oncall: pt2 module: dynamic shapes,2023-06-16 17:31:53+00:00,,1,4,triaged oncall: pt2 module: dynamic shapes,True
103756,libtorch > 1.9.1 produces segfault on Qt5 gui application exit high priority module: crash module: cpp triaged,2023-06-16 16:00:17+00:00,,2,9,high priority module: crash module: cpp triaged,True
103752,Pytorch not calling to C code from a docker container needs reproduction triaged module: docker module: __torch_function__,2023-06-16 15:32:09+00:00,,0,6,needs reproduction triaged module: docker module: __torch_function__,True
103749,SDPA produces NaN with padding mask triage review triaged oncall: transformer/mha,2023-06-16 13:59:29+00:00,,1,32,triage review triaged oncall: transformer/mha,True
103737,[FSDP] train throughput become slow down when loaded shard optimizer dict triaged module: fsdp,2023-06-16 04:41:36+00:00,,2,5,triaged module: fsdp,True
103730,[FSDP] save model checkpoint with StateDictType.LOCAL_STATE_DICT and LocalStateDictConfig(offload_to_cpu=True) fail triaged module: fsdp,2023-06-16 02:47:30+00:00,,1,3,triaged module: fsdp,True
103727,torch.compile() bug in AOTAutograd or Dynamo triaged oncall: pt2,2023-06-16 01:51:06+00:00,,1,2,triaged oncall: pt2,True
103719,DataParallel interfering with TorchDispatchMode high priority oncall: distributed triaged module: data parallel module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,2023-06-15 23:40:53+00:00,,0,9,high priority oncall: distributed triaged module: data parallel module: __torch_dispatch__ oncall: pt2 module: pt2-dispatcher,True
103716,Non actionable perf hint: reduction over non-contiguous dims feature triaged enhancement oncall: pt2 module: inductor,2023-06-15 23:14:13+00:00,,0,3,feature triaged enhancement oncall: pt2 module: inductor,False
103713,[ONNX] Discuss improvements to Diagnostic public API module: onnx triaged,2023-06-15 22:26:46+00:00,,1,6,module: onnx triaged,False
103706,TorchDynamo assertion with `try: return; finally` triaged oncall: pt2 release notes: dynamo,2023-06-15 21:20:30+00:00,,1,1,triaged oncall: pt2 release notes: dynamo,False
103683,fairseq distributed training dumps core with flash attention triaged oncall: transformer/mha,2023-06-15 16:39:38+00:00,,0,0,triaged oncall: transformer/mha,True
103682,(fsdp) Support for accessing unsharded parameters for methods other than `forward()` triaged module: fsdp,2023-06-15 16:38:12+00:00,,1,5,triaged module: fsdp,True
103681,Exported model with dropout incorrectly applies dropout during eval module: nn triaged module: correctness (silent) oncall: pt2 oncall: export,2023-06-15 16:31:01+00:00,,0,14,module: nn triaged module: correctness (silent) oncall: pt2 oncall: export,True
103672,detectron2_fcos_r_50_fpn and other models have enough graph breaks that we end up with multiple cache entries on module blocks triaged module: dynamic shapes,2023-06-15 13:26:50+00:00,,0,1,triaged module: dynamic shapes,True
103671,"""Y.getIntrusivePtr()->set_storage(X.getIntrusivePtr()->storage()); "" in C++ is not supported module: cpp triaged",2023-06-15 13:17:59+00:00,,0,0,module: cpp triaged,True
103668,MultiheadAttention should split embed_dim into four parameters triaged oncall: transformer/mha,2023-06-15 11:16:01+00:00,,0,3,triaged oncall: transformer/mha,True
103652,inductor: support horizontal reduction with vec_transpose to improve TIMM swin_base_patch4_window7_224  dynamic shape performance  triaged oncall: pt2 module: inductor module: cpu inductor,2023-06-15 05:15:10+00:00,,2,0,triaged oncall: pt2 module: inductor module: cpu inductor,False
103650,AOT autograd: Avoid dependency on strides for manual regeneration of outputs that are aliased to inputs triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-06-15 04:33:24+00:00,,0,13,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
103643,"""addmm_out_sparse_csr_impl_mkl"" not implemented for 'Byte' module: sparse triaged",2023-06-15 01:40:00+00:00,,0,3,module: sparse triaged,True
103640,Disclose C++ ATen ops type promotion rules under OpOverload in Python module: docs triaged,2023-06-15 01:26:46+00:00,,0,1,module: docs triaged,True
103626,2D  model checkpointing hangs on a ViT model oncall: distributed,2023-06-14 21:47:44+00:00,,1,0,oncall: distributed,False
103625,DISABLED test_backward_ddp_outside (__main__.TensorPipeDdpUnderDistAutogradTest) oncall: distributed module: flaky-tests skipped,2023-06-14 21:39:29+00:00,,0,1,oncall: distributed module: flaky-tests skipped,False
103622,ARM based GPU support for Distributed Data Parallelism Module oncall: distributed,2023-06-14 20:18:58+00:00,,0,0,oncall: distributed,True
103619,torch._dynamo.exc.InternalTorchDynamoError: SymNodeVariable() is not a constant on DynamicShapesMiscTests.test_slice_input triaged oncall: pt2 module: dynamic shapes,2023-06-14 19:50:54+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes,False
103615,Can't call allow_in_graph inside of a function being torch.compile'd  triaged oncall: pt2,2023-06-14 19:22:04+00:00,,1,1,triaged oncall: pt2,False
103608,Installing Torch on AMD Platform Leads to Huge Docker Image module: rocm triaged,2023-06-14 17:47:16+00:00,,0,2,module: rocm triaged,True
103602,test_fstrings2 fails with dynamic good first issue triaged module: dynamic shapes,2023-06-14 17:12:50+00:00,,0,10,good first issue triaged module: dynamic shapes,True
103589,`interpolate` with `antialias=True` on CUDA doesn't work if the difference of spatial size is large module: cuda triaged module: interpolation,2023-06-14 12:57:48+00:00,,0,7,module: cuda triaged module: interpolation,True
103588,LSTM/RNN operation agnostic module: nn triaged needs research,2023-06-14 12:22:14+00:00,,1,2,module: nn triaged needs research,True
103584,torch.cuda.mem_get_info to return 0 if CUDA context isn't initialized module: cuda triaged actionable,2023-06-14 08:09:55+00:00,,0,3,module: cuda triaged actionable,True
103581,Passing dict in datapipe/dataset will have memory leak problem triaged module: data,2023-06-14 06:34:55+00:00,,0,3,triaged module: data,True
103580,Support ByteTensor and ShortTensor for nn.Embedding and nn.EmbeddingBag module: nn triaged enhancement actionable topic: improvements,2023-06-14 05:24:03+00:00,,1,6,module: nn triaged enhancement actionable topic: improvements,False
103578,"ImportError: undefined symbol: cublasSetWorkspace_v2, version libcublas.so.11 module: binaries module: cuda triaged",2023-06-14 04:47:23+00:00,,0,0,module: binaries module: cuda triaged,True
103573,[ONNX] Support aten::mT module: onnx low priority triaged OSS contribution wanted,2023-06-14 03:18:56+00:00,,0,2,module: onnx low priority triaged OSS contribution wanted,True
103572,[ONNX] Support aten::linalg_solve_triangular module: onnx triaged,2023-06-14 03:17:48+00:00,,0,0,module: onnx triaged,False
103571,[ONNX] Support aten::linalg_cholesky_ex module: onnx triaged,2023-06-14 03:17:07+00:00,,0,0,module: onnx triaged,False
103570,File Missing When i build with C++ module: cpp triaged,2023-06-14 03:12:03+00:00,,0,6,module: cpp triaged,True
103553,Request: flag to know model is compiled after torch.compile() triaged enhancement oncall: pt2,2023-06-13 22:14:17+00:00,,0,0,triaged enhancement oncall: pt2,False
103552,Inject detailed NVTX markers into the Inductor Triton generated kernels feature triaged oncall: pt2 module: inductor,2023-06-13 21:57:57+00:00,,1,0,feature triaged oncall: pt2 module: inductor,False
103539,torch.fx.passes.split_module.split_module doesn't support dynamic shapes good first issue triaged module: dynamic shapes,2023-06-13 20:18:29+00:00,,0,2,good first issue triaged module: dynamic shapes,True
103530,Deduplicate the operands passed into torch.cond after dynamo tracing. triaged,2023-06-13 18:05:08+00:00,,1,3,triaged,False
103518,`gradcheck` produces false positives with sparse inputs when `masked=False`. module: sparse module: autograd triaged,2023-06-13 14:30:16+00:00,,0,14,module: sparse module: autograd triaged,True
103505,"[functorch] [FakeTensorMode, meta tensor] + aot_autograd Bug. triaged oncall: pt2 module: fakeTensor module: aotdispatch module: pt2-dispatcher",2023-06-13 12:28:56+00:00,,1,7,triaged oncall: pt2 module: fakeTensor module: aotdispatch module: pt2-dispatcher,True
103499,CUBLAS_WORKSPACE_CONFIG can not be parsed triaged module: cublas,2023-06-13 10:22:22+00:00,,0,2,triaged module: cublas,True
103498,ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6047) of binary: /home/win10-ubuntu/anaconda3/envs/vicuna-7b/bin/python oncall: distributed triaged,2023-06-13 09:50:22+00:00,,0,2,oncall: distributed triaged,True
103495,DISABLED test_mem_get_info (__main__.TestCuda) module: cuda triaged module: flaky-tests skipped,2023-06-13 06:40:18+00:00,,0,3,module: cuda triaged module: flaky-tests skipped,False
103484,No backward implementation for `torch._native_multi_head_attention` triaged module: multi-headed-attention,2023-06-13 03:27:11+00:00,,0,2,triaged module: multi-headed-attention,True
103483,torch._dynamo.exc.Unsupported: Tensor.backward with aten_graph=True triaged oncall: pt2 oncall: export,2023-06-13 03:17:38+00:00,,1,1,triaged oncall: pt2 oncall: export,False
103482,Document CI retry rules triaged module: devx,2023-06-13 02:39:38+00:00,,1,2,triaged module: devx,True
103475,[Inductor] Optimize More Cases of Int32 -> Int64  feature triaged enhancement module: inductor,2023-06-13 01:14:05+00:00,,0,2,feature triaged enhancement module: inductor,False
103473,Error encountered when tracing model with Dynamo/Functorch for export with trilinear interpolation triaged oncall: pt2 module: dynamic shapes oncall: export,2023-06-13 00:38:57+00:00,,0,3,triaged oncall: pt2 module: dynamic shapes oncall: export,True
103462,Memory efficient SDP yields wrong gradients triaged module: multi-headed-attention,2023-06-12 21:58:31+00:00,,0,5,triaged module: multi-headed-attention,True
103449,Asynchronous CUDA AveragedModel module: optimizer triaged needs research,2023-06-12 18:59:18+00:00,,0,3,module: optimizer triaged needs research,True
103444,Deprecation warning on lr_scheduler.step(num_steps) module: optimizer triaged actionable,2023-06-12 18:27:18+00:00,,0,4,module: optimizer triaged actionable,True
103439,test_generate_tensor_from_list_of_numpy_primitive_type fails if run under pytest triaged module: dynamic shapes,2023-06-12 15:37:45+00:00,,0,0,triaged module: dynamic shapes,True
103425,The document does not emphasize Illegal value in nn.Bilinear module: nn triaged actionable module: edge cases,2023-06-12 10:26:21+00:00,,0,2,module: nn triaged actionable module: edge cases,True
103424,The document does not emphasize hidden range in nn.Embedding needs reproduction module: docs triaged,2023-06-12 10:18:54+00:00,,0,2,needs reproduction module: docs triaged,False
103423,The document does not emphasize hidden range in nn.MaxPool2d needs reproduction module: docs triaged,2023-06-12 10:17:46+00:00,,0,2,needs reproduction module: docs triaged,False
103422,Possible memory leak when using Torch and Torchvision in conjunction with XGBoost  module: memory usage triaged module: vision,2023-06-12 10:06:36+00:00,,0,4,module: memory usage triaged module: vision,True
103417,"Torch  model compile error  ""/usr/bin/ld: cannot find -lcuda""  though cuda is installed via run file triaged oncall: pt2 upstream triton",2023-06-12 08:08:53+00:00,,0,7,triaged oncall: pt2 upstream triton,True
103415,[inductor][cpp_wrapper] Support rand fallback triaged oncall: pt2,2023-06-12 06:52:57+00:00,,1,1,triaged oncall: pt2,False
103397,LayerNorm freeze processes using torch multiprocessing module: multiprocessing triaged,2023-06-11 20:55:04+00:00,,0,2,module: multiprocessing triaged,True
103393,Typing missing on arithmetic ops on `Tensor` module: typing triaged,2023-06-11 16:50:19+00:00,,0,0,module: typing triaged,True
103382,NotImplementedError Could not run 'c10d::alltoall_' with arguments from the 'Meta' backend.  triaged,2023-06-11 03:33:30+00:00,,0,1,triaged,True
103375,Inplace binary ops on tensor subclasses can cause mypy error module: typing triaged,2023-06-11 00:11:22+00:00,,0,0,module: typing triaged,True
103372,ImportError: cannot import name 'Store' from 'torch.distributed' oncall: distributed triaged,2023-06-10 18:28:56+00:00,,0,1,oncall: distributed triaged,True
103370,torchgen/gen_backend_stubs.py compatibility with DispatchStubs triaged module: dispatch module: codegen module: structured kernels,2023-06-10 14:21:00+00:00,,0,1,triaged module: dispatch module: codegen module: structured kernels,True
103369,test_workspace_allocation_error fails on my local devgpu triaged module: cuda graphs,2023-06-10 13:37:56+00:00,,0,3,triaged module: cuda graphs,True
103367,RuntimeError: CUDA error: unknown error oncall: distributed triaged module: fsdp,2023-06-10 12:51:42+00:00,,1,6,oncall: distributed triaged module: fsdp,True
103359,Libtorch compile error when defining D_GLIBCXX_DEBUG module: build module: abi triaged,2023-06-10 01:04:55+00:00,,0,0,module: build module: abi triaged,True
103354,Add a requirements.txt for windows pip packages triaged module: devx,2023-06-09 22:41:58+00:00,,1,0,triaged module: devx,True
103352,"[feature request] Native method for iterating Python items of tensors: `iteritems()` and a new `tensor.item(i, j, k, ...)` method feature triaged module: python frontend",2023-06-09 22:09:56+00:00,,0,8,feature triaged module: python frontend,True
103343,mps and cpu give far different results when training a transformer. triaged module: mps,2023-06-09 20:55:01+00:00,,0,4,triaged module: mps,True
103336,"python test/inductor/test_split_cat_fx_passes.py -k test_consecutive_split_merge fails, but running all tests together succeeds triaged oncall: pt2",2023-06-09 18:16:25+00:00,,0,0,triaged oncall: pt2,False
103332,Improve `_group_tensors_by_device_and_dtype` module: optimizer triaged better-engineering actionable module: mta,2023-06-09 16:02:55+00:00,,0,0,module: optimizer triaged better-engineering actionable module: mta,True
103329,"RuntimeError: torch.vmap a function that includes in-place arithmetic operations on a zero-initialized tensor, an error ""vmap: inplace arithmetic(self, *extra_args) is not possible"" is raised. triaged module: functorch",2023-06-09 15:33:50+00:00,,0,4,triaged module: functorch,True
103322,Disabling ALL TestOptim on the dynamo config module: optimizer triaged skipped module: dynamo,2023-06-09 14:17:22+00:00,,0,6,module: optimizer triaged skipped module: dynamo,False
103318,Custom autograd function causes a graph break triaged oncall: pt2,2023-06-09 11:25:42+00:00,,0,4,triaged oncall: pt2,False
103316,binary_cross_entropy (loss) seems to be giving incorrect values for very negative logits module: nn triaged,2023-06-09 09:58:06+00:00,,0,5,module: nn triaged,True
103313,Fast kernels for low rank matrix multiplication triaged module: linear algebra,2023-06-09 08:44:55+00:00,,0,11,triaged module: linear algebra,True
103312,setup.py fails to pass USE_ROCM to CAFFE2 build  module: rocm triaged,2023-06-09 07:54:23+00:00,,0,1,module: rocm triaged,True
103306,DTensor uneven sharding corner cases. oncall: distributed triaged,2023-06-09 04:29:09+00:00,,1,0,oncall: distributed triaged,True
103305,distributed.gather shape constraints oncall: distributed triaged topic: docs,2023-06-09 04:08:03+00:00,,0,1,oncall: distributed triaged topic: docs,False
103276,Dynamo trouble shooting dead link good first issue triaged topic: docs oncall: pt2 module: dynamo,2023-06-08 22:22:52+00:00,,0,5,good first issue triaged topic: docs oncall: pt2 module: dynamo,True
103272,oneDNN kernel fails to compile triaged module: mkldnn,2023-06-08 21:49:11+00:00,,0,4,triaged module: mkldnn,True
103258,"Warn / deprecate / remove ProcessGroupNCCL._group_start(), _group_end() APIs oncall: distributed triaged",2023-06-08 18:37:29+00:00,,0,3,oncall: distributed triaged,True
103254,Unexpected High PCIe traffic in Distributed Training since PT 2 oncall: distributed triaged module: fsdp,2023-06-08 17:05:25+00:00,,0,27,oncall: distributed triaged module: fsdp,True
103250,torch.jit.script mean(keepdim=True) segfaults on GPU oncall: jit,2023-06-08 14:37:19+00:00,,0,0,oncall: jit,True
103243,torch.cuda.memory_reserved always returns 0 bytes module: cuda triaged,2023-06-08 09:23:35+00:00,,0,1,module: cuda triaged,True
103241,Image Processing with Pytorch triaged,2023-06-08 08:18:33+00:00,,0,1,triaged,False
103231,Benchmark --quick with huggingface runs almost indefinitely on CPU triaged oncall: pt2,2023-06-08 04:56:21+00:00,,1,3,triaged oncall: pt2,False
103222,compilation fails `error: invalid argument '-std=c++17' not allowed with 'C'` module: build triaged,2023-06-08 00:46:33+00:00,,0,1,module: build triaged,True
103221,[help] did torch.distributed.launch can be applied on k8s cluster with pytorch-operator oncall: distributed module: elastic,2023-06-08 00:44:12+00:00,,0,2,oncall: distributed module: elastic,False
103213,Undeterministic behavior in testing in dynamo. triaged oncall: pt2 module: dynamo,2023-06-07 22:48:28+00:00,,2,4,triaged oncall: pt2 module: dynamo,False
103212,PyTorch can not be compiled with MKLDNN if system compiler is clang module: build triaged module: mkldnn,2023-06-07 22:45:41+00:00,,1,2,module: build triaged module: mkldnn,True
103194,[inductor] test_fft_real_inputs fails with dynamic shapes good first issue triaged oncall: pt2 module: dynamic shapes module: inductor,2023-06-07 20:08:07+00:00,,0,2,good first issue triaged oncall: pt2 module: dynamic shapes module: inductor,True
103189,(fsdp - maybe a bug) SHARDED_STATE_DICT returns tensor with no data triaged module: fsdp,2023-06-07 19:34:51+00:00,,1,1,triaged module: fsdp,True
103173,[RFC] Emit better Telemetry in PyTorch feature module: logging triaged oncall: pt2,2023-06-07 18:27:17+00:00,,0,8,feature module: logging triaged oncall: pt2,False
103169,breakpoint() in torch.compile region behaves oddly triaged oncall: pt2,2023-06-07 18:21:50+00:00,,0,0,triaged oncall: pt2,False
103161,Calling jacrev with LSTM and functional_call  gives error triaged module: functorch,2023-06-07 17:18:19+00:00,,1,5,triaged module: functorch,True
103160,Allow overriding __repr__ to call dataclass_repr (infinite recursion right now) triaged better-engineering module: codegen,2023-06-07 17:09:23+00:00,,0,0,triaged better-engineering module: codegen,True
103150,Build fails at linking torch_shm_manager on aarch64 module: build triaged,2023-06-07 12:38:03+00:00,,0,1,module: build triaged,True
103131,"error: ‘aligned_alloc’ was not declared in this scope        static_cast<char*>(aligned_alloc(FLATBUFFERS_MAX_ALIGNMENT, size)), free); module: build triaged",2023-06-07 02:12:21+00:00,,0,1,module: build triaged,True
103117,Observed regress in DataLoader spawn from PyTorch1.13 to PyTorch2.0 high priority module: performance triaged module: regression module: data,2023-06-06 23:59:48+00:00,,1,6,high priority module: performance triaged module: regression module: data,True
103111,Turn on Inductor Max Pool2d Backward Lowering For Channels Last feature good first issue triaged oncall: pt2 module: inductor,2023-06-06 21:38:52+00:00,,1,0,feature good first issue triaged oncall: pt2 module: inductor,True
103109,Increased / more verbose type aliases for improved readability of user defined content module: typing triaged enhancement needs research,2023-06-06 21:25:17+00:00,,0,7,module: typing triaged enhancement needs research,True
103104,PyTorch should not use `windows.8xlarge.nvidia.gpu` to test binary builds module: ci triaged,2023-06-06 19:59:39+00:00,,0,0,module: ci triaged,True
103101,Refactor mm_plus_mm to check conditions upfront feature good first issue triaged oncall: pt2 module: inductor,2023-06-06 19:38:21+00:00,,0,2,feature good first issue triaged oncall: pt2 module: inductor,True
103099,torch.compile specializes on output name triaged oncall: pt2,2023-06-06 18:47:59+00:00,,1,3,triaged oncall: pt2,False
103093,Inconsistent memory allocation using FSDP between PT 2.0 and Nightlies high priority triage review oncall: distributed triaged module: fsdp,2023-06-06 17:15:47+00:00,,0,5,high priority triage review oncall: distributed triaged module: fsdp,True
103089,"[OOM] Unable to convert 30B model to ONNX, using 4x A100's module: onnx triaged",2023-06-06 15:39:17+00:00,,0,0,module: onnx triaged,False
103082,Ambiguitiy in causal-mask in scaled_dot_product_attention oncall: transformer/mha,2023-06-06 13:45:10+00:00,,0,0,oncall: transformer/mha,False
103073,torch.compile crash for tensor computing when tensor size is bigger  needs reproduction triaged oncall: pt2,2023-06-06 10:18:49+00:00,,0,5,needs reproduction triaged oncall: pt2,True
103070,Unexpected failure in LLVM JIT when running TorchScript model in C++ oncall: jit,2023-06-06 07:41:20+00:00,,0,3,oncall: jit,True
103060,Symbolic trace error about torch.nn.functional.pad triaged module: fx,2023-06-06 04:19:11+00:00,,0,1,triaged module: fx,True
103056,[Pytorch 2.0] torch::nn::Dropout output is incorrect on Windows   module: binaries module: windows module: cpp triaged module: regression,2023-06-06 03:19:56+00:00,,1,1,module: binaries module: windows module: cpp triaged module: regression,True
103055,"lit-llama lora fine tuning NetworkXUnbounded: Infinite capacity path, flow unbounded above triaged oncall: pt2 module: dynamic shapes",2023-06-06 03:10:21+00:00,,0,5,triaged oncall: pt2 module: dynamic shapes,True
103023,MPS bug: padding_idx in nn.Embedding does not prevent gradient accumulation  triaged module: mps,2023-06-05 21:35:03+00:00,,0,1,triaged module: mps,True
102999,Preserve weight_g/weight_v accessors on new weight_norm module: nn triaged module: nn.utils.parametrize,2023-06-05 19:20:14+00:00,,0,14,module: nn triaged module: nn.utils.parametrize,True
102977,raise `RuntimeError` faster when loading an object with a torch CUDA tensor on a CPU-only machine module: nn module: serialization triaged actionable,2023-06-05 15:51:44+00:00,,1,2,module: nn module: serialization triaged actionable,True
102971,Discussion and Design  for Masked Loss Functions which can be used with PackedSequence training (but not exclusively) module: loss triaged module: masked operators,2023-06-05 14:17:11+00:00,,0,6,module: loss triaged module: masked operators,True
102966,"how to workaround the error ""don't have an op for vulkan_prepack::create_linear_context"" ? module: build triaged module: vulkan ciflow/periodic",2023-06-05 09:53:28+00:00,,0,2,module: build triaged module: vulkan ciflow/periodic,True
102963,torch.svd fails on large matrices module: cuda triaged module: cublas module: linear algebra,2023-06-05 08:47:20+00:00,,0,1,module: cuda triaged module: cublas module: linear algebra,True
102953,TypeError: (): incompatible function arguments oncall: distributed triaged,2023-06-05 06:17:13+00:00,,1,2,oncall: distributed triaged,True
102948,[onnx] aten::cumprod cannot be exported to ONNX module: onnx triaged OSS contribution wanted,2023-06-05 03:16:15+00:00,,0,1,module: onnx triaged OSS contribution wanted,False
102947,"torch.onnx.export error ------RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select) module: onnx triaged",2023-06-05 02:45:17+00:00,,0,2,module: onnx triaged,True
102938,Support for efficiently processing categorical distributions with varying dimensions module: distributions feature triaged needs research,2023-06-04 20:50:26+00:00,,0,0,module: distributions feature triaged needs research,True
102936,torch.cuda.is_available() returns False on GTX 1650 with cuda 11.7 and torch==2.0.0+cpu module: build triaged,2023-06-04 16:17:09+00:00,,0,7,module: build triaged,True
102921,Unbox expectedml triaged,2023-06-04 01:46:52+00:00,,0,2,triaged,True
102911,PackedSequences on MPS accelerator yields `grad_y` missing or crashes the kernel. triaged module: mps,2023-06-03 18:27:22+00:00,,0,7,triaged module: mps,True
102905,nn.ChannelShuffle1d module: nn triaged needs research,2023-06-03 06:30:53+00:00,,1,2,module: nn triaged needs research,True
102904,Unable to checkpoint model and optimizer state when using Hybrid Sharding Strategy high priority oncall: distributed triaged module: fsdp module: distributed_checkpoint,2023-06-03 05:54:31+00:00,,0,1,high priority oncall: distributed triaged module: fsdp module: distributed_checkpoint,True
102898,After dynamo minifier generates repros that don't entirely match what we minified over triaged module: dynamo,2023-06-03 00:55:41+00:00,,0,0,triaged module: dynamo,False
102894,"BCELoss and BCEWithLogitsLoss differ when one of the input logits is float(""inf"") module: nn triaged module: edge cases",2023-06-02 23:51:09+00:00,,1,0,module: nn triaged module: edge cases,True
102878,[dynamo] Diffusers - Graph break on OrderedDict  triaged module: dynamo,2023-06-02 21:06:03+00:00,,1,8,triaged module: dynamo,False
102870,Inductor: delete code that extracts out sizevars by inspecting tensor inputs to find a size that handled it triaged better-engineering module: inductor,2023-06-02 19:14:27+00:00,,0,1,triaged better-engineering module: inductor,True
102853,pdb but for dynamo (and time travel debugging) feature triaged oncall: pt2,2023-06-02 17:44:39+00:00,,0,0,feature triaged oncall: pt2,False
102852,The operator 'aten::poisson' is not currently implemented for the MPS device triaged module: mps,2023-06-02 17:32:27+00:00,,0,0,triaged module: mps,False
102839,Dynamo should only unroll loops by a preset factor (unless otherwise explicitly instructed) triaged oncall: pt2,2023-06-02 13:00:48+00:00,,2,2,triaged oncall: pt2,False
102833,LibTorch-Lite 1.13.0.1 Crash on iOS 12 on app startup triaged module: ios,2023-06-02 10:27:53+00:00,,0,1,triaged module: ios,True
102832,TypeError: (): incompatible function arguments needs reproduction module: cpp-extensions triaged,2023-06-02 09:02:01+00:00,,0,7,needs reproduction module: cpp-extensions triaged,False
102830,Unknow error when using `make_graphed_callables` triaged module: cuda graphs,2023-06-02 07:43:17+00:00,,0,2,triaged module: cuda graphs,True
102821,"Unable to resume job using FSDP with 64 nodes, errors appeared during loading sharded optimizer state dict  triaged module: fsdp",2023-06-02 06:17:56+00:00,,1,11,triaged module: fsdp,True
102814,mark_dynamic may error too aggressively triaged oncall: pt2 module: dynamic shapes,2023-06-02 03:29:34+00:00,,0,2,triaged oncall: pt2 module: dynamic shapes,True
102812,[DTensor] Error in distribute_module with module._apply oncall: distributed triaged,2023-06-02 03:21:40+00:00,,1,0,oncall: distributed triaged,True
102811,`torch.poisson(torch.tensor([torch.inf))` returns 0 module: distributions module: error checking triaged module: edge cases,2023-06-02 03:11:05+00:00,,0,3,module: distributions module: error checking triaged module: edge cases,True
102805,Do smarter layout decisions with concatenate. feature triaged oncall: pt2 module: inductor,2023-06-02 01:51:24+00:00,,0,2,feature triaged oncall: pt2 module: inductor,False
102803,Improve shape padding in training. module: performance feature triaged oncall: pt2 module: inductor,2023-06-02 01:42:29+00:00,,0,0,module: performance feature triaged oncall: pt2 module: inductor,False
102775,DISABLED test_make_fx_symbolic_exhaustive_special_bessel_j0_cpu_float32 (__main__.TestProxyTensorOpInfoCPU) triaged module: flaky-tests skipped module: ProxyTensor,2023-06-01 21:39:31+00:00,,0,138,triaged module: flaky-tests skipped module: ProxyTensor,False
102772,Support In-place Triangular Matrix Multiplication triaged enhancement module: linear algebra,2023-06-01 20:48:35+00:00,,0,3,triaged enhancement module: linear algebra,True
102763,Followup on the extra graph breaks for yolov3 model caused by layout optimization triaged module: inductor,2023-06-01 18:40:09+00:00,,1,3,triaged module: inductor,True
102761,Pytorch Build images for RISCV64 Devices in the nightly builds module: build feature triaged,2023-06-01 18:21:32+00:00,,0,0,module: build feature triaged,True
102753,Error: no matching constructor for initialization of 'at::OptionalIntArrayRef' module: build triaged module: macos,2023-06-01 17:22:11+00:00,,0,1,module: build triaged module: macos,True
102751,DISABLED test_ddp_has_finalized (__main__.TestDistBackendWithSpawn) triaged skipped,2023-06-01 16:41:02+00:00,,0,2,triaged skipped,False
102741,Unlocking PyTorch's Power: README.md in Multiple Languages!  triaged topic: new features topic: docs,2023-06-01 15:24:45+00:00,,0,3,triaged topic: new features topic: docs,False
102740,parameterizations.orthogonal does not work as intended with nn.GRU or nn.LSTM module: rnn triaged module: nn.utils.parametrize,2023-06-01 15:24:36+00:00,,0,5,module: rnn triaged module: nn.utils.parametrize,True
102732,Building NCCL with `make -l $MAX_JOBS` slows down builds module: build triaged,2023-06-01 14:40:30+00:00,,1,1,module: build triaged,True
102731,"[FSDP]  When amp is enabled, there is a noticeable difference during training between `FSDP `and `DDP` triaged module: fsdp",2023-06-01 14:01:57+00:00,,1,1,triaged module: fsdp,True
102730,Best practices clarification for initialization strategies module: nn triaged module: initialization needs research module: python frontend,2023-06-01 13:54:50+00:00,,1,2,module: nn triaged module: initialization needs research module: python frontend,True
102727,DISABLED test_Conv2d_dilated_cuda_tf32 (__main__.TestNN) module: nn module: cuda triaged module: flaky-tests skipped,2023-06-01 12:46:40+00:00,,0,9,module: nn module: cuda triaged module: flaky-tests skipped,False
102694,Exporting the operator \\'aten::fused_moving_avg_obs_fake_quant\\' to ONNX opset version 13  is not supported module: onnx triaged,2023-06-01 02:37:46+00:00,,0,0,module: onnx triaged,False
102673,Fix dynamo-related debug Python 3.11 failures triaged bug release notes: dynamo,2023-06-01 00:06:24+00:00,,1,3,triaged bug release notes: dynamo,True
102670,Investigate the perf drop on timm for dynamic shape when layout optimization is enabled module: performance feature triaged oncall: pt2 module: inductor,2023-05-31 23:40:38+00:00,,0,3,module: performance feature triaged oncall: pt2 module: inductor,False
102667,Duplicate parameters (_flat_params and original params) in the state_dict when using `use_orig_params=True` and `StateDictType.LOCAL_STATE_DICT` triaged module: fsdp,2023-05-31 23:02:44+00:00,,1,0,triaged module: fsdp,True
102663,Test test_vjp_nn_functional_scaled_dot_product_attention_cuda_float32 fails with `query: last dimension must be contiguous` on H100 triaged oncall: transformer/mha module: functorch,2023-05-31 22:32:43+00:00,,0,2,triaged oncall: transformer/mha module: functorch,True
102653,torchscript dataclasses have bad support for class types as fields oncall: jit,2023-05-31 21:49:07+00:00,,0,0,oncall: jit,False
102631,"Error when exporting to onnx for albert-base-v2, issue with attention_mask module: onnx triaged",2023-05-31 18:29:29+00:00,,0,1,module: onnx triaged,True
102610,Cannot invoke prims.sum with output_dtype triaged module: primTorch,2023-05-31 11:06:47+00:00,,0,0,triaged module: primTorch,True
102609,[prims] torch.ops.aten.le decomposition confuses scalars and tensors triaged module: primTorch,2023-05-31 10:58:54+00:00,,0,4,triaged module: primTorch,True
102600,Support for activation checkpoint on demand in custom function module: checkpoint triaged,2023-05-31 06:41:34+00:00,,0,1,module: checkpoint triaged,True
102596,Jetson NX with torch 1.12.0 :cannot import name 'ProcessGroup' from 'torch.distributed'. oncall: distributed,2023-05-31 05:19:59+00:00,,0,7,oncall: distributed,False
102559,Mergebot should merge non-stacked PR triaged enhancement module: devx,2023-05-30 18:49:11+00:00,,2,1,triaged enhancement module: devx,True
102556,test_functional_autograd_benchmark.py::TestFunctionalAutogradBenchmark::test_fast_tasks passes with all NaNs module: autograd triaged module: functorch,2023-05-30 18:07:03+00:00,,0,0,module: autograd triaged module: functorch,True
102534,[RFC] Add third-party malloc library to improve pytorch memory performance on Windows module: performance module: windows module: cpu triaged intel,2023-05-30 15:11:59+00:00,,0,4,module: performance module: windows module: cpu triaged intel,True
102533,Segfault when running vulkan program linked against libtorch triaged module: vulkan ciflow/periodic,2023-05-30 14:15:39+00:00,,0,0,triaged module: vulkan ciflow/periodic,True
102517,[feature request] PyTorch support for sub-interpreters with PEP 684 accepted and release in Python 3.12 triaged module: python frontend,2023-05-30 07:20:13+00:00,,0,11,triaged module: python frontend,False
102515,pytorch java api documentation is not clear and does not cover example  module: docs triaged oncall: mobile oncall: java,2023-05-30 06:31:58+00:00,,0,0,module: docs triaged oncall: mobile oncall: java,False
102511,Faster BatchSampler with big batch size module: dataloader triaged,2023-05-30 06:04:53+00:00,,0,0,module: dataloader triaged,True
102507,[Inductor] [CPU] hf_Longformer performance regression > 10% on 2023-05-28 nightly release triaged oncall: pt2 module: cpu inductor,2023-05-30 03:48:49+00:00,,1,3,triaged oncall: pt2 module: cpu inductor,False
102501,[Utils][tensorboard]Enhancement: Include 'max_outputs' parameter in torch.utils.tensorboard.summary's 'image' method triaged module: tensorboard,2023-05-30 03:20:52+00:00,,0,0,triaged module: tensorboard,True
102498,[REQUEST] - Update Multiprocessing best practices with CPU device module: multiprocessing triaged docs-hackathon topic: docs,2023-05-30 02:45:38+00:00,,1,0,module: multiprocessing triaged docs-hackathon topic: docs,False
102479,torch.onnx.errors.CheckerError: The model does not have an ir_version set properly. module: onnx triaged,2023-05-29 10:48:05+00:00,,0,3,module: onnx triaged,False
102459,Matrix multiplication performance regression in case of an additional dimension of size 1 module: dependency bug triaged module: cublas,2023-05-28 22:16:48+00:00,,0,0,module: dependency bug triaged module: cublas,True
102457,Batching rule for `aten::_scaled_dot_product_efficient_attention` triaged module: vmap,2023-05-28 21:52:20+00:00,,0,11,triaged module: vmap,True
102453,RuntimeError using torch.nn.functional.pad when using MPS triaged module: mps,2023-05-28 17:00:22+00:00,,0,0,triaged module: mps,True
102447,"Add additional ""sigmoid"" approximation to GeLu activation? module: nn triaged enhancement needs research",2023-05-28 13:53:44+00:00,,1,2,module: nn triaged enhancement needs research,True
102438,DDP multi node multi gpu inconsistent params oncall: distributed,2023-05-27 15:25:05+00:00,,0,1,oncall: distributed,False
102437,discuss.pytorch.org signup issue module: docs triaged,2023-05-27 14:11:30+00:00,,0,8,module: docs triaged,True
102436,multiple mps for base X86 Mac with multiples gpus triaged module: mps,2023-05-27 12:48:56+00:00,,0,0,triaged module: mps,True
102435,torch.distributed.all_reduce() has inconsistent behavior oncall: distributed triaged,2023-05-27 10:30:08+00:00,,0,1,oncall: distributed triaged,True
102430,Add support ONNX Opset 19 module: onnx triaged,2023-05-27 06:26:57+00:00,,1,1,module: onnx triaged,False
102423,Add default device function interface for device-aware apis triaged module: dispatch,2023-05-27 02:24:20+00:00,,0,4,triaged module: dispatch,False
102417,_view_func but without keeping original view tensor alive module: autograd triaged needs design module: viewing and reshaping,2023-05-27 00:08:25+00:00,,0,1,module: autograd triaged needs design module: viewing and reshaping,True
102402,[Composable] Unified way to check if modules are managed by composable API oncall: distributed release notes: distributed (composable),2023-05-26 20:48:37+00:00,,0,0,oncall: distributed release notes: distributed (composable),False
102400,Unexpected Behavior when using torch.isclose() triaged module: NaNs and Infs module: testing,2023-05-26 20:27:31+00:00,,0,1,triaged module: NaNs and Infs module: testing,True
102374,Hooks not working in version 2.0.1+cu118 module: nn triaged,2023-05-26 15:58:12+00:00,,0,11,module: nn triaged,True
102373,"[cuda] Switching CI to CUDA 12.1 timing out linux-bionic-cuda12.1-py3.10-gcc7 / test (distributed, 3, 3, linux.8xlarge.nvidia.gpu) module: cuda module: ci triaged",2023-05-26 15:40:43+00:00,,0,2,module: cuda module: ci triaged,True
102368,Issue with ShufflerIterDataPipe in torch 1.13.1 module: dataloader triaged module: data,2023-05-26 12:43:58+00:00,,0,2,module: dataloader triaged module: data,True
102360,PyTorch hangs at import when used together with TensorFlow triaged topic: binaries,2023-05-26 08:58:13+00:00,,0,10,triaged topic: binaries,False
102355,Data type mismatch in `batch_isend_irecv` docstring example oncall: distributed,2023-05-26 07:36:55+00:00,,0,2,oncall: distributed,False
102354,"""Examples"" in ""batch_isend_irecv"" should be modified to get the correct results oncall: distributed",2023-05-26 07:25:29+00:00,,0,1,oncall: distributed,False
102339,[Dynamo] Better graph-break message for unsupported ctx managers good first issue triaged oncall: pt2 module: dynamo,2023-05-26 05:48:21+00:00,,0,1,good first issue triaged oncall: pt2 module: dynamo,True
102337,Tensors that share same underlying storage to also share gradient storage module: autograd triaged needs research module: python frontend,2023-05-26 05:08:05+00:00,,0,12,module: autograd triaged needs research module: python frontend,True
102334,There is a memory leak in torch.load module: memory usage module: serialization triaged,2023-05-26 04:14:33+00:00,,0,10,module: memory usage module: serialization triaged,True
102333,"transformer encoder-layer, the sample-Independent attn_mask(dim=3) has different behaviors when training and validating triaged oncall: transformer/mha",2023-05-26 04:13:53+00:00,,0,0,triaged oncall: transformer/mha,True
102319,Re-enable Low Memory Dropout feature triaged oncall: pt2 module: inductor,2023-05-26 00:23:39+00:00,,1,1,feature triaged oncall: pt2 module: inductor,False
102285,[Dynamo]Outdated logging setting module: docs triaged oncall: pt2,2023-05-25 18:19:10+00:00,,0,1,module: docs triaged oncall: pt2,False
102272,"after add /path_to_libtorch/libtorch/lib to LD_LIBRARY_PATH, I can't import torch_scatter. module: build module: cpp module: abi triaged",2023-05-25 15:31:59+00:00,,0,1,module: build module: cpp module: abi triaged,True
102269,Import of torch breaks standard multiprocessing module: dependency bug module: multiprocessing triaged module: openmp,2023-05-25 13:47:05+00:00,,1,12,module: dependency bug module: multiprocessing triaged module: openmp,True
102261,ExponentialLR unexpectedly calls `step()` when init argument `last_epoch` is larger than -1 module: optimizer triaged actionable module: LrScheduler,2023-05-25 07:34:07+00:00,,0,1,module: optimizer triaged actionable module: LrScheduler,True
102227,lintrunner should fail on badly formatted docstrings triaged enhancement oncall: pt2 module: devx,2023-05-24 23:56:11+00:00,,0,6,triaged enhancement oncall: pt2 module: devx,False
102211,[ONNX] test_op_consistency.py doesn't support constant inputs module: onnx triaged onnx-triaged,2023-05-24 21:36:00+00:00,,0,0,module: onnx triaged onnx-triaged,False
102207,skipIfTorchInductor Tracking Issue  good first issue triaged oncall: pt2 module: inductor,2023-05-24 21:01:30+00:00,,0,1,good first issue triaged oncall: pt2 module: inductor,True
102206,copy_'s functionalized operator keeps copied into tensor live triaged module: functionalization oncall: pt2 module: pt2-dispatcher,2023-05-24 20:56:43+00:00,,0,3,triaged module: functionalization oncall: pt2 module: pt2-dispatcher,False
102205,aot_export_joint_simple on plain callable (not graph module) doesn't attach stack traces triaged better-engineering oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-05-24 20:53:50+00:00,,1,1,triaged better-engineering oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
102201,scipy.ndimage.find_objects feature module: nn triaged needs research,2023-05-24 20:32:54+00:00,,1,11,feature module: nn triaged needs research,True
102197,torch.func.jvp fails when acting on a DistributedDataParallel model oncall: distributed module: forward ad module: functorch,2023-05-24 19:55:43+00:00,,0,4,oncall: distributed module: forward ad module: functorch,True
102187,Extend fake fast path to more situations triaged module: fakeTensor,2023-05-24 18:23:49+00:00,,0,0,triaged module: fakeTensor,True
102186,torch/distributed/_spmd/api.py should aot_module_export instead of make_fx directly oncall: distributed,2023-05-24 18:20:43+00:00,,0,0,oncall: distributed,False
102167,Calling pin_memory() fails for nested tensor  triaged module: nestedtensor topic: new features,2023-05-24 13:26:22+00:00,,0,1,triaged module: nestedtensor topic: new features,True
102162,NotImplementedError in backprop on on dense-sparse matrices module: sparse triaged,2023-05-24 12:13:09+00:00,,0,4,module: sparse triaged,True
102157,DISABLED test_build_tuple_unpack_dynamic_shapes (torch._dynamo.testing.DynamicShapesMiscTests) triaged module: flaky-tests skipped module: dynamo,2023-05-24 09:39:23+00:00,,0,3,triaged module: flaky-tests skipped module: dynamo,False
102142,Pytorch CXX11 ABI version module: build module: abi triaged,2023-05-24 03:24:31+00:00,,0,1,module: build module: abi triaged,True
102115,DISABLED test_inplace_grad_index_put_cuda_complex128 (__main__.TestBwdGradientsCUDA) module: autograd triaged module: flaky-tests skipped oncall: pt2 module: inductor module: pt2-dispatcher,2023-05-23 21:39:48+00:00,,0,4,module: autograd triaged module: flaky-tests skipped oncall: pt2 module: inductor module: pt2-dispatcher,False
102113,DISABLED test_inplace_grad_div_trunc_rounding_cuda_float64 (__main__.TestBwdGradientsCUDA) module: autograd triaged module: flaky-tests skipped oncall: pt2 module: inductor module: pt2-dispatcher,2023-05-23 21:39:32+00:00,,0,3,module: autograd triaged module: flaky-tests skipped oncall: pt2 module: inductor module: pt2-dispatcher,False
102112,DISABLED test_fn_grad_div_trunc_rounding_cuda_float64 (__main__.TestBwdGradientsCUDA) module: autograd triaged module: flaky-tests skipped oncall: pt2 module: inductor module: pt2-dispatcher,2023-05-23 21:39:25+00:00,,0,2,module: autograd triaged module: flaky-tests skipped oncall: pt2 module: inductor module: pt2-dispatcher,False
102105,Enable DEBUG asserts for C++ builds triaged module: devx,2023-05-23 19:10:24+00:00,,1,0,triaged module: devx,True
102085,BatchNorm can't be symbolically traced with torch.fx as a standalone module  triaged module: functorch,2023-05-23 13:50:21+00:00,,0,2,triaged module: functorch,True
102084,Documentation Error of torch.onnx module: onnx module: docs triaged,2023-05-23 13:46:29+00:00,,0,3,module: onnx module: docs triaged,True
102081,CPU Fallback does not convert Tensor?[] module: internals triaged,2023-05-23 13:29:53+00:00,,0,1,module: internals triaged,True
102078,AddressSanitizer: heap-buffer-overflow in test_comprehensive_nn_functional_embedding_bag_cpu_bfloat16  needs reproduction module: cpu triaged module: embedding module: decompositions,2023-05-23 12:33:00+00:00,,0,8,needs reproduction module: cpu triaged module: embedding module: decompositions,True
102071,DISABLED test_build_tuple_unpack (__main__.DynamicShapesMiscTests) triaged module: flaky-tests skipped module: dynamo,2023-05-23 09:39:27+00:00,,0,62,triaged module: flaky-tests skipped module: dynamo,False
102109,Can't vmap over a slice expression triaged module: functorch,2023-05-23 09:30:31+00:00,,0,5,triaged module: functorch,True
102063,[dynamo][BE] Revisit call_method of NNModuleVariable triaged oncall: pt2 module: dynamo,2023-05-23 06:58:35+00:00,,1,1,triaged oncall: pt2 module: dynamo,False
102044,DISABLED test_comprehensive_empty_strided_cuda_int64 (__main__.TestInductorOpInfoCUDA) triaged module: flaky-tests skipped module: inductor,2023-05-23 03:39:21+00:00,,0,1,triaged module: flaky-tests skipped module: inductor,False
102034,DISABLED test_call_parent_non_class_methods_from_child (torch._dynamo.testing.DynamicShapesMiscTests) triaged module: flaky-tests skipped module: dynamo,2023-05-23 00:57:24+00:00,,0,6,triaged module: flaky-tests skipped module: dynamo,False
102033,DISABLED test_comprehensive_empty_strided_cuda_float64 (__main__.TestInductorOpInfoCUDA) triaged module: flaky-tests skipped module: inductor,2023-05-23 00:57:21+00:00,,0,3,triaged module: flaky-tests skipped module: inductor,False
102023,torch.compile FakeTensor tracing fails with foreach ops with multiple devices module: optimizer triaged module: custom-operators release notes: foreach_frontend oncall: pt2 module: fakeTensor module: pt2-dispatcher,2023-05-22 23:02:11+00:00,,1,5,module: optimizer triaged module: custom-operators release notes: foreach_frontend oncall: pt2 module: fakeTensor module: pt2-dispatcher,True
101999,[FSDP] Ensure full precision checkpoints oncall: distributed triaged module: fsdp,2023-05-22 18:56:07+00:00,,0,0,oncall: distributed triaged module: fsdp,True
101998,[FSDP] Summon buffers in full precision oncall: distributed module: fsdp,2023-05-22 18:55:09+00:00,,0,0,oncall: distributed module: fsdp,False
101984,"[dynamo]  BackendCompilerFailed: backend='inductor' raised: NetworkXUnbounded: Infinite capacity path, flow unbounded above. triaged oncall: pt2",2023-05-22 16:38:28+00:00,,1,0,triaged oncall: pt2,False
101974,"Request for adding support for `torch.rand_like`, `torch.randn_like`, `torch.randint_like` with `torch.Generator` triaged module: random",2023-05-22 11:57:18+00:00,,0,0,triaged module: random,True
101972,No pytorch_android 2.0.x builds triaged module: android oncall: mobile,2023-05-22 11:44:54+00:00,,0,3,triaged module: android oncall: mobile,True
101968,CrossEntropyLoss output difference on Windows vs. Linux module: windows triaged,2023-05-22 09:21:04+00:00,,1,1,module: windows triaged,True
101967,scaled_dot_product_attention produces NaN when input has NaN in masked-out positions triaged module: NaNs and Infs module: edge cases module: multi-headed-attention,2023-05-22 08:19:33+00:00,,0,2,triaged module: NaNs and Infs module: edge cases module: multi-headed-attention,True
101962,SummaryWriter background thread holds the GIL for too long oncall: distributed module: tensorboard,2023-05-22 05:14:42+00:00,,0,0,oncall: distributed module: tensorboard,False
101950,torch.flip is inplaced too aggressively in torch inductor triaged bug oncall: pt2 module: inductor,2023-05-21 12:55:42+00:00,,0,4,triaged bug oncall: pt2 module: inductor,True
101936,mps device bug - a weird inconsistency on tensor indexing operations triaged module: mps,2023-05-20 21:51:57+00:00,,0,2,triaged module: mps,True
101934,Parameter gradient is not moved parameter is moved across devices module: docs module: nn triaged actionable module: correctness (silent),2023-05-20 20:54:38+00:00,,1,8,module: docs module: nn triaged actionable module: correctness (silent),True
101930,[feature request] [minor] Inplace torch.flip_ triaged enhancement module: python frontend,2023-05-20 17:31:53+00:00,,0,5,triaged enhancement module: python frontend,True
101929,can not find tensorrt triaged oncall: pt2,2023-05-20 15:47:22+00:00,,0,2,triaged oncall: pt2,False
101918,[compile] Tracker for `torchrec_dlrm` issues triaged module: custom-operators module: functionalization oncall: pt2 module: pt2-dispatcher,2023-05-20 00:22:23+00:00,,0,10,triaged module: custom-operators module: functionalization oncall: pt2 module: pt2-dispatcher,True
101890,Can't reproduce/non-deterministic results with CUDA module: cuda triaged module: determinism,2023-05-19 19:07:17+00:00,,0,3,module: cuda triaged module: determinism,True
101880,Crash on Python //  PyArrow //  needs reproduction triaged,2023-05-19 17:02:16+00:00,,0,2,needs reproduction triaged,True
101878,torch.quantile on MPS doesn't sort values when dim is not None high priority triaged module: correctness (silent) module: mps,2023-05-19 16:28:30+00:00,,0,12,high priority triaged module: correctness (silent) module: mps,True
101871,Can group convolution support other grouping methods? module: convolution triaged,2023-05-19 13:12:44+00:00,,0,0,module: convolution triaged,True
101866,torch.compile makes transformers model (llama) generating different outputs compared with the native triaged oncall: pt2 module: cpu inductor,2023-05-19 09:14:49+00:00,,2,7,triaged oncall: pt2 module: cpu inductor,False
101864,"Error, attribute exists on the Python module, but we failed to convert Python type: 'list' to a TorchScript type  oncall: jit module: onnx",2023-05-19 08:37:10+00:00,,0,0,oncall: jit module: onnx,True
101861,Observing negative number in PyTorch profiling triaged oncall: profiler,2023-05-19 08:03:04+00:00,,1,1,triaged oncall: profiler,True
101855,torch.jit.trace() Floating point exception oncall: jit,2023-05-19 05:15:11+00:00,,0,0,oncall: jit,True
101850,Unexpected modification to CPU affinity of Dataloader workers module: multiprocessing triaged module: openmp,2023-05-19 03:12:11+00:00,,0,10,module: multiprocessing triaged module: openmp,True
101801,pytorch-nightly not have torch/version.py.tpl:cuda specified module: binaries module: windows triaged,2023-05-18 15:47:48+00:00,,1,7,module: binaries module: windows triaged,True
101798,Fix absolute links in pytorch repository and allow it to be proxied oncall: releng triaged,2023-05-18 12:43:08+00:00,,0,4,oncall: releng triaged,False
101795,Implement `to_numpy` method to speed up matplotlib with PyTorch arrays triaged enhancement module: numpy needs research,2023-05-18 09:59:10+00:00,,0,5,triaged enhancement module: numpy needs research,True
101787,DISABLED test_decoder_padding_and_src_mask_bool_cpu (__main__.TestTransformersCPU) module: nn triaged module: flaky-tests skipped oncall: transformer/mha,2023-05-18 06:39:55+00:00,,0,1,module: nn triaged module: flaky-tests skipped oncall: transformer/mha,False
101781,Add support for bfloat16 in torch.from_numpy() triaged module: numpy module: bfloat16,2023-05-18 06:09:46+00:00,,0,0,triaged module: numpy module: bfloat16,True
101771,DISABLED test_fn_grad_remainder_cuda_float64 (__main__.TestBwdGradientsCUDA) triaged module: flaky-tests skipped module: inductor,2023-05-18 03:39:31+00:00,,0,1,triaged module: flaky-tests skipped module: inductor,False
101770,DISABLED test_fn_grad___rmod___cuda_float64 (__main__.TestBwdGradientsCUDA) triaged module: flaky-tests skipped module: inductor,2023-05-18 03:39:28+00:00,,0,1,triaged module: flaky-tests skipped module: inductor,False
101758,DISABLED test_cond_nested (__main__.MiscTests) triaged module: flaky-tests skipped module: dynamo,2023-05-18 00:58:09+00:00,,0,6,triaged module: flaky-tests skipped module: dynamo,False
101757,DISABLED test_fn_gradgrad_remainder_cuda_float64 (__main__.TestBwdGradientsCUDA) triaged module: flaky-tests skipped module: inductor,2023-05-18 00:58:06+00:00,,0,1,triaged module: flaky-tests skipped module: inductor,False
101755,DISABLED test_fn_grad_index_put_cuda_complex128 (__main__.TestBwdGradientsCUDA) triaged module: flaky-tests skipped module: inductor,2023-05-18 00:58:00+00:00,,0,1,triaged module: flaky-tests skipped module: inductor,False
101716,"[TorchScript] aten::__and__ with argument types: Tensor, bool not supported oncall: jit triaged",2023-05-17 18:55:40+00:00,,0,0,oncall: jit triaged,True
101699,[discussion] [feature request] Native tensor-backed string array and basic string processing functions for addition into core + discussion of extremely basic data frames (also for reducing python object heap pressure) feature triaged needs research,2023-05-17 15:29:53+00:00,,0,12,feature triaged needs research,False
101669,[refs] inplace references resize the input to match the broadcasted input shape triaged module: primTorch,2023-05-17 06:15:34+00:00,,0,1,triaged module: primTorch,True
101666,Unexpected behavior of fmod op in some float32 input module: numerical-stability triaged,2023-05-17 03:53:35+00:00,,0,4,module: numerical-stability triaged,True
101653,Unexpected behavior comparing uint8 tensor to value greater than 255 triaged module: type promotion module: edge cases,2023-05-17 01:17:16+00:00,,0,6,triaged module: type promotion module: edge cases,True
101632,torch.profiler.profile has an empty python replay stack under certain circumstances triaged oncall: profiler,2023-05-16 23:58:51+00:00,,0,4,triaged oncall: profiler,True
101625,DISABLED test_compare_cpu__refs_empty_strided_cuda_float32 (__main__.TestCommonCUDA) module: cuda triaged skipped,2023-05-16 22:20:05+00:00,,0,1,module: cuda triaged skipped,False
101624,[Dynamo + DDP] If DDP partitions FX graph generated by Dynamo correctly triaged oncall: pt2 module: dynamo,2023-05-16 22:11:17+00:00,,1,12,triaged oncall: pt2 module: dynamo,False
101609,[Dynamo] Can't inline functions under torch.nn.parallel  triaged oncall: pt2 module: dynamo,2023-05-16 20:56:44+00:00,,1,4,triaged oncall: pt2 module: dynamo,False
101603,multiple values for argument `softmax_scale` triaged module: fsdp oncall: pt2 module: distributed,2023-05-16 19:16:39+00:00,,1,4,triaged module: fsdp oncall: pt2 module: distributed,True
101586,"/Users/davidlaxer/pytorch/third_party/tensorpipe/third_party/libuv/src/unix/getaddrinfo.c:165:10: error: implicit declaration of function 'uv__idna_toascii' [-Werror,-Wimplicit-function-declaration]     rc = uv__idna_toascii(hostname, oncall: distributed triaged",2023-05-16 17:55:36+00:00,,0,0,oncall: distributed triaged,False
101566,Unable to do tensor comparison on Metal Performance Shaders (MPS) triaged module: mps,2023-05-16 14:17:42+00:00,,0,0,triaged module: mps,True
101536,"torch.cuda.set_device cannot use to set cpu device, but give an ambiguity hint triaged",2023-05-16 09:37:33+00:00,,0,2,triaged,True
101535, Exporting the operator 'aten::scatter_reduce' to ONNX opset version 15 is not supported module: onnx triaged,2023-05-16 09:23:31+00:00,,0,8,module: onnx triaged,False
101531,torch.nn.functional.scaled_dot_product_attention() : support both attn_mask and is_causal triaged module: multi-headed-attention,2023-05-16 08:16:32+00:00,,0,2,triaged module: multi-headed-attention,True
101529,Inconsistent performance degradation of 3x3 convolution (torch 2.0.1+cu118) module: performance module: cudnn module: cuda triaged module: regression,2023-05-16 07:34:32+00:00,,0,4,module: performance module: cudnn module: cuda triaged module: regression,True
101527,DISABLED test_cond_export (__main__.MiscTests) triaged module: flaky-tests skipped module: dynamo,2023-05-16 06:46:30+00:00,,0,13,triaged module: flaky-tests skipped module: dynamo,False
101525,DISABLED test_noncontiguous_samples_matmul_cuda_float32 (__main__.TestCommonCUDA) module: cuda triaged module: flaky-tests skipped,2023-05-16 06:40:21+00:00,,0,53,module: cuda triaged module: flaky-tests skipped,False
101506,Support pipeline parallelism with PyG oncall: distributed triaged topic: new features,2023-05-16 04:05:12+00:00,,0,0,oncall: distributed triaged topic: new features,False
101505,Investigate random sequence number broadcast initially incorrect oncall: distributed triaged,2023-05-16 03:55:02+00:00,,0,0,oncall: distributed triaged,True
101502,onnx runtime error module: onnx triaged,2023-05-16 03:48:53+00:00,,0,2,module: onnx triaged,True
101470,[bazel] add inductor to bazel build module: build triaged module: bazel,2023-05-16 00:16:34+00:00,,0,0,module: build triaged module: bazel,True
101463,Regression in NCCL error handling oncall: distributed,2023-05-15 22:36:29+00:00,,0,10,oncall: distributed,True
101444,RuntimeError: Triton Error [CUDA]: device-side assert triggered when trying torch.compile max-autotune on nanoGPT triaged oncall: pt2,2023-05-15 20:30:07+00:00,,0,0,triaged oncall: pt2,True
101443,Enhance FSDP debugability oncall: distributed triaged module: fsdp,2023-05-15 20:24:32+00:00,,0,2,oncall: distributed triaged module: fsdp,True
101564,not yet implemented the batching rule for torchaudio::_lfilter triaged module: batching module: functorch,2023-05-15 18:12:42+00:00,,0,0,triaged module: batching module: functorch,True
101428,2D inputs to linear layers run up to 25% slower than 4D ones on some Nvidia GPUs triaged module: cublas,2023-05-15 17:57:33+00:00,,0,1,triaged module: cublas,True
101415,import functorch.dim monkeypatches torch triaged module: functorch,2023-05-15 15:22:21+00:00,,0,1,triaged module: functorch,True
101407,Delete old vmap prototype triaged better-engineering module: functorch,2023-05-15 14:07:29+00:00,,1,0,triaged better-engineering module: functorch,True
101404,problem of compilation for torch2.0 needs reproduction module: build triaged module: third_party,2023-05-15 12:58:51+00:00,,0,3,needs reproduction module: build triaged module: third_party,True
101402,DataParallel for nested modules oncall: distributed,2023-05-15 12:05:58+00:00,,0,1,oncall: distributed,False
101398,ONNX model different to pytorch and jit trace output module: onnx triaged,2023-05-15 08:55:11+00:00,,0,0,module: onnx triaged,False
101385,torch.Tensor.is_sparse returns false for non-COO sparse tensors module: sparse triaged,2023-05-15 06:28:47+00:00,,0,1,module: sparse triaged,True
101380,all_to_all_single seems to be missing a check for checkSplitSizes when splitsize=0. oncall: distributed,2023-05-15 03:24:44+00:00,,0,3,oncall: distributed,False
101370,SparseAdam: working with dense parameters but sparse gradients - usecase  module: optimizer triaged actionable,2023-05-14 21:38:32+00:00,,0,15,module: optimizer triaged actionable,True
101369,Theme update module: docs triaged module: doc infra,2023-05-14 21:26:35+00:00,,0,1,module: docs triaged module: doc infra,True
101359,RuntimeError in Scaled Dot Product Attention Tutorial Code module: cpu triaged,2023-05-14 12:01:53+00:00,,0,2,module: cpu triaged,True
101356,inductor: inductor conv2d get a different size and stride with eager mod when input channel is zero triaged ZeroTensor module: inductor module: cpu inductor,2023-05-14 08:31:07+00:00,,2,4,triaged ZeroTensor module: inductor module: cpu inductor,False
101335,fsdp training with the seq2seqTranier module gets stuck during evaluation. triaged module: fsdp,2023-05-13 11:53:33+00:00,,0,2,triaged module: fsdp,True
101334,Functions for Calculating Skewness and Kurtosis  feature triaged module: reductions,2023-05-13 10:32:33+00:00,,0,5,feature triaged module: reductions,True
101331,Pytorch 2.1.0.dev20230512 cuda not available needs reproduction module: binaries module: cuda triaged,2023-05-13 08:24:22+00:00,,0,5,needs reproduction module: binaries module: cuda triaged,True
101321,Speed when installing from source is very low with CUDA 11 needs reproduction module: performance module: build module: cuda triaged,2023-05-12 23:12:36+00:00,,0,1,needs reproduction module: performance module: build module: cuda triaged,True
101319,Deprecated File bug low priority triaged topic: build,2023-05-12 22:44:56+00:00,,0,2,low priority triaged topic: build,True
101314,Shared library loading logic breaks when CUDA packages are installed in a non-standard location triaged module: bazel topic: build bug,2023-05-12 21:56:07+00:00,,0,4,triaged module: bazel topic: build bug,True
101294,Docs suggestion `FullyShardedDataParallel.summon_full_params` must be called on all ranks/processes triaged module: fsdp,2023-05-12 18:24:23+00:00,,0,1,triaged module: fsdp,True
101293,Operations to shared tensors in the forked process could lead to silent crash module: multiprocessing triaged,2023-05-12 17:58:47+00:00,,0,2,module: multiprocessing triaged,True
101291,fused torch.optim.AdamW isn't faster than the unfused version module: optimizer triaged,2023-05-12 17:05:06+00:00,,0,5,module: optimizer triaged,False
101289,Support fake tensor real inputs in dynamo triaged module: dynamo,2023-05-12 16:34:12+00:00,,0,1,triaged module: dynamo,False
101288,Should be ok to call _dynamo.export and torch.compile under FakeTensorMode triaged module: dynamo,2023-05-12 16:31:04+00:00,,0,0,triaged module: dynamo,True
101273,IPEX as TorchDynamo Backend Performance Dashboard triaged intel oncall: pt2,2023-05-12 10:09:21+00:00,,0,60,triaged intel oncall: pt2,False
101265,"Noisy warning - torch.fx.experimental.symbolic_shapes: [WARNING] Ignored guard (...), this could result in accuracy problems low priority triaged oncall: pt2 module: dynamic shapes",2023-05-12 06:49:16+00:00,,0,2,low priority triaged oncall: pt2 module: dynamic shapes,True
101255,torch._dynamo.exc.UserError: Dynamic control flow is not supported at the moment. triaged oncall: pt2,2023-05-12 04:40:05+00:00,,1,4,triaged oncall: pt2,False
101251,Mac m2 MPSNDArray.mm:78: failed assertion `[MPSNDArrayDescriptor sliceDimension:withSubrange:] error: dimension index (2) not within number of dimensions (2) 	Dimension indices are 0-based' triaged module: mps,2023-05-12 03:29:43+00:00,,0,3,triaged module: mps,True
101249,`einsum` is about 40x slower on CUDA than manually multiplying and summing module: performance triaged module: linear algebra,2023-05-12 02:59:52+00:00,,0,8,module: performance triaged module: linear algebra,True
101246,Tool for identifying where in eager model an operation is nondeterministic triaged module: determinism,2023-05-12 02:50:04+00:00,,0,2,triaged module: determinism,True
101241,Different results with vmap when using torch.jit.script oncall: jit triaged module: functorch,2023-05-12 01:28:50+00:00,,0,2,oncall: jit triaged module: functorch,True
101233,"How the [WARNING] using triton random, expect difference from eager arises? triaged oncall: pt2",2023-05-12 00:28:35+00:00,,0,5,triaged oncall: pt2,False
101210,GLCM implementation in pytorch C++ api and cuda module: cpp triaged,2023-05-11 19:00:14+00:00,,1,3,module: cpp triaged,True
101209,Migrate windows runners to non-ephemeral instances module: ci triaged,2023-05-11 18:48:48+00:00,,1,4,module: ci triaged,True
101192,AOTAutograd export path does not support training graphs with parameters that do not receive gradients. triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-05-11 15:11:50+00:00,,0,0,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
101191,custom_op API follow-ups triaged,2023-05-11 14:53:17+00:00,,0,6,triaged,True
101189,`dense -> sparse compressed` to work with empty batches. module: sparse triaged,2023-05-11 14:08:47+00:00,,0,0,module: sparse triaged,True
101188,Weird dataloader performance degradation caused by torch and numpy import order triaged module: openmp,2023-05-11 14:01:14+00:00,,0,4,triaged module: openmp,True
101185,Pure virtual function call exception on Python interpreter exit when using debug wheel triaged module: python frontend,2023-05-11 13:16:02+00:00,,0,9,triaged module: python frontend,True
101184,"Fork run CI from upstream remote (more than 10,000 emails)  module: ci triaged",2023-05-11 12:54:12+00:00,,0,2,module: ci triaged,True
101177,version 4.26.1 to 4.29.0 has two bugs oncall: distributed triaged,2023-05-11 09:54:38+00:00,,0,3,oncall: distributed triaged,True
101168,[torch.compile] torch._dynamo.exc.Unsupported: setattr(UserDefinedObjectVariable) for yolov7 triaged bug oncall: pt2 module: dynamo,2023-05-11 08:03:32+00:00,,1,6,triaged bug oncall: pt2 module: dynamo,True
101167,round float16 calculation error in mps backend triaged module: mps,2023-05-11 07:56:34+00:00,,0,0,triaged module: mps,True
101160,Fine-tuning HuggingFace wav2vec 2.0 with `torch.compile` oncall: distributed module: crash triaged module: nccl ezyang's list bug oncall: pt2 module: dynamic shapes,2023-05-11 06:33:41+00:00,,1,12,oncall: distributed module: crash triaged module: nccl ezyang's list bug oncall: pt2 module: dynamic shapes,True
101159,Inconsistency between GPU memory usage in torch.cuda.memory_summary and nvidia-smi module: cuda triaged,2023-05-11 06:15:29+00:00,,0,1,module: cuda triaged,True
101154,[Dynamo] TB hf_Reformer graph breaks triaged oncall: pt2 module: dynamo,2023-05-11 04:18:54+00:00,,1,3,triaged oncall: pt2 module: dynamo,False
101152,Importing torch after TensorFlow results in std::runtime_error triaged module: tensorflow,2023-05-11 03:32:12+00:00,,0,6,triaged module: tensorflow,True
101150,[ONNX] OnnxFunction of aten_index_put_bool operation isn't consistent to aten::index_put inx FX exporter module: onnx triaged onnx-triaged,2023-05-11 02:29:34+00:00,,0,1,module: onnx triaged onnx-triaged,True
101138,Fault and vauge error when invoking nvcc: The system cannot find the file specified module: windows module: cpp-extensions triaged,2023-05-10 23:15:01+00:00,,0,0,module: windows module: cpp-extensions triaged,True
101135,Pytorch compile failure on Windows with CUDA 12.1 because of lacking NVTX component module: build module: windows triaged,2023-05-10 22:36:49+00:00,,0,4,module: build module: windows triaged,True
101117,Barriers to using torch.compile directly in PyTorch library code feature triaged oncall: pt2 module: inductor module: dynamo,2023-05-10 19:47:04+00:00,,0,0,feature triaged oncall: pt2 module: inductor module: dynamo,False
101110,Tensorboard graph tracing with torch fx API triaged module: tensorboard module: fx,2023-05-10 19:04:25+00:00,,0,6,triaged module: tensorboard module: fx,True
101107,Make compiled models serializable triaged oncall: pt2,2023-05-10 18:48:57+00:00,,1,7,triaged oncall: pt2,False
101106,"addmv doesn't do type promotion correctly, triaged module: type promotion module: linear algebra",2023-05-10 18:44:02+00:00,,0,2,triaged module: type promotion module: linear algebra,True
101096,[BE] Refactor logic for MultiTensorApply triaged better-engineering actionable module: mta,2023-05-10 17:08:19+00:00,,0,4,triaged better-engineering actionable module: mta,True
101091,Cannot export quantized model to onnx: cannot call qscheme on UnknownQuantizer module: onnx oncall: quantization low priority triaged,2023-05-10 16:31:48+00:00,,0,6,module: onnx oncall: quantization low priority triaged,True
101082,Multiple Learning Rate Scheduler for Specific Parameters Groups module: optimizer triaged needs research module: LrScheduler,2023-05-10 15:11:07+00:00,,1,4,module: optimizer triaged needs research module: LrScheduler,True
101080,Sequence annotation in type hints is wrong module: typing triaged,2023-05-10 14:50:13+00:00,,0,4,module: typing triaged,True
101075,torch.lobpcg producing different largest eigenvalue than scipy and np.linalg.eig triaged module: numpy module: linear algebra,2023-05-10 14:18:05+00:00,,0,9,triaged module: numpy module: linear algebra,True
101070,Lazily format C++ stack trace if it is not used module: performance triaged,2023-05-10 13:27:41+00:00,,0,0,module: performance triaged,True
101069,"torch.autograd.detect_anomaly should report the original forward trace as part of the error, rather than as out of band warning module: autograd triaged actionable",2023-05-10 13:25:20+00:00,,0,2,module: autograd triaged actionable,True
101068,"Tensor __getitem__ not documented, sparse grad? module: sparse triaged module: advanced indexing",2023-05-10 13:16:46+00:00,,0,12,module: sparse triaged module: advanced indexing,True
101055,DISABLE libtorch-2.0.0+cu117 destructor exception oncall: jit,2023-05-10 07:38:58+00:00,,0,0,oncall: jit,True
101039,[torch.compile] the sum of `softmax` isn't `1` on cuda module: numerical-stability triaged oncall: pt2 module: inductor,2023-05-10 01:24:10+00:00,,1,5,module: numerical-stability triaged oncall: pt2 module: inductor,True
101033,Exporting the operator 'prim::is_cuda' to ONNX opset version 14 is not supported module: onnx triaged,2023-05-10 00:19:43+00:00,,0,3,module: onnx triaged,False
101031,[PT2] torch.compile doesn't perform horizontal fusion feature triaged oncall: pt2 module: inductor,2023-05-10 00:05:06+00:00,,0,0,feature triaged oncall: pt2 module: inductor,False
101013,Unsupported: ONNX export of operator interpolate (with scales) error module: onnx triaged,2023-05-09 21:06:10+00:00,,0,0,module: onnx triaged,True
101011,[Placeholder] PyTorch 2.0 Dynamo/Inductor Hack{day/week} triaged oncall: pt2 module: dynamo,2023-05-09 20:54:36+00:00,,1,0,triaged oncall: pt2 module: dynamo,False
100996,ONNX TorchDynamo Exporter  - Ability to export and load ONNX files without parameters module: onnx triaged oncall: pt2,2023-05-09 18:28:31+00:00,,0,0,module: onnx triaged oncall: pt2,False
100990,Extending compatibility of LibTorch module: cpp feature triaged needs design,2023-05-09 17:34:08+00:00,,0,1,module: cpp feature triaged needs design,True
100989,"RuntimeError: nonzero is not supported for tensors with more than INT_MAX  elements, file a support request oncall: quantization module: cuda triaged module: 64-bit module: sorting and selection",2023-05-09 17:27:03+00:00,,1,1,oncall: quantization module: cuda triaged module: 64-bit module: sorting and selection,True
100985,"native_batch_norm has different size results on ""CPU"" vs ""META"" device triaged module: meta tensors module: fakeTensor",2023-05-09 16:44:54+00:00,,0,2,triaged module: meta tensors module: fakeTensor,True
100968,AssertionError: slice.Tensor is not supported with cpp wrapper (llama) triaged oncall: pt2,2023-05-09 13:02:38+00:00,,1,1,triaged oncall: pt2,False
100960,Issues building with caffe2 enabled caffe2 triaged,2023-05-09 08:07:33+00:00,,0,1,caffe2 triaged,True
100957,PyTorch installs the file mkldnn.cmake that looks for the package MKLDNN that doesn't exist module: build triaged module: mkldnn,2023-05-09 07:40:20+00:00,,1,3,module: build triaged module: mkldnn,True
100932,torch.concat fails with float16 input in autocast(device_type=cpu) context module: cpu triaged module: bfloat16 module: amp (automated mixed precision),2023-05-09 01:26:39+00:00,,0,5,module: cpu triaged module: bfloat16 module: amp (automated mixed precision),True
100914,[MPS] Track failures of test_module.py for MPS backend triaged module: backend module: mps,2023-05-08 22:06:04+00:00,,1,0,triaged module: backend module: mps,True
100913,[onnx] UnsupportedOperatorError: Exporting the operator 'aten::l1_loss' to ONNX opset version 17 is not supported module: onnx triaged oncall: pt2,2023-05-08 21:47:59+00:00,,0,0,module: onnx triaged oncall: pt2,False
100904,Revise glossary module: docs triaged,2023-05-08 20:59:31+00:00,,0,0,module: docs triaged,True
100884,`torch.distributions.categorical.Categorical` samples indices with zero probability module: distributions triaged,2023-05-08 18:09:01+00:00,,0,0,module: distributions triaged,True
100879,MPS backend is not supported on MacOS 12.6.3 needs reproduction triaged module: mps,2023-05-08 14:24:05+00:00,,0,4,needs reproduction triaged module: mps,False
100873,onnx.export fails if do_constant_folding=False module: onnx triaged,2023-05-08 12:52:23+00:00,,0,0,module: onnx triaged,False
100850,[BUG] Poor torch.bmm performance on H100 module: performance module: cuda triaged module: cublas matrix multiplication,2023-05-08 05:08:12+00:00,,0,6,module: performance module: cuda triaged module: cublas matrix multiplication,True
100842,"Accuracy issues with Jitterated complex kernels for acos, acosh, asin, asinh, tan and tanh module: cuda good first issue triaged module: jiterator",2023-05-07 23:31:41+00:00,,0,4,module: cuda good first issue triaged module: jiterator,True
100838,Dynamo infers different return type vs. eager for `torch.ops.aten` good first issue triaged oncall: pt2 module: decompositions,2023-05-07 22:18:48+00:00,,0,6,good first issue triaged oncall: pt2 module: decompositions,True
100807,[torch.compile] returns output with WRONG SHAPE after `cat_slice_cat` triaged inductor_pattern_match,2023-05-06 18:19:52+00:00,,1,0,triaged inductor_pattern_match,True
100804,Wrong type for `get_lr` inside lr_scheduler.pyi module: optimizer module: typing triaged actionable module: LrScheduler,2023-05-06 16:15:24+00:00,,0,2,module: optimizer module: typing triaged actionable module: LrScheduler,True
100801,There is a performance drop because we have not yet implemented the batching rule for aten::native_dropout_backward triaged actionable module: functorch,2023-05-06 14:21:23+00:00,,0,1,triaged actionable module: functorch,True
100796,[Quant][pt2e] Failed to run pt2e flow on LLaMA oncall: quantization triaged oncall: pt2 module: dynamo,2023-05-06 08:28:03+00:00,,0,20,oncall: quantization triaged oncall: pt2 module: dynamo,False
100795,Quickstart notebook fails to train properly with ROCm module: rocm triaged,2023-05-06 07:50:13+00:00,,0,2,module: rocm triaged,True
100792,inductor cpp wrapper: crash when disable lowmem_dropout triaged oncall: pt2 module: cpu inductor,2023-05-06 05:40:19+00:00,,1,0,triaged oncall: pt2 module: cpu inductor,True
100790,ONNX Opset 16 GridSample Does Not Support 5D Volumetric Input Tensor module: onnx triaged,2023-05-06 04:01:16+00:00,,0,10,module: onnx triaged,False
100785,compile torch2.0 in debug mode triaged topic: build,2023-05-06 03:12:32+00:00,,0,0,triaged topic: build,True
100784,[CUDA RPC] Incorrect results of GPU Tensor transferring using RPC when parallelized with other GPU programs oncall: distributed module: cuda,2023-05-06 02:54:35+00:00,,0,1,oncall: distributed module: cuda,True
100775,[torch.compile] returns NaN for `tensor.mul(big_number).softmax()` triaged oncall: pt2 module: cpu inductor,2023-05-06 00:04:43+00:00,,1,3,triaged oncall: pt2 module: cpu inductor,False
100733,Nightly torch.compile fails with dynamically patched `nn.module.forward` module: nn triaged oncall: pt2 module: dynamo,2023-05-05 18:46:03+00:00,,0,6,module: nn triaged oncall: pt2 module: dynamo,False
100730,`torch::jit::EliminateExceptions` lowering pass never completes on specific model oncall: jit,2023-05-05 17:40:03+00:00,,0,0,oncall: jit,False
100725,[CUDA RPC] Incorrect messages in CUDA Support RPC when parallelized with other GPU programs oncall: distributed,2023-05-05 16:31:24+00:00,,0,0,oncall: distributed,True
100705,torch.cuda.amp.GradScaler initialization  triaged module: amp (automated mixed precision),2023-05-05 10:14:38+00:00,,0,2,triaged module: amp (automated mixed precision),True
100704,[Discussion] Investigate possibilities for Windows Arm64 BLAS and LAPACK module: windows triaged module: linear algebra,2023-05-05 09:58:47+00:00,,0,0,module: windows triaged module: linear algebra,True
100675,DISABLED test_inplace_gradgrad_remainder_cuda_float64 (__main__.TestBwdGradientsCUDA) module: autograd triaged module: flaky-tests skipped,2023-05-05 00:56:58+00:00,,0,1,module: autograd triaged module: flaky-tests skipped,False
100674,Add support for MaxPool3D on the MPS backend triaged module: mps,2023-05-05 00:50:53+00:00,,0,0,triaged module: mps,False
100656,"On UMA systems, pytorch fails to reserve memory exceeding the initial memory size module: rocm module: memory usage triaged",2023-05-04 21:15:44+00:00,,0,0,module: rocm module: memory usage triaged,True
100637,Optimal Batch Size Selection in Torchdynamo Benchmarks for Different GPU Memory Sizes triaged oncall: pt2,2023-05-04 17:30:50+00:00,,0,7,triaged oncall: pt2,False
100636,tracing does not work when torch.distributions is involved oncall: jit,2023-05-04 17:27:22+00:00,,0,0,oncall: jit,False
100626,Will Deep Implicit Models ever become first class citizens in PyTorch? feature module: autograd triaged oncall: pt2 module: functorch module: pt2-dispatcher,2023-05-04 11:30:17+00:00,,0,13,feature module: autograd triaged oncall: pt2 module: functorch module: pt2-dispatcher,False
100617,GPU VRAM usage significantly higher for Lenet5 models when compared to other frameworks triaged better-on-discuss-forum,2023-05-04 06:17:29+00:00,,0,2,triaged better-on-discuss-forum,True
100584,[doc] torch.scalar_tensor doc is missing module: docs triaged,2023-05-03 21:35:09+00:00,,0,0,module: docs triaged,False
100578,Add support for aten::tril_indices for MPS backend  feature triaged module: linear algebra module: mps,2023-05-03 20:42:49+00:00,,0,0,feature triaged module: linear algebra module: mps,True
100574,undocumented error on torch.autograd.Function.jvp for non-Tensor forward returns module: docs module: autograd triaged actionable,2023-05-03 19:30:57+00:00,,0,2,module: docs module: autograd triaged actionable,True
100562,Use a label instead of body text for merge blocking CI SEVs module: ci triaged,2023-05-03 17:33:25+00:00,,0,0,module: ci triaged,True
100561,[ONNX] Opset 18 support for TorchScript exporter module: onnx triaged,2023-05-03 17:24:34+00:00,,0,1,module: onnx triaged,False
100528,Backward hook execution order changes when input.requires_grad is False module: docs module: autograd module: nn triaged actionable,2023-05-03 06:13:43+00:00,,0,3,module: docs module: autograd module: nn triaged actionable,True
100520,DISABLED test_inplace_grad_div_floor_rounding_cuda_float64 (__main__.TestBwdGradientsCUDA) triaged module: flaky-tests skipped oncall: pt2 module: inductor,2023-05-03 03:39:40+00:00,,0,10,triaged module: flaky-tests skipped oncall: pt2 module: inductor,False
100468,"Accuracy repro extraction, constants in graph are not preserved exactly triaged oncall: pt2",2023-05-02 14:02:50+00:00,,1,0,triaged oncall: pt2,False
100461,Arithmetic of single-element Tensors with different dtypes on 'cpu' and 'mps' results in obscure/unhelpful `TypeError` triaged module: mps,2023-05-02 08:40:57+00:00,,0,2,triaged module: mps,True
100459,DISABLED test_wait_i_3 (__main__.TestMultiThreadedWait) oncall: distributed module: flaky-tests skipped,2023-05-02 06:39:22+00:00,,0,8,oncall: distributed module: flaky-tests skipped,False
100448,DISABLED test_wait_i_4 (__main__.TestMultiThreadedWait) oncall: distributed module: flaky-tests skipped,2023-05-02 03:39:24+00:00,,0,7,oncall: distributed module: flaky-tests skipped,False
100425,Higher GPU consumption for Lenet-5 and LSTM models when compared to other frameworks module: cuda module: memory usage triaged,2023-05-01 22:30:02+00:00,,0,1,module: cuda module: memory usage triaged,True
100419,Subgraph rewriter: Unable to match constant args triaged module: dynamo,2023-05-01 21:27:23+00:00,,2,0,triaged module: dynamo,False
100414,Can't export onnx model from a torch script model oncall: jit module: onnx triaged,2023-05-01 20:38:29+00:00,,0,2,oncall: jit module: onnx triaged,True
100411,Sparse Matrix nnz Overflow when casting from COO to CSR module: sparse triaged,2023-05-01 20:06:07+00:00,,0,2,module: sparse triaged,True
100386,Stop importing HuggingFace transformers in DataClassVariable triaged oncall: pt2 module: dynamo module: startup-tracing-compile time,2023-05-01 15:09:55+00:00,,1,1,triaged oncall: pt2 module: dynamo module: startup-tracing-compile time,False
100385,Import setuptools.command.build_ext from torch.utils.cpp_extension somehow indirectly imports Cython when it is installed triaged oncall: pt2 module: startup-tracing-compile time,2023-05-01 15:06:09+00:00,,0,0,triaged oncall: pt2 module: startup-tracing-compile time,True
100378,VecISA.__bool__ is very expensive (nearly a second) on startup triaged oncall: pt2 module: cpu inductor module: startup-tracing-compile time,2023-05-01 14:52:57+00:00,,2,1,triaged oncall: pt2 module: cpu inductor module: startup-tracing-compile time,False
100376,"_sfdp_init is extremely expensive for startup time, even on networks that don't benefit from it triaged oncall: pt2 module: startup-tracing-compile time",2023-05-01 14:11:04+00:00,,0,1,triaged oncall: pt2 module: startup-tracing-compile time,False
100370,[BUG] add 1 to different tensor but get same value triaged module: correctness (silent) module: mps,2023-05-01 10:40:36+00:00,,0,0,triaged module: correctness (silent) module: mps,True
100366,some of the enteries in the previous version of pytorch section are invalid  module: binaries module: docs triaged,2023-05-01 06:30:32+00:00,,0,1,module: binaries module: docs triaged,True
100358,Tensor on shared memory is set to 0 when using concurrent.futures and CUDA oncall: distributed triaged,2023-05-01 05:02:33+00:00,,0,0,oncall: distributed triaged,True
100347,nn.MultiheadAttention doesn't use efficient scaled_dot_product_attention module: docs oncall: transformer/mha,2023-04-30 20:40:06+00:00,,0,2,module: docs oncall: transformer/mha,False
100343,[torch.compile] `sum` out-of-bound read triaged module: cpu inductor,2023-04-30 18:44:23+00:00,,2,8,triaged module: cpu inductor,True
100334,MPS device inference all same value triaged module: mps,2023-04-30 15:11:02+00:00,,0,3,triaged module: mps,False
100320,Compiled function inside vmap triaged oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,2023-04-29 18:39:20+00:00,,0,2,triaged oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,False
100316,[torch.compile] raises RuntimeError in `sdfp_pattern_1` that `Expected size for first two dimensions of batch2 tensor` triaged bug oncall: pt2 module: inductor inductor_pattern_match,2023-04-29 16:28:59+00:00,,0,1,triaged bug oncall: pt2 module: inductor inductor_pattern_match,True
100310,Using ddp training with different machine oncall: distributed module: elastic,2023-04-29 11:32:48+00:00,,0,3,oncall: distributed module: elastic,False
100289,graph._export_onnx() incorrect data types in the binary string representation module: onnx triaged,2023-04-28 22:02:59+00:00,,0,0,module: onnx triaged,True
100278,"Dynamo capture for HigherOrderOperators, followups triaged oncall: pt2 module: dynamo",2023-04-28 20:52:29+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
100270,[Performance] Potential Performance optimization for SDPA feature triaged module: inductor module: multi-headed-attention,2023-04-28 19:06:21+00:00,,1,3,feature triaged module: inductor module: multi-headed-attention,False
100253,profiler.export_stacks doesn't return stack trace unless experimental_config is provided high priority module: regression oncall: profiler,2023-04-28 16:43:13+00:00,,1,13,high priority module: regression oncall: profiler,True
100249,"[discussion] ""TensorList"" as first-class abstraction (including python frontend) and as key for dispatch for merging `torch._foreach_*` into regular `torch.*` functions feature triaged module: nestedtensor",2023-04-28 15:32:25+00:00,,0,6,feature triaged module: nestedtensor,True
100244,torch.utils._content_store will deduplicate storage with identical contents; may be problematic for mutation module: serialization triaged,2023-04-28 14:33:52+00:00,,0,0,module: serialization triaged,True
100241,Interaction of torch.no_grad and torch.autocast context managers with torch.compile triaged oncall: pt2 module: dynamo,2023-04-28 10:50:28+00:00,,0,6,triaged oncall: pt2 module: dynamo,False
100239,torch.compile is not compatible with DPP with torch.nn.SyncBatchNorm.convert_sync_batchnorm() triaged module: ddp module: norms and normalization oncall: pt2 module: inductor module: distributed,2023-04-28 08:55:57+00:00,,0,5,triaged module: ddp module: norms and normalization oncall: pt2 module: inductor module: distributed,True
100223,Add missing `OpInfo`s for prims ops module: tests triaged module: primTorch,2023-04-28 02:14:24+00:00,,0,5,module: tests triaged module: primTorch,True
100220,tensor with dims marked with torch._dynamo.mark_dynamic loses dynamic dim marks after being moved to a different device triaged oncall: pt2 module: dynamic shapes,2023-04-28 00:16:59+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes,False
100158,`torch.sparse_csc_tensor` matrix multiplication produces MKL error SPARSE_STATUS_ALLOC_FAILED when density is too high module: sparse triaged,2023-04-27 09:27:21+00:00,,0,6,module: sparse triaged,True
100156,Illegal instruction in ARM64 (ver 2.0.0) module: binaries triaged module: arm,2023-04-27 08:46:08+00:00,,0,2,module: binaries triaged module: arm,True
100152,DISABLED test_open_device_registration (__main__.TestCppExtensionOpenRgistration) module: cpp-extensions triaged module: flaky-tests skipped,2023-04-27 06:40:10+00:00,,0,5,module: cpp-extensions triaged module: flaky-tests skipped,False
100145,This flag not work : torch.backends.cudnn.allow_tf32 = False   module: cuda triaged,2023-04-27 03:18:10+00:00,,0,2,module: cuda triaged,True
100126,DISABLED test_dtensor_device_mesh_device_conversion (__main__.DTensorMeshTest) oncall: distributed module: flaky-tests skipped,2023-04-26 21:39:51+00:00,,0,3,oncall: distributed module: flaky-tests skipped,False
100125,Error saving MONAI pytorch model to ONNX module: onnx triaged,2023-04-26 21:15:57+00:00,,0,0,module: onnx triaged,True
100123,Error building Pytorch from source module: build module: rocm triaged,2023-04-26 20:57:53+00:00,,0,4,module: build module: rocm triaged,True
100116,'pip install triton' from pinned hash gives unreliable triton module: build triaged oncall: pt2,2023-04-26 19:11:22+00:00,,0,5,module: build triaged oncall: pt2,False
100105,[pt2-functorch] torch.func.functional_call works with func.vmap but breaks for func.grad triaged oncall: pt2 module: functorch module: pt2-dispatcher,2023-04-26 17:06:24+00:00,,0,0,triaged oncall: pt2 module: functorch module: pt2-dispatcher,False
100096,Inductor origins still not accurate feature triaged oncall: pt2 module: inductor,2023-04-26 15:08:37+00:00,,0,3,feature triaged oncall: pt2 module: inductor,False
100087,TransformerEncoderLayer behavior inconsistent between training and evaluation mode oncall: transformer/mha,2023-04-26 12:44:11+00:00,,0,0,oncall: transformer/mha,False
100085,[regression] torch.norm with out dtype bfloat16 cause runtime error triaged module: regression module: norms and normalization,2023-04-26 12:40:00+00:00,,1,3,triaged module: regression module: norms and normalization,True
100080,[Indexing] Incoherent Tensor indexing for nested lists triaged module: numpy module: advanced indexing module: edge cases,2023-04-26 09:43:31+00:00,,0,2,triaged module: numpy module: advanced indexing module: edge cases,True
100075,[compile] output does not match eager mode high priority triaged oncall: pt2 module: functorch module: pt2 accuracy module: pt2-dispatcher,2023-04-26 07:25:38+00:00,,1,22,high priority triaged oncall: pt2 module: functorch module: pt2 accuracy module: pt2-dispatcher,True
100074,DISABLED test_checkpointing_resets_persistent_refs (__main__.CudaGraphTreeTests) triaged module: flaky-tests skipped module: inductor,2023-04-26 06:43:53+00:00,,1,4,triaged module: flaky-tests skipped module: inductor,False
100069,Issue with FSDP + HuggingFace generate triaged module: fsdp,2023-04-26 05:53:42+00:00,,0,14,triaged module: fsdp,True
100062,add github check that diffs generated code triaged module: infra module: codegen,2023-04-26 04:21:33+00:00,,1,3,triaged module: infra module: codegen,True
100061,torch.compile() drops the performance of validation / Dynamo is not guarding on attributes on NN modules triaged ezyang's list oncall: pt2 module: pt2 accuracy,2023-04-26 04:18:21+00:00,,0,18,triaged ezyang's list oncall: pt2 module: pt2 accuracy,False
100055,pre_autograd `make_fx` broken with simple F.linear with symbolic shape triaged oncall: pt2 module: dynamic shapes,2023-04-26 02:17:10+00:00,,0,21,triaged oncall: pt2 module: dynamic shapes,True
100054,Add compile option -Werror=return-type compile error module: build triaged actionable,2023-04-26 02:10:01+00:00,,0,2,module: build triaged actionable,True
100052,nn.Transformer out[0:-1] not precisely equal to last_out when predicting in tgt mask oncall: transformer/mha,2023-04-26 01:57:48+00:00,,0,0,oncall: transformer/mha,False
100051,Issue of HistogramObserver to handle abnormal value oncall: quantization triaged,2023-04-26 01:53:22+00:00,,1,3,oncall: quantization triaged,True
100044,[Tensor Parallel] Clarify docs oncall: distributed,2023-04-25 23:55:50+00:00,,0,0,oncall: distributed,False
100012,Dataloader multiprocess loading with num_worker > 0 calls __main__ file to run module: dataloader triaged,2023-04-25 19:16:45+00:00,,0,1,module: dataloader triaged,True
100006,Revive multigpu testing module: ci triaged,2023-04-25 17:53:01+00:00,,1,3,module: ci triaged,True
100005,torch.triu() may returns wrong values using MPS high priority triaged module: NaNs and Infs module: correctness (silent) module: mps,2023-04-25 17:50:00+00:00,,0,4,high priority triaged module: NaNs and Infs module: correctness (silent) module: mps,True
99999,Runtime Error triaged bug,2023-04-25 17:16:43+00:00,,0,1,triaged bug,True
99994,OpInfo missing for `prims.convert_element_type` triaged oncall: pt2 module: decompositions,2023-04-25 16:52:52+00:00,,0,0,triaged oncall: pt2 module: decompositions,False
99989,Copying an MPS tensor to a CPU tensor using a for loop fails triaged module: mps,2023-04-25 16:01:32+00:00,,0,0,triaged module: mps,True
99982,torch.cuda.is_available() crashes python in systems with disabled gpu module: crash module: cuda triaged module: edge cases,2023-04-25 12:46:39+00:00,,0,3,module: crash module: cuda triaged module: edge cases,True
99981,Group Norm crashes on Apple M1/MPS devices for versions 2.0+ needs reproduction triaged module: regression module: mps,2023-04-25 10:24:28+00:00,,0,8,needs reproduction triaged module: regression module: mps,True
99979,I encountered an error while trying to save the stylegan2 network as torch. onnx. export module: onnx triaged,2023-04-25 09:47:03+00:00,,0,0,module: onnx triaged,True
99978,torch.jit.trace can not trace buffer by Module.register_buffer() when use DDP Module. oncall: distributed,2023-04-25 09:26:36+00:00,,0,3,oncall: distributed,True
99949,[inductor] Autotuning leads to non determinism feature triaged oncall: pt2 module: inductor,2023-04-25 00:37:15+00:00,,0,1,feature triaged oncall: pt2 module: inductor,False
99932,FSDP + gradient clipping raises an odd warning with the simplest model on torch 2.0 oncall: distributed triaged module: fsdp,2023-04-24 22:17:50+00:00,,1,2,oncall: distributed triaged module: fsdp,True
99923,benchmarks/dynamo/ci_expected_accuracy/update_expected.py truncates file if only one shard succeeds triaged module: benchmark oncall: pt2,2023-04-24 20:04:48+00:00,,1,1,triaged module: benchmark oncall: pt2,False
99922,ciflow/inductor should run both inference and training even if inference fails module: ci triaged module: inductor module: devx,2023-04-24 20:04:05+00:00,,0,1,module: ci triaged module: inductor module: devx,True
99918,[RFC] DebugMode triaged,2023-04-24 19:22:11+00:00,,0,5,triaged,True
99908,Deprecate torch.distributed.algorithms._optimizer_overlap oncall: distributed,2023-04-24 19:07:40+00:00,,0,0,oncall: distributed,False
99903,Can the CUDA device LUID be exposed as part of _CudaDeviceProperties? module: cuda triaged enhancement actionable needs design,2023-04-24 18:14:52+00:00,,0,4,module: cuda triaged enhancement actionable needs design,True
99893,Many models are failing on periodic dynamic shape benchmark tests dynamic_aot_eager triaged oncall: pt2 module: dynamic shapes,2023-04-24 17:15:38+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes,True
99873,Dynamo config patching in our code is brittle triaged oncall: pt2 module: dynamo oncall: export,2023-04-24 13:03:11+00:00,,0,2,triaged oncall: pt2 module: dynamo oncall: export,False
99866,Logs output_code and inductor do not interact as expected module: logging triaged,2023-04-24 08:21:46+00:00,,0,0,module: logging triaged,True
99852,Slight numerical divergence between torch.compile and eager; shows up in practice on yolov3 triaged oncall: pt2 module: pt2 accuracy,2023-04-24 02:00:32+00:00,,0,3,triaged oncall: pt2 module: pt2 accuracy,False
99836,NTK notebook calculates wrong object - wrong output dimensions triaged module: functorch,2023-04-23 19:44:48+00:00,,0,0,triaged module: functorch,True
99825,"When backend is nccl, the distribution group type generated by Pytorch 2.0 shoule be ProcessGroupNCCL, but is ProcessGroup oncall: distributed",2023-04-23 12:58:05+00:00,,0,1,oncall: distributed,False
99821,Tracer cannot infer type of Seq2SeqLMOutput oncall: jit,2023-04-23 09:56:25+00:00,,0,1,oncall: jit,False
99812,cuda.is_available() error module: cuda triaged,2023-04-23 06:10:24+00:00,,0,1,module: cuda triaged,True
99807,AOTAutograd/Inductor file system cache feature triaged oncall: pt2 module: inductor,2023-04-23 02:02:03+00:00,,0,3,feature triaged oncall: pt2 module: inductor,False
99806,`cat` gradgrad tests failing module: autograd triaged module: edge cases,2023-04-23 00:36:14+00:00,,0,3,module: autograd triaged module: edge cases,True
99802,torch.multinomial() always returns [0] using MPS triaged module: mps,2023-04-22 19:36:19+00:00,,0,3,triaged module: mps,True
101073,Windows fatal exception: stack overflow while using pytorch for computing triaged module: functorch,2023-04-22 19:24:34+00:00,,0,0,triaged module: functorch,True
99797,Automatic broadcasting for sparse csr tensors module: sparse triaged,2023-04-22 16:39:17+00:00,,0,2,module: sparse triaged,True
99794,Apple metal (MPS)  device returning incorrect keypoints for YOLOv8 pose estimation model  high priority module: binaries oncall: releng triaged module: correctness (silent) module: mps,2023-04-22 11:08:37+00:00,,1,2,high priority module: binaries oncall: releng triaged module: correctness (silent) module: mps,True
99790,Cannot compile torch 1.10 in CentOS 7.3 triaged,2023-04-22 06:05:05+00:00,,0,2,triaged,True
99781,2.0.0+cu118 package missing proper libnvrtc-builtins.so.11.8 module: binaries module: cpp triaged,2023-04-22 01:17:29+00:00,,0,3,module: binaries module: cpp triaged,True
99774,"RuntimeError: Cannot call sizes() on tensor with symbolic sizes/strides w/ `dynamo.export`, `make_fx` and `functionalize` triaged module: functionalization oncall: pt2 oncall: export module: pt2-dispatcher",2023-04-21 23:37:12+00:00,,0,12,triaged module: functionalization oncall: pt2 oncall: export module: pt2-dispatcher,False
99770,Deformable Convolution export to onnx module: onnx triaged,2023-04-21 22:49:27+00:00,,0,0,module: onnx triaged,False
99722,cuda 12.0 support request for building pytorch from source code module: build module: cuda triaged enhancement,2023-04-21 15:30:05+00:00,,0,3,module: build module: cuda triaged enhancement,True
99715,no-duplicate-decl-specifier as a invalid compile flag for CXX in GCC module: build module: rocm triaged,2023-04-21 11:59:26+00:00,,0,0,module: build module: rocm triaged,True
99710,pca_lowrank and svd_lowrank broken under automatic mixed precision. module: cuda triaged module: half module: linear algebra module: amp (automated mixed precision),2023-04-21 09:39:09+00:00,,0,6,module: cuda triaged module: half module: linear algebra module: amp (automated mixed precision),True
99701,"when convert to onnx with dynamix_axis,  the Reshape op  value is always the same as static,  dynamic_axis is useless, it cant't inference right shape dynamically module: onnx triaged",2023-04-21 06:28:45+00:00,,0,6,module: onnx triaged,False
99693,"WARNING: The shape inference of prim::PadPacked type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. oncall: jit onnx-triaged",2023-04-21 03:29:24+00:00,,0,2,oncall: jit onnx-triaged,True
99690,"gpu training  work well, but cpu training not work module: cpu triaged module: fft",2023-04-21 03:10:51+00:00,,0,5,module: cpu triaged module: fft,True
99684,In torchelastic support running worker rank 0 on agent rank 0 consistently oncall: distributed triaged module: elastic,2023-04-21 02:08:22+00:00,,0,0,oncall: distributed triaged module: elastic,True
99681,`torch.ops.aten.empty` is not discoverable from `dir(torch.ops.aten)` until explicitly calling getattr triaged module: library,2023-04-21 00:32:10+00:00,,0,8,triaged module: library,True
99652,DistributedDataParallel doesn't work with complex buffers oncall: distributed module: complex,2023-04-20 19:17:12+00:00,,0,1,oncall: distributed module: complex,True
99649,[torch.compile] raises an error that expanded size doesn't match when enabling `shape_padding` triaged bug oncall: pt2 module: inductor,2023-04-20 18:49:06+00:00,,0,1,triaged bug oncall: pt2 module: inductor,True
99640,Ban GradScaler scale from being less than 1 triaged module: amp (automated mixed precision),2023-04-20 17:12:52+00:00,,0,11,triaged module: amp (automated mixed precision),True
99637,Torch hangs at import if tensorflow is imported first module: binaries triaged,2023-04-20 16:37:15+00:00,,0,4,module: binaries triaged,True
99630,Parameterisation of MultivariateNormal distribution using Cholesky decomposition of precision matrix module: distributions feature triaged,2023-04-20 15:18:05+00:00,,0,3,module: distributions feature triaged,True
99625,Conda Pytorch set processor affinity to the first physical core after fork high priority module: dependency bug module: binaries triaged module: mkl module: third_party module: intel,2023-04-20 12:49:19+00:00,,0,18,high priority module: dependency bug module: binaries triaged module: mkl module: third_party module: intel,True
99615,CUPTI Initialization error  module: cuda triaged oncall: profiler,2023-04-20 09:37:44+00:00,,0,29,module: cuda triaged oncall: profiler,True
99614,Make broadcast_coalesced to a op for processgroup oncall: distributed,2023-04-20 08:54:46+00:00,,1,1,oncall: distributed,False
99584,Training Faster R-CNN model with COCO dataset has been consistently unsuccessful. module: dataloader triaged,2023-04-20 00:49:49+00:00,,0,3,module: dataloader triaged,True
99562,lintrunner mypy raises error in numpy module: lint triaged,2023-04-19 21:16:25+00:00,,1,4,module: lint triaged,True
99561,Pytorch mobile crashes on Android when loading a custom model module: android oncall: mobile,2023-04-19 21:03:12+00:00,,0,0,module: android oncall: mobile,False
99556,torch.func.jacrev fails if model contains full_backward_hook module: autograd triaged module: functorch,2023-04-19 19:09:46+00:00,,0,0,module: autograd triaged module: functorch,True
99558,Batching rule not implemented for aten::narrow.Tensor triaged module: functorch,2023-04-19 17:56:27+00:00,,0,5,triaged module: functorch,True
99544,Cross compile Pytorch for ARM in Bazel module: build triaged enhancement module: arm,2023-04-19 17:05:53+00:00,,0,3,module: build triaged enhancement module: arm,True
99573,"Jacfwd become slower after update pytorch (""We’ve integrated functorch into PyTorch---Documentation"") high priority needs reproduction triaged module: functorch",2023-04-19 16:04:43+00:00,,0,3,high priority needs reproduction triaged module: functorch,True
99515,Support polyphase channelizer feature triaged module: fft,2023-04-19 07:59:08+00:00,,0,4,feature triaged module: fft,True
99509,'Illegal instruction (core dumped)' for gpt-j bf16 generation task using greedy search  module: crash module: cpu triaged module: intel,2023-04-19 04:24:07+00:00,,1,15,module: crash module: cpu triaged module: intel,True
99455,Not Preserving Grad For Tensor Created Inside torch.compile triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-04-18 20:24:53+00:00,,0,6,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
99438,vision_maskrcnn failing on periodic dynamic_aot_eager_torchbench triaged oncall: pt2 module: dynamic shapes,2023-04-18 18:32:57+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes,True
99432,[DTensor] parallelize_module failed with nn.Transformer and the PairwiseParallel plan oncall: distributed triaged,2023-04-18 17:39:17+00:00,,0,1,oncall: distributed triaged,True
99421,Question about GRU(RNN/LSTM) outputs shape module: docs module: nn module: rnn triaged actionable,2023-04-18 15:15:34+00:00,,1,2,module: docs module: nn module: rnn triaged actionable,True
99414,The meta implementation of `index_put` does not do any check triaged module: meta tensors release notes: Meta API,2023-04-18 12:45:34+00:00,,0,0,triaged module: meta tensors release notes: Meta API,True
99410,"torch.nn.functional.multilabel_margin_loss cuda lacks checking of ""out of bound"" module: nn module: cuda module: error checking triaged",2023-04-18 11:41:22+00:00,,0,0,module: nn module: cuda module: error checking triaged,True
99407,Torch.fx.symbolic_trace removes some of the keys from module state_dict triaged module: fx,2023-04-18 09:43:44+00:00,,0,0,triaged module: fx,True
99404,FakeTensor lacks support for sparse compressed tensors module: sparse triaged module: fakeTensor,2023-04-18 07:34:49+00:00,,0,1,module: sparse triaged module: fakeTensor,True
99397,Internal errors with cuda graph (CUBLAS_STATUS_NOT_INITIALIZED and jit failure) triaged module: cuda graphs,2023-04-18 03:39:21+00:00,,0,1,triaged module: cuda graphs,True
99390,torch.compile error triaged oncall: pt2 module: cpu inductor,2023-04-18 02:23:53+00:00,,1,2,triaged oncall: pt2 module: cpu inductor,True
99372,PyTorch 2.0.0 encountered CUDA error: an illegal memory access was encountered module: cuda triaged module: multithreading,2023-04-17 21:51:17+00:00,,0,4,module: cuda triaged module: multithreading,True
99359,[c++17] Replace lock_guard with scoped_lock  module: internals triaged,2023-04-17 18:52:31+00:00,,0,0,module: internals triaged,True
99352,add `-std=c++20` build-only CI job module: build module: ci triaged module: devx,2023-04-17 17:25:08+00:00,,0,1,module: build module: ci triaged module: devx,True
99351,we should make semantically meaningless positional arguments positional only in our operator API feature triaged actionable module: python array api module: python frontend,2023-04-17 17:18:10+00:00,,1,8,feature triaged actionable module: python array api module: python frontend,True
99316,torch.linalg.lstsq doc arguments error module: docs triaged module: linear algebra,2023-04-17 13:58:58+00:00,,0,4,module: docs triaged module: linear algebra,True
99311,Functorch pytrees with custom iterables triaged enhancement module: pytree module: functorch,2023-04-17 13:40:50+00:00,,0,3,triaged enhancement module: pytree module: functorch,True
99310,Torch func Documentation for trees module: docs triaged module: functorch,2023-04-17 13:33:08+00:00,,0,0,module: docs triaged module: functorch,True
99305,CI for s390x module: ci triaged enhancement,2023-04-17 10:47:51+00:00,,0,4,module: ci triaged enhancement,True
99304,Need TransformerEncoder to output attention map oncall: transformer/mha topic: new features,2023-04-17 10:19:30+00:00,,0,3,oncall: transformer/mha topic: new features,False
99299,There has implmenet bug in LTC IrBuilder's MakeSizeMul method. triaged lazy module: lazy,2023-04-17 09:28:30+00:00,,1,1,triaged lazy module: lazy,True
99297,Slicing and indexing support negative steps feature triaged module: advanced indexing,2023-04-17 09:09:30+00:00,,0,2,feature triaged module: advanced indexing,True
99293,Automatically set dropout for SDPA depending on training mode / `training` argument enhancement oncall: transformer/mha,2023-04-17 07:30:30+00:00,,0,2,enhancement oncall: transformer/mha,False
99287,Add `TORCH_ASSERT_ONLY_METHOD_OPERATORS` to functorch codebase triaged better-engineering module: functorch,2023-04-17 05:13:44+00:00,,0,3,triaged better-engineering module: functorch,True
99278,Build error on libstc++ header stl_alogbase.h on riscv module: build good first issue triaged,2023-04-16 21:21:35+00:00,,0,6,module: build good first issue triaged,True
99270,Remove lr_scheduler.print_lr module: optimizer triaged module: LrScheduler,2023-04-16 18:19:35+00:00,,0,1,module: optimizer triaged module: LrScheduler,True
99268,Embedding layer tensor shape needs reproduction triaged module: embedding,2023-04-16 17:36:10+00:00,,0,1,needs reproduction triaged module: embedding,True
99265,the error message of torch.addcmul is wrong module: error checking triaged,2023-04-16 13:44:32+00:00,,1,2,module: error checking triaged,True
99248,tools PYTHONPATH trick in run_test.py does not work reliably triaged module: testing,2023-04-16 01:48:27+00:00,,0,0,triaged module: testing,True
99247,Broken link for torch dynamo FAQ in docs triaged topic: docs,2023-04-15 21:52:38+00:00,,0,0,triaged topic: docs,False
99230,vision_maskrcnn failing in periodic/trunk triaged bug oncall: pt2,2023-04-15 14:05:52+00:00,,1,1,triaged bug oncall: pt2,True
99225,Libtorch consumes too much memory as 16225 module: cpp module: memory usage triaged,2023-04-15 07:15:22+00:00,,0,5,module: cpp module: memory usage triaged,True
99218,Sporadic CUDA error in `test_nccl_warn_not_in_group_debug_detail` oncall: distributed,2023-04-15 02:45:51+00:00,,0,1,oncall: distributed,True
99201,opacus_cifar10 fails in dynamo due to hooks  triaged bug oncall: pt2 module: dynamo,2023-04-14 21:04:26+00:00,,2,2,triaged bug oncall: pt2 module: dynamo,True
99200,Unused `import torch` followed by `cuml.NearestNeighbors` leads to nondeterministic segfault (during Python process exit?) needs reproduction module: crash triaged,2023-04-14 20:48:19+00:00,,0,2,needs reproduction module: crash triaged,True
99176,Add Debug builds for python with pydebug triaged module: devx,2023-04-14 18:01:20+00:00,,1,3,triaged module: devx,True
99160,"Run ChatRWKV on MBP(intel CPU)+eGPU[rx6800 16G], returna a very big number -9223372036854775808, looks like overflow triaged module: mps",2023-04-14 15:49:46+00:00,,0,1,triaged module: mps,True
99155,TORCH_COMPILE_ABLATE envvar low priority triaged enhancement oncall: pt2 module: dynamo,2023-04-14 15:00:21+00:00,,1,4,low priority triaged enhancement oncall: pt2 module: dynamo,False
99149,"Spectral Normalization can not be applied to Conv{1,2,3}d module: nn triaged needs research",2023-04-14 13:59:29+00:00,,1,0,module: nn triaged needs research,True
99147,`torch.sparse.sum` backward fails when reducing over dense dimensions. module: sparse module: autograd triaged,2023-04-14 12:53:28+00:00,,1,2,module: sparse module: autograd triaged,True
99143,No documentation to show how to implement aten::view for custom backend module: cpp-extensions module: docs triaged,2023-04-14 11:36:09+00:00,,0,0,module: cpp-extensions module: docs triaged,True
99142,"More Nested Tensor Functionality (layer_norm, cross_entropy / log_softmax&nll_loss) triaged module: nestedtensor topic: new features",2023-04-14 11:02:52+00:00,,0,1,triaged module: nestedtensor topic: new features,False
99140,"Why nn.Upsample/F.interpolate followed by nn.InstanceNorm2d will report error ""Unsupported: ONNX export of instance_norm for unknown channel size."" module: onnx triaged module: norms and normalization",2023-04-14 10:02:54+00:00,,1,1,module: onnx triaged module: norms and normalization,True
99138,torch.cuda.is_available() return False module: docs module: cuda triaged,2023-04-14 09:20:35+00:00,,0,4,module: docs module: cuda triaged,True
99126,DISABLED test_fake_crossref_backward_no_amp_index_fill_cuda_float32 (__main__.TestFakeTensorCUDA) triaged module: flaky-tests skipped module: unknown module: fakeTensor module: pt2-dispatcher,2023-04-14 03:39:53+00:00,,0,2,triaged module: flaky-tests skipped module: unknown module: fakeTensor module: pt2-dispatcher,False
99107,Invalid Reference to Class oncall: quantization low priority triaged topic: docs,2023-04-13 23:53:05+00:00,,1,1,oncall: quantization low priority triaged topic: docs,True
99082,Look into test coverage for `UntypedStorage` module: typing module: tests triaged,2023-04-13 21:02:55+00:00,,1,0,module: typing module: tests triaged,True
99042,Memory allocation issues in distributions.multivariate_normal.MultivariateNormal module: distributions triaged,2023-04-13 15:37:11+00:00,,0,2,module: distributions triaged,True
99037,AttributeError: type object 'torch._C._profiler.ProfilerActivity' has no attribute 'MPS' triaged oncall: profiler module: mps,2023-04-13 14:15:08+00:00,,1,2,triaged oncall: profiler module: mps,True
99035,Issue on building from source: Remove -mfpu=neon option on MacOS with Apple silicon module: build triaged,2023-04-13 12:59:46+00:00,,0,1,module: build triaged,True
99025,Is there a way to get the full call stack of pytorch from python to C/C++? triaged,2023-04-13 09:37:21+00:00,,0,5,triaged,True
99023,Dtype changes while going from FX graph -> Torchscript triaged FX-TorchScript Compatibility module: fx,2023-04-13 07:49:55+00:00,,0,0,triaged FX-TorchScript Compatibility module: fx,True
99012,"[BUG]Float32 attention mask not working with torch.autocast(""cpu"") triaged oncall: transformer/mha",2023-04-13 03:40:26+00:00,,0,0,triaged oncall: transformer/mha,True
99007,create_graph_input and add_grapharg should be combined into one function triaged module: dynamo,2023-04-13 02:56:04+00:00,,1,3,triaged module: dynamo,False
98978,[torch.compile] makes `linear(permute(input))` succeed for integer input in `torch.no_grad` context triaged bug module: inductor,2023-04-12 21:36:26+00:00,,0,1,triaged bug module: inductor,True
98977,[BE] Dedup the functorch skipOps mechanism and the common_method_invocations one triaged module: testing,2023-04-12 21:30:38+00:00,,0,0,triaged module: testing,True
98976,Sparse Tensor: in-place operation on detached tensors no longer raised error module: sparse triaged,2023-04-12 21:13:05+00:00,,1,5,module: sparse triaged,True
98970,[torch.compile] `replace_fx`  triaged bug module: inductor,2023-04-12 20:45:41+00:00,,0,2,triaged bug module: inductor,True
98955,Please verify 1.14.0 ONNX release candidate on TestPyPI  module: onnx triaged,2023-04-12 18:55:41+00:00,,1,3,module: onnx triaged,False
98948,behaviour of `torch.tensor()` changes after editing `Tensor.__getitem__` triaged module: python frontend,2023-04-12 16:53:22+00:00,,0,9,triaged module: python frontend,True
98947,Add `torch.cat`  support for torch native sparse tensors. (Need for PyG) module: sparse feature triaged,2023-04-12 16:36:04+00:00,,0,10,module: sparse feature triaged,False
98942,[torch.fx] Upgrade on node info triaged module: fx,2023-04-12 15:36:22+00:00,,0,2,triaged module: fx,True
98939,"torch.dist with minus norm returns tensor(0.), while with -inf can return result module: distributions triaged",2023-04-12 15:19:18+00:00,,0,2,module: distributions triaged,True
98937,TracingContext.get().frame_summary_stack doesn't produce full stack trace triaged module: dynamo,2023-04-12 14:49:19+00:00,,0,0,triaged module: dynamo,True
98929,torch.sparse_csr_tensor() stops gradients module: sparse triaged,2023-04-12 09:54:46+00:00,,1,8,module: sparse triaged,True
98925,Request for deterministic support for reflection_pad2d_backward_cuda module: cuda triaged enhancement module: padding,2023-04-12 08:49:07+00:00,,0,1,module: cuda triaged enhancement module: padding,True
98924,Integrate open device privateuse1 customized method registration triaged module: backend,2023-04-12 08:28:07+00:00,,0,1,triaged module: backend,True
98921,Unable to load MultiStepLR with torch.load(weights_only=True)  module: serialization triaged,2023-04-12 07:57:32+00:00,,0,0,module: serialization triaged,True
98917,Change module to module_ in torch/csrc/api/include/torch/python.h module: build triaged,2023-04-12 07:05:19+00:00,,0,2,module: build triaged,True
98907,Move template code to header module: cpp triaged,2023-04-12 03:32:14+00:00,,0,0,module: cpp triaged,True
98904,Test failure: TestCommonCPU.test_python_ref__refs_abs_cpu_complex32 module: tests triaged,2023-04-12 02:36:27+00:00,,0,2,module: tests triaged,True
98888,Changes to TorchScript autodiff changing default behavior are no longer accepted triage review oncall: jit,2023-04-11 23:59:42+00:00,,0,1,triage review oncall: jit,True
98882,[PT2] AOTAutograd de-dups but skips de-dup guards for DDP triaged oncall: pt2,2023-04-11 22:12:36+00:00,,1,11,triaged oncall: pt2,False
98875,DISABLED test_all_to_all_1d (__main__.DeviceMeshCollectiveTest) oncall: distributed module: flaky-tests skipped,2023-04-11 21:39:22+00:00,,1,4,oncall: distributed module: flaky-tests skipped,False
98872,Expand component configurable logging system to C++ module: logging triaged,2023-04-11 20:58:05+00:00,,1,4,module: logging triaged,True
98871,Document the user-facing API for the component-level logging system module: docs triaged,2023-04-11 20:54:19+00:00,,1,0,module: docs triaged,True
98864,Support SPDA on non-CUDA backends oncall: transformer/mha,2023-04-11 20:17:36+00:00,,0,2,oncall: transformer/mha,False
98863,Problem with instalation torch2 on a100+cu12.1 module: binaries module: cuda triaged,2023-04-11 19:52:39+00:00,,0,4,module: binaries module: cuda triaged,True
98861,Sparse Tensor not working for `torch.cat` module: sparse triaged,2023-04-11 19:00:20+00:00,,0,6,module: sparse triaged,True
98860,Sharded Grad Scaler Issue Tracker oncall: distributed triaged module: amp (automated mixed precision) module: fsdp,2023-04-11 18:51:06+00:00,,1,0,oncall: distributed triaged module: amp (automated mixed precision) module: fsdp,True
98844,[PT2] Some errors with `cond` and `torch.compile` triaged oncall: pt2 module: functorch module: pt2-dispatcher,2023-04-11 14:07:08+00:00,,0,8,triaged oncall: pt2 module: functorch module: pt2-dispatcher,False
98836,PyTorch's packaged libgomp causes significant performance penalties on CPU when used together with other Python packages module: build triaged module: multithreading,2023-04-11 10:57:50+00:00,,0,8,module: build triaged module: multithreading,True
98827,[functorch] vmap_hessian_fc - fails under torch.compile triaged oncall: pt2 module: functorch module: pt2-dispatcher,2023-04-11 07:01:07+00:00,,1,1,triaged oncall: pt2 module: functorch module: pt2-dispatcher,True
98825,[functorch] functorch_maml_omniglot - fails under torch.compile triaged oncall: pt2 module: functorch module: pt2-dispatcher,2023-04-11 06:54:23+00:00,,1,3,triaged oncall: pt2 module: functorch module: pt2-dispatcher,True
98822,[functorch] torch.compile - functorch transforms Interaction triaged oncall: pt2 module: functorch module: pt2-dispatcher,2023-04-11 06:40:59+00:00,,1,0,triaged oncall: pt2 module: functorch module: pt2-dispatcher,False
98817,[FSDP] summon_full_params with_grad=True CPU offload can crash oncall: distributed triaged module: fsdp,2023-04-11 05:01:22+00:00,,1,0,oncall: distributed triaged module: fsdp,True
98816,File-level retry enhancements triaged module: devx,2023-04-11 04:58:57+00:00,,1,0,triaged module: devx,True
98814,autocast does not work properly on embedding module triaged module: amp (automated mixed precision),2023-04-11 04:35:21+00:00,,0,4,triaged module: amp (automated mixed precision),True
98808,[FSDP] move up the first all gather oncall: distributed triaged module: fsdp,2023-04-11 01:53:37+00:00,,0,0,oncall: distributed triaged module: fsdp,True
98805,Discrepancy of supported Python versions between Get Started page and index of pre-built binaries for PIP installation module: docs triaged,2023-04-11 01:37:52+00:00,,0,0,module: docs triaged,True
98792,DataLoader doesn't accept non-cpu device for loading.  module: dataloader triaged,2023-04-10 22:32:05+00:00,,0,2,module: dataloader triaged,True
98768,[SPMD] DistCompiler graph optimization improvement oncall: distributed triaged,2023-04-10 18:21:17+00:00,,1,0,oncall: distributed triaged,False
98727,Pytorch member variable not working after converting to onnx format module: onnx triaged,2023-04-10 06:52:36+00:00,,1,11,module: onnx triaged,False
98724,Conflict between ``torch.func`` transformations and ``torch.jit.trace`` triaged module: functorch,2023-04-10 05:02:52+00:00,,0,2,triaged module: functorch,True
98707,Ubuntu 22.04 LTS issue <built-in function load_binary> returned NULL without setting an exception module: rocm triaged oncall: pt2,2023-04-09 22:18:38+00:00,,1,13,module: rocm triaged oncall: pt2,True
98695,Torchscript: Name Mangling prevents Type Refinement oncall: jit,2023-04-09 16:49:20+00:00,,0,0,oncall: jit,False
98678,DISABLED test_gradgrad_nn_GroupNorm_cuda_float64 (__main__.TestModuleCUDA) module: nn triaged skipped,2023-04-08 17:00:10+00:00,,0,4,module: nn triaged skipped,False
98677,DISABLED test_grad_nn_GroupNorm_cuda_float64 (__main__.TestModuleCUDA) module: nn triaged skipped,2023-04-08 16:58:57+00:00,,0,1,module: nn triaged skipped,False
98675,torch.matmul with batched CSR matrix module: sparse triaged,2023-04-08 16:29:08+00:00,,0,7,module: sparse triaged,True
98673,[ux] Non-blocking tensor constructors triaged enhancement has workaround module: tensor creation,2023-04-08 12:45:28+00:00,,0,0,triaged enhancement has workaround module: tensor creation,True
98668,Cannot use `checkpoint_sequential` with `torch.compile` triaged oncall: pt2,2023-04-08 06:01:09+00:00,,1,2,triaged oncall: pt2,False
98617,Add test/distributed/test_c10d_mpi.py oncall: distributed triaged,2023-04-07 19:42:01+00:00,,0,0,oncall: distributed triaged,False
98600,Wrong illustration in README.md module: docs triaged,2023-04-07 15:32:27+00:00,,0,1,module: docs triaged,True
98587,Cannot use AT_CUDA_DRIVER_CHECK from user code module: build module: cpp-extensions module: internals module: cuda triaged,2023-04-07 09:51:24+00:00,,0,2,module: build module: cpp-extensions module: internals module: cuda triaged,True
98566,`F.interpolate` and `F.grid_sample` - documentation error and bug module: docs triaged,2023-04-07 02:30:08+00:00,,0,5,module: docs triaged,True
98561,Tracker - Failing models in the torch.compile dashboard triaged oncall: pt2,2023-04-07 00:18:32+00:00,,1,1,triaged oncall: pt2,False
98557,torch.jit.script codegen warning with cuda and vmap oncall: jit triaged module: functorch,2023-04-06 23:43:55+00:00,,0,3,oncall: jit triaged module: functorch,True
98542,Training runs 50% slower when using 2 GPUs comparing to 1 oncall: distributed,2023-04-06 21:20:30+00:00,,0,0,oncall: distributed,False
98541,Memory corruption using torch.ops.* to access re-registered operator module: internals triaged,2023-04-06 21:19:23+00:00,,0,0,module: internals triaged,True
98537,Segfault when using torch.ops.* to access de-registered op module: crash triaged module: dispatch module: library,2023-04-06 20:29:49+00:00,,0,3,module: crash triaged module: dispatch module: library,True
98533,Dynamo compiled graph gets overwritten by eager in a data dependent branch when False branch is empty triaged oncall: pt2,2023-04-06 20:07:03+00:00,,2,8,triaged oncall: pt2,False
98515,torch.cond should work with expressions involving SymInt triaged module: functorch,2023-04-06 16:52:27+00:00,,1,1,triaged module: functorch,True
98503,Power VSX vectorization support disabled module: build triaged module: regression module: POWER,2023-04-06 14:13:18+00:00,,0,2,module: build triaged module: regression module: POWER,True
98499,`torch.nn.utils.rnn.unpad_sequence` modifies arguments in-place module: docs module: rnn triaged,2023-04-06 13:03:23+00:00,,0,0,module: docs module: rnn triaged,True
98498,"Higher order derivatives not working when setting compute device to `torch.device(""mps"")` module: autograd triaged module: mps",2023-04-06 12:36:57+00:00,,0,9,module: autograd triaged module: mps,False
98497,[onnx]Unsupported: ONNX export of convolution for kernel of unknown shape module: onnx triaged,2023-04-06 12:16:13+00:00,,1,7,module: onnx triaged,False
98495,Strided to batch BSR/BSC conversion fails when the number of zeros per block varies while the number of blocks per patch is constant module: sparse triaged,2023-04-06 11:28:56+00:00,,1,3,module: sparse triaged,True
98487,torch.fx.GraphModule inside custom backend has `training` attribute always set to `True` regardless of the user settings triaged oncall: pt2,2023-04-06 08:02:12+00:00,,1,0,triaged oncall: pt2,False
98486,Options are not forwarded to the custom backend triaged oncall: pt2,2023-04-06 07:44:40+00:00,,1,0,triaged oncall: pt2,False
98484,Improvements to FSDP debugability oncall: distributed triaged module: fsdp,2023-04-06 06:54:50+00:00,,0,0,oncall: distributed triaged module: fsdp,True
98481,Bring CudaPluggableAllocator to feature parity with the Native Allocator module: internals module: cuda triaged module: CUDACachingAllocator,2023-04-06 04:43:37+00:00,,0,6,module: internals module: cuda triaged module: CUDACachingAllocator,True
98467,tacotron2 times out triaged bug oncall: pt2 module: inductor,2023-04-06 00:47:45+00:00,,0,1,triaged bug oncall: pt2 module: inductor,True
98465,Need better error message when a merge cancelled because of timeout module: ci triaged,2023-04-05 23:54:00+00:00,,0,0,module: ci triaged,True
98459,Fail to pass test HAVE_XXX_REGEX while  building pytorch  module: build triaged,2023-04-05 22:56:34+00:00,,0,2,module: build triaged,True
98456,README could use link to governance high priority module: docs triaged,2023-04-05 22:32:42+00:00,,0,3,high priority module: docs triaged,True
98441,Torch Compile is slightly slower than eager mode. triaged oncall: pt2,2023-04-05 19:42:05+00:00,,0,5,triaged oncall: pt2,False
98434,assert callable(unaltered_fn) high priority triaged oncall: pt2,2023-04-05 18:52:55+00:00,,1,3,high priority triaged oncall: pt2,True
98422,[FX] Symbolic trace over `torch.Tensor.${fn}` APIs oncall: fx,2023-04-05 16:34:29+00:00,,0,0,oncall: fx,False
98419,Support backward hook optimizers in FSDP oncall: distributed triaged module: fsdp,2023-04-05 15:59:27+00:00,,0,4,oncall: distributed triaged module: fsdp,False
98416,Backwards graph is labeled incorrectly when dynamic=True triaged oncall: pt2 module: dynamic shapes,2023-04-05 14:19:49+00:00,,0,2,triaged oncall: pt2 module: dynamic shapes,True
98414,"PyTorch 1.12, high failure rate for test_optim/test_nadam module: optimizer triaged",2023-04-05 14:04:46+00:00,,0,13,module: optimizer triaged,True
98413,TORCH_COMPILE_DEBUG and TORCH_LOGS interact badly triaged oncall: pt2,2023-04-05 14:03:54+00:00,,1,2,triaged oncall: pt2,False
98409,`torch.Tensor.layout` is not documented module: docs triaged module: python frontend,2023-04-05 13:45:51+00:00,,0,1,module: docs triaged module: python frontend,True
98406,Contribute to the privateuse1 backend. module: internals triaged module: backend,2023-04-05 13:04:15+00:00,,0,8,module: internals triaged module: backend,True
98386,[PTD][Checkpoint] Enable single_file_per_rank for fsspec storage read/write oncall: distributed triaged,2023-04-05 05:56:34+00:00,,1,0,oncall: distributed triaged,True
98361,pip doesn't install the right version of pytorch when torchtext is involved module: binaries triaged,2023-04-04 23:13:44+00:00,,0,6,module: binaries triaged,True
98355,Intermittent failure of mobilenet_v3_large triaged module: flaky-tests oncall: pt2,2023-04-04 22:20:41+00:00,,1,0,triaged module: flaky-tests oncall: pt2,True
98338,[functorch] [vmap] tests fail when `_set_vmap_fallback_enabled(False)`. triaged module: functorch,2023-04-04 19:07:09+00:00,,0,0,triaged module: functorch,True
98296,"""We don't have an op for aten::bitwise_and but it isn't a special case."" when exporting NMS operation as ONNX. oncall: jit module: onnx",2023-04-04 12:52:38+00:00,,0,1,oncall: jit module: onnx,False
98291,Make BetterTransformer implementation non-blocking oncall: transformer/mha,2023-04-04 10:39:13+00:00,,0,0,oncall: transformer/mha,False
98286,"When I use the DDP model, I use a custom loss function, when the batch size changes during training, the process will be stuck. oncall: distributed triaged",2023-04-04 09:01:24+00:00,,0,2,oncall: distributed triaged,True
98273,[Inductor] [CPU] Huggingface model BartForCausalLM & MBartForCausalLM & OPTForCausalLM & PLBartForCausalLM performance regression > 10% on 2023-04-02 nightly release triaged oncall: pt2 module: inductor module: cpu inductor,2023-04-04 03:36:53+00:00,,0,0,triaged oncall: pt2 module: inductor module: cpu inductor,False
98269,"Inconsistent nn.KLDivLoss behavior: 0s in target OK on cpu, but gives nan on mps triaged module: mps",2023-04-04 02:39:53+00:00,,0,5,triaged module: mps,True
98268,hf_Longformer regression caused by https://github.com/pytorch/pytorch/pull/98119 triaged oncall: pt2,2023-04-04 02:29:05+00:00,,0,5,triaged oncall: pt2,False
98260,Broken mypy check in test_type_hints.py::TestTypeHints::test_doc_examples module: typing triaged,2023-04-04 00:08:58+00:00,,0,0,module: typing triaged,True
98259,DISABLED test_doc_examples (__main__.TestTypeHints) module: typing triaged skipped,2023-04-03 23:39:33+00:00,,1,4,module: typing triaged skipped,False
98251,[Dynamo] Enable `dynamo.export` for huggingface models w/ `ModelOutput` feature triaged module: pytree oncall: pt2 module: dynamo oncall: export,2023-04-03 21:52:18+00:00,,0,5,feature triaged module: pytree oncall: pt2 module: dynamo oncall: export,False
98222,aten::_linalg_solve_ex.result' is not currently implemented for the MPS feature triaged module: mps,2023-04-03 16:44:20+00:00,,1,11,feature triaged module: mps,False
98212,Wrong results for GELU forward pass (CPU vs MPS) while  inferencing a GLPN model from huggingface high priority triaged module: correctness (silent) module: mps,2023-04-03 13:36:12+00:00,,1,2,high priority triaged module: correctness (silent) module: mps,True
98210,torch.jit.script + legacy executor mode has diff in some pattern oncall: jit,2023-04-03 13:27:17+00:00,,0,1,oncall: jit,False
98208,Add a deterministic version of reflection_pad2d_backward_cuda module: nn triaged enhancement module: determinism actionable,2023-04-03 12:25:00+00:00,,0,4,module: nn triaged enhancement module: determinism actionable,True
98204,NaN appears when initializing tensor needs reproduction triaged module: NaNs and Infs,2023-04-03 09:54:42+00:00,,0,0,needs reproduction triaged module: NaNs and Infs,True
98203,"AssertionError: was expecting embedding dimension of 22, but got 1320 oncall: transformer/mha",2023-04-03 08:48:21+00:00,,0,3,oncall: transformer/mha,False
98200,torch.nn.init functions with `generator` argument module: nn triaged module: random actionable,2023-04-03 08:29:54+00:00,,0,5,module: nn triaged module: random actionable,True
98193,"RuntimeError: CUDA error: an illegal memory access was encountered, torch/cuda/streams.py"", line 94, in synchronize module: cuda triaged",2023-04-03 07:02:15+00:00,,0,0,module: cuda triaged,True
98189,[onnx] AdaptiveMaxPool2d can not convert to GlobalMaxPool module: onnx triaged,2023-04-03 05:58:40+00:00,,0,2,module: onnx triaged,False
98187,how can i load seperate pytorch_model.bin? module: serialization triaged,2023-04-03 05:05:07+00:00,,0,1,module: serialization triaged,True
98169,The operator 'aten::_weight_norm_interface' is not currently implemented for the MPS device. feature triaged module: mps,2023-04-02 12:24:26+00:00,,0,0,feature triaged module: mps,False
98164,forward AD implimentation : _scaled_dot_product_efficient_attention  triaged module: forward ad,2023-04-02 08:00:53+00:00,,0,5,triaged module: forward ad,True
98142,double free or corruption (fasttop) needs reproduction triaged,2023-04-01 13:35:10+00:00,,0,4,needs reproduction triaged,False
98136,A Segment Fault can be triggered in torch._grid_sampler_2d_cpu_fallback module: crash module: nn triaged,2023-04-01 08:58:55+00:00,,0,2,module: crash module: nn triaged,True
98133,[interoperability] zero-size cuda arrays do not look supported module: cuda triaged,2023-04-01 07:49:49+00:00,,0,1,module: cuda triaged,True
98124,PyTorch Profiler fails recording functions oncall: profiler,2023-04-01 05:39:06+00:00,,0,2,oncall: profiler,False
98115,Request to cherrypick a fix into v1.13.1 (v1.8 has a CVE) module: binaries triaged,2023-04-01 01:26:59+00:00,,0,6,module: binaries triaged,True
98100,Unable to run session using exported ONNX model using dictionary input module: onnx triaged,2023-03-31 22:41:15+00:00,,1,1,module: onnx triaged,False
98089,GroupNorm cpu/gpu parity tests fail with pretty large differences module: nn triaged needs research,2023-03-31 18:25:16+00:00,,1,1,module: nn triaged needs research,True
98077,Is there a recommended implementation of yuv2RGB for the current torch? module: onnx module: nn triaged module: python frontend,2023-03-31 11:12:54+00:00,,1,6,module: onnx module: nn triaged module: python frontend,True
98075,Unexpected results with torch.nn.functional.layer_norm module: numerical-stability triaged module: norms and normalization,2023-03-31 10:36:41+00:00,,0,4,module: numerical-stability triaged module: norms and normalization,True
98073, Add PrivateUse1 folder in aten/src/ATen triaged module: backend,2023-03-31 10:06:42+00:00,,0,8,triaged module: backend,True
98070,Request custom backend device memory Allocator. module: memory usage triaged module: backend module: CUDACachingAllocator,2023-03-31 09:01:36+00:00,,0,1,module: memory usage triaged module: backend module: CUDACachingAllocator,True
98064,Module 'Sequential' has no attribute '_modules' : oncall: jit,2023-03-31 07:13:17+00:00,,0,4,oncall: jit,False
98059,DISABLED test_scatter_1d (__main__.DeviceMeshCollectiveTest) oncall: distributed module: flaky-tests skipped,2023-03-31 03:40:37+00:00,,0,5,oncall: distributed module: flaky-tests skipped,False
98049,DISABLED test_all_gather_uneven (__main__.DeviceMeshCollectiveTest) oncall: distributed module: flaky-tests skipped,2023-03-31 00:58:29+00:00,,0,4,oncall: distributed module: flaky-tests skipped,False
98048,DISABLED test_broadcast_1d (__main__.DeviceMeshCollectiveTest) oncall: distributed module: flaky-tests skipped,2023-03-31 00:58:26+00:00,,0,7,oncall: distributed module: flaky-tests skipped,False
98013,Automate aarch64 builds oncall: releng triaged,2023-03-30 19:47:41+00:00,,0,0,oncall: releng triaged,True
98012,[Nova] Add metadata validation step to the smoke tests for core and domains oncall: releng triaged,2023-03-30 19:46:15+00:00,,0,0,oncall: releng triaged,False
98008,Write Binary Builds oncall runbook oncall: releng triaged,2023-03-30 19:39:11+00:00,,1,0,oncall: releng triaged,True
98007,Create release checklist template for the Launch Date oncall: releng triaged,2023-03-30 19:38:06+00:00,,0,0,oncall: releng triaged,True
98006,Create a plan on removing conda dependency from CI/CD oncall: releng triaged,2023-03-30 19:36:18+00:00,,0,0,oncall: releng triaged,True
98004,matmul with CSR matrix in inference mode throws an exception module: sparse module: autograd triaged,2023-03-30 19:15:37+00:00,,0,12,module: sparse module: autograd triaged,True
98002,DataLoader with collate_fn that returns tensors in GPU memory raises warnings when deleted module: dataloader triaged,2023-03-30 18:53:40+00:00,,0,6,module: dataloader triaged,True
97992,torch.compile not compatible with multiprocessing pool triaged bug oncall: pt2 module: inductor,2023-03-30 16:45:08+00:00,,0,1,triaged bug oncall: pt2 module: inductor,True
97991,functional collective should respect the whole mesh oncall: distributed triaged,2023-03-30 16:39:27+00:00,,1,0,oncall: distributed triaged,False
97990,Relax version dependencies on CUDA pip wheels? module: binaries module: cuda triaged,2023-03-30 16:23:28+00:00,,0,8,module: binaries module: cuda triaged,True
97976,Dynamo doesn't report accurate line numbers for <resume> in some situations triaged oncall: pt2 module: dynamo,2023-03-30 12:28:51+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
97966,torch.randn signature is missing generator module: docs triaged enhancement module: random module: python frontend,2023-03-30 08:54:29+00:00,,0,1,module: docs triaged enhancement module: random module: python frontend,True
97961,[CI/Infra] Record keeping: runner shutdown spike triaged,2023-03-30 06:55:01+00:00,,0,3,triaged,True
97915,Investigate Lazy{*}Norm{*}d modules no batch dim support module: nn triaged,2023-03-29 21:54:17+00:00,,1,0,module: nn triaged,True
97913,BUG torch.jit.annotate on List + torch.stack give wrong DTYPE oncall: jit,2023-03-29 21:40:31+00:00,,0,1,oncall: jit,True
97909,`torch.func.functional_call` doesn't work with compiled models high priority triaged module: correctness (silent) oncall: pt2 module: dynamo,2023-03-29 20:54:51+00:00,,1,7,high priority triaged module: correctness (silent) oncall: pt2 module: dynamo,True
97902,Multiple model init using OpenMP in c++ does not speed up module: multiprocessing triaged,2023-03-29 18:25:45+00:00,,0,5,module: multiprocessing triaged,True
97894,Dropout traces poorly with AotAutograd/make_fx triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-03-29 17:07:39+00:00,,0,16,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
97892,A parameterized fill value for triu and tril functions triaged enhancement module: viewing and reshaping,2023-03-29 16:55:54+00:00,,0,3,triaged enhancement module: viewing and reshaping,True
97888,Type conversion between float/complex triaged module: complex enhancement,2023-03-29 16:03:32+00:00,,0,4,triaged module: complex enhancement,True
97876,Missing torch import in _contextlib.py when using torch.jit._recursive needs reproduction oncall: jit,2023-03-29 11:21:17+00:00,,0,2,needs reproduction oncall: jit,False
97872,nn.linear not support bfloat16 needs reproduction oncall: distributed triaged,2023-03-29 09:47:35+00:00,,0,2,needs reproduction oncall: distributed triaged,False
97865,Unable to install torch on python 3.8.16 needs reproduction module: binaries triaged,2023-03-29 06:45:08+00:00,,0,4,needs reproduction module: binaries triaged,True
97861,"torch.onnx.errors.OnnxExporterError: Unsupported: ONNX export of operator unsafe_chunk, unknown dimension size. module: onnx triaged",2023-03-29 06:17:41+00:00,,1,2,module: onnx triaged,False
97856,make tensor data const correct module: internals triaged,2023-03-29 05:16:12+00:00,,1,7,module: internals triaged,True
97852,Functionalize crashes on train_step GraphModule triaged module: functionalization,2023-03-29 04:18:46+00:00,,0,0,triaged module: functionalization,True
97849,TORCH_LIBRARIES variable leads to undefined reference function error in compiling while using libtorch in c++ module: cpp triaged,2023-03-29 03:41:30+00:00,,0,10,module: cpp triaged,True
97848,Document _wrap_fx_args_as_onnxscript_args module: onnx triaged,2023-03-29 03:19:19+00:00,,0,0,module: onnx triaged,True
97847,CUDA 10.2 cudnn 8.2.4 run Conv2d error needs reproduction module: cudnn module: cuda triaged,2023-03-29 01:52:30+00:00,,0,1,needs reproduction module: cudnn module: cuda triaged,True
97827,Memory leak when saving an input tensor returned as-is if mark_dirty and running with dual tensors module: autograd triaged module: edge cases module: forward ad,2023-03-28 21:35:15+00:00,,0,0,module: autograd triaged module: edge cases module: forward ad,True
97823,Using `param in param_list` can trigger `non-singleton dimension` error? triaged module: python frontend,2023-03-28 20:39:52+00:00,,0,1,triaged module: python frontend,True
97802,Some c++ library docstrings incorrectly linked/repeated module: docs module: cpp triaged module: mps,2023-03-28 17:28:16+00:00,,0,0,module: docs module: cpp triaged module: mps,True
97788,Add SSIM as Loss Function feature module: loss triaged needs research,2023-03-28 15:35:47+00:00,,1,1,feature module: loss triaged needs research,False
97784,torch.compile fails with torch._dynamo.exc.TorchRuntimeError on  a function that contains a torch script module triaged oncall: pt2 module: dynamo,2023-03-28 15:15:01+00:00,,0,9,triaged oncall: pt2 module: dynamo,False
97783,The first epoch is very slow when using torch.compile triaged oncall: pt2 module: dynamic shapes,2023-03-28 14:49:54+00:00,,0,17,triaged oncall: pt2 module: dynamic shapes,False
97772,consider bumping `DEFAULT_PROTOCOL` module: serialization triaged,2023-03-28 13:12:53+00:00,,0,0,module: serialization triaged,True
97761,torch.testing.assert_close: allow check to fail on part on the input triaged module: testing,2023-03-28 11:06:37+00:00,,0,3,triaged module: testing,True
97760,Test Failure: TestUnaryUfuncsCPU.test_reference_numerics_normal_cos_cpu_float32 on s390x triaged module: numpy,2023-03-28 10:52:04+00:00,,0,4,triaged module: numpy,True
97757,oneDNN 3.0+ support triaged module: intel,2023-03-28 07:48:50+00:00,,0,3,triaged module: intel,True
97750,irrelevant error output for Minified repro triaged oncall: pt2 module: minifier,2023-03-28 06:25:53+00:00,,0,1,triaged oncall: pt2 module: minifier,True
97749,Bug on Minified repro example  triaged oncall: pt2 module: minifier,2023-03-28 06:15:05+00:00,,0,1,triaged oncall: pt2 module: minifier,True
97748,TypeError: 'torch._C._TensorMeta' object is not iterable module: windows module: rocm triaged,2023-03-28 06:09:55+00:00,,1,9,module: windows module: rocm triaged,True
97718,Dynamo generates invalid frame when graph-breaking due to opacus_cifar10 hooks triaged bug module: dynamo release notes: dynamo,2023-03-27 23:43:27+00:00,,2,2,triaged bug module: dynamo release notes: dynamo,True
97711,dynamo sometimes hits the cache size limit due to the foreach flag in optimizer.step() module: optimizer triaged oncall: pt2 module: dynamo,2023-03-27 21:49:39+00:00,,0,1,module: optimizer triaged oncall: pt2 module: dynamo,False
97693, Compile targts cuda:0 rather than the device the model is on triaged bug oncall: pt2 module: inductor,2023-03-27 19:02:56+00:00,,0,12,triaged bug oncall: pt2 module: inductor,True
97681,[FSDP] Consolidate test_fsdp_state_dict.py oncall: distributed triaged module: fsdp,2023-03-27 17:00:57+00:00,,1,0,oncall: distributed triaged module: fsdp,True
97676,Pytorch 2 compile + fsdp + transformers crash triaged module: xla,2023-03-27 15:46:56+00:00,,0,3,triaged module: xla,True
97670,[FSDP] test model.eval() + keep_low_precision_grads oncall: distributed triaged,2023-03-27 14:14:15+00:00,,1,0,oncall: distributed triaged,True
97668,sparse_csr_tensor matmul wrong output in bfloat16 module: sparse triaged,2023-03-27 13:54:17+00:00,,1,2,module: sparse triaged,True
97659,How do I get the original object wrapped by the torch.fx.proxy class？ triaged oncall: fx,2023-03-27 10:46:30+00:00,,0,2,triaged oncall: fx,False
97656,[bug] Internal assert failed when using pyro module: distributions triaged module: macos module: linear algebra module: python frontend,2023-03-27 09:39:20+00:00,,0,0,module: distributions triaged module: macos module: linear algebra module: python frontend,True
97653,transposed 2d copy bfloat16 support triaged intel,2023-03-27 07:41:40+00:00,,1,0,triaged intel,False
97652,torch.onnx.export support sparse tensor format module: onnx triaged,2023-03-27 07:24:09+00:00,,0,1,module: onnx triaged,False
97638,Regression in jit for f-strings with new lines oncall: jit triaged,2023-03-26 19:24:36+00:00,,0,3,oncall: jit triaged,True
97635,JAX + PyTorch produces `OMP: Error #13: Assertion failure at kmp_affinity.cpp(532)` needs reproduction triaged,2023-03-26 18:45:39+00:00,,0,3,needs reproduction triaged,True
97631,torch.zeros_like on a zero-sized BSR/BSC tensor results invalid tensor module: sparse triaged module: correctness (silent),2023-03-26 17:01:05+00:00,,0,1,module: sparse triaged module: correctness (silent),True
97623,Compile dynamic does not support GroupNorm in module triaged oncall: pt2 module: dynamic shapes,2023-03-26 08:17:50+00:00,,1,11,triaged oncall: pt2 module: dynamic shapes,True
97606,"MPS: grid_sampler_2d falls back to CPU, even though warning says it is natively supported on macOS >=13.1 triaged module: mps",2023-03-25 19:13:05+00:00,,1,5,triaged module: mps,True
97597,Insufficient MPS Documentation module: docs triaged module: mps,2023-03-25 09:52:42+00:00,,0,0,module: docs triaged module: mps,True
97595,can get_submodule be called within a ScriptFunction ? oncall: jit triaged,2023-03-25 08:57:25+00:00,,0,2,oncall: jit triaged,True
97580,Torch 2.0 import hangs forever module: build module: cuda triaged,2023-03-25 00:16:21+00:00,,0,5,module: build module: cuda triaged,True
97575,Multi-output derivative formulas can save unnecessary tensors module: autograd triaged module: nestedtensor actionable,2023-03-24 23:42:24+00:00,,0,8,module: autograd triaged module: nestedtensor actionable,True
97552,PackedSequence failure with MPS triaged module: mps,2023-03-24 18:09:04+00:00,,0,4,triaged module: mps,True
97539,InfoNCE loss for contrastive learning module: loss triaged enhancement,2023-03-24 16:56:25+00:00,,0,0,module: loss triaged enhancement,True
97504,"Burn benchmark suites into CI docker image. Not only this saves test time, but also it will get rid of occasional model installation failures. (@weiwangmeta ) triaged",2023-03-24 06:29:25+00:00,,1,0,triaged,True
97503,torch.cppExtension won't work with wsl2 triaged module: wsl,2023-03-24 06:26:52+00:00,,0,0,triaged module: wsl,True
97501,torch.compile not work in WSL triaged module: wsl oncall: pt2,2023-03-24 06:17:17+00:00,,0,2,triaged module: wsl oncall: pt2,False
97500,.set_ operation on a view (detach()) of the view tensor changes grad_fn of the original view tensor from ViewBackward0 to AsStridedBackward0 module: autograd triaged has workaround,2023-03-24 05:52:29+00:00,,0,4,module: autograd triaged has workaround,True
97499,`onnxrt` fails with compilations module: onnx triaged oncall: pt2,2023-03-24 05:44:00+00:00,,1,2,module: onnx triaged oncall: pt2,False
97498,Function Registry for extending collate_fn module: dataloader triaged enhancement,2023-03-24 05:08:30+00:00,,0,11,module: dataloader triaged enhancement,True
97484,[DTensor] Add a unittest to cover default PG condition for DeviceMesh oncall: distributed triaged,2023-03-23 23:29:07+00:00,,1,0,oncall: distributed triaged,False
97474,"make_fx(functionalize(f), tracing_mode='symbolic') breaks on torch.matmul triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher",2023-03-23 22:07:57+00:00,,0,3,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
97469,Improve collectives fingerprinting good first issue triaged module: c10d,2023-03-23 21:17:33+00:00,,0,5,good first issue triaged module: c10d,True
97456,pytorch dynamic quantized model failed to convert to onnx  module: onnx triaged,2023-03-23 18:57:19+00:00,,1,1,module: onnx triaged,False
97439,Change progressbar for hub triaged module: hub,2023-03-23 13:40:57+00:00,,0,0,triaged module: hub,True
97436,torch.compile not working with gradient checkpointing module: checkpoint triaged oncall: pt2 module: distributed,2023-03-23 11:55:13+00:00,,1,19,module: checkpoint triaged oncall: pt2 module: distributed,True
97432,suspicious memory leak when increase DataLoader's prefetch_factor and enable pin_memory module: dataloader module: memory usage triaged,2023-03-23 09:16:05+00:00,,0,0,module: dataloader module: memory usage triaged,True
97426,"Unsupported: ONNX export of operator group_norm, unknown input rank. module: onnx triaged onnx-needs-info",2023-03-23 08:00:37+00:00,,0,3,module: onnx triaged onnx-needs-info,False
97421,"After the release of pytorch 2.0.0, the compilation of ACLs is problematic. module: build triaged",2023-03-23 06:27:48+00:00,,0,1,module: build triaged,True
97414,Support running torch.compile with meta tensors feature triaged oncall: pt2 module: inductor,2023-03-23 02:13:34+00:00,,0,6,feature triaged oncall: pt2 module: inductor,False
97408,Further memcopy improvement at FX body level triaged,2023-03-23 01:34:38+00:00,,0,0,triaged,False
97402,DISABLED test_checkpoint_trigger (__main__.TestCheckpoint) triaged module: flaky-tests skipped module: unknown,2023-03-23 00:57:20+00:00,,0,12,triaged module: flaky-tests skipped module: unknown,False
97397,Import fails when both `USE_TENSORPIPE=OFF` and `USE_DISTRIBUTED=ON`. module: build triaged module: tensorpipe,2023-03-23 00:40:34+00:00,,0,0,module: build triaged module: tensorpipe,True
97395,Expanded weights tests broken triaged module: functorch,2023-03-23 00:07:08+00:00,,0,1,triaged module: functorch,True
97352,Sparse is not available on Windows module: sparse module: build module: windows triaged,2023-03-22 14:53:05+00:00,,1,0,module: sparse module: build module: windows triaged,True
97344,torch.onnx.export crashes on ReduceMax operator with onnx opset 18 module: onnx triaged,2023-03-22 12:52:09+00:00,,0,7,module: onnx triaged,False
97343,Traced module shows non-deterministic behaviour on CUDA oncall: jit,2023-03-22 12:27:52+00:00,,0,0,oncall: jit,True
97333,`torch.fmod` produces inconsistent results in eager and compile mode module: cpu triaged module: half oncall: pt2 module: inductor module: cpu inductor,2023-03-22 09:58:49+00:00,,0,4,module: cpu triaged module: half oncall: pt2 module: inductor module: cpu inductor,True
97329,"torch.ops.aten.pow(2.0, 3) return unexpected value with complex type module: internals triaged module: custom-operators",2023-03-22 08:33:52+00:00,,0,5,module: internals triaged module: custom-operators,True
97310,MPS: `unique` and `unique_consecutive` extremely slow when `return_counts=True` module: performance triaged module: mps,2023-03-22 02:46:50+00:00,,0,1,module: performance triaged module: mps,True
97295,Torch Dynamo allow_in_graph doesn't capture the custom function in graph triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-03-21 22:24:39+00:00,,1,19,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
97286,`jacrev` and `jacfwd` raise an error that `Sparse CSR tensors do not have strides` module: sparse triaged module: functorch,2023-03-21 21:39:08+00:00,,0,0,module: sparse triaged module: functorch,True
97284,"test_sparse_addmm fails on linux-bionic-py3.11-clang9 / test (crossref, 1, 2, linux.2xlarge) module: sparse module: autograd triaged module: decompositions",2023-03-21 20:57:15+00:00,,0,0,module: sparse module: autograd triaged module: decompositions,True
97283,`jacfwd` fails when computing the gradient for `channels_last` tensor triaged module: memory format module: functorch,2023-03-21 20:52:00+00:00,,0,0,triaged module: memory format module: functorch,True
97271,[composable FSDP] clip_grad_norm oncall: distributed triaged module: fsdp,2023-03-21 18:45:27+00:00,,1,0,oncall: distributed triaged module: fsdp,True
97268,Desync debugger encounters traceMap error oncall: distributed triaged module: c10d bug,2023-03-21 18:17:42+00:00,,0,0,oncall: distributed triaged module: c10d bug,True
97246,functorch roll-up issue for 2.1 triaged module: functorch,2023-03-21 13:56:09+00:00,,0,0,triaged module: functorch,True
97236,Non-deterministic results when training a model on GPU with MPS backend triaged module: determinism module: mps,2023-03-21 11:01:19+00:00,,1,2,triaged module: determinism module: mps,True
97225,Incompatibility with complex tensors needs reproduction triaged module: complex oncall: pt2,2023-03-21 07:31:34+00:00,,0,3,needs reproduction triaged module: complex oncall: pt2,True
97210,"INTERNAL ASSERT FAILED at ""../c10/cuda/CUDAGraphsC10Utils.h"":73, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus32729 triaged module: cuda graphs",2023-03-21 02:36:12+00:00,,0,6,triaged module: cuda graphs,True
97201,aten::sym_size is not using torch._ops.OpOverload in FX graph triaged module: dynamic shapes,2023-03-21 00:28:19+00:00,,1,5,triaged module: dynamic shapes,True
97196,Sequential/Partial unpickling and loading of models module: pickle triaged,2023-03-20 23:10:11+00:00,,0,1,module: pickle triaged,True
97189,torch.randint range for torch.int64 dtype seems wrong triaged topic: docs module: python frontend,2023-03-20 21:57:52+00:00,,0,5,triaged topic: docs module: python frontend,True
97188,Building LibTorch on Ubuntu with Mac M1 module: build triaged module: torchbind module: m1,2023-03-20 21:53:41+00:00,,0,1,module: build triaged module: torchbind module: m1,True
97174,Nightly conda binaries failed to pass tests since 2023-03-17  module: binaries oncall: releng module: ci triaged,2023-03-20 19:08:37+00:00,,1,2,module: binaries oncall: releng module: ci triaged,True
97163,[FSDP][optim_state_dict] Need more comprehensive tests for optim_state_dict interface oncall: distributed triaged module: fsdp,2023-03-20 16:59:41+00:00,,1,0,oncall: distributed triaged module: fsdp,True
97156,Implement `torch.distributions.Poisson.cdf()` module: distributions feature triaged,2023-03-20 15:35:48+00:00,,0,0,module: distributions feature triaged,True
97155,Custom recurrent network takes very long to compile for long sequences triaged ezyang's list oncall: pt2 module: python dispatcher,2023-03-20 15:28:06+00:00,,0,12,triaged ezyang's list oncall: pt2 module: python dispatcher,True
97154,RPC Tutorial can not profile the  rpc operations communication between workers oncall: distributed triaged module: rpc oncall: profiler,2023-03-20 15:21:22+00:00,,0,3,oncall: distributed triaged module: rpc oncall: profiler,True
97146,Problem with Hugging Face model that is not in training loop triaged oncall: pt2 module: cpu inductor,2023-03-20 11:35:12+00:00,,0,6,triaged oncall: pt2 module: cpu inductor,True
97138,RuntimeError: NYI: Named tensors are not supported with the tracer module: onnx triaged,2023-03-20 08:03:04+00:00,,0,1,module: onnx triaged,False
97135,Incorrect gradient calculation for upsample nearest on CUDA needs reproduction module: autograd module: nn triaged module: interpolation,2023-03-20 07:04:04+00:00,,1,7,needs reproduction module: autograd module: nn triaged module: interpolation,True
97128,"MultiHeadAttention, fast path broken with `bias=False` or uneven number of heads triaged oncall: transformer/mha",2023-03-20 06:01:36+00:00,,0,6,triaged oncall: transformer/mha,True
97118,nn.Conv function to compute conv formula module: docs module: nn triaged actionable,2023-03-19 23:51:25+00:00,,1,4,module: docs module: nn triaged actionable,True
97115,[Dynamo] symbolic_convert returns ValueError: Cell is empty triaged oncall: pt2 module: dynamo,2023-03-19 20:40:48+00:00,,0,13,triaged oncall: pt2 module: dynamo,False
97114,[Feature Proposal: New Distributed Training Algorithms] LSGD and EASGD oncall: distributed triaged,2023-03-19 20:34:34+00:00,,0,4,oncall: distributed triaged,True
97111,TransformerEncoder truncates output when some token positions are masked by `src_key_padding_mask` across batch triaged module: nestedtensor oncall: transformer/mha,2023-03-19 16:18:44+00:00,,0,11,triaged module: nestedtensor oncall: transformer/mha,True
97109,"""Adaptive pool MPS: input sizes must be divisible by output sizes"", I keep getting this error even when I try to adjust for size triaged module: mps",2023-03-19 14:27:41+00:00,,0,12,triaged module: mps,True
97106,slow torch import on macos  module: performance triaged module: macos,2023-03-19 06:56:28+00:00,,0,12,module: performance triaged module: macos,True
97097,torch.cuda.FloatTensor().normal_() generate (partially) different sample on different gpu machines module: docs triaged,2023-03-18 20:42:07+00:00,,0,1,module: docs triaged,True
97088,A Segment Fault can be triggered in torch.embedding triaged module: edge cases,2023-03-18 11:01:11+00:00,,0,0,triaged module: edge cases,True
97087,A Segment Fault can be triggered in torch.adjoint triaged module: edge cases,2023-03-18 10:58:24+00:00,,0,0,triaged module: edge cases,True
97086,A crash due to Floating Point Exception can be triggered in torch.index_select module: crash module: error checking triaged module: edge cases,2023-03-18 10:53:40+00:00,,1,5,module: crash module: error checking triaged module: edge cases,True
97083,Inconsitent results before/after compilation for squeeze + tensor mutation + if statement triaged bug oncall: pt2 module: aotdispatch module: inductor module: pt2-dispatcher,2023-03-18 07:46:19+00:00,,0,6,triaged bug oncall: pt2 module: aotdispatch module: inductor module: pt2-dispatcher,True
97079,[compile] KeyError: example_value needs reproduction triaged oncall: pt2,2023-03-18 05:59:25+00:00,,1,4,needs reproduction triaged oncall: pt2,False
97078,[compile] TypeError: __init__() missing 1 required positional argument: 'parent_module' needs reproduction triaged ezyang's list oncall: pt2,2023-03-18 05:55:56+00:00,,1,8,needs reproduction triaged ezyang's list oncall: pt2,False
97068,[RFC] CPU float16 performance optimization on eager mode. feature module: cpu triaged module: half,2023-03-18 01:26:22+00:00,,0,1,feature module: cpu triaged module: half,True
97047,Optimize for mobile produces incorrect result with INSERT_FOLD_PREPACK_OPS optimization oncall: jit oncall: mobile actionable,2023-03-17 20:22:12+00:00,,0,1,oncall: jit oncall: mobile actionable,True
97030,DDP static graph fails for static model oncall: distributed triaged module: ddp,2023-03-17 16:21:33+00:00,,0,2,oncall: distributed triaged module: ddp,True
97026,How to get list of all valid devices? module: docs triaged,2023-03-17 15:37:00+00:00,,0,0,module: docs triaged,False
97016,[Inductor] atomic_add does not support bf16 feature triaged oncall: pt2 module: inductor,2023-03-17 12:45:36+00:00,,0,10,feature triaged oncall: pt2 module: inductor,False
97014,deprecate integral and boolean dtype support torch.logit and torch.special.logit triaged module: deprecation module: special,2023-03-17 11:07:19+00:00,,0,0,triaged module: deprecation module: special,True
97006,[Feature Request] Compile compatible Neighborhood Algorithms for large Tensors feature triaged oncall: pt2,2023-03-17 07:17:43+00:00,,0,8,feature triaged oncall: pt2,False
97004,Small learning rate with `capturable=True` causes Adam optimizer to blow up model parameters. module: optimizer triaged,2023-03-17 06:11:36+00:00,,0,4,module: optimizer triaged,True
96996,"Get error: ""tuple index with non-constant index"" when exporting a model to ONNX format module: onnx triaged onnx-needs-info",2023-03-17 02:03:44+00:00,,1,8,module: onnx triaged onnx-needs-info,False
96982,[mps] conv1d outputs zeros triaged module: mps,2023-03-16 21:53:43+00:00,,0,0,triaged module: mps,True
96981,[ONNX] Export failed for Module with Keyword-only inputs module: onnx triaged onnx-needs-info,2023-03-16 21:19:53+00:00,,1,4,module: onnx triaged onnx-needs-info,False
96972,Adding sparse `addmv` and `triangular_solve` support on CPU - Mac OS - Apple Silicon M2 module: sparse triaged module: macos module: arm,2023-03-16 18:43:19+00:00,,1,21,module: sparse triaged module: macos module: arm,True
96964,GPU：7900xtx Pytorch2.0.0  rocBLAS error: module: binaries module: rocm triaged,2023-03-16 17:01:27+00:00,,0,4,module: binaries module: rocm triaged,True
96926,torch.onnx.export failed for models with Bernoulli operator module: onnx triaged,2023-03-16 02:21:07+00:00,,1,0,module: onnx triaged,True
96908,Doing inplace on a inplace view of tensor that retains_grad triggers internal assert module: autograd triaged has workaround,2023-03-16 00:46:31+00:00,,0,3,module: autograd triaged has workaround,True
96883,Expected scalar type Half but found Float when running nn.MultiheadAttention with AMP oncall: transformer/mha,2023-03-15 20:06:24+00:00,,0,1,oncall: transformer/mha,False
96855,Performance Drop for linalg_ldl_factor and ldl_solve triaged enhancement actionable module: vmap module: functorch,2023-03-15 01:20:04+00:00,,1,3,triaged enhancement actionable module: vmap module: functorch,True
96789,`cumprod` triggers INTERNAL ASSERT FAILED when `out` is a tensor on cuda but input is on cpu module: error checking triaged module: assert failure,2023-03-15 00:30:22+00:00,,0,0,module: error checking triaged module: assert failure,True
96779, Segmentation fault (core dumped) during Torch finetuning (at random step) needs reproduction module: crash triaged,2023-03-14 21:36:12+00:00,,0,0,needs reproduction module: crash triaged,True
96773,[MPS] pinverse dtype error triaged module: linear algebra module: mps,2023-03-14 20:47:12+00:00,,0,1,triaged module: linear algebra module: mps,True
96769,`sparse.mm` triggers INTERNAL ASSERT FAILED when backwarding module: sparse module: autograd triaged,2023-03-14 20:15:43+00:00,,0,1,module: sparse module: autograd triaged,True
96766,Follow-ups to do after adding nested checkpoint module: autograd triaged actionable,2023-03-14 19:27:23+00:00,,1,0,module: autograd triaged actionable,True
96764,Improve checkpoint thread-safety module: autograd triaged module: multithreading needs design,2023-03-14 19:22:55+00:00,,0,0,module: autograd triaged module: multithreading needs design,True
96757,[ONNX] FX exporter 'test_models_onnxruntime.py' tracker module: onnx triaged onnx-triaged,2023-03-14 17:53:09+00:00,,0,0,module: onnx triaged onnx-triaged,False
96743,Pruning under channels_last format triaged module: memory format module: pruning,2023-03-14 14:44:37+00:00,,0,2,triaged module: memory format module: pruning,True
96742,Pytorch2.0 compile error needs reproduction triaged oncall: pt2 module: distributed,2023-03-14 14:41:35+00:00,,1,8,needs reproduction triaged oncall: pt2 module: distributed,True
96738,Many padding Module fail memory_format tests module: nn triaged module: memory format actionable module: intel,2023-03-14 13:20:33+00:00,,0,5,module: nn triaged module: memory format actionable module: intel,True
96735,when run python run_test.py -i test_ops_jit error like this. ValueError: option names {'--junit-xml-reruns'} already added oncall: jit,2023-03-14 07:13:59+00:00,,0,0,oncall: jit,True
96726,Memory not release after jit.trace/freeze oncall: jit,2023-03-14 05:01:38+00:00,,0,11,oncall: jit,False
96716,[MPS] `.to('mps')` zeroes out elements in tensors taking up >=2^32 bytes triaged module: mps,2023-03-14 02:39:17+00:00,,0,0,triaged module: mps,True
96713,[Inductor] [CPU] Huggingface model MobileBertForQuestionAnswering performance regression > 10% on 2023-03-12 nightly release module: performance triaged oncall: pt2 module: cpu inductor,2023-03-14 02:16:55+00:00,,2,3,module: performance triaged oncall: pt2 module: cpu inductor,False
96704,`logical_xx` operations trigger INTERNAL ASSERT FAIL when `input` is complex tensor on cuda and `other` is on cpu triaged module: complex,2023-03-14 00:31:19+00:00,,0,1,triaged module: complex,True
96693,"torch.compile mode=""max-autotune"" precision appears to be lower triaged oncall: pt2",2023-03-13 21:46:59+00:00,,0,8,triaged oncall: pt2,False
96692,[H100] `test_ops.py::TestFakeTensorCUDA.test_fake_crossref_backward_amp_nn_functional_scaled_dot_product_attention_cuda_float32` failed module: cuda triaged module: fakeTensor,2023-03-13 21:42:15+00:00,,0,1,module: cuda triaged module: fakeTensor,True
96686,"No GPU found, using CPU during preprocessing Error processing dataset with NsfHifiGAN  needs reproduction module: windows module: cuda triaged",2023-03-13 21:10:32+00:00,,0,3,needs reproduction module: windows module: cuda triaged,True
96677,[FSDP] Make FSDP support local optimizer state_dict oncall: distributed triaged module: fsdp,2023-03-13 20:04:48+00:00,,1,0,oncall: distributed triaged module: fsdp,False
96670,Harden composable fully_shard: Checklist oncall: distributed triaged module: fsdp,2023-03-13 18:31:40+00:00,,0,1,oncall: distributed triaged module: fsdp,True
96643,PyTorch SGEMV is using 1 single core on AMD CPUs (very slow) module: rocm triaged,2023-03-13 11:28:53+00:00,,0,1,module: rocm triaged,True
96639,Completely different output between .pt and .ptl oncall: mobile,2023-03-13 08:38:36+00:00,,0,2,oncall: mobile,False
96637,Not allow force merge when lint fails and not because of broken trunk module: ci triaged,2023-03-13 07:11:05+00:00,,0,4,module: ci triaged,True
96633,Request for adding Warning/Error feature when dropout set to 1.0 in Transformer layer oncall: transformer/mha,2023-03-13 03:43:44+00:00,,0,0,oncall: transformer/mha,False
96632,"torch.cuda.graph ""Invalid capture"" with torch.linalg.solve triaged module: cuda graphs",2023-03-13 02:20:58+00:00,,0,1,triaged module: cuda graphs,True
96629,Dataloader should kill & restart workers when timeout is hit module: dataloader triaged,2023-03-13 00:58:44+00:00,,0,0,module: dataloader triaged,True
96617,Build errors in two Vulkan files oncall: mobile module: vulkan,2023-03-12 10:00:01+00:00,,0,0,oncall: mobile module: vulkan,False
96615,Tensor Permutation Along Given Axis feature triaged module: python frontend,2023-03-12 09:13:38+00:00,,0,0,feature triaged module: python frontend,True
96614,[MPS] Incorrect results for cumsum with bool tensors triaged module: mps,2023-03-12 07:35:59+00:00,,0,0,triaged module: mps,True
96613,The output of torch.histc is incorrect on both CPU and CUDA triaged module: edge cases,2023-03-12 05:20:59+00:00,,0,7,triaged module: edge cases,True
96608, No matching distribution found for torch==1.13.1+cu117 module: binaries triaged,2023-03-11 16:52:02+00:00,,0,2,module: binaries triaged,True
96602,"[MPS] softmax returns NaN attention probabilities for large tensors, in float16 and float32. triaged module: mps",2023-03-11 13:35:15+00:00,,0,0,triaged module: mps,True
96595,Why doesn't PyTorch install the REAL nvidia cuDNN pip package? module: binaries module: cuda triaged,2023-03-11 04:18:22+00:00,,0,6,module: binaries module: cuda triaged,True
96585,Proposal: Disable GC in test suite; GC after every test case triaged module: flaky-tests module: infra module: testing module: devx,2023-03-11 01:32:54+00:00,,0,6,triaged module: flaky-tests module: infra module: testing module: devx,True
96579,Wrong return type from operation on custom tensor inside registered hook  module: autograd triaged needs research module: __torch_function__ tensor subclass,2023-03-11 00:54:04+00:00,,0,4,module: autograd triaged needs research module: __torch_function__ tensor subclass,True
96560,Enable functorch testing for rocm module: rocm triaged module: functorch,2023-03-10 22:14:49+00:00,,1,3,module: rocm triaged module: functorch,True
96559,tests for linearize fail under the dynamo CI config triaged oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher,2023-03-10 22:09:43+00:00,,0,1,triaged oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher,False
96555,early stopping feature triaged needs design,2023-03-10 22:01:43+00:00,,0,0,feature triaged needs design,False
96538,[ONNX] FX exporter 'test_pytorch_onnx_onnxruntime.py' tracker module: onnx triaged onnx-triaged,2023-03-10 19:02:37+00:00,,0,1,module: onnx triaged onnx-triaged,False
96523,"MPS Backend Doc, model = YourFavoriteNet() not defined module: docs triaged actionable module: mps",2023-03-10 15:40:23+00:00,,0,3,module: docs triaged actionable module: mps,False
96518,fft should ignore dims with shape 1 triaged module: fft,2023-03-10 14:38:55+00:00,,0,0,triaged module: fft,True
96494,The sign of torch.distributions.transforms.PowerTransform seems to be incorrect module: distributions triaged,2023-03-10 05:11:14+00:00,,0,0,module: distributions triaged,True
96467,"[MINIFIER] Running code snippet with  TORCHDYNAMO_REPRO_AFTER=""dynamo"" leads to error needs reproduction triaged oncall: pt2 module: dynamic shapes module: minifier",2023-03-09 23:47:03+00:00,,0,3,needs reproduction triaged oncall: pt2 module: dynamic shapes module: minifier,True
96456,Shape Error when training HF deberta-base with Inductor good first issue triaged module: meta tensors oncall: pt2 module: pt2-dispatcher,2023-03-09 22:08:48+00:00,,0,12,good first issue triaged module: meta tensors oncall: pt2 module: pt2-dispatcher,True
96449,'aten::affine_grid_generator' to ONNX opset version 14 is not supported module: onnx triaged,2023-03-09 21:34:30+00:00,,1,2,module: onnx triaged,False
96448,Unable to move torch.jit.load-ed models to XLA devices oncall: jit module: xla,2023-03-09 21:33:59+00:00,,0,5,oncall: jit module: xla,False
96447,Information about CPU in `collect_env` is too verbose module: collect_env.py triaged,2023-03-09 21:12:34+00:00,,0,2,module: collect_env.py triaged,True
96428,Compressed sparse constructor allows mixed `int32/int64` indices which leads to dtype promotion/demotion in conversions. module: sparse triaged,2023-03-09 17:05:36+00:00,,0,2,module: sparse triaged,True
96420,Add location information when exception are thrown in `torch.jit.annotations.try_ann_to_type` oncall: jit,2023-03-09 14:08:48+00:00,,0,2,oncall: jit,True
96412,Proxy Options for Pytorch Hub triaged module: hub,2023-03-09 11:27:41+00:00,,0,1,triaged module: hub,True
96409,"Initialization on `meta` device failing for models containing `nn.utils.weight_norm`, with `NotImplementedError: Could not run 'aten::_weight_norm_interface' with arguments from the 'Meta' backend.` triaged actionable module: meta tensors",2023-03-09 09:07:17+00:00,,0,3,triaged actionable module: meta tensors,True
96396,[dynamo] add hook to modify instructions before/after instructions be generated feature triaged module: dynamo,2023-03-09 03:50:03+00:00,,0,5,feature triaged module: dynamo,False
96386,"[export] ""strict subset of traced input/output"" error when huggingface `ModelOutput` is returned triaged module: dynamo oncall: export",2023-03-09 02:35:23+00:00,,1,0,triaged module: dynamo oncall: export,True
96379,"`dynamo.export` ""input not consistent with traced input"" error when input default value type is `torch.Tensor`. triaged onnx-needs-info module: dynamo oncall: export",2023-03-09 01:29:11+00:00,,0,0,triaged onnx-needs-info module: dynamo oncall: export,True
96372,[BE] Avoid .data usage in FSDP buffer casting oncall: distributed better-engineering module: fsdp,2023-03-09 00:46:42+00:00,,0,2,oncall: distributed better-engineering module: fsdp,False
96319,views created in __torch_dispatch__ share storage but not version_counter module: autograd module: molly-guard triaged needs design module: __torch_dispatch__,2023-03-08 18:44:43+00:00,,0,11,module: autograd module: molly-guard triaged needs design module: __torch_dispatch__,True
96318,Support managed memory backed dlpack with  torch.from_dlpack feature triaged module: dlpack,2023-03-08 18:36:09+00:00,,0,1,feature triaged module: dlpack,False
96316,`FractionalMaxPool3d` INTERNAL ASSERT FAILED when computing `jacrev` module: nn triaged actionable module: edge cases,2023-03-08 18:29:32+00:00,,0,4,module: nn triaged actionable module: edge cases,True
96305,"Reuse autograd.grad graph for rapid, repeated gradient calculation feature module: autograd triaged module: cuda graphs",2023-03-08 15:12:55+00:00,,0,3,feature module: autograd triaged module: cuda graphs,True
96296,Inductor guards are not propagated to Dynamo with dynamic shapes triaged oncall: pt2 module: dynamic shapes,2023-03-08 13:08:55+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes,False
96292,Better error message when trying to run fp16 weights on CPU good first issue module: error checking triaged,2023-03-08 10:57:00+00:00,,0,6,good first issue module: error checking triaged,True
96282,DISABLED test_scatter_uneven (__main__.DeviceMeshCollectiveTest) oncall: distributed module: flaky-tests skipped,2023-03-08 06:41:22+00:00,,1,9,oncall: distributed module: flaky-tests skipped,False
96282,DISABLED test_scatter_uneven (__main__.DeviceMeshCollectiveTest) oncall: distributed module: flaky-tests skipped,2023-03-08 06:41:22+00:00,,1,9,oncall: distributed module: flaky-tests skipped,False
96277,A Segment Fault can be triggered in torch.adaptive_max_pool1d with an edge case module: crash triaged module: edge cases,2023-03-08 06:10:16+00:00,,0,0,module: crash triaged module: edge cases,True
96276,A Segment Fault can be triggered in torch.geqrf with an edge case module: crash triaged module: edge cases,2023-03-08 06:06:12+00:00,,0,0,module: crash triaged module: edge cases,True
96275,A Segment Fault can be triggered in torch.pinverse module: crash triaged module: edge cases,2023-03-08 05:59:47+00:00,,0,0,module: crash triaged module: edge cases,True
96265,RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation triaged,2023-03-08 02:17:45+00:00,,0,2,triaged,True
96236,nn.interpolate scale_factor floors output size with floating  module: nn triaged module: edge cases,2023-03-07 23:19:43+00:00,,0,0,module: nn triaged module: edge cases,True
96225,[MPS] F.conv1d and F.conv2d produce incorrect gradients when minibatch >= 2^16 triaged module: mps,2023-03-07 22:06:54+00:00,,0,0,triaged module: mps,True
96205,[Dynamo] HuggingFace transformers configuration_utils graph break workaround triaged oncall: pt2 module: dynamo,2023-03-07 17:48:55+00:00,,1,4,triaged oncall: pt2 module: dynamo,False
96198,dynamo + dict subclass + tensor instance check: NotImplementedError triaged oncall: pt2 module: dynamo,2023-03-07 15:25:32+00:00,,0,0,triaged oncall: pt2 module: dynamo,False
96187,`gradgradcheck` does not work with sparse inputs. module: sparse module: autograd triaged,2023-03-07 10:40:44+00:00,,0,3,module: sparse module: autograd triaged,True
96185,`ld: error: unknown argument '-force_load'` when linking libtorch on Android module: build oncall: mobile,2023-03-07 09:38:11+00:00,,0,1,module: build oncall: mobile,False
96161,[torchdistx] Future of the large model initialization module: nn triaged ezyang's list module: meta tensors module: fsdp,2023-03-07 01:38:06+00:00,,0,8,module: nn triaged ezyang's list module: meta tensors module: fsdp,True
96153,mps bug: failed assertion `[MPSNDArrayDescriptor sliceDimension:withSubrange:] error: subRange.start (6) is not less than length of dimension[0] (6)' high priority triaged module: regression module: viewing and reshaping module: mps,2023-03-07 00:07:43+00:00,,0,8,high priority triaged module: regression module: viewing and reshaping module: mps,True
96140,mkldnn matmul kernel may be slower than openblas kernel for very small tensor shapes module: performance triaged module: mkldnn,2023-03-06 22:24:05+00:00,,0,5,module: performance triaged module: mkldnn,True
96136,`torch.utils.checkpoint` should avoid updating BatchNorm statistics twice module: checkpoint triaged,2023-03-06 21:39:23+00:00,,0,1,module: checkpoint triaged,True
96123,make_fx tracing with dynamic shapes should also disable_slice_optimization triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-03-06 20:01:45+00:00,,0,0,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
96118,Activation Checkpointing PT2 - AOTAutograd cannot handle set_rng_state  module: checkpoint triaged oncall: pt2 module: dynamo module: distributed,2023-03-06 19:02:07+00:00,,1,1,module: checkpoint triaged oncall: pt2 module: dynamo module: distributed,False
96111,Static size boolean masking triaged module: advanced indexing,2023-03-06 17:44:34+00:00,,0,6,triaged module: advanced indexing,True
96110,torch.where behaves differently from in place replacement needs reproduction module: autograd triaged,2023-03-06 17:42:50+00:00,,0,2,needs reproduction module: autograd triaged,True
96098,Error during inference on iOS: INTERNAL ASSERT FAILED at it_type_base.h:535 oncall: jit,2023-03-06 14:40:47+00:00,,0,2,oncall: jit,True
96088,DISABLED test_nn_sequential_invocation_dynamic_shapes (torch._dynamo.testing.DynamicShapesMiscTests) triaged module: flaky-tests skipped module: unknown,2023-03-06 09:39:58+00:00,,0,1,triaged module: flaky-tests skipped module: unknown,False
96085,Add support for `__collate__` attrib on dataset elements in `default_collate` module: dataloader triaged,2023-03-06 08:29:44+00:00,,0,4,module: dataloader triaged,True
96073,Pytorch 2.0 installation tutorial does not work under Macbook module: docs triaged module: macos,2023-03-06 03:42:12+00:00,,0,0,module: docs triaged module: macos,True
96060,Linking libtorch with QT5 OpenGL application using llvmpipe mesa opengl crashes module: binaries triaged,2023-03-05 15:58:01+00:00,,0,1,module: binaries triaged,True
96056,MPS device throws error for `F.adaptive_avg_pool2d` triaged module: mps,2023-03-05 11:05:21+00:00,,0,4,triaged module: mps,True
96047,No speedup and a null pointer exception needs reproduction triaged oncall: pt2,2023-03-05 03:13:29+00:00,,0,6,needs reproduction triaged oncall: pt2,True
96046,Add arm64 builds for libtorch on MacOS with mps support triaged module: macos module: infra module: arm,2023-03-05 02:49:47+00:00,,0,1,triaged module: macos module: infra module: arm,True
96041,Cannot access data pointer of Tensor that doesn't have storage when using `torch.func.jvp` with `torch.compile` triaged actionable oncall: pt2 module: functorch module: pt2-dispatcher,2023-03-04 21:50:31+00:00,,0,6,triaged actionable oncall: pt2 module: functorch module: pt2-dispatcher,True
96036,Questions and Possible Features: Pytorch RPC 'future.wait()' will not release GIL which will block other thread's execution when using multithreading. triaged module: multithreading,2023-03-04 14:54:00+00:00,,0,1,triaged module: multithreading,True
96033,Encourage dynamo.export users to assume static by default if they call nonzero / unbacked SymInt triaged oncall: pt2 module: dynamic shapes oncall: export,2023-03-04 14:22:39+00:00,,0,2,triaged oncall: pt2 module: dynamic shapes oncall: export,False
95973,Graphstate checkpointing doesn't checkpoint ShapeEnv / shape guards triaged oncall: pt2 module: dynamic shapes,2023-03-03 16:02:58+00:00,,0,1,triaged oncall: pt2 module: dynamic shapes,False
95960,`@torch.jit.unused` does not properly ignore unsupported function signature oncall: jit,2023-03-03 11:38:31+00:00,,0,3,oncall: jit,False
95957,FSDP fails to load state dict under inference_mode triaged enhancement inference mode module: fsdp,2023-03-03 09:32:18+00:00,,1,3,triaged enhancement inference mode module: fsdp,True
95956,[vulkan] missing aten::reflection_pad1d.out operator triaged module: vulkan,2023-03-03 08:46:47+00:00,,0,0,triaged module: vulkan,True
95953,The torch.sparse document's typo error module: sparse module: docs triaged,2023-03-03 07:58:05+00:00,,0,2,module: sparse module: docs triaged,True
95946,"Build from source,  Undefined symbol: c10::detail::maybe_wrap_dim_slow(long long, long long, bool) triaged oncall: mobile module: ios",2023-03-03 06:11:09+00:00,,0,3,triaged oncall: mobile module: ios,True
95945,CPU time performance is unstable module: performance module: cpu triaged,2023-03-03 05:43:53+00:00,,1,29,module: performance module: cpu triaged,True
95944,training hangs at line torch.cuda.synchronize() module: cuda triaged module: deadlock,2023-03-03 05:10:52+00:00,,0,0,module: cuda triaged module: deadlock,True
95934,arange bug module: numerical-stability triaged,2023-03-03 02:05:52+00:00,,1,5,module: numerical-stability triaged,True
95921,ROCm distributed flaky on test_distributed_spawn module: rocm triaged,2023-03-03 00:03:01+00:00,,0,2,module: rocm triaged,True
95917,torchvision Caltech101 collate_fn error triaged,2023-03-02 23:13:10+00:00,,1,1,triaged,True
95916,autograd.functional.jacobian : tensor instead of function as input for reverse mode? feature module: autograd triaged needs research has workaround,2023-03-02 22:29:41+00:00,,0,1,feature module: autograd triaged needs research has workaround,False
95895,[PTD] dist.barrier() unreliable when using collectives from multiple threads. oncall: distributed module: c10d,2023-03-02 18:41:46+00:00,,0,1,oncall: distributed module: c10d,False
95872,corrupted size vs prev size error triaged module: ddp,2023-03-02 08:22:09+00:00,,1,1,triaged module: ddp,True
95864,Input names provided three but onnx recognizes two inputs only module: onnx triaged,2023-03-02 06:07:34+00:00,,0,1,module: onnx triaged,False
95858,[JIT] Support string type annotations in NamedTuples oncall: jit,2023-03-02 04:15:44+00:00,,1,0,oncall: jit,False
95856,`AssertionError: Activation` when compile spconv structure like `BaseBEVBackbone` needs reproduction triaged oncall: pt2 module: dynamo,2023-03-02 03:55:07+00:00,,0,3,needs reproduction triaged oncall: pt2 module: dynamo,False
95815,Let Nested Tensor Metadata be cached on GPU triaged module: nestedtensor,2023-03-01 20:32:04+00:00,,0,1,triaged module: nestedtensor,True
95789,drastic speed regression of torch.jit.load starting with the 20230301 nightly oncall: jit,2023-03-01 16:08:28+00:00,,0,3,oncall: jit,True
95786,Static asserts on accessor templates module: cpp-extensions triaged,2023-03-01 15:43:23+00:00,,0,0,module: cpp-extensions triaged,True
95785,Fully quantized model (`torch.quantization.convert`) produces incorrect output compared to analytical solution oncall: quantization triaged,2023-03-01 15:38:37+00:00,,1,3,oncall: quantization triaged,True
95779,SymInt'ify _gather_sparse_backward triaged,2023-03-01 13:14:37+00:00,,0,0,triaged,True
95776,`torch.Tensor.is_set_to` raises `NotImplementedError` when inputs contain sparse tensor  module: sparse triaged,2023-03-01 12:32:33+00:00,,0,3,module: sparse triaged,True
96740,Implementing the batching rule for aten::bucketize.Tensor. triaged actionable module: vmap module: functorch,2023-03-01 08:49:19+00:00,,1,0,triaged actionable module: vmap module: functorch,True
95768,Inconsistent behaviour of torch.all() module: cuda triaged,2023-03-01 07:23:52+00:00,,0,1,module: cuda triaged,True
95756,`torch.nanmedian` return a negative value when input is empty  module: error checking triaged,2023-03-01 02:51:34+00:00,,0,4,module: error checking triaged,True
95727,dist.barrier() should be able to go through custom backend oncall: distributed,2023-02-28 19:46:37+00:00,,0,2,oncall: distributed,False
95724,"distributed training: lots of ""Exception ignored"" at the end of each epoch oncall: distributed",2023-02-28 19:15:01+00:00,,0,0,oncall: distributed,False
95718,functorch.compile.memory_efficient_fusion errors with: RuntimeError: forward() Expected a value of type 'Tensor (inferred)' for argument 'primals_356' but instead found type 'int'.  triaged module: functorch,2023-02-28 18:13:05+00:00,,0,0,triaged module: functorch,True
95712,Multiheadattention module doesn't implement the function about kdim and vdim triaged module: multi-headed-attention,2023-02-28 16:46:53+00:00,,0,0,triaged module: multi-headed-attention,True
95711,`copy.deepcopy` does not copy gradients of nn.Parameter module: bc-breaking module: autograd module: nn triaged topic: bc breaking,2023-02-28 16:36:46+00:00,,1,6,module: bc-breaking module: autograd module: nn triaged topic: bc breaking,True
95708,Dynamo + MacOS: fatal error: 'omp.h' file not found triaged module: macos oncall: pt2,2023-02-28 15:38:28+00:00,,1,16,triaged module: macos oncall: pt2,False
95696,Can only import torch after Tensorflow accessed its gpu device module: cuda triaged,2023-02-28 09:35:44+00:00,,0,2,module: cuda triaged,True
95677,Unable to import ``torch.linalg`` needs reproduction triaged,2023-02-28 02:09:11+00:00,,0,2,needs reproduction triaged,True
95648,torch needs to SHOW that it support sm_89 even if functionally the same as sm_86 module: cuda triaged,2023-02-27 21:06:31+00:00,,0,3,module: cuda triaged,True
95645,Create a new Docker image with all inductor benchmarks and pre-trained models downloaded module: ci triaged module: devx,2023-02-27 20:24:27+00:00,,1,4,module: ci triaged module: devx,True
95628,Pytorch Home Page does not specify which version of python it requires module: docs triaged,2023-02-27 18:27:54+00:00,,0,2,module: docs triaged,False
95622,"Testing InvokeAI 2.3.1.post1, using mps, with PyTorch nightly dev20230226 yields RuntimeError cross-device copies are not allowed!) triaged module: mps",2023-02-27 17:00:48+00:00,,0,5,triaged module: mps,False
95613,[onnx] sort / argsort with `stable` argument specified cannot be exported to onnx module: onnx triaged,2023-02-27 14:16:40+00:00,,1,2,module: onnx triaged,False
95604,"Performance bugs exists in multiple convolution operations(e.g., `Convtranspose2d`) when useing the `groups` argument module: cudnn module: docs module: convolution triaged",2023-02-27 09:22:52+00:00,,0,16,module: cudnn module: docs module: convolution triaged,True
95595,TorchInductor fails with memoy violations in `test_comprehensive_grid_sampler_2d_cuda_float16` and `test_reflection_pad2d_dynamic_shapes_cuda` high priority triaged oncall: pt2 module: inductor,2023-02-27 05:44:46+00:00,,1,6,high priority triaged oncall: pt2 module: inductor,True
95590,Confusing error messages from `torch.nn.LazyLinear` in different versions. module: error checking triaged module: lazy,2023-02-27 03:31:48+00:00,,0,1,module: error checking triaged module: lazy,True
95572,Support datatype argument for torch.distributed.all_gather() (And the whole distributed module) feature triaged module: nccl module: c10d,2023-02-26 07:42:16+00:00,,1,3,feature triaged module: nccl module: c10d,True
95562,test_layer_norm_backward and test_layer_norm_backward_5d run OOM in slow gradcheck triaged module: nestedtensor,2023-02-25 20:56:29+00:00,,0,2,triaged module: nestedtensor,True
95560,torch.jit.load documentation doesn't specify if it is safe to load untrusted models or not oncall: jit module: docs security,2023-02-25 20:20:26+00:00,,0,0,oncall: jit module: docs security,True
95548," torch.distributions.kumaraswamy.Kumaraswamy generates samples outside its support (0,1) module: distributions triaged module: NaNs and Infs",2023-02-25 07:11:02+00:00,,0,0,module: distributions triaged module: NaNs and Infs,True
95538,Tensor.all() fails on MPS for tensors with more than 4 dimensions triaged module: reductions module: mps,2023-02-25 01:07:37+00:00,,0,4,triaged module: reductions module: mps,True
95501,dynamo+aot improperly handles dupe args via *args triaged bug oncall: pt2 module: dynamo,2023-02-24 20:38:42+00:00,,2,2,triaged bug oncall: pt2 module: dynamo,True
95497,Import parameters from jit oncall: jit,2023-02-24 20:21:55+00:00,,0,0,oncall: jit,True
95487,Torch RPC on multiple nodes with GPU returns a EOF error oncall: distributed triaged module: rpc module: tensorpipe,2023-02-24 17:23:46+00:00,,0,5,oncall: distributed triaged module: rpc module: tensorpipe,True
95485,Enrich shape operations with nested tensors triaged module: nestedtensor,2023-02-24 17:13:47+00:00,,0,1,triaged module: nestedtensor,True
95481,[BE] Make ActivationWrapper an abstract class oncall: distributed better-engineering,2023-02-24 16:45:01+00:00,,0,0,oncall: distributed better-engineering,False
95463,`add/add_` for CSC: errors when trying to access non-existent `crow_indices`. module: sparse triaged,2023-02-24 10:45:14+00:00,,1,1,module: sparse triaged,True
95462,Extend docs - Fixing out of memory with python garbage collection module: docs module: memory usage triaged,2023-02-24 09:30:34+00:00,,0,1,module: docs module: memory usage triaged,False
95460,torch.profiler.tensorboard_trace_handler Generates an incorrect JSON file triaged module: tensorboard oncall: visualization,2023-02-24 09:04:00+00:00,,0,0,triaged module: tensorboard oncall: visualization,True
95434,It seems that `torch.Tensor.addmv` and `torch.Tensor.addr` will check some inputs' dtype if and only if in `backward()` module: autograd triaged module: complex module: type promotion module: linear algebra actionable complex_autograd,2023-02-24 01:50:57+00:00,,0,2,module: autograd triaged module: complex module: type promotion module: linear algebra actionable complex_autograd,True
95432,Regression bug in `torch.nn.ReLU6` and `torch.nn.Hardtanh` that `inplace=True` doesn't work in PyTorch 1.10.0~1.13.1 high priority module: nn module: memory usage triaged actionable,2023-02-24 01:47:35+00:00,,1,3,high priority module: nn module: memory usage triaged actionable,True
95412,DISABLED test_variant_consistency_jit_linalg_lstsq_cpu_complex64 (__main__.TestJitCPU) triaged module: flaky-tests skipped module: unknown,2023-02-23 21:39:28+00:00,,0,10,triaged module: flaky-tests skipped module: unknown,False
95408,Parallel Associative Scan feature triaged oncall: pt2 module: functorch module: pt2-dispatcher,2023-02-23 21:09:20+00:00,,0,23,feature triaged oncall: pt2 module: functorch module: pt2-dispatcher,False
95403,test_ddp_apply_optim_in_backward in distributed_test.py fails for gloo backend oncall: distributed triaged,2023-02-23 19:50:07+00:00,,0,0,oncall: distributed triaged,True
95394,Add log highlights to Dr. CI's failed jobs triaged,2023-02-23 18:58:07+00:00,,0,0,triaged,True
95380,Investigate/add Windows Arm64 support for cpuinfo module: windows triaged module: arm,2023-02-23 16:04:26+00:00,,1,3,module: windows triaged module: arm,True
95374,Add oscillating activation functions to PyTorch. module: loss triaged needs research,2023-02-23 12:05:23+00:00,,0,3,module: loss triaged needs research,False
95369,build failed when strictly following the guidelines module: build triaged,2023-02-23 08:59:17+00:00,,0,6,module: build triaged,True
95337,Changing behavior of module.to() to better support mixed real- and complex-valued parameters module: nn triaged module: complex needs design,2023-02-22 23:19:15+00:00,,1,15,module: nn triaged module: complex needs design,True
95320,Circular padding error for 3D arrays triaged module: padding,2023-02-22 21:57:30+00:00,,0,1,triaged module: padding,True
95309,`torch.distributed.Store` triggers INTERNAL ASSER FAILED when seting oncall: distributed triaged,2023-02-22 19:34:45+00:00,,0,1,oncall: distributed triaged,True
95304,`torch.cartesian_prod` returns inconsistent dimensions with only one input triaged module: linear algebra,2023-02-22 18:29:57+00:00,,0,0,triaged module: linear algebra,True
95290,Continuous dropout layer module: nn triaged enhancement,2023-02-22 14:47:02+00:00,,1,3,module: nn triaged enhancement,True
95276,tabulate is used by `torch.fx.graph_module.GraphModule.print_tabular` but is not installed when installing pytorch triaged module: fx,2023-02-22 08:22:20+00:00,,0,0,triaged module: fx,True
95244,Make this ridiculously long error message more user friendly triaged module: infra,2023-02-21 22:11:07+00:00,,0,1,triaged module: infra,True
95238,Pytorch profiler stack exporting does not work oncall: profiler,2023-02-21 21:17:30+00:00,,0,0,oncall: profiler,False
95237,test_foreach failing cuda memory leak check module: cuda triaged module: mta,2023-02-21 21:08:59+00:00,,0,0,module: cuda triaged module: mta,True
95229,ONNX Exporter for circular padding mode in convolution ops module: onnx triaged,2023-02-21 18:05:55+00:00,,1,0,module: onnx triaged,False
95225,Remove conda virtualenv from the docker image module: binaries triaged module: docker,2023-02-21 16:28:42+00:00,,0,0,module: binaries triaged module: docker,True
95210,Add parallel attention layers and Multi-Query Attention (MQA) from PaLM to the fast path for transformers oncall: transformer/mha,2023-02-21 14:21:28+00:00,,0,16,oncall: transformer/mha,False
95207,"new backend privateuseone with ""to"" op triaged module: backend",2023-02-21 13:52:13+00:00,,0,6,triaged module: backend,True
95194,High Cuda Memory Consumption for Simple ResNet50 Inference oncall: jit,2023-02-21 09:18:15+00:00,,0,0,oncall: jit,True
95172,DISABLED test_memory_format_nn_ConvTranspose2d_cuda_complex32 (__main__.TestModuleCUDA) module: nn triaged module: flaky-tests skipped,2023-02-20 21:39:26+00:00,,0,5,module: nn triaged module: flaky-tests skipped,False
95169,COO @ COO tries to allocate way too much memory on CUDA module: sparse module: cuda triaged matrix multiplication,2023-02-20 16:08:54+00:00,,0,0,module: sparse module: cuda triaged matrix multiplication,True
95161,AOTAutograd based torch.compile doesn't capture manual seed setting in the graph triaged oncall: pt2 module: aotdispatch,2023-02-20 12:15:09+00:00,,0,3,triaged oncall: pt2 module: aotdispatch,False
95160,"Reversing along a dimension, similarly to numpy feature triaged module: numpy module: advanced indexing",2023-02-20 11:54:58+00:00,,0,0,feature triaged module: numpy module: advanced indexing,True
95146,Whether to consider native support for intel gpu？ triaged module: intel,2023-02-20 02:30:47+00:00,,0,2,triaged module: intel,True
95135,Add local version identifier to wheel file names module: build triaged,2023-02-19 13:05:09+00:00,,0,2,module: build triaged,True
95132,Differentiate with regard a subset of the input feature module: autograd triaged,2023-02-19 09:17:12+00:00,,0,4,feature module: autograd triaged,True
95129,Default value of `validate_args` is set to `True` when passed as `None` in `Multinomial` module: distributions triaged,2023-02-19 06:50:00+00:00,,0,2,module: distributions triaged,True
95124,"`INTERNAL ASSERT FAILED` -When using the PyTorch docker environment released by pytorch, a Vulcan support issue occurs module: build triaged module: docker",2023-02-19 02:30:43+00:00,,0,0,module: build triaged module: docker,True
95122,CosineAnnealingWarmRestarts but restarts are becoming more frequent triaged module: LrScheduler,2023-02-18 17:01:59+00:00,,0,0,triaged module: LrScheduler,True
95121,cuda 12 support request. module: cuda triaged,2023-02-18 16:59:16+00:00,,0,4,module: cuda triaged,True
95116,"When using `ceil_mode=True`, `torch.nn.AvgPool1d` could get negative shape. module: bc-breaking triaged module: shape checking topic: bc breaking",2023-02-18 11:48:49+00:00,,1,0,module: bc-breaking triaged module: shape checking topic: bc breaking,True
95112,"Proposal: `@capture`: Unified API for capturing functions across `{fx, proxy_tensor, dynamo}` module: onnx feature triaged oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher",2023-02-18 09:59:49+00:00,,1,7,module: onnx feature triaged oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher,False
95108,`torch.nn.LazyLinear` crash when using torch.bfloat16 dtype in pytorch 1.12.0 and 1.13.0 module: nn triaged intel,2023-02-18 01:15:23+00:00,,0,2,module: nn triaged intel,True
95103,AOTAutograd can add extra as_strided() calls when graph outputs alias inputs module: autograd triaged enhancement module: functionalization oncall: pt2 module: pt2-dispatcher,2023-02-17 23:12:16+00:00,,0,3,module: autograd triaged enhancement module: functionalization oncall: pt2 module: pt2-dispatcher,False
95100,"RuntimeError: view_as_complex is only supported for half, float and double tensors, but got a tensor of scalar type: BFloat16 triaged module: complex module: bfloat16",2023-02-17 22:18:27+00:00,,0,7,triaged module: complex module: bfloat16,True
95077,Implement a `torch.cuda.visible_device_indexes` function.  module: cuda triaged,2023-02-17 18:04:18+00:00,,0,2,module: cuda triaged,True
95075,Make artifacts easier to discover on HUD module: ci triaged,2023-02-17 17:58:22+00:00,,0,8,module: ci triaged,True
95074,A100 Perf Job artifact zipfiles unzip to generic folder that loses job information module: ci triaged,2023-02-17 17:52:37+00:00,,0,1,module: ci triaged,True
95073,`torch.cuda.device_count` cached return value does not reflect environment changes. module: cuda triaged,2023-02-17 17:52:37+00:00,,0,3,module: cuda triaged,True
95058,Upsampling ResBlock GPU memory spike module: cuda module: memory usage triaged,2023-02-17 13:11:04+00:00,,0,0,module: cuda module: memory usage triaged,True
95037,[Inductor] [CPU] Huggingface model AllenaiLongformerBase performance regression > 10% on ww07.4 triaged module: inductor module: cpu inductor,2023-02-17 04:11:51+00:00,,1,0,triaged module: inductor module: cpu inductor,True
95034,[Inductor] [CPU] Huggingface model MT5ForConditionalGeneration &T5ForConditionalGeneration & T5Small performance regression > 10% on ww07.4 triaged module: inductor module: cpu inductor,2023-02-17 03:58:59+00:00,,0,0,triaged module: inductor module: cpu inductor,True
95033,[Inductor] [CPU] Torchbench model hf_Longformer performance regression > 10% on ww07.4 triaged module: inductor module: cpu inductor,2023-02-17 03:53:42+00:00,,1,0,triaged module: inductor module: cpu inductor,True
95030,[Inductor] [CPU] Torchbench model hf_T5 & hf_T5_large & hf_T5_base performance regression > 10% on ww07.4 triaged module: inductor,2023-02-17 03:37:58+00:00,,1,3,triaged module: inductor,True
95024,cuDNN doesn't support convolutions with more than `INT_MAX` elements and native kernel uses too much memory module: cudnn module: convolution triaged module: CUDACachingAllocator,2023-02-17 02:52:23+00:00,,0,7,module: cudnn module: convolution triaged module: CUDACachingAllocator,True
95021,Custom operations in inductor feature triaged oncall: pt2 module: inductor module: dynamo,2023-02-17 01:31:32+00:00,,1,8,feature triaged oncall: pt2 module: inductor module: dynamo,False
95005,NCCL backend can't be used with a dataset that is IterDataPipe oncall: distributed module: dataloader,2023-02-16 22:10:07+00:00,,0,6,oncall: distributed module: dataloader,False
94990,interactions between views + autograd.Function + AOTAutograd causes memory leak module: autograd triaged actionable,2023-02-16 17:28:50+00:00,,1,17,module: autograd triaged actionable,True
94989,Internal Assert During Distributed Autograd Backprop oncall: distributed,2023-02-16 17:14:15+00:00,,0,0,oncall: distributed,False
94976,[libtorh]Consistency problem of gpu computing module: cuda triaged module: determinism,2023-02-16 10:16:44+00:00,,0,0,module: cuda triaged module: determinism,True
94974,Slow inference of torchscript model in C++ oncall: jit,2023-02-16 09:02:55+00:00,,0,2,oncall: jit,False
94966,CSR matrix add_ error with RuntimeError: CUDA error: kernel launch failure when calling cusparseXcsrgeam2Nnz module: sparse module: cuda triaged,2023-02-16 05:14:28+00:00,,1,3,module: sparse module: cuda triaged,True
94912,PR #88607 breaks build for POWER9 CPU module: build triaged module: POWER actionable,2023-02-15 17:53:02+00:00,,0,5,module: build triaged module: POWER actionable,True
94909,[numpy] mean & nanmean should support int dtypes triaged module: numpy module: reductions,2023-02-15 16:54:30+00:00,,0,0,triaged module: numpy module: reductions,True
94908,ASSERT(initialized()) Debug Error after JIT fusion on Windows oncall: jit module: nvfuser,2023-02-15 16:18:39+00:00,,1,3,oncall: jit module: nvfuser,True
94904,"Optimizer ""Lion"" in  Symbolic Discovery of Optimization Algorithms module: optimizer triaged needs research",2023-02-15 15:10:46+00:00,,0,1,module: optimizer triaged needs research,False
94893,Memory leak in torch.fft.rfft module: cuda module: memory usage triaged module: fft,2023-02-15 07:17:46+00:00,,0,20,module: cuda module: memory usage triaged module: fft,True
94872,[Inductor] [CPU] as_strided is much slower than empty_strided in single-thread single-batch mode in lennard_jones triaged oncall: pt2 module: inductor module: cpu inductor,2023-02-15 01:29:44+00:00,,0,2,triaged oncall: pt2 module: inductor module: cpu inductor,False
94869,aten::cudnn_convolution chooses different conv implementation given the same inputs.  module: cudnn module: cuda module: convolution triaged,2023-02-15 00:28:50+00:00,,0,8,module: cudnn module: cuda module: convolution triaged,True
94857,[FSDP] Gradients not propagating for mixed precision case oncall: distributed triaged module: fsdp,2023-02-14 21:50:24+00:00,,1,3,oncall: distributed triaged module: fsdp,True
94855,torch.compile breaks reproducibility triaged module: determinism oncall: pt2 module: inductor,2023-02-14 21:47:26+00:00,,0,11,triaged module: determinism oncall: pt2 module: inductor,False
94844,Dynamo.export should support formatting tensor value within a string feature triaged oncall: pt2 module: dynamo internal ramp-up task,2023-02-14 19:28:18+00:00,,1,4,feature triaged oncall: pt2 module: dynamo internal ramp-up task,False
94836,Allow Dynamo backends to use Inductor as fallback instead of eager mode feature triaged oncall: pt2 module: inductor,2023-02-14 18:36:47+00:00,,1,7,feature triaged oncall: pt2 module: inductor,False
94827,Linking error with Libtorch module: cpp triaged module: mkl,2023-02-14 16:44:06+00:00,,0,0,module: cpp triaged module: mkl,True
94821,Make `torch.onnx.utils._optimize_graph` use several CPU cores module: performance module: onnx triaged,2023-02-14 14:50:17+00:00,,1,2,module: performance module: onnx triaged,True
94819,`tag` parameter is ignored from NCCL P2P isend/irecv pair oncall: distributed module: nccl,2023-02-14 14:31:33+00:00,,0,1,oncall: distributed module: nccl,False
94816,grid_sample with relative grid feature module: nn triaged actionable module: interpolation,2023-02-14 14:18:34+00:00,,1,6,feature module: nn triaged actionable module: interpolation,True
94808,Memory Corruption in torch.lstm caused by edge cases module: crash module: rnn triaged module: edge cases,2023-02-14 09:09:10+00:00,,0,1,module: crash module: rnn triaged module: edge cases,True
94806,ImportError: cannot import name 'Backend' from 'torch._C._distributed_c10d' (unknown location) oncall: distributed triaged,2023-02-14 08:08:36+00:00,,0,2,oncall: distributed triaged,True
94804,Build Error: no matching function for call to ‘dnnl::graph::stream::stream(<brace-enclosed initializer list>)’ module: build triaged module: mkldnn,2023-02-14 07:50:38+00:00,,0,6,module: build triaged module: mkldnn,True
94801,Compiling PyTorch from Source on Xavier module: build triaged module: vectorization,2023-02-14 07:27:17+00:00,,0,3,module: build triaged module: vectorization,True
94792,Compiling libtorch from Source on Mac Beyond v1.11.0 module: build module: cpp triaged module: macos,2023-02-14 02:33:02+00:00,,0,6,module: build module: cpp triaged module: macos,True
94788,pytorch log level API and env var high priority feature module: logging triaged,2023-02-14 01:53:37+00:00,,1,26,high priority feature module: logging triaged,True
94779,Better Numpy API (interoperability between ML frameworks) triaged module: numpy,2023-02-13 23:00:26+00:00,,0,6,triaged module: numpy,True
94773,`torch.compile` doesn't consider the alias tensor created by `tensor[:]` triaged oncall: pt2 module: dynamo internal ramp-up task,2023-02-13 22:24:56+00:00,,0,3,triaged oncall: pt2 module: dynamo internal ramp-up task,False
94765,MPS internal error in `torch.gather` when last dimension is a singleton dimension module: crash triaged module: mps,2023-02-13 21:17:03+00:00,,0,2,module: crash triaged module: mps,True
94739,Update PyTorch's default C standard to C17 from C11 module: build triaged,2023-02-13 16:46:05+00:00,,0,1,module: build triaged,True
94718,Option to bypass NOLA check in torch.istft triaged module: fft,2023-02-13 06:26:00+00:00,,0,12,triaged module: fft,True
94711,Investigate queue disparity between `windows.4xlarge` and `linux.4xlarge` high priority module: ci triaged module: regression,2023-02-12 20:56:59+00:00,,0,2,high priority module: ci triaged module: regression,True
94705,Split getitem OpInfo into dynamic and non-dynamic inputs module: tests triaged,2023-02-12 16:34:07+00:00,,0,0,module: tests triaged,True
94704,`where` triggers INTERNAL ASSERT FAILED when `out` is a long tensor due to mixed types module: error checking triaged module: type promotion,2023-02-12 16:32:30+00:00,,0,4,module: error checking triaged module: type promotion,True
94698,A segment fault can be triggered in torch.avg_pool1d module: crash triaged module: edge cases,2023-02-12 06:25:54+00:00,,0,0,module: crash triaged module: edge cases,True
94696,A segment fault can be triggered in torch.max_pool1d_with_indices module: crash triaged module: edge cases,2023-02-12 06:10:39+00:00,,0,2,module: crash triaged module: edge cases,True
94693,inductor `compile_fx_inner` output is incorrect on graph with trailing copy_() triaged bug module: inductor,2023-02-12 01:01:20+00:00,,0,0,triaged bug module: inductor,True
94691,Nan is output by GRU on mps triaged module: mps,2023-02-12 00:20:57+00:00,,0,28,triaged module: mps,True
94675,"`UnsupportedOperatorError`, `OnnxExporterError` and `SymbolicValueError` related to MultiheadAttention export to onnx with torch.jit.script module: onnx triaged",2023-02-11 15:23:16+00:00,,1,9,module: onnx triaged,True
94669,A segment fault can be triggered in torch.svd module: crash triaged module: edge cases,2023-02-11 08:51:26+00:00,,0,0,module: crash triaged module: edge cases,True
94668,A segment fault can be triggered in torch.lstm with edge cases module: crash triaged module: edge cases,2023-02-11 08:48:00+00:00,,0,0,module: crash triaged module: edge cases,True
94654,Missing FX documents for some modules module: docs triaged,2023-02-11 03:04:50+00:00,,0,0,module: docs triaged,True
94652,dynamo: handle contiguous graph breaks  feature triaged oncall: pt2 module: dynamo module: graph breaks,2023-02-11 02:28:26+00:00,,0,5,feature triaged oncall: pt2 module: dynamo module: graph breaks,False
94620,[RFC] Add a static_graph mode for FSDP oncall: distributed triaged module: fsdp,2023-02-10 20:51:51+00:00,,0,0,oncall: distributed triaged module: fsdp,True
94614,Jetson CI needs Updates triaged module: jetson,2023-02-10 19:19:52+00:00,,0,0,triaged module: jetson,True
94609,Lots of different `nn.Sequence` instances trigger the Dynamo cache limits needs reproduction triaged has workaround oncall: pt2 module: dynamo,2023-02-10 17:44:31+00:00,,0,10,needs reproduction triaged has workaround oncall: pt2 module: dynamo,False
94602,Saving a `torch.nn.HuberLoss` using `torch.jit.script().save()` doesn't seem to implicitly convert from `int` type to `float` type. oncall: jit,2023-02-10 16:15:34+00:00,,0,0,oncall: jit,True
94594,A segment fault can be triggered in torch.histogramdd module: crash triaged module: edge cases,2023-02-10 13:50:03+00:00,,0,0,module: crash triaged module: edge cases,True
94593,Memory corruptions can be triggered in torch._remove_batch_dim triaged module: vmap,2023-02-10 13:46:52+00:00,,0,1,triaged module: vmap,True
94591,Issue with `upsample_nearest2d` decomposition triaged module: decompositions,2023-02-10 13:37:37+00:00,,0,3,triaged module: decompositions,True
94590,A Segment Fault can be triggered in torch.affine_grid_generator triaged module: edge cases,2023-02-10 13:28:31+00:00,,0,0,triaged module: edge cases,True
94586,`permute` for named tensors triaged module: named tensor,2023-02-10 12:41:12+00:00,,0,1,triaged module: named tensor,True
94575,[Dynamo] Key Mismatch When Loading Checkpoints Trained with Dynamo high priority module: serialization triaged oncall: pt2 module: dynamo,2023-02-10 05:17:32+00:00,,0,4,high priority module: serialization triaged oncall: pt2 module: dynamo,False
94544,Abort Caused by Virtual Function module: build triaged module: regression,2023-02-09 22:03:11+00:00,,0,3,module: build triaged module: regression,True
94542,torch.lgamma CUDA driver error needs reproduction triaged module: special,2023-02-09 21:02:31+00:00,,0,2,needs reproduction triaged module: special,True
94541,DISABLED test_pickle_nn_RNN_eval_mode_cuda_float64 (__main__.TestModuleCUDA) module: rnn triaged,2023-02-09 20:29:56+00:00,,0,1,module: rnn triaged,False
94511,"Performance does not meet expectations when training OPT-30 with FSDP, there may be problems with cpu offloading oncall: distributed module: fsdp",2023-02-09 14:35:11+00:00,,0,6,oncall: distributed module: fsdp,False
94504,[mypy] skipping mypy for a few torch/fx and torch/_subclass files module: lint triaged,2023-02-09 11:26:31+00:00,,0,0,module: lint triaged,False
94496,Dynamo captures only CUDA streams in FX graph triaged module: dynamo,2023-02-09 08:08:58+00:00,,0,3,triaged module: dynamo,False
94474,pybind11 SymNode binding is a footgun py::cast triaged module: pybind,2023-02-09 03:21:13+00:00,,0,0,triaged module: pybind,True
94471,[Functionalization] `index_reduce_` op tests with functionalization enabled triaged module: meta tensors module: functionalization,2023-02-09 02:50:18+00:00,,1,10,triaged module: meta tensors module: functionalization,True
94457,LSTM on CPU is significantly slower on PyTorch compared to other frameworks module: performance module: cpu triaged,2023-02-09 00:34:52+00:00,,0,4,module: performance module: cpu triaged,True
94454,Document and promise reproducibility torch.randn / torch.rand / torch.randint family behavior on CPU devices feature triaged module: random,2023-02-09 00:03:58+00:00,,0,0,feature triaged module: random,False
94451,"`jacrev` raise ""Cannot access storage of TensorWrapper"" error when computing the grad of `storage` module: autograd triaged actionable module: functorch",2023-02-08 23:05:47+00:00,,0,1,module: autograd triaged actionable module: functorch,True
94450,Pickling OneCycleLR.state_dict() with an unpickleable optimizer will result in an error. module: optimizer module: pickle triaged needs research,2023-02-08 22:58:26+00:00,,1,1,module: optimizer module: pickle triaged needs research,True
94443,A better error msg for `cuda.jiterator` when input is on `cpu` triaged module: jiterator,2023-02-08 22:12:06+00:00,,0,1,triaged module: jiterator,True
94441,`get_debug_state` a script function causes INTERNAL ASSERT FAILED oncall: jit triaged,2023-02-08 21:59:36+00:00,,0,1,oncall: jit triaged,True
94434,Exporting the operator 'aten::_transformer_encoder_layer_fwd' to ONNX opset version 13 is not supported module: onnx low priority triaged onnx-needs-info,2023-02-08 20:25:29+00:00,,1,8,module: onnx low priority triaged onnx-needs-info,False
94429,[RFC]FSDP API should make limit_all_gathers and forward_prefetch both default to be True triaged module: fsdp,2023-02-08 20:13:29+00:00,,0,1,triaged module: fsdp,True
94414,[fake_tensor] torch._subclasses.fake_tensor.DynamicOutputShapeException when calling torch.nonzero using aot_function triaged oncall: pt2 module: dynamic shapes module: graph breaks,2023-02-08 17:33:41+00:00,,0,10,triaged oncall: pt2 module: dynamic shapes module: graph breaks,True
94397,jacfwd and jacrev are fundamentally broken for complex inputs module: autograd triaged module: complex complex_autograd module: functorch,2023-02-08 14:11:22+00:00,,0,30,module: autograd triaged module: complex complex_autograd module: functorch,True
94395,`func.jacrev()` should be implemented as `func.jacfwd().mT.contiguous()` triaged module: complex module: functorch,2023-02-08 13:04:32+00:00,,0,7,triaged module: complex module: functorch,True
94388,Inconsistent results when using torch.Tensor.bernoulli with float instead of Tensor probabilities module: distributions triaged module: random module: determinism,2023-02-08 10:04:50+00:00,,0,1,module: distributions triaged module: random module: determinism,True
94374,[fx] const_fold.split_const_subgraphs leads to UserWarning triaged module: fx,2023-02-08 05:43:28+00:00,,0,1,triaged module: fx,True
94371,"QAT + torch.autocast does not work with default settings, missing fused fake_quant support for half oncall: quantization low priority triaged",2023-02-08 04:45:22+00:00,,1,0,oncall: quantization low priority triaged,True
94336,`scatter` fails the gradient computation in reverse mode for `src` when `index` is empty module: autograd triaged actionable module: scatter & gather ops,2023-02-07 20:49:15+00:00,,0,2,module: autograd triaged actionable module: scatter & gather ops,True
94333,cpu log1p for bfloat16 gives wrong result. module: cpu triaged module: bfloat16,2023-02-07 20:29:00+00:00,,0,4,module: cpu triaged module: bfloat16,True
94322,RFC: Enabling AVX512 dispatch for compute-intensive ATen ops module: performance module: cpu triaged module: intel,2023-02-07 19:05:12+00:00,,1,0,module: performance module: cpu triaged module: intel,True
94311,Unimplemented lowering - torch.jit.script oncall: jit,2023-02-07 16:22:27+00:00,,1,2,oncall: jit,False
94304,"RuntimeError: p.block != nullptr && p.block->ptr != nullptr INTERNAL ASSERT FAILED at ""../c10/cuda/CUDACachingAllocator.cpp"":1275, please report a bug to PyTorch. triaged module: assert failure module: CUDACachingAllocator",2023-02-07 12:35:54+00:00,,0,1,triaged module: assert failure module: CUDACachingAllocator,True
94294,CUBLAS_STATUS_NOT_SUPPORTED when calling cublasDgemv module: cuda triaged module: cublas,2023-02-07 09:39:08+00:00,,0,9,module: cuda triaged module: cublas,True
94293,torchdynamo.export doesn't work with float multiplication triaged oncall: pt2 module: dynamo oncall: export,2023-02-07 09:05:57+00:00,,0,2,triaged oncall: pt2 module: dynamo oncall: export,False
94292,What type of attributes does symbolic function support? triaged,2023-02-07 08:46:40+00:00,,0,0,triaged,True
94286,bugs when try parallel test code oncall: distributed,2023-02-07 07:29:03+00:00,,0,1,oncall: distributed,False
94280,"ONNX export produces hundreds of weight/bias/Matmul/etc. files alongside the `.onnx` file, and the `.onnx` file seems to be incorrect. module: onnx triaged",2023-02-07 06:23:25+00:00,,0,2,module: onnx triaged,False
94261,GroupNorm ONNX export does not reproduce same output module: onnx triaged,2023-02-07 02:33:54+00:00,,0,0,module: onnx triaged,False
94238,`PyTorchFileWriter` should drop the GIL while writing files module: serialization triaged,2023-02-07 00:14:54+00:00,,0,1,module: serialization triaged,True
94233,unsqueeze a single dimension multiple times feature triaged module: viewing and reshaping,2023-02-06 23:27:01+00:00,,0,2,feature triaged module: viewing and reshaping,True
94208,`zeros_like` + `fill_` makes the gradient computation in forward mode fail triaged module: forward ad,2023-02-06 19:30:11+00:00,,0,0,triaged module: forward ad,True
94186,Addition of hybrid CSR tensors produces incorrect and invalid CSR tensor module: sparse triaged module: correctness (silent) bug,2023-02-06 15:19:42+00:00,,0,2,module: sparse triaged module: correctness (silent) bug,True
94185,Addition of CSC/BSR/BSC tensors raises RuntimeError exceptions module: sparse triaged,2023-02-06 15:15:31+00:00,,0,0,module: sparse triaged,True
94183,Addition of batch CSR tensors produces incorrect and invalid CSR tensor module: sparse triaged module: correctness (silent) bug,2023-02-06 15:01:12+00:00,,0,2,module: sparse triaged module: correctness (silent) bug,True
94174,[pt2] The min and max parameters of torch.clamp do not support numpy format triaged oncall: pt2 module: dynamo internal ramp-up task,2023-02-06 07:43:50+00:00,,0,2,triaged oncall: pt2 module: dynamo internal ramp-up task,False
94167,"Faster `pad_sequence` and `tensor_split` function with CUDA kernel, are they possible? module: rnn triaged module: dynamic shapes",2023-02-06 03:14:30+00:00,,0,3,module: rnn triaged module: dynamic shapes,True
94164,Pytorch 2.0: Detection models from torchvision don't work with onnx and tensorrt backends module: onnx triaged oncall: pt2,2023-02-06 02:33:00+00:00,,1,7,module: onnx triaged oncall: pt2,False
94161,JIT: Dropout fails codegen on the third forward passes triaged module: nvfuser,2023-02-06 00:42:23+00:00,,0,0,triaged module: nvfuser,True
94160,Subclassed Tensors Decrease Training GPU Throughput by ~40%  module: performance triaged module: __torch_function__ tensor subclass oncall: pt2 module: pt2-dispatcher,2023-02-05 23:26:29+00:00,,0,3,module: performance triaged module: __torch_function__ tensor subclass oncall: pt2 module: pt2-dispatcher,True
94132,Asking for a LAZYMODULEMIXIN warning module: nn module: molly-guard triaged module: lazy,2023-02-04 08:48:10+00:00,,0,1,module: nn module: molly-guard triaged module: lazy,True
94131,faster WeightedRandomSampler implementation based on alias method module: dataloader triaged,2023-02-04 08:31:44+00:00,,0,8,module: dataloader triaged,True
94125,A Floating Point Exception can be trigerred in torch._C._nn.slow_conv3d module: crash triaged module: edge cases,2023-02-04 04:45:08+00:00,,1,2,module: crash triaged module: edge cases,True
94115,`cat` fails the gradient computation in forward mode with empty tensors when used with legacy vmap triaged module: edge cases module: forward ad,2023-02-04 01:58:14+00:00,,0,1,triaged module: edge cases module: forward ad,True
94111,`svd` triggers INTERNAL ASSERT FAILED when computing jacobian in forward mode module: autograd triaged module: complex has workaround module: linear algebra module: forward ad,2023-02-04 01:32:54+00:00,,0,3,module: autograd triaged module: complex has workaround module: linear algebra module: forward ad,True
94086,`MSELoss` fails to compute the gradients when inputs have different dtype module: autograd module: nn triaged actionable,2023-02-03 23:06:55+00:00,,1,1,module: autograd module: nn triaged actionable,True
94085,`unfold` fails in forward mode when unfolding a scalar tensor triaged module: forward ad,2023-02-03 22:57:22+00:00,,0,0,triaged module: forward ad,True
94083,Tracker for `scatter_reduce` additional reduction options requests triaged module: scatter & gather ops,2023-02-03 22:34:50+00:00,,0,0,triaged module: scatter & gather ops,True
94061,[dynamo] enable export path to preserve a meaningful parameter name in the exported graph module triaged enhancement oncall: pt2 module: dynamo,2023-02-03 19:33:54+00:00,,1,0,triaged enhancement oncall: pt2 module: dynamo,False
94021,Set AVX2 is minimum supported instruction set for Linux X86 triaged enhancement module: intel,2023-02-03 01:30:31+00:00,,0,2,triaged enhancement module: intel,True
94017,Type promotion for accumulate operation differs between eager and CPP dynamo  module: cpu triaged bug oncall: pt2,2023-02-03 00:27:33+00:00,,1,3,module: cpu triaged bug oncall: pt2,True
94010,Type promotion mismatch between eager and inductor pow triaged oncall: pt2 module: cpu inductor,2023-02-02 23:11:39+00:00,,0,3,triaged oncall: pt2 module: cpu inductor,False
94003,test_nccl_warn_not_in_group_debug_detail is flaky oncall: distributed triaged module: flaky-tests,2023-02-02 21:37:27+00:00,,0,0,oncall: distributed triaged module: flaky-tests,True
93982,`linalg.lstsq` fails the gradient computation in forward mode triaged module: forward ad,2023-02-02 19:44:24+00:00,,0,1,triaged module: forward ad,True
93955,"Enable Link Time Optimization in PyTorch 2.0 Release Binaries - Smaller, Faster, Better Binaries module: binaries module: performance module: build oncall: releng triaged topic: performance",2023-02-02 18:50:29+00:00,,0,3,module: binaries module: performance module: build oncall: releng triaged topic: performance,True
93947,[RFC] Support Huge Model Init Without mallocs for Compile/Distributed Use Cases oncall: distributed triaged,2023-02-02 17:24:04+00:00,,0,4,oncall: distributed triaged,True
93944,error: no member named 'residual_with_sum_zero_point' in 'ideep::attr_t module: build triaged module: macos,2023-02-02 16:41:59+00:00,,0,0,module: build triaged module: macos,True
93943,"`torch.jit.trace` memory usage increase although forward is constant, and gets much slower than forward with model depth increase oncall: jit",2023-02-02 16:34:30+00:00,,0,7,oncall: jit,True
93938,"[FSDP] `summon_full_params(writeback=True, rank0_only=True)` oncall: distributed triaged module: fsdp",2023-02-02 15:10:27+00:00,,0,0,oncall: distributed triaged module: fsdp,True
93937,onnx_torch.ModelProto exceeded maximum protobuf size of 2GB module: onnx triaged,2023-02-02 14:36:48+00:00,,0,3,module: onnx triaged,True
93935,[pt20][aot_eager] Exceed Python recursion limit with huge model or frequent recompilation triaged ezyang's list oncall: pt2 module: dynamic shapes module: dynamo,2023-02-02 14:09:54+00:00,,1,5,triaged ezyang's list oncall: pt2 module: dynamic shapes module: dynamo,True
93923,Dynamo uses CONSTANT_MATCH guards for string inputs triaged oncall: pt2 module: dynamo oncall: export,2023-02-02 09:43:36+00:00,,0,14,triaged oncall: pt2 module: dynamo oncall: export,False
93913,[BUG] jit.trace not working for torchvision ViT models oncall: jit,2023-02-02 06:44:03+00:00,,0,1,oncall: jit,True
93900,"Why does the torch model have no memory leaks under gpu, but there is a memory leak under cpu, torch version 1.10.1 needs reproduction triaged",2023-02-02 02:26:55+00:00,,0,3,needs reproduction triaged,True
93890,[Dynamo] torch.autocast context manager doesn't support graph break  triaged oncall: pt2 module: dynamo,2023-02-01 23:51:11+00:00,,0,8,triaged oncall: pt2 module: dynamo,False
93884,Importing tensorflow (2.12) before torch (2.0) hangs at import torch module: binaries triaged,2023-02-01 21:01:23+00:00,,0,3,module: binaries triaged,True
93880,"`PYTORCH_DEBUG_MODE`, better invalid index embedding lookup error message on cuda high priority triaged needs design module: python frontend",2023-02-01 19:58:47+00:00,,1,11,high priority triaged needs design module: python frontend,True
93860,Minifier related: perhaps same_two_models should reseed between the regular and optimized runs? triaged oncall: pt2 module: minifier,2023-02-01 16:19:00+00:00,,0,0,triaged oncall: pt2 module: minifier,False
93859,Bitwise-perfect method for (de)serializing tensors in base64 feature module: serialization triaged,2023-02-01 15:45:31+00:00,,0,0,feature module: serialization triaged,True
93857,Minifier has trouble correctly setting up requires_grad'ness of inputs for forward only triaged oncall: pt2 module: minifier,2023-02-01 15:31:28+00:00,,0,0,triaged oncall: pt2 module: minifier,False
93855,Enable CUPTI module: windows triaged,2023-02-01 15:26:38+00:00,,1,0,module: windows triaged,True
93854,torchdim can not be compiled for Python-3.11 on Windows module: windows triaged module: functorch,2023-02-01 15:26:31+00:00,,1,3,module: windows triaged module: functorch,True
93852,save_config/load_config for torch._dynamo.config and friends hardcodes file paths triaged oncall: pt2 module: dynamo,2023-02-01 15:08:32+00:00,,0,2,triaged oncall: pt2 module: dynamo,False
93847,Failures in cuda11.7-py3.10-gcc7-sm86-periodic-dynamo-benchmarks triaged oncall: pt2,2023-02-01 13:54:49+00:00,,0,5,triaged oncall: pt2,False
93846,large number of temporary files generated when using dataloader with num_workers>0 high priority module: dataloader triaged module: openmp,2023-02-01 13:43:01+00:00,,0,2,high priority module: dataloader triaged module: openmp,True
93843,EmbeddingBag to support mini-batches with offsets triaged enhancement module: nestedtensor,2023-02-01 12:44:02+00:00,,0,2,triaged enhancement module: nestedtensor,True
93838,"ONNX Export Fails: Model input type is Dict[str, Tensor]  module: onnx triaged",2023-02-01 09:13:24+00:00,,0,0,module: onnx triaged,True
93830,[pt2] MMDet meets Exception: Please convert all Tensors to FakeTensors first or instantiate FakeTensorMode error with aot_eager backend needs reproduction triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher,2023-02-01 07:15:02+00:00,,1,9,needs reproduction triaged oncall: pt2 module: fakeTensor module: pt2-dispatcher,True
93826,torch.jit.script does not work with DataParallel oncall: jit,2023-02-01 06:51:35+00:00,,0,0,oncall: jit,True
93394,MaskRCNN with `torch.compile` fails with `CUDA error: an illegal memory` triaged ezyang's list oncall: pt2,2023-01-31 20:43:13+00:00,,1,4,triaged ezyang's list oncall: pt2,False
93386,"[pt2] cannot compile function having `gt`, `expand` and `add_` triaged module: functionalization oncall: pt2 module: pt2-dispatcher",2023-01-31 19:17:07+00:00,,0,3,triaged module: functionalization oncall: pt2 module: pt2-dispatcher,False
93378,(DDP) RoBERTa_large training with `torch.compile` results in OOM and other issues triaged oncall: pt2,2023-01-31 17:26:12+00:00,,0,7,triaged oncall: pt2,False
93367,Aot accuracy minifier with dynamic shapes doesn't work triaged oncall: pt2,2023-01-31 14:53:21+00:00,,0,0,triaged oncall: pt2,False
93366,Option for minifier to dump the actual tensor inputs/parameters to be used triaged oncall: pt2,2023-01-31 14:51:41+00:00,,0,3,triaged oncall: pt2,False
93364,Minifier should also dump compilation artifacts from the real execution for ease of sanity checking triaged oncall: pt2,2023-01-31 14:42:48+00:00,,0,0,triaged oncall: pt2,False
93362,Make torch.testing functions overrideable with torch_function? triaged module: __torch_function__ module: testing,2023-01-31 14:41:41+00:00,,0,2,triaged module: __torch_function__ module: testing,True
93361,Inductor miscompilation with dynamic shapes from LearningToPaint triaged oncall: pt2,2023-01-31 14:25:22+00:00,,0,0,triaged oncall: pt2,False
93358,Minifier launcher incorrectly runs backwards even when original reproducer didn't run backwards triaged oncall: pt2,2023-01-31 14:01:54+00:00,,0,1,triaged oncall: pt2,False
93354,"minifier_launcher.py silently swallows ""ran into runtime exception which is likely an unrelated an issue"" warnings triaged oncall: pt2",2023-01-31 12:09:44+00:00,,0,0,triaged oncall: pt2,True
93350,Tensorboard SummaryWriter with cloud storage does not work on Mac oncall: visualization,2023-01-31 10:09:23+00:00,,0,0,oncall: visualization,False
93347,"when I want to use a new backend, how to deal with the op with 'device' argument?  triaged module: backend",2023-01-31 09:34:00+00:00,,0,1,triaged module: backend,True
93346,Quantized Transformer ONNX Export Fails module: onnx oncall: quantization low priority triaged,2023-01-31 09:29:00+00:00,,0,3,module: onnx oncall: quantization low priority triaged,True
93345,aten::int_repr not supported in torch.onnx.export module: onnx triaged,2023-01-31 08:34:45+00:00,,1,0,module: onnx triaged,False
93319,Minifier should not use pickle to save state into minifier launcher triaged module: fx module: functorch,2023-01-30 23:46:01+00:00,,0,1,triaged module: fx module: functorch,True
93317,Minifier doesn't save/load functorch config triaged module: fx module: functorch,2023-01-30 23:42:12+00:00,,0,0,triaged module: fx module: functorch,True
93311,[CI]  PyTorch Windows Test AMIs contains CUDA-11.3 installation module: windows module: ci triaged,2023-01-30 23:01:42+00:00,,1,1,module: windows module: ci triaged,True
93307,`torch.compile()` failed on Huggingface Flan-T5 `torch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(forward) [] OrderedDict()` triage review triaged oncall: pt2,2023-01-30 21:48:59+00:00,,0,2,triage review triaged oncall: pt2,True
93288,Errors when running the fsdp benchmarks for hf_Bert and hf_T5 triaged module: fsdp bug oncall: pt2 module: dynamo module: distributed,2023-01-30 18:20:46+00:00,,1,11,triaged module: fsdp bug oncall: pt2 module: dynamo module: distributed,True
93281,Estimate effort needed to bring PyTorch to Windows Arm64 module: windows triaged,2023-01-30 17:00:13+00:00,,0,1,module: windows triaged,True
93275,Bug in torch.linalg.svd  triaged module: mkl,2023-01-30 16:05:48+00:00,,0,4,triaged module: mkl,True
93264,"Bad conversion from torch.split(2d_tensor,splitsize_list) to SplitToSequence OP (onnx export) module: onnx triaged",2023-01-30 11:46:25+00:00,,0,0,module: onnx triaged,False
93262,`torch.compile` produce wrong result in `interpolate` when `mode=bilinear` triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2023-01-30 11:09:57+00:00,,0,3,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
93255,MaskRCNN model loaded fail with torch::jit::load(model_path) (C++ API) oncall: jit,2023-01-30 06:46:08+00:00,,0,0,oncall: jit,True
93240,USE_CUDNN=1 doesn't force cmake to fail if cudnn is not found module: build module: cuda triaged enhancement module: build warnings,2023-01-29 20:17:41+00:00,,0,2,module: build module: cuda triaged enhancement module: build warnings,True
93235,Well known way to request user backtrace when inside Dynamo triaged module: dynamo,2023-01-29 19:13:06+00:00,,0,0,triaged module: dynamo,False
93231,"Minifier produces minifier script that doesn't fail accuracy on Background_Matting (dynamic shapes, inductor, inference) triaged module: fx module: functorch",2023-01-29 18:42:46+00:00,,0,0,triaged module: fx module: functorch,True
93230,"Minifier does not run on LearningToPaint (dynamic shapes, inductor, inference) triaged module: inductor",2023-01-29 18:37:50+00:00,,1,1,triaged module: inductor,False
93227,squeezenet1_1 fails accuracy with AMP (but not on CI and dashboard); minifier does not work (when not using cuDNN?) triaged shadow review module: amp (automated mixed precision),2023-01-29 18:19:35+00:00,,0,2,triaged shadow review module: amp (automated mixed precision),True
93206,Build from Source Issues on MacOS Ventura 13.2 module: build triaged module: macos,2023-01-28 17:23:31+00:00,,0,4,module: build triaged module: macos,True
93197,Add Support for RockChip NPUs (RKNN(2))  triaged module: arm,2023-01-28 13:09:34+00:00,,0,0,triaged module: arm,True
93188,Why is AvgPool2D taking longer than Conv2D for the same input? module: performance module: cpu triaged,2023-01-28 00:17:08+00:00,,0,9,module: performance module: cpu triaged,True
93173,"[RFC] PT2-Friendly Traceable, Functional Collective Communication APIs oncall: distributed feature triaged oncall: pt2 module: ProxyTensor module: pt2-dispatcher",2023-01-27 21:12:12+00:00,,1,23,oncall: distributed feature triaged oncall: pt2 module: ProxyTensor module: pt2-dispatcher,False
93518,TorchDynamo Performance Dashboard (float32) triaged oncall: pt2,2023-01-27 19:46:55+00:00,,0,64,triaged oncall: pt2,False
93161,Segmentation fault between Numpy and Pytorch using torch.bmm module: binaries module: crash triaged,2023-01-27 18:18:04+00:00,,0,5,module: binaries module: crash triaged,True
93154,Support for VeLO optimizer. module: optimizer triaged needs research,2023-01-27 15:45:07+00:00,,1,2,module: optimizer triaged needs research,False
93152,Dynamo doesn't support dict(list_argument) triaged oncall: pt2 module: dynamo,2023-01-27 13:49:57+00:00,,1,1,triaged oncall: pt2 module: dynamo,False
93151,Dynamo doesn't support OrderedDict triaged oncall: pt2 module: dynamo,2023-01-27 13:48:19+00:00,,1,2,triaged oncall: pt2 module: dynamo,False
93134,Failed to Open libnvrtc-builtins.so.11.7 oncall: jit module: cuda,2023-01-27 04:47:31+00:00,,0,2,oncall: jit module: cuda,True
93121,[RFC] Flop counters in PyTorch feature triaged module: python frontend,2023-01-27 01:25:44+00:00,,0,8,feature triaged module: python frontend,True
93081,[Releng] [Conda] Optimize PyTorch packaging oncall: releng triaged,2023-01-26 18:20:59+00:00,,0,0,oncall: releng triaged,False
93061,DISABLED test_inplace_grad_index_put_cuda_float64 (__main__.TestBwdGradientsCUDA) module: cuda triaged module: flaky-tests skipped module: unknown oncall: pt2,2023-01-26 12:49:55+00:00,,0,25,module: cuda triaged module: flaky-tests skipped module: unknown oncall: pt2,False
93045,DISABLED test_forward_mode_AD_linalg_det_singular_cuda_complex128 (__main__.TestFwdGradientsCUDA) module: autograd module: rocm triaged module: flaky-tests skipped,2023-01-26 06:38:58+00:00,,0,8,module: autograd module: rocm triaged module: flaky-tests skipped,False
93044,DISABLED test_fn_grad_linalg_det_singular_cuda_complex128 (__main__.TestBwdGradientsCUDA) module: autograd module: rocm triaged module: flaky-tests skipped,2023-01-26 06:36:50+00:00,,0,7,module: autograd module: rocm triaged module: flaky-tests skipped,False
93017,numpy v1.24 does not work with `writer.add_histogram` triaged module: tensorboard,2023-01-25 21:13:21+00:00,,0,0,triaged module: tensorboard,True
93002,Replace pattern fails on incompatible function arguments oncall: fx,2023-01-25 19:16:35+00:00,,0,0,oncall: fx,False
92998,[BE] Improve FSDP <> AC Unit Tests oncall: distributed triaged module: fsdp,2023-01-25 19:01:28+00:00,,0,0,oncall: distributed triaged module: fsdp,True
92990,Feature request: access to variable oncall: transformer/mha,2023-01-25 18:27:27+00:00,,0,0,oncall: transformer/mha,False
92987,Test Failure: TestUpgraders.test_aten_div_scalar_at_3 on a big-endian machine (issue in torch.jit.load()) oncall: jit,2023-01-25 18:07:27+00:00,,0,1,oncall: jit,True
92977,ONNX export of batch_norm for unknown channel size issue. module: onnx triaged,2023-01-25 13:39:56+00:00,,0,2,module: onnx triaged,False
92967,Tracking issue for segfaults and floating point exceptions on 1.12.0 triaged,2023-01-25 04:53:55+00:00,,0,3,triaged,True
92942,test_jit_fuser_te SIGIOT's frequently during dynamo testing  triaged module: testing oncall: pt2 module: dynamo,2023-01-25 01:32:09+00:00,,1,4,triaged module: testing oncall: pt2 module: dynamo,False
92927,"Inplace fused (leaky)relu+(leaky)dropout for memory savings (I think, can be made fully allocation-less if never fully allocating random mask in FlashAttention style and recover the mask from the output) feature triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher",2023-01-24 23:18:52+00:00,,0,21,feature triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
92920,Add Stride Argument For Constructors triaged enhancement module: tensor creation,2023-01-24 21:33:51+00:00,,0,0,triaged enhancement module: tensor creation,True
92916,[Functionalization] Some ops need additional meta tensor support after functionalization triaged module: xla module: meta tensors module: functionalization,2023-01-24 20:53:52+00:00,,0,4,triaged module: xla module: meta tensors module: functionalization,True
92912,functorch.functionalize doesn't error out with logcumsumexp.out triaged module: xla module: meta tensors module: functionalization,2023-01-24 19:41:54+00:00,,0,2,triaged module: xla module: meta tensors module: functionalization,True
93517,Triton MLIR benchmarks triaged oncall: pt2,2023-01-24 18:37:07+00:00,,0,17,triaged oncall: pt2,False
92910,torch.jit.save() generates different contents in a file among different endian machines oncall: jit module: POWER,2023-01-24 18:24:24+00:00,,0,5,oncall: jit module: POWER,True
92909,[RFC] XLA Lazy Backend Support In DistributedTensor API triaged module: xla,2023-01-24 18:23:04+00:00,,2,1,triaged module: xla,True
92907,Unable to find an engine to execute when using pip to install but not with conda module: binaries module: cudnn module: convolution triaged,2023-01-24 17:51:57+00:00,,0,4,module: binaries module: cudnn module: convolution triaged,True
92888,[LibTorch] pickle_save output cannot be reloaded using pickle_load in Windows oncall: jit,2023-01-24 02:51:05+00:00,,0,0,oncall: jit,False
92884,"[RFC] Make more operations inplace (GELU, BatchNorm, LayerNorm) module: nn triaged enhancement actionable",2023-01-24 01:42:21+00:00,,0,12,module: nn triaged enhancement actionable,True
92866,JIT Function Fails when run a second time oncall: jit,2023-01-23 23:11:50+00:00,,0,2,oncall: jit,True
92855,Profiler documentation doesn't mention some exports are mutually exclusive oncall: profiler,2023-01-23 21:48:21+00:00,,0,1,oncall: profiler,False
92838,Enable OnDemand for Open Source CI  triaged,2023-01-23 19:36:51+00:00,,1,7,triaged,False
92835,Double free when running torch.linalg.ldl_solve triaged module: edge cases,2023-01-23 18:19:41+00:00,,0,1,triaged module: edge cases,True
92828,segfault when running torch.igamma triaged,2023-01-23 17:37:44+00:00,,0,1,triaged,True
92820,Ability to manually set the gradient in FSDP while inside `summon_full_params` and make it persistent needs reproduction oncall: distributed triaged module: fsdp,2023-01-23 16:41:20+00:00,,0,3,needs reproduction oncall: distributed triaged module: fsdp,True
92818,Segfault when running torch.atan2 triaged module: complex,2023-01-23 16:33:34+00:00,,0,2,triaged module: complex,True
92812,"torch.fx fails to trace through ""+"" op between torch.Size and torch.fx.proxy.Proxy oncall: fx",2023-01-23 12:35:23+00:00,,0,5,oncall: fx,False
92811,[complex] Jacobian of a non-holomorphic complex valued function triaged complex_autograd,2023-01-23 11:55:54+00:00,,0,1,triaged complex_autograd,True
92804,Dynamo graph break due to context manager do not resume inside/outside the context manager feature module: cuda triaged oncall: pt2 module: dynamo,2023-01-23 06:27:18+00:00,,1,23,feature module: cuda triaged oncall: pt2 module: dynamo,False
92801,"[BE] move _apply_to_tensors from FSDP to torch.distributed.utils, use in _recursive_to oncall: distributed triaged better-engineering module: ddp",2023-01-23 04:55:52+00:00,,0,1,oncall: distributed triaged better-engineering module: ddp,True
92794,Segmentation fault when running torch.ge needs reproduction triaged,2023-01-23 03:01:16+00:00,,0,3,needs reproduction triaged,True
92783,Process get killed when running torch.combinations needs reproduction module: performance triaged module: edge cases,2023-01-23 00:57:32+00:00,,0,1,needs reproduction module: performance triaged module: edge cases,True
92781,Floating point exception when running torch.nn.AdaptiveMaxPool3d triaged,2023-01-23 00:31:23+00:00,,0,3,triaged,True
92778,Process get killed when running torch.normal triaged module: numpy,2023-01-22 23:41:36+00:00,,0,1,triaged module: numpy,True
92776,segfault when running torch.lu_unpack module: crash triaged module: linear algebra module: edge cases,2023-01-22 23:33:45+00:00,,0,1,module: crash triaged module: linear algebra module: edge cases,True
92758,no attribute torch._dynamo unless you explicitly import torch._dynamo triaged module: dynamo,2023-01-22 03:05:37+00:00,,0,0,triaged module: dynamo,True
92752,'MPS' issue: torch.multinomial() returning [-9223372036854775808] triaged module: mps,2023-01-21 16:14:59+00:00,,0,9,triaged module: mps,True
92742,[JIT] Consecutive use of `addmm` Leads to Exception oncall: jit,2023-01-21 03:55:09+00:00,,0,2,oncall: jit,True
92740,[JIT] Applying `conv2d` over Constants Leads to Exception oncall: jit,2023-01-21 03:41:21+00:00,,0,4,oncall: jit,True
93515,Dynamo can not trace 'int(a_scalar_tensor.item())' triaged bug oncall: pt2,2023-01-21 01:16:11+00:00,,1,0,triaged bug oncall: pt2,True
92736,[FSDP] Add `foreach` support to `FSDP.clip_grad_norm_()` oncall: distributed triaged module: fsdp,2023-01-21 01:13:59+00:00,,1,0,oncall: distributed triaged module: fsdp,False
93514,iter(TensorVariable) fail triaged bug oncall: pt2,2023-01-20 23:08:25+00:00,,0,0,triaged bug oncall: pt2,True
92701,set_default_device/torch.device has performance impact for non-factory functions module: performance triaged module: __torch_function__,2023-01-20 19:57:07+00:00,,0,2,module: performance triaged module: __torch_function__,True
92694,API to check for errors in c10d.ProcessGroupNCCL oncall: distributed,2023-01-20 18:34:46+00:00,,0,3,oncall: distributed,False
92691,DDP+inductor+profiler crashes on  toy model module: crash triaged oncall: profiler bug oncall: pt2 module: inductor,2023-01-20 17:49:36+00:00,,1,9,module: crash triaged oncall: profiler bug oncall: pt2 module: inductor,True
92683,Torchscript troubles with complex values. RuntimeError: isInt() INTERNAL ASSERT FAILED oncall: jit triaged module: complex,2023-01-20 15:07:56+00:00,,0,3,oncall: jit triaged module: complex,True
92674,[JIT] `Linear` + `BatchNorm2d` Trigger Inconsistency between Eager Mode and JIT oncall: jit,2023-01-20 08:15:59+00:00,,0,1,oncall: jit,True
92670,14k github models TorchDynamo + TorchInductor bugs umbrella task triaged oncall: pt2 module: inductor module: dynamo,2023-01-20 05:38:51+00:00,,1,0,triaged oncall: pt2 module: inductor module: dynamo,False
92654,Traced model output differs on C++ and Python oncall: jit,2023-01-20 00:16:22+00:00,,0,1,oncall: jit,False
92600,Update quantization to make source files complient with /Zc:lambda module: windows oncall: quantization low priority triaged,2023-01-19 01:16:20+00:00,,1,0,module: windows oncall: quantization low priority triaged,True
92594,INTERNAL ASSERT FAILED when mixed dtypes for `addcmul_` triaged module: assert failure module: type promotion,2023-01-18 23:25:40+00:00,,0,1,triaged module: assert failure module: type promotion,True
92580,Improve Fake Tensor Error When Data Ptr is Accessed triaged module: fakeTensor,2023-01-18 21:29:05+00:00,,1,0,triaged module: fakeTensor,True
92563,[JIT] INTERNAL ASSERT FAILED when `Conv2d` and `clamp` used together oncall: jit,2023-01-18 18:46:56+00:00,,0,0,oncall: jit,True
92561,Spurious side effect diff when cond branches call different functions in outer scope triaged oncall: pt2 module: dynamo,2023-01-18 18:14:09+00:00,,1,0,triaged oncall: pt2 module: dynamo,False
92558,"[JIT][TracingCheckError] inplace ops incompatible with `contiguous(.., channels_last)` oncall: jit",2023-01-18 18:02:17+00:00,,0,1,oncall: jit,True
92554,Major bug in Transformers' masks high priority triage review oncall: transformer/mha module: correctness (silent),2023-01-18 17:45:17+00:00,,0,11,high priority triage review oncall: transformer/mha module: correctness (silent),True
92548,[JIT] Inconsistency  in tensor shape between eager mode and JIT oncall: jit,2023-01-18 15:58:19+00:00,,0,0,oncall: jit,True
92542,Pytorch AMP performance issue. triaged module: memory format module: amp (automated mixed precision),2023-01-18 13:01:02+00:00,,0,0,triaged module: memory format module: amp (automated mixed precision),True
92535,multiprocessing not work on WSL2 module: multiprocessing triaged module: wsl,2023-01-18 10:25:14+00:00,,0,3,module: multiprocessing triaged module: wsl,True
92528,"INTERNAL ASSERT FAILED: Expected OwnerRRef with id GloballyUniqueId(created_on=0, local_id=0) to be created. oncall: distributed",2023-01-18 08:14:31+00:00,,0,1,oncall: distributed,False
92398,[Inductor] support complex dtypes triaged module: complex module: random module: inductor module: cpu inductor,2023-01-18 05:31:29+00:00,,0,10,triaged module: complex module: random module: inductor module: cpu inductor,True
92375,operations failed in TorchScript interpreter oncall: jit,2023-01-18 02:44:38+00:00,,0,0,oncall: jit,False
92350,TypeError: no implementation found for 'torch._ops.aten.max.default' on types that implement __torch_dispatch__: [<class 'torch.masked.maskedtensor.core.MaskedTensor'>] triaged module: masked operators,2023-01-17 23:40:21+00:00,,0,0,triaged module: masked operators,True
93511,support setattr of arbitrary user provided types in tracing triaged bug oncall: pt2,2023-01-17 22:33:23+00:00,,0,4,triaged bug oncall: pt2,True
92339,"fft.fftshift, fft.ifftshift, roll not implemented triaged module: fft module: mps",2023-01-17 21:50:43+00:00,,0,7,triaged module: fft module: mps,True
92331,backward(inputs= does not need to execute grad_fn of the inputs module: bc-breaking module: autograd triaged actionable topic: bc breaking,2023-01-17 20:10:32+00:00,,0,1,module: bc-breaking module: autograd triaged actionable topic: bc breaking,True
92330,Simplify module backward hooks to use multi-grad hooks instead module: bc-breaking module: autograd module: nn triaged needs research topic: bc breaking,2023-01-17 20:06:44+00:00,,0,2,module: bc-breaking module: autograd module: nn triaged needs research topic: bc breaking,True
92310,[Releng] Windows AMI needs to be pinned for release high priority oncall: releng triaged,2023-01-17 16:43:50+00:00,,0,0,high priority oncall: releng triaged,True
92302,Cost & performance estimation for Windows Arm64 compilation module: windows triaged,2023-01-17 14:09:05+00:00,,0,0,module: windows triaged,True
92294,jit.fork stalls multiprocessing dataloader oncall: jit module: dataloader module: data,2023-01-17 09:52:59+00:00,,0,1,oncall: jit module: dataloader module: data,True
93510,RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation triaged bug oncall: pt2,2023-01-17 09:31:16+00:00,,0,4,triaged bug oncall: pt2,True
92292,"""Get Started"" tells us to use the anaconda installer for PyTorch 3.x - but this should be python 3.x module: binaries triaged",2023-01-17 09:05:59+00:00,,0,1,module: binaries triaged,True
92285,InstanceNorm operator support for Vulkan devices triaged module: vulkan,2023-01-17 05:30:42+00:00,,0,1,triaged module: vulkan,True
92273,Always install cpu version automatically module: binaries triaged,2023-01-17 02:05:55+00:00,,0,3,module: binaries triaged,True
92260,distributions.Beta returning incorrect results at 0 and 1 module: distributions triaged,2023-01-16 19:18:38+00:00,,0,2,module: distributions triaged,True
92259,[discussion] Fused MLPs feature triaged oncall: pt2 module: inductor,2023-01-16 18:11:32+00:00,,0,1,feature triaged oncall: pt2 module: inductor,False
92252,"`model.to(""cuda:0"")` does not release all CPU memory module: memory usage triaged",2023-01-16 15:01:17+00:00,,0,0,module: memory usage triaged,True
92251,"`torch.load(..., map_location=""cuda:0"")` allocates memory on both CPU and GPU module: serialization triaged module: python frontend",2023-01-16 14:52:21+00:00,,0,3,module: serialization triaged module: python frontend,True
92250,torch.cuda.is_available() returns True even if the CUDA hardware can't run pytorch module: cuda triaged,2023-01-16 14:27:19+00:00,,0,4,module: cuda triaged,True
92246,test_qnnpack_add fails oncall: mobile module: xnnpack,2023-01-16 13:53:18+00:00,,0,3,oncall: mobile module: xnnpack,False
92245,"CapabilityBasedPartitioner incorrectly sorts the graph, causing optimizer return/output node to be first triaged module: fx oncall: pt2",2023-01-16 13:52:24+00:00,,0,6,triaged module: fx oncall: pt2,False
92230,Add torch::jit::ScriptModule to the C++ API oncall: jit,2023-01-15 22:31:02+00:00,,0,0,oncall: jit,False
92226,Hijacked package names from nightly repository module: binaries triaged security,2023-01-15 19:54:05+00:00,,1,3,module: binaries triaged security,True
92223,Improve make_fx tracing speed module: performance triaged module: ProxyTensor,2023-01-15 17:40:40+00:00,,0,0,module: performance triaged module: ProxyTensor,True
92217,"false INTERNAL ASSERT FAILED at ""../c10/cuda/CUDAGraphsC10Utils.h"":73, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus32680 triaged module: cuda graphs",2023-01-15 10:30:32+00:00,,0,0,triaged module: cuda graphs,True
93509,Triton Autotuning Cache-Clearing Adds 256MB Memory Overhead triaged bug oncall: pt2,2023-01-13 22:57:19+00:00,,0,0,triaged bug oncall: pt2,True
92175,test_fx_passes generate bad test names module: tests oncall: fx,2023-01-13 21:46:43+00:00,,0,0,module: tests oncall: fx,False
92173,"""multi device"" tests get skipped in standard CI module: ci module: tests triaged",2023-01-13 21:15:46+00:00,,0,12,module: ci module: tests triaged,False
92171,PyTorch 1.13.1 hangs with `torch.distributed.init_process_group` oncall: distributed,2023-01-13 19:50:19+00:00,,0,1,oncall: distributed,False
92151,Exception in distributed context doesn't propagate to child processes launched with multiprocessing oncall: distributed,2023-01-13 15:28:04+00:00,,0,2,oncall: distributed,True
92148,Occassional OverflowError with mps running yolov7 triaged module: mps,2023-01-13 14:43:13+00:00,,0,0,triaged module: mps,True
92134,[Bug][Dataloader] unable to mmap 2048 bytes from file <filename not specified>: Cannot allocate memory (12) module: multiprocessing module: dataloader triaged,2023-01-13 05:52:05+00:00,,0,20,module: multiprocessing module: dataloader triaged,True
92132,Torchrun seems to have problem with virtual environment oncall: distributed triaged oncall: r2p,2023-01-13 04:10:50+00:00,,0,9,oncall: distributed triaged oncall: r2p,True
92131,DISABLED test_cuda_variable_sharing (__main__.TestMultiprocessing) triaged module: flaky-tests skipped module: unknown,2023-01-13 03:40:03+00:00,,0,1,triaged module: flaky-tests skipped module: unknown,False
92130,Unable to export timm models with torch._dynamo triaged module: dynamo,2023-01-13 03:22:23+00:00,,0,0,triaged module: dynamo,True
92128, Forward arguments are not updated in DDP oncall: distributed triaged,2023-01-13 02:36:29+00:00,,0,0,oncall: distributed triaged,True
92086,Error while building pytorch mobile binaries from source triaged oncall: mobile,2023-01-12 17:46:34+00:00,,0,4,triaged oncall: mobile,True
92078,DISABLED test_cdist_large_batch (__main__.TestMPS) triaged module: flaky-tests skipped module: mps,2023-01-12 15:47:14+00:00,,0,1,triaged module: flaky-tests skipped module: mps,False
92073,compilig MultiHeadAttention oncall: jit,2023-01-12 13:17:55+00:00,,0,2,oncall: jit,False
92072,Implement forward AD with grid_sampler_2d  triaged module: forward ad,2023-01-12 13:14:27+00:00,,0,0,triaged module: forward ad,True
92041,jit testing fails on 3.11 debug build oncall: jit,2023-01-11 22:29:18+00:00,,0,0,oncall: jit,True
92029,Update docs URLs in torch/_functorch/autograd_function.py to stable before 2.0 triaged,2023-01-11 21:06:28+00:00,,0,0,triaged,False
92011,[Releng] Add repo dispatch via webhook to trigger domain builds after the core oncall: releng triaged,2023-01-11 18:27:57+00:00,,0,2,oncall: releng triaged,True
92007,Add plots of LRSchedulers to doc to make it easier to read module: docs triaged actionable module: LrScheduler,2023-01-11 16:48:55+00:00,,0,0,module: docs triaged actionable module: LrScheduler,False
91996,autograd.functional.jacobian : Imaginary part is lost for functions with real input and complex output. module: autograd triaged has workaround,2023-01-11 08:35:50+00:00,,0,1,module: autograd triaged has workaround,True
91990,export does not support boolean tensor indexing triaged module: dynamic shapes module: dynamo,2023-01-11 04:03:16+00:00,,0,4,triaged module: dynamic shapes module: dynamo,True
91989,Torch's affinity setting lead to openvino using only one core. triaged module: intel intel priority,2023-01-11 03:34:07+00:00,,0,10,triaged module: intel intel priority,True
91986,An error happend when I convert pytorch model to onnx module: onnx triaged,2023-01-11 03:03:49+00:00,,0,3,module: onnx triaged,True
91982,sympy failure on model when dynamic_shapes=True triaged module: dynamic shapes module: dynamo,2023-01-11 01:04:18+00:00,,0,1,triaged module: dynamic shapes module: dynamo,True
91970,Unknown CUDA graph CaptureStatus21852 needs reproduction triaged,2023-01-10 19:29:16+00:00,,0,1,needs reproduction triaged,True
93502,Torchdynamo with onnxrt backend generating fake tensor errors triaged bug oncall: pt2,2023-01-10 18:52:14+00:00,,0,3,triaged bug oncall: pt2,True
91968,Pytorch is using system-installed mkl-dnn. module: build triaged,2023-01-10 18:36:27+00:00,,0,10,module: build triaged,True
92075,"Hessian produces wrong results, but works if I add a perturbation triaged",2023-01-10 18:17:25+00:00,,0,4,triaged,True
91965,Proxy/cache server option/hooks for downloading model checkpoints and dataset archive files in cloud environment triaged module: hub,2023-01-10 16:00:35+00:00,,0,1,triaged module: hub,True
91958,CUDA error `CUBLAS_STATUS_NOT_INITIALIZED`  module: cuda triaged module: cublas,2023-01-10 13:14:07+00:00,,0,1,module: cuda triaged module: cublas,True
91951,[PT2.0 Feature Proposal] GNN inference and training optimization on CPU feature triaged oncall: pt2,2023-01-10 09:03:16+00:00,,0,1,feature triaged oncall: pt2,False
91950,RuntimeError: philox_cuda_state for an unexpected CUDA generator used during capture triaged module: random module: cuda graphs,2023-01-10 08:51:39+00:00,,0,3,triaged module: random module: cuda graphs,True
93501,_pack_padded_sequence fails in dynamo due to requiring a non-fake 2nd argument triage review module: performance bug oncall: pt2 mlperf,2023-01-10 05:03:16+00:00,,1,7,triage review module: performance bug oncall: pt2 mlperf,True
91943,elastic job failed when scale down oncall: distributed triaged oncall: r2p,2023-01-10 02:37:34+00:00,,0,2,oncall: distributed triaged oncall: r2p,True
91940,torchrun elastic always “address already in use” error oncall: distributed triaged oncall: r2p,2023-01-10 02:18:58+00:00,,0,5,oncall: distributed triaged oncall: r2p,True
91936,Fails to build on ppc64le with clang module: cpu triaged module: vectorization,2023-01-10 01:19:31+00:00,,0,2,module: cpu triaged module: vectorization,True
91908,from torch import * does not import dtypes triaged actionable module: python frontend,2023-01-09 21:17:04+00:00,,0,1,triaged actionable module: python frontend,True
91903,Profiling with stack enabled results in error when Python's cProfile is also running oncall: profiler,2023-01-09 19:17:27+00:00,,0,0,oncall: profiler,True
91902,ONNXRuntime outputs numerically incorrect results for mixed precision models. module: onnx triaged,2023-01-09 19:03:30+00:00,,0,0,module: onnx triaged,True
91898,Lazily start worker threads in the autograd engine module: autograd triaged better-engineering actionable,2023-01-09 17:12:20+00:00,,1,0,module: autograd triaged better-engineering actionable,True
91889,ToTensor deadlock in subprocess module: multiprocessing triaged,2023-01-09 15:16:41+00:00,,0,4,module: multiprocessing triaged,True
91888,No setting to allow collecting the first trace early. oncall: profiler,2023-01-09 15:05:50+00:00,,0,1,oncall: profiler,False
91887,Only the first logged trace in a given log dir is visible in tensorboard. triaged module: tensorboard,2023-01-09 15:04:01+00:00,,0,1,triaged module: tensorboard,True
91879,ddp vs fsdp oncall: distributed module: fsdp,2023-01-09 12:01:48+00:00,,0,9,oncall: distributed module: fsdp,False
91863,torch.Categorical samples indexes with 0 probability when given logits as argument module: distributions triaged,2023-01-08 23:31:55+00:00,,0,1,module: distributions triaged,True
91856,torchrun --help is too slow oncall: distributed triaged oncall: r2p topic: improvements topic: performance,2023-01-08 13:01:18+00:00,,0,1,oncall: distributed triaged oncall: r2p topic: improvements topic: performance,False
91855,torchrun default value of command line options oncall: distributed triaged oncall: r2p,2023-01-08 12:18:12+00:00,,0,2,oncall: distributed triaged oncall: r2p,True
91942,jacrev over huber function needs reproduction triaged module: functorch,2023-01-07 11:48:09+00:00,,0,2,needs reproduction triaged module: functorch,True
91842,Adding a page for subfolder/subfile overview/descriptions in the developer wiki module: docs triaged,2023-01-07 06:55:51+00:00,,0,4,module: docs triaged,True
91841,torch.onnx.export is throwing RuntimeError: prim::TupleUnpack not matched to tuple construct module: onnx triaged,2023-01-07 06:42:45+00:00,,0,0,module: onnx triaged,False
91810,[Bug/functorch] Cannot use `tensor.detach().numpy()` for `GradTrackingTensor`: Cannot access data pointer of Tensor that doesn't have storage triaged module: functorch,2023-01-06 16:29:29+00:00,,0,10,triaged module: functorch,True
91809,Better API for `torch.cov` (and `Tensor.cov`) feature triaged module: python frontend,2023-01-06 16:21:35+00:00,,0,0,feature triaged module: python frontend,True
91760,Inconsistent rank among torch.distributed primitives oncall: distributed triaged,2023-01-05 13:15:35+00:00,,0,3,oncall: distributed triaged,True
91759,"Error while building pytorch from source on windows - Ninja Build Stopped, Subcommand Failed triaged",2023-01-05 10:36:00+00:00,,0,1,triaged,True
91754,CUDA error: initialization error module: cuda triaged module: regression module: nvfuser,2023-01-05 05:44:39+00:00,,0,4,module: cuda triaged module: regression module: nvfuser,True
91753,SymIntType gets translated to int when going through pybind triaged oncall: pt2 module: dynamic shapes,2023-01-05 05:05:30+00:00,,0,8,triaged oncall: pt2 module: dynamic shapes,True
91751,[bazel] replace //c10:headers dependency by //c10 dependency triaged module: bazel,2023-01-05 03:41:11+00:00,,0,0,triaged module: bazel,True
91738,tracing torchvision detection model results in an error oncall: jit module: vision,2023-01-04 23:15:01+00:00,,0,0,oncall: jit module: vision,True
91737,[MPS] Improve the performance of torch.linear() triaged enhancement module: backend module: mps,2023-01-04 23:04:36+00:00,,1,0,triaged enhancement module: backend module: mps,True
91719,Errors using torch.compile() on wav2vec2 model triaged ezyang's list oncall: pt2 module: dynamic shapes module: inductor module: dynamo,2023-01-04 19:35:36+00:00,,1,17,triaged ezyang's list oncall: pt2 module: dynamic shapes module: inductor module: dynamo,False
91716,linspace (and arange) behaves differently on GPU and CPU triaged,2023-01-04 19:01:31+00:00,,0,3,triaged,True
91713,Dynamo minifier fails with false internal assert on torch-nightly triaged oncall: pt2,2023-01-04 17:55:05+00:00,,0,0,triaged oncall: pt2,False
91710,`@torch.compile` fails with `InternalTorchDynamoError` on torch-nightly triaged oncall: pt2,2023-01-04 17:39:47+00:00,,0,1,triaged oncall: pt2,False
91700,Add vmap support for torch.linalg.vander good first issue triaged module: functorch,2023-01-04 15:48:00+00:00,,0,4,good first issue triaged module: functorch,True
91699,Segmentation fault after trying to create a tensor with float values needs reproduction module: rocm triaged,2023-01-04 15:30:05+00:00,,0,5,needs reproduction module: rocm triaged,True
91697,Build from source fails: undefined reference to caffe2::DeviceQuery module: build triaged,2023-01-04 13:54:08+00:00,,0,8,module: build triaged,True
91692,[discussion] Analyzing a list of tensors stored as intermediate values / saved_for_backward in autograd graph module: autograd good first issue triaged actionable,2023-01-04 11:12:40+00:00,,0,9,module: autograd good first issue triaged actionable,True
91686,Sparse tensor not supported (Minkowski Engine) oncall: jit,2023-01-04 07:08:23+00:00,,0,0,oncall: jit,False
91678,Wrong in building torch from source triaged,2023-01-04 02:48:39+00:00,,0,4,triaged,True
91670,AssertionError: tensor's device must be `meta` when trying to export a fake-initialized module triaged module: fakeTensor,2023-01-04 01:15:10+00:00,,0,1,triaged module: fakeTensor,True
91661,[FSDP][BE] Add check that compute device equals current device oncall: distributed triaged module: fsdp,2023-01-03 21:56:00+00:00,,1,0,oncall: distributed triaged module: fsdp,False
91655,FakeTensors not moving between device properly on Module.cuda() triaged module: fakeTensor,2023-01-03 20:22:48+00:00,,0,4,triaged module: fakeTensor,True
91653,Stochastic Illegal Memory Access error mid-epoch on AWS p4d instances module: cuda triaged module: cublas matrix multiplication,2023-01-03 19:50:56+00:00,,0,9,module: cuda triaged module: cublas matrix multiplication,True
91633,Segmentation fault when running torch.nn.functional.fractional_max_pool3d on torch 1.13.1 needs reproduction module: crash triaged,2023-01-03 15:47:18+00:00,,0,4,needs reproduction module: crash triaged,True
91630,Periodic ROCM distribtued jobs are broken module: rocm triaged,2023-01-03 15:33:46+00:00,,0,1,module: rocm triaged,True
91624,trainer triaged,2023-01-03 12:37:21+00:00,,0,2,triaged,True
91623,Investigate CUDA enabled build-time difference between MSVC and GCC+WSL module: build module: windows triaged,2023-01-03 11:50:55+00:00,,0,0,module: build module: windows triaged,True
91622,Cross-compiled libtorch Windows Arm64 binaries module: windows feature module: ci triaged module: arm,2023-01-03 11:40:28+00:00,,1,0,module: windows feature module: ci triaged module: arm,True
91618,There is no developer documentation about getting started with MPS native debugging module: docs triaged module: mps,2023-01-03 09:45:41+00:00,,0,0,module: docs triaged module: mps,True
91617,MPS: `torch.sub` erroneously returns 0 on outputs of `chunk` via `layer_norm` triaged module: mps,2023-01-03 09:39:57+00:00,,0,10,triaged module: mps,True
91615,"sparse.mm(coo, dense) produces wrong results on T4/V100 GPUs module: sparse triaged",2023-01-03 09:32:48+00:00,,0,3,module: sparse triaged,True
91608,SSL: CERTIFICATE_VERIFY_FAILED while trying to download pretrained model within a company that transforms SSL certificates for security purposes triaged module: hub,2023-01-03 05:54:24+00:00,,1,8,triaged module: hub,True
91604,wrong assert message oncall: quantization triaged,2023-01-03 02:20:25+00:00,,1,1,oncall: quantization triaged,True
91629,vmap + nn.SyncBatchNorm.convert_sync_batchnorm oncall: distributed module: data parallel module: ddp module: functorch,2023-01-02 20:10:33+00:00,,0,2,oncall: distributed module: data parallel module: ddp module: functorch,True
91599,"`mul(CSC, CSC)` fails with layout mismatch between the inputs and the output. module: sparse triaged",2023-01-02 19:49:26+00:00,,0,0,module: sparse triaged,True
91593,Division by zero error when running torch.nn.functional.lp_pool1d triaged module: edge cases,2023-01-02 18:03:06+00:00,,0,6,triaged module: edge cases,True
91590,Crashes of linalg.ldl_solve on different edge cases not coming from linalg.ldl_factor module: crash triaged module: linear algebra module: edge cases,2023-01-02 17:53:08+00:00,,0,2,module: crash triaged module: linear algebra module: edge cases,True
91582,Softmax function slows down for data with large range module: performance module: cuda triaged,2023-01-02 16:11:42+00:00,,0,0,module: performance module: cuda triaged,True
91581,LBFGS wolfe exceeds the maximum allowed iterations module: optimizer triaged actionable,2023-01-02 15:46:04+00:00,,0,1,module: optimizer triaged actionable,True
91577,[RFC] FP8 dtype introduction to PyTorch oncall: quantization triaged,2023-01-02 10:33:20+00:00,,1,14,oncall: quantization triaged,False
91574,Add BlockWise Distribution Support to the torch.distributions Package module: distributions triaged,2023-01-02 00:49:27+00:00,,0,1,module: distributions triaged,True
91570,Security policy impractical / lacks contact information? high priority module: docs triaged security,2023-01-01 16:46:36+00:00,,2,3,high priority module: docs triaged security,True
93498,torch.compiled mish function is x5 slower than eager (CPU) triaged bug oncall: pt2 module: cpu inductor,2023-01-01 11:14:12+00:00,,1,4,triaged bug oncall: pt2 module: cpu inductor,True
91566,Build Error: OpenMP library could not be found.  Proceeding might lead to highly sub-optimal performance. module: build triaged module: mkldnn module: third_party,2023-01-01 04:18:14+00:00,,0,1,module: build triaged module: mkldnn module: third_party,True
91565,min/max not supported for Long dtype on MPS triaged module: mps,2023-01-01 03:39:09+00:00,,0,2,triaged module: mps,True
91562,`torch::jit::optimize_for_inference` doesn't preserve exported methods when calling `freeze` oncall: jit,2022-12-31 20:18:46+00:00,,0,0,oncall: jit,True
91557,Segmentation fault when running torch.nn.AdaptiveMaxPool3d triaged module: edge cases,2022-12-31 16:20:43+00:00,,0,0,triaged module: edge cases,True
91556,Overflow when running torch.nn.AdaptiveMaxPool3d on torch 1.12.0 and 1.13.1 triaged module: edge cases,2022-12-31 16:11:21+00:00,,0,1,triaged module: edge cases,True
91553,Segmentation fault when running torch.nn.AdaptiveMaxPool2d triaged module: edge cases,2022-12-31 15:03:20+00:00,,0,1,triaged module: edge cases,True
91552,Overflow when running torch.nn.AdaptiveMaxPool2d triaged module: pooling module: edge cases,2022-12-31 14:58:08+00:00,,0,1,triaged module: pooling module: edge cases,True
91545,Adding label smoothing option to `nn.BCELoss`  and `nn.BCEWithLogitsLoss`? module: nn module: loss triaged actionable,2022-12-31 08:16:23+00:00,,0,10,module: nn module: loss triaged actionable,True
91543,"Python 3.11.1 , even with nightly version of PyTorch: ERROR: No matching distribution found for torch module: binaries triaged",2022-12-31 04:09:18+00:00,,0,1,module: binaries triaged,True
91542,`torch.compile` frees computation graph in a GAN training setup and tries to call `backward` a second time module: autograd triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2022-12-31 01:40:33+00:00,,0,7,module: autograd triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
91536,The speed of matrix inversion is relatively slow for many small matrices module: performance module: cuda triaged module: linear algebra,2022-12-30 13:55:54+00:00,,0,2,module: performance module: cuda triaged module: linear algebra,True
91535,"When dist.broadcast float32 to int64, it will silently generate wrong results oncall: distributed",2022-12-30 10:24:23+00:00,,0,2,oncall: distributed,False
91533,Cannot cast float64 to float32 needs reproduction module: crash triaged module: macos module: numpy,2022-12-30 08:02:35+00:00,,0,3,needs reproduction module: crash triaged module: macos module: numpy,True
91524,functorch.so is installed back into the source directory module: build triaged module: functorch,2022-12-30 00:42:22+00:00,,0,0,module: build triaged module: functorch,True
91509,[functorch] make batch norm docs point to UX limitations triaged module: functorch,2022-12-29 16:31:49+00:00,,0,0,triaged module: functorch,True
91508,Update map_nt to take into account size and strides triaged module: nestedtensor bug,2022-12-29 15:53:20+00:00,,0,0,triaged module: nestedtensor bug,True
91504,torch.jit.script ERR: RuntimeError: Can't redefine method: forward on class: __torch__.SoSadModule oncall: jit,2022-12-29 13:04:39+00:00,,0,1,oncall: jit,True
91497,DISABLED test_tensor_requires_grad (test_jit.TestScript) oncall: jit module: flaky-tests skipped,2022-12-29 03:42:15+00:00,,0,5,oncall: jit module: flaky-tests skipped,False
91495,DISABLED test_rand (test_jit.TestScript) oncall: jit module: flaky-tests skipped,2022-12-29 03:42:12+00:00,,0,10,oncall: jit module: flaky-tests skipped,False
91494,DISABLED test_optional_tensor (test_jit.TestScript) oncall: jit module: flaky-tests skipped,2022-12-29 03:41:34+00:00,,0,7,oncall: jit module: flaky-tests skipped,False
91493,DISABLED test_prim_grad_undefined (test_jit.TestScript) oncall: jit module: flaky-tests skipped,2022-12-29 03:41:34+00:00,,0,7,oncall: jit module: flaky-tests skipped,False
91492,DISABLED test_requires_grad_loop (test_jit.TestScript) oncall: jit module: flaky-tests skipped,2022-12-29 03:41:09+00:00,,0,8,oncall: jit module: flaky-tests skipped,False
91489,DISABLED test_successful (jit.test_freezing.TestMKLDNNReinplacing) oncall: jit module: flaky-tests skipped,2022-12-29 03:40:39+00:00,,0,5,oncall: jit module: flaky-tests skipped,False
91488,DISABLED test_switch_inputs_to_inplace (jit.test_freezing.TestMKLDNNReinplacing) oncall: jit module: flaky-tests skipped,2022-12-29 03:40:36+00:00,,0,4,oncall: jit module: flaky-tests skipped,False
91486,DISABLED test_always_alive_values (jit.test_freezing.TestMKLDNNReinplacing) oncall: jit module: flaky-tests skipped,2022-12-29 03:40:33+00:00,,0,5,oncall: jit module: flaky-tests skipped,False
91484,DISABLED test_optional_list (test_jit.TestScript) oncall: jit module: flaky-tests skipped,2022-12-29 03:39:46+00:00,,0,7,oncall: jit module: flaky-tests skipped,False
91482,DISABLED test_tensor_as_tensor_shape_prop (test_jit.TestScript) oncall: jit module: flaky-tests skipped,2022-12-29 03:39:40+00:00,,0,8,oncall: jit module: flaky-tests skipped,False
91481,DISABLED test_merge_liveness (jit.test_freezing.TestMKLDNNReinplacing) oncall: jit module: flaky-tests skipped,2022-12-29 03:39:37+00:00,,0,2,oncall: jit module: flaky-tests skipped,False
91471,Clean up nt impl duplicates where one can triaged better-engineering module: nestedtensor release notes: nested tensor,2022-12-28 22:22:31+00:00,,0,0,triaged better-engineering module: nestedtensor release notes: nested tensor,True
91470,torch.compile loud error on functorch transforms triaged oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher,2022-12-28 22:09:33+00:00,,0,2,triaged oncall: pt2 module: functorch module: dynamo module: pt2-dispatcher,True
91469,torch.compile with aotautograd does not support double backwards module: autograd triaged enhancement oncall: pt2 module: aotdispatch module: pt2-dispatcher,2022-12-28 21:55:27+00:00,,0,4,module: autograd triaged enhancement oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
91468,torch.compile incorrect when imperative autograd APIs are used high priority module: autograd triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2022-12-28 21:54:17+00:00,,1,4,high priority module: autograd triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
91467,DISABLED test_fs_preserve_sharing (__main__.TestMultiprocessing) module: multiprocessing triaged skipped module: dynamo,2022-12-28 21:41:24+00:00,,0,2,module: multiprocessing triaged skipped module: dynamo,False
91447,"Degenerate ranges are allowed in NumPy, but not in PyTorch. triaged module: numpy",2022-12-28 17:30:34+00:00,,0,1,triaged module: numpy,True
91439,Pytorch2.0 doesn't support compiling GRU and RNN model feature triaged oncall: pt2 module: dynamo,2022-12-28 10:15:53+00:00,,0,8,feature triaged oncall: pt2 module: dynamo,False
91437,using Tensor subclass between vmap layers triaged module: __torch_dispatch__ tensor subclass module: functorch,2022-12-28 08:28:12+00:00,,0,2,triaged module: __torch_dispatch__ tensor subclass module: functorch,True
91435,Batch_first attribute in quantizable multiheadattention oncall: transformer/mha,2022-12-28 07:31:43+00:00,,0,0,oncall: transformer/mha,False
91417,[bazel] error: use of undeclared identifier 'cudaGraphDebugDotPrint' module: build module: cuda triaged,2022-12-27 22:33:40+00:00,,0,6,module: build module: cuda triaged,True
91406,Pytorch clang-tidy header-filter is still broken module: build triaged,2022-12-27 17:02:51+00:00,,0,3,module: build triaged,True
91396,[JIT] Zero-channel conv2d cannot be applied with `optimize_for_inference` oncall: jit,2022-12-27 03:42:21+00:00,,0,1,oncall: jit,False
91395,PyObject preservation and resurrection for `StorageImpl` module: internals triaged enhancement better-engineering,2022-12-26 23:08:25+00:00,,1,18,module: internals triaged enhancement better-engineering,True
91387,getting issue 'typeindex' file not found in Littorch-Lite/install/include/ATen/core/custom_class.h oncall: mobile,2022-12-26 12:39:29+00:00,,0,2,oncall: mobile,True
91375,Internal Assert failed module: cuda triaged module: cuda graphs,2022-12-24 21:03:33+00:00,,0,0,module: cuda triaged module: cuda graphs,True
91374,[RFC] `quantile` should work for `float16`/`half` on the GPU module: cuda triaged enhancement module: half,2022-12-24 18:05:04+00:00,,0,0,module: cuda triaged enhancement module: half,True
91408,`NotImplementedError` when using `torch.distributed.launch` oncall: distributed module: functorch,2022-12-24 14:04:56+00:00,,0,4,oncall: distributed module: functorch,False
91368,"PyTorch memory leak reference cycle in for loop, Mac M1  triaged module: arm module: mps",2022-12-23 21:24:25+00:00,,1,5,triaged module: arm module: mps,True
91363,MPS backend does not accept int64 model weights or input data triaged module: mps,2022-12-23 16:24:52+00:00,,0,5,triaged module: mps,True
91360,Offer `get_buffer` and `get_submodule` in `ScriptModule`? oncall: jit,2022-12-23 14:47:03+00:00,,0,0,oncall: jit,False
91358,[JIT] .backward() not supported by JIT trace oncall: jit,2022-12-23 11:51:14+00:00,,0,1,oncall: jit,True
91356,nop_partitioner for AOTAutograd feature triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2022-12-23 10:42:03+00:00,,0,2,feature triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
91353,Perf reduction due to munmap with dataloader pinning thread ? module: dataloader triaged,2022-12-23 07:07:05+00:00,,0,4,module: dataloader triaged,True
91352,"Internal error during ONNX export, diagnostic unusable  module: onnx triaged",2022-12-23 06:43:45+00:00,,1,5,module: onnx triaged,True
91347,Remove redundant logics triaged oncall: pt2,2022-12-23 02:33:42+00:00,,0,3,triaged oncall: pt2,False
91334,Pytorch 1.13 conda package with cuda requires too many unneccessary packages module: binaries module: cuda triaged,2022-12-22 23:11:14+00:00,,0,8,module: binaries module: cuda triaged,True
91325,Support for saving multiple storages/tensors that view same data as different dtypes feature module: serialization triaged,2022-12-22 20:17:51+00:00,,1,1,feature module: serialization triaged,True
91323,Expand torch.utils._pytree.tree_map feature triaged actionable module: pytree module: functorch,2022-12-22 19:56:47+00:00,,0,0,feature triaged actionable module: pytree module: functorch,True
91311,Profiler is not properly recording seq number when any key above autograd is used oncall: profiler,2022-12-22 15:30:01+00:00,,0,0,oncall: profiler,False
91310,AOT Autograd should allow backend compilers to see input mutations triaged oncall: pt2 module: functorch module: pt2-dispatcher,2022-12-22 15:26:20+00:00,,0,1,triaged oncall: pt2 module: functorch module: pt2-dispatcher,False
91309,iSTFT produces RuntimeError with center=False and Blackman/Bartlett/Hann windows triaged module: fft,2022-12-22 13:32:19+00:00,,0,19,triaged module: fft,True
91305,`nn.TransformerEncoderLayer` fastpath (BetterTransformer) is slower than the normal path when no mask is provided oncall: transformer/mha,2022-12-22 11:09:19+00:00,,0,4,oncall: transformer/mha,False
91300,[🚀 Feature Request] pdf and sampling from Alpha-stable distribution  module: distributions feature triaged,2022-12-22 07:42:06+00:00,,0,7,module: distributions feature triaged,True
91293,torch.fx tracer emits type error when tracing module that directly contains and uses the torch.cat() function oncall: fx,2022-12-22 04:46:54+00:00,,0,8,oncall: fx,True
91280,custom Function that supports functorch jvp doesn't work with in-place triaged module: forward ad module: functorch,2022-12-21 23:03:52+00:00,,0,0,triaged module: forward ad module: functorch,True
91278,Keep getting ChildFailedError Error oncall: distributed,2022-12-21 22:31:18+00:00,,0,6,oncall: distributed,True
91274,torch.compile for calling func(**kwargs) triaged oncall: pt2,2022-12-21 22:19:37+00:00,,0,2,triaged oncall: pt2,False
93493,tensor.to_sparse() handling indices incorrectly under dynamo/fake tensor triaged bug oncall: pt2 module: dynamic shapes,2022-12-21 21:21:05+00:00,,0,2,triaged bug oncall: pt2 module: dynamic shapes,True
91263,Make quant_min/quant_max required for observer/fake_quant oncall: quantization triaged,2022-12-21 20:40:05+00:00,,1,0,oncall: quantization triaged,True
91252,Open file leak when dataloader is using persistent_workers and pin_memory AND you create multiple dataloaders.   module: dataloader triaged module: data,2022-12-21 16:41:23+00:00,,0,2,module: dataloader triaged module: data,True
91251,Potential bug found with pybind11 dec_ref while gil released triaged module: pybind shadow review bug,2022-12-21 16:40:44+00:00,,1,5,triaged module: pybind shadow review bug,True
91249,Use dynamo to detect incorrect op schemas automatically triaged oncall: pt2 module: dynamo,2022-12-21 16:19:18+00:00,,0,5,triaged oncall: pt2 module: dynamo,True
91245,Segmentation faults in DataLoader (in latest torch version). needs reproduction module: dataloader triaged,2022-12-21 13:37:52+00:00,,0,10,needs reproduction module: dataloader triaged,True
91244,first class dims leak memeory triaged module: functorch,2022-12-21 13:37:38+00:00,,0,0,triaged module: functorch,True
93492,ONNX export question (using torchdynamo) triaged bug oncall: pt2,2022-12-21 13:32:27+00:00,,0,8,triaged bug oncall: pt2,True
91229,JIT mishandles torch.round() in PyTorch 1.10.1 oncall: jit,2022-12-21 05:31:43+00:00,,0,1,oncall: jit,False
91210,Odd/hand-wavy mathematical notation for Conv2D module: docs triaged,2022-12-20 22:41:58+00:00,,0,0,module: docs triaged,True
91205,dcp resharding does not work for optimizer state_dict triaged module: distributed_checkpoint,2022-12-20 22:08:20+00:00,,1,0,triaged module: distributed_checkpoint,True
91199,functorch.functionalize doesn't work with torch.autograd.grad triaged module: functorch,2022-12-20 21:21:25+00:00,,0,2,triaged module: functorch,True
91184,DISABLED test_index_add_correctness (__main__.TestTorch) triaged module: flaky-tests skipped module: scatter & gather ops,2022-12-20 18:40:51+00:00,,0,10,triaged module: flaky-tests skipped module: scatter & gather ops,False
91182,onednn(mkldnn) backend support for quantized operators oncall: quantization triaged module: arm,2022-12-20 18:05:09+00:00,,1,20,oncall: quantization triaged module: arm,True
91173,not able to import pipelines as torch.distributed is missing oncall: distributed pipeline parallelism,2022-12-20 16:22:11+00:00,,0,2,oncall: distributed pipeline parallelism,False
91165,[FSDP] FSDP with CPU offload consumes `1.65X` more GPU memory when training models with most of the params frozen high priority triage review oncall: distributed triaged module: fsdp,2022-12-20 10:49:54+00:00,,0,18,high priority triage review oncall: distributed triaged module: fsdp,True
91156,`quantile` fails for `float16`/`half` inputs module: cpu triaged enhancement module: half,2022-12-20 06:12:31+00:00,,0,5,module: cpu triaged enhancement module: half,True
91136,[Composable] Enable summon_full_params for fully_shard oncall: distributed triaged module: fsdp,2022-12-20 00:10:13+00:00,,0,1,oncall: distributed triaged module: fsdp,True
91135,[BE] Investigate FSDP test _zero_model  oncall: distributed triaged module: fsdp,2022-12-20 00:08:49+00:00,,0,1,oncall: distributed triaged module: fsdp,True
93491,Is there a way to write passes? triaged enhancement oncall: pt2,2022-12-19 21:50:21+00:00,,0,3,triaged enhancement oncall: pt2,False
91099,[Dynamo] Graph Re-compilation Invoked by Changes of Unused Dict Values triaged oncall: pt2 module: dynamo,2022-12-19 14:48:30+00:00,,0,2,triaged oncall: pt2 module: dynamo,False
91098,[TorchScript] Failed to Forward Correct Number of Arguments to Different Functions oncall: jit,2022-12-19 14:38:07+00:00,,0,4,oncall: jit,False
93489,Remove redundant memory copy for HF multi-attention submodule for cpu path using MKL prepack triaged bug oncall: pt2 module: cpu inductor,2022-12-19 09:29:44+00:00,,1,3,triaged bug oncall: pt2 module: cpu inductor,True
91073,Implement L1 and L2 gradient as hooks with the option of changing the weight decay value. module: nn module: optimizer triaged needs design,2022-12-18 06:54:25+00:00,,0,0,module: nn module: optimizer triaged needs design,True
91072,Unexpected behavior when running torch.max in cuda needs reproduction module: cuda triaged,2022-12-18 05:08:31+00:00,,0,1,needs reproduction module: cuda triaged,True
93488,"If minifier test fails, stderr/stdout of subprocess calls is not printed triaged bug",2022-12-17 04:55:49+00:00,,1,0,triaged bug,True
91039,Simple deleting from the sys cache fails on reimport triaged enhancement module: python frontend,2022-12-16 21:54:24+00:00,,0,3,triaged enhancement module: python frontend,True
93486,A Simple Function Causing Graph Break triaged bug,2022-12-16 18:42:16+00:00,,0,1,triaged bug,True
91000,node.stack_trace does not handle escaping correctly oncall: fx,2022-12-16 14:53:54+00:00,,0,1,oncall: fx,False
90999,overflow (?) on cuda tensor after matrix multiplication module: numerical-stability module: cuda triaged,2022-12-16 14:49:24+00:00,,0,1,module: numerical-stability module: cuda triaged,True
90998,"Crash in `index_select` with singleton `self`, non-singleton `index` triaged",2022-12-16 14:09:27+00:00,,0,4,triaged,True
90992,as_strided_scatter : INTERNAL_ASSERT_FAILED for requires_grad=True and non-config input triaged module: functionalization,2022-12-16 08:51:29+00:00,,0,1,triaged module: functionalization,True
93484,TorchDynamo doesn't inline modified nn.Modules forward - Fails with Huggingface Accelerate high priority triaged bug oncall: pt2,2022-12-16 01:43:34+00:00,,1,8,high priority triaged bug oncall: pt2,True
90954,[Composable] Enable setting state_dict_type high priority triage review oncall: distributed triaged module: fsdp,2022-12-15 21:40:34+00:00,,1,0,high priority triage review oncall: distributed triaged module: fsdp,True
90952,Add support for torch.zero_grad in dynamo w/ dynamic shapes triaged oncall: pt2,2022-12-15 21:13:26+00:00,,0,1,triaged oncall: pt2,False
90923,dynamo.optimizations.training.aot_autograd does not trace correct overload triaged oncall: pt2,2022-12-15 15:11:28+00:00,,0,10,triaged oncall: pt2,False
90920,Support for Transformer Models on Android with Vulkan Backend triaged module: vulkan,2022-12-15 14:26:43+00:00,,0,4,triaged module: vulkan,True
90916,Functorch does not work with CrossEntropyLoss and label=-100 triaged actionable module: functorch,2022-12-15 12:49:35+00:00,,0,1,triaged actionable module: functorch,True
90915,Torch SummaryWriter import fails with torch 2.0 with an error on numpy.object module: tensorboard oncall: visualization,2022-12-15 12:47:34+00:00,,0,1,module: tensorboard oncall: visualization,True
93483,Error in guard code crashes process NULL ERROR: /Users/ezyang/Dev/pytorch-metal/torch/csrc/dynamo/eval_frame.c:251 triaged bug oncall: pt2,2022-12-15 10:53:05+00:00,,0,0,triaged bug oncall: pt2,True
90905,Retrieve Tensor from Tensor.data_ptr() triaged core issue,2022-12-15 09:04:08+00:00,,0,5,triaged core issue,True
90900,Check that SymPy semantics match Python semantics triaged,2022-12-15 07:26:45+00:00,,0,3,triaged,True
90895,ModuleNotFoundError: No module named 'torch._C._distributed_c10d'; 'torch._C' is not a package module: build triaged,2022-12-15 03:48:54+00:00,,0,0,module: build triaged,True
90894,DISABLED test_numpy_ref_mps_nn_functional_group_norm_mps_float32 (__main__.TestCommonMPS) triaged module: flaky-tests skipped module: mps,2022-12-15 03:41:06+00:00,,0,1,triaged module: flaky-tests skipped module: mps,False
93482,[inductor] Add more matmul configurations to `TORCHINDUCTOR_MAX_AUTOTUNE=1` mode triaged enhancement oncall: pt2,2022-12-15 01:15:55+00:00,,0,0,triaged enhancement oncall: pt2,False
90885,TorchScript with complex abs doesn't work in backward oncall: jit,2022-12-15 00:21:57+00:00,,0,1,oncall: jit,False
90878,Exporter for ONNX GroupNormalization operator module: onnx triaged,2022-12-14 23:28:30+00:00,,0,1,module: onnx triaged,False
93481,Umbrella issue for weakref related Dynamo PyTorch test suite failures triaged bug oncall: pt2,2022-12-14 22:03:27+00:00,,0,0,triaged bug oncall: pt2,True
93480,Umbrella issue for only populate real_value_cache in export test suite fails triaged bug oncall: pt2,2022-12-14 21:42:03+00:00,,0,0,triaged bug oncall: pt2,True
90866,Support arbitrary masks for _nested_tensor_from_mask in nn.TransformerEncoder triaged module: nestedtensor oncall: transformer/mha,2022-12-14 21:37:02+00:00,,0,1,triaged module: nestedtensor oncall: transformer/mha,True
93479,Umbrella issue for PyTorch test suite failures from torch.* returned non-Tensor output unimplemented triaged bug oncall: pt2,2022-12-14 21:35:37+00:00,,0,1,triaged bug oncall: pt2,True
90857,[FSDP] Prepare to deprecate `FullyShardedDataParallel.<attrs>` oncall: distributed triaged module: fsdp,2022-12-14 20:25:25+00:00,,0,0,oncall: distributed triaged module: fsdp,True
90848,[Distributed] Destruction order fiasco in ProcessGroupNCCL workCleanupLoop() oncall: distributed module: c10d,2022-12-14 17:28:54+00:00,,0,5,oncall: distributed module: c10d,False
90844,AOT Autograd doesn't respect no_grad() during input mutations triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2022-12-14 16:41:57+00:00,,0,1,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
90842,nn.MultiheadAttention softmax inconsistent in training mode  oncall: transformer/mha,2022-12-14 15:12:28+00:00,,0,4,oncall: transformer/mha,False
90837,[FSDP] `fully_shard` Follow-Ups & Known Issues oncall: distributed triaged module: fsdp,2022-12-14 13:36:25+00:00,,1,0,oncall: distributed triaged module: fsdp,True
90832,Inference Mode docs module: docs triaged inference mode,2022-12-14 10:11:37+00:00,,0,2,module: docs triaged inference mode,False
90831,Error when using torch.compile with Pytorch2.0 needs reproduction triaged oncall: pt2,2022-12-14 10:04:14+00:00,,0,2,needs reproduction triaged oncall: pt2,True
90828,Compare oneDNN and OpenBLAS backend of PyTorch on arm64 architecture module: cpu triaged module: bfloat16 module: arm,2022-12-14 08:46:39+00:00,,0,5,module: cpu triaged module: bfloat16 module: arm,True
90827,Support for Pylint module: lint triaged,2022-12-14 07:43:09+00:00,,0,0,module: lint triaged,True
90820,Support `divmod` for tensors triaged module: numpy,2022-12-14 05:26:41+00:00,,0,2,triaged module: numpy,True
90793,nn.CrossEntropyLoss error out when the sample size is large module: nn module: cuda module: memory usage triaged module: edge cases,2022-12-13 20:58:39+00:00,,0,1,module: nn module: cuda module: memory usage triaged module: edge cases,True
90786,"[Composable API] Add `fully_shard` state dict unit test after manual ""wrapping"" is supported oncall: distributed triaged module: fsdp",2022-12-13 19:43:12+00:00,,1,0,oncall: distributed triaged module: fsdp,True
90784,[FSDP] Investigate `test_fsdp_pure_fp16.py` inaccuracy oncall: distributed triaged module: fsdp,2022-12-13 19:39:37+00:00,,0,0,oncall: distributed triaged module: fsdp,True
90775,"Extend ""torch.utils.cpp_extension.load"" for both lib64 and **lib** module: cpp-extensions triaged",2022-12-13 17:35:30+00:00,,0,0,module: cpp-extensions triaged,True
90774,Cannot compile torchtext model triaged oncall: pt2 module: dynamo,2022-12-13 17:28:48+00:00,,1,3,triaged oncall: pt2 module: dynamo,False
90768,PyTorch 2.0 not working on Windows module: windows triaged oncall: pt2,2022-12-13 15:00:47+00:00,,0,18,module: windows triaged oncall: pt2,False
90760,Large slow down by not calling `torch.set_num_threads` module: performance module: cpu triaged module: linear algebra,2022-12-13 12:26:43+00:00,,0,6,module: performance module: cpu triaged module: linear algebra,True
90752,Adam (fused=True) issues module: performance module: optimizer module: cuda triaged,2022-12-13 08:32:36+00:00,,0,12,module: performance module: optimizer module: cuda triaged,True
90751,pytorch prune in libtorch module: cpp triaged,2022-12-13 08:26:07+00:00,,0,1,module: cpp triaged,True
93477, [Inductor] [CPU] optimize thread parallel and loop collapse triaged oncall: pt2 module: cpu inductor,2022-12-13 03:06:27+00:00,,1,0,triaged oncall: pt2 module: cpu inductor,False
90742,Adopt full_backward_pre_hook in DDP oncall: distributed triaged module: ddp,2022-12-13 01:54:01+00:00,,0,1,oncall: distributed triaged module: ddp,True
93476,test_copy_broadcast triaged,2022-12-12 20:13:02+00:00,,1,0,triaged,True
90708,PixelShuffle/Unshuffle Channels Last Support on GPU triaged module: memory format,2022-12-12 19:35:05+00:00,,0,0,triaged module: memory format,True
90703,RuntimeError: kind_.is_prim() INTERNAL ASSERT FAILED. Only prim ops are allowed to not have a registered operator but aten::mul doesn't have one either. We don't know if this op has side effects. module: onnx triaged module: primTorch,2022-12-12 17:35:13+00:00,,0,0,module: onnx triaged module: primTorch,True
90695,`torch.empty` produces incorrect tensors with `layout=sparse_csr|sparse_csc` on the CPU module: sparse module: docs module: cpu triaged,2022-12-12 14:43:26+00:00,,1,1,module: sparse module: docs module: cpu triaged,True
90691,[ONNX] Exporting the operator ::concat to ONNX opset version 13 is not supported. module: onnx triaged,2022-12-12 11:09:40+00:00,,0,4,module: onnx triaged,False
90688,In distributed get SIGTERM and run crash oncall: distributed,2022-12-12 08:55:18+00:00,,0,0,oncall: distributed,True
90676,RuntimeError: Error in dlopen: libnvJitLink.so.12: cannot open shared object file: No such file or directory module: crash module: distributions triaged,2022-12-12 02:01:29+00:00,,0,0,module: crash module: distributions triaged,True
90663,[Feature Request] An alternative sampling routine for Dirichlet to fix Dirichlet and Beta sampling bugs module: distributions feature triaged,2022-12-11 18:10:01+00:00,,0,2,module: distributions feature triaged,True
90643,[FSDP][BE] `test_fsdp_comm_hooks.py` cleanup oncall: distributed triaged module: fsdp,2022-12-11 04:16:04+00:00,,0,0,oncall: distributed triaged module: fsdp,True
90633,torch.min document not up to date module: docs triaged,2022-12-10 22:41:17+00:00,,0,1,module: docs triaged,True
90613,`torch.inverse` multi-threading RuntimeError: lazy wrapper should be called at most once triaged module: multithreading module: linear algebra,2022-12-10 14:35:50+00:00,,0,13,triaged module: multithreading module: linear algebra,True
90608,Operator overload priority should not rely on static initialization order oncall: jit,2022-12-10 09:55:21+00:00,,0,5,oncall: jit,True
90607,"Export to ONNX of `as_strided()` hard codes stride in the graph, although it should be dynamic module: onnx triaged release notes: onnx",2022-12-10 09:31:23+00:00,,1,2,module: onnx triaged release notes: onnx,False
90585,[threaded pg] All threads share one Random Number Generator triaged module: random module: dtensor,2022-12-10 00:07:38+00:00,,0,0,triaged module: random module: dtensor,True
93474,AttributeError: 'tuple' object has no attribute 'grad' triaged,2022-12-09 23:32:19+00:00,,1,9,triaged,True
90578,"Multiprocessing ""Error Propagation"" doesn't work for FullyShardedDataParallelism. oncall: distributed module: fsdp",2022-12-09 22:36:50+00:00,,0,2,oncall: distributed module: fsdp,False
90574,Bfloat16 tensor .numpy() support  triaged module: numpy module: bfloat16,2022-12-09 22:13:05+00:00,,0,10,triaged module: numpy module: bfloat16,True
90560,"[discussion, idea] Batched, vectorized base64 decoding / encoding + maybe RLE decoding / encoding feature triaged module: vision module: nestedtensor",2022-12-09 19:08:10+00:00,,0,7,feature triaged module: vision module: nestedtensor,True
90555,[RFC] Add torch.backends.tbb.is_available() triaged enhancement module: tbb,2022-12-09 18:30:47+00:00,,0,4,triaged enhancement module: tbb,True
90553,Embedding dynamic quantization is not documented and hard to use oncall: quantization triaged,2022-12-09 17:44:22+00:00,,1,9,oncall: quantization triaged,True
90551,Could not run 'aten::as_strided' with arguments from the 'Metal' backend. triaged module: ios,2022-12-09 16:51:38+00:00,,0,2,triaged module: ios,True
90549,Abort called in FSDP tests oncall: distributed module: fsdp,2022-12-09 15:31:22+00:00,,0,1,oncall: distributed module: fsdp,False
90547,Unable to link LibTorch against CUDA and CUDNN statically triaged module: static linking,2022-12-09 15:17:11+00:00,,0,1,triaged module: static linking,True
90544,[Dispatchable Collectives] Follow up tasks oncall: distributed triaged,2022-12-09 14:57:54+00:00,,1,0,oncall: distributed triaged,True
90540,torch.compile() BackendCompilerFailed: _compile_fn raised RuntimeError needs reproduction triaged bug oncall: pt2 module: dynamo,2022-12-09 12:40:07+00:00,,0,3,needs reproduction triaged bug oncall: pt2 module: dynamo,True
90536,Illegal hardware instruction following Real Time Inference on Raspberry Pi 4 tutorial oncall: quantization low priority triaged module: arm,2022-12-09 10:01:11+00:00,,0,4,oncall: quantization low priority triaged module: arm,True
90535,Illegal hardware instruction using torch.nn.Conv2d on aarch64 (Raspberry Pi 4) module: crash triaged,2022-12-09 09:52:05+00:00,,0,2,module: crash triaged,True
90526,valgrind failure `Conditional jump or move depends on uninitialised value(s)` triaged module: sanitizers,2022-12-09 04:40:42+00:00,,1,0,triaged module: sanitizers,True
90521,[A ERROR in Docker] RuntimeError: CUDA error: no kernel image is available for execution on the device needs reproduction module: cuda triaged module: docker,2022-12-09 02:46:16+00:00,,0,1,needs reproduction module: cuda triaged module: docker,True
90509,Can torchrun have a shell completion? oncall: distributed,2022-12-08 23:45:57+00:00,,0,0,oncall: distributed,False
90507,Functionalization on inplace_views should properly reflect autograd metadata triaged module: functionalization oncall: pt2 module: pt2-dispatcher,2022-12-08 23:36:05+00:00,,0,1,triaged module: functionalization oncall: pt2 module: pt2-dispatcher,False
90485,Tensor indexing and slicing documentation should explicitly state that indexing follows numpy semantics and link to the numpy indexing documentation. module: docs triaged module: numpy,2022-12-08 18:58:35+00:00,,0,0,module: docs triaged module: numpy,True
90481,Internal assert when ctx.saved_tensors fails when saving results of an intermediate view tensor with torch.utils.checkpoint and use_reentrant=False module: checkpoint module: autograd triaged has workaround actionable,2022-12-08 17:49:17+00:00,,0,2,module: checkpoint module: autograd triaged has workaround actionable,True
90466,Saving a scripted module to a buffer does not work. oncall: jit,2022-12-08 14:55:59+00:00,,0,0,oncall: jit,True
90465,[FSDP] Revisit meta device initialization oncall: distributed triaged module: fsdp,2022-12-08 13:27:52+00:00,,0,2,oncall: distributed triaged module: fsdp,True
90464,PR #89436 looks like it causes or enables a memory leak module: memory usage triaged module: mps,2022-12-08 12:22:03+00:00,,0,0,module: memory usage triaged module: mps,True
90460,"Assertion failed: scales.is_weights() && ""Resize scales must be an initializer!"" module: onnx triaged onnx-needs-info",2022-12-08 11:15:55+00:00,,1,1,module: onnx triaged onnx-needs-info,False
90459,Strange issue with tensor asyncio and RPC oncall: distributed,2022-12-08 10:36:56+00:00,,0,0,oncall: distributed,True
90447,Different behavior for complex numbers operations with numpy triaged module: complex module: numpy module: NaNs and Infs,2022-12-08 04:51:58+00:00,,0,0,triaged module: complex module: numpy module: NaNs and Infs,True
90440,RuntimeError: Placeholder storage has not been allocated on MPS device! triaged module: mps,2022-12-08 03:04:44+00:00,,0,4,triaged module: mps,False
90439,Torch 1.13 Onnx Scope name not correct! module: onnx triaged,2022-12-08 02:51:17+00:00,,0,7,module: onnx triaged,False
90424,A few functions in fbgemm_utils.cpp are defined in global namespace module: cpp oncall: quantization triaged,2022-12-07 23:21:57+00:00,,1,4,module: cpp oncall: quantization triaged,True
90412,Importing numpy makes Tensor min max crash triaged module: numpy,2022-12-07 21:17:44+00:00,,0,10,triaged module: numpy,True
90398,IValue(c10::List<IValue>) constructor is confusing and undocumented module: internals triaged,2022-12-07 18:03:56+00:00,,0,5,module: internals triaged,True
90395,"Cannot add target-level dependencies to non-existent target ""gloo_cuda"". module: build triaged",2022-12-07 17:38:51+00:00,,0,2,module: build triaged,True
90394,FX graph mode quant: backendconfig configuration missing for torch.nn.GRU oncall: quantization triaged,2022-12-07 17:30:24+00:00,,1,2,oncall: quantization triaged,False
90374,torch.utils.tensorboard import fails if a new protobuf > 3.20 is installed (bug in tensorboard/tensorflow but better guard against it) oncall: visualization,2022-12-07 10:06:38+00:00,,0,2,oncall: visualization,True
90373,"""Reached a code path in Module.get_extra_state() that should never be called."" needs reproduction oncall: jit triaged",2022-12-07 10:06:35+00:00,,0,1,needs reproduction oncall: jit triaged,True
90369,[JIT] Wrong type inference leads to misleading error message oncall: jit,2022-12-07 08:51:36+00:00,,0,0,oncall: jit,True
93470,Get the error: AttributeError: Can't pickle local object 'convert_frame.<locals>._convert_frame' high priority triage review triaged bug oncall: pt2 module: dynamo,2022-12-07 08:47:08+00:00,,1,14,high priority triage review triaged bug oncall: pt2 module: dynamo,True
90367,[JIT] INTERNAL ASSERT FAILED `torch.add` with boolean primitive constant oncall: jit,2022-12-07 08:23:55+00:00,,0,0,oncall: jit,True
90366,[JIT] INTERNAL ASSERT FAILED `torch.mul` with boolean primitive constant oncall: jit,2022-12-07 08:16:55+00:00,,0,0,oncall: jit,True
90365,[JIT] INTERNAL ASSERT FAILED when dispatching for `torch.Tensor.view` oncall: jit,2022-12-07 07:57:54+00:00,,0,1,oncall: jit,True
90347,[ONNX] test_mask_rcnn in test_models_onnxruntime.py failed with ONNX version==1.13.0 module: onnx triaged onnx-triaged,2022-12-07 02:25:44+00:00,,0,0,module: onnx triaged onnx-triaged,False
90320,[RFC] Allow FSDP mixed precision for only certain type of submodules oncall: distributed module: fsdp,2022-12-06 20:55:59+00:00,,1,0,oncall: distributed module: fsdp,False
90318,[Tracking Issue] Mixed precision does not work with ignored modules  oncall: distributed module: fsdp,2022-12-06 20:45:49+00:00,,1,0,oncall: distributed module: fsdp,True
90305,Inconsistent Hash of IValue between aten/src/ATen/core/ivalue.cpp and aten/src/ATen/core/Dict_inl.h oncall: jit,2022-12-06 19:00:57+00:00,,0,1,oncall: jit,True
90301,Unknown buildin op: aten::pad oncall: jit,2022-12-06 18:00:49+00:00,,0,0,oncall: jit,False
93468,torch._dynamo.exc.Unsupported: dynamic shapes: arange triaged bug oncall: pt2,2022-12-06 17:31:39+00:00,,0,62,triaged bug oncall: pt2,True
90289,quantization qconfig: can we set per-channel quant as default for qnnpack? oncall: quantization triaged,2022-12-06 15:21:35+00:00,,0,3,oncall: quantization triaged,True
90288,quantization observers: can we relax the default epsilon value? oncall: quantization low priority triaged,2022-12-06 15:14:34+00:00,,1,4,oncall: quantization low priority triaged,True
90284,Public API definition is not compatible with `torch.testing` triaged module: testing module: python frontend,2022-12-06 14:50:20+00:00,,1,11,triaged module: testing module: python frontend,True
90277,cannot backward() needs reproduction triaged module: mps,2022-12-06 12:43:56+00:00,,0,1,needs reproduction triaged module: mps,True
90263,Is it possible to add a parameter in torch.onnx.export to skip the prim::PythonOp subgraph process when exporting the autograd function? module: onnx triaged,2022-12-06 06:43:53+00:00,,1,3,module: onnx triaged,False
90261,Why torch.mode return different value between CPU and GPU module: cuda triaged,2022-12-06 06:14:55+00:00,,0,4,module: cuda triaged,True
90256,LibTorch static build from source missing libshm.so module: build triaged,2022-12-06 04:27:56+00:00,,0,0,module: build triaged,True
90245,[Distributed] `Invalid scalar type` when `dist.scatter()` boolean tensor oncall: distributed,2022-12-06 01:53:56+00:00,,0,5,oncall: distributed,True
93466,Strategy for optimizing away transient dynamic shapes / device syncs triaged bug oncall: pt2,2022-12-05 22:31:25+00:00,,0,2,triaged bug oncall: pt2,True
93465,Graph breaks with HuggingFace Stable Diffusion triaged bug,2022-12-05 19:30:10+00:00,,1,6,triaged bug,True
90194,Unexpected behaviour of 1.13.0 triaged module: advanced indexing,2022-12-05 17:53:30+00:00,,0,3,triaged module: advanced indexing,True
90181,Graph is renamed in torch.jit oncall: jit,2022-12-05 15:24:44+00:00,,0,0,oncall: jit,False
93464,wav2vec2 model: error trying to do inference triaged bug,2022-12-05 14:18:16+00:00,,2,4,triaged bug,True
90171,Option to let DistributedDataParallel know in advance unused parameters at each forward pass oncall: distributed enhancement,2022-12-05 11:43:12+00:00,,0,5,oncall: distributed enhancement,False
90169,Unable to export CFlow model to ONNX module: onnx triaged,2022-12-05 11:24:57+00:00,,0,23,module: onnx triaged,False
90147,[Feature Proposal] Extend torch hub to better support cloud serving and edge deployment feature triaged module: hub,2022-12-04 23:28:48+00:00,,0,1,feature triaged module: hub,False
90138,p.block != nullptr && p.block->ptr != nullptr INTERNAL ASSERT FAILED needs reproduction triaged,2022-12-04 16:14:51+00:00,,0,1,needs reproduction triaged,True
90126,torch._dynamo.exc.BackendCompilerFailed: compile_fx raised TypeError: tqdm.__init__() got an unexpected keyword argument 'desc' triaged module: hub,2022-12-03 21:05:00+00:00,,0,1,triaged module: hub,True
90115,Couldn't install pytorch 2.0 needs reproduction module: binaries triaged,2022-12-03 14:41:59+00:00,,0,7,needs reproduction module: binaries triaged,True
90114,documentation need to be as per python version module: docs triaged,2022-12-03 11:14:42+00:00,,0,0,module: docs triaged,True
90107,Tensor.uniform_ fails to compile when using torch._dynamo triaged module: initialization module: dynamo,2022-12-03 04:51:53+00:00,,1,2,triaged module: initialization module: dynamo,True
90057,[GradScaler] Inconsistent scale values across different GPUs caused by uneven inputs for AMP DDP training triaged module: amp (automated mixed precision),2022-12-02 13:12:54+00:00,,0,1,triaged module: amp (automated mixed precision),True
90065,forward-mode AD formula for torch.add (and possibly others) accidentally upcasts float32 to float64 module: performance module: autograd triaged actionable ZeroTensor module: forward ad module: functorch,2022-12-02 10:31:11+00:00,,1,9,module: performance module: autograd triaged actionable ZeroTensor module: forward ad module: functorch,True
90052,DDP overlapped optimizer: set grads to None enhancements oncall: distributed module: data parallel module: ddp,2022-12-02 08:13:49+00:00,,0,0,oncall: distributed module: data parallel module: ddp,True
90019,[feature request] Need dtype torch.complex64 support on MPS Device feature triaged module: mps,2022-12-01 20:49:38+00:00,,0,4,feature triaged module: mps,False
93463,Traceable tensor subclasses cannot actually be used with AOTAutograd triaged bug,2022-12-01 14:25:24+00:00,,0,0,triaged bug,True
93462,TensorWithTFOverrideVariable unwraps too early triaged bug,2022-12-01 14:19:13+00:00,,2,1,triaged bug,True
90000,"Can not use x=torch.tensor(b), to create a Tensor out of a List[List[Tensor]] (A List of Lists of Tensors) oncall: jit triaged",2022-12-01 09:59:20+00:00,,0,4,oncall: jit triaged,True
93461,Support getattr/setattr user properties on Tensor triaged bug,2022-12-01 04:42:19+00:00,,1,1,triaged bug,True
89987,"Error in Adam.step(): If capturable=True, params and state_steps must be CUDA tensors. module: optimizer triaged actionable",2022-12-01 02:52:34+00:00,,0,1,module: optimizer triaged actionable,True
93458,minified code can not produce fp64_ref result triaged,2022-11-30 22:21:57+00:00,,0,1,triaged,True
89959,Calling item() on symbolic shape fake tensor should give more clear error message triaged module: fakeTensor,2022-11-30 21:01:17+00:00,,0,1,triaged module: fakeTensor,True
89954,"Random sampling from a tensor constructed on MPS device, results in elements returning as torch.zeros(tensor[i].shape) triaged module: mps",2022-11-30 20:25:57+00:00,,0,0,triaged module: mps,True
89942,Random K compression hook in PyTorch DDP feature triaged module: ddp,2022-11-30 19:07:28+00:00,,0,1,feature triaged module: ddp,True
89908,Export to ONNX with export_modules_as_functions works wrong module: onnx triaged,2022-11-30 11:19:41+00:00,,1,1,module: onnx triaged,False
89907,nn.CrossEntropy/nn.NLLLoss : Request for option to specify invalid ignore_index for perf. optimization module: loss triaged,2022-11-30 10:54:50+00:00,,0,5,module: loss triaged,True
93457,[Dynamo] Examples that recompile beyond cache size limit   triaged,2022-11-30 07:26:36+00:00,,0,0,triaged,False
93456,Way to run accuracy minifier on only one particular subgraph triaged bug,2022-11-30 03:55:04+00:00,,1,0,triaged bug,True
89884,[RFC] PyTorch Tensor Parallel(TP) User API for Distributed Training triaged module: dtensor,2022-11-30 00:29:54+00:00,,1,4,triaged module: dtensor,True
93455,Performance regression on interpolation in Kornia triaged bug,2022-11-30 00:03:26+00:00,,0,1,triaged bug,True
89868,No pytorch_jni.dll file in libtorch 1.13.0 lib folder oncall: java,2022-11-29 21:37:43+00:00,,0,1,oncall: java,False
89835,torch1.13  quantized model export onnx error module: onnx triaged,2022-11-29 11:53:40+00:00,,0,3,module: onnx triaged,True
89834,Wrong output type hint for `F.one_hot` module: typing triaged,2022-11-29 11:03:07+00:00,,0,1,module: typing triaged,True
89829,update transformer init function oncall: transformer/mha,2022-11-29 08:58:37+00:00,,0,0,oncall: transformer/mha,False
89820,The current example for `torch.mode` is IMHO confusing and has room for improvement. module: docs triaged,2022-11-29 03:40:57+00:00,,0,0,module: docs triaged,False
89817,"Basic math operations produce a ""floating point exception"" needs reproduction module: crash module: cpu triaged",2022-11-29 02:36:33+00:00,,2,26,needs reproduction module: crash module: cpu triaged,True
89784,InvokeAI using MPS is broken by torch nightlies since torch-1.14.0.dev20221104 inclusive  triaged module: correctness (silent) module: mps,2022-11-28 20:57:36+00:00,,0,4,triaged module: correctness (silent) module: mps,True
89764,addcmul on CUDA does not have the correct FMA behavior module: numerical-stability module: cuda triaged,2022-11-28 17:38:08+00:00,,0,1,module: numerical-stability module: cuda triaged,True
89763,DISABLED test_hf_bert_ddp_inductor (__main__.TestFakeDistributedSingleProc) triaged skipped module: inductor module: distributed,2022-11-28 17:35:13+00:00,,1,1,triaged skipped module: inductor module: distributed,False
93454,MMDet 3.x cannot run successfully in inductor mode triaged bug,2022-11-28 06:11:29+00:00,,1,2,triaged bug,True
89757,third-order gradient of torch.pow with tensor args and certain input returns NaN module: autograd triaged module: NaNs and Infs,2022-11-28 05:33:34+00:00,,0,11,module: autograd triaged module: NaNs and Infs,True
89738,[MPS] Add support for aten::repeat_interleave.self_Tensor for MPS backend triaged module: mps,2022-11-28 05:12:29+00:00,,0,1,triaged module: mps,False
89730,torch.addbmm throws different exception differences on CPU and GPU. module: cuda module: error checking triaged,2022-11-28 00:16:24+00:00,,0,6,module: cuda module: error checking triaged,True
89724,Sample Weighted BatchNorm1d feature module: nn triaged needs research module: norms and normalization,2022-11-27 19:05:14+00:00,,1,0,feature module: nn triaged needs research module: norms and normalization,True
89718,`torch.Tensor.flatten` Trigger Segmentation Fault when trying to provide and output named dim module: crash triaged module: named tensor,2022-11-27 09:54:16+00:00,,0,2,module: crash triaged module: named tensor,True
89716,DDP hangs on forward pass of transformer oncall: distributed,2022-11-27 06:08:57+00:00,,0,3,oncall: distributed,False
89714,Segfault on torch.nn.functional.one_hot with large tensor on Python 3.9 needs reproduction module: crash triaged,2022-11-27 02:48:01+00:00,,0,5,needs reproduction module: crash triaged,True
89708,M1 mps issue triaged module: mps,2022-11-26 17:39:06+00:00,,0,0,triaged module: mps,True
93453,TensorWithTFOverrideVariable don't store fake tensor (they store real tensor) triaged oncall: pt2 module: dynamo,2022-11-25 17:09:00+00:00,,0,1,triaged oncall: pt2 module: dynamo,False
89688,Enable NCCL for PyTorch on Windows oncall: distributed module: windows triaged,2022-11-25 15:55:53+00:00,,0,3,oncall: distributed module: windows triaged,True
93452,Dynamo is over-guarding on Tensor locals triaged bug,2022-11-25 15:51:33+00:00,,1,2,triaged bug,True
89686,MultiProcess tests fail when run on nodes with 1 GPU oncall: distributed triaged,2022-11-25 14:51:19+00:00,,0,0,oncall: distributed triaged,True
93451,PTX codegen race? triaged bug oncall: pt2,2022-11-25 14:47:23+00:00,,0,1,triaged bug oncall: pt2,True
89684,`positive_semidefinite` constraint fails on CUDA 11.7 module: distributions triaged module: linear algebra,2022-11-25 13:47:59+00:00,,0,4,module: distributions triaged module: linear algebra,True
89675,[ONNX] torch.onnx.export snapshots the grads as constants in onnx when op is in cuda device module: onnx triaged,2022-11-25 05:22:27+00:00,,1,3,module: onnx triaged,False
89673,MPS bug on `torch.transpose` and `torch.log` triaged module: mps,2022-11-25 02:24:49+00:00,,0,2,triaged module: mps,True
89657,MPS device ComplexFloat triaged module: fft module: mps,2022-11-24 21:14:08+00:00,,0,0,triaged module: fft module: mps,True
93450,torchinductor tests attempt to access internet triaged bug,2022-11-24 16:36:32+00:00,,1,3,triaged bug,True
89634,[ONNX] torch.onnx.export can not export the grad of conv when the op is in CPU   module: onnx triaged,2022-11-24 14:34:12+00:00,,1,3,module: onnx triaged,False
89630,[dynamo] RuntimeError: Failed running call_function aten.nll_loss_backward(*(FakeTensor(FakeTensor(... triaged module: dynamo,2022-11-24 14:19:49+00:00,,0,0,triaged module: dynamo,True
89629,[dynamo] RuntimeError: Failed running call_function aten.convolution_backward(*(FakeTensor(FakeTensor(.. triaged module: dynamo,2022-11-24 14:15:32+00:00,,0,0,triaged module: dynamo,True
89627,[dynamo] RuntimeError: Failed running call_function aten.lift_fresh_copy(*(FakeTensor(FakeTensor(... triaged module: dynamo,2022-11-24 14:07:12+00:00,,0,0,triaged module: dynamo,True
89617,"Can not access to ""sbgemm"" routine with user-defined OpenBLAS module: build triaged module: bfloat16 module: openblas",2022-11-24 06:20:57+00:00,,0,0,module: build triaged module: bfloat16 module: openblas,True
89609,NVFuser failing masked.{amax|amin|sum} extremal and correctness tests triaged module: nvfuser,2022-11-24 01:10:56+00:00,,1,3,triaged module: nvfuser,True
89601,Building PyTorch with Vulkan backend fails (1.13 and master) triaged module: vulkan,2022-11-23 23:14:48+00:00,,0,0,triaged module: vulkan,True
89597,Caching a model's weights and state_dict to disk to save RAM module: nn triaged,2022-11-23 22:20:17+00:00,,0,3,module: nn triaged,True
89574,Finish deprecation of autograd decorator over class objects module: autograd triaged,2022-11-23 16:51:01+00:00,,1,0,module: autograd triaged,True
93447,[Inductor] [CPU] LSTM is not using oneDNN in tts_angular triaged,2022-11-23 08:50:21+00:00,,1,3,triaged,False
93446,[Inductor] [CPU] Vectorization not supporting python pass-in scalar double in speech_transformer triaged,2022-11-23 07:37:23+00:00,,1,0,triaged,False
93445,[accuracy] [aot_eager] mobilenet_v2_quantized_qat fails accuracy  triaged bug oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,2022-11-23 07:21:31+00:00,,1,11,triaged bug oncall: pt2 module: dynamic shapes module: aotdispatch module: pt2-dispatcher,True
93444,[Inductor] [CPU] Maxpooling is not vectorized in shufflenet_v2_x1_0 triaged,2022-11-23 06:36:02+00:00,,1,0,triaged,False
93443,Partitioner generates useless constant SymInt edges between forward-backwards triaged bug oncall: pt2,2022-11-23 04:24:14+00:00,,0,5,triaged bug oncall: pt2,True
93442,AOTAutograd generates useless tangent inputs for SymInt outputs triaged bug oncall: pt2,2022-11-23 04:21:08+00:00,,0,1,triaged bug oncall: pt2,True
89546,Unable to launch CUDA Graph with DDP model  oncall: distributed,2022-11-23 04:20:23+00:00,,0,4,oncall: distributed,False
89492,Feature Request: deterministic CUDA cumsum feature module: cuda triaged module: determinism,2022-11-22 10:01:05+00:00,,0,12,feature module: cuda triaged module: determinism,True
89491,build: cmake: functorch.so not installed at expected location module: build triaged,2022-11-22 08:27:36+00:00,,0,2,module: build triaged,True
89490,build: cmake: ability to disable -Werror* (-Werror considered harmful) module: build triaged,2022-11-22 08:21:37+00:00,,0,3,module: build triaged,False
89489,build: cmake: need to uniformize installation of libraries in CMAKE_INSTALL_LIBDIR (not lib) module: build module: cpp triaged needs research,2022-11-22 08:11:49+00:00,,0,4,module: build module: cpp triaged needs research,True
89484,"kind_.is_prim() INTERNAL ASSERT FAILED at ""../torch/csrc/jit/ir/ir.cpp"":1098 oncall: jit module: cpp",2022-11-22 04:31:15+00:00,,0,0,oncall: jit module: cpp,True
89483,Unexpected behavior from torchscript (mixing trace with script) oncall: jit,2022-11-22 04:13:30+00:00,,0,1,oncall: jit,True
89482,"torch.split: argument 'split_sizes' (position 1) must be tuple of ints, not list triaged module: sorting and selection",2022-11-22 03:44:04+00:00,,0,2,triaged module: sorting and selection,True
89459,Higher order derivatives of sinc explode triaged module: derivatives actionable,2022-11-21 21:42:24+00:00,,0,1,triaged module: derivatives actionable,True
93438,Partitioner that doesn't require functionalized graph triaged bug oncall: pt2,2022-11-21 20:40:53+00:00,,0,1,triaged bug oncall: pt2,True
93437,Accuracy minifier can find spurious accuracy failures involving uninitialized memory triaged bug oncall: pt2,2022-11-21 20:36:26+00:00,,0,1,triaged bug oncall: pt2,True
93436,Accuracy minifier should also work even if an exception is raised triaged bug oncall: pt2,2022-11-21 19:51:56+00:00,,0,2,triaged bug oncall: pt2,True
89438,Allow `low` and `high` to be tensors in `torch.randint` feature triaged module: random,2022-11-21 17:57:05+00:00,,0,4,feature triaged module: random,True
89431,The problem caused by the parameter dim of torch.norm module: error checking triaged module: edge cases,2022-11-21 15:38:07+00:00,,0,2,module: error checking triaged module: edge cases,True
89421,fx.wrap is ignored with make_fx proxy tensor tracer triaged module: fx,2022-11-21 12:14:51+00:00,,0,1,triaged module: fx,True
89420,Edge case: torch.baddbmm supports double x int8 x int8 inputs on CPU but not CUDA triaged module: type promotion module: linear algebra module: edge cases,2022-11-21 12:11:10+00:00,,0,3,triaged module: type promotion module: linear algebra module: edge cases,True
89419,torch.equal can still run successfully when the parameter types are different. triaged module: numpy module: type promotion,2022-11-21 11:56:02+00:00,,0,1,triaged module: numpy module: type promotion,True
89418,"torch.floor_divide: The dividend of torch.floor_divide is set to 0, but it can still run on the GPU. module: error checking triaged",2022-11-21 11:48:42+00:00,,0,1,module: error checking triaged,True
89417,OSError: libcublas.so.11: cannot open shared object file: No such file or directory module: binaries triaged,2022-11-21 11:37:16+00:00,,0,10,module: binaries triaged,True
89416,"When the torch.masked_select operator passes in the same parameters, it behaves differently on CPU and GPU. triaged module: masked operators",2022-11-21 11:33:48+00:00,,0,1,triaged module: masked operators,True
89408,torch.nn.MultiLabelMarginLoss has different performance on CPU and GPU. module: loss triaged,2022-11-21 08:45:40+00:00,,0,3,module: loss triaged,True
89407,[MPS] Using unsqueeze in inference mode returns anomalous result triaged inference mode module: mps,2022-11-21 08:08:48+00:00,,0,0,triaged inference mode module: mps,True
89406,stacks file from profiler is empty oncall: profiler,2022-11-21 07:44:44+00:00,,0,4,oncall: profiler,False
89395,DISABLED test_coalesce_reference_cycle_cpu_float64 (__main__.TestSparseCPU) module: sparse triaged module: flaky-tests skipped module: dynamo,2022-11-21 03:52:54+00:00,,0,8,module: sparse triaged module: flaky-tests skipped module: dynamo,False
89394,torch.nn.TransformerEncoderLayer missing exception description information. module: nn module: error checking triaged oncall: transformer/mha,2022-11-21 02:34:36+00:00,,0,2,module: nn module: error checking triaged oncall: transformer/mha,True
89393,Edge case: CPU bool abs is not supported triaged module: numpy module: edge cases,2022-11-21 02:11:15+00:00,,0,4,triaged module: numpy module: edge cases,True
89386,How can i patch the torch.jit in the second solution? Could not figure out entrypoint ? oncall: jit,2022-11-20 19:39:06+00:00,,0,0,oncall: jit,True
89381,torch.nn.ReplicationPad1d:The description of the exception information thrown is not accurate module: error checking triaged module: padding,2022-11-20 15:27:25+00:00,,0,2,module: error checking triaged module: padding,True
89372,prod_cpu not implemented for 'BFloat16' module: cpu triaged module: bfloat16,2022-11-20 08:50:47+00:00,,0,9,module: cpu triaged module: bfloat16,True
89370,torch.nn.functional.normalize: whether true is equal to 1 triaged module: norms and normalization,2022-11-20 08:09:07+00:00,,0,1,triaged module: norms and normalization,True
89369,RuntimeError: CUDA error: device-side assert triggered module: loss triaged,2022-11-20 07:39:56+00:00,,0,2,module: loss triaged,True
89362,"torch.nn.functional.embedding_bag throws an exception when it runs on a CPU, but it runs successfully on a GPU. module: nn module: cuda module: error checking triaged actionable module: embedding",2022-11-20 04:48:13+00:00,,0,2,module: nn module: cuda module: error checking triaged actionable module: embedding,True
89361,Documentation: torch.nn.functional.embedding docs could more clearly state the requirement that weight be a 2D tensor module: docs module: nn triaged module: embedding topic: docs,2022-11-20 04:27:57+00:00,,0,2,module: docs module: nn triaged module: embedding topic: docs,True
89357,Quantizable LSTM has different behavior than LSTM in bidirectional setting oncall: quantization module: rnn triaged,2022-11-20 00:41:51+00:00,,1,2,oncall: quantization module: rnn triaged,True
89354,Per-sample input xfail / test generation triaged module: testing,2022-11-19 21:20:00+00:00,,0,0,triaged module: testing,True
89344,AdaptiveAvgPool1d failed in the lower version module: nn triaged module: pooling,2022-11-19 11:43:05+00:00,,0,2,module: nn triaged module: pooling,True
89343,AdaptiveAvgPool1d throws different exceptions when using the gpu module: nn module: cuda module: error checking triaged actionable module: pooling,2022-11-19 11:18:47+00:00,,0,2,module: nn module: cuda module: error checking triaged actionable module: pooling,True
89342,torch.mm: Exceptions thrown on the CPU and GPU are inconsistent module: cuda module: error checking triaged,2022-11-19 08:18:22+00:00,,0,2,module: cuda module: error checking triaged,True
89336,"Conv2d error on M1 mac, RuntimeError: NNPACK SpatialConvolution_updateOutput failed module: convolution triaged module: macos module: arm",2022-11-19 05:29:57+00:00,,0,3,module: convolution triaged module: macos module: arm,True
93432,Should torchdynamo specialize on nn.Module triaged oncall: pt2,2022-11-18 23:49:23+00:00,,0,3,triaged oncall: pt2,False
89320,`masked_fill` with `FloatTensor` mask will never mask but fails silently. triaged module: correctness (silent) module: masked operators,2022-11-18 22:27:03+00:00,,0,0,triaged module: correctness (silent) module: masked operators,True
89303,code sharing for fundamental ops in quantization oncall: quantization triaged,2022-11-18 19:18:19+00:00,,1,0,oncall: quantization triaged,True
89301,Meta implementation for copy_ is wrong triaged module: meta tensors,2022-11-18 18:47:03+00:00,,0,0,triaged module: meta tensors,True
93431,[dynamic shapes] detectron2 dynamic shapes fails triaged oncall: pt2,2022-11-18 18:05:11+00:00,,0,5,triaged oncall: pt2,False
89293,fbgemm_avx512 build failure module: build triaged,2022-11-18 17:54:18+00:00,,0,0,module: build triaged,True
93430,[Inductor] [CPU] Crash failure in torchbench model mobilenet_v2_quantized_qat & resnet50_quantized_qat triaged bug oncall: pt2,2022-11-18 17:18:23+00:00,,1,13,triaged bug oncall: pt2,True
89283,torch.randn and torch.normal sometimes produce NaN on mps device triaged module: random module: mps,2022-11-18 14:07:05+00:00,,0,2,triaged module: random module: mps,True
89277,NotImplementedError: The operator 'aten::upsample_nearest1d.out' is not current implemented for the MPS device triaged module: mps,2022-11-18 08:53:10+00:00,,0,0,triaged module: mps,False
89275,"torch.addcdiv: input, tensor1, and tensor2 parameters should be of the same type module: docs triaged module: assert failure",2022-11-18 08:00:56+00:00,,0,2,module: docs triaged module: assert failure,True
89255,torch.lobpcg should support black-box linear operators like SciPy feature triaged module: linear algebra,2022-11-18 01:09:30+00:00,,0,3,feature triaged module: linear algebra,True
89254,"`torch.nn.ReplicationPad2D` Report ""invalid configuration argument"" Error under Compute Sanitizer module: nn module: error checking triaged module: padding",2022-11-18 01:00:59+00:00,,0,1,module: nn module: error checking triaged module: padding,True
89245,sm_80 support module: cuda triaged,2022-11-17 23:12:45+00:00,,0,6,module: cuda triaged,True
89241,"Can't use JIT modules traced with AMP autocast, with Triton Server (or any C++ environment) - freeze() issue ? oncall: jit",2022-11-17 21:40:40+00:00,,0,6,oncall: jit,True
89219,Dynamo + NNC: incorrect results with in-place ops on inputs triaged NNC module: dynamo,2022-11-17 17:21:50+00:00,,0,0,triaged NNC module: dynamo,True
89218,"`torch.nn.LayerNorm` Abort with ""invalid device ordinal"" Error module: nn module: error checking triaged module: norms and normalization",2022-11-17 16:57:43+00:00,,0,1,module: nn module: error checking triaged module: norms and normalization,True
89208,`torch.nn.CTCLoss` Trigger out-of-bound Read under Compute Sanitizer module: nn module: loss module: cuda triaged module: sanitizers,2022-11-17 13:50:21+00:00,,0,0,module: nn module: loss module: cuda triaged module: sanitizers,True
89204,Libtorch's CPU inference is much slower on Windows than on Linux module: performance module: windows triaged,2022-11-17 10:25:08+00:00,,0,4,module: performance module: windows triaged,True
89197,Collective operations do not work with `torch.BoolTensor`s on `gloo` and raise `Invalid scalar type` oncall: distributed,2022-11-17 07:12:52+00:00,,0,0,oncall: distributed,False
93425,[aot-autograd] [hf_BigBird] Output 0 of CompiledFunctionBackward is a view and is being modified inplace triaged ezyang's list bug module: aotdispatch module: pt2-dispatcher,2022-11-17 07:06:51+00:00,,1,8,triaged ezyang's list bug module: aotdispatch module: pt2-dispatcher,True
89185,[feature request] Add ability to preserve traced shape during torch.jit.save and torch.jit.load oncall: jit,2022-11-17 04:55:09+00:00,,0,2,oncall: jit,False
89160,Got many TestDTensorOpsCUDA.test_dtensor_op_db_X test failures module: cuda triaged,2022-11-16 21:24:22+00:00,,0,7,module: cuda triaged,True
89158,Support disallowing calls to certain instance methods in TorchDynamo triaged module: dynamo,2022-11-16 20:46:52+00:00,,0,0,triaged module: dynamo,False
89136,[FSDP] Adam Gives Different Results Where Only Difference Is Flattening oncall: distributed module: fsdp,2022-11-16 15:16:55+00:00,,0,1,oncall: distributed module: fsdp,False
89133,[FSDP] Investigate Unit Testing when Gradient Computation Differs on CPU/GPU oncall: distributed triaged module: fsdp,2022-11-16 14:06:25+00:00,,0,0,oncall: distributed triaged module: fsdp,True
89127,torch.normal(...) on MPS sometimes produces NaN's triaged module: mps,2022-11-16 10:40:07+00:00,,0,0,triaged module: mps,True
89125,binary_cross_entropy/bce_with_logits (+ other loss functions) for nested_tensor module: loss triaged module: nestedtensor,2022-11-16 09:48:15+00:00,,0,2,module: loss triaged module: nestedtensor,True
89124,Zero-copy way to make flat tensor into a nested_tensor given a shape triaged module: nestedtensor,2022-11-16 09:43:00+00:00,,0,1,triaged module: nestedtensor,True
89116,Implement generic batch normalization layer. oncall: distributed feature module: nn triaged needs research,2022-11-16 04:48:02+00:00,,1,10,oncall: distributed feature module: nn triaged needs research,False
89114,jit.script() fails to resolve/cast Optional[Tensor] fields of sub-modules or base classes of the object being scripted oncall: jit,2022-11-16 03:21:24+00:00,,0,1,oncall: jit,True
89105,Bad string in GLSL shader triaged module: vulkan topic: build,2022-11-16 00:57:17+00:00,,0,1,triaged module: vulkan topic: build,True
96337,pytreeify decorators feature triaged module: pytree module: functorch,2022-11-15 20:22:11+00:00,,0,7,feature triaged module: pytree module: functorch,True
89080,Unable to backprop through dense weighted sum of sparse_coo_tensors module: sparse triaged,2022-11-15 19:15:55+00:00,,0,0,module: sparse triaged,True
89076,Transformers model tracing not working oncall: jit,2022-11-15 17:30:08+00:00,,0,2,oncall: jit,False
89068,view_copy out= does not reshape zero element tensors triaged module: viewing and reshaping module: functionalization,2022-11-15 16:03:23+00:00,,0,2,triaged module: viewing and reshaping module: functionalization,True
97248,"A more systematic API for resolving the ""vmap-incompatible in-place operation"" error triaged actionable needs design module: functorch",2022-11-15 15:49:21+00:00,,0,1,triaged actionable needs design module: functorch,True
89065,Improve clarity of meaning of `torch.jit.trace`'s `example_inputs` oncall: jit,2022-11-15 15:14:13+00:00,,0,0,oncall: jit,False
89064,UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. oncall: jit,2022-11-15 15:10:43+00:00,,0,3,oncall: jit,False
89060,Extend test_proxy_tensor tests to support ops test non floating point types module: tests triaged module: ProxyTensor,2022-11-15 14:34:49+00:00,,0,0,module: tests triaged module: ProxyTensor,True
89054,Add a `device` keyword argument to `torch.manual_seed` triaged module: random,2022-11-15 11:46:20+00:00,,0,2,triaged module: random,True
89051,caffe2_interface_library CMake macro prevents linking to LibTorch as a transitive dependency module: cpp triaged,2022-11-15 09:00:40+00:00,,0,2,module: cpp triaged,True
89050,torch.distributed can't establish connection. oncall: distributed,2022-11-15 08:48:03+00:00,,0,1,oncall: distributed,False
89041,"cross compile pytoch using cmake , get an error : protobuf::protoc: command not found needs reproduction module: build triaged",2022-11-15 06:41:52+00:00,,0,0,needs reproduction module: build triaged,True
89034,[PT][1.13] torch .numpy() fn broke for some scenario module: cpu triaged module: numpy module: regression actionable ZeroTensor,2022-11-15 03:53:26+00:00,,0,7,module: cpu triaged module: numpy module: regression actionable ZeroTensor,True
89006,Add smoke-tests for CPP extensions compilations high priority module: cpp-extensions triaged module: regression,2022-11-14 19:18:48+00:00,,1,0,high priority module: cpp-extensions triaged module: regression,True
88992,Incorrect version in the instructions on official website module: docs triaged,2022-11-14 14:07:25+00:00,,1,0,module: docs triaged,True
88991,nvprims.native_batch_norm doesn't support fake tensor inputs triaged module: nvfuser module: primTorch,2022-11-14 13:15:45+00:00,,1,0,triaged module: nvfuser module: primTorch,True
88980,Glog macro redefinition problem when including headers from both libtorch and glog module: build triaged,2022-11-14 06:21:35+00:00,,0,0,module: build triaged,True
88968,M1 runner i-090e1df32b6f48a20 run out of disk space module: ci triaged module: m1,2022-11-13 16:27:08+00:00,,0,4,module: ci triaged module: m1,True
88950,`torch.nn.functional.embedding_bag` Trigger RuntimeError under UndefinedBehaviorSanitizer triaged module: sanitizers,2022-11-12 16:54:50+00:00,,0,1,triaged module: sanitizers,True
88949,`torch.set_rng_state` Trigger RuntimeError under UndefinedBehaviorSanitizer triaged module: sanitizers,2022-11-12 16:51:50+00:00,,0,1,triaged module: sanitizers,True
88948,torch.linalg.matrix_rank memory leak needs reproduction module: memory usage triaged module: linear algebra,2022-11-12 16:48:35+00:00,,0,3,needs reproduction module: memory usage triaged module: linear algebra,True
88947,`torch.Tensor.msort` Trigger RuntimeError under UndefinedBehaviorSanitizer triaged module: sanitizers,2022-11-12 16:48:12+00:00,,0,0,triaged module: sanitizers,True
88945,`torch.linalg.eigvals` Trigger RuntimeError under UndefinedBehaviorSanitizer triaged module: sanitizers,2022-11-12 16:45:29+00:00,,0,0,triaged module: sanitizers,True
88944,`torch.topk` Trigger RuntimError under UndefinedBehaviorSanitizer triaged module: sanitizers,2022-11-12 16:42:23+00:00,,0,0,triaged module: sanitizers,True
88943,`torch.vander` Trigger RuntimeError with UndefinedBehaviorSanitizer triaged module: linear algebra module: sanitizers module: edge cases,2022-11-12 16:39:18+00:00,,0,4,triaged module: linear algebra module: sanitizers module: edge cases,True
88942,`torch.svd_lowrank` Trigger RuntimeError under UndefinedBehaviorSanitizer module: sparse triaged module: linear algebra actionable module: sanitizers,2022-11-12 16:34:02+00:00,,0,4,module: sparse triaged module: linear algebra actionable module: sanitizers,True
88941,`torch.linalg.lstsq` Trigger RuntimeError under UndefinedBehaviorSanitizer triaged module: linear algebra actionable module: sanitizers module: edge cases,2022-11-12 16:30:33+00:00,,0,4,triaged module: linear algebra actionable module: sanitizers module: edge cases,True
88931,INTERNAL ASSERT FAILED. Missing scalar type infromation. triaged module: nvfuser,2022-11-12 09:51:46+00:00,,0,1,triaged module: nvfuser,True
93419,"torchdynamo is not properly setting up input tracking (e.g., for symbolic shape guards) for view bases triaged bug oncall: pt2",2022-11-12 05:26:58+00:00,,1,1,triaged bug oncall: pt2,True
88902,MPS test_numpy_ref_mps_nn_functional_group_norm_mps_float32 is flaky? triaged module: flaky-tests module: mps,2022-11-11 19:37:29+00:00,,0,1,triaged module: flaky-tests module: mps,True
93618,[dynamo+ddp+symbolic-shapes] Issue Tracker triaged oncall: pt2,2022-11-11 18:40:28+00:00,,0,3,triaged oncall: pt2,True
88893,RuntimeError: derivative for aten::mps_max_pool2d_backward is not implemented triaged module: mps,2022-11-11 17:48:25+00:00,,0,4,triaged module: mps,True
88883,Investigate why `test_aot_autograd_symbolic_exhaustive_masked_median_cpu_float32` is flaky triaged module: flaky-tests,2022-11-11 12:37:22+00:00,,0,0,triaged module: flaky-tests,True
88882,Can't import torch --> OSError related to libcublasLt.so.11 module: binaries module: cuda triaged module: regression,2022-11-11 12:25:35+00:00,,0,4,module: binaries module: cuda triaged module: regression,True
88880,Add alphatensor support for faster matrix multiplication? feature triaged matrix multiplication,2022-11-11 09:54:33+00:00,,0,2,feature triaged matrix multiplication,False
93617,[Inductor] Input Buffers Should Be Representable As Storage And Layout feature triaged oncall: pt2 module: inductor internal ramp-up task,2022-11-11 01:22:15+00:00,,0,0,feature triaged oncall: pt2 module: inductor internal ramp-up task,False
88842,test/test_ops.py is segfaulting on master build with DEBUG assets high priority module: ci triaged module: testing,2022-11-10 21:20:59+00:00,,1,12,high priority module: ci triaged module: testing,True
88838,[RFC] PyTorch DistributedTensor oncall: distributed module: dtensor,2022-11-10 19:27:21+00:00,,0,25,oncall: distributed module: dtensor,False
88813,Inductor may merge two output tensors into one triaged bug module: inductor internal ramp-up task,2022-11-10 15:09:35+00:00,,0,3,triaged bug module: inductor internal ramp-up task,True
88810,Return the attention weights using the Transformer Encoder class.  triaged oncall: transformer/mha,2022-11-10 14:13:50+00:00,,0,6,triaged oncall: transformer/mha,True
88805,[feature request] Get/set fastmath CPU bit (and some other FPU flags?) module: numerical-stability module: cpu triaged,2022-11-10 09:08:40+00:00,,0,5,module: numerical-stability module: cpu triaged,True
88802,ImportError: libcupti.so.11.2: cannot open shared object file: No such file or directory module: build module: cuda triaged,2022-11-10 07:10:20+00:00,,0,2,module: build module: cuda triaged,True
88801,[ONNX] Convert to onnx scatter op and LSTMCell op and for Loop module: onnx triaged onnx-needs-info,2022-11-10 06:46:48+00:00,,0,1,module: onnx triaged onnx-needs-info,False
88800,Quantization error between fake-quantized model and quantized model using the new observer oncall: quantization low priority triaged,2022-11-10 06:46:18+00:00,,1,9,oncall: quantization low priority triaged,True
88791,Potential bug in torch.optim.lr_scheduler.CosineAnnealingWarmRestarts triaged actionable module: LrScheduler,2022-11-10 01:18:31+00:00,,0,3,triaged actionable module: LrScheduler,True
88775,Batched Random Number Generators triaged module: random,2022-11-09 23:03:05+00:00,,0,0,triaged module: random,True
88765,torch.jit.trace() - AttributeError: 'NoneType' object has no attribute '__module__ oncall: jit,2022-11-09 20:32:15+00:00,,0,8,oncall: jit,True
88735,RuntimeError: method '__torch__.___torch_mangle_0.MyModule.sin' already defined. oncall: package/deploy,2022-11-09 11:05:42+00:00,,0,0,oncall: package/deploy,False
88732,scatter_ op convert onnx exception module: onnx triaged onnx-needs-info,2022-11-09 08:35:49+00:00,,0,1,module: onnx triaged onnx-needs-info,True
88721,DISABLED test_extract_gradients_from_optimizer_set_to_none (__main__.TestIdentifyGradients) module: flaky-tests skipped oncall: profiler,2022-11-09 03:58:04+00:00,,0,16,module: flaky-tests skipped oncall: profiler,False
88691,forward AD for _euclidean_dist module: autograd triaged actionable module: forward ad module: functorch,2022-11-08 19:28:01+00:00,,0,2,module: autograd triaged actionable module: forward ad module: functorch,True
88686,Consolidate binary build matrix for core and validation workflows module: ci triaged,2022-11-08 17:00:20+00:00,,1,0,module: ci triaged,True
88658,"nn.Linear allocate too many space which lead to CPUAllocator ""allocate memory failure"" if it's BF16. good for FP32. module: cpu triaged intel",2022-11-08 07:40:17+00:00,,0,5,module: cpu triaged intel,True
93613,Minifier crash triaged bug oncall: pt2,2022-11-08 04:20:42+00:00,,1,2,triaged bug oncall: pt2,True
88648,`MultiMarginLoss` doesn't check the value of `target` on CUDA module: cuda module: error checking triaged,2022-11-08 03:33:47+00:00,,0,0,module: cuda module: error checking triaged,True
88647,`ConvTranspose` fails on CPU but returns an empty tensor on CUDA module: cuda module: error checking triaged,2022-11-08 03:22:27+00:00,,0,1,module: cuda module: error checking triaged,True
88643,pack_sequence() always fail after set_default_tensor_type to CUDA module: rnn triaged,2022-11-08 01:53:29+00:00,,0,1,module: rnn triaged,True
88639,CUDA unknown error after suspend during debugging module: cuda triaged,2022-11-08 01:22:42+00:00,,0,0,module: cuda triaged,True
88631,GitHub first-time contributors box pops up unexpectedly module: ci triaged,2022-11-08 00:49:34+00:00,,0,0,module: ci triaged,True
88626,Cloud-based rendezvous backend / distributed store? oncall: distributed,2022-11-08 00:29:09+00:00,,0,1,oncall: distributed,False
88621,"[FSDP] FSDP produces different gradient norms vs DDP, and w/ grad norm clipping creates different training results oncall: distributed triaged release notes: distributed (fsdp)",2022-11-07 23:48:03+00:00,,1,5,oncall: distributed triaged release notes: distributed (fsdp),False
88617,"The libtorch tests Simplify.{SimplifySymbolicMinMax,SimplifyNestedMax,SimplifyNestedMin} fail on Apple Silicon oncall: jit module: m1",2022-11-07 22:56:56+00:00,,0,0,oncall: jit module: m1,True
88616,The libtorch test SequentialTest.ModuleForwardMethodOptionalArg fails on Apple Silicon module: build triaged module: m1,2022-11-07 22:54:12+00:00,,0,1,module: build triaged module: m1,True
88614,The libtorch test TestScalarTensor.TestScalarTensorMPS fails on Apple Silicon triaged actionable module: mps module: m1,2022-11-07 22:52:00+00:00,,0,1,triaged actionable module: mps module: m1,True
88613,The libtorch test ConstantPropagation.CustomClassesCanBePropagated fails on Apple Silicon oncall: quantization low priority triaged module: m1,2022-11-07 22:32:43+00:00,,1,4,oncall: quantization low priority triaged module: m1,True
88612,Bernoulli uses legacy contiguous memory format triaged module: memory format,2022-11-07 22:27:34+00:00,,0,2,triaged module: memory format,True
88609,quantization convert should warn the user if calibration has not happened oncall: quantization triaged,2022-11-07 21:55:50+00:00,,2,4,oncall: quantization triaged,True
88598,"Despite having aten::diag_embed.out, torch.diag_embed doesn't support out= argument triaged module: codegen",2022-11-07 19:26:41+00:00,,0,0,triaged module: codegen,True
88591,`pack_padded_sequence` not compatible with deterministic mode it calls `torch.scatter` module: rnn triaged module: determinism,2022-11-07 18:05:08+00:00,,0,0,module: rnn triaged module: determinism,True
88581,"cpp_extension CUDA library path hard-coded as ""lib64"" but may be ""lib"" module: cpp-extensions triaged actionable",2022-11-07 15:25:24+00:00,,0,1,module: cpp-extensions triaged actionable,True
88579,[Quant] Validate FixedQParams observers in eager mode oncall: quantization low priority triaged,2022-11-07 15:07:47+00:00,,1,1,oncall: quantization low priority triaged,True
88576,Dynamo handling for all methods of torch.Generator triaged module: dynamo,2022-11-07 14:30:44+00:00,,0,0,triaged module: dynamo,False
88574,Add support for `torch.Generator` in the FX IR oncall: fx,2022-11-07 13:34:59+00:00,,0,1,oncall: fx,False
88565,What causes CPU to degrade when I load the weight with torch.hub.load() module: cpu triaged module: hub intel,2022-11-07 04:37:31+00:00,,0,6,module: cpu triaged module: hub intel,True
88563,`nn.functional.embedding_bag` Trigger out-of-bound Read under Compute Sanitizer module: cuda triaged,2022-11-07 04:20:47+00:00,,0,4,module: cuda triaged,True
93610,We probably are allowing mutations to happen on fake tensor in VariableTracker triaged bug module: dynamo,2022-11-04 17:05:50+00:00,,1,7,triaged bug module: dynamo,True
88505,quantization: error message when using `convert_fx` on a model on cuda should be better oncall: quantization triaged,2022-11-04 17:01:02+00:00,,1,2,oncall: quantization triaged,True
93609,Don't store example_value on FX node meta triaged bug,2022-11-04 15:41:12+00:00,,1,2,triaged bug,True
88491,torch.set_grad_enabled results in RuntimeError with torch.jit.script oncall: jit,2022-11-04 13:31:41+00:00,,0,0,oncall: jit,True
88475,DISABLED test_module_attribute_mutation_violation_negative_2 (__main__.MutationExportTests) triaged module: flaky-tests skipped module: dynamo,2022-11-04 03:57:55+00:00,,0,12,triaged module: flaky-tests skipped module: dynamo,False
88474,Mixed precision training fails due to NaN in batch norm running_mean triaged module: amp (automated mixed precision),2022-11-04 03:19:18+00:00,,0,6,triaged module: amp (automated mixed precision),True
88472,DISABLED test_index_put_accumulate_large_tensor_cpu (__main__.TestIndexingCPU) triaged module: flaky-tests skipped module: dynamo,2022-11-04 02:38:29+00:00,,0,2,triaged module: flaky-tests skipped module: dynamo,False
88469,DISABLED test_fn_gradgrad_linalg_lu_factor_cuda_complex128 (__main__.TestBwdGradientsCUDA) module: autograd triaged module: flaky-tests module: linear algebra skipped module: unknown,2022-11-04 00:58:59+00:00,,0,4,module: autograd triaged module: flaky-tests module: linear algebra skipped module: unknown,False
88468,DISABLED test_module_attribute_mutation_violation_negative_1 (__main__.MutationExportTests) triaged module: flaky-tests skipped module: dynamo,2022-11-04 00:58:56+00:00,,0,10,triaged module: flaky-tests skipped module: dynamo,False
88467,DISABLED test_module_attribute_mutation_violation_negative_4 (__main__.MutationExportTests) triaged module: flaky-tests skipped module: dynamo,2022-11-04 00:58:53+00:00,,0,8,triaged module: flaky-tests skipped module: dynamo,False
88466,DISABLED test_module_attribute_mutation_violation_negative_3 (__main__.MutationExportTests) triaged module: flaky-tests skipped module: dynamo,2022-11-04 00:58:51+00:00,,0,12,triaged module: flaky-tests skipped module: dynamo,False
88464,MaxPool1D output shapes can be negative when ceil_mode=True module: nn triaged module: pooling,2022-11-04 00:16:45+00:00,,1,6,module: nn triaged module: pooling,True
88448,linear mm weight and bias dtypes mismatch bypasses oncall: jit,2022-11-03 20:55:51+00:00,,0,2,oncall: jit,False
88447,`unique` will reverse the input when `sort=False` on cpu (not sorting) module: cpu triaged intel,2022-11-03 20:30:11+00:00,,0,5,module: cpu triaged intel,True
93607,torch._dynamo.exc.Unsupported: call_function UserDefinedClassVariable() [] {} ([Feature request] Allow custom classes with custom __setattr__ method in torchdynamo) triage review triaged enhancement oncall: pt2,2022-11-03 20:16:50+00:00,,0,5,triage review triaged enhancement oncall: pt2,False
88443,Hang: sampling VonMises distribution gets stuck in rejection sampling for small kappa high priority module: distributions module: cpu triaged module: numpy module: deadlock,2022-11-03 19:33:53+00:00,,1,9,high priority module: distributions module: cpu triaged module: numpy module: deadlock,True
88423,view_as_real and split_with_sizes links in Tensor Views docs are broken module: docs triaged,2022-11-03 14:27:49+00:00,,0,1,module: docs triaged,False
88415,Enable AMP for MPS devices feature triaged module: amp (automated mixed precision) module: mps,2022-11-03 10:15:13+00:00,,0,5,feature triaged module: amp (automated mixed precision) module: mps,True
88413,Flaky dynamo test_indexing flaky with SIGKILL module: ci triaged module: dynamo,2022-11-03 06:43:42+00:00,,0,1,module: ci triaged module: dynamo,False
88410,benchmark cache persist module: cudnn module: cuda triaged,2022-11-03 06:14:19+00:00,,0,8,module: cudnn module: cuda triaged,True
88408,Unit test with `--subprocess` command doesn't respect the `-k` filter flag and runs all available sub tests module: ci module: tests triaged module: testing,2022-11-03 05:20:55+00:00,,0,0,module: ci module: tests triaged module: testing,True
88389,Whether to support libtorch source code compilation of C++11 ？ module: build triaged,2022-11-03 03:01:43+00:00,,0,2,module: build triaged,True
88380,cuDNN error (CUDNN_STATUS_NOT_SUPPORTED) for torch.nn.functional.grid_sample() module: cudnn module: cuda triaged,2022-11-03 00:14:05+00:00,,0,1,module: cudnn module: cuda triaged,True
88375,[PrimTorch] Functionalization pass removes Instance Norm / Batch Norm running stats transformations high priority triaged module: primTorch,2022-11-02 23:20:07+00:00,,1,8,high priority triaged module: primTorch,True
93604,TorchBench - moco - RuntimeError: Tensors must be CUDA and dense triaged bug,2022-11-02 18:28:13+00:00,,1,4,triaged bug,True
88327,MSE documentation is weak module: docs triaged,2022-11-02 16:45:28+00:00,,0,7,module: docs triaged,True
88325,Group losses in a common namespace module: nn module: loss triaged needs research,2022-11-02 16:24:05+00:00,,1,3,module: nn module: loss triaged needs research,True
88320,`torch.load()` cannot load data saved at non-zero position in a file (`failed finding central directory`) module: serialization triaged,2022-11-02 15:22:32+00:00,,0,1,module: serialization triaged,True
88309,ASAN shard 4 started to OOM after unrelated commit module: ci triaged,2022-11-02 14:15:03+00:00,,0,2,module: ci triaged,True
88304,AttributeError: module 'tensorboard.compat.tensorflow_stub.io.gfile' has no attribute 'MakeDirs' triaged module: tensorboard,2022-11-02 12:46:43+00:00,,0,0,triaged module: tensorboard,True
88301,1.12.1 incompatible with c++ built for 1.12.0 and vice versa oncall: jit module: cpp,2022-11-02 10:02:48+00:00,,0,9,oncall: jit module: cpp,False
93602,Diffuser pipeline device attribute broken when using optimized model triaged bug,2022-11-02 00:28:57+00:00,,1,2,triaged bug,True
88265,build: failure when upgrade oneTBB to 2021.7.0 module: build triaged module: tbb,2022-11-01 23:16:33+00:00,,0,0,module: build triaged module: tbb,True
88308,"Hessian is (incorrectly) zero when using MPS on M1 Mac, but not on cpu  triaged module: mps module: functorch",2022-11-01 23:08:30+00:00,,0,1,triaged module: mps module: functorch,True
88264,[ONNX] Flaky CI test failures with different random seed module: onnx triaged,2022-11-01 23:07:28+00:00,,0,0,module: onnx triaged,False
88251,Weird random SIGTERM occurance oncall: distributed oncall: r2p,2022-11-01 21:52:19+00:00,,0,1,oncall: distributed oncall: r2p,False
88245,Add `gloo` support for `all_to_all` oncall: distributed module: c10d,2022-11-01 20:54:23+00:00,,0,2,oncall: distributed module: c10d,False
88227,Enable `torch.topk` to support `stable` flag  triaged module: sorting and selection,2022-11-01 18:47:29+00:00,,1,6,triaged module: sorting and selection,True
88194,Add a config option to raise errors instead of warnings in nvFuser integration triaged module: nvfuser,2022-11-01 14:24:50+00:00,,1,1,triaged module: nvfuser,True
88192,[docs] torch.is_neg/torch.Tensor.is_neg not documented module: docs triaged,2022-11-01 14:17:06+00:00,,0,0,module: docs triaged,False
88191,`torch.nn.RReLU` not reporting `lower > upper` on CUDA module: cuda triaged,2022-11-01 13:34:29+00:00,,0,0,module: cuda triaged,True
88189,Moving tensor to GPU by .cuda() gets stucked when AMD Secure Encripted Virtualization (SEV) is activated module: cuda module: rocm triaged,2022-11-01 12:33:31+00:00,,0,0,module: cuda module: rocm triaged,True
88185,`torch.mm` Trigger RuntimeError with UndefinedBehaviorSanitizer triaged module: sanitizers,2022-11-01 09:27:57+00:00,,0,0,triaged module: sanitizers,True
88148,☂️  Issues that trigger crashes due to corner-case API usages triaged module: edge cases,2022-10-31 22:39:20+00:00,,0,2,triaged module: edge cases,True
88147,Conv2d is not deterministic when input tensor has different strides module: convolution triaged,2022-10-31 22:22:44+00:00,,0,3,module: convolution triaged,True
88144,AvgPool2D output shapes are inconsistent when ceil_mode=True triaged module: correctness (silent) module: pooling,2022-10-31 22:13:24+00:00,,1,6,triaged module: correctness (silent) module: pooling,True
88142,Refactor `torch.return_types.topk` to behave like a `namedtuple` or a `dict` triaged enhancement module: sorting and selection,2022-10-31 22:06:52+00:00,,0,1,triaged enhancement module: sorting and selection,True
88137,"Add eq, to, masked_select, index_select, narrow to nested tensors triaged module: nestedtensor",2022-10-31 20:58:48+00:00,,0,2,triaged module: nestedtensor,True
88136,Placing LSTM model on bfloat16 on GPU causes error module: rnn module: cuda triaged module: bfloat16,2022-10-31 20:56:56+00:00,,0,3,module: rnn module: cuda triaged module: bfloat16,True
88109,Python Dispatcher registrations beyond BackendSelect do nothing triaged module: dispatch module: python dispatcher,2022-10-31 18:25:16+00:00,,0,0,triaged module: dispatch module: python dispatcher,True
88103,ProcessGroupNCCL watchdog can't catch NCCL comm initialization issues oncall: distributed triaged module: c10d,2022-10-31 17:53:00+00:00,,0,0,oncall: distributed triaged module: c10d,True
88096,Add nondeterministic alert to `torch.Tensor.scatter()` triaged module: determinism,2022-10-31 16:42:55+00:00,,1,2,triaged module: determinism,True
88081,out of memory with pytorch version after 1.8.1 module: cuda module: memory usage triaged module: CUDACachingAllocator,2022-10-31 12:27:18+00:00,,0,0,module: cuda module: memory usage triaged module: CUDACachingAllocator,True
88072,convert torch.jit.script model to ONNX get wrong result module: onnx triaged onnx-needs-info,2022-10-31 07:41:35+00:00,,0,5,module: onnx triaged onnx-needs-info,False
88062,Cannot import `traverse_dps` from torch.data.utils.graph triaged module: data,2022-10-31 00:18:06+00:00,,0,3,triaged module: data,True
88053,Different behaviour in sparse matmul module: sparse triaged,2022-10-30 17:55:33+00:00,,0,0,module: sparse triaged,True
88047,`torch.nn.CTCLoss` Trigger heap-buffer-overflow under AddressSanitizer module: nn module: loss triaged actionable module: sanitizers module: edge cases,2022-10-30 09:00:12+00:00,,0,2,module: nn module: loss triaged actionable module: sanitizers module: edge cases,True
93596,Minifier doesn't work on DebertaForQuestionAnswering triaged bug,2022-10-30 02:31:47+00:00,,1,2,triaged bug,True
93593,Inductor gives obscure error when FX graph to be compiled returns tuple triaged bug,2022-10-30 01:15:39+00:00,,1,9,triaged bug,True
93592,Turning on minifier causes bug to go away (on DebertaForMaskedLM) triaged bug,2022-10-30 00:37:29+00:00,,1,1,triaged bug,True
88036,A segment fault can be triggered in fbgemm_pack_gemm_matrix_fp16 module: crash triaged module: third_party module: half,2022-10-29 15:01:43+00:00,,0,0,module: crash triaged module: third_party module: half,True
88027,"getting error error: namespace ""cub"" has no member ""Debug"" when try to build v1.8.2 with CUDA 11.6 module: cuda triaged",2022-10-29 02:29:25+00:00,,0,1,module: cuda triaged,True
88025,[WIP] Composable FSDP Follow-Ups oncall: distributed triaged module: fsdp,2022-10-29 00:16:08+00:00,,1,0,oncall: distributed triaged module: fsdp,True
88006,C++ Extensions can't import c10d/reducer.hpp oncall: distributed,2022-10-28 21:06:58+00:00,,0,2,oncall: distributed,False
88002,Einsum Optimization Tracker module: performance triaged module: linear algebra,2022-10-28 19:42:05+00:00,,0,0,module: performance triaged module: linear algebra,True
87995,multi-node distributed training rank0 hang at dataloader after a few epochs oncall: distributed module: dataloader,2022-10-28 18:19:54+00:00,,0,7,oncall: distributed module: dataloader,True
87992,torch.rand(...) is not consistent for large shape dimensions across GPUs (with the same random seed) module: cuda triaged module: random,2022-10-28 17:14:10+00:00,,0,5,module: cuda triaged module: random,True
87979,amp with `bf16`: backward happens in `f16` when using `@torch.cuda.amp.custom_bwd` triaged module: bfloat16 module: half module: amp (automated mixed precision),2022-10-28 12:29:14+00:00,,0,1,triaged module: bfloat16 module: half module: amp (automated mixed precision),True
87964,`torch.distributed` crash with abort only inside if oncall: distributed module: crash,2022-10-28 05:45:27+00:00,,0,1,oncall: distributed module: crash,True
87961,crash in `torch.package.PackageExporter` triaged module: edge cases,2022-10-28 05:33:59+00:00,,0,0,triaged module: edge cases,True
87960,crash when call `torch.set_num_interop_threads` twice module: crash triaged,2022-10-28 05:27:50+00:00,,0,0,module: crash triaged,True
87957,VS2022Preview ParallelCommon.cpp.obj : fatal error LNK1161: invalid export specification module: build triaged module: static linking,2022-10-28 04:43:10+00:00,,0,2,module: build triaged module: static linking,True
87956,Autograd doesn't stop executing backward graph early enough in situations involving set_ module: autograd triaged has workaround actionable,2022-10-28 02:59:58+00:00,,0,2,module: autograd triaged has workaround actionable,True
87955,AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next' module: dataloader triaged,2022-10-28 02:39:06+00:00,,0,2,module: dataloader triaged,True
88004,link error happen when intergrate libtorch to other tool module: build module: cpp triaged,2022-10-28 02:23:43+00:00,,0,7,module: build module: cpp triaged,True
93590,test_conv_large_cuda: RuntimeError: CUDA error: an illegal memory access was encountered triaged,2022-10-28 00:05:20+00:00,,0,1,triaged,True
93589,test_batchnorm_eval_cuda_float32: AttributeError: 'NoneType' object has no attribute 'clone' triaged,2022-10-27 23:58:26+00:00,,1,2,triaged,True
93588,"test_LSTM_grad_and_gradgrad_cuda_float64: ValueError: gradcheck expects at least one input tensor to require gradient, but none of the them have requires_grad=True. triaged",2022-10-27 23:55:20+00:00,,1,1,triaged,True
93585,"test_memory_format_ao_nn_quantized_MaxPool2d_cuda_float32: assert not memory_format, ""TODO"" triaged",2022-10-27 22:57:10+00:00,,0,0,triaged,True
87902,Permute module: docs triaged,2022-10-27 19:11:48+00:00,,0,0,module: docs triaged,True
87890,"""No CUDA GPUs are available"" coming from GHA g5 runners oncall: releng module: ci triaged",2022-10-27 18:56:45+00:00,,1,2,oncall: releng module: ci triaged,True
87886,Add aten::empty.memory_format for SparseMPS module: sparse triaged enhancement module: mps,2022-10-27 18:26:47+00:00,,0,22,module: sparse triaged enhancement module: mps,True
87864,Failure to export scripted models to ONNX when input is a list of tensor needs reproduction module: onnx triaged onnx-needs-info,2022-10-27 15:49:06+00:00,,0,3,needs reproduction module: onnx triaged onnx-needs-info,True
87861,RuntimeError: unable to mmap 29764 bytes from file </torch_10182_3020184674_63991>: Cannot allocate memory (12) triaged module: tensor creation,2022-10-27 14:37:28+00:00,,0,1,triaged module: tensor creation,True
87859,"M1 Mac, MPS: Buffer is not large enough triaged module: mps",2022-10-27 14:02:19+00:00,,0,0,triaged module: mps,True
93582,[Inductor] Support deterministic parallel reduction in CPP backend triaged,2022-10-27 13:26:49+00:00,,1,1,triaged,False
87841,`max_unpool3d` will trigger an assertion fail under compute sanitizer module: cuda triaged module: sanitizers module: pooling,2022-10-27 04:11:29+00:00,,0,1,module: cuda triaged module: sanitizers module: pooling,True
87800,[ONNX] Graph passes analysis module: onnx triaged onnx-triaged,2022-10-26 17:15:39+00:00,,2,1,module: onnx triaged onnx-triaged,False
87794,CUDA error: operation not permitted when stream is capturing (2 GPUs) module: multi-gpu triaged,2022-10-26 16:16:30+00:00,,0,5,module: multi-gpu triaged,True
87792,`AvgPool` and `MaxPool` will crash in JIT w/o profiling executor oncall: jit,2022-10-26 16:14:52+00:00,,0,0,oncall: jit,True
87787,`BatchNorm` a 0-shape tensor will crash in JIT trace w/o profiling executor on cuda oncall: jit,2022-10-26 15:45:52+00:00,,0,0,oncall: jit,True
87785,"ONNX-exported model cannot output Dict[str, X] or str module: onnx triaged",2022-10-26 14:58:41+00:00,,0,2,module: onnx triaged,False
93580,AssertionError: Unknown expression s2 triaged bug,2022-10-26 13:50:52+00:00,,1,4,triaged bug,True
87782,Libtorch windows binaries publishing module: ci triaged topic: binaries,2022-10-26 13:17:00+00:00,,0,0,module: ci triaged topic: binaries,True
87781,`torchtyping` annotations make saving to Torchscript fail oncall: jit,2022-10-26 12:43:31+00:00,,0,0,oncall: jit,False
87777,Improvements to fuse optimization oncall: quantization low priority triaged module: fx,2022-10-26 07:45:40+00:00,,0,2,oncall: quantization low priority triaged module: fx,True
87755,Autograd precision for CONV + BN  between pytorch version 1.11.0 and 1.12.1 module: numerical-stability module: autograd triaged module: numerical-reproducibility,2022-10-26 01:44:52+00:00,,0,4,module: numerical-stability module: autograd triaged module: numerical-reproducibility,True
87753,`torch.min`/`torch.max` returns bogus values for default int tensors on MPS triaged module: correctness (silent) module: mps,2022-10-26 01:24:50+00:00,,0,3,triaged module: correctness (silent) module: mps,True
87745,TorchDynamo: there has a accuracy issue for conv+unary(binary) post ops for gpu path triaged module: dynamo,2022-10-26 00:48:00+00:00,,0,0,triaged module: dynamo,True
87734,Checkpointing Support for Modularized Optimizers module: optimizer triaged needs design,2022-10-25 22:01:01+00:00,,0,3,module: optimizer triaged needs design,False
87733,"FakeTensorMode doesn't support two Scalar inputs, if we use prims' impl as the meta function  triaged module: primTorch module: fakeTensor",2022-10-25 21:57:05+00:00,,0,8,triaged module: primTorch module: fakeTensor,True
87706,C++ Adagrad optimizer doesn't initialize parameter state module: cpp module: optimizer triaged actionable,2022-10-25 15:04:06+00:00,,0,1,module: cpp module: optimizer triaged actionable,True
87701,pytorch/pytorch cpu official Docker images triaged module: docker,2022-10-25 14:59:36+00:00,,0,0,triaged module: docker,False
87697,Get https://github.com/pytorch/benchmark working module: windows triaged,2022-10-25 14:50:10+00:00,,0,1,module: windows triaged,True
87696,Enable PostLocalSGDOptimizer on CUDA tensors oncall: distributed module: cuda triaged,2022-10-25 14:47:57+00:00,,0,2,oncall: distributed module: cuda triaged,True
87694,Investigate possibilities of automation for build pipeline module: windows triaged,2022-10-25 14:42:44+00:00,,1,2,module: windows triaged,True
87692,"Performance issue on Windows with a ""benchmark"" comparing to Linux and WLS module: windows triaged",2022-10-25 14:34:38+00:00,,0,3,module: windows triaged,True
87673,INTERNAL ASSERT FAILED !(has_different_input_dtypes && !config.promote_inputs_to_common_dtype_ && (has_undefined_outputs || config.enforce_safe_casting_to_output_ || config.cast_common_dtype_to_outputs_)) needs reproduction triaged module: TensorIterator,2022-10-25 04:55:13+00:00,,0,3,needs reproduction triaged module: TensorIterator,True
87642,`libtorch_cpu.so` is exposing some LLVM symbols module: build module: cpp-extensions triaged,2022-10-24 21:43:57+00:00,,0,0,module: build module: cpp-extensions triaged,True
87634,Add tests for ProcessGroup cpp extensions oncall: distributed triaged,2022-10-24 20:39:19+00:00,,1,0,oncall: distributed triaged,False
93579,torchdynamo.export doesn't work with data-dependent control flow triaged enhancement,2022-10-24 18:44:25+00:00,,1,2,triaged enhancement,True
87597,ninja: build stopped: subcommand failed needs reproduction module: build triaged,2022-10-24 14:14:28+00:00,,0,1,needs reproduction module: build triaged,True
87589,"#error ""Expected GLOO_USE_CUDA to be defined"" module: build triaged module: third_party",2022-10-24 06:37:08+00:00,,0,2,module: build triaged module: third_party,True
87579,Crash on backwards step when using `batch_first=True` for LSTMs on MPS (1.14 nightly build) module: rnn triaged module: mps,2022-10-23 18:20:40+00:00,,0,0,module: rnn triaged module: mps,True
87576,Dynamic shapes exhaustive tests should fail (not xfail) if data mismatch triaged module: dynamic shapes,2022-10-23 13:58:06+00:00,,0,0,triaged module: dynamic shapes,True
87575,Functionalization does something wrong with pad backward when it uses as_strided triaged module: functionalization,2022-10-23 13:56:42+00:00,,0,0,triaged module: functionalization,True
87574,"DCE produced obviously wrong graph for pad, but test did not catch it triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher",2022-10-23 13:53:49+00:00,,0,0,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
87571,Testing insufficient to catch incorrect dispatch key for bernoulli.p re functionalization triaged module: functionalization,2022-10-23 04:21:14+00:00,,0,3,triaged module: functionalization,True
87559,diagonal of Jacobian matrix module: autograd triaged enhancement,2022-10-22 21:16:29+00:00,,0,10,module: autograd triaged enhancement,True
87556,The behavior of cast `NaN` is different on cpu and cuda triaged module: NaNs and Infs module: edge cases,2022-10-22 19:59:39+00:00,,0,0,triaged module: NaNs and Infs module: edge cases,True
87555,Improve `c10d::ReduceOp` & `torch.distributed.distributed_c10d.ReduceOp` oncall: distributed triaged,2022-10-22 19:11:45+00:00,,0,2,oncall: distributed triaged,True
87551,`bmm` will return wrong result on cpu with in-place triaged module: edge cases,2022-10-22 15:31:54+00:00,,0,2,triaged module: edge cases,True
87550,[onnx] export repeat_interleave TypeError: z_(): incompatible function arguments needs reproduction module: onnx triaged onnx-triaged,2022-10-22 15:11:41+00:00,,0,1,needs reproduction module: onnx triaged onnx-triaged,False
87542,DISABLED test_numpy_ref_mps_nn_functional_conv_transpose1d_mps_float32 (__main__.TestCommonMPS) triaged skipped,2022-10-22 04:20:11+00:00,,0,1,triaged skipped,False
87539,RAM leak when copying tensor from cpu to cuda module: cuda module: memory usage triaged,2022-10-22 02:17:55+00:00,,0,4,module: cuda module: memory usage triaged,True
87514,invalid_arguments.cpp is busted module: bootcamp triaged actionable module: python frontend,2022-10-21 22:05:26+00:00,,0,0,module: bootcamp triaged actionable module: python frontend,True
87504,Loading model trained on MPS cannot be opened on non MPS system triaged module: mps,2022-10-21 21:09:01+00:00,,0,1,triaged module: mps,True
87501,Synchronize domain builds to be executed after core build have completed module: ci triaged,2022-10-21 20:04:39+00:00,,0,0,module: ci triaged,True
87499,"built from source windows static library with multiple ""unresolved external symbol"" module: build module: windows triaged",2022-10-21 19:34:49+00:00,,1,0,module: build module: windows triaged,True
87497,Gloo errors when process's batch only indexes padding_idx of sparse embedding oncall: distributed,2022-10-21 19:11:30+00:00,,0,0,oncall: distributed,False
87494,Missing docstring for resize_as module: docs triaged,2022-10-21 19:03:18+00:00,,0,0,module: docs triaged,False
87491,TorchDynamo fails to trace the graph when custom op is being used triaged oncall: pt2 module: aotdispatch module: dynamo module: pt2-dispatcher,2022-10-21 18:28:47+00:00,,1,1,triaged oncall: pt2 module: aotdispatch module: dynamo module: pt2-dispatcher,False
87487,[JIT] Inconsistent handling of tracing dict output leads to assertion  oncall: jit,2022-10-21 18:01:00+00:00,,0,0,oncall: jit,True
87468,Categorical fails simplex validation after its own normalisation on CUDA module: distributions triaged,2022-10-21 16:07:23+00:00,,0,4,module: distributions triaged,True
87458,Placeholder tensor is empty triaged module: mps,2022-10-21 14:18:03+00:00,,0,1,triaged module: mps,True
87451,Some operations do not keep `channels_last` memory format which yields accuracy drop triaged module: memory format,2022-10-21 13:09:03+00:00,,0,1,triaged module: memory format,True
87449,pytorch could not build from source with cudnn 8.0.5 module: build module: cudnn module: cuda triaged,2022-10-21 11:16:33+00:00,,0,4,module: build module: cudnn module: cuda triaged,True
87448,Semantics of sparse operations clarification - Sparsity of the gradient with respect to a sparse tensor input module: sparse module: docs module: autograd triaged enhancement module: masked operators,2022-10-21 08:32:40+00:00,,0,0,module: sparse module: docs module: autograd triaged enhancement module: masked operators,True
87446,ipykernel crash importing torch after scipy in .ipynb file needs reproduction module: crash triaged,2022-10-21 06:42:56+00:00,,0,3,needs reproduction module: crash triaged,True
87433,index_select() applied in sparse tensor can't backprop module: sparse triaged,2022-10-21 01:14:51+00:00,,0,1,module: sparse triaged,True
87402,`lower_cholesky` constraint incorrectly fails on MPS triaged module: mps,2022-10-20 20:48:34+00:00,,0,0,triaged module: mps,True
87390,`chunk` a 0-dim tensor will crash in JIT script w/o profiling executor oncall: jit,2022-10-20 19:08:02+00:00,,0,0,oncall: jit,True
87389,Installing PyTorch with BUILD_SPLIT_CUDA=ON and CUDNN fails on linker error module: build module: cudnn module: cuda triaged actionable,2022-10-20 19:07:29+00:00,,0,4,module: build module: cudnn module: cuda triaged actionable,True
87388,Document dist.new_subgroups oncall: distributed module: docs triaged,2022-10-20 19:05:42+00:00,,0,0,oncall: distributed module: docs triaged,True
87371,Better type annotations for `torch.Tensor` subclasses module: typing triaged actionable tensor subclass,2022-10-20 16:05:50+00:00,,0,3,module: typing triaged actionable tensor subclass,True
87366,"Implementation of CG, and BICGSTAB methods module: optimizer triaged needs research",2022-10-20 14:54:38+00:00,,1,1,module: optimizer triaged needs research,True
87364,test_ao_sparsity fails when build without FBGEMM triaged,2022-10-20 14:38:47+00:00,,0,1,triaged,True
87358,Triangular solver for sparse matrices module: sparse triaged module: linear algebra,2022-10-20 12:14:08+00:00,,0,20,module: sparse triaged module: linear algebra,True
87353,Speed of torch.istft module: performance triaged module: fft,2022-10-20 08:50:16+00:00,,0,9,module: performance triaged module: fft,True
87352,RuntimeError: Tensors of type TensorImpl do not have numel module: build module: cuda triaged module: docker,2022-10-20 08:37:05+00:00,,0,0,module: build module: cuda triaged module: docker,True
87351,buffer is not large enough when running pytorch on M1 mps triaged module: mps,2022-10-20 05:47:41+00:00,,0,9,triaged module: mps,True
87350,OpenCL 3.0 support: support every GPU on earth through rusticl feature triaged,2022-10-20 05:46:12+00:00,,0,9,feature triaged,False
93556,Tracker for manually running pytorch/examples  triaged oncall: pt2,2022-10-20 03:12:14+00:00,,1,0,triaged oncall: pt2,False
93553,Better error message when attempting to `torch.save` an optimized model good first issue triaged oncall: pt2 module: dynamo,2022-10-20 02:30:44+00:00,,0,1,good first issue triaged oncall: pt2 module: dynamo,True
87289,'str' object has no attribute '__module__' in jit is_final oncall: jit,2022-10-19 15:06:59+00:00,,0,0,oncall: jit,True
87283,Missing string parsing for some parameter types in python arg parsing logic high priority triaged module: pybind,2022-10-19 12:22:47+00:00,,0,1,high priority triaged module: pybind,True
87276,torch.save throws ValueError: ctypes objects containing pointers cannot be pickled module: serialization triaged,2022-10-19 09:43:09+00:00,,0,0,module: serialization triaged,True
87268,register_package has no further documentation module: docs oncall: package/deploy,2022-10-19 03:24:54+00:00,,0,0,module: docs oncall: package/deploy,False
87267,The installation commands given on the pytorch website will not install properly module: binaries triaged,2022-10-19 03:12:10+00:00,,0,4,module: binaries triaged,True
87236,nvprims.div doesn't work with FakeTensor cpu scalars triaged module: nvfuser module: fakeTensor,2022-10-18 20:01:58+00:00,,1,2,triaged module: nvfuser module: fakeTensor,True
87222,`custom_jvp` and `custom_vjp` triaged module: functorch,2022-10-18 17:48:46+00:00,,0,3,triaged module: functorch,True
87209,"Reproducible ""CUDA error: an illegal memory access was encountered"" needs reproduction module: crash module: cuda triaged",2022-10-18 15:50:00+00:00,,0,7,needs reproduction module: crash module: cuda triaged,True
87196,Missing `docker` directory in `tools/`  module: docs triaged module: docker,2022-10-18 14:07:54+00:00,,0,1,module: docs triaged module: docker,True
87178,The autogenerated out variants via `autogen:` do not check that the dtype of the `out` kwarg via `canCast`. triaged module: codegen,2022-10-18 08:08:09+00:00,,0,0,triaged module: codegen,True
87172,Unstable results in sin/arcsin/arccos calls module: numerical-stability triaged module: edge cases,2022-10-18 05:14:21+00:00,,0,1,module: numerical-stability triaged module: edge cases,True
87170,torch.linalg.cond gives inconsistent results on CPU/CUDA triaged module: NaNs and Infs module: linear algebra,2022-10-18 04:51:53+00:00,,0,2,triaged module: NaNs and Infs module: linear algebra,True
87163,New APIs for cuda graph inspection and manipulation triaged module: cuda graphs,2022-10-18 04:14:17+00:00,,0,3,triaged module: cuda graphs,True
93543,DDPOptimizer+inductor OOMs with hf_GPT2_large and timm_vision_transformer_large triaged oncall: pt2,2022-10-18 03:26:24+00:00,,0,2,triaged oncall: pt2,False
87159,"torch/csrc/utils/python_arg_parser.h:424:94: error: format ‘%ld’ expects argument of type ‘long int’, but argument 7 has type ‘int’ module: build triaged module: arm",2022-10-18 03:00:14+00:00,,0,5,module: build triaged module: arm,True
87157,DISABLED test_expanded_reduction_cpu (__main__.CpuTests) triaged skipped module: inductor module: dynamo module: cpu inductor,2022-10-18 02:38:07+00:00,,1,2,triaged skipped module: inductor module: dynamo module: cpu inductor,False
87145,Unrecognized data format when using release libtorch libraries in debug build oncall: jit,2022-10-17 22:15:03+00:00,,0,1,oncall: jit,False
87131,torch.clamp does not clamp out of -0 from 0 when ran on the CPU triaged module: numpy module: edge cases,2022-10-17 21:04:20+00:00,,0,0,triaged module: numpy module: edge cases,True
87129,[MPS] sum on a size=1 dim is ~5x slower than squeeze triaged module: mps,2022-10-17 20:47:37+00:00,,0,5,triaged module: mps,True
87126,Bug in Histogram Observer Implementation oncall: quantization triaged,2022-10-17 20:29:29+00:00,,1,3,oncall: quantization triaged,True
87090,MPS memory usage significantly higher than on CPU module: memory usage triaged module: mps,2022-10-17 16:49:15+00:00,,0,2,module: memory usage triaged module: mps,True
87087,Failing periodic tests: test_dense_mask_index_cpu (__main__.CpuTests) & est_expanded_reduction_cpu (__main__.CpuTests) triaged module: flaky-tests,2022-10-17 16:43:24+00:00,,0,1,triaged module: flaky-tests,False
87085,gradcheck failure with sparse matrix multiplication module: sparse module: autograd triaged,2022-10-17 16:24:27+00:00,,1,1,module: sparse module: autograd triaged,True
87080,cppextension host compiler check ignores executable symbolic link in CUDA bin directory module: build module: cpp-extensions triaged,2022-10-17 15:12:55+00:00,,0,0,module: build module: cpp-extensions triaged,True
87076,Nandense layer for missing values module: sparse module: nn triaged module: masked operators,2022-10-17 12:50:26+00:00,,0,1,module: sparse module: nn triaged module: masked operators,True
87070,DISABLED test_variant_consistency_jit_linalg_lu_cuda_complex64 (__main__.TestJitCUDA) module: cuda triaged module: flaky-tests skipped module: unknown,2022-10-17 09:49:20+00:00,,0,9,module: cuda triaged module: flaky-tests skipped module: unknown,False
87056,Pipe conveys inconsistent value in GPU env module: multiprocessing triaged,2022-10-17 02:23:17+00:00,,0,0,module: multiprocessing triaged,True
87041,"Segmentation fault: 11 when running ""import torch"" on Mac OS X triaged module: macos",2022-10-16 19:57:31+00:00,,0,4,triaged module: macos,True
87033,Saving and loading from physical storage module: memory usage module: serialization triaged,2022-10-16 05:38:58+00:00,,0,2,module: memory usage module: serialization triaged,True
87031,Improve Readability of error(s) when provided unexpected keyword arguments. module: error checking triaged actionable module: python frontend,2022-10-16 05:06:26+00:00,,0,2,module: error checking triaged actionable module: python frontend,True
87019,Rewrite `narrow_copy_dense_cpu_out` using `copy_` and `narrow` triaged module: functionalization,2022-10-15 21:40:57+00:00,,0,2,triaged module: functionalization,True
87003,Multiprocessing DataLoader pickles multiprocessing.Queues incorrectly module: dataloader triaged,2022-10-14 21:49:17+00:00,,0,4,module: dataloader triaged,True
86989,Error: unknown architecture `armv7-a;' and Error: selected processor does not support `command' in ARM mode oncall: quantization low priority triaged module: arm topic: build,2022-10-14 20:15:51+00:00,,1,5,oncall: quantization low priority triaged module: arm topic: build,True
86968,Drop deprecated behavior from NumPy-style `T` triaged module: numpy module: deprecation,2022-10-14 15:50:47+00:00,,0,2,triaged module: numpy module: deprecation,True
86965,Upgrade to a newer llvm-openmp version to avoid `/dev/shm` pollution module: binaries triaged module: openmp,2022-10-14 12:57:59+00:00,,0,0,module: binaries triaged module: openmp,True
86962,PyTorch RPC crashed when using IB  oncall: distributed module: rpc,2022-10-14 08:17:38+00:00,,0,3,oncall: distributed module: rpc,False
86929,DISABLED test_vmapvjpvjp_linalg_lu_cuda_float32 (__main__.TestOperatorsCUDA) triaged module: flaky-tests skipped module: functorch,2022-10-13 18:50:10+00:00,,0,9,triaged module: flaky-tests skipped module: functorch,False
86919,Importing torch 1.12.0 breaks subprocess module needs reproduction module: binaries triaged module: macos,2022-10-13 17:47:26+00:00,,0,2,needs reproduction module: binaries triaged module: macos,True
86918,torch.cat on empty tensor is bogus triaged module: edge cases,2022-10-13 17:37:59+00:00,,0,3,triaged module: edge cases,True
86910,[FSDP] Investigate `torch.cuda.current_stream()` usage in post-backward oncall: distributed triaged module: fsdp,2022-10-13 15:43:21+00:00,,1,0,oncall: distributed triaged module: fsdp,True
86890,View-based advanced indexing (Integer array/LongTensor indexing) of nested_tensor feature triaged module: advanced indexing,2022-10-13 09:22:39+00:00,,0,1,feature triaged module: advanced indexing,True
86888,Broadcasting add for nested_tensor triaged module: nestedtensor,2022-10-13 08:16:41+00:00,,0,2,triaged module: nestedtensor,True
86887,DISABLED test_variant_consistency_jit_linalg_lu_factor_ex_cuda_complex64 (__main__.TestJitCUDA) triaged module: flaky-tests skipped module: unknown,2022-10-13 06:57:52+00:00,,0,8,triaged module: flaky-tests skipped module: unknown,False
86877,compile torch from source needs reproduction module: build triaged,2022-10-13 03:45:27+00:00,,0,3,needs reproduction module: build triaged,True
93531,TorchInductor CPU Performance Dashboard triaged oncall: pt2 module: cpu inductor,2022-10-13 02:29:48+00:00,,1,317,triaged oncall: pt2 module: cpu inductor,False
86849,`torch.distributed.all_reduce` allocates excess GPU memory when using NCCL backend oncall: distributed module: nccl has workaround,2022-10-12 23:28:55+00:00,,0,5,oncall: distributed module: nccl has workaround,False
86848,.view(dtype) on a quantized tensor throws SegmentationFault oncall: quantization triaged,2022-10-12 23:25:37+00:00,,1,2,oncall: quantization triaged,True
86830,Distributed collective ops fail in `inference_mode` for CPU-only oncall: distributed triaged module: c10d,2022-10-12 21:09:53+00:00,,0,3,oncall: distributed triaged module: c10d,True
86819,Could not run select_backward [vmap] [dlrm] [functorch] triaged module: functorch,2022-10-12 19:21:40+00:00,,0,3,triaged module: functorch,True
86818,Forward hooks for ScriptModules oncall: jit,2022-10-12 19:03:12+00:00,,0,0,oncall: jit,False
86816,JIT model returns different value on cpu with uniform-initialized input oncall: jit,2022-10-12 18:41:08+00:00,,0,0,oncall: jit,True
86814,Expanding the parameters of `torch.svd_lowrank` triaged module: linear algebra,2022-10-12 18:08:01+00:00,,0,2,triaged module: linear algebra,True
86804,JIT model will have a different jacobian after the first computation oncall: jit,2022-10-12 17:26:21+00:00,,0,0,oncall: jit,False
86798,TF32 conv_transpose2d with groups has bad precision compared to fp32 module: numerical-stability module: cuda triaged module: tf32,2022-10-12 15:29:52+00:00,,0,5,module: numerical-stability module: cuda triaged module: tf32,True
86791,We don't have an op for vulkan_prepack::conv2d_clamp_prepack but it isn't a special case. module: convolution oncall: mobile module: vulkan,2022-10-12 13:24:15+00:00,,0,4,module: convolution oncall: mobile module: vulkan,False
86782,Poisson sampling on GPU fails for high rates module: cuda triaged module: random,2022-10-12 06:05:15+00:00,,0,0,module: cuda triaged module: random,True
86770,DISABLED test_vmapjvpall_linalg_lu_cuda_float32 (__main__.TestOperatorsCUDA) triaged module: flaky-tests skipped module: functorch,2022-10-12 04:13:58+00:00,,0,7,triaged module: flaky-tests skipped module: functorch,False
86733,DISABLED test_vmapjvpvjp_linalg_lu_cuda_float32 (__main__.TestOperatorsCUDA) triaged module: flaky-tests skipped module: functorch,2022-10-11 21:44:08+00:00,,0,11,triaged module: flaky-tests skipped module: functorch,False
86732,DISABLED test_variant_consistency_jit_linalg_lu_factor_cuda_complex64 (__main__.TestJitCUDA) triaged module: flaky-tests skipped module: unknown,2022-10-11 21:43:55+00:00,,0,10,triaged module: flaky-tests skipped module: unknown,False
86718,Autograd doc does not mention torch.autograd.set_grad_enabled module: docs module: autograd triaged actionable,2022-10-11 19:42:42+00:00,,0,8,module: docs module: autograd triaged actionable,False
86717,NVFuser `FusionRootMappingMultipleBroadcast_CUDA` raises exception on sm_80+  module: cuda module: ci triaged module: nvfuser,2022-10-11 19:10:36+00:00,,0,2,module: cuda module: ci triaged module: nvfuser,True
86714,NVFuser `FusionComputeAtMultiBCast_CUDA` and `FusionDetectSelfMappedDomains_CUDA` does not raise exception on sm_80+ module: cuda module: ci triaged module: nvfuser,2022-10-11 19:04:12+00:00,,0,3,module: cuda module: ci triaged module: nvfuser,True
86711,DISABLED test_variant_consistency_jit_linalg_lu_cuda_float32 (__main__.TestJitCUDA) triaged module: flaky-tests skipped module: unknown,2022-10-11 18:48:20+00:00,,0,10,triaged module: flaky-tests skipped module: unknown,False
86710,DISABLED test_attn_cuda (__main__.TestMin) triaged module: flaky-tests skipped module: functorch,2022-10-11 18:48:18+00:00,,0,29,triaged module: flaky-tests skipped module: functorch,False
86704,Performance tests mnist_hogwild-cpu_memory CPU memory increase by 30% triaged module: regression,2022-10-11 17:30:16+00:00,,0,5,triaged module: regression,True
86694,Feature request: Deterministic test input generation feature module: tests triaged,2022-10-11 15:25:49+00:00,,1,6,feature module: tests triaged,False
86684,[ONNX] AssertionError: A mismatch between the number of arguments (5) and their descriptors (4) was found at symbolic function 'scatter' needs reproduction module: onnx triaged,2022-10-11 12:24:49+00:00,,0,1,needs reproduction module: onnx triaged,False
86683,Documentation and typing hints for RProp module: docs module: optimizer triaged actionable,2022-10-11 11:25:18+00:00,,0,2,module: docs module: optimizer triaged actionable,True
86676,Pytorch built for Jetson errors if CUDA is not found module: build module: cuda triaged module: arm,2022-10-11 07:02:09+00:00,,0,0,module: build module: cuda triaged module: arm,True
86667,Adding a linear layer leads to failure of `optimize_for_mobile` triage review oncall: jit module: mkldnn,2022-10-11 03:22:52+00:00,,0,1,triage review oncall: jit module: mkldnn,True
86662,libtorch throws `required keyword attribute 'profiled_view_size' has the wrong type` on Linux oncall: jit,2022-10-11 02:32:23+00:00,,0,3,oncall: jit,False
86660,libtorch make failed  module: build triaged,2022-10-11 02:21:28+00:00,,0,0,module: build triaged,True
86644,"[NvFuser] INTERNAL ASSERT FAIL ""ScalarType should be static for Tensors in fusion for amp optimization"" triaged module: assert failure module: nvfuser",2022-10-10 22:20:38+00:00,,1,0,triaged module: assert failure module: nvfuser,True
86641,RFC(from users): nn.Module behavior with in-place changes triaged needs design module: functorch,2022-10-10 21:29:51+00:00,,0,2,triaged needs design module: functorch,True
86627,[ONNX] CSE pass in export pollutes Scope information module: onnx triaged module: regression bug,2022-10-10 19:54:52+00:00,,0,3,module: onnx triaged module: regression bug,True
86618,Move functorch tests from functorch/test/* to test/*; delete functorch CI configs module: ci triaged module: functorch,2022-10-10 17:15:56+00:00,,0,8,module: ci triaged module: functorch,True
86616,JIT returns different values for a model on cuda and returns a strange error message on cpu oncall: jit module: correctness (silent),2022-10-10 16:38:30+00:00,,0,0,oncall: jit module: correctness (silent),True
86613,Decomposition table is ignored with use_functionalize=True in AOT Autograd triaged oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,2022-10-10 15:55:59+00:00,,0,0,triaged oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,True
86612,Nonoptimal trace of silu_backward with AOT Autograd triaged oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,2022-10-10 15:50:15+00:00,,0,2,triaged oncall: pt2 module: functorch module: aotdispatch module: pt2-dispatcher,True
86601,NVFuser batch norm with prims: internal assert failure from test suite module: tests triaged module: nvfuser module: primTorch,2022-10-10 15:05:21+00:00,,1,0,module: tests triaged module: nvfuser module: primTorch,True
86598,`squeeze_` fails with JIT but succeeds without it oncall: jit,2022-10-10 14:57:24+00:00,,0,0,oncall: jit,True
86597,JIT returns different values for `cos + frac` on cpu oncall: jit module: correctness (silent),2022-10-10 14:52:16+00:00,,0,0,oncall: jit module: correctness (silent),True
86596,`CTCLoss` returns a different value with JIT on cuda oncall: jit module: cuda module: correctness (silent),2022-10-10 14:42:09+00:00,,0,0,oncall: jit module: cuda module: correctness (silent),True
86595,JIT model with `relu+div+sgn` will crash when computing the gradient oncall: jit module: crash module: functorch,2022-10-10 14:38:23+00:00,,0,0,oncall: jit module: crash module: functorch,True
86594,JIT model with mean will crash when computing the gradients on cuda triage review oncall: jit module: crash,2022-10-10 14:35:21+00:00,,0,0,triage review oncall: jit module: crash,True
86590,"Easy way to ""freeze"" BatchNorm running_mean/running_var module: nn triaged needs research",2022-10-10 13:38:30+00:00,,1,1,module: nn triaged needs research,True
86572,Instructions for Selective Build for Mobile Linux Platform oncall: mobile,2022-10-10 03:13:44+00:00,,0,1,oncall: mobile,False
86563,[functorch] colab links on functorch 0.2.0 website should be linked to a permalinked version of the colabs module: docs triaged module: functorch,2022-10-09 23:09:56+00:00,,0,3,module: docs triaged module: functorch,True
86558,Data conversion ops ignore `memory_format=torch.contiguous_format`  triaged module: primTorch,2022-10-09 19:20:05+00:00,,0,2,triaged module: primTorch,True
86554,[NvFuser] would change the output for some inaccurate dtype triaged module: nvfuser,2022-10-09 14:30:04+00:00,,0,0,triaged module: nvfuser,True
86553,`topk` will return the wrong value and could read out-of-bound value after jit oncall: jit,2022-10-09 14:26:13+00:00,,0,0,oncall: jit,True
86552,`max_unpool` and `max_pool` will trigger INTERNAL ASSERT FAIL in JIT oncall: jit,2022-10-09 14:07:16+00:00,,0,1,oncall: jit,True
86551,`MultiLabelMarginLoss` will return incorrect values in JIT after the first run on cuda oncall: jit module: cuda triaged module: nvfuser,2022-10-09 14:03:45+00:00,,0,0,oncall: jit module: cuda triaged module: nvfuser,True
86548,About autocast triaged module: amp (automated mixed precision),2022-10-09 11:27:36+00:00,,0,4,triaged module: amp (automated mixed precision),True
86547,Segmentation fault (core dumped) in RTX3090 needs reproduction module: cuda triaged,2022-10-09 10:44:46+00:00,,0,3,needs reproduction module: cuda triaged,True
86544,Compile failed at allreduce without gloo module: build triaged,2022-10-09 05:49:42+00:00,,0,2,module: build triaged,True
86539,cuda.list_gpu_processes() uses the 'wrong' device order (PCI_BUS_ID) module: cuda triaged,2022-10-09 02:54:55+00:00,,0,3,module: cuda triaged,True
93524,Test for multiple instances inference triaged oncall: pt2 module: cpu inductor,2022-10-09 01:16:59+00:00,,2,2,triaged oncall: pt2 module: cpu inductor,False
93523,[functorch] [vmap] [SymInt][fake tensor] triaged module: functorch,2022-10-08 21:51:32+00:00,,0,3,triaged module: functorch,True
86537,Running JIT trace for many times leads to OOM oncall: jit module: memory usage triaged,2022-10-08 17:15:37+00:00,,0,6,oncall: jit module: memory usage triaged,True
86532,Conv2d will crash by using `jit.trace` oncall: jit module: crash,2022-10-08 14:41:03+00:00,,0,0,oncall: jit module: crash,True
86529,[NvFuser] JIT model with `mul+atan+sgn` will access illegal memory on cuda when computing gradient triaged module: nvfuser,2022-10-08 14:16:44+00:00,,1,8,triaged module: nvfuser,True
86525, [Distributed: RPC] Sending `nn.Parameter` as RPC argument automatically detaches from the computation graph oncall: distributed,2022-10-08 06:25:31+00:00,,0,0,oncall: distributed,False
93520,Tooling Issue Tracking triaged oncall: pt2,2022-10-08 02:35:25+00:00,,1,1,triaged oncall: pt2,True
86518,[ONNX] Memory leak module: onnx triaged,2022-10-08 02:07:23+00:00,,0,1,module: onnx triaged,False
93718,Inductor doesn't fuse outer dimension softmax into a single kernel. feature triaged oncall: pt2 module: inductor internal ramp-up task,2022-10-07 23:04:22+00:00,,0,2,feature triaged oncall: pt2 module: inductor internal ramp-up task,False
86506,[ONNX] Create an input adapter for suppling torch module input to onnxruntime module: onnx triaged needs design onnx-triaged onnx-needs-info,2022-10-07 22:04:45+00:00,,0,1,module: onnx triaged needs design onnx-triaged onnx-needs-info,False
86494,Automatic broadcasting for batch addition for sparse tensors module: sparse triaged enhancement,2022-10-07 21:00:12+00:00,,0,1,module: sparse triaged enhancement,True
86493,`mem_get_info` reserves memory and can not be destroyed / deallocated.  module: cuda triaged,2022-10-07 20:57:00+00:00,,0,2,module: cuda triaged,True
86481,onnx.export make size operations return Tensor instead of int module: onnx triaged,2022-10-07 19:28:02+00:00,,0,6,module: onnx triaged,True
86479,FSDP support to load DDP optim checkpoints oncall: distributed triaged better-engineering module: fsdp,2022-10-07 19:18:22+00:00,,0,1,oncall: distributed triaged better-engineering module: fsdp,True
86467,torch.tensor obj automatically moved to shared memory upon Process launch module: multiprocessing triaged,2022-10-07 17:59:49+00:00,,0,0,module: multiprocessing triaged,True
86465,Wrong results with torch.linalg.inv on batched matrices when using cuda module: cuda triaged module: linear algebra module: correctness (silent) module: magma,2022-10-07 17:13:07+00:00,,0,9,module: cuda triaged module: linear algebra module: correctness (silent) module: magma,True
86456,`SyncBatchNorm` doesn't work with subclass of `torch.Tensor` oncall: distributed tensor subclass,2022-10-07 10:50:28+00:00,,0,4,oncall: distributed tensor subclass,False
86455,(JIT) x:Optional[T] cannot not expect content type after `if x is None or x.shape[0]==1` oncall: jit,2022-10-07 09:54:13+00:00,,0,0,oncall: jit,True
93716,End-to-End AMP training with GradScaler triaged oncall: pt2,2022-10-07 06:07:06+00:00,,0,0,triaged oncall: pt2,False
86449,torch.cuda.empty_cache() is not working module: cuda triaged,2022-10-07 05:14:14+00:00,,0,2,module: cuda triaged,True
86446,Improve FX naming for getitem calls triaged module: fx oncall: pt2 module: aotdispatch module: pt2-dispatcher,2022-10-07 04:23:20+00:00,,0,1,triaged module: fx oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
86444,Dedicated function for shallow_copy_and_detach module: autograd triaged tensor subclass,2022-10-07 04:14:22+00:00,,0,0,module: autograd triaged tensor subclass,True
86443,Stack trace preservation should work on plain use of make_fx / AOTAutograd triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2022-10-07 04:08:05+00:00,,0,0,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
86433,DISABLED test_rmsprop (optim.test_optim.TestOptim) triaged module: flaky-tests skipped module: dynamo,2022-10-07 00:59:13+00:00,,1,18,triaged module: flaky-tests skipped module: dynamo,False
93715,[minifier] Accuracy minification triaged oncall: pt2 module: minifier,2022-10-06 23:46:05+00:00,,0,1,triaged oncall: pt2 module: minifier,False
86427,[functorch] [aot_autograd]  triaged module: vmap module: functionalization oncall: pt2 module: aotdispatch module: pt2-dispatcher,2022-10-06 20:10:01+00:00,,0,15,triaged module: vmap module: functionalization oncall: pt2 module: aotdispatch module: pt2-dispatcher,True
86381,OpInfo Tests To Validate that All Operators Are Being Tested With Strided Tensors module: tests triaged,2022-10-06 17:34:51+00:00,,0,2,module: tests triaged,True
86356,`conv_transpose` is not similar to `nn.grad.conv_input` when `output_padding` is passed with non-default values. module: nn triaged actionable,2022-10-06 07:39:53+00:00,,0,3,module: nn triaged actionable,True
86351,Jetson JIT: Memory Leak on inference after optimize_for_inference oncall: jit,2022-10-06 05:07:44+00:00,,0,0,oncall: jit,False
86340,Add complex support for SparseAdam and LBFGS optimizers module: optimizer triaged module: complex actionable,2022-10-05 23:53:05+00:00,,0,3,module: optimizer triaged module: complex actionable,True
86339,Add `maximize` support to LBFGS optimizer module: optimizer triaged actionable,2022-10-05 23:51:12+00:00,,0,0,module: optimizer triaged actionable,False
86326,`torch.special.round` doesn't support the same dtypes as `torch.round` triaged module: special,2022-10-05 21:31:11+00:00,,1,3,triaged module: special,True
86315,Feature request: Tests for `int` should be tests for `numbers.Integral` feature triaged module: numpy module: python frontend,2022-10-05 20:22:33+00:00,,0,1,feature triaged module: numpy module: python frontend,True
86298,AOT Autograd Device Partitioning triaged module: functorch,2022-10-05 18:56:59+00:00,,0,0,triaged module: functorch,True
86287,JIT `lgamma` will return `inf` only with dual input in forward mode oncall: jit,2022-10-05 17:52:06+00:00,,0,0,oncall: jit,True
86279,`torch.multinomial` on MPS crashes with `Error: total bytes of NDArray > 2**32'` triaged module: regression module: mps,2022-10-05 16:18:04+00:00,,0,0,triaged module: regression module: mps,True
86276,JIT miss the argument `as_tuple` for API `nonzero` oncall: jit,2022-10-05 15:44:58+00:00,,0,0,oncall: jit,True
86274,TransformerEncoder/TransformerDecoder has same initial parameters for all layers triaged oncall: transformer/mha,2022-10-05 15:14:54+00:00,,1,4,triaged oncall: transformer/mha,True
86271,AUTOGRAD is not working on IOS module: autograd triaged oncall: mobile module: ios,2022-10-05 14:33:16+00:00,,0,8,module: autograd triaged oncall: mobile module: ios,True
86270,Autocast with BF16 on CPU slows down model more than 2X module: performance triaged module: bfloat16 module: amp (automated mixed precision),2022-10-05 13:01:39+00:00,,0,4,module: performance triaged module: bfloat16 module: amp (automated mixed precision),True
86265,TORCH_WARN is executed just once per set of parameters module: logging triaged,2022-10-05 10:08:41+00:00,,0,7,module: logging triaged,True
86261,"ONNX export of any TorchScript submodule (scripted or traced) fails with ""Modules that are called during a trace must be registered as submodules of the thing being traced""  oncall: jit module: onnx",2022-10-05 06:37:25+00:00,,0,7,oncall: jit module: onnx,False
93709,Support guard on thread number triaged enhancement oncall: pt2 module: dynamo,2022-10-05 05:59:57+00:00,,0,0,triaged enhancement oncall: pt2 module: dynamo,False
86225,[Feature] Dispatching PyTorch Distributed Collectives oncall: distributed feature triaged module: c10d,2022-10-04 19:48:57+00:00,,2,0,oncall: distributed feature triaged module: c10d,False
86204,How to perform unstructured interpolation  triaged module: interpolation,2022-10-04 13:03:50+00:00,,0,0,triaged module: interpolation,True
86194,path in WORKSPACE triaged module: bazel,2022-10-04 08:07:09+00:00,,0,1,triaged module: bazel,True
86192,fmt/src/os.cc: error: unknown type name 'error_code'; did you mean 'std::error_code'? module: build triaged,2022-10-04 06:45:44+00:00,,0,1,module: build triaged,True
93707,Dynamo shouldn't name getitem variables getitem; instead it should derive the name from the variable that was getitem'ed triaged oncall: pt2,2022-10-04 03:17:36+00:00,,0,3,triaged oncall: pt2,False
86162,torch.nn.functional.one_hot only works for int64 module: nn triaged,2022-10-03 22:44:49+00:00,,0,1,module: nn triaged,True
86152,MPSNDArray.mm:782: failed assertion; bufer is not large enough Mac M1 MPS module: memory usage triaged module: regression module: mps,2022-10-03 20:20:59+00:00,,1,17,module: memory usage triaged module: regression module: mps,True
86131,Debuggability++: Share instructions for building exotic CI configurations module: ci triaged,2022-10-03 17:16:56+00:00,,0,0,module: ci triaged,True
86128,[TorchDispatch] Scalar Only Inputs Gets Matched To Tensor Schema triaged module: __torch_dispatch__,2022-10-03 16:52:53+00:00,,0,6,triaged module: __torch_dispatch__,True
86124,torch.jit.trace throwing Invalid name for qualified name eror  oncall: jit,2022-10-03 16:09:23+00:00,,0,0,oncall: jit,False
86120,TransformerEncoder src_key_padding_mask does not work in eval() high priority triaged oncall: transformer/mha,2022-10-03 15:46:50+00:00,,1,9,high priority triaged oncall: transformer/mha,True
86116,JIT fails to trace binary cross entropy with a strange error msg oncall: jit,2022-10-03 15:18:48+00:00,,0,0,oncall: jit,True
86112,`cdist` should succeed when `p` is integer in JIT oncall: jit,2022-10-03 13:19:04+00:00,,0,0,oncall: jit,False
86111,When will the torch.sparse module be usable? module: sparse triaged,2022-10-03 12:38:33+00:00,,0,1,module: sparse triaged,True
86110,JIT return a tensor with different datatype from the tensor w/o gradient and normal function oncall: jit,2022-10-03 12:28:47+00:00,,0,1,oncall: jit,False
86107,`F.affine_grid` crashes on MPS triaged module: mps,2022-10-03 09:22:59+00:00,,1,3,triaged module: mps,True
86097,[Activation Checkpointing] Investigate pin_memory for CPU offload oncall: distributed module: checkpoint triaged,2022-10-03 06:17:03+00:00,,0,0,oncall: distributed module: checkpoint triaged,True
86076,Figure out the future of Metal backend given the existence of MPS module: build triaged module: arm module: mps,2022-10-02 20:48:46+00:00,,0,9,module: build triaged module: arm module: mps,True
86074,torch.remainder and torch.fmod produce wrong results module: numerical-stability triaged module: correctness (silent),2022-10-02 20:21:47+00:00,,0,1,module: numerical-stability triaged module: correctness (silent),True
86055,partial view/reshaping triaged enhancement module: python frontend,2022-10-01 19:28:24+00:00,,0,1,triaged enhancement module: python frontend,True
86048,Significantly worse MPS performance between torch 1.13.0.dev20220922 and torch 1.13.0.dev20220930 module: performance triaged module: mps,2022-10-01 05:17:57+00:00,,0,2,module: performance triaged module: mps,True
86020,Functorch memory_efficient_fusion gives wrong output batch size module: docs triaged module: functorch,2022-09-30 23:00:34+00:00,,0,8,module: docs triaged module: functorch,True
85960,Discrepancy in output shape for batch_norm inference mode between CUDA and CPU module: nn module: cuda triaged actionable,2022-09-30 08:38:46+00:00,,0,7,module: nn module: cuda triaged actionable,True
85949,"[Distributed] Loading distributed checkpoint with FSDP fails with varying key errors (pos.embedding, shared.weight) oncall: distributed",2022-09-30 03:00:50+00:00,,0,1,oncall: distributed,False
85939,CUDA OOM issue when running tests in CI high priority module: ci triaged,2022-09-29 22:36:38+00:00,,0,16,high priority module: ci triaged,True
85932,Setup ssh sometimes fail module: ci triaged,2022-09-29 21:51:29+00:00,,0,3,module: ci triaged,True
85909,Steam Deck Core Dump high priority module: rocm triaged,2022-09-29 18:43:06+00:00,,1,0,high priority module: rocm triaged,True
85893,TorchScript does not recognize mix-in types with `Enum` oncall: jit,2022-09-29 14:14:37+00:00,,0,0,oncall: jit,False
85889,High occupation on GPU 0 when converting Tensor to multi GPU module: performance module: cuda triaged,2022-09-29 10:29:24+00:00,,0,3,module: performance module: cuda triaged,True
85879,DISABLED test_aot_autograd_exhaustive_as_strided_scatter_cpu_float32 (__main__.TestEagerFusionOpInfoCPU) triaged module: flaky-tests skipped module: functorch,2022-09-29 01:23:05+00:00,,0,1,triaged module: flaky-tests skipped module: functorch,False
85877,JIT model could return 'NaN' gradient after the first execution oncall: jit,2022-09-29 01:15:09+00:00,,0,1,oncall: jit,True
85852,`torch.mm` produces wrong result on cpu when using in-place computation module: docs triaged,2022-09-28 21:19:46+00:00,,0,2,module: docs triaged,True
85851,Print a warning when user specifies a qconfig for some node and the qconfig is not supported by BackendConfig oncall: quantization low priority triaged,2022-09-28 21:16:57+00:00,,1,0,oncall: quantization low priority triaged,True
85841,Setting the cuda device when using start_processes in Jupyter on Ampere leads to CUDA reinitialization error module: multiprocessing module: cuda triaged,2022-09-28 19:30:20+00:00,,0,1,module: multiprocessing module: cuda triaged,True
85834,[primTorch] Need to update data-dependent check policy triaged module: primTorch,2022-09-28 18:46:57+00:00,,0,2,triaged module: primTorch,True
85831,[FSDP] `use_orig_params=True` Follow-Ups & Known Issues oncall: distributed triaged module: fsdp,2022-09-28 18:27:16+00:00,,1,2,oncall: distributed triaged module: fsdp,True
85815,string interning for dispatcher operator names triaged module: dispatch,2022-09-28 15:35:30+00:00,,0,2,triaged module: dispatch,True
85813,TorchScript error for `Enum` inside a module oncall: jit,2022-09-28 15:15:59+00:00,,0,0,oncall: jit,True
85805,"`vector_norm` will trigger ""Tracing failed sanity checks"" for JIT when ord is boolean tensor oncall: jit",2022-09-28 14:08:38+00:00,,0,0,oncall: jit,True
85804,JIT fails to trace `sparse.mm` with a strange error oncall: jit,2022-09-28 14:00:56+00:00,,0,0,oncall: jit,True
85792,TorchScript causes range_check error after a few iterations of forward-backward passes oncall: jit,2022-09-28 06:48:25+00:00,,0,2,oncall: jit,True
85791,nn.CrossEntropyLoss overflow with FP16 and minibatch module: nn module: loss triaged module: edge cases,2022-09-28 06:31:47+00:00,,0,0,module: nn module: loss triaged module: edge cases,True
85775,Timed out receiving the shared seed from the distribtued store on Rank 2 oncall: distributed module: dataloader triaged,2022-09-28 00:30:05+00:00,,1,4,oncall: distributed module: dataloader triaged,True
85773,Conda Pytorch (Pytorch channel) in WSL2 Ubuntu can't find libcudnn shared objects module: build module: cuda triaged,2022-09-27 23:59:27+00:00,,0,11,module: build module: cuda triaged,True
93701,Replace same with TestCase assertEqual good first issue triaged oncall: pt2 module: dynamo,2022-09-27 23:54:35+00:00,,0,2,good first issue triaged oncall: pt2 module: dynamo,True
93699,retro inductor OOM triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2022-09-27 23:15:59+00:00,,0,1,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
93698,imagen inductor errors triaged oncall: pt2 module: dynamo,2022-09-27 22:43:34+00:00,,0,1,triaged oncall: pt2 module: dynamo,False
93697,Inductor stable baselines assertion errors triaged oncall: pt2 module: dynamo,2022-09-27 21:52:17+00:00,,1,2,triaged oncall: pt2 module: dynamo,False
85757,[ONNX] Conversion failed when using dict as input to a scripted module module: onnx triaged onnx-triaged,2022-09-27 21:23:48+00:00,,0,1,module: onnx triaged onnx-triaged,True
93696,Minifier should not produce repro with backward call if it is not necessary to trigger error triaged oncall: pt2,2022-09-27 20:50:41+00:00,,0,0,triaged oncall: pt2,True
93695,Composer inductor errors triaged module: fsdp oncall: pt2 module: distributed,2022-09-27 20:46:15+00:00,,0,2,triaged module: fsdp oncall: pt2 module: distributed,False
93694,Minifier dumps checkpoints which don't actually reproduce the error triaged bug oncall: pt2 module: minifier,2022-09-27 20:43:41+00:00,,0,0,triaged bug oncall: pt2 module: minifier,True
85745,[Quant] Remove or clarify the meaning of Nones in QConfig/BackendConfig oncall: quantization low priority triaged,2022-09-27 19:13:40+00:00,,1,1,oncall: quantization low priority triaged,True
85706,RuntimeError: [enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 4860032 bytes. Error code 12 (Cannot allocate memory) oncall: jit,2022-09-27 14:20:40+00:00,,0,0,oncall: jit,True
85698,AMP consumes 30x gpu memory with bmm triaged module: amp (automated mixed precision),2022-09-27 11:20:25+00:00,,0,5,triaged module: amp (automated mixed precision),True
85671,nn.Embedding weights are not synced across processes with DistributedDataParallel when other parameters are present oncall: distributed module: ddp,2022-09-26 21:59:52+00:00,,0,4,oncall: distributed module: ddp,False
85663,[ONNX] torch/onnx is using rank to differentiate between ScalarType and TensorType module: onnx triaged onnx-triaged,2022-09-26 21:12:12+00:00,,0,9,module: onnx triaged onnx-triaged,False
85660,[functorch] CUDA Graph failure with AOTAutograd triaged module: cuda graphs module: functorch,2022-09-26 21:02:26+00:00,,0,10,triaged module: cuda graphs module: functorch,True
85656,"[functorch] conv.{1, 2, 3}d should raise errors triaged module: functorch",2022-09-26 20:35:29+00:00,,0,0,triaged module: functorch,True
85652,CUDA allocator feature requests module: cuda triaged module: CUDACachingAllocator,2022-09-26 20:08:03+00:00,,0,1,module: cuda triaged module: CUDACachingAllocator,True
85642,Could not run 'aten::native_batch_norm' with arguments from the 'SparseCUDA' backend.  using batch_norm module: sparse triaged,2022-09-26 17:24:46+00:00,,0,2,module: sparse triaged,True
85625,How to install pytorch with cuda 11.7 in anaconda envirment? triaged,2022-09-26 13:15:02+00:00,,0,2,triaged,True
85621,Gloo DDP SocketTimeout error on Windows oncall: distributed triaged module: c10d,2022-09-26 06:15:25+00:00,,0,3,oncall: distributed triaged module: c10d,True
85618,Build from source failed with error of different gpu architecture (compiler shows sm_30-related error but I use sm_86 GPU) module: build module: cuda triaged,2022-09-26 01:46:09+00:00,,0,1,module: build module: cuda triaged,True
85613,[MPS?] .to(memory_format=contiguous_format) behaves incorrectly; differently to .contiguous() triaged module: mps,2022-09-25 18:19:01+00:00,,0,1,triaged module: mps,True
85607,[Distributed: RPC] Failed to initialize RPC with >18 workers oncall: distributed triaged module: rpc,2022-09-25 14:39:54+00:00,,0,4,oncall: distributed triaged module: rpc,True
85606,Creating NumPy array with `dtype=object` of PyTorch tensors fails feature triaged module: numpy,2022-09-25 13:21:37+00:00,,0,7,feature triaged module: numpy,True
85604,"Multiple GPUs get  ""errno: 98 - Address already in use"" oncall: distributed triaged module: c10d",2022-09-25 11:45:38+00:00,,0,1,oncall: distributed triaged module: c10d,True
85590,Solve default argument induced include cycles by not using defaults / moving the defaults to inl module: build module: internals triaged,2022-09-24 03:24:29+00:00,,0,1,module: build module: internals triaged,True
85588,`linalg.norm` cannot compute the grad in forward mode after script oncall: jit,2022-09-24 02:27:47+00:00,,0,0,oncall: jit,False
85585,`as_tensor` will return a different dtype with script oncall: jit,2022-09-24 01:37:47+00:00,,0,0,oncall: jit,False
85573,`jit` could make some undifferentiable APIs differentiable oncall: jit,2022-09-23 21:42:26+00:00,,0,0,oncall: jit,True
85570,`mvlgamma_` will fail when compiling with trace `jit` oncall: jit,2022-09-23 21:29:47+00:00,,0,0,oncall: jit,True
85568,`detach_` behaves differently when computing the gradients in forward mode w/ `jit` oncall: jit,2022-09-23 20:50:24+00:00,,0,0,oncall: jit,False
85558,torch.Tensor.transpose().contiguous() on dimension of size 1 gives  wrong stride  module: docs triaged,2022-09-23 18:32:22+00:00,,0,4,module: docs triaged,True
85544,NvFuser single mode changes the output triaged module: nvfuser,2022-09-23 14:35:50+00:00,,0,1,triaged module: nvfuser,True
85538,Iterative Global Pruning Cause GPU Memory Leak high priority module: nn module: memory usage triaged,2022-09-23 09:36:07+00:00,,0,5,high priority module: nn module: memory usage triaged,True
85533,"[functorch] transforms like jacrev, jacfwd, grad, etc don't work with BatchNorm triaged module: functorch",2022-09-23 07:05:25+00:00,,0,2,triaged module: functorch,True
85520,Implement `rand_like` ref and implement nvfuser_impl for `uniform` prim triaged module: nvfuser,2022-09-23 00:02:58+00:00,,1,1,triaged module: nvfuser,True
85514,The reload `MultiLabelMarginLoss` will have different gradients on cuda oncall: jit,2022-09-22 22:09:09+00:00,,0,0,oncall: jit,False
85513,"Measure impact of JIT decompositions, reconsider the design oncall: jit",2022-09-22 22:08:18+00:00,,0,3,oncall: jit,False
85505,The reload model has different (and strange) forward computation from original model with `LSTMCell` oncall: jit,2022-09-22 21:34:38+00:00,,0,0,oncall: jit,False
85499,Execute smoke test for Better Transformer feature  module: ci triaged,2022-09-22 20:19:44+00:00,,0,1,module: ci triaged,True
85485,"`max_pool2d_with_indices(self, ...)` shouldn't need to save `self` for backward module: autograd triaged",2022-09-22 18:02:58+00:00,,0,12,module: autograd triaged,True
85475,Issue with converting Comet model to ONNX. Split-node error. module: onnx triaged,2022-09-22 14:30:12+00:00,,0,5,module: onnx triaged,True
93684,Can we rewrite numpy operators to pytorch operators? good first issue triaged module: numpy,2022-09-22 12:06:59+00:00,,1,12,good first issue triaged module: numpy,True
85450,Cannot index into a tensor using indices from another device - regression from 1.12 triaged module: regression module: advanced indexing,2022-09-22 00:13:18+00:00,,1,4,triaged module: regression module: advanced indexing,True
85439,`aminmax` will trigger INTERNAL ASSERT if input is empty on cuda good first issue triaged actionable module: edge cases,2022-09-21 21:21:54+00:00,,0,23,good first issue triaged actionable module: edge cases,True
85438,Prim Output Spec is Not Always Consistent With Eager triaged module: primTorch,2022-09-21 21:20:53+00:00,,0,0,triaged module: primTorch,True
85425,Feature Request: Deterministic Algorithm for MaxPool3d feature module: nn good first issue triaged module: determinism module: pooling,2022-09-21 18:24:00+00:00,,0,6,feature module: nn good first issue triaged module: determinism module: pooling,True
85397,torch.nn.utils.prune.remove reorders the parameters of a module unexpectedly triaged module: pruning,2022-09-21 09:25:08+00:00,,0,0,triaged module: pruning,True
85392,please report a bug to PyTorch. Expected Object but got PyObject triaged module: torchbind,2022-09-21 04:03:15+00:00,,0,1,triaged module: torchbind,True
85390,Please put back missing rocm builds of Torch Vision. module: binaries module: rocm triaged,2022-09-21 03:02:34+00:00,,0,0,module: binaries module: rocm triaged,True
85387,very strange speed of torch.bmm with specific tensor shape module: performance module: cuda triaged module: linear algebra,2022-09-21 02:25:55+00:00,,0,0,module: performance module: cuda triaged module: linear algebra,True
85377,CI fails for test_compare_cpu_nn_functional_embedding_cuda_float32 which is not reproducible locally module: cuda module: tests triaged module: embedding,2022-09-20 21:59:14+00:00,,0,8,module: cuda module: tests triaged module: embedding,True
85375,Inconsistency between geometric distributions module: distributions module: molly-guard triaged,2022-09-20 21:32:10+00:00,,0,3,module: distributions module: molly-guard triaged,True
85366,More windows for filtering and spectral analysis triaged module: scipy compatibility,2022-09-20 19:33:14+00:00,,0,12,triaged module: scipy compatibility,True
85351,Point community docs to master triaged module: doc infra,2022-09-20 17:26:43+00:00,,1,0,triaged module: doc infra,False
85342,"JIT fuser issues with {ceil,floor,round,trunc}(int8)   oncall: jit NNC",2022-09-20 15:26:55+00:00,,0,0,oncall: jit NNC,False
85335,functorch aten::scatter_add_  not implemented triaged module: functorch,2022-09-20 12:33:52+00:00,,0,4,triaged module: functorch,True
85329,Crash in `torch.package.PackageExporter` module: crash triaged oncall: package/deploy imported module: edge cases,2022-09-20 08:25:03+00:00,,0,1,module: crash triaged oncall: package/deploy imported module: edge cases,True
93680,AOT Autograd traces have instability in defining the same Graph triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,2022-09-20 05:23:48+00:00,,0,0,triaged oncall: pt2 module: aotdispatch module: pt2-dispatcher,False
85302,Remove `TypedStorage` and use only `UntypedStorage` triaged module: python frontend,2022-09-19 21:00:55+00:00,,1,0,triaged module: python frontend,True
85300,torchrun substitutes host names for IP addresses oncall: distributed triaged module: elastic,2022-09-19 20:18:48+00:00,,0,10,oncall: distributed triaged module: elastic,True
85296,Have NVIDIA driver and other related dependencies as part of the Linux AMI module: ci triaged,2022-09-19 19:19:34+00:00,,0,1,module: ci triaged,True
85258,"nvFuser support for {ceil,floor,round,trunc}(int) triaged module: nvfuser",2022-09-19 14:34:54+00:00,,0,0,triaged module: nvfuser,True
85235,Add `persistent` option to `nn.Module.buffers`. module: nn triaged actionable,2022-09-18 22:43:24+00:00,,1,1,module: nn triaged actionable,True
85234,c10d all_gather aborts with Signal 8 (SIGFPE) when tensor.numel() == 0 oncall: distributed module: c10d,2022-09-18 21:13:44+00:00,,0,1,oncall: distributed module: c10d,False
85230,[MPS] load checkpoints gives zero weights when map_location is mps triaged has workaround module: correctness (silent) module: mps,2022-09-18 15:18:33+00:00,,0,4,triaged has workaround module: correctness (silent) module: mps,True
85229,TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function.  oncall: jit triaged,2022-09-18 13:25:30+00:00,,0,1,oncall: jit triaged,True
85227,topk returns different results with the same input in cuda and cpu module: cuda triaged module: sorting and selection,2022-09-18 11:59:41+00:00,,0,4,module: cuda triaged module: sorting and selection,True
85217,Segmentation fault in native_batch_norm module: crash module: error checking triaged module: norms and normalization,2022-09-17 17:26:23+00:00,,0,0,module: crash module: error checking triaged module: norms and normalization,True
85215,Floating point exception in gather gradient computation. module: crash module: error checking triaged module: edge cases,2022-09-17 17:26:10+00:00,,0,0,module: crash module: error checking triaged module: edge cases,True
85214,Segmentation fault in mkldnn_reorder_conv2d_weight and mkldnn_reorder_conv3d_weight module: crash triaged module: mkldnn module: edge cases,2022-09-17 17:26:03+00:00,,0,0,module: crash triaged module: mkldnn module: edge cases,True
85211,OSError libstdc++.so.6 at import needs reproduction module: binaries triaged,2022-09-17 17:24:10+00:00,,0,2,needs reproduction module: binaries triaged,True
85201,performance between manually created graph and CUDAGraph.replay triaged module: cuda graphs,2022-09-17 09:42:41+00:00,,0,8,triaged module: cuda graphs,True
85157,NestedTensor 2.0 issue tracking triaged module: nestedtensor,2022-09-16 16:22:18+00:00,,1,4,triaged module: nestedtensor,True
85149,Pytorch on iOS (iPhone X & XR) throwing can't allocate memory exception. Ref Logs: oncall: mobile module: ios,2022-09-16 14:34:08+00:00,,0,1,oncall: mobile module: ios,False
85147,torch::quantile performance? module: performance triaged,2022-09-16 13:20:03+00:00,,0,0,module: performance triaged,True
85127,[ONNX] Using values from a different tensor to index a tensor returns a tensor with incorrect shape in exported ONNX model oncall: jit module: onnx triaged bug,2022-09-15 23:28:47+00:00,,1,16,oncall: jit module: onnx triaged bug,True
85122,"PT Dispatcher confusing error message ""There were no tensor arguments to this function"" triaged module: dispatch",2022-09-15 22:29:24+00:00,,1,0,triaged module: dispatch,True
85121,make_traced() doesn't respect setting the seed module: tests triaged module: primTorch,2022-09-15 22:25:09+00:00,,0,0,module: tests triaged module: primTorch,True
93675,Need operator fallback stats triaged oncall: pt2 module: dynamo,2022-09-15 22:04:18+00:00,,1,6,triaged oncall: pt2 module: dynamo,False
85116,[ONNX][bug] `nn.Transformer` contains unsupported tensor scalar type module: onnx triaged onnx-triaged bug,2022-09-15 22:00:46+00:00,,0,11,module: onnx triaged onnx-triaged bug,True
93674,Need easier way to tell which step of the optimized path fails (dynamo + inductor) triaged oncall: pt2 module: dynamo,2022-09-15 21:43:34+00:00,,2,4,triaged oncall: pt2 module: dynamo,False
85113,[ONNX] Produce error message for incorrect number of dummy inputs instead of Internal assert failure needs reproduction module: onnx triaged onnx-triaged,2022-09-15 21:34:56+00:00,,0,2,needs reproduction module: onnx triaged onnx-triaged,True
85105,Python dispatch for PyOps needs to respect tensor subclasses triaged module: dispatch module: __torch_dispatch__,2022-09-15 20:11:55+00:00,,1,0,triaged module: dispatch module: __torch_dispatch__,True
85098,install libtorch cxx11 ABI as default in PyTorch pip installation module: binaries triaged,2022-09-15 18:56:38+00:00,,0,1,module: binaries triaged,True
93672,[ddp] must set `static_graph=False` when running with dynamo triaged module: ddp oncall: pt2 module: dynamo module: distributed,2022-09-15 18:51:38+00:00,,1,8,triaged module: ddp oncall: pt2 module: dynamo module: distributed,True
85096,Update `use_deterministic_algorithms` documentation and tests to include `nn.functional` counterparts for all `nn` modules module: docs module: tests triaged module: determinism,2022-09-15 18:24:43+00:00,,1,0,module: docs module: tests triaged module: determinism,True
85093,Memoizing AOT Autograd Input Conversion Breaks Training with Tied Parameters triaged module: functorch,2022-09-15 16:41:18+00:00,,0,1,triaged module: functorch,True
85088,reentrant torch.utils.checkpoint does not work with NamedTuple outputs high priority triage review oncall: distributed module: checkpoint,2022-09-15 15:46:42+00:00,,0,3,high priority triage review oncall: distributed module: checkpoint,True
85082,"[NNC] loop vectorization fails, `Ramp` and `Broadcast` undefined triaged NNC",2022-09-15 10:35:56+00:00,,0,0,triaged NNC,True
85081,primTorch/nvfuser: have a way to check that refs are added to __all__ feature triaged module: nvfuser module: primTorch,2022-09-15 10:29:33+00:00,,0,1,feature triaged module: nvfuser module: primTorch,True
85078,"libtorch create a tensor is very slow, who can tell me why module: performance module: cpp triaged",2022-09-15 08:51:58+00:00,,0,0,module: performance module: cpp triaged,True
85072,Segmentation fault in `torch.jit.wait` oncall: jit,2022-09-15 06:02:50+00:00,,0,0,oncall: jit,True
85058,Selectively sync internal Meta discussions / posts to dev-discuss.pytorch.org triaged,2022-09-15 00:10:07+00:00,,0,2,triaged,True
85036,Add an opaque epilogue in AOTAutograd for aliasing/mutations triaged module: functionalization module: functorch,2022-09-14 19:54:28+00:00,,0,2,triaged module: functionalization module: functorch,True
85027,Custom autograd functions are not inlined when export mode is ONNX_ATEN_FALLBACK module: onnx triaged,2022-09-14 18:58:05+00:00,,1,0,module: onnx triaged,False
85025,[CheckpointWrapper] Revamp API design oncall: distributed better-engineering module: fsdp,2022-09-14 18:30:08+00:00,,0,0,oncall: distributed better-engineering module: fsdp,False
84994,Cuda tensor is zero when passed through multiprocessing queue module: multiprocessing triaged,2022-09-14 10:11:11+00:00,,1,2,module: multiprocessing triaged,True
84990,Segmentation fault in `torch.futures.collect_all` module: crash module: error checking triaged,2022-09-14 07:55:01+00:00,,0,0,module: crash module: error checking triaged,True
84972,Add unit tests for test decorators feature module: tests triaged,2022-09-14 00:05:07+00:00,,1,1,feature module: tests triaged,False
84944,test_warp_softmax_64bit_indexing_cuda_float16 takes ~147GB of CPU memory and is very slow module: memory usage triaged module: testing,2022-09-13 18:41:48+00:00,,0,8,module: memory usage triaged module: testing,True
84937,DISABLED test_random_seed (__main__.TestDataLoaderUtils) module: dataloader triaged skipped,2022-09-13 16:24:58+00:00,,0,3,module: dataloader triaged skipped,False
84936,CPU and MPS floating point math is different (in a significant way) module: numerical-stability triaged module: mps,2022-09-13 16:19:49+00:00,,1,18,module: numerical-stability triaged module: mps,True
84934,RuntimeError: input_shape.size() > 0 || reshape.size() > 0INTERNAL ASSERT FAILED needs reproduction module: onnx triaged onnx-needs-info,2022-09-13 15:41:12+00:00,,0,1,needs reproduction module: onnx triaged onnx-needs-info,False
84932,Separate doc and binaries build triaged module: doc infra,2022-09-13 14:59:35+00:00,,0,0,triaged module: doc infra,False
84925,`is_pinned()` support in PrimTorch and FakeTensor. triaged module: primTorch module: fakeTensor,2022-09-13 08:12:06+00:00,,0,1,triaged module: primTorch module: fakeTensor,True
84922,Functorch functionalization causes increased memory usage triaged module: functionalization module: functorch,2022-09-13 05:58:52+00:00,,0,2,triaged module: functionalization module: functorch,True
84870,Re-Running PR Sanity Check after Adding `skip-pr-sanity-checks` Label Still Fails module: ci triaged,2022-09-12 17:28:37+00:00,,0,4,module: ci triaged,True
84864,torch.utils.checkpoint (with use_reentrant=False) doesn't work with all PyTorch features that set TLS module: checkpoint triaged module: __torch_dispatch__ tensor subclass,2022-09-12 15:13:00+00:00,,0,1,module: checkpoint triaged module: __torch_dispatch__ tensor subclass,True
84863,View consistency for PrimTorch+nvFuser tests triaged module: nvfuser module: primTorch,2022-09-12 15:02:55+00:00,,0,12,triaged module: nvfuser module: primTorch,True
84860,Feature Request: deterministic adaptive_avg_pool2d_backward_cuda feature module: cuda triaged module: determinism,2022-09-12 14:06:05+00:00,,0,0,feature module: cuda triaged module: determinism,True
93667,14k github models on PyTorch 2.0 pass rates dashboard  triaged oncall: pt2 module: dynamo,2022-09-12 06:12:13+00:00,,1,47,triaged oncall: pt2 module: dynamo,False
84847,ONNX exporter error needs reproduction module: onnx triaged onnx-needs-info,2022-09-12 01:03:38+00:00,,0,1,needs reproduction module: onnx triaged onnx-needs-info,True
84840,"TypeError: finfo(): argument 'type' (position 1) must be torch.dtype, not HFProxy triaged module: fx",2022-09-11 17:34:13+00:00,,1,0,triaged module: fx,True
84839,Profiler Hangs on Non-Blocking H2D Transfer in Non-Default Stream oncall: profiler,2022-09-11 17:14:23+00:00,,0,0,oncall: profiler,False
84835,Batch multiplication for torch.sparse matrix multiplication module: sparse triaged module: linear algebra,2022-09-11 08:19:02+00:00,,0,2,module: sparse triaged module: linear algebra,True
84831,INTERNAL ASSERT FAILED for _jit_pass_vulkan_optimize_for_mobile (Google Colab) oncall: jit oncall: mobile module: vulkan,2022-09-11 01:49:56+00:00,,0,3,oncall: jit oncall: mobile module: vulkan,True
84813,MPS: allow selecting specific MTLDevice by registryID via environment variable triaged enhancement module: mps,2022-09-10 13:32:52+00:00,,0,2,triaged enhancement module: mps,True
84805,compiling failed from source module: sparse module: build module: cuda triaged,2022-09-10 03:15:07+00:00,,0,2,module: sparse module: build module: cuda triaged,True
84782,macOS Pyinstaller: libc++abi: terminating with uncaught exception of type c10::Error: Type c10::intrusive_ptr<ConvPackedParamsBase<2>> could not be converted to any of the known types module: cpp-extensions triaged module: third_party needs research module: m1,2022-09-09 20:50:33+00:00,,0,6,module: cpp-extensions triaged module: third_party needs research module: m1,True
84751,Allow passing dict (as opposed to OrderedDict) to nn.Sequential module: nn triaged needs research,2022-09-09 13:03:16+00:00,,1,15,module: nn triaged needs research,False
84721,PyTorch-DirectML RFC feature triaged,2022-09-08 21:53:56+00:00,,0,0,feature triaged,False
84717,Test aten decompositions match their alias information triaged better-engineering module: primTorch,2022-09-08 19:41:32+00:00,,0,4,triaged better-engineering module: primTorch,True
84716,[ONNX] Speed up unit tests module: onnx triaged better-engineering,2022-09-08 19:09:11+00:00,,0,3,module: onnx triaged better-engineering,False
84711,Add documentation about backward graph gc behavior module: docs triaged,2022-09-08 17:07:36+00:00,,0,0,module: docs triaged,False
84710,About the different ways to print models module: printing triaged,2022-09-08 16:49:52+00:00,,0,0,module: printing triaged,True
84705,Set dtype if tensor converted to numpy needs reproduction triaged module: numpy,2022-09-08 15:36:54+00:00,,0,1,needs reproduction triaged module: numpy,True
84697,NotImplementedError: The operator aten::native_group_norm_backward triaged module: mps,2022-09-08 14:28:08+00:00,,0,2,triaged module: mps,True
84692,Error when trying to export MONAI model to ONNX module: onnx triaged onnx-needs-info,2022-09-08 12:41:08+00:00,,0,4,module: onnx triaged onnx-needs-info,True
84691,test_public_bindings is not robust to various build options triaged topic: build,2022-09-08 12:26:15+00:00,,0,0,triaged topic: build,True
84685,[TensorExpr] applying `rfactor` for a Mul Reducer with init value different than 1 results in wrong results oncall: jit,2022-09-08 06:04:46+00:00,,0,0,oncall: jit,False
84681,JIT will affect the gradient computation of forward mode oncall: jit,2022-09-08 02:02:30+00:00,,0,2,oncall: jit,False
84673,Autograd will take `init` module API into account when using `jit` oncall: jit,2022-09-07 23:38:05+00:00,,0,2,oncall: jit,True
84661,[ONNX] Track non-exportable pattern as diagnostics. module: onnx triaged onnx-triaged,2022-09-07 20:49:31+00:00,,1,0,module: onnx triaged onnx-triaged,True
84646,JIT script calculation/dtype inconsistent depending on operator expression oncall: jit,2022-09-07 16:20:45+00:00,,0,0,oncall: jit,True
84630,"torch.nn.functional.interpolate fails on some degenerate shapes, but passes on others triaged module: interpolation",2022-09-07 12:47:37+00:00,,0,1,triaged module: interpolation,True
84628,INTERNAL ASSERT when the type of argument is not considered in JIT oncall: jit,2022-09-07 12:35:10+00:00,,0,2,oncall: jit,True
84625,Beta distribution behaves incorrectly for small parameters module: distributions triaged module: edge cases,2022-09-07 09:26:52+00:00,,0,1,module: distributions triaged module: edge cases,True
84620,torch.hub.load local model triaged module: hub,2022-09-07 07:40:49+00:00,,0,1,triaged module: hub,True
84616,Autogenerated out functions are missing at::cpu:: and co bindings triaged module: codegen topic: build,2022-09-07 01:57:20+00:00,,0,0,triaged module: codegen topic: build,True
84615,Serialize the warmed up torchscript module oncall: jit,2022-09-07 01:54:32+00:00,,0,1,oncall: jit,False
93661,"Capture scalar outputs / dynamically sized outputs by default, partition graphs for backends that can't handle it feature triaged ezyang's list oncall: pt2 module: dynamic shapes module: inductor",2022-09-06 22:14:11+00:00,,0,4,feature triaged ezyang's list oncall: pt2 module: dynamic shapes module: inductor,False
84597,Accept SymInts and SymFloats For Scalar Inputs triaged,2022-09-06 20:49:57+00:00,,0,0,triaged,True
84593,Uneven and/or Dynamically sized collectives good first issue triaged module: c10d,2022-09-06 19:57:17+00:00,,0,7,good first issue triaged module: c10d,True
84588,torch.jit.script IndentationError: unexpected indent oncall: jit,2022-09-06 18:57:04+00:00,,0,0,oncall: jit,True
84578,module: multiprocessing SimpleQueue put cannot bigger 716 in windows.And it is not has any info.The program is blocked and does not move. module: multiprocessing triaged,2022-09-06 16:55:43+00:00,,0,0,module: multiprocessing triaged,True
84573,Tensor slice copy across multiple devices fails silently triaged module: advanced indexing,2022-09-06 15:44:31+00:00,,0,13,triaged module: advanced indexing,True
84565,Tensor Subclass that doesn't require grad may wrap a Tensor subclass that requires grad triaged tensor subclass,2022-09-06 13:52:32+00:00,,0,1,triaged tensor subclass,True
84560,[optim] asgd : handling of complex params as real params (NaN vs inf) module: optimizer triaged module: edge cases,2022-09-06 07:27:26+00:00,,1,0,module: optimizer triaged module: edge cases,True
84550,Pytorch does not recognize GPU in WSL2 triaged module: wsl,2022-09-05 19:44:48+00:00,,0,4,triaged module: wsl,True
84539,list of tensors can't be converted to a torch tensor while list of lists gets easily converted to a pytorch tensor triaged module: numpy,2022-09-05 12:19:35+00:00,,0,0,triaged module: numpy,True
84538,OpInfo tests should compare gpu to cpu implementations module: tests triaged topic: not user facing,2022-09-05 11:37:32+00:00,,1,4,module: tests triaged topic: not user facing,True
84537,Minimal example for torch.optim.SparseAdam module: sparse module: docs triaged,2022-09-05 11:12:49+00:00,,0,1,module: sparse module: docs triaged,False
84530,`tensordot` not working for dtype int32 and lower when there is only 1 element in the given axis triaged module: linear algebra actionable bug,2022-09-05 06:18:47+00:00,,0,1,triaged module: linear algebra actionable bug,True
84529,"test_prims.py:test_nvfuser_no_args_cuda, memory leak triaged module: primTorch",2022-09-05 06:12:39+00:00,,0,0,triaged module: primTorch,True
84524,nn.Softmax should not allow default/implicit/unset dim constructor argument module: nn triaged needs research module: deprecation,2022-09-04 16:14:44+00:00,,1,6,module: nn triaged needs research module: deprecation,True
84523,"Issue with MPS ops lead to make_grid broken with mps device Tensors, whole grid is the 'first' image triaged module: mps",2022-09-04 10:03:37+00:00,,0,4,triaged module: mps,True
84520,MPS backend appears to be limited to 32 bits triaged module: mps,2022-09-04 02:21:00+00:00,,1,6,triaged module: mps,True
84515,Torch.FX work with autograd.Function triaged module: fx fx,2022-09-03 14:58:24+00:00,,0,0,triaged module: fx fx,True
84510,[NVFuser] RuntimeError: ref_id_it != replayed_concrete_ids_.vector().end() INTERNAL ASSERT FAILED triaged module: assert failure module: nvfuser,2022-09-03 03:51:53+00:00,,0,6,triaged module: assert failure module: nvfuser,True
84495,functionalize: Does not compose cleanly with torch.jit.script/torch.jit.trace oncall: jit module: functionalization,2022-09-02 18:24:55+00:00,,0,3,oncall: jit module: functionalization,True
84489,"For PyTorch Nightly, failure when changing MPS device to CPU after PYTORCH_ENABLE_MPS_FALLBACK occurs. triaged module: mps",2022-09-02 14:28:54+00:00,,1,1,triaged module: mps,True
84473,Install LibTorch by Conan or other C++ package manager module: cpp triaged topic: binaries,2022-09-02 10:47:49+00:00,,0,0,module: cpp triaged topic: binaries,True
84468,[c10d] Support a public API to retrieve default process group oncall: distributed feature triaged pt_distributed_rampup,2022-09-02 08:07:02+00:00,,0,0,oncall: distributed feature triaged pt_distributed_rampup,False
84445,Strange cuda illegal memory allocation error module: cuda triaged,2022-09-01 23:15:30+00:00,,0,3,module: cuda triaged,True
84422,Set up tests to run periodically and surface them on HUD module: ci triaged,2022-09-01 16:02:17+00:00,,0,0,module: ci triaged,True
84415,Deepcopy of FX graph fails with nested make_fx and constant tensors triaged module: fx fx,2022-09-01 13:40:14+00:00,,0,4,triaged module: fx fx,True
84414,several questions about pytorch DDP triaged module: nccl module: ddp,2022-09-01 13:15:05+00:00,,0,0,triaged module: nccl module: ddp,True
84412,Odd type-casting behaviour in prims.div triaged module: type promotion module: primTorch,2022-09-01 11:17:23+00:00,,0,0,triaged module: type promotion module: primTorch,True
84370,Installation prefix is not passed to CMake appropriately needs reproduction module: build triaged,2022-08-31 20:16:50+00:00,,0,4,needs reproduction module: build triaged,True
93656,explain() has confusing explanation of graph breaks triaged oncall: pt2 module: dynamo,2022-08-31 19:09:44+00:00,,1,5,triaged oncall: pt2 module: dynamo,False
84353,torch.Size should convert all elements to ints triaged module: python frontend,2022-08-31 14:54:52+00:00,,0,0,triaged module: python frontend,True
84348,"RuntimeError: ""slow_conv2d_cpu"" not implemented for 'Half' triaged module: half",2022-08-31 13:11:19+00:00,,0,3,triaged module: half,True
84347,Lack of newly raised optimizers high priority feature module: optimizer triaged needs research,2022-08-31 12:42:45+00:00,,0,4,high priority feature module: optimizer triaged needs research,True
84346,fix ATen tests that do not compile triaged,2022-08-31 12:30:32+00:00,,0,0,triaged,True
84340,Floordiv is deprecated. oncall: jit,2022-08-31 11:40:37+00:00,,0,1,oncall: jit,False
84336,torch 1.12.1 cuda 10.2 runs slower than torch 1.8.2 cuda 10.2 module: performance module: cudnn module: cuda triaged,2022-08-31 10:34:01+00:00,,0,13,module: performance module: cudnn module: cuda triaged,True
84335,Should enable skipped tests for `to` OpInfo  triaged module: primTorch,2022-08-31 09:38:23+00:00,,0,4,triaged module: primTorch,True
84321,[Profiler] Snapshot CudaCachingAllocator on profile begin module: bootcamp oncall: profiler,2022-08-31 00:45:42+00:00,,0,3,module: bootcamp oncall: profiler,False
84318,[Profiler] Generic Tensor summary module: bootcamp oncall: profiler,2022-08-31 00:34:27+00:00,,0,0,module: bootcamp oncall: profiler,False
84316,Torch.fx tracing bug with dictionary.update calls on input triaged module: fx fx,2022-08-30 23:56:49+00:00,,0,0,triaged module: fx fx,True
84311,DecompositionInterpreter creates invalid graph triaged module: nvfuser,2022-08-30 22:13:11+00:00,,0,0,triaged module: nvfuser,True
84309,Unable to run a single convolutional layer in different CUDA-contexts module: cuda triaged,2022-08-30 21:35:48+00:00,,0,2,module: cuda triaged,True
84304,op for aten::bitwise_and during torch.jit.trace oncall: jit,2022-08-30 20:39:16+00:00,,0,2,oncall: jit,False
84290,Fix convert path for fixed qparam ops (sigmoid and softmax) oncall: quantization low priority triaged,2022-08-30 18:14:56+00:00,,0,6,oncall: quantization low priority triaged,True
84265,torch.Tensor.to.dtype_layout overload is not available in Python triaged module: codegen module: python frontend,2022-08-30 13:03:40+00:00,,0,4,triaged module: codegen module: python frontend,True
84261,relu-gru mse is 0.022 much greater than 0.003 with half dtype. needs reproduction triaged module: half,2022-08-30 11:47:06+00:00,,0,1,needs reproduction triaged module: half,True
84259,would you like  upload to the cpp libtorch to  vcpkg  package repo? module: cpp module: ci triaged enhancement topic: binaries,2022-08-30 11:26:23+00:00,,0,5,module: cpp module: ci triaged enhancement topic: binaries,True
84257,Support dict inputs and outputs when exporting to ONNX module: onnx triaged onnx-needs-info,2022-08-30 09:45:08+00:00,,0,6,module: onnx triaged onnx-needs-info,False
84247,Ensure ops account for offsets and strides triaged module: nestedtensor release notes: nested tensor,2022-08-30 03:31:59+00:00,,0,0,triaged module: nestedtensor release notes: nested tensor,True
84234,Randomness should be consistent across devices with use_deterministic_algorithms triaged module: random module: determinism,2022-08-29 23:24:43+00:00,,0,12,triaged module: random module: determinism,True
84202,Gradient value calculation error in MultiLabelMarginLoss module: loss module: cuda triaged module: correctness (silent),2022-08-29 16:30:16+00:00,,0,1,module: loss module: cuda triaged module: correctness (silent),True
84194,Pytorch gets small bias on the result of different types of divisors while doing floating point division. module: numerical-stability module: cuda triaged,2022-08-29 15:23:15+00:00,,0,4,module: numerical-stability module: cuda triaged,True
84193,Attach execution time to each node in an fx trace triaged fx hacktoberfest,2022-08-29 15:15:35+00:00,,0,5,triaged fx hacktoberfest,False
84192,CUDA 11.6 linux-bionic-cuda11.6-py3-gcc7-slow-gradcheck failure module: ci triaged module: linear algebra,2022-08-29 13:17:14+00:00,,0,9,module: ci triaged module: linear algebra,True
84189,"RuntimeError: outputs_[i]->uses().empty() INTERNAL ASSERT FAILED at /pytorch/torch/csrc/jit/ir.cpp:1027, please report a bug to PyTorch.  (eraseOutput at /pytorch/torch/csrc/jit/ir.cpp:1027) triage review oncall: jit",2022-08-29 08:20:04+00:00,,0,3,triage review oncall: jit,True
84187,[jit] WithInsertPoint can't get back to the prev_ node if the prev_ node has been destroyed oncall: jit,2022-08-29 06:55:51+00:00,,0,0,oncall: jit,False
84181,Session of Google Colab crashes when `torch.utils::SummaryWriter` is called after importing `torchaudio` high priority module: crash triaged module: tensorboard,2022-08-28 15:14:37+00:00,,1,4,high priority module: crash triaged module: tensorboard,True
84178,Support setting strides on quantized weights of Embedding oncall: quantization low priority triaged,2022-08-28 09:36:52+00:00,,1,1,oncall: quantization low priority triaged,True
84175,FSDP Forward order differs from that of first run oncall: distributed module: fsdp,2022-08-27 23:34:52+00:00,,0,7,oncall: distributed module: fsdp,False
84167,`linux-bionic-cuda10.2-py3.9-gcc7`  multigpu test are broken oncall: distributed module: ci module: regression,2022-08-27 14:04:31+00:00,,0,2,oncall: distributed module: ci module: regression,False
84148,[Quant] Reference get_default_qconfig_mapping in docs/tutorials oncall: quantization triaged,2022-08-26 21:21:30+00:00,,1,0,oncall: quantization triaged,False
84140,Please include virtual/physical batch sizes in the tutorials module: docs triaged,2022-08-26 20:23:20+00:00,,0,0,module: docs triaged,True
84138,MPS convolution is sometimes returning NaNs for valid inputs. triaged module: mps,2022-08-26 20:07:01+00:00,,0,9,triaged module: mps,True
84135,[jit] ignored method calling static method results in an error oncall: jit,2022-08-26 18:51:55+00:00,,0,0,oncall: jit,True
84071,Move self.subtest calls in FSDP test suite to run_subtests utility oncall: distributed triaged better-engineering module: fsdp,2022-08-25 18:26:08+00:00,,0,3,oncall: distributed triaged better-engineering module: fsdp,True
84064,Better error message for qlinear_prepack oncall: quantization triaged,2022-08-25 17:00:58+00:00,,0,0,oncall: quantization triaged,True
84055,scripted fasterRCNN model cannot be loaded with libtorch c++ API oncall: jit module: cpp,2022-08-25 13:15:21+00:00,,0,3,oncall: jit module: cpp,True
84052, Index out of bounds Error with PerChannel Quantization  oncall: quantization triaged,2022-08-25 12:14:54+00:00,,1,3,oncall: quantization triaged,True
84051,model.load_state_dict won't work in Child process if a sufficiently large tensor was padded in the Parent (even if empty padded) high priority module: multiprocessing module: memory usage triaged,2022-08-25 11:41:09+00:00,,1,7,high priority module: multiprocessing module: memory usage triaged,True
84050,I cannot install pytorch by Bad CRC-32 for file 'torch/lib/libtorch_cpu.so' module: build triaged,2022-08-25 11:16:00+00:00,,0,0,module: build triaged,True
84049,TorchScript unsupport tuple unpacking  as function inputs. oncall: jit,2022-08-25 10:45:55+00:00,,0,0,oncall: jit,False
84045,COREMLTOOLs/NNPACK Python Issue triaged module: nnpack,2022-08-25 09:06:40+00:00,,0,1,triaged module: nnpack,True
84044,"hipErrorNoBinaryForGpu, but reversed module: rocm triaged",2022-08-25 09:04:21+00:00,,0,1,module: rocm triaged,True
84039,[MPS] MPSNDArray error: product of dimension sizes > 2**31 triaged module: mps,2022-08-25 06:25:09+00:00,,1,31,triaged module: mps,True
93643,Large number of WONT CONVERTs on detectron2 model triaged oncall: pt2,2022-08-25 04:25:31+00:00,,0,2,triaged oncall: pt2,False
84014,"fill_ OpInfo code not used, also, doesn't test the case where the second argument is a Tensor module: tests triaged",2022-08-24 20:39:11+00:00,,0,0,module: tests triaged,True
84009,[Nested Tensor] Enable Nestedtensor to work with OpInfos triaged module: nestedtensor,2022-08-24 19:27:01+00:00,,0,0,triaged module: nestedtensor,True
84003,Linux cuda-11.x binary build  jobs intermittently take more than 4 hours high priority oncall: releng module: ci triaged,2022-08-24 18:09:19+00:00,,1,5,high priority oncall: releng module: ci triaged,True
83994,General NestedTensor op coverage tracking issue feature triaged module: nestedtensor,2022-08-24 16:57:07+00:00,,0,2,feature triaged module: nestedtensor,True
83986,PyTorch EC2 runners can not be used with standard actions module: ci triaged,2022-08-24 14:53:54+00:00,,0,5,module: ci triaged,True
83980,Scatter min/max reduce operation that returns the corresponding indices triaged enhancement module: scatter & gather ops,2022-08-24 14:08:00+00:00,,0,1,triaged enhancement module: scatter & gather ops,True
83979,Undefined reference in libtorch_cpu.so `...std::__cxx11::basic_string...` module: build triaged,2022-08-24 13:50:31+00:00,,0,0,module: build triaged,True
83977,pytorch 1.12.1 doesn't build with ffmpeg 5.0 module: build triaged,2022-08-24 13:18:50+00:00,,0,1,module: build triaged,True
83968,"Python3 Depletes 2021 M1 Mac Memory Running Training Ops For Model's M, L and X triaged module: macos",2022-08-24 08:02:50+00:00,,0,2,triaged module: macos,True
83956,[FSDP] Make sharded / unsharded check more robust oncall: distributed triaged module: fsdp,2022-08-24 01:31:25+00:00,,0,0,oncall: distributed triaged module: fsdp,True
83947,Are PyTorch Android nightly builds getting automatically published module: ci triaged module: android,2022-08-23 23:19:36+00:00,,0,0,module: ci triaged module: android,True
83941,empty_quantized should probably be new_empty_quantized oncall: quantization triaged,2022-08-23 21:42:14+00:00,,0,0,oncall: quantization triaged,True
83948,Add torch nightly builds pipeline for aarch64 linux module: ci triaged enhancement module: arm,2022-08-23 19:31:08+00:00,,0,5,module: ci triaged enhancement module: arm,True
83932,Hitting rate limits for pytorchbot token  triaged module: infra,2022-08-23 18:35:45+00:00,,0,1,triaged module: infra,True
83931,primTorch: support refs and decompositions when ATen and Python disagree triaged module: primTorch,2022-08-23 18:32:57+00:00,,0,1,triaged module: primTorch,True
83929, ModuleNotFoundError: No module named 'torch.ao.quantization.experimental' oncall: quantization low priority triaged,2022-08-23 18:27:58+00:00,,0,5,oncall: quantization low priority triaged,True
83923,Support primtorch view ops in functionalization triaged module: viewing and reshaping module: functionalization module: primTorch,2022-08-23 18:13:16+00:00,,1,9,triaged module: viewing and reshaping module: functionalization module: primTorch,True
83914,"RAM not free when deleting a model in CPU? worse after inference, is there some cache hidden? module: memory usage triaged",2022-08-23 15:31:14+00:00,,0,1,module: memory usage triaged,True
83910,Tracking nested tensor functions with backward kernels registered in derivatives.yaml triaged module: nestedtensor,2022-08-23 15:24:15+00:00,,0,0,triaged module: nestedtensor,True
83909,Grad strides do not match bucket view strides oncall: distributed triaged module: memory format module: ddp,2022-08-23 15:10:30+00:00,,0,4,oncall: distributed triaged module: memory format module: ddp,True
83902,"Bug in batch names with matmul (result tensor has names=('i', 'i', 'k')). triaged module: named tensor",2022-08-23 08:11:16+00:00,,0,3,triaged module: named tensor,True
83901,pytorch 1.12.1 Adam Optimizer Malfunction!!! needs reproduction module: optimizer triaged,2022-08-23 08:09:36+00:00,,0,1,needs reproduction module: optimizer triaged,False
83884,Improve FSDP error msg on wrong attr access oncall: distributed module: bootcamp triaged pt_distributed_rampup module: fsdp,2022-08-23 02:44:09+00:00,,0,0,oncall: distributed module: bootcamp triaged pt_distributed_rampup module: fsdp,True
83863,bfloat16 matmul gives incorrect result on CPU (without mkldnn) module: cpu triaged module: bfloat16 module: linear algebra,2022-08-22 18:41:27+00:00,,0,5,module: cpu triaged module: bfloat16 module: linear algebra,True
83854,Pytorch/Nova CI should monitor service outages for major dependencies module: ci triaged needs design,2022-08-22 16:40:03+00:00,,0,1,module: ci triaged needs design,True
83851,torch fx cannot trace assert for some cases triaged fx,2022-08-22 16:09:04+00:00,,0,2,triaged fx,True
83826,test_lazy spuriously fails if LAPACK is not installed module: tests triaged module: lazy,2022-08-21 21:55:35+00:00,,0,0,module: tests triaged module: lazy,True
83824,RuntimeError: Interrupted system call when doing distributed training oncall: distributed module: c10d,2022-08-21 20:49:17+00:00,,0,4,oncall: distributed module: c10d,False
93639,Explore TorchInductor optimization pass to reorder kernel bodies feature triaged oncall: pt2 module: inductor,2022-08-21 17:26:17+00:00,,0,0,feature triaged oncall: pt2 module: inductor,False
83818,torch.linalg.eigh crashe for matrices of size 2895×2895 or larger on eigen and M1 module: crash triaged module: linear algebra module: m1,2022-08-21 07:59:58+00:00,,0,8,module: crash triaged module: linear algebra module: m1,True
83817,[feature request] Add new device type works on CPU triaged enhancement,2022-08-21 07:03:42+00:00,,0,3,triaged enhancement,False
83800,torch.var_mean is slower than layer norm module: performance module: nn triaged needs research,2022-08-20 12:50:12+00:00,,0,3,module: performance module: nn triaged needs research,True
83795,Error on installation module: rocm triaged,2022-08-20 08:47:08+00:00,,0,6,module: rocm triaged,True
83775,[Nested Tensor] Move nested tensor specific ops to nested namespace triaged module: nestedtensor,2022-08-19 21:50:44+00:00,,2,1,triaged module: nestedtensor,True
93638,Inductor Error: aten.fill_.Tensor triaged oncall: pt2,2022-08-19 21:44:22+00:00,,0,1,triaged oncall: pt2,False
83773,[Nested Tensor] view + inplace for autograd.  module: autograd triaged module: nestedtensor,2022-08-19 21:42:56+00:00,,1,0,module: autograd triaged module: nestedtensor,True
83769,[TorchTidy] Check if `set_to_none` would change optimizer semantics. oncall: profiler,2022-08-19 20:37:36+00:00,,0,1,oncall: profiler,False
83764,Missing header file triaged,2022-08-19 19:55:37+00:00,,0,4,triaged,True
93635,tabulate.tabulate causes a lot of memory to be allocated in yolov3 triaged oncall: pt2 module: dynamo,2022-08-19 18:38:06+00:00,,1,2,triaged oncall: pt2 module: dynamo,False
83749,[Nested Tensor] Update TestCase.AssertEqual triaged module: nestedtensor module: testing,2022-08-19 18:10:16+00:00,,1,0,triaged module: nestedtensor module: testing,True
83737,Profiler reports different # of Calls depending on group_by_stack_n oncall: profiler,2022-08-19 15:36:53+00:00,,0,1,oncall: profiler,False
83733,BCELoss results in autocast CUDA warning triaged module: amp (automated mixed precision),2022-08-19 11:40:39+00:00,,0,8,triaged module: amp (automated mixed precision),True
83726,nvfuser + prim stack generated illegal PTX code on hardware with sm <= 70 triaged module: nvfuser module: primTorch,2022-08-19 06:08:33+00:00,,0,0,triaged module: nvfuser module: primTorch,True
83721,How to export a simple model using List.__contains__ to ONNX module: onnx triaged onnx-needs-info,2022-08-19 03:05:43+00:00,,0,4,module: onnx triaged onnx-needs-info,False
83714,Build from source failed on MacOS 10.6 with CUDA 10.1  module: build triaged module: macos,2022-08-19 00:36:15+00:00,,0,1,module: build triaged module: macos,True
83710,[Bug] Circular Import  caffe2 triaged,2022-08-18 23:55:39+00:00,,0,3,caffe2 triaged,True
83702,Inconsistency between index_select and __get_item__ triaged module: advanced indexing,2022-08-18 21:46:59+00:00,,0,1,triaged module: advanced indexing,True
83694,distributed tests take a long time oncall: distributed module: ci,2022-08-18 18:40:27+00:00,,0,6,oncall: distributed module: ci,False
93633,botorch dynamo errors triaged oncall: pt2 module: dynamo,2022-08-18 17:50:42+00:00,,0,1,triaged oncall: pt2 module: dynamo,False
83672,quantization: unexpected casting of tensor min and max to int in histogram observer oncall: quantization triaged,2022-08-18 15:18:55+00:00,,1,3,oncall: quantization triaged,True
93631,[inductor] Lower aten.cumsum triaged oncall: pt2 module: inductor,2022-08-18 15:11:31+00:00,,1,14,triaged oncall: pt2 module: inductor,False
83657,[Discussion] Add custom device triaged module: dispatch module: backend,2022-08-18 08:03:10+00:00,,0,4,triaged module: dispatch module: backend,True
83655,[feature request] PyTorch vmap for efficient Evolutionary Strategies feature triaged module: vmap module: functorch,2022-08-18 06:47:22+00:00,,0,1,feature triaged module: vmap module: functorch,False
83624,Unhelpful error message from torch.linalg.ldl_factor triaged module: linear algebra module: edge cases,2022-08-17 20:54:21+00:00,,0,1,triaged module: linear algebra module: edge cases,True
83606,"test_profiler_experimental_tree_cuda_detailed is too unstable, and as its CUDA only difficult to regen high priority triage review module: tests triaged oncall: profiler",2022-08-17 16:58:37+00:00,,0,1,high priority triage review module: tests triaged oncall: profiler,True
83589,Speedup for adding images to tensorboard oncall: visualization,2022-08-17 13:12:45+00:00,,0,0,oncall: visualization,False
83585,Segfault when profiling with_stack=True on model with jit.optimize_for_inference oncall: jit,2022-08-17 11:43:22+00:00,,0,0,oncall: jit,True
83584,Profiler can only print first 5 entries in stack traces because of hard-coded limit oncall: profiler,2022-08-17 11:30:51+00:00,,0,0,oncall: profiler,False
83583,Silent promotion of bool to int in the dispatcher triaged module: type promotion module: pybind module: library,2022-08-17 09:01:06+00:00,,0,0,triaged module: type promotion module: pybind module: library,True
83579,Conv1d: NNPACK SpatialConvolution_updateOutput failed when batchsize or padding is too large module: nn module: convolution triaged module: nnpack,2022-08-17 06:53:13+00:00,,0,2,module: nn module: convolution triaged module: nnpack,True
83577,libtorch malloc cause coredump   module: crash module: cpp triaged module: static linking,2022-08-17 05:10:28+00:00,,0,1,module: crash module: cpp triaged module: static linking,True
83564,KL-divergence of two Generalized Dirichlet distributions module: distributions feature triaged,2022-08-17 00:42:53+00:00,,0,0,module: distributions feature triaged,True
83551,OpenJDK libtorch_cpu.so stack guard warning oncall: java,2022-08-16 22:12:57+00:00,,0,0,oncall: java,False
93627,PyTorch test suite regression test_module_backward_global_hook_writeable triaged oncall: pt2 module: dynamo,2022-08-16 21:29:27+00:00,,1,6,triaged oncall: pt2 module: dynamo,False
83540,I have the same issue as @samgelman on my MacOS. triaged module: macos module: openmp module: third_party,2022-08-16 20:40:17+00:00,,1,3,triaged module: macos module: openmp module: third_party,True
83537,Add a new argument `check_inf=True` (by default) or check_pos_inf / check_neg_inf to anomaly mode module: autograd triaged enhancement,2022-08-16 20:04:02+00:00,,0,4,module: autograd triaged enhancement,True
83529,Adding Levenberg-marquardt optimizer in PyTorch feature module: optimizer triaged needs research,2022-08-16 18:29:30+00:00,,0,2,feature module: optimizer triaged needs research,False
83521,quantize_per_tensor/quantize_per_channel operators should honor the quant_min/quant_max from observer oncall: quantization triaged,2022-08-16 16:44:07+00:00,,0,0,oncall: quantization triaged,True
83510,Cdist backward dependent on compute_mode module: autograd triaged actionable,2022-08-16 12:11:59+00:00,,0,3,module: autograd triaged actionable,True
83509, Build and Run QNNPACK on X86 module: build triaged,2022-08-16 11:16:10+00:00,,0,8,module: build triaged,True
83507,"[Installation] conda installation hangs on ""Solving environment"" module: binaries triaged",2022-08-16 10:16:41+00:00,,1,5,module: binaries triaged,True
83494,`torch.pinverse` produces wrong output! module: docs triaged module: linear algebra,2022-08-16 01:57:34+00:00,,0,2,module: docs triaged module: linear algebra,True
83493,Calling torch.linalg.cholesky on a CPU tensor requires compiling PyTorch with LAPACK. triaged module: linear algebra,2022-08-16 01:39:12+00:00,,0,3,triaged module: linear algebra,True
83492,`Frozen` module for transfer learning. module: nn triaged needs design,2022-08-16 01:04:56+00:00,,1,1,module: nn triaged needs design,True
83441,Find way to add comments to merge_rules json module: ci triaged,2022-08-15 18:42:25+00:00,,1,4,module: ci triaged,True
83394,[ONNX] Convert GFPGANv1.3.pth to onnx module: onnx triaged,2022-08-15 00:22:12+00:00,,0,5,module: onnx triaged,False
83393,Test public bindings in CI gives weird output on error high priority module: ci module: tests triaged module: python frontend,2022-08-14 13:36:56+00:00,,0,5,high priority module: ci module: tests triaged module: python frontend,True
83392,"How to turn off determinism just for specific operations, e.g. upsampling through bilinear interpolation? module: cuda triaged module: determinism",2022-08-14 12:15:32+00:00,,0,0,module: cuda triaged module: determinism,True
83388,"zero-numel tensor has ""RuntimeError: strides[cur - 1] == sizes[cur] * strides[cur] INTERNAL ASSERT FAILED"" in multi-thread. oncall: jit triaged module: nvfuser",2022-08-14 05:29:09+00:00,,1,2,oncall: jit triaged module: nvfuser,True
83383,PyTorch profiler is spammy oncall: profiler,2022-08-14 01:40:05+00:00,,0,5,oncall: profiler,False
83381,`test_profiler_experimental_tree_cuda_detailed` fails with mismatches in the profile output oncall: profiler,2022-08-13 23:59:52+00:00,,0,2,oncall: profiler,False
83379,[caffee2] Windows build / 'metanet_pb2' (a circular import)  Anaconda caffe2,2022-08-13 19:53:32+00:00,,0,0,caffe2,False
83376,Complex-Valued Gaussian distributions module: distributions triaged module: complex module: random,2022-08-13 14:32:40+00:00,,0,19,module: distributions triaged module: complex module: random,True
93624,"KeyError `shape,stack,cos` on pennylane quantum circuit triaged oncall: pt2 module: dynamo",2022-08-13 00:06:56+00:00,,1,2,triaged oncall: pt2 module: dynamo,False
83351,DDP + FSDP: Investigate behavior for nn.Module APIs high priority triage review oncall: distributed triaged better-engineering module: ddp module: fsdp,2022-08-12 20:50:41+00:00,,0,0,high priority triage review oncall: distributed triaged better-engineering module: ddp module: fsdp,True
83349,checkpoint function is not jit compatible oncall: jit,2022-08-12 20:25:13+00:00,,0,0,oncall: jit,False
83320, Torch1.10.2 is slower than torch1.9.1 oncall: distributed triaged module: ddp,2022-08-12 07:55:24+00:00,,0,7,oncall: distributed triaged module: ddp,True
83315,dataparallel function doesn't work triaged module: data parallel,2022-08-12 04:01:17+00:00,,0,1,triaged module: data parallel,True
83313,torch.Tag doesn't have accurate mypy info module: typing triaged,2022-08-12 03:24:33+00:00,,0,3,module: typing triaged,True
93620,Long test time for PyTorch test_fx::TestVisionTracing with dynamo enabled triaged oncall: pt2,2022-08-11 22:35:24+00:00,,0,5,triaged oncall: pt2,False
83292,TorchVision testing in CI + test_fx triaged module: fx fx,2022-08-11 20:18:25+00:00,,0,1,triaged module: fx fx,True
83289,Improvements to ProcessGroupGloo monitored_barrier high priority triage review oncall: distributed triaged module: c10d,2022-08-11 19:29:21+00:00,,0,1,high priority triage review oncall: distributed triaged module: c10d,True
83281,addcdiv_ (in and out of place) not implemented for torch.float16 and cpu low priority triaged enhancement module: half,2022-08-11 18:29:11+00:00,,0,2,low priority triaged enhancement module: half,True
83272,bmm operator in bfloat16 has low TFLOPS for some tensor shapes with CUDA 11.6 triaged module: cublas,2022-08-11 17:38:12+00:00,,0,9,triaged module: cublas,True
83271,cannot import name 'ProcessGroup' from 'torch.distributed'  oncall: distributed triaged module: macos,2022-08-11 17:28:32+00:00,,0,2,oncall: distributed triaged module: macos,True
83264,Emulating FP64 and increased precisions on Apple silicon feature triaged needs research module: mps,2022-08-11 15:31:00+00:00,,0,3,feature triaged needs research module: mps,True
83257,PyYAML not listed as a dependency oncall: releng triaged,2022-08-11 13:37:48+00:00,,0,1,oncall: releng triaged,True
83250,Using Pytorch and Mapbox in the same project oncall: mobile,2022-08-11 10:25:23+00:00,,0,1,oncall: mobile,False
83245,"During DDP training timm densenet121, mobilenetv2(v3) models do not save state_dict correctly. oncall: distributed triaged module: ddp",2022-08-11 09:30:03+00:00,,0,3,oncall: distributed triaged module: ddp,True
83244,torch.nn.Upsample's error message is inconsistent with the documentation module: docs module: nn triaged actionable,2022-08-11 09:25:02+00:00,,0,4,module: docs module: nn triaged actionable,True
83243,RPC: wait method of Future object return 0 sometimes in rpc framework high priority oncall: distributed triaged module: rpc,2022-08-11 09:23:13+00:00,,0,3,high priority oncall: distributed triaged module: rpc,True
83241,torch.nn.TripletMarginLoss margin can be less than 0 module: nn triaged,2022-08-11 08:34:41+00:00,,0,0,module: nn triaged,True
83238,The type of parameter 'p' in torch.nn.TripletMarginLoss wrong module: nn triaged,2022-08-11 08:12:49+00:00,,0,0,module: nn triaged,True
83234,torch.nn.ReplicationPad{1|2}d supports more input dimension than are written on documentation module: docs module: nn triaged,2022-08-11 07:19:20+00:00,,0,1,module: docs module: nn triaged,True
83232,torch.nn.PixelShuffle error message wrong module: nn triaged,2022-08-11 06:13:25+00:00,,0,0,module: nn triaged,True
83229,torch.nn.MaxUnpool2d get negative size tensor module: nn triaged,2022-08-11 03:55:32+00:00,,0,0,module: nn triaged,True
83221,torch.nn.InstanceNorm{1|2|3}d doesn't verify the value type of parameter num_features module: nn triaged,2022-08-11 00:16:21+00:00,,0,2,module: nn triaged,True
83214,torchgen.model.FunctionSchema.parse fails with following ops' schema  triaged module: codegen,2022-08-10 22:53:03+00:00,,0,2,triaged module: codegen,True
83204,Enable freezing parts of the model in Fully Sharded Data Parallel oncall: distributed triaged module: fsdp,2022-08-10 21:51:29+00:00,,1,13,oncall: distributed triaged module: fsdp,True
83197,Check support of FSDP + set_materialize_grads(False) oncall: distributed triaged module: fsdp,2022-08-10 20:00:40+00:00,,0,0,oncall: distributed triaged module: fsdp,True
83193,"module 'torch.distributed' has no attribute 'pipeline' - macOS, PyTorch 1.12.1 oncall: distributed triaged pipeline parallelism release notes: distributed (pipeline)",2022-08-10 19:25:16+00:00,,0,1,oncall: distributed triaged pipeline parallelism release notes: distributed (pipeline),True
83175,"torch.nn.GRU runs long time, when num_layers is large module: nn triaged module: edge cases",2022-08-10 14:28:15+00:00,,0,0,module: nn triaged module: edge cases,True
83169,torch.nn.functional.softplus / torch.nn.Softplus parameter beta can be set to zero module: nn triaged,2022-08-10 13:06:35+00:00,,0,0,module: nn triaged,True
83168,deepcopy of LazyLinear fails module: nn triaged actionable,2022-08-10 12:44:12+00:00,,0,1,module: nn triaged actionable,True
83163,torch.nn.functional.log_softmax  parameter '_stacklevel' undocumented module: nn triaged actionable,2022-08-10 09:07:35+00:00,,0,7,module: nn triaged actionable,True
83161,Optimize for mobile metal model oncall: mobile,2022-08-10 08:43:18+00:00,,0,0,oncall: mobile,False
83159,Expand Learning rate scheduling to any optimization hyperparameter feature module: optimizer triaged needs design module: LrScheduler,2022-08-10 08:16:24+00:00,,0,0,feature module: optimizer triaged needs design module: LrScheduler,False
83157,Fail to install torch for source module: build triaged,2022-08-10 08:08:34+00:00,,0,0,module: build triaged,True
83153,torch.nn.Hardtanh allows min_val > max_val module: nn triaged,2022-08-10 06:44:04+00:00,,0,1,module: nn triaged,True
83152,"When padding is big int, torch.nn.functional.fold runs too long and can't return result module: nn triaged",2022-08-10 06:11:57+00:00,,0,0,module: nn triaged,True
83151,Make FSDP easier to debug when erroring in backward pass high priority triage review oncall: distributed triaged module: fsdp,2022-08-10 06:02:33+00:00,,0,2,high priority triage review oncall: distributed triaged module: fsdp,True
83149,bf16 strided tensor wrong calculation high priority triaged module: bfloat16 module: correctness (silent) module: reductions module: intel,2022-08-10 05:44:04+00:00,,0,11,high priority triaged module: bfloat16 module: correctness (silent) module: reductions module: intel,True
83148,Cannot call CUDAGeneratorImpl::current_seed during CUDA graph capture module: cuda triaged,2022-08-10 05:22:28+00:00,,0,2,module: cuda triaged,True
83144,[MPS] Bug on training CNN+LSTM triaged module: mps,2022-08-10 05:09:28+00:00,,0,10,triaged module: mps,True
83143,Bug in building pytorch deploy from source in macos USE_DEPLOY=1  oncall: package/deploy imported,2022-08-10 04:53:39+00:00,,0,0,oncall: package/deploy imported,True
83135,torch.nn.functional.avg_pool{1|2|3}d error message does not match what is described in the documentation module: docs module: nn triaged,2022-08-10 01:11:59+00:00,,0,0,module: docs module: nn triaged,True
83112,One dlpack to rule them all triaged better-engineering module: dlpack,2022-08-09 21:16:04+00:00,,1,0,triaged better-engineering module: dlpack,True
83111,[FSDP] `test_summon_single_param()` is misleading triaged module: fsdp,2022-08-09 21:15:30+00:00,,1,1,triaged module: fsdp,True
83098,Redirect the old metrics.pytorch.org url to the new page module: ci triaged,2022-08-09 19:17:16+00:00,,0,0,module: ci triaged,True
83082,[CI] Create periodic fuzzy testing for PyTorch build flags module: ci triaged,2022-08-09 16:22:59+00:00,,0,0,module: ci triaged,True
83081,[CI] Split up periodic.yml into forward-fixable.yml and periodic.yml module: ci triaged,2022-08-09 16:19:38+00:00,,0,0,module: ci triaged,True
83074,DPP training incompatibility with checkpoint and detach oncall: distributed triaged module: ddp,2022-08-09 15:15:27+00:00,,0,2,oncall: distributed triaged module: ddp,True
83070,make_fx + aot_autograd segfaults module: crash triaged module: fx fx module: functorch,2022-08-09 13:54:01+00:00,,0,1,module: crash triaged module: fx fx module: functorch,True
83064,Updating the LTS version of the torch (1.8.2 -> 1.10.2\\1.11.2?) module: binaries triaged,2022-08-09 11:05:54+00:00,,0,1,module: binaries triaged,True
83060,torch.empty_strided argument 'size'and 'stride' documentation wrong module: docs triaged,2022-08-09 08:35:54+00:00,,0,0,module: docs triaged,True
83052,FSDP init can crash with shared parameters high priority triage review oncall: distributed triaged module: fsdp,2022-08-09 03:24:31+00:00,,0,2,high priority triage review oncall: distributed triaged module: fsdp,True
83045,[JIT] Scripting modules fails for modules that contain nested NamedTuples oncall: jit,2022-08-09 01:10:03+00:00,,0,2,oncall: jit,True
83032,Support for CSR Tensor with NN layers module: sparse module: nn triaged,2022-08-08 22:18:23+00:00,,0,5,module: sparse module: nn triaged,True
83019,TestCommon.test_dtypes error message is confusing triaged module: testing,2022-08-08 20:13:11+00:00,,0,0,triaged module: testing,True
83015,Incorrect tensor conversion to m1 MPS. triaged module: mps,2022-08-08 19:58:59+00:00,,0,12,triaged module: mps,True
82960,torch.bitwise_xor argument 'other' documentation wrong module: docs triaged,2022-08-08 08:10:13+00:00,,0,0,module: docs triaged,True
82951,torch.profiler's FLOPs measure only counts operations involving '+' and '*' . oncall: profiler,2022-08-08 04:02:48+00:00,,0,0,oncall: profiler,False
82926,"Slice operation on ""ragged"" dimension in NestedTensor triaged enhancement module: nestedtensor",2022-08-06 04:56:02+00:00,,0,1,triaged enhancement module: nestedtensor,True
82919,Adding a warning of non-compatibility with forward hooks for the fast path of TransformerEncoderLayer triaged oncall: transformer/mha,2022-08-05 23:01:59+00:00,,1,0,triaged oncall: transformer/mha,True
82915,DISABLED test_tensorboard_trace_handler (__main__.TestProfiler) module: flaky-tests skipped oncall: profiler,2022-08-05 21:40:33+00:00,,0,14,module: flaky-tests skipped oncall: profiler,False
82902,functorch slow tests not being run in slow CI module: ci module: tests triaged module: functorch,2022-08-05 19:10:24+00:00,,0,0,module: ci module: tests triaged module: functorch,True
82894,linalg and lu tests fail when run in parallel on linux cuda high priority module: cuda module: ci triaged module: linear algebra,2022-08-05 17:56:30+00:00,,0,14,high priority module: cuda module: ci triaged module: linear algebra,True
82886,CUDA graph capturing fails for nn.Embedding and large batch sizes module: cuda triaged module: embedding module: cuda graphs,2022-08-05 16:02:52+00:00,,0,6,module: cuda triaged module: embedding module: cuda graphs,True
82879,`torch.tensor` and `torch.as_tensor` keyword argument `device` documentation wrong module: docs triaged module: tensor creation,2022-08-05 08:14:05+00:00,,0,0,module: docs triaged module: tensor creation,True
82872,Unknown builtin op: torchvision::deform_conv2d oncall: jit,2022-08-05 06:10:04+00:00,,0,3,oncall: jit,False
82871,GPU arch 8.6 is not covered by the `TORCH_CUDA_ARCH_LIST = All` option  module: build module: cuda triaged,2022-08-05 05:28:59+00:00,,1,1,module: build module: cuda triaged,True
82843,Tensor operation hangs when used with multiprocessing module: multiprocessing triaged module: determinism shadow review,2022-08-04 20:32:09+00:00,,0,5,module: multiprocessing triaged module: determinism shadow review,True
82831,Error building Pytorch 13.1 from Source on OS X 12.5 module: build module: protobuf triaged,2022-08-04 17:58:34+00:00,,0,5,module: build module: protobuf triaged,True
82823,getDLContext in DLConvertor.h cannot be found triaged module: dlpack,2022-08-04 16:34:45+00:00,,0,3,triaged module: dlpack,True
82813,functionalize and make_fx are not composable resulting in segfault and cuda error module: crash triaged module: fx fx module: functorch,2022-08-04 12:06:12+00:00,,0,4,module: crash triaged module: fx fx module: functorch,True
82802,"[ROCm] build instruction is haphazard missing information unclear, build does not work module: docs module: rocm triaged",2022-08-04 06:15:18+00:00,,1,5,module: docs module: rocm triaged,True
82793,Profiling results on CPU is not reliable module: performance triaged,2022-08-04 03:04:34+00:00,,0,6,module: performance triaged,True
82789,[LibTorch] the C++ api needs detailed error reports like pytorch  module: logging triaged enhancement,2022-08-04 01:38:27+00:00,,0,0,module: logging triaged enhancement,True
82785,UnaryUfuncInfo Sample Generation Ignores sample_kwarg function high priority triaged module: correctness (silent) module: testing,2022-08-04 00:23:05+00:00,,1,1,high priority triaged module: correctness (silent) module: testing,True
82764,Subclass of Tensor doesn't support __format__ triaged tensor subclass,2022-08-03 22:39:05+00:00,,0,0,triaged tensor subclass,True
82762,Fill in a bool Tensor not supported in jit oncall: jit,2022-08-03 22:33:54+00:00,,0,0,oncall: jit,False
82761,torch.Tensor.bag() should automatically implement bagging triaged enhancement,2022-08-03 22:33:18+00:00,,0,0,triaged enhancement,True
82756,Met bugs ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 0 oncall: distributed oncall: r2p,2022-08-03 21:11:34+00:00,,0,0,oncall: distributed oncall: r2p,False
82751,Refactor how errors decide whether to append C++ stacktrace triaged better-engineering,2022-08-03 20:28:56+00:00,,0,0,triaged better-engineering,True
82727,DecompositionInterpreter creates invalid graphs for FX graph modules created with torch.fx.symbolic_trace triaged module: fx fx,2022-08-03 16:00:26+00:00,,0,0,triaged module: fx fx,True
93797,torchdynamo backend failure suppression is insufficient when backend fails at runtime triaged oncall: pt2 module: dynamo,2022-08-03 15:31:20+00:00,,0,8,triaged oncall: pt2 module: dynamo,True
82725,"Automating release process - Binary validation, Automatically generating get started page module: ci triaged",2022-08-03 15:00:23+00:00,,0,4,module: ci triaged,True
82724,cur_dim == dimINTERNAL ASSERT FAILED at module: onnx triaged onnx-triaged,2022-08-03 13:50:25+00:00,,0,3,module: onnx triaged onnx-triaged,True
82718,"tensor.unfold don't check the parameter size value, that maybe less than 0.  module: error checking triaged module: edge cases",2022-08-03 12:29:34+00:00,,0,0,module: error checking triaged module: edge cases,True
82712,Tensorboard py-profiler shows no device info in Operator view oncall: profiler,2022-08-03 11:30:45+00:00,,0,1,oncall: profiler,False
82710,build fail when using lto with gcc module: build triaged,2022-08-03 10:30:02+00:00,,0,0,module: build triaged,True
82687,Move nested-tensor tutorial from prototype triaged module: nestedtensor,2022-08-02 23:32:26+00:00,,1,1,triaged module: nestedtensor,True
82684,SequentialLR does not work correctly with multiple ConstantLR triaged module: LrScheduler,2022-08-02 22:43:24+00:00,,0,0,triaged module: LrScheduler,True
82677,RReLU doc doesn't specify the eval mode behaving just like LeakyReLU module: docs module: nn triaged actionable topic: docs,2022-08-02 21:44:16+00:00,,1,1,module: docs module: nn triaged actionable topic: docs,False
82669,unittest.subTest and way to selectively mark subTests as expected failures triaged better-engineering module: testing,2022-08-02 20:34:16+00:00,,0,8,triaged better-engineering module: testing,True
82668,Schema information for torch.* operations triaged module: __torch_function__ module: testing,2022-08-02 20:27:29+00:00,,0,2,triaged module: __torch_function__ module: testing,True
82660,in-place variants should get their own OpInfos triaged better-engineering module: testing,2022-08-02 18:35:30+00:00,,0,6,triaged better-engineering module: testing,True
82635,[Torchscript] torch.min returns wrong gradient when inputs are equal oncall: jit,2022-08-02 05:16:53+00:00,,0,0,oncall: jit,False
82634,[Torchscript] some activations backward are not fused when used with linear oncall: jit,2022-08-02 05:11:21+00:00,,0,0,oncall: jit,False
82627,PyTorch crashes when running with OpenACC module: crash triaged module: openmp module: third_party,2022-08-01 23:35:50+00:00,,0,6,module: crash triaged module: openmp module: third_party,True
82616,FakeTensor Support For Pickling triaged module: fakeTensor,2022-08-01 22:35:43+00:00,,0,2,triaged module: fakeTensor,True
82610,contiguous() not work for rank 1 length 1 tensor. triaged module: dlpack,2022-08-01 21:34:03+00:00,,0,10,triaged module: dlpack,True
82598,Deep copy models with `create_feature_extractor` produces different parameters triage review triaged module: vision oncall: fx,2022-08-01 20:29:17+00:00,,0,6,triage review triaged module: vision oncall: fx,True
82583,DataLoader parameter pin_memory_device should accept torch.device type module: dataloader triaged,2022-08-01 17:20:42+00:00,,0,1,module: dataloader triaged,True
82577,RFC: Add flag for RNN decomposition to all RNN modules feature module: rnn triaged,2022-08-01 16:31:07+00:00,,0,4,feature module: rnn triaged,True
82565,PyTorch for quantum mechanics feature triaged function request module: scientific computing,2022-08-01 12:40:41+00:00,,0,4,feature triaged function request module: scientific computing,False
82550,`torch.cat` can break `torch.jit.ScriptModule` when in inference mode oncall: jit,2022-07-31 16:35:31+00:00,,0,5,oncall: jit,False
82547,make_fx is broken for all tracing modes high priority module: crash triaged module: fx fx,2022-07-31 10:48:28+00:00,,0,6,high priority module: crash triaged module: fx fx,True
82546,Libtorch C++ torch::stack error needs reproduction module: cpp triaged,2022-07-31 09:09:05+00:00,,0,1,needs reproduction module: cpp triaged,True
82545,Incorrect CPU implementation of CTCLoss backward step module: autograd module: loss triaged,2022-07-31 08:56:10+00:00,,0,8,module: autograd module: loss triaged,True
82542,Is there Doc that explains how to call an extension op in another extension implementation? module: docs module: cpp triaged,2022-07-31 06:20:02+00:00,,0,3,module: docs module: cpp triaged,False
82534,Use NestedTensor in RNN models triaged enhancement module: nestedtensor,2022-07-30 18:26:58+00:00,,0,7,triaged enhancement module: nestedtensor,True
82532,[ONNX] Memory leak when exporting a jit model to onnx needs reproduction oncall: jit module: onnx,2022-07-30 17:30:25+00:00,,0,2,needs reproduction oncall: jit module: onnx,False
82518,Split up `common_methods_invocations.py`? triaged needs research better-engineering module: testing,2022-07-30 01:28:23+00:00,,1,12,triaged needs research better-engineering module: testing,True
82517,Symbolic tensors are not printable module: printing triaged module: dynamic shapes,2022-07-30 01:18:34+00:00,,0,1,module: printing triaged module: dynamic shapes,True
82510,Complex addition result in NaN when it shouldn't triaged module: complex module: NaNs and Infs,2022-07-29 23:43:46+00:00,,0,3,triaged module: complex module: NaNs and Infs,True
82494,Implement torch.clamp() on sparse tensors with SparseCPU backend module: sparse triaged,2022-07-29 20:04:40+00:00,,0,4,module: sparse triaged,True
82479,Cloning conjugate tensor in torch_dispatch context produces non equality. triaged module: complex module: __torch_dispatch__,2022-07-29 15:48:43+00:00,,0,2,triaged module: complex module: __torch_dispatch__,True
93793,Guide for diagnosing excess graph breaks module: docs triaged oncall: pt2 module: dynamo,2022-07-29 15:19:28+00:00,,0,3,module: docs triaged oncall: pt2 module: dynamo,False
82465,Does torch.utils.checkpoint compatible with torch.cuda.make_graphed_callables? module: checkpoint triaged module: cuda graphs,2022-07-29 10:59:19+00:00,,0,7,module: checkpoint triaged module: cuda graphs,True
82464,SyncBatchNorm does not work on CPU oncall: distributed module: nn,2022-07-29 10:48:16+00:00,,0,5,oncall: distributed module: nn,False
82451,add support for bitwise operations with floating point numbers oncall: jit,2022-07-29 03:35:58+00:00,,0,0,oncall: jit,False
82443,Quantization issue in transformers oncall: quantization low priority triaged,2022-07-28 23:46:27+00:00,,2,18,oncall: quantization low priority triaged,True
82430,Minor inconsistency in description of `attn_output_weights` in MultiheadAttention docs module: docs module: nn triaged actionable,2022-07-28 20:59:42+00:00,,0,1,module: docs module: nn triaged actionable,True
82419,The torch::deploy document is not updated triaged module: deploy,2022-07-28 18:15:44+00:00,,0,0,triaged module: deploy,True
82417,[JIT] _unsafe_view returns alias when size(input) = size argument oncall: jit,2022-07-28 17:55:28+00:00,,0,0,oncall: jit,True
82397,Bilinear interpolation with antialiasing is slow in performance module: performance triaged,2022-07-28 05:36:03+00:00,,0,4,module: performance triaged,True
82382,Problems in built-from-source pytorch with USE_DEPLOY=1 in Ubuntu triaged module: deploy,2022-07-28 00:07:25+00:00,,0,0,triaged module: deploy,True
82377,masked_scatter_ is very lacking module: docs triaged module: scatter & gather ops,2022-07-27 23:19:37+00:00,,0,1,module: docs triaged module: scatter & gather ops,True
82357,ufmt and flake8 lints race triaged,2022-07-27 19:52:00+00:00,,0,3,triaged,True
82354,Offer a way to really force merges via pytorchbot module: ci triaged,2022-07-27 19:31:28+00:00,,0,1,module: ci triaged,True
82340,DISABLED test_op_has_batch_rule_nn_functional_conv_transpose3d_cuda_float32 (__main__.TestVmapOperatorsOpInfoCUDA) triaged module: flaky-tests skipped module: vmap module: functorch,2022-07-27 18:42:52+00:00,,0,5,triaged module: flaky-tests skipped module: vmap module: functorch,False
82324,[JIT] SchemaInfo warning appears out in the wild oncall: jit,2022-07-27 15:47:44+00:00,,0,0,oncall: jit,True
82318,test_make_fx_symbolic_exhaustive should pass dynamic ints for shape arguments triaged fx module: dynamic shapes,2022-07-27 14:57:15+00:00,,0,2,triaged fx module: dynamic shapes,True
82316,Add more Vulkan operations triaged module: vulkan ciflow/periodic,2022-07-27 14:26:22+00:00,,0,1,triaged module: vulkan ciflow/periodic,True
82312,"A/libc: Fatal signal 6 (SIGABRT), code -1 (SI_QUEUE) in tid 9792 (Background), pid 9674 (ample.testtorch) oncall: mobile",2022-07-27 13:21:52+00:00,,0,15,oncall: mobile,False
82308,torch.einsum gets wrong results randomly when training with multi-gpu oncall: distributed,2022-07-27 12:52:17+00:00,,0,1,oncall: distributed,False
82306,when distribute training  load pretrain model error oncall: distributed module: serialization,2022-07-27 11:50:22+00:00,,0,1,oncall: distributed module: serialization,True
82303,Race condition between torch.tensor's view and /= (/= returns incorrect result) triaged module: partial aliasing,2022-07-27 09:19:22+00:00,,0,3,triaged module: partial aliasing,True
82293,pytorch's checkpoint_wrapper does not save memory while fairscale's checkpoint_wrapper saves huge memory high priority oncall: distributed module: checkpoint,2022-07-27 03:54:20+00:00,,1,4,high priority oncall: distributed module: checkpoint,True
82282,`torch.matrix_exp` doesn't handle NaN properly module: cuda triaged module: NaNs and Infs module: linear algebra,2022-07-27 00:28:15+00:00,,1,3,module: cuda triaged module: NaNs and Infs module: linear algebra,True
82276,DEBUG=1 env var doesn't actually set DEBUG preprocessor macro module: build triaged enhancement,2022-07-27 00:02:19+00:00,,0,2,module: build triaged enhancement,True
82259,[Reproducibility] Make tests say when unusual environment variables are set that change behavior of the test module: ci triaged,2022-07-26 21:07:56+00:00,,1,4,module: ci triaged,True
82242,logspace inconsistently casts inputs to int before performing computation triaged module: tensor creation,2022-07-26 19:51:18+00:00,,0,0,triaged module: tensor creation,True
82239,primtorch refs should be composite compliant triaged module: __torch_dispatch__ tensor subclass module: primTorch,2022-07-26 19:35:33+00:00,,0,0,triaged module: __torch_dispatch__ tensor subclass module: primTorch,True
82230,logspace and linspace off by one on cuda for integer dtypes for some inputs triaged module: tensor creation,2022-07-26 17:24:11+00:00,,0,1,triaged module: tensor creation,True
82229,[Profiler] Allow profiler to gracefully fail without interrupting workflow. oncall: profiler,2022-07-26 17:16:21+00:00,,1,0,oncall: profiler,False
82228,[Profiler] Allow profiler to gracefully fail without interrupting workflow. oncall: profiler,2022-07-26 17:16:08+00:00,,0,0,oncall: profiler,False
93791,Reordering test in PyTorch test suite induces dynamo failure triaged bug oncall: pt2 module: dynamo,2022-07-26 14:56:54+00:00,,0,2,triaged bug oncall: pt2 module: dynamo,True
82219,[feature request] DataLoader to accept num_threads argument to auto-set number of threads for OpenMP / intra-op parallelism module: dataloader triaged module: openmp,2022-07-26 13:47:14+00:00,,0,1,module: dataloader triaged module: openmp,True
82218,OOM during backward() leads to memory leaks needs reproduction module: autograd module: memory usage triaged,2022-07-26 13:30:30+00:00,,0,12,needs reproduction module: autograd module: memory usage triaged,True
82217,backward not available for index and mask  needs reproduction module: autograd triaged module: sorting and selection,2022-07-26 12:31:21+00:00,,0,2,needs reproduction module: autograd triaged module: sorting and selection,True
82216,iOS TestApp from mobile performance recipes tutorial doesn't build on macOS oncall: mobile module: ios,2022-07-26 12:28:02+00:00,,0,0,oncall: mobile module: ios,False
82212,"RuntimeError: ""reflection_pad2d"" not implemented for 'Half' in autocast enabled region triaged module: amp (automated mixed precision)",2022-07-26 09:16:42+00:00,,0,0,triaged module: amp (automated mixed precision),True
82200,"model.to(device) takes time forever on A40-8Q, NVIDIA. cuda11.1, torch1.9.1. module: cuda triaged",2022-07-26 02:51:02+00:00,,0,1,module: cuda triaged,True
82197,Provide error handling for ops that don't yet support Dynamic Shape triaged lazy,2022-07-26 01:42:50+00:00,,1,3,triaged lazy,True
82185,DataLoader: `pin_memory` should respect object attributes before object collection type module: dataloader triaged,2022-07-25 23:52:07+00:00,,0,1,module: dataloader triaged,True
82159,`torch.sum` promotes integral tensors to `int64`. module: docs triaged module: type promotion actionable module: reductions,2022-07-25 20:30:42+00:00,,0,3,module: docs triaged module: type promotion actionable module: reductions,True
82156,[Checkpoint] Support multiple unpack in saved tensor hooks module: checkpoint triaged,2022-07-25 20:01:54+00:00,,0,0,module: checkpoint triaged,True
82153,DistributedDataParallel hangs when not using GPU 0 oncall: distributed module: ddp,2022-07-25 19:45:11+00:00,,0,7,oncall: distributed module: ddp,True
82145,set_grad_enabled not respected when running on a web server module: dependency bug module: autograd triaged actionable,2022-07-25 19:11:55+00:00,,0,4,module: dependency bug module: autograd triaged actionable,True
82140,Stop manually binding sparse factory functions module: sparse triaged,2022-07-25 18:40:44+00:00,,0,0,module: sparse triaged,True
82139,Re-enable DynamicQuantModule in iOS simulator tests module: ci triaged module: ios,2022-07-25 18:32:25+00:00,,1,2,module: ci triaged module: ios,True
82132,External libraries cannot have a requirements.txt that needs to install a cpp_extension module: cpp-extensions triaged,2022-07-25 17:06:31+00:00,,0,0,module: cpp-extensions triaged,True
82109,Move functorch tests to under test/ module: tests triaged,2022-07-25 14:26:54+00:00,,0,0,module: tests triaged,True
82099,UserWarning: operator() sees varying value in profiling oncall: jit,2022-07-25 11:01:00+00:00,,0,2,oncall: jit,False
82098,[feature request] Discover actually loaded shared libraries at runtime module: build triaged enhancement,2022-07-25 10:06:18+00:00,,0,4,module: build triaged enhancement,True
82095,torch.concat type hints fail for keyword argument module: typing triaged,2022-07-25 09:25:40+00:00,,0,1,module: typing triaged,True
82093,"When using libtorch v1.10.2, calling at::slow_conv_dilated3d directly returns wrong results on cpu backend module: cpp module: convolution triaged",2022-07-25 09:12:07+00:00,,0,0,module: cpp module: convolution triaged,True
82091,"RuntimeError: [1] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Timeout waiting for key: default_pg/0/0 after 1800000 ms  oncall: distributed triaged module: nccl module: c10d",2022-07-25 06:54:21+00:00,,0,8,oncall: distributed triaged module: nccl module: c10d,True
82088,linear.matrix_power is not composite compliant triaged module: linear algebra,2022-07-25 04:09:06+00:00,,0,3,triaged module: linear algebra,True
82084,Untangle TorchScript prim ops in aten namespace oncall: jit,2022-07-25 03:30:26+00:00,,0,0,oncall: jit,False
82081,Could be clearer that Cross Entropy takes logits as input module: docs module: nn module: loss triaged actionable,2022-07-25 01:43:20+00:00,,0,1,module: docs module: nn module: loss triaged actionable,True
82077,Using DDP with num_workers > 0 hangs before entering the first training epoch loop oncall: distributed module: dataloader,2022-07-24 22:26:24+00:00,,0,9,oncall: distributed module: dataloader,True
82076,Autocast documentation examples would break module: docs triaged module: amp (automated mixed precision),2022-07-24 20:34:25+00:00,,0,2,module: docs triaged module: amp (automated mixed precision),True
82073,CUDACachingAllocator should be cuda memory merge/compact friendly module: cuda triaged module: CUDACachingAllocator,2022-07-24 11:44:20+00:00,,0,1,module: cuda triaged module: CUDACachingAllocator,True
82072,cant build with USE_VULKAN=1 high priority module: build triaged oncall: mobile module: vulkan,2022-07-24 06:36:18+00:00,,0,7,high priority module: build triaged oncall: mobile module: vulkan,True
82070,[FSDP] deepcopy FSDP model for EMA results in error oncall: distributed module: fsdp,2022-07-24 01:28:04+00:00,,1,2,oncall: distributed module: fsdp,True
82068,upsample_bilinear2d() received an invalid combination of arguments module: onnx module: nn triaged module: regression,2022-07-23 22:02:27+00:00,,0,5,module: onnx module: nn triaged module: regression,True
82065,optimize_for_mobile vulkan_prepack::conv2d_clamp_prepack oncall: mobile module: vulkan,2022-07-23 20:00:21+00:00,,0,1,oncall: mobile module: vulkan,False
82061,Documentation for torch.cuda.Event(blocking=True) is wrong module: docs module: cuda triaged,2022-07-23 18:12:28+00:00,,0,2,module: docs module: cuda triaged,True
82053,Inconsistent implementation of quant_utils:: ChooseQuantizationParams compared with fbgemm:: ChooseQuantizationParams oncall: quantization triaged,2022-07-23 08:30:02+00:00,,1,3,oncall: quantization triaged,True
82041,[Misleading] The doc started using Tensorflow terminology in the document to explain how to use the Pytorch code. module: docs triaged,2022-07-23 01:43:39+00:00,,0,0,module: docs triaged,False
82033,[PyTorch/XLA] Improve the XLA PR landing process module: ci triaged module: xla,2022-07-23 00:19:19+00:00,,0,7,module: ci triaged module: xla,True
81996,linspace cpu and sometimes cuda is wrong on integral types triaged module: correctness (silent) module: python frontend module: edge cases,2022-07-22 16:33:41+00:00,,0,1,triaged module: correctness (silent) module: python frontend module: edge cases,True
81988,Unify c10::Event and at::cuda::CUDAEvent module: cuda triaged better-engineering,2022-07-22 15:19:13+00:00,,0,0,module: cuda triaged better-engineering,True
81985,"nn.InstanceNorm and nn.GroupNorm are affected by padding, so they need to masking triaged module: nestedtensor module: norms and normalization module: padding module: masked operators oncall: pt2",2022-07-22 14:49:27+00:00,,0,15,triaged module: nestedtensor module: norms and normalization module: padding module: masked operators oncall: pt2,True
81983,backwards compatibility ALLOWLIST is misused module: ci triaged,2022-07-22 14:29:55+00:00,,0,1,module: ci triaged,True
81982,test_sparse_matmul_cpu_complex128 fails on my local copy module: sparse triaged module: complex,2022-07-22 14:16:20+00:00,,0,1,module: sparse triaged module: complex,True
81979,test_sparse_spdiags_cpu_bool fails on my local working copy module: sparse triaged,2022-07-22 14:03:30+00:00,,0,1,module: sparse triaged,True
81963,Tensor.backward type hints clarification module: docs module: autograd module: typing triaged actionable,2022-07-22 04:18:52+00:00,,0,7,module: docs module: autograd module: typing triaged actionable,True
81959,Overloading multiple signatures for a single ref triaged module: primTorch,2022-07-22 00:42:07+00:00,,0,1,triaged module: primTorch,True
81955,Investigate adding shell linter/checker to CI module: ci triaged,2022-07-21 23:55:16+00:00,,1,2,module: ci triaged,True
81945,Investigate adding Dockerfile linter hadolint to CI module: ci triaged,2022-07-21 23:06:39+00:00,,1,1,module: ci triaged,True
81943,Investigate if it's okay to throw a RuntimeError instead of TypeError here : https://github.com/pytorch/pytorch/pull/79560/files#diff-415017bcad4fa6cd6d3dfe5f6ea1caffcd7122b46b8c1e4825f7d889efc80a62R1816 triaged,2022-07-21 22:51:00+00:00,,0,0,triaged,False
81938,"Devirtualize sym_sizes, virtualize sym_sizes_custom triaged",2022-07-21 22:40:18+00:00,,0,0,triaged,True
81937,Add more autograd tests with symints triaged,2022-07-21 22:38:55+00:00,,0,0,triaged,False
81935,implement sym_numel triaged,2022-07-21 22:37:52+00:00,,0,1,triaged,True
81932,Make sure we always redispatch through a dispatcher for all SymInt ops triaged,2022-07-21 22:36:21+00:00,,0,0,triaged,True
81912,Unknown builtin op: aten::broadcast_shapes oncall: jit,2022-07-21 20:22:05+00:00,,0,3,oncall: jit,False
81899,Dependency header directory is not properly expanded in the utils.cpp_extention in ninja mode module: cpp-extensions triaged,2022-07-21 17:01:12+00:00,,0,4,module: cpp-extensions triaged,True
81883,RuntimeError: CUDA error: no kernel image is available for execution on the device module: cuda triaged,2022-07-21 13:56:00+00:00,,0,19,module: cuda triaged,True
81876,dtype mismatch when after using auto mixed precision triaged module: amp (automated mixed precision),2022-07-21 13:05:41+00:00,,0,7,triaged module: amp (automated mixed precision),True
81868,grid_sample and mode='bilinear' induces errors at discrete pixel locations module: nn triaged,2022-07-21 11:21:16+00:00,,0,4,module: nn triaged,True
81856,Compatibility with newest MKL module: build triaged module: mkl,2022-07-21 09:20:12+00:00,,0,0,module: build triaged module: mkl,True
81855,Enable jit error when using FSDP oncall: jit,2022-07-21 07:59:02+00:00,,0,2,oncall: jit,True
81808,Workflows fail silently when the workflow file is invalid module: ci triaged,2022-07-20 20:26:06+00:00,,0,1,module: ci triaged,True
81801,"Rename DispatchKey Dense/Sparse/etc to DenseFunctionality/SparseFunctionality, use original name for alias module: internals triaged",2022-07-20 19:30:33+00:00,,0,1,module: internals triaged,True
81768,TestTagsCPU.test_tags__refs_constant_pad_nd_cpu_float32 flaky with dynamo & pytest module: ci triaged,2022-07-20 15:38:50+00:00,,0,6,module: ci triaged,False
81750,Modernize logging tensor in torch.testing._internal module: internals module: logging triaged,2022-07-20 02:08:59+00:00,,0,6,module: internals module: logging triaged,True
81749,BatchNorm for complex tensor triaged module: complex module: primTorch,2022-07-20 01:33:02+00:00,,0,5,triaged module: complex module: primTorch,True
81732,DISABLED test_non_contiguous_tensors_nn_ConvTranspose1d_cuda_complex32 (__main__.TestModuleCUDA) module: nn triaged module: flaky-tests module: complex skipped,2022-07-19 21:40:44+00:00,,1,13,module: nn triaged module: flaky-tests module: complex skipped,False
93786,Support JaggedTensor/KeyedJaggedTensor from TorchRec in TorchDynamo triaged oncall: pt2 module: dynamo,2022-07-19 20:35:31+00:00,,0,1,triaged oncall: pt2 module: dynamo,False
81722,Inconsistent naming convention for end of enum in DispatchKey triaged module: dispatch,2022-07-19 19:52:41+00:00,,0,0,triaged module: dispatch,True
81717,PyTorch Embedding Op with max_norm is not working as expected module: cuda triaged module: norms and normalization module: embedding bug,2022-07-19 19:02:22+00:00,,0,2,module: cuda triaged module: norms and normalization module: embedding bug,True
81703,Dispatcher debug/logging mode triaged module: dispatch,2022-07-19 15:42:32+00:00,,0,4,triaged module: dispatch,True
81692,Failed to static link latest cuDNN while compiling module: build module: cudnn triaged,2022-07-19 12:06:31+00:00,,0,5,module: build module: cudnn triaged,True
81684,Message exchange failure when perform alltoallv (cpus)  high priority triage review oncall: distributed module: c10d,2022-07-19 06:31:51+00:00,,0,2,high priority triage review oncall: distributed module: c10d,True
81682,Python operator registration API for subclasses feature triaged module: dispatch module: __torch_dispatch__,2022-07-19 03:52:23+00:00,,0,0,feature triaged module: dispatch module: __torch_dispatch__,True
81681,FakeTensor consolidated strategy for in_kernel_invocation and dispatch keys triaged module: fakeTensor,2022-07-19 03:28:45+00:00,,0,3,triaged module: fakeTensor,True
81680,Provide an option to disable CUDA_GCC_VERSIONS module: build triaged,2022-07-19 03:18:13+00:00,,0,5,module: build triaged,True
81678,Export quantized shufflenet_v2_x0_5 to ONNX module: onnx triaged onnx-triaged,2022-07-19 02:53:33+00:00,,1,4,module: onnx triaged onnx-triaged,True
81669,Register refs for CompositeImplicitAutograd ops as decompositions triaged module: primTorch,2022-07-18 23:19:43+00:00,,0,6,triaged module: primTorch,True
81667,[Tracker] AO migration of quantization from `torch.nn` to `torch.ao.nn` oncall: quantization low priority triaged,2022-07-18 22:34:31+00:00,,5,2,oncall: quantization low priority triaged,True
81654,[packaging] Conda install missing python local version label (+cu123 or +cpu) oncall: releng triaged,2022-07-18 20:46:39+00:00,,0,0,oncall: releng triaged,True
81651,optimize_for_mobile has an issue with constant operations at the end of a loop oncall: mobile,2022-07-18 18:50:32+00:00,,0,2,oncall: mobile,True
81650,RFC: auto-generated plain Tensor argument only sparse primitives module: sparse triaged,2022-07-18 18:50:12+00:00,,0,2,module: sparse triaged,True
81649,Idiom for PrimTorch refs for Tensor methods triaged module: primTorch,2022-07-18 18:26:06+00:00,,0,0,triaged module: primTorch,True
81648,`sparse_coo.to_dense()` produces different results between CPU and CUDA backends for boolean non-coalesced inputs. module: sparse triaged,2022-07-18 18:20:56+00:00,,0,4,module: sparse triaged,True
81635,Windows Debug binaries crash on forward: assert fail on IListRefIterator destructor oncall: jit,2022-07-18 15:00:49+00:00,,0,1,oncall: jit,True
81626,DISABLED test_profiler (test_jit.TestJit) oncall: jit module: flaky-tests skipped,2022-07-18 06:45:15+00:00,,0,6,oncall: jit module: flaky-tests skipped,False
81625,[bug] the output shape from torch::mean and torch::var is different  in libtorch module: cpp triaged,2022-07-18 06:31:34+00:00,,0,0,module: cpp triaged,True
81622,[Distributed] test_dynamic_rpc_existing_rank_can_communicate_with_new_rank_cuda fails in caching allocator oncall: distributed,2022-07-18 03:15:32+00:00,,1,1,oncall: distributed,False
81620,PyTorch 1.12 cu113 Illegal Memory Access or Internal Error instead of Out of Memory cases module: cudnn module: cuda triaged,2022-07-17 21:43:25+00:00,,0,6,module: cudnn module: cuda triaged,True
81608,"FakeTensorMode cannot handle non-fake tensor, but non-fake tensors can arise from non-interposable Tensor construction calls needs reproduction triaged oncall: pt2",2022-07-17 03:51:25+00:00,,0,3,needs reproduction triaged oncall: pt2,True
81568,Improve interaction of PyTorch downstream libraries and torchdeploy triaged module: deploy,2022-07-15 19:18:13+00:00,,0,1,triaged module: deploy,True
81565,__getitem__ is returned as an OverloadPacket instead of an OpOverload in __torch_dispatch__ triaged module: __torch_dispatch__ bug,2022-07-15 18:53:36+00:00,,0,5,triaged module: __torch_dispatch__ bug,True
81559,[Profiler] Defer thread assignment for python startup events. triaged oncall: profiler,2022-07-15 17:55:51+00:00,,1,0,triaged oncall: profiler,True
81554,float' object is not callable when using scheduler.step() with MultiplicativeLR module: optimizer triaged actionable,2022-07-15 15:10:49+00:00,,0,2,module: optimizer triaged actionable,True
81552,Support Swift Package Manager (SPM) for iOS oncall: mobile,2022-07-15 11:39:10+00:00,,0,3,oncall: mobile,False
81545,Precision error from torch.distributed.send() to recv() oncall: distributed,2022-07-15 05:45:01+00:00,,0,3,oncall: distributed,True
81544,Torch does not build with Lazy TS disabled module: build triaged,2022-07-15 05:41:26+00:00,,0,0,module: build triaged,True
81543,Linking pytorch libraries causes sstream behavior to be overridden globally module: build triaged,2022-07-15 04:53:09+00:00,,0,1,module: build triaged,True
81541,[vulkan]compiling VulkanOpContext.cpp with some errors module: build triaged module: vulkan,2022-07-15 03:40:00+00:00,,0,2,module: build triaged module: vulkan,True
81539,CapabilityBasedPartitioner treats non-compute ops inconsistently triaged module: fx module: CapabilityBasedPartitioner module: fx.passes,2022-07-15 03:38:45+00:00,,0,0,triaged module: fx module: CapabilityBasedPartitioner module: fx.passes,True
81532,forward program terminated from __cxa_pure_virtual needs reproduction module: crash module: cpp module: autograd triaged shadow review,2022-07-15 03:17:53+00:00,,0,1,needs reproduction module: crash module: cpp module: autograd triaged shadow review,True
81531,CapabilityBasedPartitioner doesn't support horizontal (vertical?) fusion triaged module: nvfuser,2022-07-15 03:03:15+00:00,,0,0,triaged module: nvfuser,True
81482,[onnx] Add support for prim::DictConstruct in pytorch-ONNX converter module: onnx triaged,2022-07-14 18:38:01+00:00,,1,2,module: onnx triaged,False
81478,[onnx] support more combinations of args/kwargs as model inputs for pytorch-onnx converter module: onnx triaged,2022-07-14 18:10:42+00:00,,1,0,module: onnx triaged,False
81465,jit gives surprising results with lists of objects oncall: jit,2022-07-14 12:30:38+00:00,,0,0,oncall: jit,True
81460,[JIT] Request Constant Propagation to keep fake_quantize_per_tensor_affine and fake_quantize_per_channel_affine on the graph oncall: jit,2022-07-14 08:37:34+00:00,,0,1,oncall: jit,False
81459,Missing corner case handling in ATen ctc_loss implementation module: loss module: error checking triaged,2022-07-14 07:03:28+00:00,,0,0,module: loss module: error checking triaged,True
81448,torch.utils.checkpoint optimization opportunity module: autograd triaged enhancement has workaround,2022-07-14 01:19:50+00:00,,0,2,module: autograd triaged enhancement has workaround,True
81446,torch.randint should accept high=2**63 triaged module: random module: edge cases,2022-07-14 00:51:32+00:00,,0,0,triaged module: random module: edge cases,True
81428,torch.stft does not normalize non-rectangular windows correctly triaged module: complex module: fft,2022-07-13 21:04:48+00:00,,0,1,triaged module: complex module: fft,True
81426,[FSDP] `test_mp_embedding_reduce()` fails with `transformer_auto_wrap_policy()` triaged module: fsdp,2022-07-13 20:41:38+00:00,,0,0,triaged module: fsdp,True
81417,Add a check to detect mutation of the inputs during backward module: autograd module: molly-guard triaged actionable,2022-07-13 18:55:46+00:00,,0,0,module: autograd module: molly-guard triaged actionable,True
81413,torch.searchsorted error message and documentation is unclear module: docs triaged module: sorting and selection,2022-07-13 17:48:05+00:00,,0,1,module: docs triaged module: sorting and selection,True
81412,num_worker and prefetch_factor in DataLoader do not scale module: multiprocessing module: dataloader triaged,2022-07-13 17:33:20+00:00,,0,11,module: multiprocessing module: dataloader triaged,True
81405,Implement shape/size functions for nestedtensor triaged module: nestedtensor,2022-07-13 14:50:48+00:00,,0,6,triaged module: nestedtensor,False
81385,"""Attempted to resize a view tensor to a larger size. This is not allowed in the functionalization pass"" reported on non view tensor triaged module: functionalization",2022-07-13 03:49:34+00:00,,0,1,triaged module: functionalization,True
81383,Investigate ncclRedOpCreatePreMulSum operator for gradient reduction oncall: distributed,2022-07-13 03:16:20+00:00,,0,1,oncall: distributed,False
81381,quantization: QConfigMapping should be easy to print oncall: quantization triaged,2022-07-13 00:54:57+00:00,,1,0,oncall: quantization triaged,True
81361,Segfault with fake tensor triaged module: fakeTensor,2022-07-12 22:10:11+00:00,,0,1,triaged module: fakeTensor,True
81358,[Prims+NvFuser] Issue with aten.where.ScalarSelf triaged module: nvfuser,2022-07-12 22:02:33+00:00,,0,0,triaged module: nvfuser,True
81337,JIT trace takes forever on a simple method oncall: jit,2022-07-12 17:47:06+00:00,,0,3,oncall: jit,True
81333,Reductions on tensors larger than GPU memory feature module: cuda triaged needs research,2022-07-12 16:53:44+00:00,,0,0,feature module: cuda triaged needs research,True
81323,`torch.overrides.get_testing_overrides` does not function as intended for native tensor methods/operations triaged module: __torch_function__,2022-07-12 15:10:02+00:00,,0,1,triaged module: __torch_function__,True
81317,Incorrect results for mean or sum kernels on aarch64 when building with gcc-7 triaged module: arm,2022-07-12 14:11:10+00:00,,0,2,triaged module: arm,True
81307,[Prims+NvFuser] Non-fusible ops Tracker triaged module: nvfuser,2022-07-12 03:44:09+00:00,,0,12,triaged module: nvfuser,True
81297,Files downloaded with torch.hub should respect umask triaged module: hub,2022-07-12 00:16:12+00:00,,0,4,triaged module: hub,True
81287,Runtime error in Libtorch cpp project (Didn't find engine for operation quantized::conv2d_prepack NoQEngine) oncall: quantization triaged,2022-07-11 22:56:17+00:00,,1,3,oncall: quantization triaged,True
81259,Refactor linter adapters to avoid code duplication module: lint triaged enhancement,2022-07-11 18:07:08+00:00,,0,1,module: lint triaged enhancement,True
81257,High GPU context memory on Torch 1.11.0 but none on Torch 1.10.0 needs reproduction module: cuda triaged,2022-07-11 17:53:17+00:00,,0,9,needs reproduction module: cuda triaged,True
81255,[FSDP] Avoid explicit replace of activation checkpoint prefixes oncall: distributed better-engineering module: fsdp,2022-07-11 17:11:11+00:00,,0,0,oncall: distributed better-engineering module: fsdp,False
81245,"Libtorch cannot load TrochScript Module correctly, when a network contains conv2d(inchannels=64, outchannels=128, kernelsize=1) . oncall: jit",2022-07-11 15:44:07+00:00,,0,0,oncall: jit,True
81244,CapabilityBasedPartitioner does not work correctly with mutating operations triaged module: fx module: CapabilityBasedPartitioner,2022-07-11 15:40:52+00:00,,0,1,triaged module: fx module: CapabilityBasedPartitioner,True
81240,Functionalization and fake tensors failure in torture test triaged module: __torch_dispatch__ module: functionalization,2022-07-11 14:58:40+00:00,,0,1,triaged module: __torch_dispatch__ module: functionalization,True
81229,torch.fx.node.map_aggregate and torch.utils._pytree.tree_map do the same thing triaged module: fx module: pytree,2022-07-11 12:55:53+00:00,,0,0,triaged module: fx module: pytree,True
81213,DISABLED test_trace_dependencies (test_analyze.TestAnalyze) triaged module: flaky-tests skipped module: deploy oncall: package/deploy imported,2022-07-11 09:42:39+00:00,,0,5,triaged module: flaky-tests skipped module: deploy oncall: package/deploy imported,False
81195,torch._weight_norm with specified dim returns wrong output module: nn module: error checking triaged module: regression module: norms and normalization,2022-07-11 05:12:01+00:00,,1,13,module: nn module: error checking triaged module: regression module: norms and normalization,True
81186,grad not preserved during copying or pickling triaged module: python frontend,2022-07-10 18:28:35+00:00,,0,2,triaged module: python frontend,True
81185,[Mac M1] `torch.mm` sometimes produces incorrect results high priority module: cpu triaged module: correctness (silent) module: arm module: m1,2022-07-10 18:21:12+00:00,,1,26,high priority module: cpu triaged module: correctness (silent) module: arm module: m1,True
81172,build libtorch with the same mkl as Matlab module: binaries triaged module: mkl,2022-07-09 18:22:51+00:00,,0,0,module: binaries triaged module: mkl,True
81167,move bazel files out of pytorch repo root triaged module: bazel,2022-07-09 07:04:47+00:00,,0,3,triaged module: bazel,True
81162,SparseAdam performance issue during optimizer step module: performance module: sparse module: optimizer triaged,2022-07-09 01:58:46+00:00,,0,0,module: performance module: sparse module: optimizer triaged,True
81140,libprotobuf version compatibility  triaged module: build warnings,2022-07-08 21:11:16+00:00,,1,7,triaged module: build warnings,True
81127,Docker updates cause subsequent builds to fail high priority module: ci triaged,2022-07-08 18:34:06+00:00,,1,2,high priority module: ci triaged,True
81115,torch.package can not be used to serialize `resnet18` from TorchVision-0.12 high priority module: vision module: regression oncall: package/deploy imported,2022-07-08 16:59:33+00:00,,0,1,high priority module: vision module: regression oncall: package/deploy imported,True
81110,CI: Run cpu tests in parallel processes? module: ci triaged,2022-07-08 15:28:46+00:00,,0,3,module: ci triaged,True
81104,Resize/reshape of sparse compressed tensors - design module: sparse triaged,2022-07-08 12:42:17+00:00,,0,4,module: sparse triaged,True
81102,[discussion] Consolidation of audio-visual I/O in a new package module: build triaged module: vision module: third_party,2022-07-08 11:54:59+00:00,,0,28,module: build triaged module: vision module: third_party,True
81100,[jit] Failed to load a saved scripted function oncall: jit,2022-07-08 09:42:24+00:00,,0,0,oncall: jit,True
81085,RuntimeError: required keyword attribute 'value' is undefined high priority triage review oncall: jit,2022-07-08 06:19:32+00:00,,0,19,high priority triage review oncall: jit,True
81084,[ONNX] Exporting the operator `::svd` to ONNX opset version 13 is not supported. module: onnx triaged OSS contribution wanted onnx-triaged,2022-07-08 05:47:44+00:00,,0,7,module: onnx triaged OSS contribution wanted onnx-triaged,False
81065,[Releng] Improve the tutorials release process module: build module: ci triaged,2022-07-07 22:19:47+00:00,,0,0,module: build module: ci triaged,True
81046,three typing inconsistencies on Tensor methods module: typing triaged,2022-07-07 14:17:48+00:00,,0,4,module: typing triaged,True
80986,"[Prims+NVFuser] nvFuser running into ""Tensors of type SparseTensorImpl do not have strides"" triaged module: nvfuser",2022-07-06 18:54:27+00:00,,0,3,triaged module: nvfuser,True
80985,Nested tensor: Support Noncontiguous Buffer triaged topic: not user facing release notes: nested tensor,2022-07-06 18:52:53+00:00,,1,1,triaged topic: not user facing release notes: nested tensor,False
80973,[ONNX] Tool to find mismatch in exported ONNX model module: onnx triaged onnx-triaged onnx-needs-info,2022-07-06 17:29:58+00:00,,1,4,module: onnx triaged onnx-triaged onnx-needs-info,False
80966,[Prims+NVFuser] Aten2Aten decomp hurting performance triaged module: nvfuser module: primTorch,2022-07-06 16:21:29+00:00,,0,7,triaged module: nvfuser module: primTorch,True
80954,ExpandedWeights sometimes fail silently and doesn't compute .grad_sample attribute module: nn triaged,2022-07-06 12:33:04+00:00,,0,2,module: nn triaged,True
80951,ExpandedWeights can't handle modules with tied weights module: nn triaged,2022-07-06 12:09:07+00:00,,0,2,module: nn triaged,True
80946,torch.nn.functional.linear fails for multi-dimensional bias from torch 1.12 module: nn triaged module: regression,2022-07-06 10:00:28+00:00,,0,6,module: nn triaged module: regression,True
80942,[LTC] OOM on mnist example triaged module: lazy,2022-07-06 08:08:25+00:00,,1,19,triaged module: lazy,True
80940,Wrong example of sliced computation in doc page Numerical Accuracy module: docs triaged module: numerical-reproducibility,2022-07-06 06:58:51+00:00,,0,2,module: docs triaged module: numerical-reproducibility,False
80939,[jit] script backward wrong gradient oncall: jit,2022-07-06 05:53:13+00:00,,0,0,oncall: jit,True
80932,Position embedding aware global circular convolution feature module: nn triaged,2022-07-06 03:20:33+00:00,,0,1,feature module: nn triaged,True
80929,"Interpolation artifacts when using nn.interpolate, trilinear mode for 3D label images module: nn triaged module: interpolation",2022-07-06 01:50:39+00:00,,0,5,module: nn triaged module: interpolation,True
80921,[primTorch] `|` operator does not work with FakeTensor in _refs feature triaged module: meta tensors module: primTorch,2022-07-05 23:46:20+00:00,,0,0,feature triaged module: meta tensors module: primTorch,True
80903,make_fx doesn't work with truly dynamic argument functions (e.g. fx.Interpreter) triaged module: fx,2022-07-05 19:16:23+00:00,,0,11,triaged module: fx,True
80875,slow test infra cannot handle nested suites module: ci triaged,2022-07-05 14:59:32+00:00,,0,0,module: ci triaged,True
80874,C++ extensions inject a bunch of compilation flags module: binaries module: cpp-extensions triaged better-engineering,2022-07-05 14:57:22+00:00,,0,3,module: binaries module: cpp-extensions triaged better-engineering,True
80867,[BE] Refactor FSDP Unit Tests triaged better-engineering module: fsdp,2022-07-05 14:30:01+00:00,,1,3,triaged better-engineering module: fsdp,True
80863,SummaryWriter add_embedding issue with label_img oncall: visualization,2022-07-05 13:28:03+00:00,,0,7,oncall: visualization,True
80861,"jit.freeze throws RuntimeError: stack_out && stack_out->size() == 1 INTERNAL ASSERT FAILED at ""../torch/csrc/jit/passes/frozen_conv_folding.cpp"":281 oncall: jit",2022-07-05 09:41:49+00:00,,0,4,oncall: jit,True
80857,Compatibility List module: binaries module: docs triaged,2022-07-05 07:06:14+00:00,,0,2,module: binaries module: docs triaged,True
80851,[bug][nvfuser] Applying nvfuser to the model leads to runtime error triaged module: nvfuser,2022-07-04 22:00:51+00:00,,1,23,triaged module: nvfuser,True
80832,[DDP] doesn't support multiple backwards when static_graph=True oncall: distributed module: ddp,2022-07-04 08:16:49+00:00,,0,0,oncall: distributed module: ddp,False
80829,Can torchscript dump backward graph? oncall: jit,2022-07-04 06:55:11+00:00,,0,1,oncall: jit,False
80827,Inconsistent computation of gradient in MaxUnPooling module: autograd triaged module: determinism actionable module: correctness (silent),2022-07-04 06:21:56+00:00,,0,19,module: autograd triaged module: determinism actionable module: correctness (silent),True
80826,Ne op does not behaves as expected with nan high priority needs reproduction triaged,2022-07-04 05:45:37+00:00,,0,3,high priority needs reproduction triaged,True
80824,"When running GPT trainning with megatron,  the program quit due to torch.distributed.elastic.agent.server.api:Received 1 death signal, shutting down workers oncall: distributed module: elastic",2022-07-03 23:41:43+00:00,,0,1,oncall: distributed module: elastic,False
80821,Add typing support to ModuleList and ModuleDict module: typing triaged,2022-07-03 21:55:12+00:00,,0,4,module: typing triaged,True
80808,"The result of doing a dot product between two vectors, using einsum, depends on another unrelated vector triaged module: numerical-reproducibility",2022-07-03 14:17:11+00:00,,0,5,triaged module: numerical-reproducibility,True
80804,`torch.renorm` gives wrong gradient for 0-valued input when `p` is even and `maxnorm=0`. module: autograd triaged module: edge cases,2022-07-03 05:03:05+00:00,,0,0,module: autograd triaged module: edge cases,True
80803,`hardshrink` gives wrong gradient for 0 input when `lambd` is 0. module: autograd triaged module: edge cases,2022-07-03 04:52:04+00:00,,0,0,module: autograd triaged module: edge cases,True
80776,`torch.inverse()` crash in cuda triaged module: linear algebra module: correctness (silent),2022-07-01 17:11:51+00:00,,0,1,triaged module: linear algebra module: correctness (silent),True
80774,RPC: Make RRefProxy callable oncall: distributed enhancement module: rpc,2022-07-01 15:32:24+00:00,,0,0,oncall: distributed enhancement module: rpc,False
80771,Anaconda is not a package manager module: docs triaged,2022-07-01 14:29:07+00:00,,0,0,module: docs triaged,True
80765,Let torch.utils.tensorboard support multiprocessing module: multiprocessing triaged module: tensorboard,2022-07-01 09:14:09+00:00,,0,2,module: multiprocessing triaged module: tensorboard,True
80762,`atan2` will gradcheck fail when `other` is a tensor with `int8` dtype module: autograd triaged module: edge cases,2022-07-01 07:34:08+00:00,,0,0,module: autograd triaged module: edge cases,True
80761,`det` will return wrong gradient for `1x1` matrix with 0 value. module: autograd triaged module: edge cases,2022-07-01 07:22:01+00:00,,0,1,module: autograd triaged module: edge cases,True
80756,"[ONNX] RuntimeError: 0 INTERNAL ASSERT FAILED at ""/pytorch/torch/csrc/jit/ir/ir.cpp"":518 oncall: jit module: onnx onnx-needs-info",2022-07-01 03:55:33+00:00,,0,1,oncall: jit module: onnx onnx-needs-info,False
92033,Unable to use vmap atop torch.distribution functionality high priority triaged module: functorch,2022-06-30 22:07:18+00:00,,0,9,high priority triaged module: functorch,True
80742,Add TorchDynamo as a submodule to Pytorch? module: build triaged,2022-06-30 21:19:02+00:00,,0,22,module: build triaged,True
80738,Output for `aten::_native_multi_head_attention` appears inconsistent with entry in `native_functions.yaml` oncall: transformer/mha,2022-06-30 20:46:37+00:00,,1,2,oncall: transformer/mha,False
80606,[jit.script] jit.script give uncertain results using torch.half oncall: jit module: nvfuser,2022-06-30 14:09:46+00:00,,0,2,oncall: jit module: nvfuser,True
80605,pad_sequence and pack_sequence should support length zero tensors module: rnn triaged enhancement,2022-06-30 13:53:13+00:00,,0,0,module: rnn triaged enhancement,True
80595,Overlapping Optimizer.step() with DDP backward oncall: distributed module: optimizer,2022-06-30 08:06:13+00:00,,0,5,oncall: distributed module: optimizer,False
80594,RuntimeError: DataLoader worker (pid 22822) is killed by signal: Aborted.  module: dataloader triaged,2022-06-30 07:54:18+00:00,,0,3,module: dataloader triaged,True
80588,Semi-reproducible random torch.baddbmm NaNs needs reproduction triaged module: NaNs and Infs,2022-06-30 03:54:25+00:00,,0,9,needs reproduction triaged module: NaNs and Infs,True
80580,`torch.ops.aten.find` inconsistent with `str.find` module: cpp triaged module: sorting and selection,2022-06-30 01:05:05+00:00,,0,1,module: cpp triaged module: sorting and selection,True
80577,2-dimensional arange triaged enhancement module: nestedtensor module: tensor creation,2022-06-30 00:38:43+00:00,,0,4,triaged enhancement module: nestedtensor module: tensor creation,True
80574,`bmm_sparse_cuda` kernel for `bfloat16` module: sparse module: cuda triaged module: bfloat16,2022-06-29 22:55:02+00:00,,0,0,module: sparse module: cuda triaged module: bfloat16,True
80561,Cannot run scripted BERT_Pytorch oncall: jit,2022-06-29 20:16:46+00:00,,0,0,oncall: jit,False
80553,Nonliner conjugate gradient optimizer + Hager-Zhang line search feature module: optimizer triaged needs research,2022-06-29 18:31:49+00:00,,0,1,feature module: optimizer triaged needs research,False
80551,NVFuser should extend caching to remove necessity for PrimTorch's executor to Provide Tensor Contiguity Info triaged module: nvfuser module: primTorch,2022-06-29 17:58:52+00:00,,1,9,triaged module: nvfuser module: primTorch,True
80549,Allow parameterization of Layouts module: sparse feature triaged module: python frontend,2022-06-29 17:43:25+00:00,,0,4,module: sparse feature triaged module: python frontend,True
80541,[Prims+NVFuser] Prims with missing NVFuser ops triaged module: nvfuser module: primTorch,2022-06-29 17:17:54+00:00,,0,9,triaged module: nvfuser module: primTorch,True
80496,DDP find_unused_parameters=True does not work for Sparse gradients oncall: distributed,2022-06-29 03:37:12+00:00,,0,1,oncall: distributed,False
80494,[bug] libtorch bug in nn::MultiheadAttention and nn::Transformer module: cpp module: nn triaged oncall: transformer/mha module: correctness (silent),2022-06-29 03:12:17+00:00,,0,7,module: cpp module: nn triaged oncall: transformer/mha module: correctness (silent),True
80488,Negative values still produced by torch.nn.functional.kl_div high priority module: nn triaged,2022-06-29 01:16:35+00:00,,0,10,high priority module: nn triaged,True
80458,Revisit OpInfo samples for nn.functional.max_poolNd module: nn triaged actionable module: pooling module: testing,2022-06-28 16:53:37+00:00,,0,1,module: nn triaged actionable module: pooling module: testing,True
80439,scatter_reduce choosed indices triaged enhancement module: scatter & gather ops,2022-06-28 09:34:44+00:00,,0,0,triaged enhancement module: scatter & gather ops,True
80431,CMake Error: File /opt/pytorch/build_variables.bzl does not exist. triaged module: regression module: docker,2022-06-28 07:50:36+00:00,,0,1,triaged module: regression module: docker,True
80427,Torch fx print line number of each node triaged module: fx,2022-06-28 06:39:35+00:00,,0,0,triaged module: fx,True
93774,Guard Failures in T5 Model triaged bug oncall: pt2 module: dynamo,2022-06-28 02:22:35+00:00,,1,21,triaged bug oncall: pt2 module: dynamo,True
80420,[DDP] output_device argument appears completely unused oncall: distributed triaged better-engineering module: ddp,2022-06-28 01:23:06+00:00,,0,0,oncall: distributed triaged better-engineering module: ddp,True
80417,[c10d] Async object-based collectives oncall: distributed triaged module: c10d,2022-06-28 01:09:09+00:00,,0,0,oncall: distributed triaged module: c10d,True
80411,Tracker: Slow gradcheck failures possibly indicating incorrect gradients module: autograd triaged actionable,2022-06-28 00:36:38+00:00,,0,1,module: autograd triaged actionable,True
80380,Support for learnable p Values in LPPOOL like Pool module: nn triaged needs research,2022-06-27 20:01:05+00:00,,0,0,module: nn triaged needs research,True
80377,Modify _add_docstr to also set the correct module for the APIs triaged better-engineering actionable module: python frontend,2022-06-27 19:25:09+00:00,,0,3,triaged better-engineering actionable module: python frontend,True
80372,[BE] Update ProcessGroupWrapper tests to test other collective message oncall: distributed triaged better-engineering module: c10d,2022-06-27 18:51:07+00:00,,0,0,oncall: distributed triaged better-engineering module: c10d,True
80349,Distributed Store `get` doesn't work well with `add` high priority triage review oncall: distributed module: docs triaged module: c10d,2022-06-27 16:05:16+00:00,,1,4,high priority triage review oncall: distributed module: docs triaged module: c10d,True
80338,DISABLED test_lobpcg (__main__.TestAutograd) module: autograd triaged skipped,2022-06-27 12:38:31+00:00,,1,3,module: autograd triaged skipped,False
80337,Illegal Memory Access from nonzero method when Tensor is Too Large module: dependency bug module: crash triaged module: edge cases,2022-06-27 11:54:09+00:00,,0,3,module: dependency bug module: crash triaged module: edge cases,True
80321, java.lang.ExceptionInInitializerError         at org.pytorch.NativePeer.initHybrid(Native Method) oncall: mobile,2022-06-27 07:41:46+00:00,,0,0,oncall: mobile,False
93772,Add support for torch.nn.quantized.modules.FloatFunctional needs reproduction module: nn triaged enhancement oncall: pt2,2022-06-27 06:54:23+00:00,,0,2,needs reproduction module: nn triaged enhancement oncall: pt2,False
80308,CosineAnnealingWarmRestarts with initial warm up and weight decay applied on consecutive cycles without warm up feature module: optimizer triaged needs research,2022-06-26 20:37:06+00:00,,0,1,feature module: optimizer triaged needs research,True
80302,AttributeError: 'LinearPackedParams' object has no attribute '_modules' needs reproduction oncall: quantization module: nn triaged,2022-06-26 16:12:30+00:00,,0,3,needs reproduction oncall: quantization module: nn triaged,True
80301,"Need ""valid"" and ""same"" padding mode for convTranspose2d feature module: nn module: convolution triaged module: padding",2022-06-26 14:22:01+00:00,,0,1,feature module: nn module: convolution triaged module: padding,True
80296,Sort tensors inplace feature triaged module: sorting and selection,2022-06-26 08:07:08+00:00,,0,2,feature triaged module: sorting and selection,True
80259,Cudnn batch norm kernel (batchnorm_bwtr_nhwc_semiPersist) gets blocked by overlapping NCCL all_reduce calls module: dependency bug module: cudnn triaged module: nccl module: memory format,2022-06-25 00:19:56+00:00,,0,4,module: dependency bug module: cudnn triaged module: nccl module: memory format,True
80256,[complex] dropout and it's variants should support complex tensors feature module: nn triaged module: complex,2022-06-24 23:31:44+00:00,,0,2,feature module: nn triaged module: complex,True
80242,Write some torch.distributed.nn.* tests for the new dispatcher passable ops oncall: distributed triaged,2022-06-24 20:34:50+00:00,,1,0,oncall: distributed triaged,True
80241,Change c10d APIs in ProcessGroup to accept const std::vector<at::Tensor>& oncall: distributed triaged,2022-06-24 20:26:04+00:00,,1,0,oncall: distributed triaged,True
80238,test_conv_backend tests OOMing in 10.2 slow_gradcheck CI module: nn module: ci module: convolution triaged,2022-06-24 19:43:00+00:00,,0,3,module: nn module: ci module: convolution triaged,True
80231,[Prims+NVFuser] Supports 0-sized inputs triaged module: nvfuser module: primTorch,2022-06-24 17:45:17+00:00,,0,1,triaged module: nvfuser module: primTorch,True
80230,[Prims+NVFuser] Aten2Prim refs tracking items triaged module: nvfuser module: primTorch,2022-06-24 17:36:31+00:00,,0,1,triaged module: nvfuser module: primTorch,True
80226,Support tensor subclasses as `UninitializedParameter`s module: nn triaged enhancement module: lazy tensor subclass,2022-06-24 15:33:37+00:00,,0,0,module: nn triaged enhancement module: lazy tensor subclass,True
80221,OpInfos for torch.ops.aten operations feature module: tests triaged,2022-06-24 13:48:30+00:00,,0,0,feature module: tests triaged,True
80208,F.binary_cross_entropy_with_logits unexpected behaviour module: nn module: loss triaged,2022-06-24 05:09:30+00:00,,0,1,module: nn module: loss triaged,True
80206,`soft_margin_loss` gives wrong gradient when `target` with dtype uint8 module: autograd module: nn module: loss triaged,2022-06-24 03:20:22+00:00,,0,0,module: autograd module: nn module: loss triaged,True
80204,`max_unpool` gives wrong gradient when `indices` has duplicate module: autograd module: nn triaged module: pooling module: edge cases,2022-06-24 03:18:52+00:00,,0,0,module: autograd module: nn triaged module: pooling module: edge cases,True
80189,[NVFuser] Investigate models without any fusion groups found  triaged module: nvfuser,2022-06-23 23:04:57+00:00,,0,0,triaged module: nvfuser,True
80187,[NVFuser] Investigate modules with bad performance relative to eager triaged module: nvfuser module: primTorch,2022-06-23 22:55:20+00:00,,1,20,triaged module: nvfuser module: primTorch,True
80172,Torch.fx: add reporting of the name of a module not found during tracing triaged module: fx,2022-06-23 20:41:54+00:00,,0,0,triaged module: fx,True
93770,Catch value errors if cell in match_nested_cell is empty needs reproduction triaged bug oncall: pt2 module: dynamo,2022-06-23 19:44:02+00:00,,1,2,needs reproduction triaged bug oncall: pt2 module: dynamo,True
80168,GEGLU activation module: nn triaged enhancement needs research,2022-06-23 19:41:15+00:00,,0,1,module: nn triaged enhancement needs research,True
80167,AMP step() enforce synchronization triaged module: amp (automated mixed precision),2022-06-23 19:40:18+00:00,,0,3,triaged module: amp (automated mixed precision),True
80161,[RFC] Module specific workflows module: rocm triaged,2022-06-23 18:56:21+00:00,,0,3,module: rocm triaged,True
80157,Elliptic Functions and Integrals feature triaged module: special,2022-06-23 18:30:44+00:00,,0,0,feature triaged module: special,True
80154,[primTorch] No _refs support for torch.Tensor.requires_grad.__get__ triaged module: primTorch,2022-06-23 18:11:51+00:00,,0,1,triaged module: primTorch,True
80152,Orthogonal Polynomials feature triaged module: special,2022-06-23 18:07:17+00:00,,0,0,feature triaged module: special,True
80151,activation checkpointing with non_reentrant implementation memory leaks high priority triage review oncall: distributed triaged,2022-06-23 17:37:18+00:00,,1,2,high priority triage review oncall: distributed triaged,True
80142,CPUProfilingAllocator greedy allocation plan generation failed oncall: mobile,2022-06-23 14:55:49+00:00,,0,2,oncall: mobile,False
80134,[feature request] Add support for a custom DatasetFetcher in DataLoader  module: dataloader triaged enhancement module: data,2022-06-23 12:08:54+00:00,,0,1,module: dataloader triaged enhancement module: data,True
80132,Expose more MAGMA backends for solve_triangular triaged module: linear algebra module: magma,2022-06-23 11:52:29+00:00,,0,0,triaged module: linear algebra module: magma,True
80118,"Allow a user provided ""test name - test time"" mapping file work with pytorch's test sharding mechanism module: ci triaged",2022-06-23 02:35:53+00:00,,0,5,module: ci triaged,True
80104,Provide error message when thread pool is exhausted in RPC high priority oncall: distributed triaged module: rpc,2022-06-23 00:01:16+00:00,,0,1,high priority oncall: distributed triaged module: rpc,True
80080,Complex support in DDP oncall: distributed triaged module: ddp,2022-06-22 20:26:04+00:00,,0,1,oncall: distributed triaged module: ddp,True
80067,"FakeTensor: Support torch.tensor([FakeTensor, 0]) triaged module: meta tensors",2022-06-22 19:16:12+00:00,,0,7,triaged module: meta tensors,True
80061,pow CUDA tensor raised to CPU scalar tensor result can't backward properly module: autograd triaged actionable,2022-06-22 18:43:33+00:00,,0,4,module: autograd triaged actionable,True
80033,Support `antialias` option on `torch.interpolate` for ONNX export module: onnx triaged onnx-triaged,2022-06-22 14:22:42+00:00,,1,11,module: onnx triaged onnx-triaged,False
80025,`torch.special.gammainc` backward pass with respect to the first argument module: distributions module: autograd triaged,2022-06-22 12:54:41+00:00,,0,7,module: distributions module: autograd triaged,True
80022,memory leaking when doing all_to_all_single communication  oncall: distributed triaged module: c10d,2022-06-22 09:24:49+00:00,,0,2,oncall: distributed triaged module: c10d,True
80017,RPC init fails and crashes when world_size is greater than 18 oncall: distributed triaged module: rpc,2022-06-22 07:06:01+00:00,,1,15,oncall: distributed triaged module: rpc,True
80016,[ONNX] Input node deleted when converting a Conditional random field model module: onnx triaged,2022-06-22 06:35:56+00:00,,0,6,module: onnx triaged,False
80012,static builds are broken by MKL_DNN module: build triaged module: regression module: third_party,2022-06-22 05:55:39+00:00,,1,1,module: build triaged module: regression module: third_party,True
80007,when forward use **kwargs，how to construct the example_ Inputs parameter in jit.trace? oncall: jit,2022-06-22 03:20:17+00:00,,0,2,oncall: jit,False
79997,Comprehensive documentation for Tensor indexing? module: docs triaged module: advanced indexing,2022-06-22 01:20:01+00:00,,0,3,module: docs triaged module: advanced indexing,True
79987,Deterministic `index_put` on CUDA fails when broadcasting is required triaged module: advanced indexing,2022-06-21 22:33:09+00:00,,0,4,triaged module: advanced indexing,True
79977,[CI] Do we run all cpp tests on CI? module: ci triaged,2022-06-21 21:33:47+00:00,,1,3,module: ci triaged,True
79967,[feature request] LazyTensor that provides/loads/computes its contents only upon request to be returned from torch.load feature triaged module: lazy,2022-06-21 20:19:46+00:00,,0,11,feature triaged module: lazy,False
79895,Modify update-viable-strict GHA to use internal version of checkout module: ci triaged,2022-06-20 20:45:13+00:00,,0,0,module: ci triaged,True
79893,Write lint for isGreen module: ci triaged,2022-06-20 20:41:33+00:00,,0,0,module: ci triaged,True
79888,`CosineAnnealingWarmRestarts` does not update parameters added with `add_param_group` triaged module: LrScheduler,2022-06-20 19:08:37+00:00,,0,1,triaged module: LrScheduler,True
79877,test_meta_vstack_cuda_int16 (__main__.TestMetaCUDA) Fails with DEBUG=1 module: autograd triaged needs research,2022-06-20 17:26:09+00:00,,0,3,module: autograd triaged needs research,False
79875,"A/libc: Fatal signal 6 (SIGABRT), code -6 in tid 11742 (objectdetection)  oncall: jit oncall: mobile",2022-06-20 17:03:14+00:00,,0,4,oncall: jit oncall: mobile,False
79867,All {view}_scatter variants should support all (or most) dtypes triaged module: functionalization,2022-06-20 14:35:38+00:00,,0,1,triaged module: functionalization,True
79853,[bazel] [ci] `//:lazy_tests` Could not run 'aten::mul.Tensor' with arguments from the 'Lazy' backend module: cuda triaged lazy module: lazy module: bazel,2022-06-19 22:31:36+00:00,,0,0,module: cuda triaged lazy module: lazy module: bazel,True
79851,[bazel] [ci] `//:module_test` CUDA error: CUDA driver version is insufficient for CUDA runtime version module: cuda triaged module: bazel,2022-06-19 22:21:38+00:00,,0,3,module: cuda triaged module: bazel,True
79848,Automatically calculate output_shape of sequential model (or any other fCNN) triaged module: meta tensors,2022-06-19 09:57:45+00:00,,0,1,triaged module: meta tensors,True
79847,Multi-node training meets unknown error oncall: distributed triaged,2022-06-19 06:57:06+00:00,,0,3,oncall: distributed triaged,True
79842,Automatically use CUDA triaged needs research module: python frontend,2022-06-18 21:44:09+00:00,,0,2,triaged needs research module: python frontend,True
79802,[ONNX] Replace test inheritance for `test/onnx/test_models.py` with parameterizing  module: onnx triaged onnx-triaged,2022-06-17 18:35:52+00:00,,0,0,module: onnx triaged onnx-triaged,False
79788,Parameter.__deepcopy__ doesn't preserve view relationships module: nn triaged module: correctness (silent),2022-06-17 16:24:06+00:00,,0,0,module: nn triaged module: correctness (silent),True
79787,Improve clarity by making sharding a static nightly update module: ci triaged,2022-06-17 14:46:33+00:00,,0,3,module: ci triaged,True
79785,android-tests is often flaky module: ci triaged,2022-06-17 14:36:17+00:00,,1,3,module: ci triaged,True
79766,[FSDP] Test that module using mixed precision can be loaded into non-mp module triaged better-engineering module: fsdp,2022-06-17 01:38:57+00:00,,0,0,triaged better-engineering module: fsdp,True
79751,[JIT] failures with nested with blocks + loop continuation oncall: jit,2022-06-16 21:44:45+00:00,,0,0,oncall: jit,True
79739,quantization: misleading backend config for linear_dynamic_fp16 oncall: quantization triaged,2022-06-16 19:32:08+00:00,,1,0,oncall: quantization triaged,True
79715,[FX] TypeError when tracing cat taking split's output as input triaged module: fx,2022-06-16 15:37:35+00:00,,0,0,triaged module: fx,True
79709,ONEDNN testing is not done properly in quantization codebase oncall: quantization triaged,2022-06-16 15:23:28+00:00,,1,3,oncall: quantization triaged,True
79705,gradgradcheck fails for torch.native_layer_norm module: double backwards module: autograd triaged,2022-06-16 14:25:56+00:00,,0,0,module: double backwards module: autograd triaged,True
79703,Float and double tensors randomly initialized with the same seed get different values for size >= 16 triaged module: random,2022-06-16 14:01:23+00:00,,0,0,triaged module: random,True
79684,Does Torch JIT Support Trace High-level Custom Op? oncall: jit,2022-06-16 06:11:51+00:00,,0,0,oncall: jit,True
79647,tensorboard SummaryWriter.add_graph fails when model uses empty tuples triaged module: tensorboard,2022-06-15 22:10:15+00:00,,0,0,triaged module: tensorboard,True
79644,[FSDP] Progress of ParamExecOrderWrapPolicy in progress triaged module: fsdp,2022-06-15 21:48:44+00:00,,1,0,in progress triaged module: fsdp,True
79629,Missing the time unit in duration time of DDP logging triaged module: ddp,2022-06-15 17:54:18+00:00,,0,2,triaged module: ddp,True
79620,[FSDP] Verify that FSDP-managed parameters are the same across ranks triaged better-engineering module: fsdp,2022-06-15 17:14:50+00:00,,1,0,triaged better-engineering module: fsdp,True
79606,PyTorch Preview (Nightly) version number does not comply with Conda conventions module: binaries triaged,2022-06-15 13:20:19+00:00,,0,0,module: binaries triaged,True
79604,Some unit tests are failing oncall: distributed triaged module: docker,2022-06-15 11:22:26+00:00,,0,2,oncall: distributed triaged module: docker,True
79592,[LTC] Introduce a `MetricsReport` python binding and allow backend to add their report as string triaged lazy,2022-06-15 01:58:09+00:00,,0,6,triaged lazy,False
79563,__torch__dispatch does not return new output in inplace function triaged module: __torch_dispatch__,2022-06-14 21:13:14+00:00,,0,1,triaged module: __torch_dispatch__,True
79542,Unable to use a parameter with torch.sparse_coo layout with DDP oncall: distributed module: sparse triaged module: ddp,2022-06-14 17:39:46+00:00,,0,1,oncall: distributed module: sparse triaged module: ddp,True
79528,test_ops.py extremely slow on cuda11.3 triaged,2022-06-14 15:42:45+00:00,,0,3,triaged,True
79518,"Display a ""reference"" link for ops that points to primTorch implementations module: docs triaged better-engineering module: primTorch",2022-06-14 13:57:40+00:00,,0,0,module: docs triaged better-engineering module: primTorch,True
79510,DISABLED test_checkpoint_wrapper_parity (__main__.CheckpointWrapperTest) triaged module: flaky-tests skipped module: fsdp,2022-06-14 06:43:37+00:00,,1,10,triaged module: flaky-tests skipped module: fsdp,False
79477,Implement NestedTensor size function module: nestedtensor oncall: transformer/mha,2022-06-13 23:31:58+00:00,,2,2,module: nestedtensor oncall: transformer/mha,False
79476,[META] Sign up to discuss significantly modifying CI module: ci triaged,2022-06-13 23:28:00+00:00,,0,0,module: ci triaged,True
79469,Add a new _broadcast_coalesced op for DDP triaged,2022-06-13 22:44:22+00:00,,1,0,triaged,True
79468,Ensure the guards in distributed_c10d.py wrappers get executed in the replay of the graph oncall: distributed triaged,2022-06-13 22:43:20+00:00,,1,0,oncall: distributed triaged,False
79467,Add autograd support for dispatch passable c10d ops oncall: distributed triaged,2022-06-13 22:41:53+00:00,,1,0,oncall: distributed triaged,False
79464,Iteration # 1-offset in DDP logging oncall: distributed triaged module: ddp,2022-06-13 22:04:44+00:00,,1,1,oncall: distributed triaged module: ddp,True
79459,[BE][ZeRO] Enable multigpu unit tests oncall: distributed triaged better-engineering,2022-06-13 21:31:38+00:00,,0,0,oncall: distributed triaged better-engineering,True
79453,[LTC] Make `torch::lazy::BackendImplInterface::ExecuteComputation` takes `ComputationPtr` instead of `Computation` triaged lazy,2022-06-13 20:56:47+00:00,,0,1,triaged lazy,True
79452,Use c10d broadcast_object in Zero oncall: distributed module: bootcamp good first issue triaged better-engineering pt_distributed_rampup,2022-06-13 20:40:30+00:00,,0,4,oncall: distributed module: bootcamp good first issue triaged better-engineering pt_distributed_rampup,True
79425,API for accessing SymIntNode mandates refcount bump even when it is unnecessary triaged,2022-06-13 16:48:10+00:00,,0,0,triaged,True
79418,Add doc formatting check to lintrunner module: lint triaged better-engineering,2022-06-13 15:21:07+00:00,,0,1,module: lint triaged better-engineering,True
79407,Conda enviroment triaged,2022-06-13 08:29:17+00:00,,0,1,triaged,True
79395,SymInt equality tests are unsound triaged,2022-06-13 02:31:25+00:00,,0,0,triaged,True
79388,Init connect timeout when use torch.distributed.run oncall: distributed oncall: r2p,2022-06-13 01:17:48+00:00,,0,9,oncall: distributed oncall: r2p,False
79387,caffe2_nvrtc is produced even when it won't be used module: build triaged module: selective build,2022-06-13 00:29:58+00:00,,0,0,module: build triaged module: selective build,True
79383,Incorrect image upscaling on MPS backend triaged module: mps,2022-06-12 18:58:13+00:00,,1,13,triaged module: mps,True
79382,torch failure to open libcuda.so.1 on macOS triaged module: macos,2022-06-12 17:02:28+00:00,,0,0,triaged module: macos,True
79375,TorchScript bidirectional lnlstm from example doesn't work oncall: jit,2022-06-12 08:54:27+00:00,,0,1,oncall: jit,False
79359,[build] No documented way to install C++ binaries for pure-python development of pytorch module: binaries module: build triaged,2022-06-12 02:02:08+00:00,,0,1,module: binaries module: build triaged,True
79355,[bazel] build spams warnings triaged module: bazel,2022-06-11 22:24:24+00:00,,0,0,triaged module: bazel,True
79352,Adam not optimally implemented: unnecessary torch.div module: performance module: optimizer triaged actionable,2022-06-11 14:45:23+00:00,,0,6,module: performance module: optimizer triaged actionable,True
79351,[bazel] ability to run gpu tests on gpu machines in RBE triaged module: bazel,2022-06-11 14:44:55+00:00,,0,0,triaged module: bazel,True
79349,PyTorch get positive log_prob of a multivariate normal distribution  triaged,2022-06-11 12:28:15+00:00,,0,1,triaged,False
79337,Conda install from pytorch-nightly channel does not install the expected version on macOS module: binaries triaged module: macos,2022-06-11 05:38:39+00:00,,0,1,module: binaries triaged module: macos,True
79336,Batches are being duplicated from go http call triaged,2022-06-11 03:53:44+00:00,,0,1,triaged,True
79333,[ONNX] Internal assert error during export oncall: jit module: onnx onnx-triaged,2022-06-11 03:37:37+00:00,,0,10,oncall: jit module: onnx onnx-triaged,True
79325,[NVFuser] hitting fallbacks on demucs (from torchbench + lazy tensor) triaged module: nvfuser,2022-06-10 22:27:21+00:00,,0,1,triaged module: nvfuser,True
79307,`prepare_qat_fx` docstring doesn't run oncall: quantization triaged module: fx,2022-06-10 19:40:40+00:00,,1,0,oncall: quantization triaged module: fx,True
79299,PyTorch gets stuck when using an NVLink/A6000 and more than two GPUs oncall: distributed triaged module: ddp,2022-06-10 19:20:03+00:00,,0,4,oncall: distributed triaged module: ddp,True
93763,allowed_functions_module_string_ignorelist doesn't work very well triaged oncall: pt2 module: dynamo,2022-06-10 18:01:35+00:00,,0,3,triaged oncall: pt2 module: dynamo,False
79275,testSerializationInterop in test/cpp/jit/torch_python_test.cpp has not run in over two years oncall: jit,2022-06-10 13:50:09+00:00,,0,1,oncall: jit,False
79272,"PyTorch leaks a macro definition called ""CHECK"" in the C++ version module: cpp triaged",2022-06-10 11:25:18+00:00,,0,0,module: cpp triaged,True
79261,[NVFuser] bad performance on pyhpc_isoneutral_mixing triaged module: nvfuser,2022-06-10 02:37:35+00:00,,0,0,triaged module: nvfuser,True
79250,[BE] Generalize recursive wrapping utility oncall: distributed better-engineering module: fsdp,2022-06-09 23:54:45+00:00,,1,0,oncall: distributed better-engineering module: fsdp,False
79246,[NVFuser] bad performance on mobilenet_v2 and mobilenet_v3_large triaged module: nvfuser,2022-06-09 23:24:25+00:00,,0,0,triaged module: nvfuser,True
79244,[NVFuser] bad performance on pyhpc_equation_of_state triaged module: nvfuser,2022-06-09 22:54:18+00:00,,1,0,triaged module: nvfuser,True
79222,scripted fft Convolutions are faster than nn.Conv1d with large kernels module: performance module: cuda module: convolution triaged,2022-06-09 18:33:19+00:00,,0,1,module: performance module: cuda module: convolution triaged,True
79208,[ONNX] Enable more operators to support data propagation module: onnx triaged onnx-triaged,2022-06-09 17:31:00+00:00,,1,0,module: onnx triaged onnx-triaged,True
79205,out-of-place functional optimizers: functional optimizers may not be composite compliant module: optimizer triaged needs research module: __torch_dispatch__ tensor subclass,2022-06-09 16:51:05+00:00,,0,0,module: optimizer triaged needs research module: __torch_dispatch__ tensor subclass,False
79202,[bug] Device dispatcher can choose CPU path for CUDA tensors. module: build triaged module: dispatch module: codegen,2022-06-09 16:02:21+00:00,,0,5,module: build triaged module: dispatch module: codegen,True
79197,[feature request] Support dataclass derivations of nn.Module module: nn triaged,2022-06-09 15:02:27+00:00,,0,7,module: nn triaged,True
79195,"[bug] fill_, masked_fill_ : fill ops allow lossy downcasting of fill value module: bc-breaking triaged topic: bc breaking module: primTorch",2022-06-09 13:49:56+00:00,,0,0,module: bc-breaking triaged topic: bc breaking module: primTorch,True
79421,Mismatch in clang toolchain lead to binary incompatibilities on M1 between torch and torchvision module: binaries module: ci triaged module: macos module: arm,2022-06-09 08:13:37+00:00,,1,18,module: binaries module: ci triaged module: macos module: arm,True
79191,"Triangular solve fails on batches of matrices of size > (*, 524280) module: cuda triaged module: linear algebra",2022-06-09 07:08:12+00:00,,0,10,module: cuda triaged module: linear algebra,True
79177,_make_elementwise_unary_reference and other function factories in torch._refs don't set __name__ correctly triaged module: primTorch,2022-06-09 01:30:29+00:00,,0,0,triaged module: primTorch,True
79171,DistributedDataParallel `static_graph=True` fails to handle unused parameters oncall: distributed triaged module: ddp,2022-06-09 00:03:29+00:00,,1,5,oncall: distributed triaged module: ddp,True
79164,PyTorch/XLA's DDP XLABackend is broken by upstream change oncall: distributed triaged module: xla module: ddp,2022-06-08 22:03:08+00:00,,1,5,oncall: distributed triaged module: xla module: ddp,True
79145,Redundant info are saved when using torch.save to save part of torch.tensor module: serialization triaged,2022-06-08 19:24:47+00:00,,0,3,module: serialization triaged,True
79138,[AUTOGRAD] support implicit reductions with SymInts in autograd. triaged lazy,2022-06-08 18:08:40+00:00,,1,0,triaged lazy,False
79137,[AUTOGRAD] codegen to use sym_sizes for ops w/ symint overloads in derivative formulas triaged lazy,2022-06-08 18:03:40+00:00,,0,0,triaged lazy,False
79130,torchvision.models.mobilenetv3 can't save pre-trained model to custom dir? triaged module: vision,2022-06-08 16:47:27+00:00,,0,0,triaged module: vision,True
79120,Hide or fuse TupleConstruct / TupleUnpack from tensorboard graph triaged module: tensorboard,2022-06-08 11:59:41+00:00,,0,0,triaged module: tensorboard,True
79117,[ONNX] `.squeeze(1)` on the B X T (not B X 1 X T) tensor causes export error in masking module: onnx triaged onnx-triaged,2022-06-08 10:40:20+00:00,,0,0,module: onnx triaged onnx-triaged,True
79118,About the source code of  triaged topic: docs,2022-06-08 09:27:13+00:00,,0,1,triaged topic: docs,False
79091,Error with Named Tensors and multiple threads triaged module: named tensor,2022-06-08 00:27:27+00:00,,0,1,triaged module: named tensor,True
79089,Improve PrimTorch testing for view consistency. module: tests triaged module: viewing and reshaping module: primTorch,2022-06-08 00:22:55+00:00,,0,3,module: tests triaged module: viewing and reshaping module: primTorch,True
79083,CI workflow creates too many tags in RSS feed module: ci triaged,2022-06-07 23:37:30+00:00,,1,4,module: ci triaged,True
79073,"Multi-node, Multi-GPU set up tutorial for Slurm cluster oncall: distributed triaged",2022-06-07 22:26:13+00:00,,0,1,oncall: distributed triaged,True
93760,inspect.signature.bind is not supported triaged oncall: pt2 module: dynamo,2022-06-07 22:09:35+00:00,,0,5,triaged oncall: pt2 module: dynamo,False
79053,[MetaIssue] Propagating SymInts through Autograd triaged lazy,2022-06-07 20:18:12+00:00,,0,1,triaged lazy,True
79052,Mirror and implement `SymbolicIntNode` API for `SymInt` so we can trace in C++ triaged lazy,2022-06-07 20:04:12+00:00,,0,0,triaged lazy,True
79050,[MetaIssue] Investigate if we should be reusing primtorch formulas for `is_dynamic` triaged lazy,2022-06-07 20:01:12+00:00,,1,1,triaged lazy,True
79049,DDP Freezes w/ No Output for PyTorch Geometric GNN Multi-GPU Node Classification oncall: distributed triaged module: ddp,2022-06-07 19:44:23+00:00,,0,10,oncall: distributed triaged module: ddp,True
79039,Add Autograd Support for Nested Tensor triaged module: nestedtensor,2022-06-07 18:44:00+00:00,,1,0,triaged module: nestedtensor,False
79032,Add a test that shows that lazy_ir reuse breaks SizeNodes triaged lazy,2022-06-07 17:44:23+00:00,,1,2,triaged lazy,True
79031,Implement SymbolicIntNode interface for lazy (i.e. lazy::SymbolicIntNode) triaged lazy,2022-06-07 17:33:23+00:00,,0,0,triaged lazy,True
79030,Devirtualize `sym_sizes`. It still has to work for python tensor subclasses and LTC/Xla triaged lazy,2022-06-07 17:31:00+00:00,,0,1,triaged lazy,True
79021,Building PyTorch from Source with BUILD_LAZY_TS_BACKEND_ON module: build triaged,2022-06-07 17:14:53+00:00,,0,0,module: build triaged,True
79020,"When setting sizes and strides on a tensor subclass in `THPVariable_make_wrapper_subclass`, also make offset symbolic triaged lazy module: lazy",2022-06-07 17:12:08+00:00,,0,1,triaged lazy module: lazy,True
79018,Random functions should infer device from user-specified Generator triaged enhancement module: random,2022-06-07 15:42:50+00:00,,0,0,triaged enhancement module: random,False
79016,Corner cases of ShardedTensor checkpoint when using TorchRec oncall: distributed module: checkpoint triaged sharded_tensor release notes: distributed (sharded),2022-06-07 13:21:44+00:00,,1,6,oncall: distributed module: checkpoint triaged sharded_tensor release notes: distributed (sharded),True
79014,RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR module: crash module: cudnn module: cuda triaged module: ddp,2022-06-07 13:09:54+00:00,,0,0,module: crash module: cudnn module: cuda triaged module: ddp,True
79013,Multi30k can't be downloaded the destination domain can't be reached triaged,2022-06-07 13:07:01+00:00,,0,1,triaged,True
79008,torchscript jit trace support custom op without  specific csrc and .so oncall: jit,2022-06-07 11:43:00+00:00,,0,0,oncall: jit,False
79004,Doc on index of CPU Device seems wrong module: cpp triaged,2022-06-07 08:49:50+00:00,,0,0,module: cpp triaged,False
79003,Libtorch C++ mobile build linking error module: build oncall: mobile,2022-06-07 08:29:21+00:00,,0,3,module: build oncall: mobile,True
78987,DataLoader leaking resources? module: dataloader triaged,2022-06-07 01:26:18+00:00,,0,9,module: dataloader triaged,True
78961,[forwardAD] torch.no_grad has no effect under forward_ad triaged module: forward ad,2022-06-06 19:50:23+00:00,,0,1,triaged module: forward ad,True
78954,Can we have Additive Attention? triaged oncall: transformer/mha,2022-06-06 18:38:56+00:00,,0,2,triaged oncall: transformer/mha,True
78929,Add type() support for mps backend triaged actionable module: mps,2022-06-06 14:45:05+00:00,,1,1,triaged actionable module: mps,True
78924,"If large enough tensor is being cloned, parallel dataloading hangs on M1 Mac high priority module: dataloader triaged module: macos module: deadlock module: arm",2022-06-06 13:26:11+00:00,,0,10,high priority module: dataloader triaged module: macos module: deadlock module: arm,True
78921,Do we really need sampler for IterableDataset? module: performance module: dataloader triaged,2022-06-06 09:35:07+00:00,,0,1,module: performance module: dataloader triaged,True
78920,Strange tracing result with torchscript oncall: jit,2022-06-06 07:26:45+00:00,,0,0,oncall: jit,False
78917,LambdaLR changes the learning rate in an undesired way triaged module: LrScheduler,2022-06-06 06:35:58+00:00,,0,2,triaged module: LrScheduler,True
78892,torch.fx deepcopy does not copy attributes added to GraphModule or Nodes triaged module: fx,2022-06-05 12:57:07+00:00,,0,0,triaged module: fx,True
78885,[distributed_test.py] Improve `test_barrier` oncall: distributed module: tests better-engineering,2022-06-05 07:41:25+00:00,,0,0,oncall: distributed module: tests better-engineering,False
78884,Abnormal GPU memory usage when using CUDA tensors with multiprocessing module: multiprocessing module: cuda module: memory usage triaged module: jiterator,2022-06-05 03:36:22+00:00,,0,4,module: multiprocessing module: cuda module: memory usage triaged module: jiterator,True
78882,Cannot build master on AWS cluster: error: ‘__fatDeviceText’ was not declared in this scope module: build module: cuda triaged,2022-06-04 20:01:50+00:00,,0,0,module: build module: cuda triaged,True
78880,fatal_signal_asan_no_sig_test in current master hang. module: cpp triaged module: deadlock module: testing,2022-06-04 12:32:12+00:00,,0,3,module: cpp triaged module: deadlock module: testing,True
78878,Improving clarity in the docs of different losses module: docs module: nn triaged module: python frontend,2022-06-04 09:20:39+00:00,,0,1,module: docs module: nn triaged module: python frontend,True
78876,Remove const from function return type if returning const value module: cpp triaged,2022-06-04 09:03:13+00:00,,0,1,module: cpp triaged,True
78873,[Profiler] Capture more information about inputs triaged oncall: profiler,2022-06-04 04:04:14+00:00,,1,0,triaged oncall: profiler,True
78871,[RecordFunction] Hold a durable schema reference triaged oncall: profiler,2022-06-04 03:47:58+00:00,,1,0,triaged oncall: profiler,True
78848,MPS: Adding int64 tensor does not work on AMD GPU triaged module: correctness (silent) module: mps,2022-06-03 21:28:08+00:00,,0,2,triaged module: correctness (silent) module: mps,True
78845,"[Modes] no_dispatch is not the same as DisableTorchFunction, causing differences in modes triaged module: __torch_function__ module: __torch_dispatch__",2022-06-03 20:39:19+00:00,,0,4,triaged module: __torch_function__ module: __torch_dispatch__,True
78842,Add TORCH_SHOW_CPP_STACKTRACES when TORCH_DISTRIBUTED_DEBUG = detail oncall: distributed module: bootcamp pt_distributed_rampup module: c10d,2022-06-03 20:11:31+00:00,,0,0,oncall: distributed module: bootcamp pt_distributed_rampup module: c10d,False
78834,[ONNX] Re-design `torch.onnx.export` module: onnx triaged needs design onnx-triaged,2022-06-03 18:06:25+00:00,,0,3,module: onnx triaged needs design onnx-triaged,False
78831,Mac M1 Build Failure on DEBUG=1 needs reproduction module: build triaged module: macos module: arm,2022-06-03 17:49:58+00:00,,0,1,needs reproduction module: build triaged module: macos module: arm,True
78829,Certain import order triggers segmentation fault module: crash triaged has workaround,2022-06-03 17:38:39+00:00,,0,11,module: crash triaged has workaround,True
78812,TorchScript inference get intermediate result? oncall: jit feature,2022-06-03 15:14:37+00:00,,0,1,oncall: jit feature,False
78809,Feature Request: Hessenberg and Schur decompositions feature triaged module: linear algebra,2022-06-03 14:21:29+00:00,,0,7,feature triaged module: linear algebra,True
78808,Feature request: Integer system decompositions feature triaged module: linear algebra,2022-06-03 14:17:03+00:00,,0,1,feature triaged module: linear algebra,True
78805,"torch.jit.script segmentation fault (pytorch debayer module) 1.10, 1.11 and nightly oncall: jit",2022-06-03 13:43:05+00:00,,0,0,oncall: jit,True
78800,Efficiency of unary operations on CPU for large tensors module: performance triaged,2022-06-03 11:07:31+00:00,,0,0,module: performance triaged,True
78786,Deprecate hardtanh type promotion behavior. module: nn triaged module: primTorch,2022-06-03 01:01:45+00:00,,0,2,module: nn triaged module: primTorch,True
78774,[FSDP] Customizable gradient pre-divide for mixed precision training oncall: distributed triaged module: fsdp,2022-06-02 22:22:33+00:00,,0,1,oncall: distributed triaged module: fsdp,True
78761,Extend tag testing for aliases triaged module: testing,2022-06-02 20:48:22+00:00,,0,0,triaged module: testing,True
78759,Add `inplace_view` tag for `resize_` triaged module: viewing and reshaping,2022-06-02 20:43:36+00:00,,0,4,triaged module: viewing and reshaping,True
78754,Getting NotImplementedError when trying to implement E2E support for `prim::is_nested` Op in torch-mlir. triaged module: nestedtensor oncall: transformer/mha,2022-06-02 20:04:49+00:00,,0,3,triaged module: nestedtensor oncall: transformer/mha,False
78744,Unable to programmatically update models using references from model.named_modules()...requires additional parsing module: nn triaged enhancement,2022-06-02 17:32:02+00:00,,0,2,module: nn triaged enhancement,True
78743,Expose docs from the yaml for each torch.Tag in Python  module: docs triaged module: dispatch module: library,2022-06-02 17:17:57+00:00,,0,0,module: docs triaged module: dispatch module: library,True
78742,Add a gallery of examples with sphinx-gallery module: docs triaged,2022-06-02 17:05:47+00:00,,0,1,module: docs triaged,False
78741,Test approximation and numerical stability of numerical operators  module: numerical-stability triaged module: testing,2022-06-02 17:01:48+00:00,,0,0,module: numerical-stability triaged module: testing,True
78738,[primTorch] Sensible Error Messages triaged module: primTorch,2022-06-02 16:48:08+00:00,,0,5,triaged module: primTorch,True
78737,New c10 constants module: internals triaged,2022-06-02 16:46:43+00:00,,0,3,module: internals triaged,True
78729,[Better Engineering] Make OpInfo-based test failures easy to reproduce module: tests triaged better-engineering,2022-06-02 14:56:24+00:00,,0,1,module: tests triaged better-engineering,True
78722,AlBert quantization oncall: quantization triaged,2022-06-02 12:41:20+00:00,,1,4,oncall: quantization triaged,True
78721,[ONNX] Scripted `reshape` incorrect if shape is dynamically calculated module: onnx triaged onnx-triaged bug,2022-06-02 12:38:50+00:00,,0,1,module: onnx triaged onnx-triaged bug,True
78720,ValueError during `yaml.dump(dtype)` module: serialization triaged,2022-06-02 12:34:29+00:00,,0,0,module: serialization triaged,True
78708,BuildExtension does not choose correct CUDA installation module: cpp-extensions module: cuda triaged,2022-06-02 07:07:24+00:00,,0,0,module: cpp-extensions module: cuda triaged,True
78681,"Unable to install Preview (Nightly) on M1 macOS: ""Symbol not found"" module: binaries triaged",2022-06-01 21:55:11+00:00,,0,4,module: binaries triaged,True
78656,Allow batch_norm_backward_elemt and batch_norm_gather_stats_with_counts handle 0 counts triaged enhancement module: norms and normalization,2022-06-01 19:19:48+00:00,,0,0,triaged enhancement module: norms and normalization,True
78638,"torch.distributed.init_process_group(backend=""nccl"") NCCL version error oncall: distributed",2022-06-01 18:23:38+00:00,,0,2,oncall: distributed,True
78634,Debug job does not build in debug mode module: ci triaged,2022-06-01 18:13:53+00:00,,0,2,module: ci triaged,True
78624,linalg.pinv_singular tests are slow module: autograd module: tests triaged module: linear algebra,2022-06-01 16:19:10+00:00,,0,0,module: autograd module: tests triaged module: linear algebra,True
78618,Module parameters/submodules can be shadowed by class attributes silently module: nn triaged actionable,2022-06-01 13:39:55+00:00,,0,2,module: nn triaged actionable,True
78606,[FSDP] Enhance sync_module_states for auto wrapping oncall: distributed better-engineering module: fsdp,2022-06-01 04:47:24+00:00,,0,0,oncall: distributed better-engineering module: fsdp,False
78605,torch.svd_lowrank fails for complex matrices triaged module: complex module: linear algebra,2022-06-01 04:43:03+00:00,,0,1,triaged module: complex module: linear algebra,True
78581,RFC: Improve the performance and usability of linear algebra on CUDA devices module: cuda triaged module: linear algebra module: magma,2022-05-31 21:24:56+00:00,,0,2,module: cuda triaged module: linear algebra module: magma,True
78559,[JIT] autodiff implementation of rand_like function is outdated  oncall: jit,2022-05-31 19:09:26+00:00,,0,0,oncall: jit,False
78530,LibTorch cannot be used without nvcc module: build triaged,2022-05-31 14:56:31+00:00,,0,2,module: build triaged,True
78519,test_python_dispatch fails on DEBUG=1 triaged module: dispatch,2022-05-31 11:46:03+00:00,,0,7,triaged module: dispatch,True
78518,Exponentiating floating number with cuda tensor is slow module: cuda triaged topic: performance,2022-05-31 11:25:45+00:00,,0,0,module: cuda triaged topic: performance,True
78513,clear input shape declaration  on pytorch model inputs and outputs triaged module: shape checking module: python frontend,2022-05-31 10:11:03+00:00,,0,0,triaged module: shape checking module: python frontend,True
78507,Parallel execution of multiple unrelated statements written sequentially triaged enhancement,2022-05-31 05:28:58+00:00,,0,1,triaged enhancement,True
78489,[1.9.1] [collect_env] collect_env does not collect actual runtime-loaded cudnn version module: cudnn module: collect_env.py triaged enhancement,2022-05-30 15:21:46+00:00,,0,6,module: cudnn module: collect_env.py triaged enhancement,True
99719,New feature requested: vmap for torch.histc high priority triaged module: functorch,2022-05-30 15:05:00+00:00,,0,10,high priority triaged module: functorch,True
78487,torch.fx: symbolic_trace: ones() received an invalid combination of arguments triaged module: fx,2022-05-30 12:59:44+00:00,,0,2,triaged module: fx,True
78486,Exception in torch.jit.script doesn't indicate where in the code the problem lies. oncall: jit,2022-05-30 10:59:41+00:00,,0,2,oncall: jit,True
78484,torch.lerp: discrepancy between CUDA and CPU (with extremal inputs) triaged module: NaNs and Infs,2022-05-30 09:50:06+00:00,,0,3,triaged module: NaNs and Infs,True
78483,is the issue resolved? windows not pytorch_jni in path oncall: java,2022-05-30 05:48:42+00:00,,0,1,oncall: java,True
78482,RuntimeError: Event device type CUDA does not match blocking stream’s device type CPU  module: autograd module: cuda module: tests triaged,2022-05-30 05:22:21+00:00,,0,5,module: autograd module: cuda module: tests triaged,True
78481,[onnx] RuntimeError: Attribute 'axes' is expected to have field 'ints' module: onnx triaged onnx-needs-info,2022-05-30 03:09:12+00:00,,0,4,module: onnx triaged onnx-needs-info,False
78475,`with torch.backends.cudnn.flags(deterministic=True)` doesn't give an exception for ctc_loss backward on CUDA module: cudnn triaged module: determinism,2022-05-29 14:23:20+00:00,,1,21,module: cudnn triaged module: determinism,True
78450,"Softmax, LogSoftmax are over parameterized module: nn triaged",2022-05-28 09:26:02+00:00,,0,1,module: nn triaged,True
78444,`layer_norm` triggers INTERNAL ASSERT with input requiring grad + zero-size int tensor module: autograd triaged actionable,2022-05-28 01:47:09+00:00,,0,0,module: autograd triaged actionable,True
78443,`index_fill` will trigger INTERNAL ASSERT when float tensor requiring grad + int tensor module: autograd triaged actionable,2022-05-28 01:29:24+00:00,,0,0,module: autograd triaged actionable,True
78435,fx.Tracer with param_shapes_constant=True not working for RobertaForMaskedLM triaged module: fx,2022-05-27 21:33:23+00:00,,0,0,triaged module: fx,True
78422,Permutation of Sparse Tensor module: sparse triaged,2022-05-27 13:15:49+00:00,,0,5,module: sparse triaged,True
78414,.lldbinit for lldb debuger feature triaged module: macos actionable,2022-05-27 08:30:53+00:00,,0,1,feature triaged module: macos actionable,True
78413,torch.angle differs from np.angle for -0. triaged module: numpy module: primTorch,2022-05-27 07:47:11+00:00,,0,2,triaged module: numpy module: primTorch,True
93756,Torchdynamo for Deepspeed and FSDP feature triaged module: fsdp oncall: pt2 module: dynamo module: distributed,2022-05-26 21:46:51+00:00,,0,2,feature triaged module: fsdp oncall: pt2 module: dynamo module: distributed,False
78367,Split up and reorganize RPC tests oncall: distributed triaged better-engineering module: rpc,2022-05-26 16:14:38+00:00,,1,2,oncall: distributed triaged better-engineering module: rpc,True
78346,`gradcheck` fails for `torch.distribution.transform` APIs in forward mode module: distributions module: autograd triaged module: forward ad,2022-05-26 07:35:05+00:00,,0,0,module: distributions module: autograd triaged module: forward ad,True
78332,TRACK: integral + floating inputs to an op with floating requiring grad result in INTERNAL_ASSERT module: autograd triaged actionable,2022-05-26 02:26:46+00:00,,0,2,module: autograd triaged actionable,True
78274, Memory allocation errors when attempting to initialize a large number of small feed-forward networks in RAM with shared memory despite having enough memory  module: memory usage triaged,2022-05-25 17:08:36+00:00,,0,4,module: memory usage triaged,True
78262,Request for adding the possibility for training on sparse tensors module: sparse triaged,2022-05-25 15:52:10+00:00,,0,3,module: sparse triaged,True
78261,"pytorch-android-lite use its own libfbjni.so, which is not compatible with any other version at all.. module: binaries triaged module: android",2022-05-25 15:34:38+00:00,,0,1,module: binaries triaged module: android,True
78260,[CI] Detect when tests are no longer running from CI module: ci triaged,2022-05-25 15:25:31+00:00,,0,0,module: ci triaged,True
78255,Floating point exception in _conv_depthwise2d triaged module: edge cases,2022-05-25 14:33:08+00:00,,0,1,triaged module: edge cases,True
78253,Any plan to add Noam scheduling? triaged module: LrScheduler,2022-05-25 12:22:13+00:00,,0,2,triaged module: LrScheduler,True
78249,`max_unpool2d` is not deterministic module: numerical-stability module: nn triaged module: determinism module: pooling,2022-05-25 10:24:51+00:00,,1,3,module: numerical-stability module: nn triaged module: determinism module: pooling,True
78248," USE_NATIVE_ARCH flag causes nvcc build failure due to ""'arch=native': expected a number"" module: build triaged",2022-05-25 07:32:46+00:00,,0,1,module: build triaged,True
78210,Performance with MPS on AMD GPUs are worse than CPU module: performance triaged module: mps,2022-05-24 21:31:11+00:00,,0,8,module: performance triaged module: mps,True
78205,DISABLED test_complex_half_reference_testing_as_strided_scatter_cuda_complex32 (__main__.TestCommonCUDA) module: rocm triaged skipped,2022-05-24 20:58:14+00:00,,0,1,module: rocm triaged skipped,False
78201,nn.Sequential causes fx.replace_pattern to not find any match.  triaged module: fx,2022-05-24 20:43:51+00:00,,0,0,triaged module: fx,True
78185,test_to (__main__.TestTorch) fails with multiple gpus module: multi-gpu triaged actionable,2022-05-24 17:22:31+00:00,,0,1,module: multi-gpu triaged actionable,True
78172,Allow specifying pickle module for torch.package enhancement oncall: package/deploy imported,2022-05-24 12:19:47+00:00,,0,0,enhancement oncall: package/deploy imported,False
78170,[chalf] reference_testing: low quality test for fast growing ops triaged module: complex module: half,2022-05-24 11:31:50+00:00,,0,4,triaged module: complex module: half,True
78159,[Optimizer Overlap] Parameter group support oncall: distributed triaged module: ddp,2022-05-24 04:09:52+00:00,,1,0,oncall: distributed triaged module: ddp,True
78158,[Optimizer Overlap] Proper checkpointing support oncall: distributed triaged module: ddp,2022-05-24 04:07:20+00:00,,2,1,oncall: distributed triaged module: ddp,True
78157,[Optimizer Overlap] Custom optimizer registration oncall: distributed triaged module: ddp,2022-05-24 03:55:57+00:00,,1,0,oncall: distributed triaged module: ddp,True
78153,`pack_sequence` crash triaged module: edge cases,2022-05-24 01:53:35+00:00,,0,0,triaged module: edge cases,True
78151,`ctc_loss` will backward crash module: autograd triaged module: edge cases,2022-05-24 01:29:33+00:00,,0,0,module: autograd triaged module: edge cases,True
78143,`baddmm` triggers INTERNAL ASSERT FAILED when input requires grad module: autograd triaged actionable,2022-05-24 00:50:01+00:00,,0,0,module: autograd triaged actionable,True
78141,"`matmul, mm` triggers INTERNAL ASSERT FAILED when input requires grad module: autograd triaged actionable",2022-05-24 00:44:42+00:00,,0,0,module: autograd triaged actionable,True
78133,Enhancements to AliasDB to handle in-place operations oncall: jit,2022-05-23 21:58:37+00:00,,0,0,oncall: jit,False
78131,Segfault in _pad_packed_sequence triaged module: edge cases,2022-05-23 21:11:42+00:00,,0,0,triaged module: edge cases,True
78130,Segfault in _grid_sampler_2d_cpu_fallback triaged module: edge cases,2022-05-23 21:10:35+00:00,,0,0,triaged module: edge cases,True
78129,Segfault in _embedding_bag_forward_only triaged module: edge cases,2022-05-23 21:09:16+00:00,,0,0,triaged module: edge cases,True
78128,Segfault in torch._C._nn.thnn_conv2d triaged module: edge cases,2022-05-23 21:06:50+00:00,,0,0,triaged module: edge cases,True
78127,Segfault in torch._C._nn.reflection_pad2d triaged module: edge cases,2022-05-23 21:05:18+00:00,,0,0,triaged module: edge cases,True
78126,Segfault in max_unpool3d triaged module: edge cases,2022-05-23 21:00:57+00:00,,0,0,triaged module: edge cases,True
78125,Segfault in grid_sampler_3d triaged module: edge cases,2022-05-23 20:57:10+00:00,,0,0,triaged module: edge cases,True
78122,Segfault in bincount triaged module: edge cases,2022-05-23 20:50:38+00:00,,0,1,triaged module: edge cases,True
78109,Doesn't work when register hook to torch.nn.MultiheadAttention.out_proj oncall: transformer/mha,2022-05-23 18:21:24+00:00,,1,9,oncall: transformer/mha,False
78102,[ONNX] Support tensors as scale and zero_point arguments module: onnx triaged OSS contribution wanted onnx-triaged,2022-05-23 17:13:00+00:00,,1,1,module: onnx triaged OSS contribution wanted onnx-triaged,True
78082,RFC: Move functorch into pytorch/pytorch triaged,2022-05-23 14:36:17+00:00,,0,6,triaged,False
78075,torch.multiprocessing.spawn raise PicklingError inside a decorator module: multiprocessing module: serialization triaged,2022-05-23 10:40:18+00:00,,0,1,module: multiprocessing module: serialization triaged,True
78071,[primTorch] item prim can't return a bool properly triaged module: primTorch,2022-05-23 04:01:34+00:00,,0,0,triaged module: primTorch,True
78070,[primTorch] Meta function for item creates a dummy value triaged module: primTorch,2022-05-23 03:46:07+00:00,,0,2,triaged module: primTorch,True
78068,DISABLED test_init_from_local_shards (__main__.TestShardedTensorFromLocalShards) oncall: distributed module: flaky-tests skipped,2022-05-23 00:57:36+00:00,,0,2,oncall: distributed module: flaky-tests skipped,False
78067,Installation on Jetson target board triaged module: jetson,2022-05-23 00:36:25+00:00,,0,1,triaged module: jetson,True
78065,Gamma and Related Functions feature triaged module: special,2022-05-22 21:04:05+00:00,,0,3,feature triaged module: special,True
78064,nn.CosineSimilarity returns value larger than 1 module: nn triaged module: correctness (silent),2022-05-22 19:57:02+00:00,,1,14,module: nn triaged module: correctness (silent),True
78063,Adam is 30% slower than SGD on Apple Metal. module: performance module: optimizer triaged module: mps,2022-05-22 18:08:45+00:00,,0,9,module: performance module: optimizer triaged module: mps,True
78061,Python memory allocator called without holding the GIL when running torchrun under Python debug version triaged oncall: r2p,2022-05-22 12:35:11+00:00,,0,1,triaged oncall: r2p,True
78053,toleranceOverride should override atol and rtol even when explicitly specified in a test module: tests triaged module: primTorch,2022-05-22 00:39:00+00:00,,0,0,module: tests triaged module: primTorch,True
78050,RFC: [primTorch] Stride-agnostic Operator Semantics triaged module: python frontend module: primTorch,2022-05-21 21:49:30+00:00,,0,13,triaged module: python frontend module: primTorch,True
78047,DDP multi host with single GPU each.  oncall: distributed triaged,2022-05-21 20:07:36+00:00,,0,3,oncall: distributed triaged,True
78044,FFT operators are not supported on MPS device high priority triaged module: complex module: fft topic: new features module: mps,2022-05-21 16:07:39+00:00,,0,46,high priority triaged module: complex module: fft topic: new features module: mps,True
78034,"Error occurred , when compile source code setting  BUILD_CAFFE2=ON module: build caffe2 triaged",2022-05-21 04:38:29+00:00,,0,1,module: build caffe2 triaged,True
78018,Three memory copies of every dataloader cpu tensor module: multiprocessing module: dataloader module: cuda triaged enhancement,2022-05-20 21:46:01+00:00,,0,3,module: multiprocessing module: dataloader module: cuda triaged enhancement,True
77981,Override sym_sizes to create LTC IR for SymIntNode triaged lazy,2022-05-20 17:28:14+00:00,,1,1,triaged lazy,True
77973,"forward-mode support for ""logically composite"" operators triaged module: derivatives module: forward ad",2022-05-20 15:44:10+00:00,,0,1,triaged module: derivatives module: forward ad,True
77967,Inference Tensors should not be allowed to hold `grad_fn` triaged inference mode,2022-05-20 15:15:44+00:00,,0,0,triaged inference mode,True
77963,`logaddexp2` fails to backward module: autograd triaged module: edge cases,2022-05-20 14:46:58+00:00,,0,0,module: autograd triaged module: edge cases,True
77962,Operating on boolean torch tensor and numpy array casts to `unit8` triaged module: numpy module: type promotion module: boolean tensor,2022-05-20 13:42:42+00:00,,0,1,triaged module: numpy module: type promotion module: boolean tensor,True
77961,Exporting the operator isinstance to ONNX opset version 13 is not supported module: onnx triaged onnx-needs-info,2022-05-20 12:57:34+00:00,,0,8,module: onnx triaged onnx-needs-info,False
77955,NaN tensor values problem for GTX16xx users  (no problem on other devices) module: cudnn triaged,2022-05-20 08:52:39+00:00,,0,1,module: cudnn triaged,True
77951,`topk` returns different results with the same input twice in cuda module: cuda triaged module: python frontend,2022-05-20 07:48:48+00:00,,0,0,module: cuda triaged module: python frontend,True
77946,[failing test] test_foreach::test_binary_op_scalarlist_fastpath triaged,2022-05-20 06:16:59+00:00,,0,5,triaged,True
77939,Fails to compile with GCC 12.1.0 module: build triaged,2022-05-20 04:14:42+00:00,,0,12,module: build triaged,True
77901,Heap corruption in slow_conv_transpose3d module: convolution triaged module: edge cases,2022-05-19 21:23:21+00:00,,0,0,module: convolution triaged module: edge cases,True
77900,Floating point exception in slow_conv3d module: convolution triaged module: edge cases,2022-05-19 21:23:17+00:00,,0,0,module: convolution triaged module: edge cases,True
77899,Floating point exception in native_channel_shuffle triaged release notes: python_frontend,2022-05-19 21:23:13+00:00,,0,0,triaged release notes: python_frontend,True
77894,Floating point exception in channel_shuffle triaged release notes: python_frontend,2022-05-19 21:22:46+00:00,,0,0,triaged release notes: python_frontend,True
77893,Segmentation fault in _remove_batch_dim needs reproduction triaged module: vmap,2022-05-19 21:22:41+00:00,,0,1,needs reproduction triaged module: vmap,True
77880,Make the appropriate backend `DimensionNode` visible to LTC core triaged lazy,2022-05-19 19:39:03+00:00,,1,1,triaged lazy,True
77869,Throw warning if python optimise flags are enabled triaged module: python frontend,2022-05-19 18:26:18+00:00,,0,2,triaged module: python frontend,True
77844,Conv2D with large different number of input and output channels gives a CUDNN_STATUS_INTERNAL_ERROR module: cudnn triaged,2022-05-19 10:30:25+00:00,,0,0,module: cudnn triaged,True
77842,ONNX export of CumSum produces different data type module: onnx triaged onnx-triaged,2022-05-19 10:02:29+00:00,,0,1,module: onnx triaged onnx-triaged,True
77840,Legacy model format is not supported on mobile oncall: mobile,2022-05-19 08:45:31+00:00,,0,0,oncall: mobile,False
77839,BUG: reference count leak when using `THPLayout_New` and `THPMemoryFormat_New` (static analyzer reports) module: memory usage triaged module: python frontend,2022-05-19 08:40:38+00:00,,0,1,module: memory usage triaged module: python frontend,True
77838,Sporadic convolution error with dilation=0 module: convolution triaged,2022-05-19 08:28:27+00:00,,0,0,module: convolution triaged,True
77837,TorchScript attempts to compile dead branch of torch.jit.is_scripting oncall: jit,2022-05-19 07:59:31+00:00,,0,2,oncall: jit,False
77821,cannot convert to channels last format for conv2d conv3d hybrid model module: convolution triaged,2022-05-19 02:45:17+00:00,,0,4,module: convolution triaged,True
77818,torch.nn.Conv3D on MPS backend triaged topic: new features module: mps,2022-05-19 01:46:03+00:00,,0,16,triaged topic: new features module: mps,False
77814,"`addmv, mv` will trigger INTERNAL ASSERT FAILED when input requiring grad module: autograd triaged actionable",2022-05-19 01:35:36+00:00,,0,0,module: autograd triaged actionable,True
77808,`Could not start gRPC server` flakiness in XLA tests triaged module: xla,2022-05-19 00:16:08+00:00,,0,10,triaged module: xla,True
77801,`torch.utils.benchmark.examples.blas_compare` can not be parsed by Python-3.7 runtime triaged module: benchmark,2022-05-18 22:38:06+00:00,,0,0,triaged module: benchmark,True
77764,General MPS op coverage tracking issue feature triaged module: mps,2022-05-18 18:12:47+00:00,,0,936,feature triaged module: mps,True
77742,strange behaviour in torch.div high priority triaged,2022-05-18 14:09:33+00:00,,1,2,high priority triaged,True
77738,net_observer_reporter_print.h missing module: build triaged,2022-05-18 13:35:12+00:00,,0,0,module: build triaged,True
77737,"torchrun leads to `ModuleNotFoundError: No module named 'tensorboard'`, but python -m torch.distributed.launch is ok triaged oncall: r2p",2022-05-18 12:59:24+00:00,,0,4,triaged oncall: r2p,True
77736,TimeSeriesDataset retrieve columns feature triaged,2022-05-18 12:40:55+00:00,,0,2,feature triaged,False
77733,Adding Vulkan Support  triaged module: vulkan,2022-05-18 08:30:33+00:00,,0,4,triaged module: vulkan,True
77731,complex abs strides are wrong on empty tensors and tensors with 1 dimension triaged module: complex module: primTorch,2022-05-18 06:40:41+00:00,,0,2,triaged module: complex module: primTorch,True
77724,FSDP: enhanced shared parameter support oncall: distributed triaged module: fsdp,2022-05-18 04:02:58+00:00,,0,3,oncall: distributed triaged module: fsdp,True
77675,PrimTorch refs do not match argument naming with their PyTorch counterparts triaged module: primTorch,2022-05-17 19:46:31+00:00,,0,1,triaged module: primTorch,True
77668,Extend BC test to test for __torch_function__ overridability triaged module: __torch_function__,2022-05-17 18:30:14+00:00,,0,0,triaged module: __torch_function__,True
77646,Werror=nonnull in dataloader.cpp (part of tests) module: dataloader triaged,2022-05-17 13:52:37+00:00,,0,3,module: dataloader triaged,True
77614,PyTorch fails to build on gcc 12 due to gloo module: build triaged module: third_party,2022-05-17 00:20:36+00:00,,0,2,module: build triaged module: third_party,True
77589,How to handle __module__  attribute for Public API bindings module: tests triaged,2022-05-16 20:40:52+00:00,,2,1,module: tests triaged,True
77583,FSDP: test mixed precision with checkpoint oncall: distributed triaged module: fsdp,2022-05-16 18:42:02+00:00,,0,0,oncall: distributed triaged module: fsdp,True
77576,`stateless.functional_call` doesn't work with `nn.DataParallel` module: nn triaged module: data parallel actionable,2022-05-16 18:14:58+00:00,,0,3,module: nn triaged module: data parallel actionable,True
77558,Investigate sharded gradscaler OOM on CPU workloads oncall: distributed module: fsdp,2022-05-16 16:06:16+00:00,,2,0,oncall: distributed module: fsdp,False
77552,Functional Jacobian does not work with Torchdiffeq module: autograd triaged,2022-05-16 15:24:53+00:00,,0,0,module: autograd triaged,True
77548,lintrunner doesn't give good error message suggesting lintrunner init module: lint triaged,2022-05-16 14:42:03+00:00,,0,1,module: lint triaged,True
77546,Build check for AVX512 fails with AMD CPU and march=native module: build triaged module: vectorization,2022-05-16 14:30:43+00:00,,0,0,module: build triaged module: vectorization,True
77544,Modernize LoggingTensorMode triaged module: __torch_dispatch__,2022-05-16 13:40:09+00:00,,0,1,triaged module: __torch_dispatch__,True
77538,Failed to run on iOS - Couldn't find an operator for `aten::conv1d` triaged module: ios,2022-05-16 13:04:25+00:00,,0,3,triaged module: ios,True
77537,batch Kronecker product  triaged enhancement,2022-05-16 12:31:30+00:00,,0,5,triaged enhancement,True
77529,softmarginloss should use `log1p` and has an incorrect out= behaviour. module: loss triaged,2022-05-16 10:18:45+00:00,,0,0,module: loss triaged,True
77527,CUDA: Illegal memory access in `torch.linalg.solve()` high priority module: cuda triaged module: linear algebra,2022-05-16 08:24:55+00:00,,0,12,high priority module: cuda triaged module: linear algebra,True
77523,DDP window TCP bug [socket.cpp:558] [c10d] The client socket has failed to  oncall: distributed,2022-05-16 07:49:09+00:00,,0,10,oncall: distributed,True
77515,Inplace Bool API + `sum` will trigger INTERNAL ASSERT FAILED module: autograd triaged module: edge cases,2022-05-16 00:52:17+00:00,,0,0,module: autograd triaged module: edge cases,True
77514,`max_pool1d` can succeed when padding is negative for tensor requiring grad module: nn triaged actionable module: pooling module: edge cases,2022-05-16 00:41:37+00:00,,0,2,module: nn triaged actionable module: pooling module: edge cases,True
77478,Standalone unittests for checkpoint_wrapper oncall: distributed module: bootcamp triaged better-engineering module: fsdp,2022-05-14 07:58:15+00:00,,1,0,oncall: distributed module: bootcamp triaged better-engineering module: fsdp,True
77463,conda CPU installation for LTS fails with UnsatisfiableError triaged module: lts,2022-05-13 22:52:28+00:00,,0,5,triaged module: lts,True
77454,More clarity in doc for `torch.cuda.Event.record`? module: docs triaged,2022-05-13 21:45:13+00:00,,0,0,module: docs triaged,False
77439,FSDP: Mixed precision should not cast ignored buffers oncall: distributed triaged module: fsdp,2022-05-13 19:01:28+00:00,,0,0,oncall: distributed triaged module: fsdp,True
77415,Suboptimal error message - nn.Linear with double argument module: error checking triaged,2022-05-13 13:51:41+00:00,,0,3,module: error checking triaged,True
77413,Process hangs after calling conv2d() in pytorch 1.11.0 with CUDA 11.3 module: cudnn module: convolution triaged module: deadlock,2022-05-13 13:02:38+00:00,,0,0,module: cudnn module: convolution triaged module: deadlock,True
77411,Allow force building with/without AVX module: build triaged,2022-05-13 10:53:31+00:00,,0,0,module: build triaged,True
77410,torch.onnx.export does not track Tensor.data.size() for dynamic axes module: onnx triaged onnx-triaged release notes: onnx,2022-05-13 10:53:02+00:00,,0,1,module: onnx triaged onnx-triaged release notes: onnx,False
77397,Large numerical inconsistency for `torch.einsum` on RTX30 series GPU. module: cuda triaged module: tf32,2022-05-13 01:27:22+00:00,,0,4,module: cuda triaged module: tf32,True
77380,microbenchmark-style tests module: tests triaged,2022-05-12 21:55:17+00:00,,0,0,module: tests triaged,True
77374,[distributed] c10d crashing on assert oncall: distributed,2022-05-12 21:07:39+00:00,,0,4,oncall: distributed,False
77354,"outputs_[i]->uses().empty()INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1646755853042/work/torch/csrc/jit/ir/ir.cpp"":1314, please report a bug to PyTorch.  oncall: jit",2022-05-12 15:49:38+00:00,,1,0,oncall: jit,True
77342,DISABLED test_ddp_profiling_autograd_profiler (__main__.TestDistBackendWithSpawn) oncall: distributed module: flaky-tests skipped,2022-05-12 06:47:01+00:00,,1,4,oncall: distributed module: flaky-tests skipped,False
77340,Disable issue doesn't disable multiple dtypes correctly module: ci triaged,2022-05-12 06:37:01+00:00,,1,1,module: ci triaged,True
77332,"wrong overload resolved for `torch.mul(x, 4)` in `__torch_dispatch__` triaged module: __torch_dispatch__",2022-05-12 06:14:45+00:00,,0,3,triaged module: __torch_dispatch__,True
77317,DISABLED test_DistributedDataParallel (__main__.TestDistBackendWithSpawn) oncall: distributed module: flaky-tests skipped,2022-05-12 00:57:50+00:00,,1,6,oncall: distributed module: flaky-tests skipped,False
77284,non-rentrant checkpointing uses same memory as non-checkpointed code high priority oncall: distributed module: checkpoint triaged,2022-05-11 19:58:59+00:00,,1,0,high priority oncall: distributed module: checkpoint triaged,True
77265,Subclasses with unwrapping `__torch_dispatch__` impls as parameters module: nn triaged module: __torch_dispatch__ tensor subclass,2022-05-11 16:07:31+00:00,,0,2,module: nn triaged module: __torch_dispatch__ tensor subclass,True
77256,CrossEntropyLoss computes SoftMax always across the second dimension module: nn module: loss triaged actionable,2022-05-11 15:10:08+00:00,,0,5,module: nn module: loss triaged actionable,True
77253,lintrunner not working module: lint triaged,2022-05-11 14:38:54+00:00,,0,4,module: lint triaged,True
77251,The codegen unconditionaly generate code even when it is not going to be used triaged module: codegen,2022-05-11 14:21:33+00:00,,0,1,triaged module: codegen,True
77241,libtorch1.8 torch::sigmoid is wrong module: cpp triaged module: jetson,2022-05-11 09:27:06+00:00,,0,0,module: cpp triaged module: jetson,True
77233,`tensordot` does check the dtype of empty tensor triaged module: linear algebra,2022-05-11 05:41:41+00:00,,0,0,triaged module: linear algebra,True
77232,Write decomposition conditionals in a way that leads to simpler shape expressions triaged module: primTorch,2022-05-11 05:14:32+00:00,,1,5,triaged module: primTorch,True
77231,`torch.scatter_add` will succeed when the `index` is a complex tensor triaged module: scatter & gather ops,2022-05-11 05:12:17+00:00,,0,10,triaged module: scatter & gather ops,True
77230,fast `gradcheck` fails when outputs that do not require grad precede outputs that do module: autograd triaged module: linear algebra actionable,2022-05-11 04:11:59+00:00,,0,7,module: autograd triaged module: linear algebra actionable,True
77223,torch.ops.aten.ceil(1.5) returns Tensor rather than scalar triaged module: type promotion,2022-05-11 02:33:59+00:00,,0,1,triaged module: type promotion,True
77216,[primTorch] Reduction references don't return views consistent with their original operators triaged module: primTorch,2022-05-11 01:15:47+00:00,,0,4,triaged module: primTorch,True
77200,[RFC] Allow device override during Tensor unpickling without torch.load module: pickle module: serialization triaged enhancement,2022-05-10 21:28:32+00:00,,0,1,module: pickle module: serialization triaged enhancement,True
93753,Don't populate f_locals to check guards triaged enhancement oncall: pt2 module: dynamo,2022-05-10 19:05:31+00:00,,1,3,triaged enhancement oncall: pt2 module: dynamo,False
77184,Undefined symbol error when compiling and loading C++ extension module: cpp-extensions module: cpp triaged,2022-05-10 18:17:29+00:00,,0,0,module: cpp-extensions module: cpp triaged,True
77176,Improve the overall design of MPSGraphCache triaged enhancement module: backend module: mps,2022-05-10 17:09:39+00:00,,1,1,triaged enhancement module: backend module: mps,True
77171,Allow users to express fused matmul/bias/relu triaged enhancement needs research module: python frontend,2022-05-10 16:34:30+00:00,,0,3,triaged enhancement needs research module: python frontend,True
77170,Move the MPSGuardImpl to inherit from NoOpDeviceGuardImpl triaged module: backend module: mps,2022-05-10 16:33:17+00:00,,0,0,triaged module: backend module: mps,True
77167,nn.functional.pad accepts bool values but raises internal assert when converted to JIT oncall: jit,2022-05-10 15:11:31+00:00,,0,1,oncall: jit,True
77166,"torch.cholesky has been deprecated in favour of torch.linalg.cholesky. However, torch.cholesky_inverse remains as is. It should also be moved to torch.linalg triaged module: linear algebra",2022-05-10 14:31:13+00:00,,0,3,triaged module: linear algebra,True
77165,Automate cleanup of header includes module: internals triaged,2022-05-10 14:24:50+00:00,,1,2,module: internals triaged,True
77161,SymInt shouldn't be in dynamic_type.h module: cpp triaged,2022-05-10 11:44:39+00:00,,0,0,module: cpp triaged,True
77159,"A somewhat cryptic error message (for newcomers) - ""Cannot re-initialize CUDA in forked subprocess"" - report and suggestion for a possible solution module: dataloader triaged",2022-05-10 09:32:18+00:00,,0,15,module: dataloader triaged,True
77155,FSDP: ability to ignore parameters high priority triage review oncall: distributed module: fsdp,2022-05-10 09:09:15+00:00,,0,5,high priority triage review oncall: distributed module: fsdp,True
77154,Distributed Weighted Sampler. oncall: distributed,2022-05-10 08:51:35+00:00,,0,2,oncall: distributed,False
77144,Add Tensor compare support for MPS backend triaged module: backend module: testing module: mps,2022-05-10 05:31:46+00:00,,2,4,triaged module: backend module: testing module: mps,True
77141,[FSDP] `ignored_modules` follow-ups oncall: distributed module: fsdp,2022-05-10 03:48:36+00:00,,0,1,oncall: distributed module: fsdp,False
77140,"torch.randperm uses too much cpu, but not efficient. module: performance triaged module: random module: multithreading",2022-05-10 03:46:41+00:00,,0,1,module: performance triaged module: random module: multithreading,True
77118,__name__ on OpOverload should not contain period triaged module: __torch_dispatch__,2022-05-10 00:23:41+00:00,,0,2,triaged module: __torch_dispatch__,True
77113,broadcast_object_list with GPU tensors can lead to deadlock on PyTorch CI machines high priority triage review oncall: distributed module: fsdp,2022-05-09 23:36:46+00:00,,0,0,high priority triage review oncall: distributed module: fsdp,True
77067,Unable to continue adding modules to `nn.Sequential` after using `del` method module: nn triaged enhancement,2022-05-09 10:31:19+00:00,,0,3,module: nn triaged enhancement,True
77053,Incorrect documentation in ``gumble_softmax`` function. module: docs module: nn triaged,2022-05-08 19:51:15+00:00,,0,1,module: docs module: nn triaged,True
77052,Building from source results in broken __version__ module: binaries module: build triaged,2022-05-08 18:30:52+00:00,,1,6,module: binaries module: build triaged,True
77050,ENORMOUS OVERHEAD from mp.get_context('spawn') module: performance module: multiprocessing triaged module: POWER,2022-05-08 15:57:54+00:00,,0,5,module: performance module: multiprocessing triaged module: POWER,True
77049,Peak GPU-memory usage extremely huge when sorting with torch.sort module: cuda module: memory usage triaged module: sorting and selection,2022-05-08 15:35:39+00:00,,0,7,module: cuda module: memory usage triaged module: sorting and selection,True
77047,Stop calling sizes/numel/dim/is_contiguous on undefined tensors module: internals triaged,2022-05-08 12:34:14+00:00,,0,0,module: internals triaged,True
77046,torch.stack test_conj_view and test_neg_view are failing after 77043 triaged module: complex module: viewing and reshaping module: primTorch,2022-05-08 08:28:08+00:00,,0,0,triaged module: complex module: viewing and reshaping module: primTorch,True
77027,Kill use of TensorImpl::ShareExternalPointer in torch/csrc/jit/tensorexpr/external_functions.cpp oncall: jit,2022-05-07 17:18:44+00:00,,0,0,oncall: jit,False
77016,Where is fx2trt fx to tensorrt tool? triaged module: fx,2022-05-07 08:43:04+00:00,,0,0,triaged module: fx,True
76972,Display EC2 information module: ci triaged,2022-05-06 17:37:27+00:00,,1,0,module: ci triaged,True
76966,[bug] `NATIVE` and `OMP` `parallel_for` implementations are inconsistent. triaged module: openmp module: multithreading,2022-05-06 16:19:44+00:00,,0,0,triaged module: openmp module: multithreading,True
76962,DISABLED test_comprehensive_linalg_ldl_factor_ex_cuda (__main__.TestDecompCUDA) triaged module: linear algebra skipped,2022-05-06 14:25:09+00:00,,0,1,triaged module: linear algebra skipped,False
76960,"When using Rsqrt, the output of the 1/x process is very likely to have nan/inf needs reproduction triaged",2022-05-06 13:55:35+00:00,,0,0,needs reproduction triaged,True
76959,"When using Lambda, the output of the 1/x process is very likely to have nan/inf needs reproduction triaged",2022-05-06 13:53:35+00:00,,0,0,needs reproduction triaged,True
76956,GlobalAvgPool2d causes the inconsistency of output between frameworks needs reproduction triaged,2022-05-06 12:56:36+00:00,,0,0,needs reproduction triaged,True
76955,GaussianNoise causes the inconsistency of output between frameworks needs reproduction triaged,2022-05-06 12:50:31+00:00,,0,1,needs reproduction triaged,True
76954,ReduceSum causes the inconsistency of output between frameworks needs reproduction triaged,2022-05-06 12:47:55+00:00,,0,1,needs reproduction triaged,True
76944,primTorch references don't handle scalar x scalar inputs correctly triaged module: type promotion module: primTorch,2022-05-06 06:22:46+00:00,,0,0,triaged module: type promotion module: primTorch,True
76936,"Private API for accessing all ""internal"" attributes on Tensors module: internals triaged",2022-05-06 02:07:57+00:00,,0,0,module: internals triaged,True
76933,NVFuser opinfos - check for CudaFusionGroup in the graph triaged module: nvfuser,2022-05-06 01:03:47+00:00,,0,0,triaged module: nvfuser,True
76927,Can't pickle model torch._C._distributed_c10d.ProcessGroupNCCL' object  oncall: distributed module: c10d module: ddp,2022-05-05 23:54:13+00:00,,0,1,oncall: distributed module: c10d module: ddp,True
76924,[RFC] Upstream current implementation of ssd_offload from fairscale FSDP to Torch Distributed FSDP oncall: distributed module: fsdp,2022-05-05 22:49:16+00:00,,0,3,oncall: distributed module: fsdp,False
76920,Avoid Self-loops on Module Creation module: nn triaged,2022-05-05 21:04:55+00:00,,0,3,module: nn triaged,True
76913,Pytorch return TCPStore( RuntimeError: Connection reset by peer)  oncall: distributed module: c10d,2022-05-05 20:07:05+00:00,,0,7,oncall: distributed module: c10d,False
76906,torch.nn.functional.linear sometimes incorrectly accepts arguments of the different type module: nn triaged module: type promotion,2022-05-05 18:39:19+00:00,,0,0,module: nn triaged module: type promotion,True
76891,Unwanted behavior with some in-place operations on CPU triaged module: viewing and reshaping,2022-05-05 11:52:39+00:00,,0,1,triaged module: viewing and reshaping,True
76887,Multiprocessing DataLoader hangs on exception inside iterator when using a simple queue and a producer thread module: multiprocessing module: dataloader triaged,2022-05-05 09:32:36+00:00,,0,1,module: multiprocessing module: dataloader triaged,True
76885,EmbeddingBag: Does CUDA calculate error in EmbeddingBag forward when include_last_offset=True ? module: nn triaged module: embedding,2022-05-05 09:03:39+00:00,,1,7,module: nn triaged module: embedding,True
76881,RecursionError when running torch.jit.script inside JitTestCase oncall: jit,2022-05-05 06:18:00+00:00,,0,0,oncall: jit,True
76871,PrimTorch binary refs do not handle CUDA + CPU scalar tensors correctly triaged module: primTorch,2022-05-05 03:13:15+00:00,,1,3,triaged module: primTorch,True
76865,Object-base collectives create tensors at unexpected devices high priority oncall: distributed better-engineering module: c10d,2022-05-05 00:17:03+00:00,,1,3,high priority oncall: distributed better-engineering module: c10d,True
76856,Feature requests for optimizer overlapping high priority triage review oncall: distributed module: ddp module: fsdp,2022-05-04 22:49:10+00:00,,1,1,high priority triage review oncall: distributed module: ddp module: fsdp,True
76853,Inconsistent results between Pow and Float Pow with their numpy references for complex types triaged module: complex module: NaNs and Infs,2022-05-04 22:32:54+00:00,,0,2,triaged module: complex module: NaNs and Infs,True
76844,Unify torch.ops argument parsing code with PythonArgParser triaged module: __torch_dispatch__ module: python frontend,2022-05-04 20:53:06+00:00,,0,0,triaged module: __torch_dispatch__ module: python frontend,True
76838,Windows CUDA TTS tracking task module: ci triaged,2022-05-04 18:54:24+00:00,,1,18,module: ci triaged,True
76827,TYPEIGNORE lint run locally disagrees with CI module: lint triaged module: macos,2022-05-04 17:57:11+00:00,,0,0,module: lint triaged module: macos,True
76807,There is a bug with latest stable torch version and the following Nightly versions related to `optimize_for_mobile` oncall: mobile,2022-05-04 10:31:31+00:00,,0,5,oncall: mobile,True
76806,torch.Tensor.__rdiv__ long x scalar float type promotion is incorrect triaged module: type promotion module: primTorch,2022-05-04 10:03:31+00:00,,0,0,triaged module: type promotion module: primTorch,True
76804,"torch.add bool x bool allows integer alpha, inconsistent with other dtype type checking triaged module: primTorch",2022-05-04 09:53:27+00:00,,0,0,triaged module: primTorch,True
76798,`gradcheck` for `torch.solve` may trigger INTERNAL ASSERT FAILED needs reproduction module: autograd triaged,2022-05-04 08:22:17+00:00,,0,7,needs reproduction module: autograd triaged,True
76786,"`cumprod, prod` will backward fail if `dtype` argument is different than the dtype of input tensor module: autograd triaged module: complex complex_autograd",2022-05-04 02:35:22+00:00,,0,0,module: autograd triaged module: complex complex_autograd,True
76785,"`addr, baddmm, dist, l1_loss` will backward fail when input tensors have different dtypes module: autograd triaged module: complex actionable complex_autograd",2022-05-04 02:31:27+00:00,,0,3,module: autograd triaged module: complex actionable complex_autograd,True
76783,`gradcheck` fails for `torch.trace` module: autograd triaged actionable,2022-05-04 02:02:09+00:00,,0,0,module: autograd triaged actionable,True
76780,`gradcheck` should support the comparison of NaN module: autograd triaged enhancement actionable,2022-05-04 00:47:35+00:00,,0,0,module: autograd triaged enhancement actionable,True
76779,"Strange warning from `matmul(..., out=...)` triaged module: linear algebra",2022-05-04 00:31:34+00:00,,0,7,triaged module: linear algebra,True
76778,`torch.addmv` backward fails module: autograd triaged module: complex actionable,2022-05-04 00:28:15+00:00,,0,3,module: autograd triaged module: complex actionable,True
76775,[JIT] Infinite RecursionError with self-referential models (also affects `__repr__`)!! oncall: jit,2022-05-03 23:37:28+00:00,,0,1,oncall: jit,True
76774,Support Positional-only Arguments in JIT oncall: jit,2022-05-03 22:44:42+00:00,,0,1,oncall: jit,False
76772,[typing] distribution.lazy_property is not typed module: distributions triaged,2022-05-03 20:47:44+00:00,,0,0,module: distributions triaged,True
76760,hanging process with init_process_group(backend='mpi') cannot be killed  oncall: distributed,2022-05-03 18:27:04+00:00,,0,3,oncall: distributed,False
76755,Quantization in Libtorch oncall: quantization low priority triaged,2022-05-03 17:59:40+00:00,,0,3,oncall: quantization low priority triaged,True
76751,Default repr of __get__ methods in __torch_function__ is bad triaged module: __torch_function__,2022-05-03 17:11:52+00:00,,0,3,triaged module: __torch_function__,True
76750,Bug in dataloader iterator found by mypy module: dataloader triaged,2022-05-03 16:58:25+00:00,,0,1,module: dataloader triaged,True
76747,[checkpoint] Stable file format for checkpoints oncall: distributed triaged sharded_tensor,2022-05-03 16:46:52+00:00,,0,0,oncall: distributed triaged sharded_tensor,True
76745,[checkpoint] Handle overlapping storage during save and load oncall: distributed triaged sharded_tensor,2022-05-03 16:40:09+00:00,,0,0,oncall: distributed triaged sharded_tensor,True
76743,Supporting torch.tensor.apply_ over GPU module: cuda triaged,2022-05-03 16:12:50+00:00,,0,3,module: cuda triaged,True
76734,__torch_function__ callers should always pass kwargs triaged module: __torch_function__,2022-05-03 14:08:41+00:00,,0,0,triaged module: __torch_function__,True
76733,test_python_reference_meta_functions takes too long to run module: tests triaged,2022-05-03 13:57:53+00:00,,0,5,module: tests triaged,True
76732,Adding Polyloss to `torch` module: nn module: loss triaged,2022-05-03 13:47:21+00:00,,0,1,module: nn module: loss triaged,True
76730,[JIT] magic methods do not work after reloading model oncall: jit,2022-05-03 13:01:32+00:00,,0,0,oncall: jit,True
76726,Torchscript model Runtime Error after quantization oncall: jit oncall: quantization low priority triaged,2022-05-03 08:13:01+00:00,,0,15,oncall: jit oncall: quantization low priority triaged,True
76709,`reshape` for distributions. module: distributions feature triaged,2022-05-03 00:27:53+00:00,,0,6,module: distributions feature triaged,True
76705,"Torch `x += y.bmm(z)` is faster than `x.baddbmm_(y, z)` module: performance triaged module: linear algebra",2022-05-02 23:50:13+00:00,,0,2,module: performance triaged module: linear algebra,True
76689,Performance bad on ARM AArch64 for PyTorch C++ module: build triaged module: arm,2022-05-02 20:22:07+00:00,,0,4,module: build triaged module: arm,True
76686,[torch::deploy] Remove `manager_` from the constructor and deconstructor of `InterpreterSession` module: deploy oncall: package/deploy imported,2022-05-02 19:02:27+00:00,,1,1,module: deploy oncall: package/deploy imported,False
76685,[torch::deploy] move create_movable to interpreter_manager oncall: package/deploy imported,2022-05-02 19:00:53+00:00,,1,0,oncall: package/deploy imported,False
76683,[torch::deploy] remove reliance on manager_ for unload oncall: package/deploy imported,2022-05-02 18:45:35+00:00,,1,0,oncall: package/deploy imported,False
76682,[torch::deploy] Remove manager_ from AquireSession oncall: package/deploy imported,2022-05-02 18:40:03+00:00,,1,0,oncall: package/deploy imported,False
76672,C++ CUDA assign existing memory to forward method. oncall: jit,2022-05-02 17:28:34+00:00,,0,1,oncall: jit,False
76659,Remove all docstrings when python is running in optimization mode module: performance module: docs triaged better-engineering module: python frontend,2022-05-02 15:02:34+00:00,,0,7,module: performance module: docs triaged better-engineering module: python frontend,True
76656,Clarify dependency on NumPy (related to maskedtensor?) triaged,2022-05-02 13:34:42+00:00,,0,5,triaged,True
76655,[feature request] no-param sort to exploit parallelization module: performance module: cpu triaged module: multithreading module: sorting and selection,2022-05-02 12:50:40+00:00,,0,1,module: performance module: cpu triaged module: multithreading module: sorting and selection,True
76654,`torch.sort` does not exploit parallelization when invoked without the `dim` parameter. module: performance feature module: cpu triaged module: sorting and selection,2022-05-02 12:22:21+00:00,,0,5,module: performance feature module: cpu triaged module: sorting and selection,True
76649,Add `ldl_unpack` functionality feature triaged module: linear algebra,2022-05-02 07:27:57+00:00,,0,2,feature triaged module: linear algebra,True
76646,`torch.nn.HuberLoss` backwards unexpectedly fail module: nn module: loss triaged module: type promotion,2022-05-02 02:42:47+00:00,,0,0,module: nn module: loss triaged module: type promotion,True
76644,`torch.smm` backward fail with strange error message module: sparse triaged,2022-05-02 02:09:23+00:00,,0,2,module: sparse triaged,True
76643,Pytorch can't process special unicode needs reproduction module: serialization triaged,2022-05-02 01:25:56+00:00,,0,2,needs reproduction module: serialization triaged,True
76638,Design API for accessing sparse tensor indices module: sparse triaged,2022-05-01 20:32:20+00:00,,0,9,module: sparse triaged,True
76636,WeightNorm: Add reset_parameters Linear override feature module: nn triaged module: norms and normalization,2022-05-01 17:33:08+00:00,,0,2,feature module: nn triaged module: norms and normalization,True
76632,RestrictPtrTraits in CUDA potentially has no effect. module: cuda triaged better-engineering,2022-05-01 08:56:57+00:00,,0,2,module: cuda triaged better-engineering,True
76627,`Tensor.logit`'s signature in doc misses `eps` argument module: docs triaged module: python frontend,2022-05-01 05:42:16+00:00,,0,0,module: docs triaged module: python frontend,True
76618,pylint segfault module: lint triaged,2022-04-30 12:38:33+00:00,,0,0,module: lint triaged,True
76617,Improve error message for `unfold` when generating tensor with negative dimension triaged actionable module: shape checking module: viewing and reshaping,2022-04-30 11:49:08+00:00,,0,2,triaged actionable module: shape checking module: viewing and reshaping,True
76613,MeanVarianceNormalization feature module: nn triaged module: norms and normalization,2022-04-30 03:05:00+00:00,,0,2,feature module: nn triaged module: norms and normalization,True
76612,OpInfo incorrectly advertises lu_solve support on CUDA even when compiled without magma module: tests triaged better-engineering module: linear algebra,2022-04-30 02:20:01+00:00,,0,1,module: tests triaged better-engineering module: linear algebra,True
76611,OpInfo CUDA bfloat16 support detection is buggy module: tests triaged better-engineering,2022-04-30 02:15:26+00:00,,0,4,module: tests triaged better-engineering,True
76609,AttributeError: '_thread._local' object has no attribute 'rel_tol' (cannot use TestCase.assertEqual from other threads) module: tests triaged better-engineering,2022-04-30 01:41:17+00:00,,0,2,module: tests triaged better-engineering,True
76585,`torch.linalg.cond` has different results for tensor requiring autograd module: autograd triaged module: linear algebra,2022-04-29 11:02:16+00:00,,0,0,module: autograd triaged module: linear algebra,True
76580,Random Generator for Dropout feature triaged module: random,2022-04-29 05:07:48+00:00,,0,2,feature triaged module: random,True
76578,[JIT][Autocast] Batchnorm folding pass during freezing doesn't preserve types oncall: jit,2022-04-29 02:37:09+00:00,,1,0,oncall: jit,True
76571,torch.unique() nondeterministic behavior on nan inputs (on GPU) high priority module: cuda triaged module: NaNs and Infs module: determinism module: sorting and selection,2022-04-29 00:29:32+00:00,,0,5,high priority module: cuda triaged module: NaNs and Infs module: determinism module: sorting and selection,True
76558,[jiterator] perf regression when jiterating few ops for complex dtype module: performance triaged module: complex module: jiterator,2022-04-28 19:43:46+00:00,,0,2,module: performance triaged module: complex module: jiterator,True
76557,Pip packaging and publishing improvements in pytorch wheels for better integration with poetry module: binaries triaged,2022-04-28 19:40:35+00:00,,0,11,module: binaries triaged,True
76555,[numpy] Missing Tensor-Scalar support for multiple binary ops  triaged module: numpy module: python frontend,2022-04-28 19:32:25+00:00,,0,5,triaged module: numpy module: python frontend,True
93749,Better handling for exec triaged enhancement oncall: pt2 module: dynamo,2022-04-28 17:28:19+00:00,,0,1,triaged enhancement oncall: pt2 module: dynamo,False
76543,Segfault in ~PyFunctionPreHook needs reproduction module: crash module: autograd triaged,2022-04-28 17:26:02+00:00,,0,18,needs reproduction module: crash module: autograd triaged,True
76532,Add ability to add custom suffixes to tensor repr module: printing triaged enhancement needs design tensor subclass module: python frontend,2022-04-28 14:10:54+00:00,,0,5,module: printing triaged enhancement needs design tensor subclass module: python frontend,True
76528,Discrepancy in einsum when done in batch vs non-batch triaged module: linear algebra module: tf32,2022-04-28 10:49:49+00:00,,0,5,triaged module: linear algebra module: tf32,True
76527,Non target rank receives result of 'reduce' op when backend is 'gloo' oncall: distributed,2022-04-28 10:47:38+00:00,,0,0,oncall: distributed,False
76522,`torch.clamp` does not distribute gradients as element-wise`min/max` do module: autograd triaged actionable,2022-04-28 04:40:04+00:00,,0,10,module: autograd triaged actionable,True
76519,Fix layout of masked output when all sparse dimensions are reduced module: sparse triaged,2022-04-28 03:58:24+00:00,,0,0,module: sparse triaged,True
76514,Clean up PyTorch's private operators module: internals triaged,2022-04-28 01:43:44+00:00,,0,2,module: internals triaged,True
76512,Make Torch FX function `_torchscript_type_to_python_type` public oncall: jit module: fx,2022-04-28 01:23:10+00:00,,0,1,oncall: jit module: fx,False
76510,`Tensor.register_hook()` Source Link Broken module: docs triaged,2022-04-28 00:36:39+00:00,,0,0,module: docs triaged,True
76503,[JIT] make IRAttributeError extend jit::Error oncall: jit,2022-04-27 23:11:02+00:00,,0,0,oncall: jit,True
76491,[NVFuser] Automated generation of microbenchmarks triaged module: nvfuser,2022-04-27 21:30:15+00:00,,0,0,triaged module: nvfuser,True
76487,`F.interpolate` uses incorrect size when `align_corners=True` triaged module: interpolation,2022-04-27 21:15:27+00:00,,1,7,triaged module: interpolation,True
76486,[Tracer] RuntimeError: _Map_base::at when tracing fake quantization oncall: jit,2022-04-27 21:10:50+00:00,,2,8,oncall: jit,False
76483,Expand pow and float_pow sampling function for more coverage module: tests triaged,2022-04-27 21:05:57+00:00,,0,7,module: tests triaged,True
76465,Pyre type checking fails  module: typing triaged,2022-04-27 19:03:47+00:00,,0,0,module: typing triaged,True
76464,Add post-AccumulateGrad hook as a nice public API feature module: autograd triaged has workaround actionable,2022-04-27 19:03:20+00:00,,1,5,feature module: autograd triaged has workaround actionable,False
76451,[checkpoint] Extension hooks to support logging and telemetry triaged sharded_tensor,2022-04-27 16:37:39+00:00,,0,0,triaged sharded_tensor,False
76449,Enhance _verify_param_shape_across_processes high priority triage review oncall: distributed better-engineering pt_distributed_rampup,2022-04-27 16:32:19+00:00,,0,0,high priority triage review oncall: distributed better-engineering pt_distributed_rampup,True
76441,[checkpoint] Switch away from pickle-base serialization triaged sharded_tensor,2022-04-27 15:04:39+00:00,,0,4,triaged sharded_tensor,True
76433,Remove _log_softmax/_softmax in favor of log_softmax and softmax respectively. module: internals triaged,2022-04-27 09:07:31+00:00,,0,0,module: internals triaged,True
76410,TorchFunction handling and overload resolution very slow in `torch.ops` oncall: jit,2022-04-26 22:17:12+00:00,,0,2,oncall: jit,False
76395,[checkpoint] SPMD distributed checkpoint coordination oncall: distributed sharded_tensor,2022-04-26 19:14:05+00:00,,0,2,oncall: distributed sharded_tensor,False
76389,Error in DistributedDataParallel with 'CPU' device oncall: distributed,2022-04-26 18:03:21+00:00,,0,1,oncall: distributed,True
76387,[checkpoint] Make prepare_sharded_tensor_read and prepare_sharded_tensor_write public oncall: distributed sharded_tensor,2022-04-26 17:54:09+00:00,,0,5,oncall: distributed sharded_tensor,False
76383,Suggestion to throw a UserWarning when a user forgot .eval() mode during inference module: nn module: molly-guard triaged,2022-04-26 16:59:59+00:00,,0,5,module: nn module: molly-guard triaged,True
76371,[ONNX] Intermediate values are encoded when exporting operators with custom namespace module: onnx triaged onnx-triaged,2022-04-26 14:14:43+00:00,,0,2,module: onnx triaged onnx-triaged,True
76370,onnx export fails when using torchvision.transforms.CenterCrop module: onnx triaged onnx-triaged,2022-04-26 14:14:06+00:00,,0,3,module: onnx triaged onnx-triaged,True
76362,Whether 'targetSize' in inferExpandGeometryImpl needs to be checked when it is less than 0 module: error checking triaged,2022-04-26 03:37:57+00:00,,0,1,module: error checking triaged,True
76354,NVFuser failing extremal opinfos triaged module: nvfuser,2022-04-26 00:18:40+00:00,,0,9,triaged module: nvfuser,True
76347,`index_select` allows negative `index` for sparse but not for strided `self` module: sparse triaged module: advanced indexing,2022-04-25 22:28:40+00:00,,0,0,module: sparse triaged module: advanced indexing,True
76344,"[ONNX] Use topk to export max(dim,keepdim) to onnx module: onnx triaged onnx-triaged bug",2022-04-25 22:06:43+00:00,,0,1,module: onnx triaged onnx-triaged bug,True
76338,torch.bucketize doc typo on the left boundary when 'right=True' module: docs triaged module: sorting and selection,2022-04-25 20:51:11+00:00,,0,0,module: docs triaged module: sorting and selection,True
76331,[checkpoint] Avoid loading whole tensor when resharding  triaged sharded_tensor,2022-04-25 19:05:37+00:00,,0,0,triaged sharded_tensor,True
76327,[checkpoint] Add extension points to avoid the default serialization behavior triaged sharded_tensor,2022-04-25 18:29:52+00:00,,0,0,triaged sharded_tensor,False
76326,[checkpoint] Support models with different cross-rank metadata triaged sharded_tensor,2022-04-25 18:15:54+00:00,,0,0,triaged sharded_tensor,False
76325,[checkpoint] Use fsspec to support object storage triaged sharded_tensor,2022-04-25 18:12:36+00:00,,0,0,triaged sharded_tensor,False
76324,Bessel and Related Functions feature triaged module: special,2022-04-25 18:10:09+00:00,,0,12,feature triaged module: special,True
76323,[Checkpoint] Add module documentation triaged sharded_tensor,2022-04-25 18:05:53+00:00,,0,0,triaged sharded_tensor,True
76309,More understandable name column of the table of the profiling result. oncall: profiler,2022-04-25 14:02:16+00:00,,0,1,oncall: profiler,False
76304,Quantization fails when padding parameter given as string oncall: quantization low priority triaged,2022-04-25 12:28:18+00:00,,1,3,oncall: quantization low priority triaged,True
76300,Unexpected _LinAlgError appeared only on my device triaged module: linear algebra,2022-04-25 09:11:05+00:00,,0,1,triaged module: linear algebra,True
76295,TorchScript: jit.script fails when using 'tolist' with int32 tensors. oncall: jit,2022-04-25 03:38:55+00:00,,0,2,oncall: jit,True
76294,torch._remove_batch_dim is interceptable by __torch_function__ / batch tensors don't print correctly triaged module: vmap module: __torch_function__,2022-04-25 02:29:44+00:00,,0,5,triaged module: vmap module: __torch_function__,True
76288,Multi-GPU distributed training reports errors oncall: distributed triaged module: c10d,2022-04-24 18:04:55+00:00,,0,3,oncall: distributed triaged module: c10d,True
76287,torch.elastic fails to shutdown despite crashed processes oncall: distributed module: elastic,2022-04-24 15:38:37+00:00,,0,31,oncall: distributed module: elastic,False
76282,`torch.cuda.amp.GradScaler` may skip parameter synchronization required by post localSGD optimizer oncall: distributed module: amp (automated mixed precision),2022-04-24 07:57:07+00:00,,0,0,oncall: distributed module: amp (automated mixed precision),False
76274,[ONNX] About custom operator convert PreciseRoIPooling to ONNX module: onnx triaged onnx-needs-info,2022-04-24 02:28:40+00:00,,0,7,module: onnx triaged onnx-needs-info,False
76244,Initial integration of ZenDNN as backend into PyTorch feature module: convolution triaged,2022-04-22 16:50:48+00:00,,0,8,feature module: convolution triaged,True
76232,Observing a strange behavior - Row parallelism module: numerical-stability module: nn triaged module: correctness (silent),2022-04-22 14:02:17+00:00,,0,5,module: numerical-stability module: nn triaged module: correctness (silent),True
76225,"RuntimeError: bucket_count == per_bucket_sizes.size()INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1646755853042/work/torch/csrc/distributed/c10d/reducer.cpp"":980, please report a bug to PyTorch.  oncall: distributed triaged",2022-04-22 07:40:09+00:00,,1,2,oncall: distributed triaged,True
76206,Update NCCL to 2.12 oncall: distributed module: build triaged,2022-04-22 00:07:11+00:00,,0,4,oncall: distributed module: build triaged,True
76191,"[Feature request] Exclusive prefix sum, `torch.cumsum(input, dim=0, exclusive=True)` high priority module: cuda triaged enhancement module: reductions",2022-04-21 19:53:49+00:00,,1,5,high priority module: cuda triaged enhancement module: reductions,True
76186,fx: cannot find module <built-in method matmul> when using apex.amp triaged module: fx,2022-04-21 18:40:33+00:00,,0,1,triaged module: fx,True
76185,Ensure custom Function are correct in double backward setting module: autograd triaged enhancement actionable,2022-04-21 18:01:03+00:00,,0,2,module: autograd triaged enhancement actionable,True
76181,Allow `torch.fx` tracing on TorchScript models oncall: jit module: fx,2022-04-21 16:58:52+00:00,,0,6,oncall: jit module: fx,False
76176,Indexing assignment can have no effect on CUDA with deterministic algorithms high priority triaged module: advanced indexing,2022-04-21 14:56:38+00:00,,0,7,high priority triaged module: advanced indexing,True
76175,Many dispatch keys do not print to string correctly triaged module: dispatch,2022-04-21 14:38:14+00:00,,0,1,triaged module: dispatch,True
76166,at::real and at::imag as methods triaged module: complex,2022-04-21 09:10:16+00:00,,1,1,triaged module: complex,True
76160,test_wishart_log_prob fails locally for me module: distributions module: tests triaged,2022-04-21 03:29:34+00:00,,0,4,module: distributions module: tests triaged,True
76155,Replace `RuntimeError` by custom exception for unsupported ONNX operators during export module: onnx triaged onnx-triaged topic: improvements,2022-04-20 23:41:56+00:00,,0,7,module: onnx triaged onnx-triaged topic: improvements,True
76138,Coverage test is only checking packages and not all submodules module: docs triaged module: python frontend,2022-04-20 20:42:44+00:00,,0,0,module: docs triaged module: python frontend,True
76128,test_license_for_wheel always fails on my local dev copy module: tests triaged,2022-04-20 19:04:56+00:00,,1,6,module: tests triaged,True
76127,run_test.py option to write out failed tests module: tests triaged better-engineering,2022-04-20 19:02:37+00:00,,0,0,module: tests triaged better-engineering,True
76113,Deprecation warning from SequentialLR high priority triaged module: deprecation module: LrScheduler,2022-04-20 15:49:50+00:00,,0,1,high priority triaged module: deprecation module: LrScheduler,True
76108,RuntimeError: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\caffe2\\serialize\\inline_container.cc:300] module: serialization triaged,2022-04-20 14:10:30+00:00,,0,15,module: serialization triaged,True
76106,Allow any operation that takes a Storage to also take a contiguous Tensor instead feature triaged module: python frontend,2022-04-20 13:48:10+00:00,,0,0,feature triaged module: python frontend,True
76105,Failed to build on Ubuntu 18.04 due to bad MPI linker flags module: build triaged,2022-04-20 13:47:29+00:00,,0,7,module: build triaged,True
76103,Some test failed when running in parallel. oncall: distributed,2022-04-20 13:27:18+00:00,,0,14,oncall: distributed,False
76082,Eliminate uses of deprecated `FindCUDA.cmake` module: build module: cuda triaged better-engineering,2022-04-20 01:45:58+00:00,,1,6,module: build module: cuda triaged better-engineering,True
76071,HIPFFT_EXEC_FAILED when using AMD GPU run FFT module: cuda triaged module: fft,2022-04-19 22:10:55+00:00,,0,1,module: cuda triaged module: fft,True
76070,NVFuser microbenchmark classifier - hash on memory formats triaged module: nvfuser,2022-04-19 21:16:29+00:00,,0,0,triaged module: nvfuser,True
76069,`init_process_group` hanging on HPC multi-node system w GPU  oncall: distributed triaged,2022-04-19 21:10:02+00:00,,0,7,oncall: distributed triaged,True
76047,NNC failing opinfo accuracy tests NNC,2022-04-19 15:56:51+00:00,,0,0,NNC,False
76043,RuntimeError: bucket_count == per_bucket_sizes.size() INTERNAL ASSERT FAILED module: cuda triaged module: ddp,2022-04-19 15:35:44+00:00,,0,0,module: cuda triaged module: ddp,True
76040,SSL certificate error: urlopen error [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1091)> triaged,2022-04-19 14:56:14+00:00,,0,1,triaged,True
76039,[RFC] NPU device for PyTorch oncall: distributed,2022-04-19 14:40:13+00:00,,0,2,oncall: distributed,False
76037,__torch_function__ and generator input hazard triaged module: __torch_function__,2022-04-19 13:25:00+00:00,,0,0,triaged module: __torch_function__,True
76031,Computer using CPU instead of GPU nvidia with CUDA module: cuda triaged,2022-04-19 10:56:14+00:00,,0,12,module: cuda triaged,True
76030,Dirichlet with small concentration module: distributions triaged,2022-04-19 10:20:18+00:00,,0,2,module: distributions triaged,True
76029,Mobile assets upload could break third party mirrors due to binary data size oncall: mobile,2022-04-19 10:09:34+00:00,,0,0,oncall: mobile,False
76028,A bug in instructions for building PyTorch with ASAN module: docs module: ci triaged topic: bug fixes,2022-04-19 09:58:36+00:00,,0,0,module: docs module: ci triaged topic: bug fixes,True
76026,Jit torchscript for prediction is missing 'forward' when using forward hooks oncall: jit,2022-04-19 09:25:27+00:00,,0,0,oncall: jit,False
76025,Numerical instability: matrix multiplication got different results on cpu and gpu  triaged module: tf32,2022-04-19 09:21:30+00:00,,0,4,triaged module: tf32,True
76024,The prediction results of different equipment are inconsistent needs reproduction module: nn triaged,2022-04-19 09:05:28+00:00,,0,4,needs reproduction module: nn triaged,True
76013,test_jit.py TestWarn.test_warn and friends doesn't work under pytest oncall: jit,2022-04-19 02:29:40+00:00,,0,1,oncall: jit,False
76012,torch.nn.LayerNorm is very slow on GPU (much slower than a custom LayerNorm version in the ConvNext model) module: nn module: cuda triaged,2022-04-19 02:27:00+00:00,,0,4,module: nn module: cuda triaged,True
76011,backcompat tests in test_nn.py are slow module: nn module: tests triaged,2022-04-19 01:48:45+00:00,,0,0,module: nn module: tests triaged,True
76007,"Build a default NVFuser comparison callback, e.g. for use with torchbench triaged module: nvfuser",2022-04-19 00:32:15+00:00,,0,0,triaged module: nvfuser,True
75986,gql_mocks.json has really long lines triaged better-engineering,2022-04-18 18:54:21+00:00,,0,0,triaged better-engineering,True
75984,DISABLED test_zero_model_parallel_parameters_as_bucket_view_True (__main__.TestZeroRedundancyOptimizerDistributed) oncall: distributed module: rocm skipped,2022-04-18 18:20:38+00:00,,0,2,oncall: distributed module: rocm skipped,False
75982,"API to determine if a torch.return_type is a ""structseq"" feature triaged",2022-04-18 17:26:24+00:00,,0,1,feature triaged,False
75963,Add build support for GCC 11.2 needs reproduction module: build triaged,2022-04-18 12:43:47+00:00,,0,1,needs reproduction module: build triaged,True
75960,"jit/_trace.py"", line 71, in _unique_state_dict     filtered_dict[k] = v.detach() AttributeError: 'torch.dtype' object has no attribute 'detach' oncall: jit",2022-04-18 08:14:00+00:00,,0,2,oncall: jit,False
75956,[JIT] [Autocast] JIT Autocast Pass operations' list should be extendable and consistent with imperative path oncall: jit,2022-04-18 02:35:16+00:00,,2,4,oncall: jit,True
75949,Potential memory leak in Adam optimizer in AMD chips (CPU) needs reproduction module: optimizer module: rocm module: memory usage triaged,2022-04-17 19:54:01+00:00,,0,3,needs reproduction module: optimizer module: rocm module: memory usage triaged,True
75943,FSDP remove the requirement of all trainable parameters   high priority oncall: distributed triaged module: fsdp,2022-04-17 00:44:29+00:00,,1,12,high priority oncall: distributed triaged module: fsdp,True
75940,Add nesting of nested Tensor triaged module: nestedtensor,2022-04-16 09:53:27+00:00,,0,5,triaged module: nestedtensor,True
75936,AllGather with backward support async_op=True oncall: distributed,2022-04-16 06:37:17+00:00,,0,0,oncall: distributed,False
75935,torch.jit.trace error when custom autograd function used in the model oncall: jit,2022-04-16 02:54:16+00:00,,0,3,oncall: jit,True
75926,Disable TracerWarnings on NVFuser opinfo tests oncall: jit module: bootcamp triaged,2022-04-15 22:58:19+00:00,,1,0,oncall: jit module: bootcamp triaged,True
75925,autogen-58 microbenchmark fails on NNC gpu fusion NNC,2022-04-15 22:50:36+00:00,,0,0,NNC,False
75923,aten::_softmax.out doesn't work with non-contiguous Tensors  module: nn triaged,2022-04-15 21:59:19+00:00,,0,1,module: nn triaged,True
75912,"interaction with psychopy during imports, script exits with: free(): invalid pointer. Aborted (core dumped) triaged",2022-04-15 20:21:57+00:00,,0,6,triaged,True
75911,'python setup.py build' failed but succeed using  'pip install -v .' which calls 'python setup.py build'. module: build triaged,2022-04-15 20:16:56+00:00,,0,1,module: build triaged,True
75910,[FSDP] Verify buffer checkpointing high priority triage review oncall: distributed module: fsdp,2022-04-15 20:11:59+00:00,,1,0,high priority triage review oncall: distributed module: fsdp,True
75909,Add batching rules for `{view}_copy` operators triaged module: batching,2022-04-15 19:55:42+00:00,,0,0,triaged module: batching,True
75904,Move _SKIP_PYTHON_BINDINGS to native_functions.yaml triaged module: codegen,2022-04-15 19:00:26+00:00,,0,0,triaged module: codegen,True
75903,torch.jit.script'd function very slow on first invocation on latest nightly oncall: jit NNC module: nvfuser,2022-04-15 18:48:12+00:00,,0,17,oncall: jit NNC module: nvfuser,True
75895,add -D_GLIBCXX_ASSERTIONS in debug mode module: build triaged,2022-04-15 17:16:22+00:00,,0,0,module: build triaged,True
75864,"INTERNAL ASSERT FAILED at ""vulkan_rewrite.cpp"":272 oncall: mobile",2022-04-15 08:51:48+00:00,,0,0,oncall: mobile,False
75862,LayerNorm and GroupNorm with num_groups=1 not equivalent module: nn triaged module: norms and normalization,2022-04-15 08:04:15+00:00,,0,6,module: nn triaged module: norms and normalization,True
75798,Fix workaround `__module__` used to appease public binding checks triaged,2022-04-14 15:59:55+00:00,,0,0,triaged,True
75794,Different result with JIT oncall: jit,2022-04-14 14:36:54+00:00,,0,2,oncall: jit,False
75788,`torch.jit.script` Script functions do return `requires_grad = False` if `torch.no_grad()` has been used oncall: jit,2022-04-14 09:50:58+00:00,,1,1,oncall: jit,False
75785,"[ONNX] Expected quantizer->qscheme() == kPerTensorAffine to be true, but got false. oncall: quantization low priority triaged",2022-04-14 06:23:09+00:00,,0,2,oncall: quantization low priority triaged,True
75778,`torch.matmul` produces wrong results on A4000 for matrices (n*m) with large m and small n  high priority triaged module: tf32,2022-04-14 02:24:03+00:00,,0,5,high priority triaged module: tf32,True
75773,Handle noncontiguous inputs in distributed backend layer oncall: distributed triaged module: c10d,2022-04-14 00:46:24+00:00,,0,1,oncall: distributed triaged module: c10d,True
75747,Depthwise Conv1d performance (a naive CUDA kernel is 10x faster) module: cuda triaged,2022-04-13 18:08:27+00:00,,0,4,module: cuda triaged,True
75740,Large numerical error when applying nn.Linear in RTX A6000 with cuda>=11.1 high priority module: cuda triaged module: tf32,2022-04-13 16:26:38+00:00,,0,3,high priority module: cuda triaged module: tf32,True
75737,torch.device missing doctring module: docs triaged,2022-04-13 15:33:39+00:00,,0,0,module: docs triaged,False
75733,"`torch.sum, prod, cumsum, cumprod, sparse.sum` INTERNAL ASSERT FAIL module: error checking triaged module: reductions",2022-04-13 14:27:35+00:00,,0,1,module: error checking triaged module: reductions,True
75725,Warning originating in C10 backend does not get translated to Python warning if run from subprocess high priority triage review oncall: distributed,2022-04-13 09:09:54+00:00,,0,4,high priority triage review oncall: distributed,True
75721,Support batch indexing with sparse tensors with torch.sparse module: sparse triaged,2022-04-13 03:04:41+00:00,,0,6,module: sparse triaged,True
75703,Let's host NVIDIA dependencies in our own S3 module: ci triaged,2022-04-12 22:14:54+00:00,,2,14,module: ci triaged,True
75701,Einsum should have an `out=` parameter triaged module: linear algebra,2022-04-12 20:53:29+00:00,,0,7,triaged module: linear algebra,True
75680,Addressing skips in OpInfo nn.functional.binary_cross_entropy_with_logits module: nn module: tests triaged,2022-04-12 18:29:54+00:00,,0,0,module: nn module: tests triaged,True
75673,Tensorboard Issue with visualizing the connections of encoder-decoder network oncall: visualization,2022-04-12 17:19:19+00:00,,0,1,oncall: visualization,True
75667,Implement histc for bfloat16 on CPU triaged module: bfloat16 module: sorting and selection,2022-04-12 15:34:38+00:00,,0,0,triaged module: bfloat16 module: sorting and selection,True
93746,Off main thread symbolic evaluation module: internals triaged enhancement oncall: pt2 module: dynamo module: startup-tracing-compile time,2022-04-12 13:55:35+00:00,,0,4,module: internals triaged enhancement oncall: pt2 module: dynamo module: startup-tracing-compile time,True
75662,"multiprocessing and torch.tensor, Cannot allocate memory error module: multiprocessing triaged",2022-04-12 12:43:52+00:00,,0,5,module: multiprocessing triaged,True
75659,Misleading documentation for cholesky_inverse module: docs triaged module: linear algebra,2022-04-12 11:14:35+00:00,,0,0,module: docs triaged module: linear algebra,True
75657,1.11.0 distribution train different with 1.8.1 oncall: distributed,2022-04-12 10:44:06+00:00,,0,2,oncall: distributed,False
75655,`jit(Function)` results in double execution oncall: jit,2022-04-12 09:41:23+00:00,,0,3,oncall: jit,False
75654,jit fails when trying to assign values to model via hook oncall: jit,2022-04-12 08:50:35+00:00,,0,1,oncall: jit,True
75652,Op segfaults with ForwardAD and Subclassed Tensor as Tangent triaged module: forward ad,2022-04-12 07:11:58+00:00,,0,3,triaged module: forward ad,True
75634,[ONNX] Enable stacktrace print for TORCH_INTERNAL_ASSERT errors in export. module: onnx triaged onnx-triaged,2022-04-11 23:02:26+00:00,,0,3,module: onnx triaged onnx-triaged,True
75625,[ONNX] Support unit tests in scripting that we already support in tracing module: onnx triaged onnx-triaged,2022-04-11 22:23:52+00:00,,1,1,module: onnx triaged onnx-triaged,False
75599,kthvalue 20x slower than sort  module: performance triaged module: sorting and selection,2022-04-11 16:50:24+00:00,,0,5,module: performance triaged module: sorting and selection,True
75586,Add ZeroTensor support for `mm` module: performance module: autograd triaged actionable ZeroTensor,2022-04-11 00:51:16+00:00,,0,0,module: performance module: autograd triaged actionable ZeroTensor,True
75582,Add `balance` flag to `random_split` triaged enhancement module: data,2022-04-10 13:26:15+00:00,,0,7,triaged enhancement module: data,True
75577,Cannot use socks5h proxy because of urllib: `urlopen error Remote end closed connection without response` triaged module: vision module: hub,2022-04-09 19:08:47+00:00,,0,2,triaged module: vision module: hub,True
75568,mypy typing strategy for Tensor-likes (`__torch_function__`) module: typing triaged module: __torch_function__,2022-04-09 16:08:29+00:00,,0,0,module: typing triaged module: __torch_function__,True
75565,make lint should advertise make setup_lint module: build module: lint triaged,2022-04-09 15:54:40+00:00,,0,2,module: build module: lint triaged,True
75562,"clang-tidy ""error: do not use const_cast"" cppcoreguidelines-pro-type-const-cast is counterproductive module: cpp module: lint triaged",2022-04-09 15:26:18+00:00,,0,3,module: cpp module: lint triaged,True
75556,[JIT] `torch.jit.ignore` is not working on hooks oncall: jit,2022-04-09 04:47:59+00:00,,0,0,oncall: jit,True
75554,torch.overrides testing is not catching people adding new kwargs and not passing on to handle_torch_function module: tests triaged module: __torch_function__,2022-04-09 03:09:58+00:00,,0,0,module: tests triaged module: __torch_function__,True
75549,`torch.linalg.lstsq` raises `CUBLAS_STATUS_EXECUTION_FAILED` for large `B` in CUDA tensors triaged module: cublas,2022-04-09 01:52:41+00:00,,0,1,triaged module: cublas,True
75512,Connection closed by peer when using dist.isend in gloo backend oncall: distributed triaged,2022-04-08 13:44:31+00:00,,0,4,oncall: distributed triaged,True
75510,"[doc] view appears to mean different things, `view/reshape` vs `transpose/permute`. module: docs triaged module: numpy module: partial aliasing module: viewing and reshaping topic: docs",2022-04-08 13:29:40+00:00,,0,3,module: docs triaged module: numpy module: partial aliasing module: viewing and reshaping topic: docs,True
75504,Profiling graphed callables or cuda graphs raises a RuntimeError high priority triage review oncall: profiler module: cuda graphs,2022-04-08 09:02:47+00:00,,0,7,high priority triage review oncall: profiler module: cuda graphs,True
75503,make_dual errors out when primal is a Tensor and tangent is a subclass Tensor triaged module: forward ad,2022-04-08 07:49:20+00:00,,0,6,triaged module: forward ad,True
75502,TensorBoard frontend fails to display embeddings when `add_embedding()` writes large `label_img` module: tensorboard oncall: visualization,2022-04-08 07:45:29+00:00,,0,2,module: tensorboard oncall: visualization,True
75500,RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED module: dependency bug module: cudnn module: cuda module: convolution triaged,2022-04-08 07:26:46+00:00,,0,3,module: dependency bug module: cudnn module: cuda module: convolution triaged,True
75495,Forward AD convolution fails for the empty backend  module: convolution triaged module: forward ad,2022-04-08 05:23:51+00:00,,0,0,module: convolution triaged module: forward ad,True
75494,`torch.pinverse` on CUDA tensors produces non-optimal output for certain type of (invertible) matrix on torch > 1.7.1 but not on torch <= 1.7.1 module: numerical-stability module: cuda triaged module: linear algebra,2022-04-08 04:34:49+00:00,,0,7,module: numerical-stability module: cuda triaged module: linear algebra,True
75480,IndexError: Caught IndexError in replica 0 on device 0. oncall: distributed triaged,2022-04-08 02:16:32+00:00,,0,3,oncall: distributed triaged,False
75474,[NVFuser] call kernels with informative names (e.g. pow_mul_add) triaged module: nvfuser,2022-04-07 23:27:33+00:00,,0,1,triaged module: nvfuser,True
75467,torch.package: log.info when loading packages w/ externed modules + documentation oncall: package/deploy imported,2022-04-07 22:17:23+00:00,,1,0,oncall: package/deploy imported,False
75465,[JIT] Bool Should Subtype NumberType oncall: jit,2022-04-07 21:58:48+00:00,,0,1,oncall: jit,False
75462,NotImplemented confusion between __torch_function__ and __rpow__ (and other dunder magic methods) triaged module: __torch_function__,2022-04-07 21:15:34+00:00,,0,0,triaged module: __torch_function__,True
75458,Placing model on bfloat16 on CPU make it freeze/hang needs reproduction triaged module: deadlock module: bfloat16 module: intel,2022-04-07 20:28:28+00:00,,2,11,needs reproduction triaged module: deadlock module: bfloat16 module: intel,True
75440,[quant] PackedLinearWeight::apply_dynamic_impl does not handle ReLUFused template argument oncall: quantization low priority triaged,2022-04-07 16:54:48+00:00,,0,1,oncall: quantization low priority triaged,True
75432,Rollup: Top forward-over-reverse formulas high priority module: autograd triaged actionable module: forward ad,2022-04-07 16:10:07+00:00,,0,1,high priority module: autograd triaged actionable module: forward ad,True
75427,`torch.cuda.is_bf16_supported()` seem to not work properly module: cuda triaged module: bfloat16,2022-04-07 14:37:13+00:00,,0,2,module: cuda triaged module: bfloat16,True
75419,Require PyTorch test suite to be warnings clean module: tests triaged better-engineering,2022-04-07 13:26:47+00:00,,0,2,module: tests triaged better-engineering,True
100628,Feature request: fast way to approximate the diagonal of the hessian module: autograd triaged enhancement needs design module: functorch,2022-04-07 07:26:37+00:00,,0,11,module: autograd triaged enhancement needs design module: functorch,True
75391,Pytorch linalg test failure with cuda 11.6 module: cuda triaged module: linear algebra,2022-04-07 00:09:33+00:00,,0,10,module: cuda triaged module: linear algebra,True
75383,[ONNX] Error when exporting adaptive_max_pool2d to ONNX module: crash module: onnx triaged onnx-triaged bug,2022-04-06 23:10:47+00:00,,0,5,module: crash module: onnx triaged onnx-triaged bug,True
75375,Pytorch test failure with CUDA 11.6 module: cuda triaged,2022-04-06 22:29:18+00:00,,0,7,module: cuda triaged,True
75371,"NVFuser bad ""reshape"" performance triaged module: nvfuser",2022-04-06 22:03:27+00:00,,0,2,triaged module: nvfuser,True
75363,conv3d has numerical issue where same input produces output that are not bit-wise identical module: convolution triaged module: determinism,2022-04-06 21:39:33+00:00,,0,2,module: convolution triaged module: determinism,True
75342,`torch.fx.operator_schemas.normalize_function` is too permissive triaged module: fx,2022-04-06 18:16:29+00:00,,0,0,triaged module: fx,True
75336,Libtorch Crash after Attempting to Evaluate Model after Opening an OpenGL Context oncall: jit NNC,2022-04-06 17:27:53+00:00,,1,24,oncall: jit NNC,True
75334,ProcessGroupNCCL is relying on UB to support bool data type oncall: distributed module: nccl,2022-04-06 17:06:04+00:00,,0,1,oncall: distributed module: nccl,False
75309,[jit] failing OpInfo JIT tests for conv1d and complex input oncall: jit,2022-04-06 05:13:28+00:00,,1,5,oncall: jit,True
75308,Using Adaptive Model for Pytorch Mobile oncall: mobile,2022-04-06 04:09:07+00:00,,0,0,oncall: mobile,False
75287,[RFC] Consolidated and unified state_dict and load_state_dict hooks module: bootcamp feature module: nn good first issue triaged better-engineering pt_distributed_rampup,2022-04-05 22:31:35+00:00,,1,14,module: bootcamp feature module: nn good first issue triaged better-engineering pt_distributed_rampup,True
75280,[JIT] log_extract.py improvements oncall: jit,2022-04-05 21:26:42+00:00,,0,0,oncall: jit,False
75265,Kernel Dies while using conv2d layer / function needs reproduction triaged,2022-04-05 17:57:42+00:00,,0,2,needs reproduction triaged,False
75251,Potential runtime optimization of Mish activation feature module: nn triaged module: python frontend,2022-04-05 09:38:52+00:00,,0,2,feature module: nn triaged module: python frontend,True
75242,"large model, low memory: need `torch.load` that loads one submodule at a time module: serialization triaged",2022-04-05 01:05:43+00:00,,0,2,module: serialization triaged,True
75240,Large cumulative sums appear to be nondeterministic.  high priority module: cuda triaged module: determinism,2022-04-05 00:45:49+00:00,,0,25,high priority module: cuda triaged module: determinism,True
75225,One of the backends for `lu_factor` fails on windows. triaged module: linear algebra,2022-04-04 21:31:11+00:00,,0,0,triaged module: linear algebra,True
75217,Storing LTC tensor shape information in jit::Value oncall: jit,2022-04-04 20:22:26+00:00,,0,5,oncall: jit,True
75215,[torch.distributed] Document bfloat16 support oncall: distributed module: docs,2022-04-04 20:02:51+00:00,,0,0,oncall: distributed module: docs,False
75213,Improve wording in _store_based_barrier logging and identify ranks that have not joined oncall: distributed triaged better-engineering,2022-04-04 19:47:35+00:00,,0,1,oncall: distributed triaged better-engineering,True
75198,`torch.cuda.get_device_name` fails to identify RTX 3090 Ti  module: cuda triaged,2022-04-04 16:36:02+00:00,,0,0,module: cuda triaged,True
75186,Failed to build `convert_and_benchmark.cc` due to missing `net_observer_reporter_print.h`. caffe2 triaged,2022-04-04 14:32:13+00:00,,0,5,caffe2 triaged,True
75184,"PyTorch source code compile fail after ""Built target fbgemm_avx2"" module: build triaged",2022-04-04 13:40:52+00:00,,0,0,module: build triaged,True
75180,Add BUILD_LAZY_CUDA_SPARSE option module: sparse module: build module: cuda triaged,2022-04-04 11:46:59+00:00,,0,1,module: sparse module: build module: cuda triaged,True
75173,LowRankMultivariateNormal doesn't work with 0 diagonal module: distributions triaged,2022-04-04 06:42:24+00:00,,0,0,module: distributions triaged,True
75172,Runtime configuration to disable TORCH_WARN temporally? module: internals triaged enhancement,2022-04-04 06:26:33+00:00,,0,3,module: internals triaged enhancement,True
75171,torch.jit.load fails when path contains non-ascii characters oncall: jit,2022-04-04 04:31:55+00:00,,0,0,oncall: jit,True
75168,DISABLED test_reduce_full_group_max (__main__.TestDistBackendWithSpawn) oncall: distributed skipped,2022-04-04 03:26:28+00:00,,0,1,oncall: distributed skipped,False
75166,Disable Python mode (torch dispatch mode) inside of mode-induced __torch_dispatch__call triaged actionable module: __torch_dispatch__,2022-04-04 02:44:56+00:00,,0,3,triaged actionable module: __torch_dispatch__,False
75158,Offical libtorch (pytorch c++ frontend) docker image module: binaries feature triaged,2022-04-03 15:36:08+00:00,,0,1,module: binaries feature triaged,True
75147,Dataloader hangs. Potential deadlock with `set_num_threads` in worker processes? module: dataloader triaged module: deadlock,2022-04-02 18:00:03+00:00,,0,3,module: dataloader triaged module: deadlock,True
75139,DISABLED test_post_localSGD_optimizer_parity_with_hierarchical_sgd_grad_is_view (__main__.TestDistBackendWithSpawn) oncall: distributed module: rocm skipped,2022-04-02 00:57:16+00:00,,0,2,oncall: distributed module: rocm skipped,False
75129,[BE] add documentation for adjust learning rate when going to distributed training oncall: distributed triaged better-engineering,2022-04-01 19:35:52+00:00,,0,0,oncall: distributed triaged better-engineering,True
75097,destroy_process_group() has a certain probability of hangs oncall: distributed,2022-04-01 10:09:10+00:00,,0,6,oncall: distributed,False
75090,torch.fx deepcopy bug triaged module: fx,2022-04-01 06:55:38+00:00,,0,2,triaged module: fx,True
75087,extend torch.distributed's `--tee` to log the nodename oncall: distributed enhancement oncall: r2p,2022-04-01 04:17:08+00:00,,0,3,oncall: distributed enhancement oncall: r2p,False
75052,DISABLED test_post_localSGD_optimizer_parity_with_hierarchical_sgd (__main__.TestDistBackendWithSpawn) oncall: distributed module: rocm skipped,2022-04-01 00:58:09+00:00,,0,2,oncall: distributed module: rocm skipped,False
75038,Generate source location information in TS LTC backend oncall: jit,2022-03-31 20:55:40+00:00,,0,0,oncall: jit,False
93745,Make it possible to use TorchDynamo from within PyTorch core triaged enhancement oncall: pt2 module: dynamo,2022-03-31 20:04:27+00:00,,1,0,triaged enhancement oncall: pt2 module: dynamo,False
75031,comm hook error in BWD pass oncall: distributed module: ddp,2022-03-31 18:51:32+00:00,,0,3,oncall: distributed module: ddp,True
75025,torch.cuda.init() unstacks existing CUDA contexts module: cuda triaged,2022-03-31 16:57:06+00:00,,0,3,module: cuda triaged,True
75012,[JIT] nested dictionaries are not traced correctly. oncall: jit,2022-03-31 09:29:51+00:00,,1,3,oncall: jit,False
75005,Cannot create TorchScript file after converting into a quantized model oncall: jit,2022-03-31 05:54:54+00:00,,0,4,oncall: jit,False
75002,JIT autocasting fails on Optional[Tensor] oncall: jit,2022-03-31 04:03:29+00:00,,0,1,oncall: jit,True
74991,Tensor Subclass not preserved for Tensor subclasses created via inheritance of a TensorImpl subclass triaged better-engineering module: __torch_dispatch__ tensor subclass,2022-03-30 22:37:22+00:00,,0,4,triaged better-engineering module: __torch_dispatch__ tensor subclass,True
74964,Add Unit Tests for Torch.Package Subclasses [Waiting on changes to land] oncall: package/deploy imported,2022-03-30 18:11:05+00:00,,1,0,oncall: package/deploy imported,False
74956,Allow `torch.package` to accept `*.pyc` files oncall: package/deploy imported,2022-03-30 14:25:23+00:00,,0,0,oncall: package/deploy imported,False
74953,LSTM quantization fails if proj_size > 0 oncall: quantization low priority triaged,2022-03-30 13:23:31+00:00,,0,0,oncall: quantization low priority triaged,True
74952,[feature request] Make `index_select` parallel. module: performance feature triaged module: advanced indexing,2022-03-30 13:00:10+00:00,,0,1,module: performance feature triaged module: advanced indexing,True
74951,[feature request] `coalesce` to support `dim` argument. module: performance module: sparse feature triaged,2022-03-30 12:08:06+00:00,,0,3,module: performance module: sparse feature triaged,True
74943,Calling `torch.ops.aten.add_` is ludicrously slow oncall: jit triaged,2022-03-30 02:10:08+00:00,,1,8,oncall: jit triaged,True
74942,Store SourceDataset in MapDataset using pointer module: cpp module: dataloader triaged,2022-03-30 01:29:40+00:00,,0,0,module: cpp module: dataloader triaged,True
74940,Feature Request: Add J0 J1 J2 H0 H1 H2 Bessel functions feature triaged module: special,2022-03-30 01:06:39+00:00,,0,2,feature triaged module: special,False
74937,[docs] RandomSampler has unrendered back-ticks module: docs triaged,2022-03-30 01:01:49+00:00,,0,0,module: docs triaged,False
74935,[RFC]A suggestion of channels last memory format implementation for 3D tensor feature triaged module: memory format,2022-03-30 00:08:17+00:00,,0,12,feature triaged module: memory format,True
74911,"Include finfo(dtype).[min, max, eps, tiny] in the extremal test case module: tests triaged",2022-03-29 16:29:11+00:00,,0,1,module: tests triaged,True
74909,torch.package fails to import if dataclass and __future__.annotations present oncall: package/deploy imported,2022-03-29 15:47:08+00:00,,0,0,oncall: package/deploy imported,False
74907,[Proposal] Use batched oprations to accelerate PowerSGD module: performance oncall: distributed module: optimizer triaged module: ddp module: mta,2022-03-29 13:47:21+00:00,,0,2,module: performance oncall: distributed module: optimizer triaged module: ddp module: mta,True
74904,Adding novel 'AdaFamily' optimizer  module: optimizer triaged needs research function request,2022-03-29 10:08:43+00:00,,0,2,module: optimizer triaged needs research function request,False
74901,"torch.fx.wrap will not work, when encapsulate the code triaged module: fx",2022-03-29 07:50:03+00:00,,0,1,triaged module: fx,True
74896,Install PyTorch from source on power machine module: build triaged module: POWER,2022-03-29 07:15:57+00:00,,0,5,module: build triaged module: POWER,True
74884,Problematic ASGD Optimizer module: optimizer triaged enhancement module: LrScheduler,2022-03-29 01:58:52+00:00,,0,0,module: optimizer triaged enhancement module: LrScheduler,True
74876,Failure to set number of threads on AWS Lambda module: cpu triaged module: multithreading,2022-03-29 00:56:24+00:00,,0,3,module: cpu triaged module: multithreading,True
74855,Enable dtype keyword argument for to_dense method module: sparse triaged enhancement,2022-03-28 19:37:31+00:00,,0,0,module: sparse triaged enhancement,True
74842,error in quantization by quantize_fx.prepare_fx triaged module: fx,2022-03-28 17:58:30+00:00,,0,0,triaged module: fx,True
74839,Feature request: Root-finding functionality (like scipy.optimize.root_scalar) feature triaged module: scientific computing,2022-03-28 17:15:39+00:00,,0,11,feature triaged module: scientific computing,False
74824,C10d Elastic Training master_addr ERROR oncall: distributed oncall: r2p,2022-03-28 08:51:32+00:00,,0,21,oncall: distributed oncall: r2p,True
74809,Incorrect results for `torch.distributed.gather` for tensor created from permuted NumPy array oncall: distributed module: c10d,2022-03-27 13:53:47+00:00,,0,3,oncall: distributed module: c10d,True
74801,[ONNX] Export fails with inplace assignments module: onnx triaged onnx-triaged onnx-needs-info,2022-03-26 15:17:28+00:00,,0,2,module: onnx triaged onnx-triaged onnx-needs-info,False
74799,Irrelevant warning during ONNX export of torch.jit.ScriptFunction: Model has no forward function oncall: jit module: onnx onnx-triaged,2022-03-26 14:35:33+00:00,,0,1,oncall: jit module: onnx onnx-triaged,True
74788,[Structured] Make it possible to override create_out implementation on a per dispatch key basis (structured sparse) triaged enhancement module: structured kernels,2022-03-26 02:26:31+00:00,,0,4,triaged enhancement module: structured kernels,True
74771,[bug] `torch.multinomial` should throw error as documented triaged topic: docs,2022-03-25 19:27:05+00:00,,0,1,triaged topic: docs,True
74748,[jiterator] Jiterate Complex Ops triaged module: complex module: jiterator,2022-03-25 15:35:43+00:00,,0,1,triaged module: complex module: jiterator,True
74746,Change the type hint for nn.Module.__call__ to be friendly to overrides. module: nn module: typing triaged enhancement,2022-03-25 15:01:38+00:00,,0,5,module: nn module: typing triaged enhancement,True
74739,GradScaler support FP16 gradients? triaged module: amp (automated mixed precision),2022-03-25 06:47:59+00:00,,0,7,triaged module: amp (automated mixed precision),True
74734,RuntimeError: Unconvertible NCCL type Short when sending torch.cuda.ShortTensor. oncall: distributed module: c10d,2022-03-25 03:15:29+00:00,,0,8,oncall: distributed module: c10d,True
93743,Support data-dependent control flow operators module: onnx triaged enhancement oncall: pt2 module: dynamo,2022-03-25 00:54:29+00:00,,1,15,module: onnx triaged enhancement oncall: pt2 module: dynamo,False
74679,Embedding isn't determinstic on linux-xenial-cuda11.3-py3.7-gcc7 module: cuda triaged module: embedding,2022-03-24 14:52:55+00:00,,0,3,module: cuda triaged module: embedding,True
74677,`check_batched_forward_grad` fails for `torch.norm` and related ops module: autograd triaged module: batching module: norms and normalization,2022-03-24 14:43:22+00:00,,0,0,module: autograd triaged module: batching module: norms and normalization,True
74670,Can't install Pytorch needs reproduction module: build triaged,2022-03-24 11:05:16+00:00,,0,4,needs reproduction module: build triaged,True
74669,String dtypes for torch Tensors feature triaged,2022-03-24 10:26:06+00:00,,0,2,feature triaged,True
74666,Re-initializing torch.distributed process_group hangs with destroy_process_group oncall: distributed,2022-03-24 05:56:04+00:00,,0,4,oncall: distributed,False
74642,Some easy way to add xfails to OpInfos feature triaged module: testing,2022-03-23 21:38:20+00:00,,0,2,feature triaged module: testing,True
74634,Sampling Issue With Distributions triaged module: half,2022-03-23 20:22:57+00:00,,0,4,triaged module: half,True
74627,Feature request: INT4 format support oncall: quantization feature triaged,2022-03-23 18:54:05+00:00,,2,20,oncall: quantization feature triaged,False
74620,Improve sharding algorithm for ASAN (any maybe other jobs as well) module: ci triaged enhancement,2022-03-23 17:04:46+00:00,,0,0,module: ci triaged enhancement,True
74616,"__rpow__(self, other) OpInfo should not test the case where `other` is a Tensor module: tests triaged",2022-03-23 15:28:17+00:00,,0,2,module: tests triaged,True
74614,compilation error with PyTorch v1.11 for CPU on ppc64le triaged module: POWER,2022-03-23 14:08:12+00:00,,0,18,triaged module: POWER,True
74613,OpInfo request for `nn.functional` and `unbind` high priority module: tests triaged,2022-03-23 14:02:06+00:00,,0,0,high priority module: tests triaged,True
74605,torch.profiler.profile does't work well for CPU model when not using torch.profiler.schedule  oncall: profiler,2022-03-23 09:45:16+00:00,,0,6,oncall: profiler,False
74604,"Training, Forward / backward pass with _different_ batch-size, no speedup observed when backward pass has smaller batch-size feature module: autograd triaged",2022-03-23 08:36:15+00:00,,0,5,feature module: autograd triaged,True
74602,My_rank in the implementation of torch.distributed.scatter_object_list should be global oncall: distributed module: c10d,2022-03-23 06:14:44+00:00,,0,3,oncall: distributed module: c10d,False
74593,torch.nn.ConvTranspose2d's example in docstring is invalid module: docs module: nn triaged actionable,2022-03-23 01:02:34+00:00,,0,2,module: docs module: nn triaged actionable,True
74590,[CTA] Let's Stamp Out Flaky Tests! oncall: distributed oncall: jit module: sparse module: onnx oncall: quantization module: multiprocessing module: autograd module: nn module: optimizer module: dataloader module: cuda module: rocm module: tests module: hub module: data parallel module: linear algebra module: rpc oncall: profiler module: mta onnx-triaged oncall: r2p,2022-03-22 23:07:39+00:00,,0,4,oncall: distributed oncall: jit module: sparse module: onnx oncall: quantization module: multiprocessing module: autograd module: nn module: optimizer module: dataloader module: cuda module: rocm module: tests module: hub module: data parallel module: linear algebra module: rpc oncall: profiler module: mta onnx-triaged oncall: r2p,True
74589,Test test_ops_jit is taking too much time on ASAN oncall: jit,2022-03-22 23:00:41+00:00,,0,1,oncall: jit,False
74588,[FSDP]  using CPUOffload creates 3-10x slowdown due to slow cpu optimizer step/update oncall: distributed triaged module: fsdp,2022-03-22 22:50:02+00:00,,0,5,oncall: distributed triaged module: fsdp,True
74554,Specifying left-right-padding as tuple for asymmetric padding feature module: nn triaged module: padding,2022-03-22 17:06:28+00:00,,0,1,feature module: nn triaged module: padding,True
74547,importing open3d before pytorch causes matmul to produce a segfault module: crash module: cuda triaged module: cublas module: regression module: third_party,2022-03-22 15:30:55+00:00,,0,1,module: crash module: cuda triaged module: cublas module: regression module: third_party,True
74544,Many inplace operators are not being tested for variant consistency (test_variant_consistency_eager) module: tests triaged better-engineering,2022-03-22 14:48:36+00:00,,0,0,module: tests triaged better-engineering,True
74540,No factory functions for strided quantized tensors oncall: quantization feature low priority triaged,2022-03-22 13:50:03+00:00,,0,6,oncall: quantization feature low priority triaged,True
74539,Making PyTorch more pythonic:  the PyTorch functions should be inspectable. feature triaged,2022-03-22 13:46:53+00:00,,0,1,feature triaged,False
74537,ComplexHalf Coverage Tracker feature triaged module: complex module: half,2022-03-22 13:35:08+00:00,,1,1,feature triaged module: complex module: half,True
74531,Feature Request: torch.testing.make_scalar (make_tensor for scalars) feature triaged module: testing,2022-03-22 08:15:23+00:00,,0,2,feature triaged module: testing,False
74528,Supports for dist.send/dist.recv sending and recving torch.shorttensor oncall: distributed module: c10d function request,2022-03-22 07:06:43+00:00,,0,1,oncall: distributed module: c10d function request,False
74519,Lazy TS Test Failures triaged lazy,2022-03-22 03:32:38+00:00,,0,0,triaged lazy,True
74514,Figure out a lint for directly returning a nullptr instead of raising python_error triaged better-engineering release notes: python_frontend,2022-03-22 01:26:39+00:00,,0,0,triaged better-engineering release notes: python_frontend,True
74503,Compilation error on M1 Mac module: build triaged module: macos module: arm,2022-03-21 22:15:36+00:00,,0,0,module: build triaged module: macos module: arm,True
74500,One should be able to query if a DecorateInfo is a xfail or skip triaged enhancement module: testing,2022-03-21 21:47:24+00:00,,0,0,triaged enhancement module: testing,False
74491,__torch_function__ is hitting private functions in some cases triaged enhancement module: __torch_function__,2022-03-21 20:09:00+00:00,,0,1,triaged enhancement module: __torch_function__,True
74485,dir(torch._VF) doesn't work triaged better-engineering release notes: python_frontend,2022-03-21 19:09:44+00:00,,0,1,triaged better-engineering release notes: python_frontend,True
74454,torch has no attribute sparse_csr_tensor module: sparse triaged,2022-03-21 06:14:51+00:00,,0,20,module: sparse triaged,True
74442,kron has unnecessary and undocumented dependence on memory layout triaged module: memory format,2022-03-20 22:17:05+00:00,,0,2,triaged module: memory format,True
74422,Embedding Pytorch in C++ using pybind fails on interpreter shutdown module: crash triaged module: pybind module: third_party,2022-03-19 01:30:46+00:00,,0,2,module: crash triaged module: pybind module: third_party,True
74421,"can not build pytorch, failing due to missing _ctypes module module: build triaged",2022-03-18 23:35:14+00:00,,0,0,module: build triaged,True
74420,`torch.histogram` has wrong output dtype and doesn't support integer inputs feature triaged module: numpy module: sorting and selection,2022-03-18 23:13:11+00:00,,0,1,feature triaged module: numpy module: sorting and selection,True
74415,"Warning: ""Specified kernel cache directory could not be created"" triaged module: jiterator",2022-03-18 15:02:22+00:00,,0,8,triaged module: jiterator,True
74411,"UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead. module: nn triaged",2022-03-18 08:19:12+00:00,,0,1,module: nn triaged,True
74398,_run_ninja_build failing with clang module: build triaged,2022-03-18 00:17:51+00:00,,0,0,module: build triaged,True
74393,Quantized cannot inference with cuda oncall: quantization module: cuda triaged,2022-03-17 22:36:41+00:00,,1,5,oncall: quantization module: cuda triaged,True
74391,Can this cudaDeviceSynchronize call be removed? module: cuda triaged,2022-03-17 21:49:05+00:00,,0,0,module: cuda triaged,True
74389,torch.package unpickling transforms: ModuleNotFoundError: No module named 'torch._C._linalg'; 'torch._C' is not a package module: pickle module: serialization triaged module: linear algebra,2022-03-17 21:04:28+00:00,,0,3,module: pickle module: serialization triaged module: linear algebra,True
74384,Package and Deploy: Generate External Python Registration files at runtime [WIP] triaged module: deploy,2022-03-17 19:47:17+00:00,,1,0,triaged module: deploy,True
93739,"[Dynamo] Prints, logging, and warnings module: logging triaged enhancement oncall: pt2 module: dynamo",2022-03-17 16:01:44+00:00,,0,1,module: logging triaged enhancement oncall: pt2 module: dynamo,False
93737,Improve handling of generators triaged enhancement module: random oncall: pt2 module: dynamo,2022-03-17 15:33:31+00:00,,0,1,triaged enhancement module: random oncall: pt2 module: dynamo,False
74363,"Efficient very large (e.g., 31x31) depth-wise convolution module: convolution triaged needs research",2022-03-17 04:38:06+00:00,,0,2,module: convolution triaged needs research,True
74337,None returned from data loader causes debugging difficulty from collate function module: dataloader triaged,2022-03-16 21:10:04+00:00,,0,5,module: dataloader triaged,True
74329,CompositeImplicitAutograd addr implementation does not have correct behavior for bool (meta gives wrong dtype) triaged module: meta tensors,2022-03-16 20:13:06+00:00,,0,1,triaged module: meta tensors,True
74328,instantiate_device_type_tests is misnamed triaged module: testing,2022-03-16 20:05:49+00:00,,0,2,triaged module: testing,True
74324,torch.combinations requires unnecessary CUDA sync module: cuda triaged topic: performance,2022-03-16 19:26:57+00:00,,0,0,module: cuda triaged topic: performance,True
74323,[c10d] `gather`/`scatter` inconsistency behavior on non-dst/src rank oncall: distributed better-engineering module: c10d,2022-03-16 19:21:38+00:00,,0,1,oncall: distributed better-engineering module: c10d,True
74320,Make OpInfo repr more useful triaged better-engineering module: testing,2022-03-16 19:05:52+00:00,,0,3,triaged better-engineering module: testing,True
74301,Error in lobpcg when using largest=False module: sparse triaged module: masked operators,2022-03-16 15:11:14+00:00,,0,3,module: sparse triaged module: masked operators,True
74300,repeat_interleave is not meta friendly; not transform friendly either triaged tensor subclass,2022-03-16 14:42:26+00:00,,0,0,triaged tensor subclass,True
74299,Gradients tests are very time consuming module: autograd module: ci triaged,2022-03-16 14:32:57+00:00,,0,4,module: autograd module: ci triaged,True
74296,Time Gated Lstm feature module: nn module: rnn triaged,2022-03-16 13:42:51+00:00,,0,1,feature module: nn module: rnn triaged,True
74295,"TypeError: new_empty(): argument 'size' (position 1) must be tuple of ints, not list module: error checking triaged",2022-03-16 13:40:47+00:00,,0,0,module: error checking triaged,True
74290,Loading of packaged model object fails with torch==1.11.0 oncall: package/deploy imported,2022-03-16 10:37:57+00:00,,0,1,oncall: package/deploy imported,False
74282,RuntimeError: cuDNN error: CUDNN_STATUS_VERSION_MISMATCH for torchvision models module: binaries module: cudnn triaged module: vision,2022-03-16 05:40:45+00:00,,0,3,module: binaries module: cudnn triaged module: vision,True
74281,Move torchbench workflow to `workflow_dispatch`? module: ci triaged,2022-03-16 05:03:09+00:00,,0,0,module: ci triaged,True
74272,[PyTorch] Lightweight dispatch oncall: mobile mobile_perf,2022-03-16 00:57:53+00:00,,1,1,oncall: mobile mobile_perf,False
74368,"Can't download torch vision models for older versions via torch.hub.load: ""multiple choices"" triaged module: hub module: models",2022-03-15 21:53:43+00:00,,0,3,triaged module: hub module: models,True
74256,Create secure credential storage for metrics credentials and associated documentation on how to regenerate them if needed module: ci triaged,2022-03-15 20:21:20+00:00,,1,1,module: ci triaged,True
74248,dir() on torch.ops.aten doesn't work triaged enhancement tensor subclass,2022-03-15 18:45:57+00:00,,0,1,triaged enhancement tensor subclass,True
74236,"""histogram_cpu"" not implemented for 'Int' triaged module: sorting and selection",2022-03-15 09:26:26+00:00,,0,0,triaged module: sorting and selection,True
74235,RuntimeError: CUDA error: unspecified launch failure module: cudnn module: cuda triaged,2022-03-15 09:23:09+00:00,,0,32,module: cudnn module: cuda triaged,True
74222,Autograd API to get saved for backwards tensors for an autograd graph module: double backwards module: autograd triaged,2022-03-15 03:38:33+00:00,,0,1,module: double backwards module: autograd triaged,True
74221,`package.PackageExporter` does not actually appear to have a `file_structure` method module: docs oncall: package/deploy imported,2022-03-15 03:10:23+00:00,,0,0,module: docs oncall: package/deploy imported,False
93724,Don't over-specialize on list append/clear triaged enhancement oncall: pt2 module: dynamo,2022-03-15 02:46:40+00:00,,1,2,triaged enhancement oncall: pt2 module: dynamo,False
74218,Cannot get op through FX when using nn.Sequential triaged module: fx,2022-03-15 02:28:16+00:00,,0,0,triaged module: fx,True
74208,DISABLED test_trace_buffer_slice (__main__.TestFX) triaged skipped module: fx,2022-03-14 23:11:14+00:00,,0,2,triaged skipped module: fx,False
74187,[docker] test_corrcoef_cpu_complex64 fails on CPU build module: tests triaged,2022-03-14 19:52:57+00:00,,0,3,module: tests triaged,True
74185,[DDP] Parallelize initialization collectives oncall: distributed triaged better-engineering pt_distributed_rampup module: ddp,2022-03-14 19:45:31+00:00,,1,0,oncall: distributed triaged better-engineering pt_distributed_rampup module: ddp,True
74180,DISABLED test_init_rpc_twice (__main__.TensorPipeRpcTest) oncall: distributed triaged module: flaky-tests module: rpc skipped,2022-03-14 18:41:19+00:00,,1,1,oncall: distributed triaged module: flaky-tests module: rpc skipped,False
74167,Slower performance of `torch.mm` method with sparse CSR tensor module: sparse triaged module: mkl,2022-03-14 15:50:55+00:00,,1,5,module: sparse triaged module: mkl,True
74162,consume_prefix_in_state_dict_if_present can not remove prefix of _metadata. oncall: distributed module: ddp,2022-03-14 09:15:00+00:00,,0,4,oncall: distributed module: ddp,False
74155,[Android Studio] DefaultCPUAllocator: not enough memory: you tried to allocate 280166432 bytes module: memory usage triaged module: android,2022-03-14 01:29:24+00:00,,0,0,module: memory usage triaged module: android,True
74153,[BE][Docs][FSDP] Clarify microbatching support and tradeoffs oncall: distributed triaged better-engineering module: fsdp,2022-03-13 23:05:19+00:00,,0,0,oncall: distributed triaged better-engineering module: fsdp,False
74152,All-caps names for pages are confusing triaged module: doc infra,2022-03-13 20:22:34+00:00,,0,0,triaged module: doc infra,False
74148,powersgd can not get linear growth due to  extra 2 times allreduce  oncall: distributed triaged module: ddp topic: performance,2022-03-13 13:31:08+00:00,,0,2,oncall: distributed triaged module: ddp topic: performance,True
74146,RuntimeError: Connection reset by peer when backened by NCCL module: cuda triaged module: nccl,2022-03-13 11:56:36+00:00,,0,0,module: cuda triaged module: nccl,True
74143,Tracing model parameter shapes without instantiating the model parameters feature module: nn triaged,2022-03-13 04:49:16+00:00,,0,12,feature module: nn triaged,True
74138,`torch.set_printoptions` overwrites settings of its own previous calls module: printing triaged module: ux,2022-03-12 03:44:45+00:00,,0,6,module: printing triaged module: ux,True
74134,No module named 'pygame': soft_actor_critic triaged module: lazy,2022-03-12 01:33:57+00:00,,1,1,triaged module: lazy,True
93720,Support capturing code in exception handlers triaged enhancement oncall: pt2 module: dynamo,2022-03-11 20:11:52+00:00,,0,1,triaged enhancement oncall: pt2 module: dynamo,True
74115,DistributedDataParallel high peak memory usage with find_unused_parameters=True high priority triage review oncall: distributed triaged module: ddp,2022-03-11 18:17:58+00:00,,1,1,high priority triage review oncall: distributed triaged module: ddp,True
74112,torch.jit.script does not custom state_dicts oncall: jit oncall: quantization low priority triaged,2022-03-11 17:26:42+00:00,,1,3,oncall: jit oncall: quantization low priority triaged,True
74101,C++ Context::setDeterministicAlgorithms default 2nd arg not defined in header module: cpp triaged module: determinism,2022-03-11 14:49:00+00:00,,0,2,module: cpp triaged module: determinism,True
74095,torchscript RNN modules cannot move between GPU oncall: jit,2022-03-11 10:12:45+00:00,,0,2,oncall: jit,False
74092,Two consecutive nn.LayerNorm are used in transformer model when norm_first is False module: nn triaged oncall: transformer/mha,2022-03-11 06:42:21+00:00,,0,4,module: nn triaged oncall: transformer/mha,True
74058,torch.fx.symbolic_trace is non-deterministic triaged module: fx,2022-03-10 21:27:58+00:00,,0,1,triaged module: fx,True
74052,assertEqual gives confusing error when comparing tuple with Tensor with Tensor module: tests triaged,2022-03-10 20:14:15+00:00,,0,0,module: tests triaged,True
74041,FSDP does not work on GLOO backend oncall: distributed triaged better-engineering module: fsdp,2022-03-10 17:45:18+00:00,,1,7,oncall: distributed triaged better-engineering module: fsdp,True
74036,torch.nn.Module.__init__ does not call super().__init__ module: nn triaged,2022-03-10 16:45:42+00:00,,0,4,module: nn triaged,True
74034,[torch.onnx] ONNX export failed on adaptive_avg_pool2d because input size not accessible not supported module: onnx triaged,2022-03-10 14:26:04+00:00,,1,12,module: onnx triaged,False
74032,ONNX export of torch.histc module: onnx triaged onnx-triaged,2022-03-10 13:25:08+00:00,,0,2,module: onnx triaged onnx-triaged,True
74030,"[JIT] tuple of float is accepted in List[float] but not in Union[float, List[float]] oncall: jit",2022-03-10 12:43:32+00:00,,1,4,oncall: jit,False
74024,Ambiguous docstring on `register_module_forward_hook` module: docs module: nn triaged actionable,2022-03-10 07:36:35+00:00,,0,3,module: docs module: nn triaged actionable,True
74022,Inconsistent implementation on SWA module: docs module: optimizer triaged actionable,2022-03-10 07:25:56+00:00,,0,6,module: docs module: optimizer triaged actionable,True
74019,https://discuss.pytorch.org/ is down triaged,2022-03-10 06:31:19+00:00,,0,6,triaged,False
74014,torch.utils.data.Dataset combined with pycuda issue triaged module: data,2022-03-10 03:18:39+00:00,,0,5,triaged module: data,True
73994,Lazy: add support for index ops triaged module: lazy,2022-03-09 23:51:00+00:00,,0,0,triaged module: lazy,True
73992,[FX] Tensor constants are not lifted to attributes in direct Graph construction triaged module: fx,2022-03-09 23:34:49+00:00,,0,0,triaged module: fx,True
73989,Invalid example tensor: detectron2_maskrcnn triaged module: lazy,2022-03-09 22:41:22+00:00,,1,0,triaged module: lazy,True
73985,Release note bot is not sending messages on PRs anymore oncall: releng triaged,2022-03-09 22:20:34+00:00,,0,0,oncall: releng triaged,False
73972,Forward_AD and Torchscript Functions results in Nones or wrong values. oncall: jit module: autograd module: forward ad,2022-03-09 17:06:44+00:00,,0,2,oncall: jit module: autograd module: forward ad,True
73960,processes hang when executing cross-machine asynchronous P2P communication on NCCL backend oncall: distributed module: nccl module: c10d,2022-03-09 13:47:41+00:00,,1,6,oncall: distributed module: nccl module: c10d,True
73941,"[Quant] Framework observes weight in convert, changing numerics oncall: quantization low priority triaged",2022-03-08 21:48:52+00:00,,1,0,oncall: quantization low priority triaged,True
73930,"User raised TypeError from __torch_dispatch__ gets turned into generic ""unsupported operand type"" message module: bootcamp triaged module: __torch_dispatch__",2022-03-08 18:38:02+00:00,,0,7,module: bootcamp triaged module: __torch_dispatch__,True
73924,torch.cuda.amp: Remove SPMD DDP doc portion oncall: distributed module: docs module: cuda triaged module: amp (automated mixed precision),2022-03-08 16:34:25+00:00,,0,0,oncall: distributed module: docs module: cuda triaged module: amp (automated mixed precision),True
73920,"pytorch/pytorch:1.8.1-cuda10.2-cudnn7-devel docker container contains cudnn 8.2, not 7.x as the name implies triaged module: docker",2022-03-08 16:19:58+00:00,,0,1,triaged module: docker,True
73916,Invalid code inrandom_ kernel module: distributions triaged,2022-03-08 12:15:22+00:00,,0,0,module: distributions triaged,True
73910,Adam optimizer doesn't work with CyclicLR scheduler but works with OneCycleLR. module: optimizer triaged module: LrScheduler,2022-03-08 08:26:26+00:00,,0,5,module: optimizer triaged module: LrScheduler,True
73909,SummaryWriter reports encoding error triaged module: tensorboard,2022-03-08 07:22:31+00:00,,0,0,triaged module: tensorboard,True
73885,operation not supported crash when initializing RPC tensorpipe high priority triage review oncall: distributed module: crash triaged,2022-03-07 23:01:57+00:00,,0,1,high priority triage review oncall: distributed module: crash triaged,True
73883,Possible issue with memory allocation. module: memory usage triaged module: numpy,2022-03-07 22:49:59+00:00,,0,3,module: memory usage triaged module: numpy,True
73871,ONNX: Document public API for shape inference for custom symbolics module: onnx triaged onnx-triaged,2022-03-07 20:41:30+00:00,,0,1,module: onnx triaged onnx-triaged,True
73870,max_pool1d() returns when given invalid large `kernel_size` inputs module: nn triaged module: correctness (silent) module: edge cases,2022-03-07 20:32:16+00:00,,0,0,module: nn triaged module: correctness (silent) module: edge cases,True
73869,[FSDP][BE] TestAutoWrap should inherit from MultiProcessTestCase oncall: distributed triaged better-engineering module: fsdp,2022-03-07 20:17:58+00:00,,0,1,oncall: distributed triaged better-engineering module: fsdp,True
73860,tensor subclass and `__torch_function__` performance issues triaged module: __torch_function__,2022-03-07 18:25:50+00:00,,0,6,triaged module: __torch_function__,True
73848,torch.distributions.multinomial.Multinomial (an example mistake of docs)? module: distributions module: docs triaged,2022-03-07 09:22:50+00:00,,0,0,module: distributions module: docs triaged,True
73841,max_pool1d(): `RuntimeError: [enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory` triaged module: pooling,2022-03-06 22:18:14+00:00,,0,0,triaged module: pooling,True
73840,"max_pool1d(): argument 'dilation' must be tuple of ints, but found element of type int at pos 1 triaged module: pooling module: edge cases",2022-03-06 21:57:33+00:00,,0,3,triaged module: pooling module: edge cases,True
73838,"Tensor.new_tensor now preserves input tensor device, making it inconsistent with documentation triaged module: tensor creation",2022-03-06 16:18:28+00:00,,0,2,triaged module: tensor creation,True
73833,Support unscaling grad on CPU feature module: cpu triaged module: amp (automated mixed precision),2022-03-06 06:37:55+00:00,,2,1,feature module: cpu triaged module: amp (automated mixed precision),True
73832,Legacy sparse tensor constructor (e.g. torch.cuda.sparse.FloatTensor) silently ignores device kwarg triaged module: tensor creation,2022-03-06 04:10:45+00:00,,0,2,triaged module: tensor creation,True
73829,"Run libtorch examples, export error ""undefined reference to xxx"" module: binaries module: build module: abi triaged",2022-03-05 14:01:44+00:00,,0,6,module: binaries module: build module: abi triaged,True
73828,torch.profiler schedule function doesn't work as expected oncall: profiler,2022-03-05 13:53:59+00:00,,0,0,oncall: profiler,False
73826,Better Error report in torch.distribution.*.sample (when passing a non-iterable) module: distributions module: error checking triaged,2022-03-05 08:04:10+00:00,,0,2,module: distributions module: error checking triaged,True
73825,TypeError: cannot pickle 'torch._C._distributed_c10d._ProcessGroupGloo' object oncall: distributed triaged module: c10d module: ddp,2022-03-05 07:54:10+00:00,,0,6,oncall: distributed triaged module: c10d module: ddp,True
73792,Bug: torch.distributions.mixture_same_distribution._pad_mixture_dimension high priority module: distributions triaged module: correctness (silent),2022-03-04 17:03:58+00:00,,0,2,high priority module: distributions triaged module: correctness (silent),True
73784,[FSDP] Add Gradient Accumulation Outside `no_sync()` Compatibility with CPU Offloading feature triaged module: fsdp,2022-03-04 14:45:45+00:00,,0,0,feature triaged module: fsdp,False
73767,[Discussion][FSDP] Enhancements to auto_wrap_policy oncall: distributed triaged module: fsdp,2022-03-04 00:41:08+00:00,,0,6,oncall: distributed triaged module: fsdp,True
73764,Group convolution slower than manually running separate convolutions in CUDA streams module: performance module: cudnn module: cuda module: convolution triaged,2022-03-03 23:45:33+00:00,,0,9,module: performance module: cudnn module: cuda module: convolution triaged,True
73755,CI: Bake as many dependencies as we can in the AMI (windows) module: ci triaged,2022-03-03 20:45:59+00:00,,1,2,module: ci triaged,True
73738,torch.ao.quantization.ReuseInputObserver should also reuse the dtype of the input oncall: quantization low priority triaged,2022-03-03 14:44:46+00:00,,1,3,oncall: quantization low priority triaged,True
73722,Why is distributed RPC using the default pickler? oncall: distributed triaged module: rpc,2022-03-03 03:43:23+00:00,,0,1,oncall: distributed triaged module: rpc,True
73721,Audit exception rewrapping to ensure stack traces are preserved oncall: distributed triaged better-engineering module: rpc,2022-03-03 03:28:05+00:00,,0,0,oncall: distributed triaged better-engineering module: rpc,True
73714,Release pytorch docker images with newer python versions oncall: releng triaged module: docker,2022-03-03 01:15:47+00:00,,1,14,oncall: releng triaged module: docker,True
73711,"Refactor/Cleanup LazyTensor, LTCTensorImpl, Data triaged module: lazy",2022-03-03 01:05:09+00:00,,0,5,triaged module: lazy,True
73710,Refactor/Cleanup Lazy Tensor Core triaged module: lazy,2022-03-03 01:02:40+00:00,,0,1,triaged module: lazy,True
73709,DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training feature module: memory usage triaged,2022-03-03 00:48:13+00:00,,0,1,feature module: memory usage triaged,True
73699,How to get tolerance override in OpInfo-based test? module: docs triaged module: testing,2022-03-02 22:48:11+00:00,,0,8,module: docs triaged module: testing,True
73697,Improving error message RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior. module: autograd triaged better-engineering actionable,2022-03-02 22:28:46+00:00,,0,20,module: autograd triaged better-engineering actionable,True
73683,test_broadcast_coalesced_nccl fails on A100 GPUs oncall: distributed triaged module: c10d,2022-03-02 20:07:12+00:00,,0,1,oncall: distributed triaged module: c10d,True
73665,Nice way to override string representation of tensor subclasses triaged enhancement tensor subclass,2022-03-02 17:13:36+00:00,,0,1,triaged enhancement tensor subclass,True
73661,`torch.fx.ProxyableClassMeta` does not work if Proxy objects are not included in constructor arguments triaged module: fx,2022-03-02 16:54:55+00:00,,0,0,triaged module: fx,True
73656,torchrun: Hostname/endpoint mismatch not handled triaged module: c10d,2022-03-02 16:01:16+00:00,,0,11,triaged module: c10d,True
73646,Pure C binding/wrapper with libtorch for inference applications feature triaged,2022-03-02 14:41:18+00:00,,0,3,feature triaged,False
73640,[Feature Request] Allow torch.Generator to be passed to torch.nn.init functions module: nn triaged,2022-03-02 10:21:39+00:00,,0,3,module: nn triaged,False
73638,addcdiv is failing the ASAN test for zero divisors module: error checking module: tests triaged,2022-03-02 08:10:43+00:00,,0,2,module: error checking module: tests triaged,True
73608,`torch.jit.load` fails when function parameters use non-ASCII characters oncall: jit,2022-03-01 23:09:45+00:00,,0,0,oncall: jit,False
73604,Support views in custom autograd functions module: autograd triaged enhancement actionable tensor subclass,2022-03-01 21:21:40+00:00,,0,7,module: autograd triaged enhancement actionable tensor subclass,False
73602,Report DDP efficiency metric to users and guide them for setting up DDP correctly high priority triage review oncall: distributed triaged,2022-03-01 20:39:34+00:00,,0,0,high priority triage review oncall: distributed triaged,True
73600,Add a section in DDP tutorial to explain why DDP sometimes is slower than local training and how to improve it oncall: distributed triaged module: ddp,2022-03-01 20:34:58+00:00,,0,0,oncall: distributed triaged module: ddp,True
73585,Add optional log_scale argument for torch.distributions.Normal module: distributions triaged,2022-03-01 18:52:51+00:00,,0,0,module: distributions triaged,True
73571,conv with padding='same' fails on lazy torchscript nvfuser oncall: jit,2022-03-01 16:49:08+00:00,,0,0,oncall: jit,False
73568,Bug when using `nn.Linear` needs reproduction module: cuda triaged module: linear algebra,2022-03-01 16:20:12+00:00,,0,3,needs reproduction module: cuda triaged module: linear algebra,True
73563,Pytorch Installation from source fails module: build triaged,2022-03-01 08:59:45+00:00,,0,1,module: build triaged,True
73556,Files exported by `profiler.export_stacks()` are not compatible with flamegraph oncall: profiler,2022-03-01 02:22:37+00:00,,0,2,oncall: profiler,False
73554,Build failure using GCC 11.2.0 module: build triaged module: mkldnn module: third_party,2022-03-01 01:27:40+00:00,,0,5,module: build triaged module: mkldnn module: third_party,True
73542,Investigate why tensor shapes are not populated when printing WorkNCCL for broadcast high priority triage review oncall: distributed better-engineering module: c10d module: ddp,2022-02-28 23:07:43+00:00,,0,2,high priority triage review oncall: distributed better-engineering module: c10d module: ddp,True
73537,Functionalization doesn't work when applied twice with wrapper tensor triaged tensor subclass module: functionalization,2022-02-28 21:25:17+00:00,,0,7,triaged tensor subclass module: functionalization,True
73534,"PyTorch for ROCm on a Supported Device Throws ""hipErrorNoBinaryForGpu"" module: rocm triaged",2022-02-28 21:11:21+00:00,,0,4,module: rocm triaged,False
73531,Mention docker build process in RELEASE.md and automate building those for release module: ci triaged enhancement module: docker,2022-02-28 20:08:35+00:00,,1,1,module: ci triaged enhancement module: docker,True
73517,Teach tools.codegen.api.translate about IValues module: bootcamp triaged better-engineering module: codegen,2022-02-28 16:59:53+00:00,,0,0,module: bootcamp triaged better-engineering module: codegen,True
73515,`torch.distributed.nn.functional.all_gather`: Tensors must be contiguous oncall: distributed module: bootcamp triaged better-engineering pt_distributed_rampup module: c10d,2022-02-28 16:28:24+00:00,,1,6,oncall: distributed module: bootcamp triaged better-engineering pt_distributed_rampup module: c10d,True
73513,Wrapper tensor level confusion triaged needs research module: __torch_dispatch__ tensor subclass,2022-02-28 16:07:41+00:00,,0,4,triaged needs research module: __torch_dispatch__ tensor subclass,True
73507,Implement SiLU method for the QuantizedCPU backend oncall: quantization low priority triaged,2022-02-28 14:44:23+00:00,,0,2,oncall: quantization low priority triaged,True
73506,Idiom for extensible string printing for TensorImpl subclasses module: internals module: printing triaged,2022-02-28 14:36:51+00:00,,0,4,module: internals module: printing triaged,True
73505,Figure out what to do with functions that take both Tensor and TensorOptions triaged module: codegen,2022-02-28 14:03:42+00:00,,0,0,triaged module: codegen,True
73504,`ge` and `div` behaves differently when converting an overflow number triaged module: type promotion module: edge cases,2022-02-28 12:52:37+00:00,,0,0,triaged module: type promotion module: edge cases,True
73502,`storage` does support `complex32` tensor triaged module: complex module: half,2022-02-28 11:23:57+00:00,,0,3,triaged module: complex module: half,True
73501,`index_copy` has different index behavior with `index_fill` triaged module: advanced indexing,2022-02-28 11:15:12+00:00,,0,1,triaged module: advanced indexing,True
73488,After XNNPack update `TestXNNPACKSerDes.test_linear` started to fail high priority module: rocm triaged module: regression module: xnnpack,2022-02-28 00:44:45+00:00,,1,4,high priority module: rocm triaged module: regression module: xnnpack,True
73487,PyTorch not recognizing GPU on WSL - installed cudnn and cuda module: cuda triaged module: wsl,2022-02-27 23:35:31+00:00,,0,42,module: cuda triaged module: wsl,True
73484,Rename `keep_vars` in `nn.Module.state_dict` module: nn triaged,2022-02-27 17:09:52+00:00,,0,0,module: nn triaged,True
73479,Strange case of empty non-coalesced sparse tensor module: sparse triaged,2022-02-26 22:22:32+00:00,,0,16,module: sparse triaged,True
73475,different results on each batch_size in torch==1.10.2+cu113 on RTX 3080 triaged module: numerical-reproducibility,2022-02-26 18:38:13+00:00,,0,3,triaged module: numerical-reproducibility,True
73474,DISABLED test_python_ir_utils (__main__.TestJit) oncall: jit skipped,2022-02-26 17:33:59+00:00,,0,2,oncall: jit skipped,False
73469,torch.fx failed when tracing functions from other Libs. triaged module: fx,2022-02-26 07:16:24+00:00,,0,1,triaged module: fx,True
73466,[feature request] torch.hub.load_state_dict_from_url to be replaced by a new good general download-a-file function and to also support local paths and google drive links / private github release links triaged module: hub,2022-02-26 01:33:00+00:00,,0,9,triaged module: hub,False
73461,`TestCommonCUDA.test_noncontiguous_samples_pca_lowrank_cuda_float32` fails on A100 due to TF32 operation in `svd_lowrank` module: cuda module: tests triaged module: linear algebra module: tf32,2022-02-25 23:49:38+00:00,,0,0,module: cuda module: tests triaged module: linear algebra module: tf32,True
73446,[FSDP] test full_state_dict if we are already in full parameter summoning context oncall: distributed module: fsdp,2022-02-25 18:56:30+00:00,,1,0,oncall: distributed module: fsdp,False
93719,Use bytecode rewriting to offer a __future__ feature triaged oncall: pt2 module: dynamo,2022-02-25 15:47:56+00:00,,0,6,triaged oncall: pt2 module: dynamo,False
73425,ufunc codegen support for dtypes that are supported on CUDA but not CPU triaged module: codegen,2022-02-25 14:12:42+00:00,,0,0,triaged module: codegen,True
73424,"""munmap_chunk(): invalid pointer"" interaction error with pytorch (< 1.10), pybind, and cv_bridge triaged module: lts",2022-02-25 13:12:06+00:00,,0,2,triaged module: lts,True
73419,test_jit_cuda_fuser fails on non-CUDA node for 1.11.0rc3 oncall: jit,2022-02-25 10:42:13+00:00,,0,0,oncall: jit,False
73417,MaybeEncodingError: Error sending result module: multiprocessing module: dataloader triaged,2022-02-25 09:40:19+00:00,,0,1,module: multiprocessing module: dataloader triaged,True
73413,Conv3D consumes lots of memory on Mac with Apple Silicon module: convolution triaged module: macos,2022-02-25 03:03:24+00:00,,0,1,module: convolution triaged module: macos,True
73412,Build from source failed module: build triaged,2022-02-25 02:33:16+00:00,,0,1,module: build triaged,True
73394,Make it easier to figure out if packages need to be interned/mocked/externed module: build triaged,2022-02-24 21:58:45+00:00,,1,0,module: build triaged,True
73380,elastic/rendezvous: _matches_machine_hostname doesn't resolve hostnames fully triaged module: elastic oncall: r2p topic: bug fixes,2022-02-24 20:51:50+00:00,,2,1,triaged module: elastic oncall: r2p topic: bug fixes,True
73359,"Pytorch Typing, for Tensor type annotations module: typing triaged",2022-02-24 16:41:20+00:00,,0,2,module: typing triaged,True
73355,Review and refactor  the way libcublas static libraries are linked module: build oncall: releng triaged module: cublas,2022-02-24 14:43:12+00:00,,0,0,module: build oncall: releng triaged module: cublas,True
73354,Add the capability to export GradMultiply to ONNX module: onnx triaged,2022-02-24 12:17:59+00:00,,1,2,module: onnx triaged,False
73352,Segfault on unloading a model oncall: jit module: crash module: cpp,2022-02-24 09:34:17+00:00,,0,0,oncall: jit module: crash module: cpp,True
73349,Large performance difference of loss.backward() between torch-1.9.0 and torch-1.8.0 module: autograd triaged module: regression,2022-02-24 09:07:58+00:00,,0,2,module: autograd triaged module: regression,True
73332,BatchNorm with LSTM in DistributedDataParallel throws one of the variables needed for gradient computation has been modified by an inplace operation oncall: distributed,2022-02-24 00:35:04+00:00,,0,4,oncall: distributed,False
73304,"fx.symbolic_trace docs says that it ignores different values of concrete_args, but it doesn't triaged module: fx",2022-02-23 18:43:31+00:00,,1,1,triaged module: fx,False
73303,"[FSDP] test state_dict APIs with arguments such as destination, prefix oncall: distributed module: fsdp",2022-02-23 18:41:53+00:00,,0,1,oncall: distributed module: fsdp,False
73294,Internal assert failed at rref_context.cpp oncall: distributed module: crash,2022-02-23 15:55:24+00:00,,0,7,oncall: distributed module: crash,True
73268,Add detection of interned submodule of externed module in PackageExporter triaged module: deploy,2022-02-23 01:22:11+00:00,,1,0,triaged module: deploy,True
73236,Inconsistent numpy indexing triaged module: numpy module: advanced indexing,2022-02-22 20:33:51+00:00,,0,4,triaged module: numpy module: advanced indexing,True
73222,Having rrelu functional + module take a generator object to match native functions entry module: nn triaged module: random,2022-02-22 18:58:06+00:00,,0,0,module: nn triaged module: random,True
73218,"This is not completely a bug, but something that will be amazing if can be taken care of regarding nn.DataParallel. triaged module: data parallel",2022-02-22 18:44:54+00:00,,0,1,triaged module: data parallel,True
73205,Bug in label smoothing with ignored samples module: loss triaged,2022-02-22 12:35:23+00:00,,0,4,module: loss triaged,True
73196,`torch.pow` errors out on specific input module: cuda triaged,2022-02-21 21:26:48+00:00,,0,0,module: cuda triaged,True
73190,Segmentation fault in max_pool1d module: error checking triaged module: pooling module: edge cases,2022-02-21 17:37:00+00:00,,0,1,module: error checking triaged module: pooling module: edge cases,True
73186,Segmentation fault in fractional_max_pool3d high priority module: cpu module: error checking triaged module: pooling,2022-02-21 17:31:43+00:00,,0,1,high priority module: cpu module: error checking triaged module: pooling,True
73185,Segmentation fault in fractional_max_pool2d high priority module: cpu module: error checking triaged module: pooling,2022-02-21 17:30:40+00:00,,0,1,high priority module: cpu module: error checking triaged module: pooling,True
73182,Segmentation fault in _sobol_engine_scramble_ module: error checking triaged module: random module: edge cases,2022-02-21 17:28:15+00:00,,0,0,module: error checking triaged module: random module: edge cases,True
73181,Segmentation fault in _sobol_engine_initialize_state_ module: error checking triaged module: random module: edge cases,2022-02-21 17:26:58+00:00,,0,0,module: error checking triaged module: random module: edge cases,True
73180,Segmentation fault in _sobol_engine_ff_ module: error checking triaged module: random module: edge cases,2022-02-21 17:25:46+00:00,,0,0,module: error checking triaged module: random module: edge cases,True
73179,Floating point exception in _nnpack_spatial_convolution high priority module: cpu module: error checking module: convolution triaged module: nnpack,2022-02-21 17:23:42+00:00,,0,1,high priority module: cpu module: error checking module: convolution triaged module: nnpack,True
73176,"Cuda lacks checking of ""out of bound"" module: nn module: loss module: cuda module: error checking triaged",2022-02-21 14:44:36+00:00,,0,6,module: nn module: loss module: cuda module: error checking triaged,True
73175,[feature request] Support tensor count vector argument in torch.split  module: bootcamp triaged enhancement,2022-02-21 14:25:48+00:00,,0,5,module: bootcamp triaged enhancement,False
73174,`LayerNorm` triggers INTERNAL ASSERT high priority module: cpu module: error checking triaged module: norms and normalization,2022-02-21 12:55:16+00:00,,0,1,high priority module: cpu module: error checking triaged module: norms and normalization,True
73172,LazyLinear with equal in_features and out_features module: nn triaged needs research,2022-02-21 12:13:41+00:00,,0,1,module: nn triaged needs research,True
73171,libtorch need operator= in torch::Device module: cpp triaged enhancement,2022-02-21 10:47:57+00:00,,0,1,module: cpp triaged enhancement,True
73161,native_batch_norm and native_layer_norm have strange epsilon behaviors triaged module: norms and normalization,2022-02-20 19:29:35+00:00,,0,2,triaged module: norms and normalization,True
73160,Preprocessing function for backend coreml is not registered triaged oncall: mobile,2022-02-20 18:55:40+00:00,,0,3,triaged oncall: mobile,False
73154,`max_unpool2d` returns a tensor with negative dimension high priority module: cpu triaged module: pooling,2022-02-20 10:20:12+00:00,,0,3,high priority module: cpu triaged module: pooling,True
73150,How do we handle metadata-modifying in-place operators (like `squeeze_`) with `__torch_dispatch__`? triaged module: __torch_dispatch__,2022-02-20 00:56:15+00:00,,0,3,triaged module: __torch_dispatch__,True
73145,DISABLED test_sparse_addmm_cpu_bfloat16 (__main__.TestSparseCPU) module: sparse triaged skipped,2022-02-19 21:02:09+00:00,,1,22,module: sparse triaged skipped,False
73143,Add nan-safe einsum and bilinear triaged enhancement module: linear algebra needs design,2022-02-19 16:04:31+00:00,,0,2,triaged enhancement module: linear algebra needs design,True
73142,Support passing Python code as a string to torch.jit.script oncall: jit,2022-02-19 11:18:56+00:00,,0,3,oncall: jit,False
73141,libtorch: `DistributedRandomSampler` uses the same random order in every epoch triaged module: data,2022-02-19 10:19:49+00:00,,0,15,triaged module: data,True
73137,`create_graph=True` results in grad_fn error for differentiable functions module: autograd triaged,2022-02-19 03:42:48+00:00,,0,4,module: autograd triaged,True
73134,Test functionalization doesn't run module: internals triaged,2022-02-19 01:11:57+00:00,,1,1,module: internals triaged,True
73131,Support writing tensorboard traces to AWS S3 (and other cloud storage services) in profiler feature triaged oncall: profiler,2022-02-19 00:23:03+00:00,,0,1,feature triaged oncall: profiler,False
73121,make c++ logger preamble meaningful triaged module: c10d,2022-02-18 22:30:07+00:00,,0,0,triaged module: c10d,True
73113,PyTorch fails to compile on gcc 11.2 due to breakpad module: build triaged,2022-02-18 20:34:28+00:00,,0,4,module: build triaged,True
73083,Improve test_test_history.py module: ci triaged,2022-02-18 16:57:26+00:00,,1,0,module: ci triaged,True
73081,Additional out of the box DDP comm hooks oncall: distributed better-engineering,2022-02-18 16:34:09+00:00,,0,0,oncall: distributed better-engineering,False
73080,[FSDP checkpoint] Test replace_by_prefix util oncall: distributed triaged module: fsdp,2022-02-18 15:43:26+00:00,,0,0,oncall: distributed triaged module: fsdp,True
73074,Multiple new caffe2-related build failures. module: build caffe2 triaged,2022-02-18 12:38:27+00:00,,0,1,module: build caffe2 triaged,True
73072,Exporting model to TorchScript fails once I load the saved torchscript file oncall: jit,2022-02-18 10:13:12+00:00,,0,0,oncall: jit,False
73070,`enumerate_support` for continuous distributions module: distributions triaged,2022-02-18 08:21:01+00:00,,1,0,module: distributions triaged,True
73065,`index_fill_` accepts wrong dtype for meta tensors module: tests triaged module: meta tensors,2022-02-18 07:36:53+00:00,,0,0,module: tests triaged module: meta tensors,True
73062,"Clang Compilation Error: more than one constructor applies to convert from ""ptrdiff_t"" to ""c10::Scalar"" module: build module: cuda triaged module: macos",2022-02-18 04:05:06+00:00,,0,0,module: build module: cuda triaged module: macos,True
73051,Cannot install latest pytorch into Docker on Apple M1 module: binaries triaged module: macos module: arm,2022-02-17 23:24:21+00:00,,0,0,module: binaries triaged module: macos module: arm,True
73050,aten::batch_norm schema does not mention mutation of running_mean/running_var triaged,2022-02-17 23:01:41+00:00,,0,7,triaged,True
73048,Enhanced local_state_dict FSDP checkpoint tests oncall: distributed module: fsdp,2022-02-17 22:54:39+00:00,,1,0,oncall: distributed module: fsdp,False
73046,Run test_fsdp_core parity test for FSDP model checkpoint oncall: distributed module: fsdp,2022-02-17 22:38:58+00:00,,1,0,oncall: distributed module: fsdp,False
73043,[Main Issue] FSDP Model Checkpoint oncall: distributed triaged module: fsdp,2022-02-17 22:32:10+00:00,,2,0,oncall: distributed triaged module: fsdp,True
73034,Is it intentional that PyTorch linux binaries aren't manylinux1 compliant? module: binaries triaged,2022-02-17 21:08:57+00:00,,0,1,module: binaries triaged,True
73016,Unable to build and use libtorch function via pybind11: undefined symbol error upon import module: cpp triaged module: mkldnn,2022-02-17 17:11:11+00:00,,0,14,module: cpp triaged module: mkldnn,True
73009,Improve loading for nn.modules.lazy.LazyModuleMixin module: nn triaged actionable,2022-02-17 15:09:19+00:00,,0,2,module: nn triaged actionable,True
73008,Followup requires for MKL link issue / cannot find -lmkl_core module: binaries module: cpp triaged,2022-02-17 14:55:55+00:00,,0,7,module: binaries module: cpp triaged,True
73007,Feature: support better rendering for ..deprecated Sphinx directive module: docs triaged,2022-02-17 14:29:33+00:00,,0,2,module: docs triaged,False
73003,pin_memory hangs instead of throwing module: dataloader module: cuda triaged,2022-02-17 08:52:24+00:00,,0,2,module: dataloader module: cuda triaged,True
72960,Refactor/Cleanup lazy IR codegen triaged module: lazy,2022-02-17 00:34:22+00:00,,1,0,triaged module: lazy,True
72948,Feature: a consistent Python and C++ logging facility that handles different classes of warnings module: logging triaged,2022-02-16 21:58:59+00:00,,1,10,module: logging triaged,True
72933,Why there are 8 flavors of iOS build jobs for every commit module: ci triaged module: ios,2022-02-16 20:01:24+00:00,,0,7,module: ci triaged module: ios,True
72915,Is the current behavior with addcmul and integer dtypes intended? module: tests triaged,2022-02-16 15:13:21+00:00,,0,3,module: tests triaged,True
72911,Vectorized Jacobian and Hessian errors with ffts module: autograd triaged module: vmap,2022-02-16 12:13:19+00:00,,0,1,module: autograd triaged module: vmap,True
72909,"`torch.svd_lowrank` should set the default value of `q` as `min(6, m, n)` triaged module: linear algebra",2022-02-16 11:33:01+00:00,,0,1,triaged module: linear algebra,True
72906,Replace deprecated distutils package in torch/utils/tensorboard/__init__.py oncall: visualization,2022-02-16 08:32:26+00:00,,0,2,oncall: visualization,False
72897,Hanging Validation oncall: distributed module: cuda triaged module: deadlock module: amp (automated mixed precision) module: ddp,2022-02-16 00:19:13+00:00,,0,11,oncall: distributed module: cuda triaged module: deadlock module: amp (automated mixed precision) module: ddp,True
72880,Bazel fails in an obscure way if submodules are not initialized low priority triaged module: third_party module: bazel,2022-02-15 20:30:01+00:00,,0,3,low priority triaged module: third_party module: bazel,True
72874,SequentialLR scheduler incorrect initialization module: optimizer triaged module: LrScheduler,2022-02-15 19:53:38+00:00,,0,1,module: optimizer triaged module: LrScheduler,True
72859,Profiler crashes with ProfilerActivity.CUDA on AWS p4d.24xlarge with A100-SXM4-40GB high priority triage review module: cuda oncall: profiler,2022-02-15 16:57:09+00:00,,0,0,high priority triage review module: cuda oncall: profiler,True
72858,Profiler crashes in export_chrome_trace with seg fault if any of record_shapes=True or with_flops=True oncall: profiler,2022-02-15 16:54:31+00:00,,0,2,oncall: profiler,True
72850,N-dimensional Convolutions module: convolution triaged enhancement,2022-02-15 13:33:52+00:00,,0,0,module: convolution triaged enhancement,True
72835,torch.distributed hangs at barrier() oncall: distributed module: c10d,2022-02-15 00:35:42+00:00,,0,3,oncall: distributed module: c10d,False
72831,Toggling deterministic mode for individual autograd backward functions module: autograd triaged needs research module: determinism,2022-02-15 00:20:51+00:00,,1,11,module: autograd triaged needs research module: determinism,True
72824,Some loss functions support `dtype` broadcast but some do not module: nn module: loss triaged module: type promotion,2022-02-14 23:54:56+00:00,,0,2,module: nn module: loss triaged module: type promotion,True
72821,"`{Batch,Instance}Norm{1,2,3}d` works when `num_features != C`! module: nn triaged",2022-02-14 23:43:38+00:00,,0,0,module: nn triaged,True
72814,Standardize Naming for Workflows/Jobs module: ci triaged,2022-02-14 21:58:57+00:00,,0,1,module: ci triaged,True
72791,"why ram memory surges while loading model, with change in torch load device from CPU to GPU module: cuda module: memory usage triaged",2022-02-14 09:49:00+00:00,,0,3,module: cuda module: memory usage triaged,True
72788,Docs bug: type annotations for linspace (and logspace) start and end arguments is wrong module: docs triaged module: tensor creation,2022-02-14 07:37:24+00:00,,1,8,module: docs triaged module: tensor creation,True
72784,Torch version in docker container does not match tag triaged module: docker,2022-02-14 01:10:14+00:00,,0,0,triaged module: docker,True
72782,Error during training: falseINTERNAL ASSERT FAILED high priority needs reproduction module: multiprocessing triaged module: assert failure,2022-02-13 23:31:53+00:00,,0,12,high priority needs reproduction module: multiprocessing triaged module: assert failure,True
72775,[vulkan] Vulkan backend fails creating tensor on x86_64 Linux triaged module: vulkan,2022-02-13 12:31:36+00:00,,0,2,triaged module: vulkan,True
72768,Add NCCL and MPI version printing to torch.utils.collect_env module: collect_env.py triaged module: mpi enhancement module: nccl,2022-02-12 18:40:57+00:00,,0,0,module: collect_env.py triaged module: mpi enhancement module: nccl,True
72766,Feature Request: Deterministic MaxPool3d and AvgPool3d feature module: nn triaged module: determinism module: pooling,2022-02-12 06:44:23+00:00,,0,7,feature module: nn triaged module: determinism module: pooling,True
72759,Add softplus inverse module: numerical-stability triaged function request,2022-02-12 00:25:16+00:00,,0,4,module: numerical-stability triaged function request,True
72749,"pytorchmergebot doesn't react to comments left from ""files"" tab module: ci triaged",2022-02-11 22:03:32+00:00,,1,6,module: ci triaged,True
72746,CPU execution/dispatch time dominates and slows down small TorchScript GPU models  oncall: jit,2022-02-11 21:33:48+00:00,,0,0,oncall: jit,True
72737,Pattern Matching with Tensors feature triaged function request,2022-02-11 20:05:48+00:00,,0,3,feature triaged function request,False
72714,Add `pct_end` parameter to `OneCycleLR` module: optimizer triaged enhancement module: LrScheduler,2022-02-11 15:32:04+00:00,,0,0,module: optimizer triaged enhancement module: LrScheduler,True
72712,Better support for pypip packages implementing torch cuda extentions module: binaries feature triaged needs design release notes: releng,2022-02-11 14:48:25+00:00,,0,6,module: binaries feature triaged needs design release notes: releng,True
72711,Feature request: Implement `gelsd` backend for `linalg.lstsq` via `linalg.svd` feature triaged module: linear algebra,2022-02-11 13:54:22+00:00,,0,2,feature triaged module: linear algebra,False
72709,Feature: Use iterative refinement algorithm from cuSOLVER for the least-squares solver on CUDA feature module: cuda triaged module: linear algebra,2022-02-11 11:24:33+00:00,,0,5,feature module: cuda triaged module: linear algebra,True
72672,nn.functional No-batch-dim support should have OpInfo examples module: nn module: tests triaged,2022-02-10 21:02:50+00:00,,0,0,module: nn module: tests triaged,True
72659, Add a unit test which uses an extension module + ordered importer  oncall: package/deploy imported,2022-02-10 16:57:15+00:00,,1,0,oncall: package/deploy imported,False
72650,Some inplace ops don't raise on incompatible shapes and meta device module: tests triaged module: meta tensors,2022-02-10 07:58:15+00:00,,0,1,module: tests triaged module: meta tensors,True
72643,upstream `apex.normalization.FusedRMSNorm` feature module: nn triaged module: norms and normalization,2022-02-10 01:41:39+00:00,,0,24,feature module: nn triaged module: norms and normalization,True
72617,TorchScript assertion failure for a `List[...]` inside a `NamedTuple` oncall: jit,2022-02-09 21:06:39+00:00,,0,0,oncall: jit,True
72591,"Can't forward pass conv2d with kernel_size=1, and padding=1 needs reproduction module: build module: nn module: convolution triaged module: macos module: nnpack module: arm",2022-02-09 17:30:59+00:00,,0,2,needs reproduction module: build module: nn module: convolution triaged module: macos module: nnpack module: arm,True
72582,"Avoid using thrust:: directly, use THRUST_NS_QUALIFIER:: instead module: build module: cuda triaged",2022-02-09 12:48:12+00:00,,0,7,module: build module: cuda triaged,True
72581,Zero-copy on shared memory of NVIDIA Jetson devices feature triaged module: jetson,2022-02-09 12:14:21+00:00,,0,3,feature triaged module: jetson,False
72580,"Setting a list of Modules as an attribute does not work like setting a Module as an attribute, and throws no warning module: nn triaged",2022-02-09 11:30:40+00:00,,0,6,module: nn triaged,True
72558,Pin dependencies + expand the current linter  module: bootcamp module: ci triaged,2022-02-08 23:14:27+00:00,,0,0,module: bootcamp module: ci triaged,True
72556,"Clarify test dependencies (e.g., into a test-requirements.txt file) module: bootcamp module: ci triaged",2022-02-08 23:11:30+00:00,,0,2,module: bootcamp module: ci triaged,True
72555,Enforce quotas on CI users module: ci triaged enhancement,2022-02-08 23:05:01+00:00,,0,2,module: ci triaged enhancement,True
72545,Split up torch.distributions docs into multiple pages module: distributions module: docs triaged,2022-02-08 20:55:14+00:00,,0,2,module: distributions module: docs triaged,True
72537,Feature: Add tril_embed and triu_embed feature triaged module: linear algebra module: tensor creation,2022-02-08 19:28:17+00:00,,0,1,feature triaged module: linear algebra module: tensor creation,True
72536,Remove Caffe2 triage review caffe2 better-engineering,2022-02-08 19:26:11+00:00,,0,0,triage review caffe2 better-engineering,True
72525,KL divergence between two Continuous Bernoulli is negative module: distributions triaged,2022-02-08 18:00:16+00:00,,0,0,module: distributions triaged,True
72516,test_del (jit.test_builtins.TestBuiltins) fails due to highlight assertions oncall: jit,2022-02-08 10:38:51+00:00,,1,2,oncall: jit,True
72498,torch.jit.script + torch.split + onnxruntime incompatibility oncall: jit,2022-02-08 06:21:36+00:00,,0,1,oncall: jit,False
72482,Improve torch::deploy documentation module: deploy oncall: package/deploy imported,2022-02-08 00:31:23+00:00,,2,0,module: deploy oncall: package/deploy imported,False
72452,Re-raise the exception when the `forward` of a parametrization raises module: nn triaged module: nn.utils.parametrize,2022-02-07 21:02:10+00:00,,0,5,module: nn triaged module: nn.utils.parametrize,True
72450,Memory parity with JAX attention module: memory usage oncall: transformer/mha,2022-02-07 20:20:45+00:00,,0,20,module: memory usage oncall: transformer/mha,False
72448,Use Module `__getattr__` in `torch.ops` and friends triaged better-engineering,2022-02-07 19:45:42+00:00,,0,2,triaged better-engineering,True
72435,ProcessGroupWrapper: Additional Improvements oncall: distributed better-engineering module: c10d module: ddp,2022-02-07 16:48:29+00:00,,0,2,oncall: distributed better-engineering module: c10d module: ddp,True
72421,Feature Request: A simpler decorator for disabling mixed precision feature triaged module: amp (automated mixed precision),2022-02-07 14:02:58+00:00,,0,0,feature triaged module: amp (automated mixed precision),False
72418,input.dim() == 4 INTERNAL ASSERT FAILED mkldnn/Pooling.cpp:201 high priority needs reproduction module: nn triaged module: mkldnn module: pooling,2022-02-07 10:57:21+00:00,,0,1,high priority needs reproduction module: nn triaged module: mkldnn module: pooling,True
72408,Matrix multiplication is 30 times slower for integers than floats on CPU module: performance module: cpu triaged module: linear algebra,2022-02-07 04:58:18+00:00,,0,5,module: performance module: cpu triaged module: linear algebra,True
72393,Some system-installed headers are mistakenly used. module: build triaged,2022-02-06 13:41:37+00:00,,0,1,module: build triaged,True
72388,ArgMax for Multi Dimensional Tensor feature triaged module: ux module: reductions,2022-02-05 07:24:22+00:00,,0,0,feature triaged module: ux module: reductions,True
72366,Custom ProcessGroup Destructor Not Correctly Called in PT 1.10 oncall: distributed module: c10d module: ddp,2022-02-04 21:50:26+00:00,,0,2,oncall: distributed module: c10d module: ddp,True
72362,Add ZeroTensor fastpath for torch.bmm triaged ZeroTensor,2022-02-04 20:32:03+00:00,,0,0,triaged ZeroTensor,False
72361,Add ZeroTensor fastpath for torch.baddbmm triaged ZeroTensor,2022-02-04 20:31:08+00:00,,0,0,triaged ZeroTensor,False
72359,Add ZeroTensor fastpath for torch.addmv triaged ZeroTensor,2022-02-04 20:29:10+00:00,,0,0,triaged ZeroTensor,False
72358,Add ZeroTensor fastpath for torch.addbmm triaged ZeroTensor,2022-02-04 20:28:03+00:00,,0,0,triaged ZeroTensor,False
72347,Add ZeroTensor fastpath for torch.addcmul triaged ZeroTensor,2022-02-04 18:37:16+00:00,,0,0,triaged ZeroTensor,False
72346,torch.distributed.new_group() should consolidate the type of rank list oncall: distributed module: c10d,2022-02-04 18:35:27+00:00,,0,2,oncall: distributed module: c10d,False
72345,Add ZeroTensor fastpath for torch.addmm triaged ZeroTensor,2022-02-04 18:34:48+00:00,,1,0,triaged ZeroTensor,False
72341,InstanceNorm doesn't preserve memory format triaged module: memory format module: norms and normalization,2022-02-04 17:55:53+00:00,,0,2,triaged module: memory format module: norms and normalization,True
72338,[JIT] Channels last optimization pass oncall: jit,2022-02-04 16:52:36+00:00,,0,0,oncall: jit,False
72332,Ability to assign to `tensor.require_grad` might lead to bugs module: error checking triaged module: ux,2022-02-04 12:30:55+00:00,,0,0,module: error checking triaged module: ux,True
72330,Add and Mul torch tensors on Metal (IOS) oncall: mobile module: ios,2022-02-04 10:05:57+00:00,,1,2,oncall: mobile module: ios,False
72329,Can't Export Pytorch Model to ONNX module: onnx triaged onnx-needs-info,2022-02-04 09:45:13+00:00,,1,3,module: onnx triaged onnx-needs-info,False
72317,Feature request: a mode to disallow calling prototype or beta features module: bc-breaking triaged topic: bc breaking module: python frontend,2022-02-04 07:34:32+00:00,,0,2,module: bc-breaking triaged topic: bc breaking module: python frontend,True
72295,[JIT][Autocasting] Add autocasting & constant propagation as part of freezing oncall: jit,2022-02-03 21:50:06+00:00,,0,0,oncall: jit,False
72283,Add Sparsemax function to libtorch feature module: nn triaged,2022-02-03 19:28:54+00:00,,0,7,feature module: nn triaged,False
72280,DISABLED test_memory_profiler (__main__.TestProfiler) high priority triage review triaged module: flaky-tests skipped oncall: profiler,2022-02-03 19:11:52+00:00,,0,11,high priority triage review triaged module: flaky-tests skipped oncall: profiler,True
72263,Removing deprecated `cpp_custom_type_hack` oncall: jit oncall: quantization triaged better-engineering oncall: profiler,2022-02-03 16:02:21+00:00,,1,1,oncall: jit oncall: quantization triaged better-engineering oncall: profiler,True
72258,NotImplementedError in torch.distributions module: distributions triaged,2022-02-03 15:35:18+00:00,,0,4,module: distributions triaged,True
72257,`torch.hasnan` module: bootcamp triaged enhancement module: NaNs and Infs,2022-02-03 14:58:47+00:00,,0,3,module: bootcamp triaged enhancement module: NaNs and Infs,True
72254,Caffe2 uses FFMPEG functions that are deprecated in FFMPEG 4.0 and gone in 5.0 module: dependency bug caffe2 triaged,2022-02-03 11:50:46+00:00,,0,4,module: dependency bug caffe2 triaged,True
72253,Transformer Initialization module: docs module: nn triaged actionable oncall: transformer/mha,2022-02-03 10:08:48+00:00,,0,2,module: docs module: nn triaged actionable oncall: transformer/mha,True
72252,Mobile: minimize inferencing latency by having better control over available CPU cores oncall: mobile,2022-02-03 09:54:52+00:00,,1,0,oncall: mobile,False
72240,recurrent neural network module module: docs module: nn module: rnn triaged,2022-02-03 05:47:56+00:00,,0,2,module: docs module: nn module: rnn triaged,True
72220,[JIT] Assert that the autodiff implementation of backward() returns the correct number of values oncall: jit,2022-02-03 00:28:14+00:00,,1,0,oncall: jit,True
72194,`TestCase.assertEqual` has `equal_nan` default to `True` module: tests triaged module: testing,2022-02-02 19:41:34+00:00,,0,1,module: tests triaged module: testing,True
72189,test_fn_fwgrad_bwgrad_[trapezoid|trapz]_cuda_complex128 causes CUDA memory exception module: crash module: cuda module: ci triaged ZeroTensor,2022-02-02 18:35:18+00:00,,0,0,module: crash module: cuda module: ci triaged ZeroTensor,True
72182, test_fn_fwgrad_bwgrad_special_ndtr_cuda_float64 fails module: crash module: cuda triaged ZeroTensor,2022-02-02 17:13:51+00:00,,0,0,module: crash module: cuda triaged ZeroTensor,True
72179,"Mechanism for Tensor subclasses to ""disable autograd"" feature module: autograd triaged actionable module: __torch_dispatch__",2022-02-02 16:31:21+00:00,,0,1,feature module: autograd triaged actionable module: __torch_dispatch__,False
72175,"Torch.onnx.export, RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! module: onnx triaged onnx-triaged",2022-02-02 13:27:31+00:00,,0,14,module: onnx triaged onnx-triaged,False
72146,Also allow dicts as type of `params=` field in param groups of optimizers module: optimizer triaged enhancement,2022-02-01 22:17:10+00:00,,0,0,module: optimizer triaged enhancement,False
72138,[RFC] PyTorch Sharder for distributed training oncall: distributed triaged sharded_tensor,2022-02-01 20:29:49+00:00,,2,7,oncall: distributed triaged sharded_tensor,True
72134,Add support for `complex` `mean` to the `normal` operator module: distributions feature triaged module: complex,2022-02-01 20:06:18+00:00,,0,0,module: distributions feature triaged module: complex,True
72117,Force PyTorch to clear CUDA cache module: cuda triaged,2022-02-01 17:11:52+00:00,,0,8,module: cuda triaged,True
72112,Unable to parse IR generated by Lazy Tensor Core with TorchScript Backend oncall: jit module: lazy,2022-02-01 15:53:46+00:00,,0,3,oncall: jit module: lazy,False
72110,Conversion Error in pytorch mobile with metal module: memory format oncall: mobile,2022-02-01 15:26:18+00:00,,0,2,module: memory format oncall: mobile,True
72107,DistributedDataParallel creates too many threads oncall: distributed module: dataloader better-engineering module: ddp,2022-02-01 13:47:57+00:00,,0,2,oncall: distributed module: dataloader better-engineering module: ddp,True
72106,Update MKL version used with MAGMA module: ci triaged module: magma,2022-02-01 13:38:35+00:00,,0,4,module: ci triaged module: magma,True
72105,ImportError: cannot import name '_VF' from partially initialized module 'torch' needs reproduction low priority triaged,2022-02-01 12:50:53+00:00,,0,8,needs reproduction low priority triaged,True
72099,halt or exit function implementation oncall: distributed feature module: c10d,2022-02-01 03:06:47+00:00,,0,4,oncall: distributed feature module: c10d,False
72065,Batched sparse-sparse matrix multiplication/ sparse torch.einsum module: sparse feature triaged,2022-01-31 16:26:53+00:00,,0,8,module: sparse feature triaged,True
72061,"`svd_backward`: does not handle inputs of rank `r < min(m, n)`. module: autograd triaged module: linear algebra",2022-01-31 15:00:33+00:00,,1,0,module: autograd triaged module: linear algebra,True
72055,index_add : Inconsistent between CPU and CUDA module: autograd module: cuda triaged,2022-01-31 10:08:26+00:00,,0,0,module: autograd module: cuda triaged,True
72053,index_put : INTERNAL ASSERT FAILED high priority module: cuda triaged,2022-01-31 08:36:29+00:00,,1,1,high priority module: cuda triaged,True
72046,Building PyTorch with Vulkan backend don't work triaged module: vulkan,2022-01-30 18:20:04+00:00,,0,0,triaged module: vulkan,True
72045,`pip==22.0` breaks installation process high priority module: binaries oncall: releng triaged,2022-01-30 14:42:55+00:00,,0,14,high priority module: binaries oncall: releng triaged,True
72041,RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward module: autograd triaged module: nn.utils.parametrize,2022-01-29 19:25:30+00:00,,0,2,module: autograd triaged module: nn.utils.parametrize,True
72034,Bug about distributed launch oncall: distributed,2022-01-29 01:51:55+00:00,,0,6,oncall: distributed,True
72029,JIT function crashing or failing (depending on profiling) oncall: jit NNC,2022-01-29 00:48:44+00:00,,0,0,oncall: jit NNC,True
72025,[c10d] destruction of Store objects oncall: distributed module: c10d,2022-01-28 23:17:11+00:00,,0,1,oncall: distributed module: c10d,False
72022,[PT-D] To make the ShardedTensor Reshard more generic oncall: distributed triaged sharded_tensor,2022-01-28 22:59:32+00:00,,1,0,oncall: distributed triaged sharded_tensor,True
71988,multiple PRs on pytorch are closed by push to unrelated branches such as pytorch-canary triaged module: third_party,2022-01-28 15:26:52+00:00,,0,1,triaged module: third_party,True
71978,from_blob / make_tensor support MemoryFormat triaged module: memory format,2022-01-28 06:18:58+00:00,,0,0,triaged module: memory format,True
71951,Multivariate normal defined by eigendecomposition  module: distributions triaged,2022-01-27 23:42:23+00:00,,0,3,module: distributions triaged,True
71929,[JIT] Results are different when saving in TorchScript Format through train/eval mode oncall: jit,2022-01-27 19:04:24+00:00,,0,1,oncall: jit,False
71919,Graph Mode Quantization does not keep NamedTuple Information oncall: quantization low priority triaged module: fx,2022-01-27 16:12:51+00:00,,1,0,oncall: quantization low priority triaged module: fx,True
71918,Support mixed python scalar/tensor types for torch.clamp's min/max args feature triaged module: type promotion,2022-01-27 16:01:59+00:00,,0,1,feature triaged module: type promotion,True
71912,`torch._sources.normalize_source_lines()` and consequently `torch.jit.script` fail with lambda functions oncall: jit,2022-01-27 13:46:02+00:00,,0,2,oncall: jit,False
71911,Functions depending on SVD are broken for inputs with non-finite values with MKL 2022+ and OpenBLAS 0.3.15+ triaged module: mkl module: linear algebra module: openblas,2022-01-27 12:55:03+00:00,,0,0,triaged module: mkl module: linear algebra module: openblas,True
71896,CUDA Graph API Improvement triaged module: cuda graphs,2022-01-27 01:58:33+00:00,,0,6,triaged module: cuda graphs,True
71889,Make scale/zero_point accessible from TorchScript traced module for QFlaotFunctional modules oncall: quantization low priority triaged,2022-01-26 23:00:15+00:00,,0,1,oncall: quantization low priority triaged,True
71877,Add `output_size` argument to `Upsample` forward method (just like for `ConvTranspose` Modules) module: nn triaged enhancement,2022-01-26 20:08:41+00:00,,0,6,module: nn triaged enhancement,True
71872,Clarify the behavior of DataLoader sampler and batch_sampler parameters module: docs module: dataloader triaged,2022-01-26 19:06:53+00:00,,0,4,module: docs module: dataloader triaged,True
71855,Regression in multi-node training speed with Transformers + PyTorch high priority triage review oncall: distributed triaged module: regression oncall: transformer/mha module: ddp,2022-01-26 12:57:06+00:00,,0,3,high priority triage review oncall: distributed triaged module: regression oncall: transformer/mha module: ddp,True
71849,"addcmul, byte_channels_last fail test_nnc_correctness opinfo tests under UBSAN module: sanitizers NNC",2022-01-26 07:00:16+00:00,,0,0,module: sanitizers NNC,False
71843,Migrate master to main: https://github.com/pytorch/tutorials  triaged module: infra,2022-01-26 04:19:51+00:00,,0,4,triaged module: infra,True
71842,Migrate master to main: https://github.com/pytorch/ignite  triaged module: infra,2022-01-26 04:19:49+00:00,,0,0,triaged module: infra,True
71841,Migrate master to main: https://github.com/pytorch/ELF  triaged module: infra,2022-01-26 04:19:47+00:00,,0,0,triaged module: infra,True
71840,Migrate master to main: https://github.com/pytorch/captum  triaged module: infra,2022-01-26 04:19:41+00:00,,0,0,triaged module: infra,True
71839,Migrate master to main: https://github.com/pytorch/glow  triaged module: infra,2022-01-26 04:19:39+00:00,,0,0,triaged module: infra,True
71838,Migrate master to main: https://github.com/pytorch/serve  triaged module: infra,2022-01-26 04:19:37+00:00,,0,0,triaged module: infra,True
71837,Migrate master to main: https://github.com/pytorch/xla  triaged module: infra,2022-01-26 04:19:35+00:00,,0,0,triaged module: infra,True
71836,Migrate master to main: https://github.com/pytorch/QNNPACK  triaged module: infra,2022-01-26 04:19:32+00:00,,0,0,triaged module: infra,True
71835,Migrate master to main: https://github.com/pytorch/tnt  triaged module: infra,2022-01-26 04:19:30+00:00,,0,0,triaged module: infra,True
71834,Migrate master to main: https://github.com/pytorch/hub  triaged module: infra,2022-01-26 04:19:28+00:00,,0,0,triaged module: infra,True
71833,Migrate master to main: https://github.com/pytorch/extension-cpp  triaged module: infra,2022-01-26 04:19:26+00:00,,0,0,triaged module: infra,True
71832,Migrate master to main: https://github.com/pytorch/android-demo-app  triaged module: infra,2022-01-26 04:19:24+00:00,,0,0,triaged module: infra,True
71831,Migrate master to main: https://github.com/pytorch/translate  triaged module: infra,2022-01-26 04:19:16+00:00,,0,0,triaged module: infra,True
71830,Migrate master to main: https://github.com/pytorch/elastic  triaged module: infra,2022-01-26 04:19:13+00:00,,0,0,triaged module: infra,True
71828,Migrate master to main: https://github.com/pytorch/tvm  triaged module: infra,2022-01-26 04:19:09+00:00,,0,0,triaged module: infra,True
71826,Migrate master to main: https://github.com/pytorch/ios-demo-app  triaged module: infra,2022-01-26 04:19:04+00:00,,0,0,triaged module: infra,True
71825,Migrate master to main: https://github.com/pytorch/accimage  triaged module: infra,2022-01-26 04:19:01+00:00,,0,0,triaged module: infra,True
71824,Migrate master to main: https://github.com/pytorch/extension-ffi  triaged module: infra,2022-01-26 04:18:58+00:00,,0,0,triaged module: infra,True
71823,Migrate master to main: https://github.com/pytorch/nestedtensor  triaged module: infra,2022-01-26 04:18:56+00:00,,0,0,triaged module: infra,True
71822,Migrate master to main: https://github.com/pytorch/cppdocs  triaged module: infra,2022-01-26 04:18:54+00:00,,0,0,triaged module: infra,True
71821,Migrate master to main: https://github.com/pytorch/workshops  triaged module: infra,2022-01-26 04:18:52+00:00,,0,0,triaged module: infra,True
71820,Migrate master to main: https://github.com/pytorch/extension-script  triaged module: infra,2022-01-26 04:18:49+00:00,,0,0,triaged module: infra,True
71819,Migrate master to main: https://github.com/pytorch/java-demo  triaged module: infra,2022-01-26 04:18:47+00:00,,0,0,triaged module: infra,True
71818,Migrate master to main: https://github.com/pytorch/csprng  triaged module: infra,2022-01-26 04:18:44+00:00,,0,0,triaged module: infra,True
71817,Migrate master to main: https://github.com/pytorch/pytorch_sphinx_theme  triaged module: infra,2022-01-26 04:18:42+00:00,,0,0,triaged module: infra,True
71816,Migrate master to main: https://github.com/pytorch/rfcs  triaged module: infra,2022-01-26 04:18:40+00:00,,0,0,triaged module: infra,True
71815,Migrate master to main: https://github.com/pytorch/add-annotations-github-action  triaged module: infra,2022-01-26 04:18:37+00:00,,0,0,triaged module: infra,True
71814,Migrate master to main: https://github.com/pytorch/ossci-job-dsl  triaged module: infra,2022-01-26 04:18:35+00:00,,0,0,triaged module: infra,True
71813,Migrate master to main: https://github.com/pytorch/pytorch-integration-testing  triaged module: infra,2022-01-26 04:18:32+00:00,,0,0,triaged module: infra,True
71812,Migrate master to main: https://github.com/pytorch/pytorch-ci-dockerfiles  triaged module: infra,2022-01-26 04:18:29+00:00,,0,0,triaged module: infra,False
71810,Migrate master to main: https://github.com/pytorch/labeler-github-action  triaged module: infra,2022-01-26 04:17:56+00:00,,0,0,triaged module: infra,True
71806,Migrate master to main: https://github.com/pytorch/pytorch triaged module: infra,2022-01-26 03:24:13+00:00,,1,3,triaged module: infra,True
71784,[JIT] addmm differs from eager mode oncall: jit,2022-01-25 22:21:52+00:00,,0,1,oncall: jit,False
71774,matmul returns uninitialized memory for int64 tensors with inner dimension of zero high priority triaged module: linear algebra module: correctness (silent),2022-01-25 18:11:30+00:00,,0,2,high priority triaged module: linear algebra module: correctness (silent),True
71772,[ONNX] Support aten::bilinear module: onnx triaged onnx-triaged,2022-01-25 17:51:43+00:00,,0,1,module: onnx triaged onnx-triaged,True
71760,Adapting the citation style according to GitHub's CFF triaged module: doc infra,2022-01-25 11:40:03+00:00,,0,0,triaged module: doc infra,True
71725,"When someone calls detach() on a __torch_dispatch__ object, detach() gets called twice triaged module: __torch_dispatch__",2022-01-24 21:48:40+00:00,,0,2,triaged module: __torch_dispatch__,True
71721,Improve performance of index for quantized ops oncall: quantization low priority triaged,2022-01-24 21:10:28+00:00,,0,0,oncall: quantization low priority triaged,True
71715,[FX] Support call_method in NormalizeArgs triaged module: fx,2022-01-24 18:44:37+00:00,,0,0,triaged module: fx,True
71704,M1 Pro Apple Silicon chip support. needs reproduction triaged module: macos module: nnpack module: arm,2022-01-24 15:14:36+00:00,,0,2,needs reproduction triaged module: macos module: nnpack module: arm,True
71698,[libtorch]can not save a  vector<int> to AutogradContex->saved_data. module: cpp module: autograd triaged,2022-01-24 09:57:12+00:00,,0,2,module: cpp module: autograd triaged,True
71697,Libtorch dlls delayed loading module: cuda triaged module: static linking,2022-01-24 09:01:42+00:00,,0,2,module: cuda triaged module: static linking,True
71689,torch.distributions.categorical.Categorical does not work with 0 batch size module: distributions triaged,2022-01-24 02:08:52+00:00,,0,0,module: distributions triaged,True
71683,EMA optimizer: class-form and function-form (using new foreach_lerp) - can be used for explicit robust updates of BatchNorm stats module: optimizer triaged function request module: mta,2022-01-23 10:06:36+00:00,,0,7,module: optimizer triaged function request module: mta,False
71682,_GLIBCXX_USE_CXX11_ABI=0 does not work when building from source code module: build triaged,2022-01-23 06:13:47+00:00,,0,0,module: build triaged,True
71678,torch.bmm backward with sparse input module: sparse triaged,2022-01-22 22:13:28+00:00,,0,7,module: sparse triaged,True
71673,Fancy indexing bug when combining masks with indexes triaged module: numpy module: advanced indexing,2022-01-22 11:22:12+00:00,,0,18,triaged module: numpy module: advanced indexing,True
71671,Many APIs `INTERNAL ASSERT FAILED` when promoting `complex32` dtype triaged module: complex,2022-01-22 06:13:50+00:00,,0,3,triaged module: complex,True
71660,Build release binaries with USE_GLOG=ON by default module: build triaged enhancement module: infra,2022-01-21 23:09:43+00:00,,0,5,module: build triaged enhancement module: infra,True
71636,`torch.median` will return -2147483648 when input is an empty tensor module: error checking triaged,2022-01-21 14:09:14+00:00,,0,3,module: error checking triaged,True
71635,"`torch.nn.functional.{instance, batch}_norm` trigger INTERNAL ASSERT FAILED when input is empty tensor with `complex32` triaged module: complex module: half",2022-01-21 14:03:24+00:00,,0,1,triaged module: complex module: half,True
71633,Negative Exponents of Int tensors result in output of zero module: docs triaged,2022-01-21 12:24:54+00:00,,0,3,module: docs triaged,True
71631,Wrapping make_graphed_callables with autocast issue module: cuda triaged module: amp (automated mixed precision) module: cuda graphs,2022-01-21 07:48:49+00:00,,0,1,module: cuda triaged module: amp (automated mixed precision) module: cuda graphs,True
71630,More than 4 Dimensions for Channel Last Memory Format triaged enhancement module: memory format,2022-01-21 07:12:59+00:00,,0,0,triaged enhancement module: memory format,True
71629,"`torch.{max,min}` have strange error message when `input.numel()==0` triaged module: reductions",2022-01-21 06:37:39+00:00,,0,3,triaged module: reductions,True
71613,NCCL Backend does not support ComplexFloat data type high priority triage review oncall: distributed module: complex module: nccl has workaround,2022-01-21 00:59:17+00:00,,0,27,high priority triage review oncall: distributed module: complex module: nccl has workaround,True
71595,Optimizer Overlap: Follow up features oncall: distributed triaged module: ddp,2022-01-20 22:39:48+00:00,,1,0,oncall: distributed triaged module: ddp,True
71575,Better Engineering: test_..._mem_overlap in test_torch.py should be ported to ErrorInputs module: tests triaged better-engineering,2022-01-20 19:54:42+00:00,,0,2,module: tests triaged better-engineering,True
71574,Better Engineering: Create test_dlpack module: tests triaged better-engineering,2022-01-20 19:52:00+00:00,,0,0,module: tests triaged better-engineering,True
71552,[docs] Tensor.uniform_ docs are not clear about whether from/to boundary values are included in sampling or not module: docs triaged module: random,2022-01-20 15:06:26+00:00,,0,3,module: docs triaged module: random,False
71548,Support freezing of models containing ModuleContainerIndex oncall: jit,2022-01-20 12:54:35+00:00,,0,0,oncall: jit,False
71545,pip installation SSLError [SSL: CERTIFICATE_VERIFY_FAILED] needs reproduction module: binaries triaged,2022-01-20 08:58:23+00:00,,0,4,needs reproduction module: binaries triaged,True
71543,Initialize DataLoader workers in parallel triaged module: data,2022-01-20 07:15:43+00:00,,0,1,triaged module: data,True
71541,"`torch.sub` behaves differently with `add`, `mul`, `div` module: error checking triaged",2022-01-20 06:23:00+00:00,,0,2,module: error checking triaged,True
71518,Compilation instructions are not exhaustive: <<parameter packs not expanded with ‘...’>> on Fedora 35/CUDA 11.6 module: build triaged,2022-01-20 01:13:24+00:00,,0,9,module: build triaged,True
71495,Memory Leak in PyTorch 1.10.1 module: cuda module: memory usage triaged,2022-01-19 21:36:16+00:00,,0,2,module: cuda module: memory usage triaged,True
71479,[numpy compat] torch.stack and torch.tensor doesn't support nested list+tensors (NumPy does support) - at least document the difference in the error message triaged module: numpy needs design module: viewing and reshaping,2022-01-19 16:37:46+00:00,,0,5,triaged module: numpy needs design module: viewing and reshaping,True
71477,"`torch.cum{min,max}, torch.sort, argsort` do not check the `dim` when the input is 0-d tensor module: error checking triaged module: sorting and selection",2022-01-19 15:51:54+00:00,,0,2,module: error checking triaged module: sorting and selection,True
71475,The signature of `torch.nanmedian` in the doc is wrong triaged module: NaNs and Infs,2022-01-19 14:35:00+00:00,,0,0,triaged module: NaNs and Infs,True
71473,pytorch installation error with cuda!! module: binaries triaged,2022-01-19 13:35:40+00:00,,0,1,module: binaries triaged,True
71472,Feature request: Add complex support to `torch.nanmean` feature triaged module: complex module: NaNs and Infs module: reductions,2022-01-19 09:28:08+00:00,,0,2,feature triaged module: complex module: NaNs and Infs module: reductions,True
71471,"[pytorch1.5.0] subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '6']' returned non-zero exit status 2. module: build triaged",2022-01-19 09:17:27+00:00,,0,2,module: build triaged,True
71470,torch.jit.script failed to compile nn.MultiheadAttention when specifying the kdim and vdim parameters. oncall: jit,2022-01-19 07:52:50+00:00,,0,1,oncall: jit,True
71465,torch.nn.LayerNorm support for arbitrary axis in order to allow NCHW application high priority module: nn triaged,2022-01-19 05:31:20+00:00,,0,19,high priority module: nn triaged,True
71446,Discussion of TorchQuantum and QuantumNAS triaged module: complex complex_autograd,2022-01-18 21:03:12+00:00,,0,6,triaged module: complex complex_autograd,True
71414,Gamma distribution returns some wrong extreme values  module: numerical-stability module: distributions module: cuda triaged,2022-01-18 17:19:37+00:00,,0,0,module: numerical-stability module: distributions module: cuda triaged,True
71409,Random Shuffle along Axis feature triaged module: random,2022-01-18 16:17:30+00:00,,0,5,feature triaged module: random,True
71407,"Push to fork failed with cryptic ""refusing to allow a Personal Access Token to create or update workflow `.github/workflows/run_torchbench.yml` without `workflow` scope"" module: ci triaged",2022-01-18 15:40:32+00:00,,0,1,module: ci triaged,True
71404,[feature request] [discussion] Generalize / recommend behavior of reset_parameters (and potentially rename) module: nn triaged module: fx,2022-01-18 12:40:01+00:00,,0,11,module: nn triaged module: fx,True
71403,[discussion] torch.flatten to allow unsqueeze of inexisting dimension triaged module: batching,2022-01-18 12:15:26+00:00,,0,3,triaged module: batching,True
71398,cannot pickle 'torch._C.Generator' object for torch.Generator triaged module: random,2022-01-18 06:20:39+00:00,,0,0,triaged module: random,True
71396,a lot nightly builds are canceled due to VM errors since Jan14 module: build module: ci triaged,2022-01-18 04:07:54+00:00,,0,3,module: build module: ci triaged,True
71392,Error in `torch.trapz` documentation module: docs triaged,2022-01-18 01:50:10+00:00,,0,7,module: docs triaged,True
71391,Calling .backward() inside of an LBFGS closure function throws an exception in Libtorch v1.6.0+ module: cpp triaged module: regression,2022-01-18 01:37:36+00:00,,0,2,module: cpp triaged module: regression,True
71389,Create a nested directory while saving objects using `torch.save` module: serialization triaged enhancement,2022-01-17 20:57:58+00:00,,0,2,module: serialization triaged enhancement,True
71386,Direct Implementation of K-Nearest neighbor (KNN) in pytorch feature module: nn triaged needs design,2022-01-17 16:37:11+00:00,,0,11,feature module: nn triaged needs design,False
71383,Allow to shutdown persistent workers triaged module: multithreading,2022-01-17 12:04:53+00:00,,0,0,triaged module: multithreading,True
71379,Leaky cmake cuda compile options module: build module: cpp-extensions module: cpp triaged,2022-01-17 06:28:16+00:00,,0,0,module: build module: cpp-extensions module: cpp triaged,True
71377,is_alias_of support for storageless tensors triaged module: partial aliasing module: ddp module: lazy,2022-01-17 06:18:14+00:00,,0,0,triaged module: partial aliasing module: ddp module: lazy,True
71367,"F.cross_entropy do not have a deterministic implementation,  adding deterministic support for this operation. module: loss triaged",2022-01-16 12:13:33+00:00,,0,0,module: loss triaged,True
71366,nn.Batchnorm1d input shape notation inconsistency module: docs triaged module: norms and normalization,2022-01-16 09:45:43+00:00,,0,2,module: docs triaged module: norms and normalization,True
71364,make pytorch support different hardware acceleratioin configuration module: binaries module: cuda low priority triaged,2022-01-16 07:27:28+00:00,,0,1,module: binaries module: cuda low priority triaged,True
71359,"Torchvision Installation Logic Python vs C++ ""Mismatch"" module: build triaged",2022-01-15 23:36:07+00:00,,0,2,module: build triaged,True
71357,[feature request] Multidim support for softmax/log_softmax/softmin  triaged enhancement module: norms and normalization,2022-01-15 16:20:33+00:00,,0,0,triaged enhancement module: norms and normalization,False
71340,locally installed PyTorch upgraded or superseded on windows/macos conda tests triaged module: lts,2022-01-15 00:15:59+00:00,,0,3,triaged module: lts,True
71331,Change the order of checks for tensor indexing errors triaged module: advanced indexing,2022-01-14 21:29:30+00:00,,0,0,triaged module: advanced indexing,True
71330,[RFC] Gossip SGD (as a DDP Communication Hook) oncall: distributed feature module: optimizer triaged module: ddp,2022-01-14 21:27:21+00:00,,0,3,oncall: distributed feature module: optimizer triaged module: ddp,True
71329,"[RFC] Implement array methods (extend, insert, pop, etc) for container classes feature module: nn triaged",2022-01-14 21:15:37+00:00,,0,1,feature module: nn triaged,True
71322,Allow users to pass use_reentrant=False to checkpoint_sequential module: checkpoint triaged,2022-01-14 19:47:51+00:00,,0,0,module: checkpoint triaged,True
71317,[torch.deploy] Using zipped torch modules in torch.deploy interpreter  triaged days oncall: package/deploy imported,2022-01-14 18:45:45+00:00,,1,0,triaged days oncall: package/deploy imported,True
71309,"path\\tp\\torch\\torch.h(14,1): fatal error C1001: Internal compiler error. module: dependency bug module: windows module: cpp triaged",2022-01-14 15:18:12+00:00,,1,9,module: dependency bug module: windows module: cpp triaged,True
71305,Convert a tensor with type caffe2::TypeMeta to std::vector triaged module: meta tensors,2022-01-14 12:32:28+00:00,,0,0,triaged module: meta tensors,True
71303,[RFC] Cross-Process Performance Analysis: Straggler Detection high priority triage review oncall: distributed feature module: c10d module: ddp,2022-01-14 09:31:25+00:00,,0,5,high priority triage review oncall: distributed feature module: c10d module: ddp,True
71301,Why AMP make backward speed more slow? module: cuda triaged,2022-01-14 07:51:21+00:00,,0,4,module: cuda triaged,True
71300,"Kernel fusion for Gather, Apply, Scatter (GAS) model module: sparse triaged module: scatter & gather ops",2022-01-14 07:40:52+00:00,,0,3,module: sparse triaged module: scatter & gather ops,True
71288,`memory_format` argument isn't supported in torchscript for tensor.is_contiguous() oncall: jit,2022-01-13 23:51:05+00:00,,0,0,oncall: jit,False
71286,"[JIT][tensorexpr] cat, batch_norm opinfo tests failing oncall: jit NNC",2022-01-13 23:22:28+00:00,,0,0,oncall: jit NNC,True
71277,JIT support for `torch.__version__` & str comparison operations oncall: jit module: bootcamp,2022-01-13 19:58:49+00:00,,0,1,oncall: jit module: bootcamp,True
71274,upstream `apex.optimizers.FusedAdam` to replace `torch.optim.AdamW` module: optimizer triaged,2022-01-13 19:34:59+00:00,,0,12,module: optimizer triaged,False
71272,"UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate   warnings.warn(""Seems like `optimizer.step()` has been overridden after learning rate scheduler needs reproduction module: optimizer triaged module: LrScheduler",2022-01-13 19:03:46+00:00,,0,4,needs reproduction module: optimizer triaged module: LrScheduler,True
71266,"""Memory Leak"" when creating an iterator from a tensor module: memory usage triaged",2022-01-13 14:32:52+00:00,,0,2,module: memory usage triaged,True
71263,AdaptiveAvgPool2d Failed to jit script oncall: jit,2022-01-13 12:06:29+00:00,,0,4,oncall: jit,True
71261,The jit model will fail when calling the torch.autograd.functional.jacobian with multiple inputs and setting the vectorize to true. oncall: jit module: bootcamp NNC,2022-01-13 09:32:04+00:00,,0,2,oncall: jit module: bootcamp NNC,True
71249,[docs] nn.Sequential docs should list member functions module: docs triaged,2022-01-12 23:47:00+00:00,,0,8,module: docs triaged,False
71228,1.10.11 fails to compile libtorch_cpu with -fopenmp module: build triaged module: undefined reference,2022-01-12 18:16:52+00:00,,0,0,module: build triaged module: undefined reference,True
71227,Move torch::deploy tests to their own workflow job high priority triaged module: deploy,2022-01-12 18:11:57+00:00,,1,2,high priority triaged module: deploy,True
71222,torch.linalg.lstsq is nondeterministic needs reproduction triaged module: determinism module: linear algebra,2022-01-12 17:16:23+00:00,,0,26,needs reproduction triaged module: determinism module: linear algebra,True
71211,Lack of type check in `nn.functional` APIs module: nn triaged enhancement,2022-01-12 14:57:05+00:00,,0,2,module: nn triaged enhancement,True
71210,axis to dim remapping is not working for flip and roll triaged module: python array api,2022-01-12 13:57:58+00:00,,0,3,triaged module: python array api,True
71209,support setting `keepdim` without setting `dim` triaged module: python array api,2022-01-12 13:56:27+00:00,,0,1,triaged module: python array api,True
71205,Possible security issue of `torch.hub.load` triaged module: hub,2022-01-12 08:53:50+00:00,,0,5,triaged module: hub,True
71204,`torch.diag` unexpectedly fails triaged module: numpy module: viewing and reshaping,2022-01-12 08:41:06+00:00,,0,2,triaged module: numpy module: viewing and reshaping,True
71203,Keys of a `ModuleDict` cannot have the same name as existing `ModuleDict` class attributes. module: nn triaged,2022-01-12 07:41:29+00:00,,0,9,module: nn triaged,True
71195,DISABLED test_send_recv_all_to_all (__main__.ProcessGroupGlooTest) oncall: distributed skipped,2022-01-12 02:13:14+00:00,,0,1,oncall: distributed skipped,False
71187,DataLoader tests are quite flaky high priority module: dataloader triaged,2022-01-11 23:04:08+00:00,,0,16,high priority module: dataloader triaged,True
71165,traced module fails on second execution oncall: jit,2022-01-11 18:22:14+00:00,,0,3,oncall: jit,True
71156,Slow backward for matrix multiplication of two sparse COO tensors on CPU module: sparse triaged,2022-01-11 11:02:07+00:00,,0,9,module: sparse triaged,True
71155,PyTorch bug: Cannot pass gradient through index_add high priority module: autograd triaged module: advanced indexing module: correctness (silent),2022-01-11 07:45:14+00:00,,1,5,high priority module: autograd triaged module: advanced indexing module: correctness (silent),True
71152,clang format hash mismatched for linux64 module: ci module: lint triaged,2022-01-11 06:28:10+00:00,,0,0,module: ci module: lint triaged,True
71151,The implement of `containsTensorType(const TypePtr& t)` in jit pass `PropagateInputShapes` ignore some situation? oncall: jit,2022-01-11 04:53:02+00:00,,0,0,oncall: jit,False
71149,Memory leak in distributions.multivariate_normal.MultivariateNormal high priority needs reproduction module: distributions module: memory usage triaged,2022-01-11 01:58:24+00:00,,0,6,high priority needs reproduction module: distributions module: memory usage triaged,True
71134,Slowdown in torch.distributed.new_group when scaling to large clusters.  oncall: distributed triaged module: c10d,2022-01-10 23:40:28+00:00,,0,0,oncall: distributed triaged module: c10d,True
71117,Rollup: forward-mode AD operator coverage module: autograd triaged actionable module: forward ad,2022-01-10 18:23:09+00:00,,0,6,module: autograd triaged actionable module: forward ad,True
71095,`TestOperators.test_c2_op` : different raw data generated for the test in my local and CI environment module: onnx triaged onnx-triaged,2022-01-10 15:19:58+00:00,,0,1,module: onnx triaged onnx-triaged,True
71084,"`torch.{inverse,cholesky}` have wrong shape check of square matrices module: error checking triaged module: linear algebra",2022-01-10 08:38:16+00:00,,0,1,module: error checking triaged module: linear algebra,True
71082,`torch.combinations` will allocate large memory when `r` is greater than the length of input feature module: memory usage triaged module: sorting and selection,2022-01-10 08:28:08+00:00,,0,5,feature module: memory usage triaged module: sorting and selection,True
71078,"`torch.nn.{Constant,Zero}Pad` unexpectedly fail module: error checking triaged module: padding",2022-01-10 06:04:25+00:00,,0,1,module: error checking triaged module: padding,True
71076,Error in `torch.Tensor.logit` documentation module: docs triaged,2022-01-10 04:44:04+00:00,,0,1,module: docs triaged,True
71071,Feature Request: torch.special.ellipe feature triaged module: special,2022-01-09 20:09:52+00:00,,0,4,feature triaged module: special,False
71069,C++ torch::nn::Sequential clone() method overwrites child module names module: cpp module: nn triaged actionable,2022-01-09 16:32:58+00:00,,0,1,module: cpp module: nn triaged actionable,True
71059,`torch.scatter` will return random value when `input` is empty tensor module: error checking triaged module: scatter & gather ops,2022-01-08 15:37:31+00:00,,0,1,module: error checking triaged module: scatter & gather ops,True
71058,`torch.Tensor.where` cannot work when `y` is float triaged module: type promotion,2022-01-08 15:18:11+00:00,,1,2,triaged module: type promotion,True
71049,Distributed broadcast fails with simple GPU tensor on Windows + GLOO oncall: distributed module: c10d,2022-01-08 05:49:44+00:00,,0,6,oncall: distributed module: c10d,True
71029,"remote failure INTERNAL ASSERT FAILED at ""../torch/csrc/distributed/rpc/rref_context.cpp"":389 oncall: distributed triaged module: rpc",2022-01-07 22:28:28+00:00,,0,1,oncall: distributed triaged module: rpc,True
71028,Skip LSTM quantization by default in get_default_qconfig_dict and get_default_qat_qconfig_dict oncall: quantization low priority triaged,2022-01-07 22:24:49+00:00,,1,1,oncall: quantization low priority triaged,True
71022,installation of pytorch `cpuonly` from conda with `nomkl` installs `mkl` module: binaries triaged,2022-01-07 21:15:33+00:00,,1,1,module: binaries triaged,True
70995,Rewrite tests in test_nn to not depend on LAPACK module: nn module: tests triaged actionable,2022-01-07 15:32:45+00:00,,0,3,module: nn module: tests triaged actionable,True
70970,argmin/argmax incorrect doc for the first form module: docs triaged module: reductions,2022-01-07 04:33:48+00:00,,1,1,module: docs triaged module: reductions,True
70954,Training grouped Conv2D is slow module: performance module: cudnn module: convolution triaged,2022-01-06 23:32:39+00:00,,0,16,module: performance module: cudnn module: convolution triaged,True
70951,A diagnostics mode to report when constraints aren't being met for optimal performance oncall: profiler,2022-01-06 23:08:32+00:00,,0,7,oncall: profiler,False
70940,"ThreadLocalState::setThreadLocalState is not setting the ""enabled"" flag of SavedTensorDefaultHooks triaged module: multithreading",2022-01-06 18:58:45+00:00,,0,0,triaged module: multithreading,True
70931,Redefinition of `cub` namespace misses Debug module: build module: cuda triaged,2022-01-06 17:59:24+00:00,,0,8,module: build module: cuda triaged,True
70926,Feature Request: Implement `torch.sparse.spdiags` and `torch.sparse.diags` module: sparse feature triaged,2022-01-06 14:55:24+00:00,,1,10,module: sparse feature triaged,False
70925,allow `dim=None` in `concat` triaged module: viewing and reshaping module: python array api,2022-01-06 14:51:17+00:00,,0,3,triaged module: viewing and reshaping module: python array api,True
70924,change supported arguments for parameter `dim` in `squeeze` module: bc-breaking triaged module: python array api topic: bc breaking,2022-01-06 14:50:45+00:00,,0,4,module: bc-breaking triaged module: python array api topic: bc breaking,True
70921,`sort` should only return the sorted input module: bc-breaking triaged module: sorting and selection module: python array api topic: bc breaking,2022-01-06 14:36:23+00:00,,0,1,module: bc-breaking triaged module: sorting and selection module: python array api topic: bc breaking,True
70920,`unique` should be split into four partial functions triaged module: python array api,2022-01-06 14:19:19+00:00,,0,1,triaged module: python array api,True
70919,`linspace` should support an `endpoint` parameter triaged module: tensor creation module: python array api,2022-01-06 14:05:24+00:00,,0,3,triaged module: tensor creation module: python array api,True
70916,uint8 scalar tensors cannot be used for integer indexing triaged module: python array api,2022-01-06 13:54:27+00:00,,0,4,triaged module: python array api,True
70915,`arange` should return empty array if bounds are inconsistent with step sign triaged module: tensor creation module: python array api,2022-01-06 13:45:38+00:00,,0,1,triaged module: tensor creation module: python array api,True
70914,support setting `step` in `arange` without setting `end` triaged module: tensor creation module: python array api,2022-01-06 13:43:43+00:00,,0,1,triaged module: tensor creation module: python array api,True
70910,`eye` should support other diagonals than the main one triaged module: tensor creation module: python array api,2022-01-06 11:12:45+00:00,,0,2,triaged module: tensor creation module: python array api,True
70906,`full` should take an integer size triaged module: tensor creation module: python array api,2022-01-06 10:49:23+00:00,,0,3,triaged module: tensor creation module: python array api,True
70901,Accuracy problem of `torch.batch_norm_gather_stats_with_counts` when `running_mean` is half tensor module: numerical-stability module: cuda triaged module: half,2022-01-06 08:53:29+00:00,,0,4,module: numerical-stability module: cuda triaged module: half,True
70892,Memory leak while training model generated by torch.fx.symbolic_trace() in data parallel mode triaged module: fx,2022-01-06 06:46:34+00:00,,0,0,triaged module: fx,True
70870,Add flag for functional.Jacobian to return output as well module: autograd triaged enhancement,2022-01-05 21:01:32+00:00,,0,1,module: autograd triaged enhancement,True
70865,Support hooks-based checkpointing API with DDP high priority triage review oncall: distributed module: ddp,2022-01-05 19:56:33+00:00,,1,1,high priority triage review oncall: distributed module: ddp,True
70755,DISABLED test_barrier_timeout_group (__main__.TestDistBackendWithSpawn) oncall: distributed skipped,2022-01-05 17:47:14+00:00,,0,1,oncall: distributed skipped,False
70754,DISABLED test_all_reduce_coalesced_group_min (__main__.TestDistBackendWithSpawn) oncall: distributed skipped,2022-01-05 17:45:57+00:00,,0,1,oncall: distributed skipped,False
70753,DISABLED test_gpu_simple (__main__.TensorPipeCudaDistAutogradTest) high priority triage review oncall: distributed module: flaky-tests module: rpc skipped,2022-01-05 17:45:50+00:00,,1,1,high priority triage review oncall: distributed module: flaky-tests module: rpc skipped,False
70735,named tensor doesn't work with deepcopy triaged module: named tensor,2022-01-05 16:53:27+00:00,,0,0,triaged module: named tensor,True
70702,conv3d padding=`same` gradgradcheck fails on CUDA needs reproduction module: autograd module: convolution triaged module: determinism,2022-01-05 15:20:17+00:00,,0,1,needs reproduction module: autograd module: convolution triaged module: determinism,True
70701,AdaptiveAvgPool1d - RuntimeError: shmem_size <= sharedMemPerBlockINTERNAL ASSERT FAILED module: cuda triaged,2022-01-05 15:14:11+00:00,,0,4,module: cuda triaged,True
70678,Decouple `TensorIteratorBase` output from structured kernel outputs. triaged module: structured kernels,2022-01-05 13:36:55+00:00,,0,1,triaged module: structured kernels,True
70669,CUSOLVER_STATUS_EXECUTION_FAILED when using the torch.logdet() module: cuda triaged module: linear algebra,2022-01-05 09:15:56+00:00,,0,16,module: cuda triaged module: linear algebra,True
70660,[FSDP] Run parity tests for activation checkpoint and offload oncall: distributed triaged better-engineering module: fsdp,2022-01-05 03:07:29+00:00,,1,0,oncall: distributed triaged better-engineering module: fsdp,True
70654,[RFC] UCC integration in ProcessGroupNCCL oncall: distributed feature triaged,2022-01-05 00:03:56+00:00,,0,13,oncall: distributed feature triaged,False
70627,Torchscript Compiled functions don't support keyword-only arguments with defaults oncall: jit,2022-01-04 19:38:49+00:00,,0,0,oncall: jit,False
70609,gloo_test test_close_connection not working as intended due to unwanted comma caffe2 triaged,2022-01-04 17:10:57+00:00,,0,0,caffe2 triaged,True
70608,Cannot compile C++ documentation: Sphynx assertion module: docs triaged,2022-01-04 16:58:18+00:00,,0,1,module: docs triaged,True
70583,Add nondeterministic alert to `torch.scatter_` module: cuda triaged module: determinism,2022-01-03 17:46:32+00:00,,1,0,module: cuda triaged module: determinism,True
70573,JIT magic method which returns class instance fails. oncall: jit,2022-01-03 09:48:23+00:00,,0,0,oncall: jit,True
70559,Behavior of torch.nn.functional.interpolate with unchanged output size and recompute_scale_factor=False module: nn triaged module: interpolation,2022-01-02 18:01:59+00:00,,0,2,module: nn triaged module: interpolation,True
70555,[feature request] Exponential moving average (EMA) of a tensor across a dimension feature triaged module: numpy,2022-01-02 09:06:31+00:00,,0,1,feature triaged module: numpy,True
70546,DISABLED test_tensorpipe_set_default_timeout (__main__.TensorPipeTensorPipeAgentRpcTest) high priority triage review oncall: distributed module: flaky-tests module: rpc skipped,2022-01-01 03:26:48+00:00,,1,11,high priority triage review oncall: distributed module: flaky-tests module: rpc skipped,False
70544,Tensor loses `bool` method during scripting oncall: jit,2021-12-31 19:00:18+00:00,,0,0,oncall: jit,False
70540,cumcount / cumulative count triaged enhancement,2021-12-31 07:44:14+00:00,,0,1,triaged enhancement,True
70533,JIT / TorchScript should support for sum(List[torch.Tensor]) like non-JIT PyTorch already does oncall: jit Stale,2021-12-30 22:16:24+00:00,,0,1,oncall: jit Stale,False
70529,BatchNorm on variable-length sequences or batches feature triaged,2021-12-30 16:21:47+00:00,,0,2,feature triaged,False
70521,tmpxft_00008487_00000000-6_THCStorage.compute_86.cudafe1.cpp:(.text+0x60b): additional relocation overflows omitted from the output when build PyTorch 1.8.2 from source module: build module: cuda triaged,2021-12-30 08:15:25+00:00,,0,4,module: build module: cuda triaged,True
70519,RuntimeError: cublas runtime error needs reproduction triaged module: cublas,2021-12-30 06:32:55+00:00,,0,1,needs reproduction triaged module: cublas,True
70513,Conda Repodata.json file not found in Pytorch channel module: binaries triaged,2021-12-30 03:37:06+00:00,,0,0,module: binaries triaged,True
70511,Strange behavior of torch.jit.trace when moving parameters across device oncall: jit Stale,2021-12-30 02:55:26+00:00,,0,1,oncall: jit Stale,False
70505,`UninitializedParameter.to(device='meta')` creates a zero-sized meta tensor instead of remaining uninitialized triaged module: meta tensors module: lazy,2021-12-29 21:45:54+00:00,,0,1,triaged module: meta tensors module: lazy,True
70504,backward checks len of inputs before it's converted to a tuple module: autograd triaged actionable,2021-12-29 21:17:55+00:00,,0,4,module: autograd triaged actionable,True
70502,torch.get_autocast_cpu_dtype() returns a new dtype (still?) high priority oncall: releng triaged,2021-12-29 18:14:24+00:00,,0,7,high priority oncall: releng triaged,True
70500,Adding the new Phish activation function module: nn triaged function request Stale,2021-12-29 17:49:27+00:00,,0,3,module: nn triaged function request Stale,False
70498,Multiple invalid summaries in torch.nn documentation page module: docs module: nn triaged,2021-12-29 16:58:34+00:00,,0,2,module: docs module: nn triaged,True
70490,Cuda sync mode input checking is wrong for non-string/int inputs. module: cuda triaged Stale,2021-12-29 13:54:49+00:00,,0,0,module: cuda triaged Stale,True
70487,Fusion of Convolution and BatchNorm module: cudnn feature triaged,2021-12-29 07:09:45+00:00,,0,0,module: cudnn feature triaged,True
70485,Cannot run FX tracer on a vision transformer model triaged Stale module: fx,2021-12-29 06:49:19+00:00,,0,0,triaged Stale module: fx,True
70480,Dynamic quantified Conv2d have accuracy issue oncall: quantization low priority triaged,2021-12-29 03:20:08+00:00,,1,6,oncall: quantization low priority triaged,True
70455,Make it possible to remove all hooks on a specified module without needing the hook handles module: nn triaged enhancement Stale,2021-12-28 17:15:31+00:00,,0,6,module: nn triaged enhancement Stale,True
70454,Building docs locally fails module: docs triaged,2021-12-28 16:53:38+00:00,,0,1,module: docs triaged,True
70450,torch.jit.optimized_execution is not mentioned anywhere in the docs oncall: jit Stale,2021-12-28 15:16:55+00:00,,0,4,oncall: jit Stale,False
70449,[FX] Detect attribute mutation during tracing triaged module: fx,2021-12-28 14:58:10+00:00,,0,1,triaged module: fx,True
70447,The same code can not be reproduced on multiple GPUs with dataparallel triaged module: data parallel Stale,2021-12-28 12:33:55+00:00,,0,1,triaged module: data parallel Stale,True
70446,Exporting the operator prim_DictConstruct to ONNX opset version 13 is not supported module: onnx triaged onnx-triaged,2021-12-28 12:21:31+00:00,,0,3,module: onnx triaged onnx-triaged,True
70419,LazyModules `cls_to_become` field exposes implementation detail module: docs module: nn triaged Stale,2021-12-27 07:53:53+00:00,,0,2,module: docs module: nn triaged Stale,True
70416,libtorch cuda use too much system memory module: cpp triaged Stale,2021-12-27 03:03:04+00:00,,0,0,module: cpp triaged Stale,True
70413,"PyTorch crashes without an error message, when running this code snippet with torch.tensor subclassing & forward hooks (Not sure what the exact cause is, but the code snippet reliably causes it) triaged Stale tensor subclass",2021-12-26 18:33:55+00:00,,0,0,triaged Stale tensor subclass,True
70398,`torch.broadcast_to` can create tensor with negative dimension. triaged module: correctness (silent) module: shape checking,2021-12-25 07:21:20+00:00,,0,1,triaged module: correctness (silent) module: shape checking,True
70397,`torch.empty_strided` works when the stride is negative! triaged module: correctness (silent) module: tensor creation,2021-12-25 04:58:50+00:00,,0,1,triaged module: correctness (silent) module: tensor creation,True
70394,Intel MKL FATAL ERROR: This system does not meet the minimum requirements for use of the Intel(R) Math Kernel Library. module: dependency bug module: binaries triaged module: macos module: mkl module: intel,2021-12-24 16:29:23+00:00,,0,6,module: dependency bug module: binaries triaged module: macos module: mkl module: intel,True
70392,third_party/breakpad/ compilation failure triaged module: third_party,2021-12-24 14:46:20+00:00,,0,1,triaged module: third_party,True
70391,linalg.lstsq INTERNAL ASSERT FAILED triaged module: mkl module: linear algebra Stale,2021-12-24 13:28:38+00:00,,0,0,triaged module: mkl module: linear algebra Stale,True
70388,RuntimeError: tensor has too many (>25) dims when permuting tensor with GPU backend triaged module: assert failure Stale,2021-12-24 11:44:03+00:00,,0,1,triaged module: assert failure Stale,True
70386,AT_ASSERT fail with DataLoaderOptions().drop_last() module: dataloader triaged Stale,2021-12-24 07:14:34+00:00,,0,1,module: dataloader triaged Stale,True
70352,Rprop Optimizer: UnboundLocalError: local variable 'step_size_min' referenced before assignment module: optimizer triaged actionable,2021-12-23 16:11:58+00:00,,0,3,module: optimizer triaged actionable,False
70350,Core dumped with large matmul on aarch64 module: crash triaged module: arm Stale,2021-12-23 14:06:12+00:00,,0,4,module: crash triaged module: arm Stale,True
70348,nan return by nn.CrossEntropyLoss when all the labels are ignore_index in torch 1.11 module: nn triaged Stale,2021-12-23 12:33:40+00:00,,0,4,module: nn triaged Stale,True
70347,boolean mask + ellipsis lead to incorrect indexing triaged module: numpy module: advanced indexing Stale,2021-12-23 11:59:04+00:00,,0,3,triaged module: numpy module: advanced indexing Stale,True
70346,Add hints for gradient long time overflow when using torch.cuda.amp module: cuda triaged module: amp (automated mixed precision) Stale,2021-12-23 07:04:43+00:00,,0,0,module: cuda triaged module: amp (automated mixed precision) Stale,True
70342,Empty or NaN data pollute gradient even if they are not involved during backward module: autograd triaged module: NaNs and Infs,2021-12-23 03:31:37+00:00,,0,2,module: autograd triaged module: NaNs and Infs,True
70267,Inconsistent multi-node latency with NCCL and OpenMPI oncall: distributed module: nccl module: openmp,2021-12-21 22:53:34+00:00,,0,2,oncall: distributed module: nccl module: openmp,True
70265,"Shape parameter inconsistency in torch.Tensor.view, torch.reshape, torch.Tensor.reshape module: docs triaged module: viewing and reshaping",2021-12-21 22:37:00+00:00,,0,1,module: docs triaged module: viewing and reshaping,True
70245,"SEGFAULT on ""import torch"" needs reproduction module: crash triaged",2021-12-21 13:27:18+00:00,,0,7,needs reproduction module: crash triaged,True
70243,Unable to compile PyTorch when libcudart_static.so is not available  module: build module: cuda triaged,2021-12-21 12:36:59+00:00,,0,10,module: build module: cuda triaged,True
70242,`TensorIterator`: provide a two-argument version of `set_output` triaged module: TensorIterator module: structured kernels,2021-12-21 12:27:21+00:00,,0,3,triaged module: TensorIterator module: structured kernels,True
70241,CPU parallelization across batch has random faulty behavior on backward triaged module: multithreading,2021-12-21 12:25:08+00:00,,0,2,triaged module: multithreading,True
70240,JIT: wrong list/tuple length if using Union oncall: jit Stale,2021-12-21 12:00:51+00:00,,0,0,oncall: jit Stale,False
70239,`pytorch` hangs during interaction with `ray` package module: cpu triaged module: deadlock,2021-12-21 11:10:16+00:00,,0,3,module: cpu triaged module: deadlock,True
70238,`TensorIterator`: refactor `build_ternary_op` to match binary versions triaged module: TensorIterator,2021-12-21 11:04:56+00:00,,0,8,triaged module: TensorIterator,True
70223,"jit: Confusing behavior w/ torch.autograd.grad, iterative loop, and printing? oncall: jit Stale",2021-12-21 04:47:14+00:00,,0,0,oncall: jit Stale,False
70222,Grad strides do not match bucket view strides triaged module: ddp,2021-12-21 03:12:41+00:00,,0,0,triaged module: ddp,True
70199,Missing Docker image for 1.10.1 module: binaries triaged module: docker Stale,2021-12-20 19:45:55+00:00,,0,1,module: binaries triaged module: docker Stale,True
70191,[libtorch] Loading in Java two differente libtorch_cpu.so from different versions fails module: binaries feature triaged oncall: java,2021-12-20 17:58:43+00:00,,0,5,module: binaries feature triaged oncall: java,True
70184,Optimization: convolution_backward doesn't always need to call .contiguous on certain inputs module: performance module: convolution triaged,2021-12-20 15:35:37+00:00,,0,1,module: performance module: convolution triaged,True
70180,Can you provide the torch.trt module to directly convert the pytorch weights to tensorrt? triaged enhancement module: unknown,2021-12-20 12:54:53+00:00,,0,0,triaged enhancement module: unknown,True
70173,Dynamic Tensor Rematerialization (DTR) feature module: memory usage triaged,2021-12-20 07:08:43+00:00,,0,1,feature module: memory usage triaged,True
70171,Channels last performance problem module: performance triaged module: memory format Stale,2021-12-20 05:05:43+00:00,,0,4,module: performance triaged module: memory format Stale,True
70170,EmbeddingBag allows out of index ranges.! module: dependency bug triaged,2021-12-20 03:36:20+00:00,,0,7,module: dependency bug triaged,True
70166,[feature request] Autocast module and function wrappers feature triaged module: amp (automated mixed precision),2021-12-19 21:45:14+00:00,,0,10,feature triaged module: amp (automated mixed precision),True
70162,Dot product return completely incorrect result when using pip but not when using conda high priority module: cuda triaged module: correctness (silent) module: lts,2021-12-19 10:43:51+00:00,,0,9,high priority module: cuda triaged module: correctness (silent) module: lts,True
70160,NewOperatorRegistrationTest.testImplNoDefGetsCaught failed. module: cpp module: tests triaged module: dispatch,2021-12-18 22:36:44+00:00,,0,4,module: cpp module: tests triaged module: dispatch,True
70159,[Android] Unknown builtin op: aten::reflection_pad3d oncall: mobile Stale,2021-12-18 22:18:32+00:00,,0,1,oncall: mobile Stale,False
70158,pin_memory *still* destroys custom containers  module: dataloader triaged Stale,2021-12-18 20:50:39+00:00,,0,3,module: dataloader triaged Stale,True
70155,RuntimeError: Tensor must be CUDA and dense when calling all_gather_object even though there is no tensor in the object. oncall: distributed module: c10d,2021-12-18 16:48:24+00:00,,0,0,oncall: distributed module: c10d,True
70138,build_ios.sh prevents iOS.cmake from configuring ios deployment target correctly module: build triaged module: ios Stale,2021-12-17 22:31:06+00:00,,0,0,module: build triaged module: ios Stale,True
70135,API to support combined activation offloading or checkpointing oncall: distributed module: autograd triaged module: ddp module: fsdp,2021-12-17 21:43:59+00:00,,1,16,oncall: distributed module: autograd triaged module: ddp module: fsdp,True
70125,channels_last/channels_last_3d memory format not supported for some modules on ROCm that should be supported on CUDA module: rocm triaged Stale,2021-12-17 19:39:56+00:00,,0,0,module: rocm triaged Stale,True
70108,Derivative for _ctc_loss_backward high priority module: double backwards module: autograd triaged actionable,2021-12-17 15:39:39+00:00,,0,8,high priority module: double backwards module: autograd triaged actionable,True
70102,torch jit script segm fault oncall: jit Stale,2021-12-17 11:47:32+00:00,,0,0,oncall: jit Stale,True
70100,Feature Request: dim parameter of torch.nn.functional.normalize should accept tuples feature module: nn triaged actionable module: norms and normalization Stale,2021-12-17 10:03:49+00:00,,0,2,feature module: nn triaged actionable module: norms and normalization Stale,False
70099,"Question:  what is ""Parameter indices""? oncall: distributed Stale",2021-12-17 09:34:29+00:00,,0,0,oncall: distributed Stale,False
70095,Add support for a `default` arg in `ModuleDict.pop` feature module: nn triaged Stale,2021-12-17 07:39:17+00:00,,0,4,feature module: nn triaged Stale,False
70073,Feature request: [STFT] Add warning message if signal length is not a multiple of hop_length in torch.stft feature triaged module: complex module: fft,2021-12-16 23:05:30+00:00,,0,8,feature triaged module: complex module: fft,True
70065,Tool for detecting inefficent striding for nn.Conv2d module: cudnn triaged enhancement oncall: profiler Stale,2021-12-16 21:31:53+00:00,,0,2,module: cudnn triaged enhancement oncall: profiler Stale,True
70060,[RFC] Activation Checkpoint API improvements oncall: distributed module: checkpoint module: ddp Stale module: fsdp,2021-12-16 21:06:48+00:00,,1,0,oncall: distributed module: checkpoint module: ddp Stale module: fsdp,True
70058,Unknown builtin op: spconv::get_indice_pairs  oncall: jit Stale,2021-12-16 20:56:33+00:00,,0,1,oncall: jit Stale,False
70054,[c10d] have a way to determine two ranks on the same hosts or not oncall: distributed module: c10d,2021-12-16 18:51:00+00:00,,0,1,oncall: distributed module: c10d,False
70051,[feature request] Support python decorators in TorchScript oncall: jit,2021-12-16 17:37:08+00:00,,0,1,oncall: jit,False
70048,Tensor transfer between gpus doesnt work needs reproduction module: cuda triaged Stale,2021-12-16 16:03:23+00:00,,0,6,needs reproduction module: cuda triaged Stale,True
70047,[JIT] Cannot `jit.export` a `@staticmethod` oncall: jit Stale,2021-12-16 14:13:29+00:00,,0,2,oncall: jit Stale,True
70045,Could we leave the _two_ most recent nightlies in the conda channel? module: binaries feature module: ci triaged,2021-12-16 13:27:56+00:00,,0,4,module: binaries feature module: ci triaged,True
70041,Multiprocessing - shared memory module: multiprocessing triaged,2021-12-16 12:21:00+00:00,,0,2,module: multiprocessing triaged,True
70040,Error in `torch.cdist` documentation module: docs triaged module: distance functions,2021-12-16 12:08:03+00:00,,0,2,module: docs triaged module: distance functions,True
70038,comile error needs reproduction module: build triaged module: mkldnn module: third_party,2021-12-16 09:56:15+00:00,,0,1,needs reproduction module: build triaged module: mkldnn module: third_party,True
70037,Question about collect tensor in distributed dataparallel? oncall: distributed Stale,2021-12-16 09:40:42+00:00,,0,0,oncall: distributed Stale,False
70015,Multigpu test configs intermittently timeout high priority triage review module: cuda module: ci module: flaky-tests,2021-12-16 00:20:23+00:00,,0,5,high priority triage review module: cuda module: ci module: flaky-tests,True
70008,Torch function runtime seemingly dependent on scipy call needs reproduction module: performance triaged module: linear algebra,2021-12-15 22:41:53+00:00,,0,1,needs reproduction module: performance triaged module: linear algebra,True
69991,Composite Compliance Problems Tracker module: internals triaged module: linear algebra module: __torch_dispatch__,2021-12-15 20:32:28+00:00,,0,7,module: internals triaged module: linear algebra module: __torch_dispatch__,True
69984,Docs for torch.nn.MSELoss are confusing module: docs module: nn module: loss triaged Stale,2021-12-15 18:04:19+00:00,,0,1,module: docs module: nn module: loss triaged Stale,True
69972,torchscript does not work with `SyncBatchNorm` layers oncall: jit Stale,2021-12-15 13:16:02+00:00,,0,0,oncall: jit Stale,False
69969,cpu - gpu calculation results differs by far with torch.nn.functional.linear module: numerical-stability module: cuda triaged,2021-12-15 08:55:53+00:00,,0,3,module: numerical-stability module: cuda triaged,True
69966,torch.nn.DataParallel caused inference failure with cpu set as device on NV machine needs reproduction triaged module: data parallel Stale,2021-12-15 06:49:31+00:00,,0,0,needs reproduction triaged module: data parallel Stale,True
69960,"FeatureAlphaDropout doesn't drop channels for (C, D, H, W) module: nn triaged module: batching Stale",2021-12-15 04:30:04+00:00,,0,2,module: nn triaged module: batching Stale,True
69938,"JIT is overriding variables of type List[int] to type Tuple[int, int] for an unknown reason during function calls oncall: jit",2021-12-14 22:34:53+00:00,,0,0,oncall: jit,True
69932,"Would pytorch like some free multi-factor authentication (MFA) tokens from Google & GitHub, via the OpenSSF? triaged",2021-12-14 22:09:21+00:00,,0,1,triaged,True
69931,[JIT] torch.exp roughly 20 times slower in TorchScript vs. PyTorch oncall: jit Stale,2021-12-14 21:56:39+00:00,,0,1,oncall: jit Stale,False
69921,Context manager to enable/disabled TensorFloat32 on demand triaged module: tf32,2021-12-14 19:10:07+00:00,,0,14,triaged module: tf32,True
69912,"torch.nn.functional.ctc_loss with invalid input produce NaN or infinity gradient, while the batch entries are fine module: nn module: loss module: error checking triaged Stale",2021-12-14 17:50:54+00:00,,0,4,module: nn module: loss module: error checking triaged Stale,True
69900,Port MarginRankingLoss to TensorIterator module: performance module: nn module: loss triaged enhancement,2021-12-14 14:33:59+00:00,,0,1,module: performance module: nn module: loss triaged enhancement,True
69893,subclassing torch.Tensor triaged enhancement tensor subclass,2021-12-14 10:15:37+00:00,,0,3,triaged enhancement tensor subclass,True
69892,[feature request] Support `like=` argument in tensor factory methods feature triaged module: numpy module: tensor creation,2021-12-14 09:44:38+00:00,,0,7,feature triaged module: numpy module: tensor creation,True
69891,[FSDP] Enable tests for Gloo backend oncall: distributed better-engineering Stale module: fsdp,2021-12-14 09:18:04+00:00,,0,0,oncall: distributed better-engineering Stale module: fsdp,False
69889,Exporting the operator unfold to ONNX is not supported. module: onnx triaged onnx-triaged,2021-12-14 08:26:20+00:00,,0,11,module: onnx triaged onnx-triaged,True
69858,Feature Request: Bayesian Personalized Ranking Loss feature module: nn module: loss triaged actionable,2021-12-13 18:59:48+00:00,,0,4,feature module: nn module: loss triaged actionable,False
69841,`CosineAnnealingWarmRestarts` should use integer epoch module: optimizer triaged module: LrScheduler,2021-12-13 15:13:09+00:00,,0,2,module: optimizer triaged module: LrScheduler,True
69831,Incorrect error for integer `out=` dtypes when a float is expected. triaged module: type promotion module: structured kernels,2021-12-13 12:26:06+00:00,,0,1,triaged module: type promotion module: structured kernels,True
69830,Conversion error from pytorch model to libtorch model oncall: jit Stale,2021-12-13 12:25:10+00:00,,0,4,oncall: jit Stale,True
69822,Implement torch.*_like tensor creation functions on sparse inputs module: sparse feature triaged module: tensor creation,2021-12-13 08:41:28+00:00,,0,13,module: sparse feature triaged module: tensor creation,True
69814,JIT / TorchScript docs missing any mention of typing.cast oncall: jit Stale,2021-12-12 16:36:22+00:00,,0,0,oncall: jit Stale,False
69804,typing error in the signatures of `torch.unbind` when getting them using `torch.fx`. triaged module: fx,2021-12-11 15:00:06+00:00,,0,0,triaged module: fx,True
69803,torchrl feature triaged,2021-12-11 12:12:15+00:00,,0,2,feature triaged,False
69801,"Dropout2d doesn't drop channels for (C, H, W) high priority module: nn triaged module: correctness (silent)",2021-12-11 01:59:55+00:00,,1,5,high priority module: nn triaged module: correctness (silent),True
69800,Incomplete error message at tensor indexing (when indexing with set) module: error checking triaged Stale,2021-12-11 01:01:43+00:00,,0,0,module: error checking triaged Stale,True
69794,torch.optim.lr_scheduler.SequentialLR.get_last_lr() does not work triaged module: LrScheduler,2021-12-10 22:32:53+00:00,,0,4,triaged module: LrScheduler,True
69786,Implement aten::equal for sparse tensors module: sparse triaged Stale,2021-12-10 19:51:59+00:00,,0,6,module: sparse triaged Stale,True
69782,Torch Profiler does not count FLOPs for backward pass  oncall: profiler,2021-12-10 19:23:53+00:00,,0,6,oncall: profiler,False
69772,RuntimeError: t == DeviceType::CUDAINTERNAL ASSERT FAILED when trying to calculate gradients module: cuda triaged,2021-12-10 18:05:04+00:00,,0,3,module: cuda triaged,True
69757,[Feature request] nn.Model API: Standard model interface module: nn triaged needs research,2021-12-10 13:32:23+00:00,,0,1,module: nn triaged needs research,True
69756,Introduction of nn.LazyRNNs module: nn module: rnn triaged enhancement Stale,2021-12-10 13:25:18+00:00,,0,0,module: nn module: rnn triaged enhancement Stale,True
69753,last_epoch parameter of CyclicLR and OneCycleLR is not the number of epochs module: optimizer triaged Stale module: LrScheduler,2021-12-10 11:05:54+00:00,,0,0,module: optimizer triaged Stale module: LrScheduler,True
69744,Add ability to ignore arguments/outputs in `torch.autograd.functional.jacobian` module: autograd triaged enhancement Stale,2021-12-10 02:18:05+00:00,,0,0,module: autograd triaged enhancement Stale,True
69741,JIT: Support for `torch.autograd.functional.jacobian` in TorchScript oncall: jit Stale,2021-12-10 02:02:42+00:00,,0,14,oncall: jit Stale,False
69721,Alternative to keras StringLookup in Pytorch feature module: nn triaged,2021-12-09 22:01:15+00:00,,0,3,feature module: nn triaged,False
69714,Possibly out of date error in autograd codegen module: autograd triaged Stale,2021-12-09 20:24:31+00:00,,0,2,module: autograd triaged Stale,True
69689,[bug] the LTS torch==1.8.2 pip package is incomplete module: cuda triaged Stale module: lts,2021-12-09 17:35:33+00:00,,0,5,module: cuda triaged Stale module: lts,True
69688,`torch.cuda.set_per_process_memory_fraction()` does not perform VRAM isolation module: docs module: cuda module: memory usage triaged,2021-12-09 16:54:34+00:00,,0,1,module: docs module: cuda module: memory usage triaged,True
69687,Performance improvement in Autograd Forward AD using ZeroTensors module: performance module: autograd triaged actionable module: forward ad,2021-12-09 16:37:06+00:00,,0,0,module: performance module: autograd triaged actionable module: forward ad,True
69685,Wrong PyTorch version in Docker image triaged module: docker Stale,2021-12-09 16:08:26+00:00,,0,1,triaged module: docker Stale,True
69676,"[LazyTensor] model after to(device), traced IR changed and lose type info triaged lazy Stale",2021-12-09 11:04:21+00:00,,0,0,triaged lazy Stale,True
69665,No dtype check for zero sparse tensor! module: sparse triaged Stale,2021-12-09 06:19:28+00:00,,0,4,module: sparse triaged Stale,True
69663,"After updated to pytorch1.10.0 cuda11.1, NCCL is not available oncall: distributed triaged Stale",2021-12-09 03:53:39+00:00,,0,1,oncall: distributed triaged Stale,True
69659,DISABLED test_fs_pool (__main__.TestMultiprocessing) high priority triage review module: multiprocessing module: flaky-tests skipped,2021-12-09 03:13:12+00:00,,0,17,high priority triage review module: multiprocessing module: flaky-tests skipped,False
69654,Fused AlphaFold 2 modules feature triaged,2021-12-09 01:04:14+00:00,,0,6,feature triaged,True
69627,c2r fft input generation module: tests triaged module: fft,2021-12-08 19:34:13+00:00,,0,3,module: tests triaged module: fft,True
69616,Multi-GPU training stuck when using grad scaler oncall: distributed module: cuda module: amp (automated mixed precision) Stale,2021-12-08 17:59:32+00:00,,0,4,oncall: distributed module: cuda module: amp (automated mixed precision) Stale,True
69611,_pickle.UnpicklingError: pickle data was truncated - Windows multiprocessing during training module: windows module: multiprocessing module: dataloader module: serialization triaged,2021-12-08 15:15:15+00:00,,0,5,module: windows module: multiprocessing module: dataloader module: serialization triaged,True
69610,[Question] How to extract/expose the complete PyTorch computation graph (forward and backward)? module: autograd triaged oncall: visualization,2021-12-08 14:37:00+00:00,,0,12,module: autograd triaged oncall: visualization,True
69609,[mobile_optimizer] RuntimeError: We don't have an op for metal_prepack::conv2d_prepack oncall: mobile Stale,2021-12-08 13:36:18+00:00,,0,5,oncall: mobile Stale,False
69608,Optimize For Inference - add warning/error upon serialization oncall: jit,2021-12-08 13:02:54+00:00,,0,1,oncall: jit,False
69594,RuntimeError: Operation not supported   oncall: distributed module: c10d Stale,2021-12-08 04:02:14+00:00,,0,2,oncall: distributed module: c10d Stale,False
69591,Make DDP + Zero have the same communication volume as regular DDP oncall: distributed module: ddp Stale,2021-12-08 02:26:46+00:00,,0,3,oncall: distributed module: ddp Stale,True
69577,[subgraph_rewriter] Add match_filter for the matches triaged Stale module: fx,2021-12-07 23:54:05+00:00,,1,1,triaged Stale module: fx,True
69563,[Distributed Tests] Print stacktraces of all processes when one fails oncall: distributed better-engineering Stale,2021-12-07 21:48:00+00:00,,0,0,oncall: distributed better-engineering Stale,True
69538,cuSOLVER backend for Sparse CSR direct linear solvers module: sparse module: cuda triaged module: linear algebra,2021-12-07 17:01:58+00:00,,0,8,module: sparse module: cuda triaged module: linear algebra,True
69532,[feature request] A rank-revealing SVD for better stability in backward. module: numerical-stability feature module: autograd triaged module: linear algebra,2021-12-07 15:55:22+00:00,,1,0,module: numerical-stability feature module: autograd triaged module: linear algebra,True
69531,"`(svd|pca)_lowrank`: backward is unstable when for a matrix `A`, the parameter `q` is set to a value `q > rank(A)`. module: numerical-stability module: autograd triaged module: linear algebra Stale",2021-12-07 15:33:29+00:00,,0,6,module: numerical-stability module: autograd triaged module: linear algebra Stale,True
69530,Extend composite compliant testing to backward formulas triaged Stale module: __torch_dispatch__,2021-12-07 14:38:55+00:00,,0,0,triaged Stale module: __torch_dispatch__,True
69524,[feature request] Accepting python scalar inputs for torch.minimum/torch.maximum and friends triaged module: distance functions,2021-12-07 11:44:32+00:00,,0,2,triaged module: distance functions,True
69522,PyTorch 1.9.1 incorrect result for all-reduce oncall: distributed,2021-12-07 10:26:03+00:00,,0,1,oncall: distributed,True
69520,race condition of agent's TCP store on exit_barrier() triaged Stale oncall: r2p,2021-12-07 09:39:19+00:00,,0,1,triaged Stale oncall: r2p,True
69519,Feature Request: CUDA torch.histogram (and histogramdd) module: bootcamp feature module: cuda triaged actionable module: sorting and selection,2021-12-07 09:06:54+00:00,,0,11,module: bootcamp feature module: cuda triaged actionable module: sorting and selection,True
69516,`torch.sparse.softmax` and `torch.sparse.log_softmax` do not support negative dim. module: sparse triaged Stale,2021-12-07 07:49:01+00:00,,0,2,module: sparse triaged Stale,True
69512,Inconsistent behavior of cosine_similarity between fp16 and fp32 inputs high priority module: numerical-stability triaged module: half module: distance functions,2021-12-07 05:47:48+00:00,,0,3,high priority module: numerical-stability triaged module: half module: distance functions,True
69506,[performance] a profiler util to show a rough break-down of the types of ops used by a model oncall: profiler Stale,2021-12-07 02:20:15+00:00,,0,1,oncall: profiler Stale,False
69505,"with the same environment ,pytorch 1.8 worked but 1.10 can't work module: binaries module: cuda triaged Stale module: wsl",2021-12-07 01:35:16+00:00,,0,2,module: binaries module: cuda triaged Stale module: wsl,True
69491,[RFC] Support MemoryView for Tensors module: internals feature module: cuda triaged module: numpy module: viewing and reshaping,2021-12-06 22:32:20+00:00,,0,6,module: internals feature module: cuda triaged module: numpy module: viewing and reshaping,True
69471,Include Declarations.yaml in Libtorch distributions module: cpp triaged module: codegen Stale,2021-12-06 19:19:15+00:00,,0,12,module: cpp triaged module: codegen Stale,True
69469,Distribution `covariance` property module: distributions triaged,2021-12-06 19:00:49+00:00,,0,0,module: distributions triaged,True
69467,Distribution `cross_entropy` method module: distributions triaged,2021-12-06 18:58:40+00:00,,0,1,module: distributions triaged,True
69453,Jacobian elliptic functions feature triaged module: special,2021-12-06 15:04:42+00:00,,0,1,feature triaged module: special,True
69452,Complete elliptic integral of the first kind feature triaged module: special,2021-12-06 15:01:42+00:00,,0,1,feature triaged module: special,True
69451,Exponentially scalable modified Bessel function of the second kind feature triaged module: special,2021-12-06 14:57:06+00:00,,0,1,feature triaged module: special,True
69448,RuntimeError: PyTorch convert function for op 'inverse' not implemented. needs reproduction triaged,2021-12-06 13:21:23+00:00,,0,1,needs reproduction triaged,False
69442,The formula for KL-divloss is wrong in the document module: docs module: nn triaged Stale,2021-12-06 07:58:24+00:00,,0,1,module: docs module: nn triaged Stale,True
69435,"torch.is_tensor(obj) doesn't work with JIT, despite the fact that isinstance(obj, Tensor) already works with JIT oncall: jit Stale",2021-12-05 19:52:10+00:00,,0,0,oncall: jit Stale,True
69434,"JIT RuntimeError:  'Union[Tensor, List[float], List[int]]' object is not subscriptable oncall: jit",2021-12-05 19:27:07+00:00,,1,4,oncall: jit,True
69433,`torch.transpose` should raise an error when indexing 0 for 0 dimensional tensor. triaged module: viewing and reshaping,2021-12-05 14:34:42+00:00,,0,4,triaged module: viewing and reshaping,True
69431,"[proposal] [util] torch.to(obj, device) supporting recursive lists/dicts/tuples of tensors probably by uplifting/promoting torch.distributed.utils._recursive_to triaged needs research function request",2021-12-05 13:46:06+00:00,,0,16,triaged needs research function request,False
69425,Integrate Libtorch into Unreal Engine 4: _ivalue_INTERNAL ASSERT FAILED oncall: jit Stale,2021-12-05 04:48:49+00:00,,0,5,oncall: jit Stale,True
69408,`torch.hstack` should raise an error when tensor is 0 dimensional module: docs triaged Stale,2021-12-04 02:04:13+00:00,,0,1,module: docs triaged Stale,True
69386,Port `normal` to structured kernel triaged module: structured kernels,2021-12-03 20:46:50+00:00,,0,15,triaged module: structured kernels,True
69364,"[feature request] quantized and low-level int8 operators (matmul, gemm etc) on CUDA + integrate LLM.int8 + integrate ZeroQuant? oncall: quantization module: cuda triaged",2021-12-03 13:09:26+00:00,,2,51,oncall: quantization module: cuda triaged,True
69363,"Citation request for ""probabilities for each class"" in the doc's description on cross-entropy module: docs module: nn triaged",2021-12-03 11:34:50+00:00,,0,2,module: docs module: nn triaged,False
69360,1.10.0 failed to build due to missing TensorBody.h module: build triaged Stale,2021-12-03 10:22:44+00:00,,0,3,module: build triaged Stale,True
69359,"[feature request] More masked reductions: amin/amax, argmin/argmax, quantile, mean/var/std/std_mean/var_mean module: sparse triaged module: masked operators",2021-12-03 10:19:09+00:00,,0,6,module: sparse triaged module: masked operators,True
69354,Missing instruction in recipe for Andoid/mobile interpreter oncall: mobile Stale,2021-12-03 08:10:27+00:00,,0,0,oncall: mobile Stale,False
69353,Pruning `torch.nn.MultiheadAttention` causes RuntimeError triaged oncall: transformer/mha module: pruning Stale,2021-12-03 08:04:23+00:00,,0,1,triaged oncall: transformer/mha module: pruning Stale,True
69352,I want to know how to read the LMDB file once when using DDP oncall: distributed module: dataloader,2021-12-03 07:34:16+00:00,,0,4,oncall: distributed module: dataloader,True
69348,`torch.sspaddmm` should broadcast the input tensor module: sparse triaged Stale,2021-12-03 06:55:32+00:00,,0,4,module: sparse triaged Stale,True
69347,Memory leak when moving tensors between Metal and CPU on iOS oncall: mobile,2021-12-03 05:04:04+00:00,,0,7,oncall: mobile,False
69325,`nn.functional.fractional_max_pool2d` and `nn.functional.fractional_max_pool3d` produce incorrect output on non-contiguous inputs module: nn triaged Stale,2021-12-02 22:31:31+00:00,,0,1,module: nn triaged Stale,True
69321,Poor Transformer inference scalability on CPU oncall: transformer/mha Stale,2021-12-02 21:53:44+00:00,,0,3,oncall: transformer/mha Stale,False
69320,[JIT] Break up JIT Opinfo Tests  oncall: jit better-engineering,2021-12-02 21:45:42+00:00,,0,0,oncall: jit better-engineering,True
69316,`torch.tensor` relies on implicit conversion being deprecated in Python 3.10 high priority triaged module: numpy module: tensor creation,2021-12-02 21:11:08+00:00,,1,1,high priority triaged module: numpy module: tensor creation,True
69308,Enable DDP checkpointing tests for Gloo backend oncall: distributed better-engineering module: ddp Stale,2021-12-02 20:32:30+00:00,,1,0,oncall: distributed better-engineering module: ddp Stale,True
69290,Deprecation warnings generated when including header files in C++17 code triaged module: build warnings Stale,2021-12-02 11:59:36+00:00,,0,0,triaged module: build warnings Stale,True
69288,Inplace and `out` variants for `positive` operator in PyTorch triaged module: ux Stale,2021-12-02 09:36:51+00:00,,0,1,triaged module: ux Stale,True
69287,`out=` variant for `conj` unary operator triaged module: complex Stale,2021-12-02 09:35:09+00:00,,0,3,triaged module: complex Stale,True
69286,"subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '32']' returned non-zero exit status 1. module: build triaged Stale",2021-12-02 09:22:09+00:00,,0,2,module: build triaged Stale,True
69281,Implement reparameterized sampling for LKJCholesky distribution w.r.t. concentration parameter module: distributions triaged Stale,2021-12-02 07:16:38+00:00,,1,1,module: distributions triaged Stale,True
69250,TorchScript incomptaible with non-persistent buffers oncall: jit Stale,2021-12-01 20:45:50+00:00,,0,0,oncall: jit Stale,False
69233,[ROCM] elementwise kernel launch module: rocm triaged Stale,2021-12-01 17:29:32+00:00,,0,0,module: rocm triaged Stale,True
69213,torch.stack should have a pin_memory parameter triaged module: memory format,2021-12-01 14:08:15+00:00,,0,1,triaged module: memory format,True
69207,Function(s) expecting a tuple argument don't accept generators module: nn triaged Stale,2021-12-01 11:22:01+00:00,,0,1,module: nn triaged Stale,True
69203,Error in SVD cusolver on Linux needs reproduction triaged module: linear algebra,2021-12-01 09:04:14+00:00,,0,19,needs reproduction triaged module: linear algebra,True
69202,CPU only mode with cudatoolkit 11.3 module: binaries triaged,2021-12-01 08:58:21+00:00,,0,2,module: binaries triaged,True
69199,Protocol buffers library mismatch. module: protobuf triaged module: tensorboard,2021-12-01 07:57:08+00:00,,0,2,module: protobuf triaged module: tensorboard,True
69197,RendezvousConnectionError when use C10d on multi nodes triaged oncall: r2p,2021-12-01 07:18:27+00:00,,0,3,triaged oncall: r2p,True
69179,DISABLED test_builtin_remote_message_dropped_timeout_to_self (__main__.FaultyFaultyAgentRpcTest) high priority triage review oncall: distributed module: flaky-tests module: rpc skipped,2021-12-01 00:06:15+00:00,,1,1,high priority triage review oncall: distributed module: flaky-tests module: rpc skipped,False
69178,DISABLED test_remote_timeout_to_here_in_jit (__main__.FaultyJitFaultyAgentRpcTest) high priority triage review oncall: distributed oncall: jit module: flaky-tests module: rpc skipped,2021-12-01 00:05:55+00:00,,1,1,high priority triage review oncall: distributed oncall: jit module: flaky-tests module: rpc skipped,True
69158,Fusing module with multiple hooks causes `RuntimeError: OrderedDict mutated during iteration` in fuse_known_modules oncall: quantization low priority triaged Stale,2021-11-30 21:08:10+00:00,,1,2,oncall: quantization low priority triaged Stale,True
69095,[FX] `concrete_args` with unpacking breaks `fx.Interpreter` triaged Stale module: fx,2021-11-30 18:23:09+00:00,,0,0,triaged Stale module: fx,True
69086,Confusing value of `TORCH_CUDA_ARCH_LIST` module: build triaged Stale,2021-11-30 17:01:06+00:00,,0,0,module: build triaged Stale,True
69084,Unicode Normalization with Torchscript oncall: jit Stale,2021-11-30 16:38:26+00:00,,0,0,oncall: jit Stale,False
69080,Generalized matmul feature triaged module: linear algebra,2021-11-30 15:45:45+00:00,,0,5,feature triaged module: linear algebra,True
69078,RuntimeError: Global alloc not supported yet in TorchScript oncall: jit,2021-11-30 15:14:05+00:00,,0,7,oncall: jit,False
69072,Libtorch dll versioning module: binaries triaged enhancement Stale,2021-11-30 11:10:44+00:00,,0,2,module: binaries triaged enhancement Stale,True
69063,make: Makefile: No such file or directory module: build triaged Stale,2021-11-30 03:05:03+00:00,,0,3,module: build triaged Stale,True
69033,Lazy Tensor Core Documentation Out-of-Date module: docs triaged module: xla module: lazy,2021-11-29 22:04:32+00:00,,0,24,module: docs triaged module: xla module: lazy,True
69031,DDP only syncs parameters used in most recent pass when `find_unused_parameters` is True. oncall: distributed better-engineering pt_distributed_rampup module: ddp Stale,2021-11-29 21:56:47+00:00,,1,5,oncall: distributed better-engineering pt_distributed_rampup module: ddp Stale,True
69025,[subgraph_rewriter] Support skipping matching for some ops triaged Stale module: fx,2021-11-29 20:53:56+00:00,,0,0,triaged Stale module: fx,False
69023,`TestProfilerCUDA. test_mem_leak` failing for CUDA 11.5 on Linux triage review module: performance module: cuda module: ci module: memory usage module: infra oncall: profiler,2021-11-29 20:27:56+00:00,,1,4,triage review module: performance module: cuda module: ci module: memory usage module: infra oncall: profiler,True
69013,libtorch_cuda links against wrong libnccl.so module: build module: abi triaged module: third_party Stale,2021-11-29 18:35:40+00:00,,0,0,module: build module: abi triaged module: third_party Stale,True
69009,Mamba does not respect `cpuonly` when creating a conda environment from YAML file oncall: releng triaged Stale,2021-11-29 17:54:10+00:00,,0,0,oncall: releng triaged Stale,True
69006,[JIT] allow `Optional[torch.Generator]` be tracable in Python oncall: jit,2021-11-29 17:31:45+00:00,,0,4,oncall: jit,False
68987,Operator is not supported in mobile module. oncall: mobile,2021-11-29 14:19:37+00:00,,0,2,oncall: mobile,False
68984,speed_benchmark_torch crashes when trying to run a model using Vulkan backend on Android oncall: mobile Stale,2021-11-29 12:21:55+00:00,,0,0,oncall: mobile Stale,False
68982,PyTorch installation doesn't work with Python 3.10.0  oncall: releng triaged,2021-11-29 12:01:21+00:00,,0,6,oncall: releng triaged,True
68979,Support for arbitrary schedulers in SequentialLR feature module: optimizer triaged module: LrScheduler,2021-11-29 09:10:11+00:00,,0,0,feature module: optimizer triaged module: LrScheduler,True
68978,SequentialLR cannot be used with ReduceLROnPlateau due to .step() not allowing for optional arguments module: optimizer triaged module: LrScheduler,2021-11-29 09:02:48+00:00,,0,7,module: optimizer triaged module: LrScheduler,True
68968,torch.jit.trace with pack_padded_sequence  cannot do dynamic batch oncall: jit Stale,2021-11-29 02:07:56+00:00,,0,0,oncall: jit Stale,False
68967,[feature request] torch.argminmax feature triaged,2021-11-28 22:33:07+00:00,,0,0,feature triaged,False
68966,`torch.ldexp` generated tests fail on call to `torch.mul` module: autograd triaged,2021-11-28 22:10:39+00:00,,0,1,module: autograd triaged,True
68961,Deterministic implementation for upsample_bilinear2d_backward_cuda feature module: cuda triaged module: determinism,2021-11-27 16:39:02+00:00,,0,3,feature module: cuda triaged module: determinism,True
68959,Deterministic implementation for grid_sampler_2d_backward_cuda feature module: cuda triaged module: determinism,2021-11-27 15:53:37+00:00,,0,1,feature module: cuda triaged module: determinism,True
68946,Strange ouput of torch.histc high priority triaged module: correctness (silent) module: sorting and selection,2021-11-26 19:11:27+00:00,,0,5,high priority triaged module: correctness (silent) module: sorting and selection,True
68944,Pytorch to ONNX model conversion module: onnx triaged,2021-11-26 13:26:15+00:00,,1,3,module: onnx triaged,False
68940,Breakpoint training in a Tensorboard log oncall: visualization Stale,2021-11-26 11:07:51+00:00,,0,0,oncall: visualization Stale,False
68935,repeat_interleave hangs in a forked subprocess module: multiprocessing triaged,2021-11-26 08:54:37+00:00,,0,0,module: multiprocessing triaged,True
68931,Dead lock happened ran pytorch 1.9.0 cuda11.2 on xeon gold 6326 cpu oncall: distributed triaged module: data parallel,2021-11-26 07:33:50+00:00,,0,1,oncall: distributed triaged module: data parallel,True
68927,Thank you for the diagnostic script. oncall: visualization,2021-11-26 04:16:14+00:00,,0,1,oncall: visualization,False
68925,How to implement `bucket_by_sequence_length` with IterableDataset and DataLoader module: dataloader triaged module: data,2021-11-26 03:06:17+00:00,,0,1,module: dataloader triaged module: data,True
68923,[cpp extension] making it possible to parallelize the building process module: build module: cpp-extensions module: ci triaged,2021-11-26 02:14:02+00:00,,0,0,module: build module: cpp-extensions module: ci triaged,True
68921,[Bug] forward function error occurs when the scriptmodule (made in Python) including the hook function is loaded from C++(Libtorch1.8.2) oncall: jit Stale,2021-11-26 01:52:51+00:00,,0,1,oncall: jit Stale,True
68920,Breakes with -OO flag in script needs reproduction triaged module: regression,2021-11-25 19:57:00+00:00,,0,4,needs reproduction triaged module: regression,True
68917,"Bugs related to NCCL on RTX 6000, code freezes with no output when using DistributedDataParallel  oncall: distributed module: ddp",2021-11-25 16:03:15+00:00,,0,0,oncall: distributed module: ddp,True
68916,Crash loading fairseq based model triaged module: hub,2021-11-25 14:27:32+00:00,,0,1,triaged module: hub,True
68909,Bug about Dropout CUDA Kernel module: nn module: cuda triaged,2021-11-25 07:50:43+00:00,,0,4,module: nn module: cuda triaged,True
68905,Shared `~/.cache/torch_extensions` needs to be pytorch version aware. module: cpp-extensions triaged enhancement,2021-11-25 02:18:38+00:00,,0,0,module: cpp-extensions triaged enhancement,False
68901,Python version is 3.7.11 in latest pytorch docker image oncall: releng triaged module: docker,2021-11-24 22:52:30+00:00,,0,3,oncall: releng triaged module: docker,True
68893,NCCL Network is unreachable / Connection refused when initializing DDP oncall: distributed module: nccl,2021-11-24 19:17:30+00:00,,0,3,oncall: distributed module: nccl,False
68892,fusion in fx graph mode did not take care of direct attribute access oncall: quantization low priority triaged,2021-11-24 18:46:56+00:00,,1,0,oncall: quantization low priority triaged,True
68880,Setting 'padding' to a string will fail exporting Conv2d operator to ONNX. module: onnx triaged onnx-triaged,2021-11-24 16:03:07+00:00,,0,9,module: onnx triaged onnx-triaged,True
68879,Stop gradient option for padding module: autograd module: nn triaged enhancement needs research Stale,2021-11-24 15:32:40+00:00,,0,2,module: autograd module: nn triaged enhancement needs research Stale,True
68871,[LibTorch-Lite] Add a custom flag to build LibTorch-Lite with LAPACK included enhancement oncall: mobile module: ios,2021-11-24 09:45:55+00:00,,0,5,enhancement oncall: mobile module: ios,False
68866,wait() does not block the default stream for NCCL's asynchronous P2P operations oncall: distributed module: nccl,2021-11-24 08:26:05+00:00,,0,3,oncall: distributed module: nccl,False
68864,Incorrect accuracy  pytorch(1.0.1) and libtorch   because of nn.LSTM needs reproduction triaged,2021-11-24 07:32:45+00:00,,0,5,needs reproduction triaged,True
68862,find_unused_parameters of DDP oncall: distributed module: ddp,2021-11-24 06:52:29+00:00,,0,0,oncall: distributed module: ddp,False
68861,[In Gunicorn & multiprocessing environment] Cannot re-initialize CUDA in forked subprocess module: multiprocessing triaged Stale,2021-11-24 06:19:27+00:00,,0,4,module: multiprocessing triaged Stale,True
68809,torch.profiler.profile().export_stacks() never finishes. oncall: profiler Stale,2021-11-23 10:40:25+00:00,,0,5,oncall: profiler Stale,False
68803,[DDP] Verify ignored parameter names in debug mode during init oncall: distributed triaged better-engineering module: c10d Stale,2021-11-23 09:03:50+00:00,,0,0,oncall: distributed triaged better-engineering module: c10d Stale,True
68800,[1.10 regression][jit][cuda fuse] cat codegen wrong var name oncall: jit Stale,2021-11-23 06:27:20+00:00,,0,2,oncall: jit Stale,True
68798,Clarify variables of BatchNorm*d functions module: docs module: nn triaged needs research module: norms and normalization Stale,2021-11-23 04:12:45+00:00,,0,1,module: docs module: nn triaged needs research module: norms and normalization Stale,True
68789,[FSDP] Wrap API improvements triaged module: fsdp,2021-11-23 01:33:41+00:00,,1,0,triaged module: fsdp,True
68785,[elastic/distributed] API to retrieve consolidated worker information oncall: distributed better-engineering pt_distributed_rampup oncall: r2p,2021-11-23 00:36:13+00:00,,0,2,oncall: distributed better-engineering pt_distributed_rampup oncall: r2p,False
68781,[docs] Clarify DDP activation checkpointing support oncall: distributed module: docs triaged better-engineering pt_distributed_rampup module: ddp Stale,2021-11-22 23:30:15+00:00,,0,1,oncall: distributed module: docs triaged better-engineering pt_distributed_rampup module: ddp Stale,True
68780,Incorrect documentation for `BCEWithLogitsLoss` `weight`? module: docs module: nn triaged Stale,2021-11-22 23:23:39+00:00,,0,0,module: docs module: nn triaged Stale,True
68768,[feature request] making pytorch less noisy  high priority module: cpp-extensions feature triaged needs design module: ux,2021-11-22 21:15:21+00:00,,1,10,high priority module: cpp-extensions feature triaged needs design module: ux,True
68759,`test_variant_consistency_jit` errors for `torch.nn.functional.prelu` oncall: jit Stale,2021-11-22 20:36:16+00:00,,0,0,oncall: jit Stale,False
68752,`softmin` and `softmax` operators support different input dtypes based on whether the `dtype` kwarg is passed module: nn triaged module: ux,2021-11-22 19:37:52+00:00,,0,3,module: nn triaged module: ux,True
68746,Add kl_divergence between Normal and Laplace distribution. module: distributions triaged enhancement Stale,2021-11-22 18:38:42+00:00,,0,0,module: distributions triaged enhancement Stale,True
68742,RFC: Deprecate Bottleneck high priority triage review module: performance module: cpu module: deprecation oncall: profiler Stale,2021-11-22 18:22:14+00:00,,0,5,high priority triage review module: performance module: cpu module: deprecation oncall: profiler Stale,True
68734,SequentialLR object has no attribute '_last_lr' module: optimizer triaged module: LrScheduler,2021-11-22 16:30:51+00:00,,0,1,module: optimizer triaged module: LrScheduler,True
68726,Failed to create Gloo new group after initialized with NCCL oncall: distributed module: nccl,2021-11-22 10:20:41+00:00,,0,8,oncall: distributed module: nccl,False
68712,torch.utils.data.Sampler is not recognized as a collections.abc.Sized module: dataloader module: typing triaged Stale,2021-11-21 21:15:07+00:00,,0,3,module: dataloader module: typing triaged Stale,True
68703,Feature request: Extend `torch.eye` for creating a batch of eye matrices  feature triaged module: tensor creation,2021-11-21 13:25:55+00:00,,0,3,feature triaged module: tensor creation,True
68701,Tensor precision manifests differently between CPU and GPU. module: numerical-stability module: cuda triaged module: arm Stale module: jetson,2021-11-21 11:46:52+00:00,,0,5,module: numerical-stability module: cuda triaged module: arm Stale module: jetson,True
68699,Transpose of a sparse tensor is not a view operation module: sparse module: autograd triaged needs design module: viewing and reshaping Stale,2021-11-21 10:11:52+00:00,,0,6,module: sparse module: autograd triaged needs design module: viewing and reshaping Stale,True
68696,Is it possible to use torch.linalg.cholesky for cholesky decomposition of a huge matrix that doesn't fit in memory? triaged module: linear algebra Stale,2021-11-20 23:33:34+00:00,,0,3,triaged module: linear algebra Stale,True
68655,[ux] F.binary_cross_entropy (and maybe other losses) to auto-cast bool category mask to float module: nn triaged enhancement module: ux Stale,2021-11-19 18:11:39+00:00,,0,3,module: nn triaged enhancement module: ux Stale,True
68648,[feature request] Sticky/force regime of train/eval modes module: nn triaged enhancement needs research module: ux,2021-11-19 15:52:09+00:00,,0,1,module: nn triaged enhancement needs research module: ux,True
68643,DISABLED test_ind_worker_queue (__main__.TestIndividualWorkerQueue) high priority module: dataloader triaged module: macos skipped,2021-11-19 14:16:46+00:00,,0,11,high priority module: dataloader triaged module: macos skipped,True
68638,ONNX export support for quantize_fx module: onnx triaged Stale onnx-triaged,2021-11-19 08:58:01+00:00,,0,6,module: onnx triaged Stale onnx-triaged,False
68626,`jit.script` doesn't support `device(type='cuda')` oncall: jit,2021-11-19 01:54:08+00:00,,0,1,oncall: jit,False
68622,"torch.{tensor, Tensor, LongTensor, ...} isn't captured under `enable_python_mode()` triaged module: __torch_dispatch__",2021-11-19 00:41:42+00:00,,0,2,triaged module: __torch_dispatch__,True
68616,"`torch.flip`, `torch.roll`, `torch.tile` has arguments `dims=` for both dim/multidim while other functions have argument `dim=` for the same usage module: internals triaged module: python frontend",2021-11-18 23:48:15+00:00,,0,7,module: internals triaged module: python frontend,True
68610,`torch.unique_consecutive`: passing positional optional arguments results in empty tensors triaged module: sorting and selection,2021-11-18 20:26:08+00:00,,0,1,triaged module: sorting and selection,True
68595,Inconsistent list indexing behavior triaged module: advanced indexing,2021-11-18 16:16:21+00:00,,0,2,triaged module: advanced indexing,True
68575,[ux] Print tensor's dtype by default triaged enhancement module: ux,2021-11-18 10:06:52+00:00,,0,0,triaged enhancement module: ux,True
68568,"About ModuleDict indexing when save model based on ""torch.jit.script()"" method oncall: jit",2021-11-18 02:55:06+00:00,,0,2,oncall: jit,True
68559,LibTorch -> TorchScript -> PyTorch (Python) fails with `AttributeError: 'RecursiveScriptModule' object has no attribute 'forward'` oncall: jit,2021-11-18 01:16:43+00:00,,0,4,oncall: jit,False
68553,[distributed] elastic/agent/server/api.py could react faster on exit / Ctrl+D / Ctrl+C triaged oncall: r2p,2021-11-17 23:27:59+00:00,,0,4,triaged oncall: r2p,True
68538,`vmap` performance warnings from `jacobian` module: performance triaged module: vmap,2021-11-17 20:26:52+00:00,,0,2,module: performance triaged module: vmap,True
68536,Exporting a model with `_save_for_lite_interpreter()` fails with Segmentation fault oncall: mobile,2021-11-17 20:09:31+00:00,,0,1,oncall: mobile,True
68519,dataloader will miss batch data when num worker>0 module: dataloader triaged,2021-11-17 14:07:54+00:00,,0,2,module: dataloader triaged,True
68516,"[ux] [feature request] Arguments to modules(), named_modules(), children(), named_children() to filter modules of specific types module: nn triaged enhancement needs research",2021-11-17 11:16:51+00:00,,0,5,module: nn triaged enhancement needs research,True
68514,Add common regularizations to PyTorch. feature module: nn triaged needs research module: norms and normalization,2021-11-17 09:17:08+00:00,,0,0,feature module: nn triaged needs research module: norms and normalization,False
68513,torch.fx cannot trace torch.Size() properly triaged module: fx,2021-11-17 08:15:05+00:00,,0,0,triaged module: fx,True
68512,'replicate' padding in convolution is 77 times slower on cpu than 'zeros' module: performance module: nn triaged module: padding,2021-11-17 07:38:20+00:00,,0,3,module: performance module: nn triaged module: padding,True
68507,Multiprocessing hang and queue short circuiting module: multiprocessing triaged module: deadlock,2021-11-17 05:26:18+00:00,,0,0,module: multiprocessing triaged module: deadlock,True
68506,cudnn_convolution_relu is 17x slower for 7x7 convolutions and channels last module: cudnn module: cuda triaged,2021-11-17 04:08:10+00:00,,0,0,module: cudnn module: cuda triaged,True
68498,Review disabling tests workflow triaged,2021-11-17 00:59:02+00:00,,1,1,triaged,False
68475,[LTC][BE] Make the compile flags more strict triaged lazy,2021-11-16 21:50:24+00:00,,0,0,triaged lazy,True
68474,[LTC][BE] Fix all compile warnings triaged lazy,2021-11-16 21:50:04+00:00,,0,0,triaged lazy,True
68464,"Changing the type of dtype argument in softmax, log_softmax, and softmin is possibly BC-breaking. module: bc-breaking triaged module: norms and normalization topic: bc breaking",2021-11-16 20:18:10+00:00,,0,0,module: bc-breaking triaged module: norms and normalization topic: bc breaking,True
68430,Strides issue on unsqueezed channels last tensor triaged module: memory format,2021-11-16 15:27:20+00:00,,0,2,triaged module: memory format,True
68429,istft gradcheck fails on ROCm module: autograd module: rocm triaged module: fft,2021-11-16 15:23:18+00:00,,0,1,module: autograd module: rocm triaged module: fft,True
68423,`torch.utils.checkpoint.checkpoint_sequential` is not optimal module: checkpoint triaged,2021-11-16 13:21:51+00:00,,0,0,module: checkpoint triaged,True
68422,There is a problem with GPU memory reclamation module: memory usage triaged,2021-11-16 12:59:49+00:00,,0,10,module: memory usage triaged,True
68420,`nn.functional.max_unpool(2|3)d`: failing shape check for correct inputs (with `dilation > 1`) with specified `output_size` module: nn triaged module: pooling,2021-11-16 11:45:43+00:00,,0,1,module: nn triaged module: pooling,True
68407,[Runtime Error in ddp_pipeline.py] RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation. high priority triage review oncall: distributed module: docs triaged module: ddp,2021-11-16 04:01:35+00:00,,1,14,high priority triage review oncall: distributed module: docs triaged module: ddp,True
68405,"Windows multi machine multi card training, card initialization in GLOO communication. oncall: distributed module: windows triaged",2021-11-16 03:27:27+00:00,,0,7,oncall: distributed module: windows triaged,True
68394,"INTERNAL ASSERT FAILED at ""../aten/src/ATen/MapAllocator.cpp"":323, please report a bug to PyTorch. unable to mmap 68 bytes from file </torch_530808_7992>: Cannot allocate memory (12) --------------------------------------------------------------------------- module: multiprocessing module: cpu triaged",2021-11-16 00:04:06+00:00,,0,5,module: multiprocessing module: cpu triaged,True
68393,[subgraph_rewriter] Support for non-Tensor replacement triaged module: fx,2021-11-16 00:00:30+00:00,,0,0,triaged module: fx,True
68385,ImportError: cannot import name 'ProcessGroup' from 'torch.distributed' module: binaries oncall: distributed module: macos,2021-11-15 22:12:10+00:00,,0,8,module: binaries oncall: distributed module: macos,True
68377,Misleading Error Message from nn.RNN when Passing Incorrect Data Type module: rnn triaged,2021-11-15 19:51:36+00:00,,0,1,module: rnn triaged,True
68352,`Delta` not defined in the docs of `HingeEmbeddingLoss` module: docs module: nn module: loss triaged actionable,2021-11-15 15:17:40+00:00,,0,1,module: docs module: nn module: loss triaged actionable,True
68349,"SIGILL, Illegal Instruction from libtorch_cpu.so when callling backward() function in a VM. module: dependency bug module: crash module: cpu triaged module: mkldnn",2021-11-15 14:50:51+00:00,,0,2,module: dependency bug module: crash module: cpu triaged module: mkldnn,True
68340,Torch 1.10.0 - RuntimeError: requires_grad_ is not supported on ScriptModules oncall: jit,2021-11-15 14:19:20+00:00,,0,1,oncall: jit,False
68337,"`nn.functional.max_unpool{n}d`: shape checks fail with `output_size=(C, ...)`. module: nn triaged module: pooling",2021-11-15 12:16:39+00:00,,0,0,module: nn triaged module: pooling,True
68332,[feature request] Provide functional form of scheduler formulas (and reconsider older decisions of not doing it) feature module: optimizer triaged needs research module: LrScheduler,2021-11-15 11:05:46+00:00,,0,9,feature module: optimizer triaged needs research module: LrScheduler,False
68331,Casting real parameter to complex during forward produces warning on backward module: autograd triaged module: complex complex_autograd,2021-11-15 09:51:33+00:00,,0,14,module: autograd triaged module: complex complex_autograd,True
68330,"isObject()INTERNAL ASSERT FAILED at ""../aten/src/ATen/core/ivalue_inl.h"":115, please report a bug to PyTorch. Expected Object but got Tensor oncall: jit module: android oncall: mobile",2021-11-15 09:34:37+00:00,,1,3,oncall: jit module: android oncall: mobile,True
68323,sparse.mm: CUDA error: internal error when calling `cusparseSpGEMM_workEstimation [...]` module: sparse module: cuda triaged,2021-11-14 10:42:09+00:00,,1,10,module: sparse module: cuda triaged,True
68320,Potential race conditions between multiple workers trying to download and cache the same file in torch.hub.load_state_dict_from_url and torch.hub.download_url_to_file <- duplicate dataset/model downloads across DDP workers high priority triaged module: hub actionable,2021-11-13 22:04:43+00:00,,0,21,high priority triaged module: hub actionable,True
68315,[docs] Strange signatures for torch.autocast module: docs triaged module: amp (automated mixed precision),2021-11-13 03:42:30+00:00,,0,1,module: docs triaged module: amp (automated mixed precision),True
68301,[FX] [BUG] Tensor.{inplace_method}_(.) is eliminated as dead code triaged module: fx,2021-11-13 00:03:32+00:00,,0,3,triaged module: fx,True
68298,[Lazy] Add torch.autograd.Function wrappers for the following ops in order to support dynamic shapes triaged lazy,2021-11-12 23:27:41+00:00,,0,0,triaged lazy,False
68297,[DDP] Debug mode should ensure reduction is finished in backward pass oncall: distributed better-engineering pt_distributed_rampup module: ddp,2021-11-12 23:18:01+00:00,,0,0,oncall: distributed better-engineering pt_distributed_rampup module: ddp,True
68291,"INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1634272068185/work/aten/src/ATen/native/LinearAlgebraUtils.h"":244 triaged module: linear algebra",2021-11-12 22:38:26+00:00,,0,11,triaged module: linear algebra,True
68288,List of issues to unblock dynamic shapes in Lazy Tensor triaged lazy,2021-11-12 22:31:48+00:00,,0,0,triaged lazy,True
68287,[Lazy] Add torch.autograd.Function wrappers for the following ops in order to support dynamic shapes triaged lazy,2021-11-12 22:16:43+00:00,,0,0,triaged lazy,False
68272,Query CUDA version LibTorch has been compiled with module: cuda triaged function request,2021-11-12 19:58:32+00:00,,0,1,module: cuda triaged function request,True
68258,Option to configure TorchElastic multiprocessing/api.py PContext.close() timeout triaged enhancement,2021-11-12 17:15:06+00:00,,1,1,triaged enhancement,True
68256,RuntimeError: CUDA error: initialization error when calling torch.distributed.init_process_group using torch multiprocessing oncall: distributed,2021-11-12 16:31:34+00:00,,0,4,oncall: distributed,True
68249,Does Pytorch1.10 enable cuda graph default? triaged module: cuda graphs,2021-11-12 13:19:03+00:00,,0,3,triaged module: cuda graphs,True
68248,"bug when loss backward, related to AdaptiveAvgPool1d module: cuda triaged module: pooling",2021-11-12 13:10:44+00:00,,0,4,module: cuda triaged module: pooling,True
68208,Make `torch.Tensor.view` support autograd for appropriate cases module: autograd triaged enhancement module: viewing and reshaping,2021-11-11 22:41:02+00:00,,1,8,module: autograd triaged enhancement module: viewing and reshaping,True
68204,Feature Request: Scriptable Python Source-of-Truth for operator shape functions feature triaged,2021-11-11 21:31:13+00:00,,0,3,feature triaged,False
68193,clang-tidy shows false positive failure on PRs that rename files module: ci module: lint triaged,2021-11-11 19:13:58+00:00,,0,1,module: ci module: lint triaged,True
68190,[FX][docs] Document gotcha about `training` flag triaged module: fx,2021-11-11 19:06:32+00:00,,1,0,triaged module: fx,False
68179,torch.nn.Linear wrong type annotation for bias module: nn module: typing triaged,2021-11-11 16:00:22+00:00,,0,4,module: nn module: typing triaged,True
68174,RFC: “What’s in a (NumPY) name?” — Changing PyTorch Operator Names high priority triaged module: numpy,2021-11-11 14:33:39+00:00,,0,7,high priority triaged module: numpy,True
68171,DISABLED test_nadam (__main__.TestOptim) high priority triage review module: flaky-tests skipped module: dynamo,2021-11-11 14:17:12+00:00,,0,25,high priority triage review module: flaky-tests skipped module: dynamo,False
68169,torch.histogram: Sum of PDFs is num of bins instead of 1 module: docs triaged module: sorting and selection,2021-11-11 10:48:40+00:00,,1,2,module: docs triaged module: sorting and selection,True
68165,Upgrade NCCL2.11.4 in the new PyTorch module: cuda triaged,2021-11-11 08:03:31+00:00,,0,0,module: cuda triaged,True
68163,wsl2 install failed from source code module: build triaged module: mkl,2021-11-11 06:37:37+00:00,,0,0,module: build triaged module: mkl,True
68144,Unstable buggy calculation. needs reproduction module: optimizer triaged,2021-11-11 01:23:11+00:00,,0,2,needs reproduction module: optimizer triaged,False
68114,CPU Memory Deallocation  module: memory usage triaged,2021-11-10 13:07:47+00:00,,0,5,module: memory usage triaged,True
68110,zeta(1/2) returns nan triaged module: special,2021-11-10 07:24:23+00:00,,1,3,triaged module: special,True
68105,Some type combinations of cublas gemm are not supported when they should module: cuda triaged module: type promotion module: linear algebra,2021-11-10 04:44:40+00:00,,0,2,module: cuda triaged module: type promotion module: linear algebra,True
68104,DDP is not working with Pytorch Lightning oncall: distributed,2021-11-10 04:32:00+00:00,,0,2,oncall: distributed,False
68102,Can not guarantee reproducible for `nn.ReflectionPad2d` triaged module: numerical-reproducibility,2021-11-10 02:45:04+00:00,,0,2,triaged module: numerical-reproducibility,True
68088,[distributed] Restructure our distributed collective tests oncall: distributed module: c10d,2021-11-09 22:10:10+00:00,,0,1,oncall: distributed module: c10d,False
68081,[rfc] Target Determinator module: ci triaged,2021-11-09 20:30:43+00:00,,0,0,module: ci triaged,True
68070,C++ at::Tensor's pinned_memory status is not printing out correctly. high priority module: cpp triaged module: correctness (silent),2021-11-09 18:02:15+00:00,,1,1,high priority module: cpp triaged module: correctness (silent),True
68065,Incorrect return type annotation for _get_mobile_model_contained_types oncall: jit,2021-11-09 15:46:35+00:00,,0,0,oncall: jit,True
68064,In place `or` operator doesn't work in torchscript oncall: jit,2021-11-09 15:40:04+00:00,,0,0,oncall: jit,False
68063,pytorch/ExampleRepo module: ci triaged,2021-11-09 15:37:12+00:00,,0,2,module: ci triaged,True
68060,functorch transforms are silently incorrect with autograd.Function high priority module: autograd triaged module: vmap module: correctness (silent),2021-11-09 15:05:02+00:00,,0,1,high priority module: autograd triaged module: vmap module: correctness (silent),True
68058,Some lr-schedulers docs seems to have typos/missing information module: docs module: optimizer triaged module: LrScheduler,2021-11-09 13:58:06+00:00,,0,8,module: docs module: optimizer triaged module: LrScheduler,True
68055,Release official Python 3.9 images module: binaries triaged,2021-11-09 10:23:47+00:00,,0,3,module: binaries triaged,True
68049,Request to revise the Pytorch tutorial. oncall: jit module: docs module: cpp,2021-11-09 05:48:03+00:00,,0,3,oncall: jit module: docs module: cpp,False
68046,"register_ Hook causes the CUDA out of memory, and remove() is useless module: autograd triaged",2021-11-09 03:38:15+00:00,,0,1,module: autograd triaged,True
68041,[RFC] APEX style fused optimizers in PyTorch module: performance module: optimizer triaged module: mta,2021-11-09 01:47:05+00:00,,0,3,module: performance module: optimizer triaged module: mta,True
68017,MacOS CI result is not in /README.md module: docs triaged module: macos actionable,2021-11-08 19:01:29+00:00,,0,2,module: docs triaged module: macos actionable,True
68014,Add TORCHELASTIC_RESTART_COUNT env variable to DDP logging oncall: distributed module: bootcamp better-engineering pt_distributed_rampup module: ddp,2021-11-08 18:26:04+00:00,,1,2,oncall: distributed module: bootcamp better-engineering pt_distributed_rampup module: ddp,True
68009,Properly document the `to.dtype_layout` overload of `Tensor.to` module: docs triaged actionable,2021-11-08 17:38:59+00:00,,0,0,module: docs triaged actionable,True
67999,Improved Transformer and MultiHeadAttention design oncall: transformer/mha,2021-11-08 14:31:33+00:00,,0,3,oncall: transformer/mha,False
67989,the abnormal display of bottleneck of resnet in tensorboard triaged module: tensorboard,2021-11-08 10:16:13+00:00,,0,0,triaged module: tensorboard,True
67983,torch.jit.export does not working oncall: jit,2021-11-08 06:31:16+00:00,,0,7,oncall: jit,True
67979,Feature Request: Support prelu_cuda for BFloat16 module: bootcamp module: nn module: cuda triaged enhancement module: bfloat16,2021-11-08 03:33:05+00:00,,0,4,module: bootcamp module: nn module: cuda triaged enhancement module: bfloat16,True
67978,c10::CUDAError oncall: distributed module: cuda triaged module: nccl,2021-11-08 03:07:27+00:00,,0,24,oncall: distributed module: cuda triaged module: nccl,True
67972,Distributed.TCPStore oncall: distributed,2021-11-07 22:46:24+00:00,,0,8,oncall: distributed,False
67970,Error when using torch.fx on bert triaged module: fx,2021-11-07 15:18:50+00:00,,0,3,triaged module: fx,True
67969,"[docs] F.multilabel_soft_margin_loss does not list arg reduction, but lists outdated size_average/reduce (true for other losses as well) module: docs module: nn triaged actionable",2021-11-07 11:29:49+00:00,,0,1,module: docs module: nn triaged actionable,False
67968,"unresolved external symbol ""__declspec(dllimport) struct _object * __cdecl THPVariable_Wrap module: build triaged",2021-11-07 10:12:26+00:00,,0,7,module: build triaged,True
67958,The `SequentialLR` scheduler uses a deprecated pattern  module: optimizer module: tests triaged module: LrScheduler,2021-11-06 17:20:17+00:00,,0,0,module: optimizer module: tests triaged module: LrScheduler,True
67955,Torch 1.9.1.post3 - Error in magma_getdevice_arch: MAGMA not initialized (call magma_init() first) or bad device triaged,2021-11-06 14:24:34+00:00,,0,5,triaged,True
67937,Add support for LTC ops needed by backwards/autograd/autocast triaged module: lazy,2021-11-05 23:05:55+00:00,,0,3,triaged module: lazy,False
67913,Tracker: ModuleInfo-based testing module: nn module: tests triaged tracker,2021-11-05 14:49:07+00:00,,0,3,module: nn module: tests triaged tracker,True
67902,torch::jit::pickle_load is unable to read some files saved by torch.save oncall: jit,2021-11-05 08:42:17+00:00,,0,4,oncall: jit,True
67893,AdaptiveMaxPool2d bug oncall: jit,2021-11-05 04:21:16+00:00,,0,1,oncall: jit,True
67889,[libtorch][torch.jit.script][input dynamic shape]Error in loading “. Pt” file on C + +: index out of range oncall: jit,2021-11-05 02:30:17+00:00,,0,0,oncall: jit,True
67887,Add more explanation on multithreaded graph building of Autograd module: docs module: multiprocessing module: autograd triaged module: multithreading actionable,2021-11-04 23:52:44+00:00,,0,7,module: docs module: multiprocessing module: autograd triaged module: multithreading actionable,True
67879,AMP does not improve performance triaged module: amp (automated mixed precision),2021-11-04 22:15:46+00:00,,0,2,triaged module: amp (automated mixed precision),True
67838,`test_variant_consistency_jit_contiguous_cpu_float32` from `test_ops` fails oncall: jit,2021-11-04 11:14:04+00:00,,0,0,oncall: jit,False
67835,It seems to be a bug！！！ needs reproduction triaged,2021-11-04 10:36:17+00:00,,0,1,needs reproduction triaged,True
67834,Model tracing not working oncall: jit,2021-11-04 09:56:22+00:00,,0,5,oncall: jit,False
67797,Conv2d kernel performance regression on CPU since PyTorch 1.9 module: performance module: convolution triaged module: mkldnn module: regression,2021-11-03 23:00:04+00:00,,0,23,module: performance module: convolution triaged module: mkldnn module: regression,True
67773,Instability in `test_input_weight_equalization_activation_values` test for random test values. oncall: quantization low priority module: tests triaged,2021-11-03 16:32:01+00:00,,1,0,oncall: quantization low priority module: tests triaged,True
67772,Functions are rendered incorrectly module: docs module: autograd triaged,2021-11-03 16:21:54+00:00,,0,0,module: docs module: autograd triaged,True
67761,Allow LRScheduler to take in param_groups directly without an optimizer module: optimizer triaged enhancement module: LrScheduler,2021-11-03 11:19:38+00:00,,0,0,module: optimizer triaged enhancement module: LrScheduler,True
67760,Add an LRScheduler interface for torch schedulers. module: optimizer triaged enhancement module: LrScheduler,2021-11-03 10:36:58+00:00,,0,3,module: optimizer triaged enhancement module: LrScheduler,True
67758,Building libtorch from source requires too much RAM module: build module: cuda triaged,2021-11-03 08:40:09+00:00,,0,1,module: build module: cuda triaged,True
67751,Remove logic for constructing symbolic shapes in Profiler triaged better-engineering jit-backlog,2021-11-03 06:02:31+00:00,,1,1,triaged better-engineering jit-backlog,True
67745,A `sieve` operation for separating values of a tensor into two tensors based on a condition. triaged enhancement module: ux module: viewing and reshaping,2021-11-03 03:43:59+00:00,,0,0,triaged enhancement module: ux module: viewing and reshaping,True
67740,`torch.utils.data.random_split` example broken in 1.10 needs reproduction triaged module: data,2021-11-03 03:16:50+00:00,,0,2,needs reproduction triaged module: data,True
67712,"Decorate tests with a ""deadline"" module: ci triaged better-engineering",2021-11-02 21:16:38+00:00,,2,2,module: ci triaged better-engineering,True
67687,"Trying to quantize and save a pre-trained deberta model, where i get a runtime error  oncall: jit triaged",2021-11-02 16:35:23+00:00,,0,2,oncall: jit triaged,True
67684,[feature request] torch.clamp on BoolTensors triaged enhancement module: numpy module: boolean tensor,2021-11-02 16:28:59+00:00,,0,5,triaged enhancement module: numpy module: boolean tensor,True
67683,PyTorch Profiler for distributed time count oncall: distributed module: tensorboard oncall: profiler,2021-11-02 16:10:50+00:00,,0,3,oncall: distributed module: tensorboard oncall: profiler,False
67682,Feature request: bfloat16 support for CUDA matmuls module: cuda triaged enhancement module: bfloat16 matrix multiplication,2021-11-02 15:59:36+00:00,,0,7,module: cuda triaged enhancement module: bfloat16 matrix multiplication,True
67680,OOM with a lot of GPU memory left module: cuda module: memory usage triaged,2021-11-02 13:55:04+00:00,,0,36,module: cuda module: memory usage triaged,True
67674,torch.jit.script fails to find attribute '_modules' of nn.Module oncall: jit,2021-11-02 09:48:44+00:00,,0,1,oncall: jit,True
67662,"""self.graph.owning_module not set for purity check"" error when trying to remove a node from torch.fx.graph triaged module: fx",2021-11-02 04:19:17+00:00,,0,0,triaged module: fx,True
67659,`test_variant_consistency_jit` failing for `max_unpool2d` oncall: jit,2021-11-02 03:25:15+00:00,,0,0,oncall: jit,False
67653,"TSAN issue in autograd ""set_next_edges"" module: autograd triaged actionable module: sanitizers",2021-11-02 02:12:08+00:00,,0,10,module: autograd triaged actionable module: sanitizers,True
67651,MultiHeadAttention in quantizable seems incorrect with batch_first=True oncall: quantization low priority triaged oncall: transformer/mha,2021-11-01 23:47:47+00:00,,1,3,oncall: quantization low priority triaged oncall: transformer/mha,True
67646,test_jit_fuser_legacy and test_jit_fuser_te are failing with SIGIOT high priority triage review oncall: jit module: tests NNC,2021-11-01 22:32:36+00:00,,0,35,high priority triage review oncall: jit module: tests NNC,True
67642,Is there a pre-built pytorch package for CUDA 10.1? module: build triaged,2021-11-01 21:56:30+00:00,,0,1,module: build triaged,True
67634,Feature request: Grid sample on complex tensors with float grid feature module: nn triaged module: complex needs research module: interpolation,2021-11-01 19:57:37+00:00,,0,3,feature module: nn triaged module: complex needs research module: interpolation,True
67603,The minimum supported version of pytorch when using toch-mobile？ oncall: mobile,2021-11-01 01:28:02+00:00,,1,0,oncall: mobile,False
67602,[feature request] Add no-op set_epoch on Sampler class feature module: dataloader triaged,2021-10-31 21:49:31+00:00,,0,0,feature module: dataloader triaged,False
67599,"Mobile builds need to make custom build of libtorch to run, let's just include it to make it easier oncall: mobile",2021-10-31 19:45:13+00:00,,0,0,oncall: mobile,False
67598,unnecessary warning with autocast  triaged module: amp (automated mixed precision),2021-10-31 13:10:12+00:00,,0,5,triaged module: amp (automated mixed precision),True
67597,torch.kthvalue (and maybe torch.quantile) should support argument descending/largest (=True) like topk/sort feature triaged module: sorting and selection,2021-10-31 10:10:54+00:00,,0,1,feature triaged module: sorting and selection,True
67594,Feature request: set_grad_enabled(*mods_or_params) as a safe context manager feature module: autograd module: nn low priority triaged,2021-10-31 01:45:22+00:00,,0,3,feature module: autograd module: nn low priority triaged,True
67590,Add `OptState.UPDATED` to `torch.cuda.amp.GradScaler` feature triaged module: amp (automated mixed precision),2021-10-30 16:22:53+00:00,,0,0,feature triaged module: amp (automated mixed precision),True
67589,Expose the `OptState` in `torch.cuda.amp.GradScaler` feature triaged module: amp (automated mixed precision),2021-10-30 16:22:43+00:00,,0,0,feature triaged module: amp (automated mixed precision),True
67587,Re-implement `Optimizer.__repr__` module: optimizer triaged needs design module: python frontend,2021-10-30 14:41:47+00:00,,0,4,module: optimizer triaged needs design module: python frontend,True
67586,SequentialLR have a question and why it use `step(epoch)` module: optimizer triaged module: LrScheduler,2021-10-30 08:25:40+00:00,,0,3,module: optimizer triaged module: LrScheduler,True
67571,[discussion] Have default size/shape equal to a scalar triaged module: random,2021-10-29 22:00:45+00:00,,0,1,triaged module: random,True
67570,RFC: Overlap optimizer computation with DDP/FSDP backward oncall: distributed module: ddp,2021-10-29 21:43:54+00:00,,1,1,oncall: distributed module: ddp,False
67565,Adding a new kwarg to a torch.nn.functional function breaks FC for JIT  oncall: jit module: nn triaged has workaround better-engineering,2021-10-29 20:03:59+00:00,,0,3,oncall: jit module: nn triaged has workaround better-engineering,True
67543,[FX] Support `int` and similar variants in FX triaged module: fx,2021-10-29 11:01:47+00:00,,0,0,triaged module: fx,True
67542,Segfault from ONNX exporting code where torch.jit.script and torch.no_grad are used together oncall: jit,2021-10-29 10:42:03+00:00,,0,0,oncall: jit,False
67541,TestBenchNetwork::test_forward[resnet50_jit-profiling-te] is failing intermittently high priority triage review oncall: jit NNC,2021-10-29 09:02:29+00:00,,0,2,high priority triage review oncall: jit NNC,True
67540,LTC asynchronous executor is not synced with python code and ignores context manager. triaged LazyTensor_nvfuser_integration module: lazy,2021-10-29 08:38:59+00:00,,0,3,triaged LazyTensor_nvfuser_integration module: lazy,True
67535,[LTC] `UncachedCompile` numbers increases for several iterations until fixed  triaged lazy module: lazy,2021-10-29 05:20:02+00:00,,0,4,triaged lazy module: lazy,True
67493,XLA not being tested in TestAutogradDeviceType module: autograd module: tests triaged module: xla,2021-10-28 23:03:01+00:00,,0,7,module: autograd module: tests triaged module: xla,True
67482,Improve docs coverage testing module: docs triaged better-engineering,2021-10-28 21:09:28+00:00,,0,2,module: docs triaged better-engineering,False
67481,Batchnorm2D does behaves differently with different shapes when batch_size=1 module: nn triaged module: norms and normalization,2021-10-28 21:07:25+00:00,,0,1,module: nn triaged module: norms and normalization,True
67474,[JIT][Feature] Runtime Verifier of Alias Annotations oncall: jit,2021-10-28 20:33:56+00:00,,0,2,oncall: jit,False
67473,"[Feature] Separate, Queryable Alias Parser  oncall: jit",2021-10-28 20:29:44+00:00,,0,2,oncall: jit,False
67470,"integer (and possibly float16) matmul fails test_noncontiguous_samples on CPU (and all backward dtypes, too?) high priority triaged module: linear algebra module: correctness (silent)",2021-10-28 20:17:05+00:00,,0,0,high priority triaged module: linear algebra module: correctness (silent),True
67463,`test_forward_mode_AD` hangs for `nn.functional.cosine_embedding_loss` module: autograd module: nn triaged module: deadlock module: forward ad,2021-10-28 19:25:28+00:00,,0,5,module: autograd module: nn triaged module: deadlock module: forward ad,True
67462,Jacobian mismatch for `nn.functional.ctc_loss` high priority module: autograd module: nn triaged actionable module: correctness (silent),2021-10-28 19:20:47+00:00,,0,1,high priority module: autograd module: nn triaged actionable module: correctness (silent),True
67458,OpInfo JIT tests fail for torch.nonzero oncall: jit,2021-10-28 18:28:58+00:00,,0,0,oncall: jit,True
67457,"Reductions on zero-size dims: 1) by accepting a custom default value, 2) if tensor has another non-reduced zero-size dim triaged enhancement module: reductions",2021-10-28 17:58:38+00:00,,0,2,triaged enhancement module: reductions,True
67450,CUDA gradcheck tests can occasionally leak memory in HUD CI high priority module: cuda module: memory usage module: tests triaged,2021-10-28 15:42:37+00:00,,0,9,high priority module: cuda module: memory usage module: tests triaged,True
67448,Deadlock in test_multiprocessing_spawn.py high priority module: multiprocessing triaged module: deadlock,2021-10-28 15:28:23+00:00,,0,0,high priority module: multiprocessing triaged module: deadlock,True
67446,`/var/lib/jenkins/workspace/xla/test/test_mp_rendezvous.py` potentially flaky high priority triage review triaged module: flaky-tests module: xla,2021-10-28 15:19:04+00:00,,0,1,high priority triage review triaged module: flaky-tests module: xla,True
67439,Exception in thread when using dataloader module: dataloader triaged module: multithreading,2021-10-28 12:22:52+00:00,,0,0,module: dataloader triaged module: multithreading,True
67420,Avoid warnings when jitting pytorch modules oncall: jit,2021-10-28 05:21:31+00:00,,0,0,oncall: jit,True
67402,[Documentation] Incomplete FX module triaged module: fx,2021-10-27 23:43:00+00:00,,0,1,triaged module: fx,True
67392,Missing doc for torch.jit functions oncall: jit module: docs,2021-10-27 22:44:55+00:00,,0,0,oncall: jit module: docs,False
67390,Missing doc for torch.distributions functions module: distributions module: docs triaged,2021-10-27 22:43:34+00:00,,0,0,module: distributions module: docs triaged,True
67389,Missing doc for torch.distributed functions high priority triage review oncall: distributed module: docs better-engineering,2021-10-27 22:42:14+00:00,,0,0,high priority triage review oncall: distributed module: docs better-engineering,True
67388,Missing doc for torch.cuda functions module: docs module: cuda triaged,2021-10-27 22:39:48+00:00,,0,0,module: docs module: cuda triaged,True
67387,Missing doc for torch.autograd functions module: docs module: autograd triaged,2021-10-27 22:36:04+00:00,,0,1,module: docs module: autograd triaged,False
67377,[LTC] Replace mv with addmv in ts_native_functions.yaml triaged,2021-10-27 21:30:26+00:00,,1,0,triaged,True
67364,[FX] FX torchvision tests fail with torchvision 0.10 triaged module: fx,2021-10-27 20:32:45+00:00,,0,0,triaged module: fx,True
67362,JIT fuser throws compilation error (1.10 regression) oncall: jit module: regression NNC,2021-10-27 20:03:41+00:00,,0,4,oncall: jit module: regression NNC,True
67350,[FX] autowrap_functions doesn't work for module-scoped functions triaged module: fx,2021-10-27 18:35:13+00:00,,0,1,triaged module: fx,True
67349,PyTorch Profiler built with Bazel doesn't produce GPU trace oncall: profiler module: bazel,2021-10-27 18:31:35+00:00,,0,6,oncall: profiler module: bazel,False
67337,Support prim_layout operator in onnx module: onnx triaged onnx-triaged,2021-10-27 15:59:42+00:00,,0,4,module: onnx triaged onnx-triaged,True
67333,Feature request: Complex Number Support for Special Functions feature triaged module: complex module: special,2021-10-27 15:21:00+00:00,,0,2,feature triaged module: complex module: special,True
67324,torch.stft - fill_cuda not implemented for ComplexHalf module: cuda triaged module: complex module: half,2021-10-27 08:20:24+00:00,,0,9,module: cuda triaged module: complex module: half,True
67321,alias to generate tensor with random uniform distribution. feature good first issue triaged module: random,2021-10-27 06:15:46+00:00,,0,14,feature good first issue triaged module: random,True
67312,ProcessGroupNCCLTest.testSequenceNumInit is failing on 4xlarge oncall: distributed triaged module: c10d,2021-10-27 03:24:47+00:00,,1,2,oncall: distributed triaged module: c10d,True
67284,Failed to build with master. module: build triaged module: mkl,2021-10-26 20:46:37+00:00,,0,0,module: build triaged module: mkl,True
67279,register_dispatch_key should provide a way to tell if a native function has a no-op meta kernel triaged module: dispatch module: lazy,2021-10-26 19:08:03+00:00,,1,3,triaged module: dispatch module: lazy,True
67247,pytorch_linux_xenial_py3_6_gcc5_4_test may timeout during test_multiprocessing_spawn high priority module: multiprocessing module: tests triaged,2021-10-26 05:42:32+00:00,,0,1,high priority module: multiprocessing module: tests triaged,True
67241,"init_process_group hangs for multi-node, Pytorch > v1.3.1 and file init_method oncall: distributed module: nccl module: deadlock",2021-10-26 03:31:10+00:00,,0,4,oncall: distributed module: nccl module: deadlock,True
67240,Feature: Add derivative for channel_shuffle feature module: autograd module: nn triaged,2021-10-26 03:15:54+00:00,,1,11,feature module: autograd module: nn triaged,False
67237,[docs] Google finds docs pages that give 404 high priority module: docs triaged,2021-10-26 02:02:29+00:00,,1,15,high priority module: docs triaged,True
67208,TestForeachCUDA.test_binary_op_tensorlists_fastpath__foreach_add_cuda_bool and TestForeachCUDA.test_pointwise_op_fastpath__foreach_addcmul_cuda_uint8 fail intermittently module: cuda module: tests triaged module: mta,2021-10-25 20:23:44+00:00,,0,1,module: cuda module: tests triaged module: mta,True
67187,Expose `isDifferentiableType` to python module: autograd triaged better-engineering actionable,2021-10-25 15:24:41+00:00,,0,0,module: autograd triaged better-engineering actionable,True
67168,Multiply two named tensor causes RuntimeError triaged module: named tensor,2021-10-25 03:10:11+00:00,,0,0,triaged module: named tensor,True
67160,"Include function name in ""This Python function is annotated to be ignored and cannot be run"" error message oncall: jit",2021-10-24 20:16:59+00:00,,0,0,oncall: jit,True
67159,test_local_optimizer_parity (__main__.TestZeroRedundancyOptimizerDistributed) is flaky on rocm high priority triage review module: rocm module: flaky-tests,2021-10-24 20:15:48+00:00,,0,1,high priority triage review module: rocm module: flaky-tests,True
67158,Make streams used for NCCL operations configurable oncall: distributed triaged module: nccl,2021-10-24 17:13:26+00:00,,0,3,oncall: distributed triaged module: nccl,True
67153,Build not working with cuda 11.5 module: cuda triaged,2021-10-24 05:39:28+00:00,,0,7,module: cuda triaged,True
67146,`torch.jit.is_scripting()` not set when scripting a Module oncall: jit,2021-10-23 13:34:23+00:00,,0,6,oncall: jit,True
67142,"enumerate(..., start=idx) is not working correctly while scripting oncall: jit",2021-10-23 10:47:45+00:00,,0,3,oncall: jit,False
67136,addition of loss function RMSE in the torch.nn  feature module: nn module: loss triaged actionable,2021-10-23 04:51:44+00:00,,0,8,feature module: nn module: loss triaged actionable,True
67111,[doc] Can we update TorchScript union support? oncall: jit module: docs,2021-10-22 20:04:54+00:00,,0,0,oncall: jit module: docs,False
67099,JIT: inconsistency in LSTM.forward oncall: jit,2021-10-22 16:38:47+00:00,,0,0,oncall: jit,True
67092,Sparse matrix multiplication (torch.sparse.mm) NotImplementedError module: sparse feature triaged,2021-10-22 15:00:11+00:00,,1,1,module: sparse feature triaged,True
67087,Feature Request: 1d grid sample feature module: nn triaged needs research,2021-10-22 11:58:49+00:00,,0,3,feature module: nn triaged needs research,False
67084,Failing test_neg_view_nn_functional_embedding_cuda_float64 high priority triage review module: nn module: cuda module: flaky-tests module: complex actionable,2021-10-22 09:16:57+00:00,,0,1,high priority triage review module: nn module: cuda module: flaky-tests module: complex actionable,True
67071,Forward results vary depending on batch size on A100 machine module: cuda module: convolution triaged module: numerical-reproducibility,2021-10-22 02:15:36+00:00,,0,4,module: cuda module: convolution triaged module: numerical-reproducibility,True
67030, fx graph mode quantizing error   triaged module: fx,2021-10-21 17:24:24+00:00,,0,0,triaged module: fx,True
67011,Importing numpy interacts with `tensor.sum` perf module: performance triaged module: numpy,2021-10-21 10:19:01+00:00,,0,3,module: performance triaged module: numpy,True
67008,kwonly arguments without defaults don't work with test_overrides.py module: tests triaged better-engineering,2021-10-21 08:56:58+00:00,,0,0,module: tests triaged better-engineering,True
66992,Bug? :Run torch.unique twice get different running time?  module: performance triaged module: sorting and selection module: benchmark,2021-10-21 03:29:24+00:00,,0,4,module: performance triaged module: sorting and selection module: benchmark,True
66963,Significantly difference in execution time when convolution is run as nn.Conv2d and as nn.Sequential module: performance module: cuda module: convolution triaged,2021-10-20 20:31:54+00:00,,0,0,module: performance module: cuda module: convolution triaged,True
66937,[doc] Long Function Name (C++) overlapping on the side menu module: docs triaged,2021-10-20 12:56:57+00:00,,0,0,module: docs triaged,False
66936,Lifetime issues when recording external CUDA streams with the caching allocator module: cuda triaged,2021-10-20 12:15:00+00:00,,0,1,module: cuda triaged,True
66930,JIT vs. eager mismatches for jit.traced `int8` to `int32` casting oncall: jit NNC,2021-10-20 08:56:58+00:00,,0,1,oncall: jit NNC,True
66924,[BUG] Inconsistent initialization on different machines (tensor.uniform_()) triaged module: random,2021-10-20 03:41:25+00:00,,0,6,triaged module: random,True
66921,readme not update module: docs triaged,2021-10-20 01:41:20+00:00,,0,0,module: docs triaged,True
66907,torch.chunk return type may not be documented correctly module: docs triaged,2021-10-19 23:38:46+00:00,,0,1,module: docs triaged,True
66902,Modify Dr. CI so it could detect runner disconnection failures module: ci triaged,2021-10-19 23:13:55+00:00,,0,0,module: ci triaged,True
66894,test_nccl_barrier_timeout_new_group_non_member fails intermittently high priority triage review oncall: distributed module: tests,2021-10-19 20:21:48+00:00,,0,0,high priority triage review oncall: distributed module: tests,True
66875,"Make the evaluated value of function f(x) accessible from `torch.autograd.functional.jacobian(f,x)` module: autograd triaged",2021-10-19 15:53:50+00:00,,0,1,module: autograd triaged,True
66874,Error using _stateless version of Module module: nn triaged enhancement actionable,2021-10-19 15:53:44+00:00,,0,4,module: nn triaged enhancement actionable,True
66868,torch.triu behaves differently when diagonal out of range module: error checking triaged,2021-10-19 14:41:19+00:00,,0,0,module: error checking triaged,True
66858,CUDAgraph error while capturing  triaged module: cuda graphs,2021-10-19 10:35:24+00:00,,0,1,triaged module: cuda graphs,True
66853,"Libtorch C++ model forward  crashed on windows10, CUDA 11.2, Qt ,RTX 3070, but libtorch C++ works with cpu successfully module: windows triaged",2021-10-19 03:37:24+00:00,,0,9,module: windows triaged,True
66841,"[Torchscript] Comparison between list and int returns true  [1] == (1, ) == 1 oncall: jit",2021-10-19 02:17:15+00:00,,0,0,oncall: jit,False
66824,Migrate C++ tests to Python runner module: tests triaged,2021-10-18 23:54:10+00:00,,0,0,module: tests triaged,True
66816,Jit fuser half precision support oncall: jit,2021-10-18 20:34:19+00:00,,0,5,oncall: jit,False
66813,Use `__slots__` for the `nn.Module` class module: performance module: nn good first issue triaged better-engineering actionable,2021-10-18 20:12:49+00:00,,1,11,module: performance module: nn good first issue triaged better-engineering actionable,True
66806,Missing doc for `torch.segment_reduce` module: docs triaged module: numpy,2021-10-18 18:25:09+00:00,,0,1,module: docs triaged module: numpy,True
66791,jit tracer：Lost imported pre-model parameters oncall: jit,2021-10-18 14:42:30+00:00,,0,0,oncall: jit,True
66787,loading large model not finished after 16 hours module: performance module: serialization triaged,2021-10-18 10:20:20+00:00,,0,4,module: performance module: serialization triaged,True
66782,"Segmentation Fault when importing Torch, 2021 version module: binaries module: crash triaged",2021-10-18 09:22:36+00:00,,0,17,module: binaries module: crash triaged,True
66781,lr_scheduler.py  /  list index out of range needs reproduction module: optimizer triaged module: LrScheduler,2021-10-18 08:58:41+00:00,,0,2,needs reproduction module: optimizer triaged module: LrScheduler,True
66775,Feature request: FFT operations on Metal feature module: ci triaged module: macos oncall: mobile module: fft,2021-10-18 05:20:16+00:00,,0,7,feature module: ci triaged module: macos oncall: mobile module: fft,True
66768,Error: `copy_to_metal_ is implemented only for float dtype` oncall: mobile module: ios,2021-10-17 16:55:04+00:00,,0,0,oncall: mobile module: ios,False
66755,Domain Transformation APIs for LibTorch and LibTorch-Lite module: cpp triaged module: vision module: data,2021-10-16 12:12:54+00:00,,0,1,module: cpp triaged module: vision module: data,True
66751,torch.nn.functional.embedding behave differently in two cases of cpu and cuda module: error checking triaged module: embedding,2021-10-16 02:52:15+00:00,,0,2,module: error checking triaged module: embedding,True
66750,torch.nn.EmbeddingBag behave differently in two cases of cpu and cuda module: error checking triaged module: embedding,2021-10-16 02:40:55+00:00,,0,1,module: error checking triaged module: embedding,True
66707,`layer_norm` needs to be done in fp32 for fp16 inputs module: numerical-stability triaged module: half actionable module: norms and normalization,2021-10-15 18:30:53+00:00,,0,9,module: numerical-stability triaged module: half actionable module: norms and normalization,True
66678,Adding modern linear transformer variants to `transformer` module oncall: transformer/mha,2021-10-15 03:27:47+00:00,,0,0,oncall: transformer/mha,False
66656,RFC: Create unified CI experience for pytorch and domain libraries module: binaries module: ci triaged better-engineering,2021-10-14 20:50:51+00:00,,1,0,module: binaries module: ci triaged better-engineering,True
66651,[JIT] Profile optional tensor oncall: jit,2021-10-14 20:16:33+00:00,,0,0,oncall: jit,True
66640,Static Build of Libtorch not linking correctly module: build triaged,2021-10-14 18:04:50+00:00,,0,0,module: build triaged,True
66627,Overhaul error handling in `TCPStore` oncall: distributed better-engineering,2021-10-14 15:23:52+00:00,,0,1,oncall: distributed better-engineering,True
66624,Separate libtorch and non-libtorch specific files under `torch/csrc/distributed` triage review oncall: distributed better-engineering module: rpc,2021-10-14 14:58:36+00:00,,0,0,triage review oncall: distributed better-engineering module: rpc,True
66623,Constant folding in symbolic shape inference fails: expected scalar type Long but found Float oncall: jit,2021-10-14 14:02:02+00:00,,0,2,oncall: jit,False
66573,tools/amd_build/build_amd.py should fail if any file fails to write module: build module: rocm triaged,2021-10-13 20:52:38+00:00,,0,5,module: build module: rocm triaged,True
66565,Bazel target all_tests improperly reports failures on CPU-only (non-CUDA) build module: bootcamp module: tests triaged module: bazel,2021-10-13 19:06:07+00:00,,0,0,module: bootcamp module: tests triaged module: bazel,True
66563,Give a better error message when REGISTER_DISPATCH is used in improper context module: internals triaged module: dispatch,2021-10-13 18:49:29+00:00,,0,0,module: internals triaged module: dispatch,True
66547,"destroy_process_group() does not clear worker count, leading to ""Timed out initializing process group in store based barrier"" oncall: distributed",2021-10-13 10:59:23+00:00,,0,2,oncall: distributed,False
66545,Forward method slows down at some point if it is executed repeatedly on Android with NNAPI module: android oncall: mobile mobile_perf,2021-10-13 08:52:42+00:00,,0,0,module: android oncall: mobile mobile_perf,False
66542,Is there a bug in transposed convolution？ module: nn module: convolution triaged,2021-10-13 06:12:20+00:00,,0,0,module: nn module: convolution triaged,True
66504,BatchNorm runtimeError: one of the variables needed for gradient computation has been modified by an inplace operation high priority triage review oncall: distributed better-engineering module: ddp,2021-10-12 20:27:16+00:00,,1,10,high priority triage review oncall: distributed better-engineering module: ddp,True
66491,Normalize handling of scalar arguments feature module: tests triaged better-engineering,2021-10-12 17:45:44+00:00,,0,4,feature module: tests triaged better-engineering,True
66489,The uniform operator param names in the C++ impl use python keywords triaged module: numpy,2021-10-12 17:10:22+00:00,,0,1,triaged module: numpy,True
66482,BAR1 memory of GPU is not released when main process is killed. module: dataloader module: cuda module: memory usage triaged,2021-10-12 15:30:30+00:00,,0,19,module: dataloader module: cuda module: memory usage triaged,True
66476,"0INTERNAL ASSERT FAILED, We don't have an op for aten::eq but it isn't a special case oncall: jit triaged",2021-10-12 11:24:24+00:00,,0,0,oncall: jit triaged,True
66473,nccl comm init needs a global barrier. module: cuda triaged module: nccl,2021-10-12 06:30:09+00:00,,0,0,module: cuda triaged module: nccl,True
66460,Inconsistencies in LSTM outputs when processing sequence stepwise on CPU module: numerical-stability module: rnn triaged,2021-10-11 23:37:26+00:00,,0,1,module: numerical-stability module: rnn triaged,True
66410,CIFAR10 doesn't work on M1 MacBook  triaged module: arm,2021-10-11 10:24:57+00:00,,0,6,triaged module: arm,True
66408,Feature Request: Source code pages should include link to GitHub file module: docs feature triaged,2021-10-11 08:31:54+00:00,,0,14,module: docs feature triaged,False
66403,[opinfo] Confusing interface for `ops` decorator  module: tests triaged,2021-10-11 06:39:33+00:00,,0,1,module: tests triaged,True
66366,Feature Request: Add constant padding_mode for grid_sample feature module: nn triaged module: padding module: interpolation,2021-10-09 11:50:40+00:00,,0,5,feature module: nn triaged module: padding module: interpolation,True
66364,Hope to obtain authorization for Chinese translation of official tutorials. module: docs triaged,2021-10-09 11:29:08+00:00,,0,1,module: docs triaged,True
66359,question on installing GPU-enabled Pytorch module: binaries module: cuda triaged module: lts,2021-10-09 05:11:56+00:00,,0,1,module: binaries module: cuda triaged module: lts,True
66357,OpInfos disabled for batched forward grad computation module: autograd module: tests triaged module: vmap module: forward ad,2021-10-09 02:25:49+00:00,,0,0,module: autograd module: tests triaged module: vmap module: forward ad,True
66341,Invalid handling of out args with type Tensor[] in native_functions.yaml module: internals triaged module: codegen,2021-10-08 20:43:47+00:00,,0,1,module: internals triaged module: codegen,True
66335,`torch.fx.symbolic_trace()` loses module class information triaged module: fx,2021-10-08 19:02:07+00:00,,0,6,triaged module: fx,True
66305,Modify convolution kernels for ops triaged module: fft,2021-10-08 06:19:20+00:00,,0,2,triaged module: fft,True
66284,Memory leak issue with pytorch_java_only 1.9.0 and libtorch 1.9.0+cpu module: memory usage triaged module: android oncall: mobile oncall: java mobile_perf,2021-10-07 21:25:56+00:00,,0,6,module: memory usage triaged module: android oncall: mobile oncall: java mobile_perf,True
66271,1.10.0-rc1 fails to compile with gcc 6.4.0 module: build triaged,2021-10-07 18:21:36+00:00,,0,7,module: build triaged,True
66251,Should the drop_last parameter of Dataloader be mutually exclusive with the batch samplers? module: dataloader triaged,2021-10-07 14:40:08+00:00,,0,0,module: dataloader triaged,True
66249,Latency issue with torch.sin module: performance triaged,2021-10-07 12:32:34+00:00,,0,7,module: performance triaged,True
66247,Floating point exception in mkl_vml_serv_GetMinN () on a specific computer high priority needs reproduction module: binaries module: crash triaged module: mkl,2021-10-07 12:20:07+00:00,,0,4,high priority needs reproduction module: binaries module: crash triaged module: mkl,True
66224,[feature request][quant] Support FakeQuant qconfigs in `test_module_init` module: tests triaged better-engineering,2021-10-06 20:38:11+00:00,,0,0,module: tests triaged better-engineering,True
66223,Data Loader tests hang when run in ASAN test job module: dataloader module: tests triaged,2021-10-06 20:18:11+00:00,,0,4,module: dataloader module: tests triaged,True
66208,Add LTC-TS support for ops used in SGD optimizer triaged module: lazy,2021-10-06 17:05:01+00:00,,1,0,triaged module: lazy,False
66200,An exported onnx model can't reduce on dim with value of 0 if 'keepdims' is false module: onnx triaged,2021-10-06 14:40:29+00:00,,1,3,module: onnx triaged,False
66199,Move the `queue_callback()` API out of `Variable._execution_engine` and into a public API module: autograd triaged better-engineering actionable,2021-10-06 14:18:30+00:00,,0,0,module: autograd triaged better-engineering actionable,True
66197,`torch.fx.replace_pattern` doesn't work with untraceable wrapped functions triaged module: fx,2021-10-06 12:12:49+00:00,,0,1,triaged module: fx,True
66196,Specifying USE_VULKAN=0 in launching build_android.sh does not disable Vulkan module: build triaged module: vulkan,2021-10-06 12:04:51+00:00,,0,1,module: build triaged module: vulkan,True
66162,Cuda Out of Memory - MMDMatic module: memory usage triaged,2021-10-05 20:37:13+00:00,,0,3,module: memory usage triaged,True
66150,JIT: empty_like doesn't support requires_grad oncall: jit,2021-10-05 18:04:58+00:00,,0,0,oncall: jit,False
66116,Removal of BufferedShuffleDataset module: docs triaged module: data,2021-10-05 09:11:40+00:00,,0,0,module: docs triaged module: data,True
66094,[JIT][Sym Shape Analysis] Extend analysis to extract output shape logic oncall: jit module: bootcamp,2021-10-04 22:29:34+00:00,,1,0,oncall: jit module: bootcamp,True
66083,[feature request] Allow broadcasting for F.huber_loss and potentially other losses feature module: loss triaged module: shape checking,2021-10-04 15:59:50+00:00,,0,5,feature module: loss triaged module: shape checking,True
66076,Bad Input to torch.nn.NLLLoss causes CUDA Error module: loss module: cuda triaged,2021-10-04 12:59:20+00:00,,0,2,module: loss module: cuda triaged,True
66073,[feature request] BatchNorm frozen mode in core module: nn triaged enhancement module: norms and normalization,2021-10-04 12:08:14+00:00,,0,5,module: nn triaged enhancement module: norms and normalization,True
66066,Finishing OpInfos: test_autograd.py module: autograd module: tests triaged better-engineering hackathon,2021-10-04 08:13:14+00:00,,0,2,module: autograd module: tests triaged better-engineering hackathon,True
66065,torch.jit.script fails to cast explicit Optional parameter to bool oncall: jit,2021-10-04 06:35:51+00:00,,0,0,oncall: jit,True
66046,PicklingError when saving a ddp module with torch.save() oncall: distributed,2021-10-02 06:10:12+00:00,,0,2,oncall: distributed,False
66042,[LTC] Investigate why max_pool2d_with_indices needs explicit eager fallback triaged lazy module: lazy,2021-10-02 04:21:22+00:00,,0,0,triaged lazy module: lazy,True
66033,Crashed with `terminating with uncaught exception of type std::__1::system_error: condition_variable wait failed: Invalid argument` needs reproduction module: binaries module: crash triaged module: macos module: multithreading,2021-10-01 22:11:03+00:00,,0,5,needs reproduction module: binaries module: crash triaged module: macos module: multithreading,True
66026,`__torch_dispatch__` can result in returning `None` for an op that should return Tensors. triaged module: pybind module: __torch_dispatch__,2021-10-01 21:25:13+00:00,,0,3,triaged module: pybind module: __torch_dispatch__,True
66017,Torchscript `jit.script` breaks with OSError if dataclass used to pass results during model execution oncall: jit,2021-10-01 18:53:32+00:00,,0,0,oncall: jit,True
65985,torch.cuda.power_usage (ESG 🌱) module: cuda oncall: profiler,2021-10-01 06:32:34+00:00,,0,0,module: cuda oncall: profiler,True
65984,libtorch: collate_fn equivalent module: cpp module: dataloader triaged enhancement,2021-10-01 06:14:18+00:00,,0,3,module: cpp module: dataloader triaged enhancement,True
65983,"traced EmbeddingBag cannot be used with different batch size, functional.embedding_bag is not de oncall: jit",2021-10-01 06:10:47+00:00,,0,2,oncall: jit,False
65950,"Storage `is` should work if the storages ""are the same"" feature triaged",2021-09-30 20:49:15+00:00,,1,2,feature triaged,False
65949,torch.unique signature is not descriptive module: docs triaged,2021-09-30 20:47:07+00:00,,0,1,module: docs triaged,True
65936,[POLL][RFC] DataParallel Deprecation triaged module: deprecation,2021-09-30 17:38:39+00:00,,0,19,triaged module: deprecation,True
65918,C++ extensions don't correctly set USE_C10D_FOO preprocessor macros oncall: distributed,2021-09-30 12:25:45+00:00,,0,1,oncall: distributed,False
65915,"I am getting undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE error. This I am getting when I am trying to ""import torch from nemo.collections import nlp"". I am trying to use pytorch ngc container 21.05. I tried to import torch before the nemo extension. Please suggest how I can resolve this. oncall: jit",2021-09-30 12:08:43+00:00,,0,1,oncall: jit,False
65913,Support differentiability through clone and update feature module: autograd module: nn triaged needs design,2021-09-30 10:57:26+00:00,,0,2,feature module: autograd module: nn triaged needs design,True
65910,Deprecation: Remove nn.functional.sigmoid  module: nn triaged module: deprecation,2021-09-30 08:54:48+00:00,,0,1,module: nn triaged module: deprecation,True
65907,Inplace error of BatchNorm layer in DistributedDataParallel module oncall: distributed,2021-09-30 08:28:19+00:00,,0,3,oncall: distributed,True
65890,`pad_packed_sequence` is not exactly the inverse of `pack_padded_sequence` module: rnn triaged,2021-09-30 07:15:30+00:00,,0,0,module: rnn triaged,True
65881,torch.hub.load can confuse external python package with local python package.  triaged module: hub,2021-09-30 06:03:53+00:00,,0,1,triaged module: hub,True
65868,torch.mul is not consistent with torch.multiply module: numerical-stability triaged module: numpy,2021-09-30 00:29:04+00:00,,0,1,module: numerical-stability triaged module: numpy,True
65837,NVFuser Examples to Help with Integration with Lazy Tensor Core triaged LazyTensor_nvfuser_integration module: lazy,2021-09-29 19:00:07+00:00,,1,11,triaged LazyTensor_nvfuser_integration module: lazy,True
65825,Incorrect reference to previous kernel in the warning displayed when overriding a previously registered kernel for the same operator and the same dispatch triaged module: dispatch,2021-09-29 16:38:45+00:00,,0,2,triaged module: dispatch,True
65824,Failing to compile /torch/csrc/jit/frontend/ir_emitter.cpp with pointer is null warning oncall: jit,2021-09-29 16:05:37+00:00,,0,1,oncall: jit,True
65815,Overflow error in torch.linalg uncaught high priority module: numerical-stability triaged module: multithreading module: linear algebra module: correctness (silent) module: reductions,2021-09-29 12:49:03+00:00,,0,10,high priority module: numerical-stability triaged module: multithreading module: linear algebra module: correctness (silent) module: reductions,True
65814,INTERNAL ASSERT FAILED when reusing JIT compiled module oncall: jit,2021-09-29 11:52:06+00:00,,0,0,oncall: jit,True
65813,[RFC] Low-level speed optimizations for PowerSGD oncall: distributed enhancement module: ddp,2021-09-29 11:46:45+00:00,,0,8,oncall: distributed enhancement module: ddp,False
65811,"Beta distribution yields wrong results for a,b >1 and x=0 or x=1 module: distributions triaged",2021-09-29 08:47:04+00:00,,0,0,module: distributions triaged,True
65788,Autocast sometimes discards changes to mutable arguments triaged module: amp (automated mixed precision) module: intel,2021-09-28 22:52:37+00:00,,0,2,triaged module: amp (automated mixed precision) module: intel,True
65776,`torch.jit.save` fails on Windows oncall: jit,2021-09-28 20:42:03+00:00,,0,0,oncall: jit,True
65774,Non-persistent modules module: nn low priority module: serialization triaged enhancement,2021-09-28 20:31:45+00:00,,0,2,module: nn low priority module: serialization triaged enhancement,True
65766,Temporarily disabling grad doesn't work if AMP is enabled triaged module: amp (automated mixed precision),2021-09-28 19:20:47+00:00,,0,9,triaged module: amp (automated mixed precision),True
65761,torch.utils._pytree -> stable high priority feature triaged better-engineering needs design,2021-09-28 19:15:51+00:00,,0,13,high priority feature triaged better-engineering needs design,True
65760,Codegen issues with Tensor(a!)? arguments in native_functions.yaml schemas triaged module: codegen,2021-09-28 18:34:10+00:00,,0,1,triaged module: codegen,True
65753,[POC] Convolution-relu fusion via torch function and one-step lazy evaluation triaged module: __torch_function__ module: lazy,2021-09-28 17:30:19+00:00,,0,1,triaged module: __torch_function__ module: lazy,True
65752,Deprecate torch.cross with optional dim triaged module: deprecation,2021-09-28 17:08:14+00:00,,0,2,triaged module: deprecation,True
65739,Sobol state API module: docs triaged module: random,2021-09-28 12:26:27+00:00,,0,1,module: docs triaged module: random,True
65734,pytorch mobile: some quant models on android cause memory leak module: android oncall: mobile,2021-09-28 05:53:19+00:00,,0,1,module: android oncall: mobile,False
65718,Migrate current Windows CI scripts off of batch module: windows module: ci triaged better-engineering,2021-09-27 22:05:18+00:00,,1,6,module: windows module: ci triaged better-engineering,True
65702,Indicate support for more general use of `pos_weight` in binary cross entropy module: docs module: nn triaged actionable,2021-09-27 17:25:30+00:00,,0,1,module: docs module: nn triaged actionable,True
65698,torch.onnx.export error  module: onnx triaged onnx-needs-info,2021-09-27 16:43:13+00:00,,1,3,module: onnx triaged onnx-needs-info,True
65694,Plain Tensor serialization does not save the content of `__dict__`. module: serialization triaged,2021-09-27 15:12:30+00:00,,0,1,module: serialization triaged,True
65693,Add at least one config running GPU testing on sm_80+ cards module: ci triaged enhancement,2021-09-27 14:47:01+00:00,,1,2,module: ci triaged enhancement,True
65688,Parametrization silently disables RNN weight updates  module: nn module: rnn triaged,2021-09-27 11:41:57+00:00,,0,4,module: nn module: rnn triaged,True
65687,"repeat_interleave changes memory format from ""channels last"" to ""contiguous"" triaged module: memory format",2021-09-27 11:33:52+00:00,,0,0,triaged module: memory format,True
65683,`torch.nn.functional.l1_loss` fails gradgradcheck for complex inputs module: autograd module: nn triaged module: complex,2021-09-27 09:45:27+00:00,,0,0,module: autograd module: nn triaged module: complex,True
65673,[libtorch]incorrect sigmoid result on arm chip(rk3326) triaged module: arm,2021-09-27 02:28:17+00:00,,0,1,triaged module: arm,True
65670,Memory Leak in Distributed RPC nightly oncall: distributed triaged,2021-09-26 19:43:24+00:00,,0,3,oncall: distributed triaged,True
65665,Pipe method for torch tensors module: dataloader triaged,2021-09-26 07:00:58+00:00,,0,2,module: dataloader triaged,True
65664,C10dProcessGroupSerialization.test_process_group_as_module_member is flaky oncall: distributed triaged,2021-09-26 06:59:18+00:00,,0,0,oncall: distributed triaged,True
65663,Model Evaluation Returns Nan Values Sometimes for the Same Input triaged module: NaNs and Infs,2021-09-26 06:29:48+00:00,,0,3,triaged module: NaNs and Infs,True
65662,mobile deployment: optimized_for_mobile---wrap_cpp_module error oncall: mobile mobile_perf,2021-09-26 03:19:14+00:00,,0,2,oncall: mobile mobile_perf,True
65661,[BUG] ConvTranspose with out_channels=1  module: convolution triaged,2021-09-26 00:34:47+00:00,,0,2,module: convolution triaged,True
65660,[JIT] allow annotation with `nn.Module` for documentation purposes. oncall: jit,2021-09-25 20:38:55+00:00,,0,0,oncall: jit,False
65657,Transpose channels when exporting to ONNX module: onnx triaged,2021-09-25 16:05:47+00:00,,1,2,module: onnx triaged,False
65653,[JIT] torch.load(model) fails for Unicode Variable Names. oncall: jit,2021-09-25 10:43:30+00:00,,0,2,oncall: jit,True
65650,Does ZeroRedundancyOptimizer support re-split partition on load_state_dict? oncall: distributed triaged,2021-09-25 04:49:19+00:00,,1,0,oncall: distributed triaged,True
65646,[LTC] Building unit tests on Debian results in double defined _GLIBCXX_USE_CXX11_ABI flag triaged module: lazy,2021-09-25 01:32:11+00:00,,0,4,triaged module: lazy,True
65643,[JIT] Start Using and Optimizing on Symbolic Shape Inference in Freezing oncall: jit,2021-09-24 23:40:22+00:00,,1,2,oncall: jit,False
65640,[LTC] Memory growing up during training until OOM triaged lazy module: lazy,2021-09-24 22:39:27+00:00,,0,19,triaged lazy module: lazy,True
65637,Test ZeRO on gloo backend for GPU oncall: distributed triaged pt_distributed_rampup module: ddp,2021-09-24 22:17:55+00:00,,0,1,oncall: distributed triaged pt_distributed_rampup module: ddp,True
65630,Constants in torch.Jit.script oncall: jit,2021-09-24 20:50:20+00:00,,0,0,oncall: jit,False
65628,Add torch.distributed.shard package oncall: distributed triaged sharded_tensor,2021-09-24 19:56:56+00:00,,0,0,oncall: distributed triaged sharded_tensor,False
65622,Transformer modules should not hard-code the activation function module: nn triaged,2021-09-24 18:48:27+00:00,,0,8,module: nn triaged,True
65619,Integration of __torch_dispatch__ with masked tensor semantics triaged module: __torch_dispatch__,2021-09-24 18:12:41+00:00,,0,10,triaged module: __torch_dispatch__,True
65617,Derivative not implemented for narrow_copy module: autograd triaged actionable,2021-09-24 16:44:24+00:00,,1,4,module: autograd triaged actionable,True
65611,"`torch.jit.trace` fails claiming ""forward method already defined"" oncall: jit",2021-09-24 14:01:41+00:00,,0,5,oncall: jit,True
65607,torch.dtype is int in JIT instead of torch.dtype oncall: jit,2021-09-24 09:10:55+00:00,,0,7,oncall: jit,False
65604,pytorch serializes entire tensor when you try to pickle a slice module: serialization triaged,2021-09-24 06:24:08+00:00,,0,2,module: serialization triaged,True
65588,Make torch.utils.tensorboard.add_scalar accepts float global_step  oncall: visualization,2021-09-23 23:38:08+00:00,,0,0,oncall: visualization,False
65577,Support cat() for meta tensors triaged enhancement module: meta tensors,2021-09-23 20:47:28+00:00,,0,0,triaged enhancement module: meta tensors,True
65576,[LTC] Code-gen IRs and TorchScript lowerings needed by running full TorchBench oncall: jit triaged module: lazy,2021-09-23 20:20:43+00:00,,1,12,oncall: jit triaged module: lazy,True
65572,PyTorch setup.py should confirm with user if the user changes build flags module: build triaged,2021-09-23 20:09:30+00:00,,0,2,module: build triaged,True
65569,Support 'meta' device for 'shard_parameter' API oncall: distributed triaged sharded_tensor,2021-09-23 19:56:51+00:00,,0,0,oncall: distributed triaged sharded_tensor,False
65559,TestMultiprocessing.test_fs_sharing is flaky high priority triage review oncall: distributed module: tests triaged module: flaky-tests,2021-09-23 18:22:49+00:00,,0,1,high priority triage review oncall: distributed module: tests triaged module: flaky-tests,True
65536,Pass backward flags such as retain_graph to context of custom torch.autograd.Function module: autograd triaged,2021-09-23 14:53:38+00:00,,0,1,module: autograd triaged,True
65532,TorchScript can't handle super() in subclass of nn.Sequential oncall: jit,2021-09-23 13:40:39+00:00,,0,1,oncall: jit,False
65528,I cannot use x.to(GPU) or x.cuda(GPU) module: cuda triaged module: cuda graphs,2021-09-23 11:05:50+00:00,,0,10,module: cuda triaged module: cuda graphs,True
65526,[docs] Better explain signature of toch.randn module: docs triaged,2021-09-23 10:19:50+00:00,,0,1,module: docs triaged,False
65522,"pytorch framework tests using make_tensor hangs with pytest's boxed exec option ""--forked"" module: tests triaged",2021-09-23 07:38:46+00:00,,0,9,module: tests triaged,True
65521,DISABLED test_profiler (__main__.TestJit) high priority triage review oncall: jit module: tests skipped oncall: profiler,2021-09-23 07:13:01+00:00,,0,10,high priority triage review oncall: jit module: tests skipped oncall: profiler,False
65520,Improve input checking for running_var of nn.functional.batch_norm on CPU/GPU module: nn triaged,2021-09-23 05:41:38+00:00,,0,0,module: nn triaged,True
65512, Performance problems of eigh operator on CPU module: performance triaged,2021-09-23 02:07:25+00:00,,0,0,module: performance triaged,True
65510,nn.conv1d padding='same' module: docs module: convolution triaged,2021-09-23 01:48:29+00:00,,0,1,module: docs module: convolution triaged,True
65494,Feature request: add bool dtype support to CPU abs() triaged enhancement,2021-09-22 20:58:27+00:00,,0,2,triaged enhancement,False
65490,optimize_for_mobile() assert fail: missing op for prepacked::conv2d_clamp_prepack oncall: mobile mobile_perf,2021-09-22 19:20:49+00:00,,0,3,oncall: mobile mobile_perf,False
65475,building pytorch from source without conda module: build triaged,2021-09-22 17:00:16+00:00,,0,2,module: build triaged,True
65473,API torch.ops.image.read_file reports RuntimeError - No such operator image::read_file high priority module: internals triaged module: vision module: custom-operators,2021-09-22 15:29:05+00:00,,0,17,high priority module: internals triaged module: vision module: custom-operators,True
65465,[LTC] Fail to run testcase of latest lazy_tensor_core branch triaged lazy module: lazy,2021-09-22 13:40:12+00:00,,1,2,triaged lazy module: lazy,True
65464,linking error when building on Linux module: build module: cuda triaged,2021-09-22 13:39:11+00:00,,0,3,module: build module: cuda triaged,True
65462,"Why here save `store`, rather than `prefix_store`? oncall: distributed triaged",2021-09-22 13:19:29+00:00,,1,1,oncall: distributed triaged,True
65449,Make SummaryWriter.add_image supports PIL image directly  oncall: visualization,2021-09-22 06:02:38+00:00,,0,1,oncall: visualization,False
65448,Magma : Intel MKL Errors triaged module: mkldnn module: intel module: magma,2021-09-22 04:44:45+00:00,,0,2,triaged module: mkldnn module: intel module: magma,True
65447,No support for torch.trace on CPU for float16 tensors triaged enhancement module: half,2021-09-22 04:44:41+00:00,,0,0,triaged enhancement module: half,True
65446,libtorch compile problem. How to get the correct protobuf version? what PROTOBUF_VERSION <3011000 and 3011004 <PROTOBUF_MIN_PROTOC_VERSION? module: build module: protobuf triaged,2021-09-22 04:28:27+00:00,,0,1,module: build module: protobuf triaged,True
65436,Make DDP uneven inputs work with custom buffer reduction oncall: distributed triaged pt_distributed_rampup module: ddp,2021-09-21 23:37:51+00:00,,0,0,oncall: distributed triaged pt_distributed_rampup module: ddp,True
65428,module 'torch.distributed' has no attribute 'optim' oncall: distributed triaged,2021-09-21 21:43:34+00:00,,0,4,oncall: distributed triaged,True
65426,Add a `__torch_pre_dispatch__` that will live at the top of the dispatcher feature triaged module: dispatch,2021-09-21 20:23:01+00:00,,0,2,feature triaged module: dispatch,True
65423,Enable is_contiguous slowpath for torch_dispatch triaged module: __torch_dispatch__,2021-09-21 18:50:41+00:00,,0,7,triaged module: __torch_dispatch__,True
65418,Sending tensors from short-lived multiprocessing process fails on Linux module: multiprocessing triaged,2021-09-21 17:22:19+00:00,,0,0,module: multiprocessing triaged,True
65400,torch.sparse.sum on scalar sparse tensor fails when dim is specified module: sparse triaged,2021-09-21 12:23:02+00:00,,0,0,module: sparse triaged,True
65398,[JIT] JIT does not support non-persistent buffers oncall: jit,2021-09-21 11:56:00+00:00,,0,0,oncall: jit,True
65397,Vague linkage when using libcxxabi built torch in torch_glow oncall: jit,2021-09-21 11:45:18+00:00,,0,2,oncall: jit,False
65393,PyTorch Profiler is not working with CUDA needs reproduction module: windows oncall: profiler,2021-09-21 07:40:49+00:00,,0,7,needs reproduction module: windows oncall: profiler,False
65392,torch.sparse.sum result has wrong dtype when reducing over all dimensions high priority module: sparse triaged needs design module: correctness (silent),2021-09-21 07:36:12+00:00,,0,11,high priority module: sparse triaged needs design module: correctness (silent),True
65386,[LTC] Setup a subset of TorchBench for the current implementation triaged module: lazy,2021-09-21 01:41:55+00:00,,1,4,triaged module: lazy,True
65375,Make python_error use c10/util/Exception.h C++ backtrace machinery module: error checking triaged better-engineering,2021-09-20 21:47:59+00:00,,0,0,module: error checking triaged better-engineering,True
65366,Exception thrown in final autograd callback (queue_callback) not caught if not on CPU thread: terminate called after throwing an instance of 'python_error' module: crash module: bootcamp module: autograd triaged,2021-09-20 20:05:06+00:00,,0,7,module: crash module: bootcamp module: autograd triaged,True
65357,Improve DDP tutorial high priority triage review oncall: distributed module: docs triaged better-engineering module: ddp,2021-09-20 18:19:51+00:00,,1,1,high priority triage review oncall: distributed module: docs triaged better-engineering module: ddp,True
65342,Unexpected behaviour when resuming from checkpoint using CosineAnnealingLR module: docs module: optimizer triaged module: LrScheduler,2021-09-20 15:43:40+00:00,,0,4,module: docs module: optimizer triaged module: LrScheduler,True
65339,Make it easier to accurately reflect storage/view relationships in meta/wrapper tensors triaged module: meta tensors module: __torch_dispatch__,2021-09-20 15:12:48+00:00,,0,6,triaged module: meta tensors module: __torch_dispatch__,True
65329,How can I re-weight a sample based on both class weights and instance weights? module: loss triaged enhancement,2021-09-20 12:49:37+00:00,,0,1,module: loss triaged enhancement,True
65325,ChannelShuffle: Missing CUDA implementation module: cuda triaged enhancement,2021-09-20 10:40:14+00:00,,0,2,module: cuda triaged enhancement,True
65313,openSUSE Build: CMake Generate step failed.  Build files cannot be regenerated correctly. module: build triaged,2021-09-19 15:31:00+00:00,,0,0,module: build triaged,True
65307,[numpy] Add torch.newdim/torch.newaxis feature triaged module: numpy,2021-09-19 05:37:25+00:00,,0,0,feature triaged module: numpy,True
65306,[numpy] Add `iscomplexobj` and `isrealobj` feature triaged module: complex module: numpy,2021-09-19 05:23:41+00:00,,1,6,feature triaged module: complex module: numpy,True
65305,Add support for HF NLP models in torch.package/deploy oncall: package/deploy imported,2021-09-18 22:46:51+00:00,,0,1,oncall: package/deploy imported,False
65301,Anomaly detection: Error detected in CudnnRnnBackward0 module: cudnn module: rnn module: cuda triaged,2021-09-18 17:47:55+00:00,,0,18,module: cudnn module: rnn module: cuda triaged,True
65297,[JIT] TorchScript not works as expected when ModuleDict has modules with different signatures oncall: jit,2021-09-18 10:44:38+00:00,,0,0,oncall: jit,True
65261,Add an `all_paths` method to PackageExporter triaged oncall: package/deploy imported,2021-09-18 00:02:16+00:00,,1,0,triaged oncall: package/deploy imported,True
65253,TestCppExtensionJit's test_crash_handler fails for force_on_cpu job on GHA oncall: jit triaged,2021-09-17 22:40:36+00:00,,1,2,oncall: jit triaged,True
65230,RuntimeError: Trying to create tensor with negative dimension -1741885395: [-1741885395] module: sparse triaged,2021-09-17 17:03:19+00:00,,0,1,module: sparse triaged,True
65229,at::parallel_for created max_threads for inputs larger than GRAIN_SIZE module: performance triaged module: openmp module: multithreading,2021-09-17 17:01:29+00:00,,0,1,module: performance triaged module: openmp module: multithreading,True
65218,Indexing a tensor with a NumPy array sometimes works and sometimes doesn't triaged module: numpy module: advanced indexing,2021-09-17 14:52:16+00:00,,0,1,triaged module: numpy module: advanced indexing,True
65216,Add function that retrieves a batch from a DataLoader module: dataloader triaged enhancement,2021-09-17 14:48:27+00:00,,0,6,module: dataloader triaged enhancement,True
65215,_disabled_torch_function_impl should not be necessary for __torch_dispatch__ triaged module: __torch_function__,2021-09-17 14:40:06+00:00,,0,2,triaged module: __torch_function__,True
65198,Multi-processing leaking file descriptors module: multiprocessing module: dataloader triaged,2021-09-17 05:55:19+00:00,,0,12,module: multiprocessing module: dataloader triaged,True
65193,[Better Engineering] Generalize DDPSink and consolidate it with FSDP's pre-backward hooks triaged better-engineering module: fsdp,2021-09-17 03:08:21+00:00,,0,0,triaged better-engineering module: fsdp,True
65166,Review the Tensor and Operator Basics Onboarding page module: docs triaged,2021-09-16 20:52:55+00:00,,1,0,module: docs triaged,True
65163,Parametrization cannot be parametrized module: nn triaged,2021-09-16 18:34:45+00:00,,0,1,module: nn triaged,True
65161,error: ‘_mm512_set_epi8’ was not declared in this scope when build from source on AMD 3600 CPU module: build triaged module: vectorization,2021-09-16 18:28:22+00:00,,0,3,module: build triaged module: vectorization,True
65156,[feature request] Jagged / padding version of torch.stack / torch.cat + some general nested tensor discussion module: sparse feature triaged module: nestedtensor module: padding,2021-09-16 17:42:40+00:00,,0,33,module: sparse feature triaged module: nestedtensor module: padding,True
65153,Unification of model initialization methods / naming across domain libraries + support of skip_init triaged enhancement module: initialization,2021-09-16 17:19:34+00:00,,0,11,triaged enhancement module: initialization,True
65151,Registering a global fallback for all operators that defaults us to assuming that autograd is not implemented module: bc-breaking feature module: autograd module: molly-guard triaged better-engineering topic: bc breaking,2021-09-16 16:41:15+00:00,,0,0,module: bc-breaking feature module: autograd module: molly-guard triaged better-engineering topic: bc breaking,True
65144,jit.trace's returned module returns a tuple instead of a named tuple oncall: jit,2021-09-16 14:54:17+00:00,,0,0,oncall: jit,False
65140,profiler_legacy `_parse_legacy_records` deduplication logic has false-positive. triaged oncall: profiler,2021-09-16 11:04:39+00:00,,0,0,triaged oncall: profiler,True
65132,How to reference a tensor variable from a superclass of `torch.Tensor`? triaged module: xla,2021-09-16 07:31:05+00:00,,0,1,triaged module: xla,True
65131,Adding a device variable to a `torch.Tensor` subclass fails triaged module: xla,2021-09-16 07:27:35+00:00,,0,2,triaged module: xla,True
65117,[Comment] remove torch.cuda.synchronize() in state_dict() triaged better-engineering module: fsdp,2021-09-16 00:25:24+00:00,,0,0,triaged better-engineering module: fsdp,True
65100,[Bug] [CUDA IPC] CUDA IPC memory cost module: performance module: multiprocessing module: cuda triaged,2021-09-15 21:36:14+00:00,,0,2,module: performance module: multiprocessing module: cuda triaged,True
65098,Bug: torch.jit.load cannot load from IOBuffer oncall: jit,2021-09-15 21:34:12+00:00,,0,1,oncall: jit,True
65096,[FX] `passes.split_module` doesn't have documentation triaged module: fx,2021-09-15 20:48:47+00:00,,0,0,triaged module: fx,True
65083,THPVariable_subclass_dealloc (i.e. tp_dealloc for torch.Tensor) should handle Python error state gracefully high priority module: internals triaged module: correctness (silent),2021-09-15 18:44:22+00:00,,0,3,high priority module: internals triaged module: correctness (silent),True
65069,torch.linalg.cholesky does not raise a RuntimeError when the matrix is not symmetric module: docs triaged module: linear algebra,2021-09-15 16:13:26+00:00,,0,1,module: docs triaged module: linear algebra,True
65067,[DataPipe] Add Test Cases to Ensure Correct Behaviors When IterDataPipes Reset triaged module: data,2021-09-15 16:07:47+00:00,,0,2,triaged module: data,True
65062,Gloo and TensorPipe depend on different version of libuv oncall: distributed module: build triaged module: third_party module: tensorpipe,2021-09-15 15:06:49+00:00,,0,2,oncall: distributed module: build triaged module: third_party module: tensorpipe,True
65058,Add nnapi serialization for module components in Wav2Vec2Model module: android oncall: mobile,2021-09-15 14:36:41+00:00,,0,3,module: android oncall: mobile,False
65052,Use hyphens to separate long CLI parameters instead of underscores oncall: distributed better-engineering,2021-09-15 12:50:05+00:00,,0,1,oncall: distributed better-engineering,False
65051,deepcopy of Lazy modules returns an exception module: nn triaged actionable module: __torch_function__ module: lazy,2021-09-15 12:09:53+00:00,,0,3,module: nn triaged actionable module: __torch_function__ module: lazy,True
65050,torch.cross precision problem module: numerical-stability triaged,2021-09-15 09:11:11+00:00,,0,0,module: numerical-stability triaged,True
65049,Setting a random seed or passing a generator to Modules for deterministic sampling. module: nn triaged enhancement module: random module: determinism,2021-09-15 08:43:21+00:00,,0,12,module: nn triaged enhancement module: random module: determinism,True
65038,"[pytorch v1.9.0] for torch sync.pipeline in a single GPU, passing tensor layer by layer is better or using the skip-connection API is better? oncall: distributed triaged pipeline parallelism",2021-09-15 03:40:06+00:00,,0,7,oncall: distributed triaged pipeline parallelism,True
65027,CUDA out of memory needs reproduction module: cuda module: memory usage triaged,2021-09-14 23:06:17+00:00,,0,2,needs reproduction module: cuda module: memory usage triaged,True
65022,"[AWS EC2 P3DN, EFA is enabled] Torch RPC tensorpipe/common/ibv.h:172 """": Operation not supported oncall: distributed triaged module: rpc module: tensorpipe pipeline parallelism",2021-09-14 21:56:32+00:00,,0,12,oncall: distributed triaged module: rpc module: tensorpipe pipeline parallelism,True
65002,Traceback tensor mode feature triaged better-engineering,2021-09-14 17:15:25+00:00,,0,0,feature triaged better-engineering,True
64997,FX failed to normalize op `block_diag` and `broadcast_tensors` triaged module: fx,2021-09-14 16:16:15+00:00,,0,1,triaged module: fx,True
64986,RandomSampler / DistributedSampler does not seem really random oncall: distributed module: dataloader,2021-09-14 12:19:33+00:00,,0,5,oncall: distributed module: dataloader,False
64957,`INTERNAL ASSERT FAILED` after `torch.jit.freeze` oncall: jit,2021-09-13 22:22:57+00:00,,0,1,oncall: jit,True
64947,Quantile is limited to 16 million elements and have poor performance. module: performance triaged module: numpy module: sorting and selection,2021-09-13 19:14:20+00:00,,0,5,module: performance triaged module: numpy module: sorting and selection,True
64939,BC CI error message should link to some information about how to squash the warning module: ci triaged better-engineering,2021-09-13 17:33:53+00:00,,0,0,module: ci triaged better-engineering,True
64936,[proposal] Let's unify the various gather/scatter ops. triaged module: advanced indexing better-engineering module: scatter & gather ops,2021-09-13 17:04:54+00:00,,0,5,triaged module: advanced indexing better-engineering module: scatter & gather ops,True
64932,[RFC] TorchStore - A Shared-Memory Tensor Store feature triaged,2021-09-13 15:37:29+00:00,,1,44,feature triaged,False
64931,native functions should not be allowed to take in a `grad` argument module: autograd triaged actionable module: codegen,2021-09-13 15:29:38+00:00,,0,2,module: autograd triaged actionable module: codegen,True
64918,TorchScript error loading module with float literal default argument oncall: jit,2021-09-13 10:31:07+00:00,,0,0,oncall: jit,True
64905,[docs] [feature request] Tutorial on implementing correctly distributed dataset evaluators oncall: distributed,2021-09-13 07:42:59+00:00,,0,2,oncall: distributed,False
64897,Either support torch.mean on BoolTensors or fix the error message module: error checking triaged module: boolean tensor module: reductions,2021-09-12 15:50:05+00:00,,0,6,module: error checking triaged module: boolean tensor module: reductions,True
64896,pytorch1.9.0 can not use torch.jit.load() to load model which save in pytorch1.3 oncall: jit,2021-09-12 14:44:47+00:00,,0,2,oncall: jit,True
64889,"""RuntimeError: CUDA error: out of memory"" for no reason  needs reproduction module: cuda module: memory usage triaged",2021-09-12 06:11:15+00:00,,0,2,needs reproduction module: cuda module: memory usage triaged,True
64881,Add support for GPU based data loading and transformation module: dataloader triaged,2021-09-11 08:06:11+00:00,,0,1,module: dataloader triaged,True
64874,I am unable to detect torch.Tensor._version change during __torch_dispatch__ call triaged module: __torch_dispatch__,2021-09-11 02:25:54+00:00,,0,8,triaged module: __torch_dispatch__,True
64868,CMake outputs lots of warnings in CI module: ci triaged,2021-09-11 00:14:44+00:00,,0,0,module: ci triaged,True
64864,Base parallelism on CPU count available to process rather than system total triaged module: multithreading,2021-09-10 23:24:59+00:00,,0,1,triaged module: multithreading,True
64845,Track Reliability of Test Reporting and Uploading triaged,2021-09-10 21:07:57+00:00,,0,2,triaged,True
64818,Inconsistent NaN handling by cholesky between CPU and CUDA triaged module: NaNs and Infs module: linear algebra,2021-09-10 15:22:34+00:00,,0,1,triaged module: NaNs and Infs module: linear algebra,True
64812,PyTorch inference on tensors of a particular size cause Illegal Instruction (core dumped) on Jetson Nano triaged module: arm module: jetson,2021-09-10 13:06:55+00:00,,0,1,triaged module: arm module: jetson,True
64804,RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation needs reproduction module: nn triaged,2021-09-10 07:35:45+00:00,,0,1,needs reproduction module: nn triaged,True
64799,Add support to profile all RPC worker threads that are run on a process oncall: distributed,2021-09-10 03:54:16+00:00,,0,5,oncall: distributed,False
64795,profiler crashes the program when trying to stop on different threads without context manager oncall: distributed module: multithreading oncall: profiler,2021-09-10 02:38:21+00:00,,0,5,oncall: distributed module: multithreading oncall: profiler,False
64794,linalg.lstsq out variant fails internal assert because it uses non-inplace view op for some inputs high priority module: ci triaged module: linear algebra,2021-09-10 02:21:26+00:00,,0,2,high priority module: ci triaged module: linear algebra,True
64775,[JIT] Canonicalize aten::rsub  oncall: jit,2021-09-09 20:03:09+00:00,,0,0,oncall: jit,True
64772,[Efficiency] No shard + CPU offload in FSDP  oncall: distributed triaged module: fsdp,2021-09-09 19:04:39+00:00,,1,0,oncall: distributed triaged module: fsdp,True
64766,Ability to explicitly close/dispose a DataLoader feature module: dataloader module: memory usage triaged module: data,2021-09-09 18:39:29+00:00,,0,19,feature module: dataloader module: memory usage triaged module: data,True
64745,Consolidate ProcessGroup allgather_coalesced and allgather oncall: distributed module: bootcamp triaged better-engineering module: c10d,2021-09-09 14:57:08+00:00,,1,1,oncall: distributed module: bootcamp triaged better-engineering module: c10d,True
64744,Consolidate ProcessGroup allreduce_coalesced and allreduce oncall: distributed module: bootcamp triaged better-engineering module: c10d,2021-09-09 14:46:53+00:00,,1,2,oncall: distributed module: bootcamp triaged better-engineering module: c10d,True
64743,Named Tensors to support custom unify functions triaged enhancement module: named tensor,2021-09-09 13:44:57+00:00,,0,2,triaged enhancement module: named tensor,True
64730,Enable BatchNorm to use running mean/variance during train feature module: nn triaged needs research module: norms and normalization,2021-09-09 07:54:18+00:00,,0,7,feature module: nn triaged needs research module: norms and normalization,True
64711,Improve Docs builds to not have to push to a repository to update module: docs triaged enhancement module: doc infra,2021-09-08 22:57:01+00:00,,1,2,module: docs triaged enhancement module: doc infra,False
64709,Add out= resize warning to cat and maybe_native_stack triaged better-engineering module: viewing and reshaping,2021-09-08 22:50:17+00:00,,0,0,triaged better-engineering module: viewing and reshaping,True
64700,Torchscript Variable Length Tuples oncall: jit,2021-09-08 21:25:46+00:00,,0,0,oncall: jit,False
64692,Remove the dependency with CUDA_LIBRARIES in TorchConfig.cmake module: build triaged,2021-09-08 20:14:44+00:00,,0,2,module: build triaged,True
64690,_disabled_torch_function_impl is unimplementable (must be special cased) triaged module: __torch_function__,2021-09-08 20:12:08+00:00,,0,6,triaged module: __torch_function__,True
64688,Backend capability chart for nccl GPU send/recv may be stale oncall: distributed module: nccl,2021-09-08 20:00:39+00:00,,0,3,oncall: distributed module: nccl,False
64684,[Beta] Have a generic `no_sync` for all synchronized training features  oncall: distributed triaged module: fsdp,2021-09-08 19:21:32+00:00,,1,0,oncall: distributed triaged module: fsdp,True
64683,[Tools] Extend generic join to support FSDP oncall: distributed triaged module: fsdp,2021-09-08 19:19:23+00:00,,0,0,oncall: distributed triaged module: fsdp,True
64669,[Tools] Improve FSDP debuggability oncall: distributed triaged module: fsdp,2021-09-08 17:50:25+00:00,,0,0,oncall: distributed triaged module: fsdp,True
64667,[Tools] Add FSDP logging data oncall: distributed triaged module: fsdp,2021-09-08 17:47:44+00:00,,0,1,oncall: distributed triaged module: fsdp,True
64661,[Efficiency] Support optimizer overlap with backward pass in FSDP oncall: distributed triaged module: fsdp,2021-09-08 17:31:45+00:00,,1,1,oncall: distributed triaged module: fsdp,False
64657,[Beta] FSDP initialization redesign oncall: distributed triaged module: fsdp,2021-09-08 17:13:20+00:00,,0,0,oncall: distributed triaged module: fsdp,True
64645,Compilation from source fails module: build module: protobuf module: abi triaged,2021-09-08 12:53:07+00:00,,0,3,module: build module: protobuf module: abi triaged,True
64644,Improve meta tensor testing module: tests triaged module: structured kernels,2021-09-08 12:23:13+00:00,,0,0,module: tests triaged module: structured kernels,True
64637,Evaluating on single GPU (DDP) triaged module: ddp,2021-09-08 08:57:36+00:00,,0,0,triaged module: ddp,True
64636,[docs or feature request] torchelastic: OOM recovery / skipping batches (e.g. if inf loss or nan gradients) module: docs triaged module: ddp oncall: r2p,2021-09-08 08:52:30+00:00,,0,0,module: docs triaged module: ddp oncall: r2p,False
64635,'LSTM' object has no attribute '_flat_weights_names' module: rnn triaged,2021-09-08 08:40:03+00:00,,0,2,module: rnn triaged,True
64628,[jit] cannot compile staticmethod of classes when they have a unscriptable __init__ oncall: jit,2021-09-08 06:40:30+00:00,,0,0,oncall: jit,False
64560,Private bytes consumption exception needs reproduction module: cuda module: memory usage triaged,2021-09-07 02:08:38+00:00,,0,6,needs reproduction module: cuda module: memory usage triaged,True
64548,Interpreter initialized state changed by torch dispatch triaged module: deploy module: __torch_dispatch__,2021-09-06 16:14:14+00:00,,0,1,triaged module: deploy module: __torch_dispatch__,True
64544,[Feature Request] Any plan to add 'Sparse Convolution' as default nn module? module: sparse feature module: convolution triaged,2021-09-06 08:49:58+00:00,,0,3,module: sparse feature module: convolution triaged,True
64541,[docs] Version switcher to not reset home page unconditionally module: docs triaged module: doc infra,2021-09-06 07:36:56+00:00,,0,0,module: docs triaged module: doc infra,False
64539,[LTC][Documentation] Python examples in API_GUIDE.md don't work module: docs triaged lazy module: lazy,2021-09-06 06:11:35+00:00,,0,1,module: docs triaged lazy module: lazy,True
64535,Memory Leak in MKL OpenMP on AVX2 machine module: cpu module: memory usage triaged module: mkldnn module: mkl module: multithreading module: intel,2021-09-06 03:00:39+00:00,,0,7,module: cpu module: memory usage triaged module: mkldnn module: mkl module: multithreading module: intel,True
64532,Replace `clone.detach` with `detach.clone` module: optimizer triaged enhancement actionable,2021-09-06 00:31:54+00:00,,0,3,module: optimizer triaged enhancement actionable,True
64527,"Asking for ""git submodule update"" when building a specific version module: build triaged",2021-09-05 09:32:10+00:00,,0,0,module: build triaged,True
64525,nn.TransformerEncoder - all nan values issues when src_key_padding_mask provided module: NaNs and Infs oncall: transformer/mha,2021-09-05 05:32:07+00:00,,0,11,module: NaNs and Infs oncall: transformer/mha,False
64518,TensorPipeDistAutogradTest is frequently failing high priority triage review oncall: distributed module: tests triaged better-engineering,2021-09-05 02:32:02+00:00,,0,6,high priority triage review oncall: distributed module: tests triaged better-engineering,True
64509,Return attention weights in nn.Transformer feature oncall: transformer/mha,2021-09-04 18:00:59+00:00,,0,0,feature oncall: transformer/mha,False
64502,[numpy] torch.nonzero is similar to np.argwhere not np.nonzero triaged module: numpy,2021-09-04 07:25:54+00:00,,0,5,triaged module: numpy,True
64497,FX tracing on Fairseq-based roberta fails. triaged module: fx,2021-09-03 15:57:12+00:00,,0,0,triaged module: fx,True
64492,AttributeError: Can't pickle local object 'schedule.<locals>.schedule_fn' oncall: profiler,2021-09-03 13:29:08+00:00,,1,4,oncall: profiler,False
64491,TypeError when using torch.cuda.list_gpu_processes() on Windows with the WDDM driver module: cuda triaged,2021-09-03 12:45:51+00:00,,0,0,module: cuda triaged,True
64490,Load torchscript model with custom operators (GNN) on Windows oncall: jit,2021-09-03 12:38:36+00:00,,0,1,oncall: jit,False
64488,It' so strange!!! module: serialization module: android oncall: mobile,2021-09-03 12:06:55+00:00,,0,3,module: serialization module: android oncall: mobile,False
64487,Error in torchScript based on Pytorch tutorial oncall: jit,2021-09-03 11:33:50+00:00,,0,0,oncall: jit,True
64477,torch.onnx.export crash while export quantized model module: onnx triaged,2021-09-03 06:39:31+00:00,,1,3,module: onnx triaged,True
64462,torch.package: re-export docs suggestions don't work + other sharp edges oncall: package/deploy imported,2021-09-02 23:34:02+00:00,,0,2,oncall: package/deploy imported,False
64458,Rename C++ Serialization APIs module: serialization triaged,2021-09-02 22:56:06+00:00,,0,1,module: serialization triaged,True
64426,Add support for ROCm MIOpen miopenConvolutionForwardBias and miopenConvolutionBackwardBias module: rocm triaged,2021-09-02 15:01:53+00:00,,0,1,module: rocm triaged,False
64419,Windows Torch.distributed Multi-GPU training with Gloo backend not working oncall: distributed,2021-09-02 06:35:02+00:00,,0,4,oncall: distributed,False
64412,Memory leak in multi-thread inference high priority module: performance module: memory usage triaged module: multithreading,2021-09-02 01:43:33+00:00,,0,8,high priority module: performance module: memory usage triaged module: multithreading,True
64407,Create a boxed/templated ADInplaceOrView kernel module: autograd triaged,2021-09-01 23:41:12+00:00,,0,2,module: autograd triaged,True
64394,[Prototype][RFC] PyTorch FullyShardedDataParallel(FSDP) API Proposal oncall: distributed triaged module: fsdp,2021-09-01 21:05:45+00:00,,0,2,oncall: distributed triaged module: fsdp,True
64384,[doc] incomplete `get_num_interop_threads` doc module: docs triaged,2021-09-01 17:51:11+00:00,,0,1,module: docs triaged,False
64363,Add support for c++ Profiler APIs module: cpp oncall: profiler,2021-09-01 16:31:28+00:00,,0,0,module: cpp oncall: profiler,False
64359,[feature request] `numpy.append` / `numpy.insrt` / `numpy.delete` equivalents and implement dynamic arrays (reallocate storage with a surplus) feature triaged module: numpy module: viewing and reshaping,2021-09-01 14:08:44+00:00,,0,4,feature triaged module: numpy module: viewing and reshaping,True
64345,Profiler UTF-8 decode issue high priority triage review oncall: profiler,2021-09-01 08:18:04+00:00,,0,1,high priority triage review oncall: profiler,True
64334,How to add nan value judgment for variable t0_1 in fused_clamp kernel generated by torch/csrc/jit/tensorexpr/cuda_codegen.cpp. oncall: jit,2021-09-01 02:35:39+00:00,,0,0,oncall: jit,False
64333,When compiling TAG 1.9.0 or master for android，An error occurred and the compilation failed！ABIS_LIST=arm64-v8a module: android oncall: mobile,2021-09-01 02:26:00+00:00,,0,1,module: android oncall: mobile,True
64327,RFC: multi-part `torch.load`/`torch.save` to support huge models and/or low CPU memory module: nn module: serialization triaged,2021-08-31 23:55:02+00:00,,0,29,module: nn module: serialization triaged,True
64311,issues flutter in android studio triaged module: android oncall: mobile,2021-08-31 21:38:34+00:00,,0,1,triaged module: android oncall: mobile,True
64308,Building from source results in no member named 'cerr' in namespace 'std' module: build triaged,2021-08-31 21:10:20+00:00,,0,2,module: build triaged,True
64306,GradGrad of max_pool2d fails with empty batch dimension module: autograd module: nn triaged,2021-08-31 20:53:54+00:00,,0,1,module: autograd module: nn triaged,True
64292,torch.empty_like not taking None as layout? module: docs triaged module: tensor creation,2021-08-31 18:12:20+00:00,,0,1,module: docs triaged module: tensor creation,True
64284,FR: Record results of OpInfo reference tests and detect when numerics of an operator change feature module: tests triaged,2021-08-31 17:00:32+00:00,,0,0,feature module: tests triaged,True
64260,Using Cloudpickle for pickling jitted functions oncall: jit,2021-08-31 11:33:42+00:00,,0,0,oncall: jit,False
64258,Incubation of Graphical Representation in TF2 of HParams module: tensorboard oncall: visualization,2021-08-31 11:13:26+00:00,,0,0,module: tensorboard oncall: visualization,False
64254,[feature request] New copyseq_ method  feature triaged module: tensor creation,2021-08-31 07:30:19+00:00,,0,1,feature triaged module: tensor creation,True
64247,How to optimize jit-script model performance (backend device is gpu) oncall: jit,2021-08-31 05:31:44+00:00,,0,0,oncall: jit,False
64225,"[clang-tidy] Errors aren't reported, runner fails if no ranges are found module: ci triaged",2021-08-30 21:50:07+00:00,,0,0,module: ci triaged,True
64208,"[feature request] ""Batched"" index_select (i.e. simplified torch.gather with not specifying full index) high priority triaged enhancement module: scatter & gather ops",2021-08-30 18:23:01+00:00,,0,21,high priority triaged enhancement module: scatter & gather ops,True
64206,Document how to generate Pybind bindings for C++ Autograd module: cpp triaged,2021-08-30 18:12:12+00:00,,0,0,module: cpp triaged,True
64187,Uniformly test that defaulted Tensor arguments appropriately handle __torch_function__ module: tests triaged module: __torch_function__,2021-08-30 15:35:50+00:00,,0,2,module: tests triaged module: __torch_function__,True
64186, Complex recurrent layers produce NaN as grad  module: nn triaged module: complex module: NaNs and Infs,2021-08-30 15:28:29+00:00,,0,2,module: nn triaged module: complex module: NaNs and Infs,True
64185,PyTorch public API cleanup high priority triaged better-engineering,2021-08-30 15:26:25+00:00,,0,6,high priority triaged better-engineering,True
64162,[feature request] torch.Generator constructor to accept seed directly triaged module: random,2021-08-30 07:37:28+00:00,,0,1,triaged module: random,True
64156,Get zero when using torch.matmul and torch.dot with 1-D tensor in torch-1.9.0-cp39-none-macosx_11_0_arm64.whl needs reproduction triaged module: correctness (silent) module: arm,2021-08-30 04:47:21+00:00,,0,1,needs reproduction triaged module: correctness (silent) module: arm,True
64150,Enhance DDP doc to have a complete example including data loading oncall: distributed,2021-08-29 22:40:18+00:00,,0,2,oncall: distributed,False
64145,F.conv2d: confusing error message when using uint8 input instead of float32 module: error checking module: convolution triaged enhancement,2021-08-29 19:37:28+00:00,,0,2,module: error checking module: convolution triaged enhancement,True
64143,AttributeError: module 'torchvision.models' has no attribute 'load_model' needs reproduction module: serialization triaged,2021-08-29 18:54:55+00:00,,0,3,needs reproduction module: serialization triaged,True
64138,Longer int and normal Floats conversion to FloatTensor is strange module: docs triaged,2021-08-28 19:20:01+00:00,,0,3,module: docs triaged,True
64133,nn.MultiheadAttention does not belong to activation function module: docs module: nn triaged,2021-08-28 03:44:21+00:00,,0,0,module: docs module: nn triaged,True
64130,Every process takes a tiny memory on each nvidia GPU card when using DDP. oncall: distributed,2021-08-28 02:28:12+00:00,,0,1,oncall: distributed,False
64124,ciflow tracking meta issues triaged,2021-08-27 23:37:43+00:00,,0,4,triaged,True
64119,Cleanup tests in rpc_test.py oncall: distributed module: tests,2021-08-27 22:04:54+00:00,,0,0,oncall: distributed module: tests,False
64107,as_tensor and negative strided np arrays triaged module: numpy module: tensor creation,2021-08-27 19:48:12+00:00,,0,7,triaged module: numpy module: tensor creation,True
64106,modulefinder_determinator is incompatible with imports into the test/ directory module: tests triaged,2021-08-27 19:16:14+00:00,,0,1,module: tests triaged,True
64097,CUDA error: CUBLAS_STATUS_INVALID_VALUE needs reproduction module: cuda triaged,2021-08-27 17:01:08+00:00,,0,4,needs reproduction module: cuda triaged,True
64096,String frontend doesn't support tuple unpacking  oncall: jit,2021-08-27 15:40:10+00:00,,0,0,oncall: jit,False
64093,test_backward_accumulate_grads (__main__.TensorPipeDistAutogradTest) is flaky oncall: distributed triaged module: flaky-tests module: rpc,2021-08-27 14:16:48+00:00,,2,2,oncall: distributed triaged module: flaky-tests module: rpc,False
64079,torch.equal does not support sparse tensors module: sparse triaged,2021-08-27 03:26:49+00:00,,0,1,module: sparse triaged,True
64067,Execute C++ based GTest unit tests from a python wrapper good first issue module: ci triaged enhancement,2021-08-26 23:47:20+00:00,,1,15,good first issue module: ci triaged enhancement,True
64060,Consolidating NCCL version parsing functions oncall: distributed module: bootcamp module: c10d,2021-08-26 22:25:53+00:00,,0,5,oncall: distributed module: bootcamp module: c10d,False
64060,Consolidating NCCL version parsing functions oncall: distributed module: bootcamp module: c10d,2021-08-26 22:25:53+00:00,,0,5,oncall: distributed module: bootcamp module: c10d,False
64058,Some linalg operations are not taking advantage from batched computation module: performance triaged module: linear algebra,2021-08-26 21:51:25+00:00,,0,4,module: performance triaged module: linear algebra,True
64048,"Be able to use pytest to run a ""core"" set of tests low priority module: tests triaged enhancement",2021-08-26 19:40:17+00:00,,1,1,low priority module: tests triaged enhancement,True
64043,Uncleared memory use after torch.load() module: memory usage module: serialization triaged,2021-08-26 18:58:01+00:00,,0,4,module: memory usage module: serialization triaged,True
64041,torch.unique acting up for a binary tensor triaged module: numpy module: boolean tensor module: correctness (silent) module: sorting and selection,2021-08-26 18:37:08+00:00,,0,9,triaged module: numpy module: boolean tensor module: correctness (silent) module: sorting and selection,True
64038,torch.nn.functional.softmax _stacklevel undocumented module: docs module: nn triaged,2021-08-26 17:39:49+00:00,,0,5,module: docs module: nn triaged,True
64025,RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR module: cudnn triaged,2021-08-26 16:14:08+00:00,,0,8,module: cudnn triaged,True
64023,A modest proposal: delete arithmetic overloads from c10::Half triaged module: half,2021-08-26 16:02:56+00:00,,0,5,triaged module: half,True
64006,Missing device and dtype description in torch.nn.MultiheadAttention oncall: transformer/mha,2021-08-26 06:25:40+00:00,,0,2,oncall: transformer/mha,False
64005,[LTC] Find a way to convert at::Generator into torch::jit::NamedValue oncall: jit module: lazy,2021-08-26 06:09:09+00:00,,0,1,oncall: jit module: lazy,True
63998,cat out= variant memory_format inconsistent for cpu and cuda module: cuda triaged module: memory format,2021-08-26 02:29:36+00:00,,0,1,module: cuda triaged module: memory format,True
63992,Direct inversion and linear systems solutions for small matrices module: performance module: cuda triaged module: linear algebra,2021-08-25 22:54:35+00:00,,1,12,module: performance module: cuda triaged module: linear algebra,True
63972,bazel build warning: Artifact 'torch/csrc/api/include/torch/version.h' is duplicated module: build triaged module: build warnings module: bazel,2021-08-25 18:32:37+00:00,,0,2,module: build triaged module: build warnings module: bazel,True
63971,benchmark.Compare raises: TypeError: object of type 'NoneType' has no len() module: error checking triaged module: benchmark,2021-08-25 18:25:59+00:00,,0,2,module: error checking triaged module: benchmark,True
63970,[LTC] Investigate a testing utility to make sure correct lowering has happened triaged module: lazy,2021-08-25 18:23:14+00:00,,0,2,triaged module: lazy,True
63958,torch.gather with sparse_grad=True does not work with SGD optimizer with momentum; gives bad error message module: sparse module: optimizer module: error checking triaged actionable,2021-08-25 15:50:10+00:00,,0,2,module: sparse module: optimizer module: error checking triaged actionable,True
63935,Inconsistent behaviour of index put when assigning from other type triaged module: type promotion module: advanced indexing,2021-08-25 08:09:46+00:00,,0,1,triaged module: type promotion module: advanced indexing,True
63929,[Poll] Support higher-order gradient computation in DDP oncall: distributed module: c10d module: ddp,2021-08-25 05:56:09+00:00,,0,10,oncall: distributed module: c10d module: ddp,False
63921,More efficient colon backwards in advanced indexing module: performance triaged module: advanced indexing,2021-08-25 03:10:59+00:00,,0,3,module: performance triaged module: advanced indexing,True
63920,quicklint clang-tidy step fails with many unrelated errors module: lint triaged,2021-08-25 02:54:24+00:00,,0,0,module: lint triaged,True
63919,quicklint mypy step fails with assertion module: typing module: lint triaged,2021-08-25 02:52:24+00:00,,0,0,module: typing module: lint triaged,True
63917,Add tests for DDP broadcast_buffers=True high priority triage review oncall: distributed module: bootcamp pt_distributed_rampup module: ddp,2021-08-25 02:11:41+00:00,,0,1,high priority triage review oncall: distributed module: bootcamp pt_distributed_rampup module: ddp,True
63911,Add efficient symmetric tensor representations triaged module: linear algebra module: __torch_function__,2021-08-25 00:48:38+00:00,,0,1,triaged module: linear algebra module: __torch_function__,True
63897,Scalar construction in C++: PY`torch.tensor(7)` !~= C++`at::tensor(7)` module: cpp triaged module: tensor creation,2021-08-24 22:50:42+00:00,,0,0,module: cpp triaged module: tensor creation,True
63870,torch median / nanmedian w/ nans speed module: performance triaged module: NaNs and Infs module: sorting and selection module: reductions,2021-08-24 18:21:59+00:00,,0,2,module: performance triaged module: NaNs and Infs module: sorting and selection module: reductions,True
63855,Include tensor shape in its default print formatting module: printing triaged enhancement,2021-08-24 11:12:44+00:00,,0,15,module: printing triaged enhancement,True
63849,"Installed successfully the torch-1.9 and torch_xla-1.9. RuntimeError requested XLA_GPU:0，but available devices are [CPU:0, XLA_CPU:0 ] module: binaries triaged module: xla",2021-08-24 09:08:29+00:00,,0,1,module: binaries triaged module: xla,True
63847,RuntimError:CUDA error: an illegal memory access was encountered when copying data to CUDA needs reproduction module: cuda triaged,2021-08-24 08:46:15+00:00,,0,5,needs reproduction module: cuda triaged,True
63846,[torch.distributed] Make dynamic_rendezvous log handler configurable feature triaged module: elastic oncall: r2p,2021-08-24 07:30:53+00:00,,0,3,feature triaged module: elastic oncall: r2p,True
63837,`torch.scatter` doesn't fail correctly on CUDA (memory overlap) module: cuda module: tests triaged module: scatter & gather ops module: structured kernels,2021-08-24 05:37:44+00:00,,0,1,module: cuda module: tests triaged module: scatter & gather ops module: structured kernels,True
63832,[RFC] Add `torch.distributed.run` as a console script in pytorch's setup.py oncall: distributed module: elastic,2021-08-24 04:48:28+00:00,,0,1,oncall: distributed module: elastic,False
63815,[FX] Documentation for Graph.flatten_inps and Graph.unflatten_outs triaged module: fx,2021-08-23 22:56:57+00:00,,0,0,triaged module: fx,True
63812,Calling backward with create_graph on the output of a DistributedDataParallel throws error oncall: distributed triaged module: ddp,2021-08-23 21:43:11+00:00,,1,14,oncall: distributed triaged module: ddp,True
63802,[torch.distributed.launch|run] Hangs on SIGINT when using a TCPStore backed rdzv_backend  oncall: distributed triaged module: c10d,2021-08-23 20:46:07+00:00,,0,4,oncall: distributed triaged module: c10d,True
63781,[rfc] Hardcoded Target Determination feature module: tests triaged,2021-08-23 17:19:14+00:00,,0,1,feature module: tests triaged,True
63767,`__torch_function__` docs should use @classmethod module: docs triaged module: __torch_function__,2021-08-23 15:16:00+00:00,,0,7,module: docs triaged module: __torch_function__,True
63749,Segmentation fault when using C++/pybind11 module without also importing torch module: cpp-extensions triaged module: pybind module: regression,2021-08-23 09:09:39+00:00,,1,1,module: cpp-extensions triaged module: pybind module: regression,True
63740,DDP Gradient reduction is not triggered after calling autograd.backward() oncall: distributed,2021-08-23 00:08:03+00:00,,0,6,oncall: distributed,False
63726,a problem happened in torch.randperm needs reproduction triaged module: random,2021-08-21 13:34:05+00:00,,0,7,needs reproduction triaged module: random,True
63725,[Feature Request] add graph to hparams for tensorboard module: tensorboard oncall: visualization,2021-08-21 11:57:54+00:00,,0,1,module: tensorboard oncall: visualization,False
63720,Torch.save with zip serialization causes memory bloat module: serialization triaged,2021-08-21 00:47:17+00:00,,0,2,module: serialization triaged,True
63648,[JIT] Typing on math.ceil is inaccurate for Scalar input oncall: jit,2021-08-20 17:02:15+00:00,,1,3,oncall: jit,True
63645,Create a new OptionalArrayRef template class to replace Optional<ArrayRef> module: internals triaged,2021-08-20 13:14:46+00:00,,0,0,module: internals triaged,True
63644,LeakyReLU and Elu use more VRAM than needed module: autograd module: nn triaged,2021-08-20 12:36:02+00:00,,0,0,module: autograd module: nn triaged,True
63624,Raw saved tensors can survive the deletion of the underlying SavedVariable object module: autograd triaged,2021-08-20 02:40:50+00:00,,1,0,module: autograd triaged,True
63623,Expose a SavedTensorsHooks nn.Module for users to register saved tensors hooks feature module: autograd module: nn triaged,2021-08-20 01:57:03+00:00,,1,4,feature module: autograd module: nn triaged,True
63618,C++ torch::cuda::synchronize speeds up training module: cuda triaged,2021-08-19 23:03:22+00:00,,0,2,module: cuda triaged,True
63610,Enable some amount of CI tests on lazy_tensor_staging branch oncall: jit module: lazy,2021-08-19 21:45:09+00:00,,1,0,oncall: jit module: lazy,False
63581,Java NVIDIA GPU Inference Support triaged oncall: mobile,2021-08-19 16:18:27+00:00,,1,8,triaged oncall: mobile,True
63574,compile_commands.json doesn't contain nvcc invocations module: build triaged,2021-08-19 15:27:25+00:00,,0,0,module: build triaged,True
63559,"Functional grid_sample: example of padding_mode=""reflection"" description error module: docs module: nn triaged module: interpolation",2021-08-19 09:04:01+00:00,,0,1,module: docs module: nn triaged module: interpolation,True
63558,universal binaries for libtorch on mac (x86_64+arm) module: binaries feature triaged module: macos module: arm,2021-08-19 08:24:08+00:00,,0,30,module: binaries feature triaged module: macos module: arm,True
63555,pytorch 1.8.1+cu111 used much more CPU RAM than pytorch 1.8.1 after run `import torch` module: binaries module: cuda module: cpu module: memory usage triaged,2021-08-19 07:38:32+00:00,,0,4,module: binaries module: cuda module: cpu module: memory usage triaged,True
63490,[FX] Data type of target should be validated on Node construction and lint triaged module: fx,2021-08-18 17:44:43+00:00,,0,0,triaged module: fx,True
63488,MultiStepLR with different gammas for each parameter group module: optimizer triaged module: LrScheduler,2021-08-18 16:32:44+00:00,,0,0,module: optimizer triaged module: LrScheduler,True
63486,Distributed / MPI / CUDA: Incorrect messages received from isend/irecv in PyTorch 1.9 high priority oncall: distributed triaged module: correctness (silent),2021-08-18 15:53:20+00:00,,0,5,high priority oncall: distributed triaged module: correctness (silent),True
63485,Preserve tensor subclasses when unpacking a SavedTensor module: autograd triaged,2021-08-18 15:27:54+00:00,,1,3,module: autograd triaged,True
63479,Cannot include extension.h under Windows - linker error  THPVariable_Wrap module: cpp-extensions module: cpp triaged,2021-08-18 12:41:38+00:00,,0,8,module: cpp-extensions module: cpp triaged,True
63477,Supporting Pytorch for Custom Compiler Backend triaged module: dispatch module: backend,2021-08-18 12:32:06+00:00,,0,0,triaged module: dispatch module: backend,True
63476,Error loading Pytorch lite model in Android oncall: mobile,2021-08-18 12:29:24+00:00,,2,1,oncall: mobile,True
63475,C++ Inference error causes Identity Module oncall: jit triaged,2021-08-18 12:08:55+00:00,,0,1,oncall: jit triaged,True
63466,Add a GitHub actions workflow for Macos module: ci triaged module: infra,2021-08-18 07:33:32+00:00,,0,1,module: ci triaged module: infra,True
63455,distributed_error oncall: distributed,2021-08-18 02:45:35+00:00,,0,2,oncall: distributed,False
63445,"getNcclVersion() in NCCLUtils.cpp handles ""2.10.3"" incorrectly oncall: distributed module: nccl module: ddp",2021-08-18 00:23:50+00:00,,1,3,oncall: distributed module: nccl module: ddp,True
63441,OpenCV causes backpropagation to get stuck triaged module: deadlock module: multithreading,2021-08-17 23:19:49+00:00,,0,0,triaged module: deadlock module: multithreading,True
63405,torch.arange has issue tracing with bool input oncall: jit,2021-08-17 17:07:17+00:00,,0,1,oncall: jit,True
63403,Consolidate distributed benchmark folders oncall: distributed module: bootcamp pt_distributed_rampup module: ddp,2021-08-17 16:28:07+00:00,,1,1,oncall: distributed module: bootcamp pt_distributed_rampup module: ddp,True
63395,How to efficiently (without looping) get data from tensor predicted by a torchscript in C++? oncall: jit,2021-08-17 12:42:58+00:00,,0,1,oncall: jit,False
63356,[python codegen] correctly plumb TensorOptions defaults through the python binding layer. triaged module: codegen,2021-08-16 20:51:46+00:00,,1,0,triaged module: codegen,True
63337,Property based testing like AFL module: tests triaged,2021-08-16 17:00:41+00:00,,0,0,module: tests triaged,True
63322,lstm's input h0 and c0 bug needs reproduction module: rnn triaged module: numpy module: NaNs and Infs,2021-08-16 14:44:42+00:00,,0,3,needs reproduction module: rnn triaged module: numpy module: NaNs and Infs,True
63313,Automatic mixed precision works worse for 3D neural networks. module: performance triaged module: amp (automated mixed precision),2021-08-16 11:54:23+00:00,,0,2,module: performance triaged module: amp (automated mixed precision),True
63305,Alternate Model Loading - Android triaged enhancement module: android oncall: mobile,2021-08-16 08:11:33+00:00,,2,4,triaged enhancement module: android oncall: mobile,True
63299,Channels last mode doesn't work with inputs with 0 batch dimension module: nn triaged module: memory format,2021-08-16 01:36:51+00:00,,1,2,module: nn triaged module: memory format,True
63297,Named tensor in tracer triaged module: named tensor,2021-08-15 23:15:22+00:00,,0,2,triaged module: named tensor,True
63295,pytorch 1.8.2  cuda test errors module: build module: autograd module: cuda triaged module: linear algebra,2021-08-15 18:17:07+00:00,,0,6,module: build module: autograd module: cuda triaged module: linear algebra,True
63293,Support `torch.linalg.outer` feature triaged module: linear algebra module: python array api,2021-08-15 13:44:09+00:00,,0,3,feature triaged module: linear algebra module: python array api,True
63291,[typing] new_ones has wrong signature module: bootcamp module: typing triaged,2021-08-15 06:41:08+00:00,,1,3,module: bootcamp module: typing triaged,True
63290,DistributedDataParallel documentation oncall: distributed triaged better-engineering module: ddp,2021-08-15 05:16:41+00:00,,0,5,oncall: distributed triaged better-engineering module: ddp,True
63282,Serialising `torch.bool` generates a warning about `np.bool` being deprecated.  module: error checking module: serialization triaged module: numpy,2021-08-14 16:56:37+00:00,,0,1,module: error checking module: serialization triaged module: numpy,True
63265,torch.scatter_: support index.size(d) > src.size(d) good first issue triaged enhancement module: scatter & gather ops,2021-08-14 00:36:11+00:00,,0,26,good first issue triaged enhancement module: scatter & gather ops,True
63257,`help(torch.finfo)` doesn't give useful information even though class has documentation module: docs triaged,2021-08-13 21:29:38+00:00,,0,0,module: docs triaged,True
63247,Add a timeout argument to RPC shutdown() oncall: distributed module: bootcamp,2021-08-13 18:49:59+00:00,,1,1,oncall: distributed module: bootcamp,False
63226,"cuda.is_available returns True in pycharms python console, False in code module: cuda triaged",2021-08-13 12:24:41+00:00,,0,2,module: cuda triaged,True
63223,[bug] torch.distributed.elastic logging: Failed to print the first info statement triaged module: elastic oncall: r2p,2021-08-13 10:16:55+00:00,,0,7,triaged module: elastic oncall: r2p,True
63217,Torch.nn.functional.interpolate() function memory release problem module: memory usage triaged module: interpolation,2021-08-13 06:11:19+00:00,,0,1,module: memory usage triaged module: interpolation,True
63211,Memory leak in `ddp_zero_hook.py` hooks module: memory usage triaged module: ddp,2021-08-13 03:28:31+00:00,,0,0,module: memory usage triaged module: ddp,True
63207,"document our compatibility with upstream packages (e.g. languages, compilers, tools) module: docs triaged",2021-08-13 01:58:27+00:00,,1,0,module: docs triaged,True
63205,split up test/test_autograd.py module: autograd triaged better-engineering,2021-08-13 01:27:03+00:00,,1,1,module: autograd triaged better-engineering,True
63188,ROCm build documentation lacks needed dependencies module: build module: docs module: rocm triaged actionable,2021-08-12 21:46:53+00:00,,0,10,module: build module: docs module: rocm triaged actionable,True
63171,[BE] Clean up DDP Single-Program Multi-Device Vestige in reducer.cpp oncall: distributed module: bootcamp,2021-08-12 19:14:44+00:00,,1,1,oncall: distributed module: bootcamp,False
63159,PyTorch Profiler oncall: profiler mlops,2021-08-12 17:58:31+00:00,,0,8,oncall: profiler mlops,False
63145,torch.jit.trace does not work if there is an autograd in the function oncall: jit,2021-08-12 11:50:10+00:00,,0,0,oncall: jit,False
63140,[documentation] torch.distributed.elastic: illustrate how to write load_checkpoint and save_checkpoint in Train Script  module: docs triaged module: elastic oncall: r2p,2021-08-12 08:54:25+00:00,,0,1,module: docs triaged module: elastic oncall: r2p,True
63138,Sign in slogdet is set to requires_grad = False even when using complex numbers.  triaged module: complex module: linear algebra complex_autograd,2021-08-12 08:40:54+00:00,,0,0,triaged module: complex module: linear algebra complex_autograd,True
63124,torch.jit.trace quantized bigbird leads to 0INTERNAL ASSERT FAILED runtime error oncall: jit,2021-08-12 00:44:32+00:00,,0,3,oncall: jit,True
63113,Implement half_to_float flag for both _log_softmax and _softmax operators triaged module: lazy,2021-08-11 22:36:04+00:00,,0,0,triaged module: lazy,True
63085,Split up test_nn.py module: nn triaged better-engineering,2021-08-11 16:02:29+00:00,,0,3,module: nn triaged better-engineering,True
63082,script/build_android nn::Module support triaged module: android oncall: mobile,2021-08-11 14:40:40+00:00,,0,0,triaged module: android oncall: mobile,True
63079,"PyTorch 1.9.0, test_optim fails on Nvidia A10. module: optimizer module: cuda module: tests triaged",2021-08-11 13:15:51+00:00,,0,11,module: optimizer module: cuda module: tests triaged,True
63078,torch.cuda.amp fails with torch.sparse.softmax module: sparse triaged module: amp (automated mixed precision),2021-08-11 12:44:50+00:00,,0,0,module: sparse triaged module: amp (automated mixed precision),True
63076,Tracing models all the way down to basic functions feature triaged module: fx,2021-08-11 08:50:33+00:00,,0,15,feature triaged module: fx,True
63069,CUDA error: invalid configuration argument for torch.sparse tensor backward module: sparse module: cuda triaged,2021-08-11 04:17:29+00:00,,0,0,module: sparse module: cuda triaged,True
63057,Find out why TestSparseCPU.test_softmax_cpu_float64 grad and input dtypes are different triaged module: structured kernels,2021-08-11 01:41:51+00:00,,0,0,triaged module: structured kernels,True
63041,[RFC] Should DDP support custom reduction logic for registered module buffers? oncall: distributed triaged module: ddp,2021-08-10 19:37:24+00:00,,1,5,oncall: distributed triaged module: ddp,True
63038,Use pytorch-probot for PyTorch-specific stuff + MOTD in Dr. CI comment triaged module: infra needs design,2021-08-10 18:59:59+00:00,,0,0,triaged module: infra needs design,False
63034,[bug] nn.functional.pad (circular) ubsan failure module: nn triaged module: padding,2021-08-10 18:36:57+00:00,,0,3,module: nn triaged module: padding,True
63023,[feature request] Type promotions for Boolean tensors with sub operation + Numpy compatability triaged module: numpy module: type promotion module: boolean tensor,2021-08-10 11:10:48+00:00,,0,1,triaged module: numpy module: type promotion module: boolean tensor,True
63020,Update _symbolic_trace.py for proxy_placeholder function triaged module: fx,2021-08-10 07:32:13+00:00,,0,0,triaged module: fx,False
63016,grid_sample should not require grid to be the same dimension as input. module: nn triaged enhancement module: interpolation,2021-08-10 05:18:11+00:00,,0,0,module: nn triaged enhancement module: interpolation,True
63004,ignore_index does not work as described with nn.CrossEntropyLoss module: nn triaged,2021-08-09 22:54:26+00:00,,0,6,module: nn triaged,True
62998,Add nn.Module full pre-backward hooks feature module: nn triaged,2021-08-09 21:32:21+00:00,,1,5,feature module: nn triaged,True
62989,Doc page for functions with overload is not properly indented module: docs triaged,2021-08-09 19:53:42+00:00,,0,2,module: docs triaged,False
62983,Implement `set_flush_denormal ` for ARM triaged enhancement module: arm,2021-08-09 18:02:57+00:00,,0,0,triaged enhancement module: arm,True
62982,Don't use try-catch to handle overload resolution handling in torch.ops module: bootcamp triaged module: __torch_dispatch__,2021-08-09 17:51:36+00:00,,1,1,module: bootcamp triaged module: __torch_dispatch__,True
62973,Audit use of `C10_UNUSED` triaged better-engineering,2021-08-09 16:06:31+00:00,,0,2,triaged better-engineering,True
62962,[JIT] NVRTC unknown error  oncall: jit NNC,2021-08-09 12:21:48+00:00,,0,7,oncall: jit NNC,True
62955,Make axes selection keyword arguments in torch.diagonal and torch.transpose consistent triaged module: numpy module: linear algebra module: python array api,2021-08-08 18:34:17+00:00,,0,3,triaged module: numpy module: linear algebra module: python array api,True
62934,`tools/autograd/derivatives.yaml` doesn't support methods on optional tensor module: autograd triaged actionable module: codegen,2021-08-07 02:57:26+00:00,,0,1,module: autograd triaged actionable module: codegen,True
62933,Compilation error with gcc11 and libstdc++11 (thread_id != operator removed) triaged module: mkldnn,2021-08-07 00:39:23+00:00,,0,6,triaged module: mkldnn,True
62931,GPU compatibility check without initializing CUDA module: cuda triaged,2021-08-06 22:56:36+00:00,,0,3,module: cuda triaged,True
62926,cudnn_convolution_add_relu fails under basic conditions module: dependency bug module: cudnn triaged,2021-08-06 21:37:36+00:00,,0,3,module: dependency bug module: cudnn triaged,True
62925,[Testing] memory_format decorator module: tests triaged module: memory format,2021-08-06 20:47:15+00:00,,0,1,module: tests triaged module: memory format,True
62917,Use `linecache.lazycache` in torch.fx module: bootcamp triaged module: fx,2021-08-06 19:43:42+00:00,,0,1,module: bootcamp triaged module: fx,True
62902,distributed/elastic/multiprocessing/api_test is flaky on ROCm module: rocm triaged module: flaky-tests needs research oncall: r2p,2021-08-06 17:44:37+00:00,,0,2,module: rocm triaged module: flaky-tests needs research oncall: r2p,False
62888,Add a mechanism to not universally dispatch to python for all for all operations when using __torch_dispatch__  triaged module: __torch_dispatch__,2021-08-06 14:29:47+00:00,,0,0,triaged module: __torch_dispatch__,True
62885,Type of first constructor parameter of ElasticDistributedSampler and DistributedSampler oncall: distributed module: dataloader,2021-08-06 14:17:03+00:00,,0,3,oncall: distributed module: dataloader,True
62884,Conjugate fallback dispatch key should be per-backend triaged module: complex module: dispatch,2021-08-06 13:56:52+00:00,,0,0,triaged module: complex module: dispatch,True
62876,Key already registered with the same priority: uv module: internals triaged module: tensorpipe,2021-08-06 05:46:57+00:00,,0,5,module: internals triaged module: tensorpipe,True
62865,Multiplication of non-int torch.tensor with string gives unexpected TypeError module: error checking triaged,2021-08-06 00:21:06+00:00,,0,1,module: error checking triaged,True
62862,Convert unset variables in `Reducer::Timer` to use c10::optional triaged better-engineering pt_distributed_rampup module: ddp,2021-08-05 23:25:15+00:00,,0,0,triaged better-engineering pt_distributed_rampup module: ddp,True
62854,"aten::pixel_shuffle(Tensor self, int upscale_factor) -> (Tensor): Argument self not provided. oncall: jit",2021-08-05 22:31:31+00:00,,0,1,oncall: jit,True
62849,"slice_embed, select_embed like diag_embed module: autograd triaged module: viewing and reshaping",2021-08-05 21:51:27+00:00,,0,8,module: autograd triaged module: viewing and reshaping,True
62845,allow specification of variadic arguments in native_functions.yaml module: internals triaged,2021-08-05 21:06:24+00:00,,0,1,module: internals triaged,True
62821,Enable fft  support for mobile builds enhancement module: android oncall: mobile module: arm module: fft,2021-08-05 15:52:30+00:00,,0,5,enhancement module: android oncall: mobile module: arm module: fft,True
62812,Support `torch.linalg.einsum` feature triaged module: linear algebra,2021-08-05 10:10:59+00:00,,0,8,feature triaged module: linear algebra,True
62790,Remove c10d::kDefaultFirstBucketBytes oncall: distributed module: bootcamp triaged pt_distributed_rampup module: ddp,2021-08-05 01:11:09+00:00,,0,0,oncall: distributed module: bootcamp triaged pt_distributed_rampup module: ddp,True
62788,PyTorch(with mkldnn) in inference mode using non-optimal prop_kind for convolution triaged module: mkldnn,2021-08-05 00:42:12+00:00,,0,4,triaged module: mkldnn,False
62784,Runtime error when passing dim as None in torch.squeeze (and Tensor.squeeze) triaged module: numpy,2021-08-05 00:14:28+00:00,,0,1,triaged module: numpy,True
62780,Seeing unit test failures for ProcessGroupShareTensorTest high priority triage review oncall: distributed module: nccl module: c10d,2021-08-05 00:01:23+00:00,,0,3,high priority triage review oncall: distributed module: nccl module: c10d,True
62775,[TorchScript] Support for registering builtin operators with control flow oncall: jit,2021-08-04 22:53:17+00:00,,0,2,oncall: jit,False
62742,Add a way to edit CI env variables from PRs low priority triaged better-engineering,2021-08-04 18:44:02+00:00,,0,1,low priority triaged better-engineering,True
62699,Can not parse the caffe2 pretrain model. google.protobuf.message.DecodeError: Error parsing message caffe2 triaged,2021-08-04 02:45:34+00:00,,0,0,caffe2 triaged,True
62695,Use fast gpuAtomicAdd for FP16 data type in CUDA kernels module: cuda triaged module: half,2021-08-04 00:06:54+00:00,,0,0,module: cuda triaged module: half,True
62683,[FX] Support pytree `concrete_args` for *args triaged module: fx,2021-08-03 21:57:59+00:00,,1,1,triaged module: fx,True
62676,pytorch_mobile custom build Module.forward null reference.  module: android oncall: mobile,2021-08-03 20:35:09+00:00,,1,5,module: android oncall: mobile,False
62675,dynamic shapes triaged module: lazy,2021-08-03 20:34:45+00:00,,1,1,triaged module: lazy,True
62673,Make printing to stdout / stderr in tests in CI an error triaged better-engineering,2021-08-03 20:20:31+00:00,,0,0,triaged better-engineering,True
62672,Add Convolution support for lazy tensor module: convolution triaged module: lazy,2021-08-03 20:13:15+00:00,,1,0,module: convolution triaged module: lazy,True
62661,Rewrite PyTorch CUDA backend in Triton module: cuda triaged,2021-08-03 17:37:13+00:00,,0,22,module: cuda triaged,True
62656,"Building pytorch from source results in ""an object with that name is already defined"" error on import module: build triaged module: pybind",2021-08-03 17:19:19+00:00,,0,0,module: build triaged module: pybind,True
62655,Make KernelFunction::makeFromUnboxedFunctor infer KernelFunctor from input argument module: bootcamp triaged module: dispatch,2021-08-03 17:16:27+00:00,,0,0,module: bootcamp triaged module: dispatch,True
62654,Checkpointing without re-entrant autograd module: autograd triaged,2021-08-03 17:15:10+00:00,,2,0,module: autograd triaged,True
62650,Error while importing torch: libtorch_python.so: undefined symbol: PyThread_tss_alloc module: abi triaged,2021-08-03 15:28:13+00:00,,0,0,module: abi triaged,True
62648,Feature request: Generate Sobol points directly on the GPU module: distributions feature module: cuda triaged module: tensor creation,2021-08-03 14:26:54+00:00,,0,1,module: distributions feature module: cuda triaged module: tensor creation,True
62644,waste too much time in first two pictures needs reproduction triaged,2021-08-03 09:23:38+00:00,,0,2,needs reproduction triaged,False
62640,add gamma to CosineAnnealingWarmRestarts so max lr can decrease in cycles module: optimizer triaged module: LrScheduler,2021-08-03 07:51:49+00:00,,0,1,module: optimizer triaged module: LrScheduler,True
62630,Instructions for CMAKE_PREFIX_PATH seem to be broken module: build module: docs triaged,2021-08-03 01:22:23+00:00,,0,1,module: build module: docs triaged,True
62619,LazyTensor support for fuser integration investigation triaged LazyTensor_nvfuser_integration module: lazy,2021-08-02 22:56:53+00:00,,0,4,triaged LazyTensor_nvfuser_integration module: lazy,True
62616,LazyTensor cuda fallback triaged LazyTensor_nvfuser_integration module: lazy,2021-08-02 22:38:26+00:00,,0,14,triaged LazyTensor_nvfuser_integration module: lazy,True
62614,LazyTensor recomputes intermediate tensor in fragmented computation triaged LazyTensor_nvfuser_integration module: lazy,2021-08-02 22:30:08+00:00,,1,5,triaged LazyTensor_nvfuser_integration module: lazy,True
62606,Guidance on where to add documentation module: docs triaged,2021-08-02 20:40:17+00:00,,1,3,module: docs triaged,False
62603,Unexpected `is_training` after using `train` in a JIT Module oncall: jit,2021-08-02 20:29:05+00:00,,0,0,oncall: jit,True
62602,"""test_events_wait"" is flaky on ROCm module: rocm triaged module: flaky-tests",2021-08-02 20:27:36+00:00,,0,3,module: rocm triaged module: flaky-tests,True
62591,Unnecessary python bindings and documentation for internal functions/ops triaged module: codegen,2021-08-02 18:11:32+00:00,,0,3,triaged module: codegen,True
62588,static library kineto_LIBRARY-NOTFOUND not found. triaged module: build warnings,2021-08-02 17:55:32+00:00,,0,3,triaged module: build warnings,True
62566,`c10::optional<T>` operators should delegate to corresponding operators on `T` module: cpp triaged,2021-08-02 14:10:56+00:00,,0,0,module: cpp triaged,True
62556,[TorchScript] Inconsistency between Python and PyTorch when raising 0 to a negative power oncall: jit,2021-08-02 05:40:05+00:00,,1,0,oncall: jit,True
62554,`torch.fx.node.normalized_arguments` does not pass `normalize_to_only_use_kwargs` module: bootcamp triaged module: fx,2021-08-02 04:22:52+00:00,,1,1,module: bootcamp triaged module: fx,True
62547,[FX] to_folder breaks when a directly-traced `nn.Sequential` is dumped module: bootcamp triaged module: fx,2021-08-01 13:16:09+00:00,,1,1,module: bootcamp triaged module: fx,True
62545,Incosistency with args for `nn.functional.max_poolNd` vs `nn.MaxPoolNd` functions module: nn triaged module: pooling,2021-08-01 07:31:00+00:00,,1,4,module: nn triaged module: pooling,True
62543,FAIL: test_threshold (test_jit_fuser.TestFuser) oncall: jit,2021-08-01 06:15:28+00:00,,0,4,oncall: jit,False
62542,3 Times memory cost when loading the model to torch.nn.parallel.DistributedDataParallel oncall: distributed module: ddp,2021-08-01 05:20:26+00:00,,0,1,oncall: distributed module: ddp,False
62540,[Meta] Change default branch name to `main` for repos in `pytorch` project module: bootcamp triaged module: infra,2021-07-31 19:45:24+00:00,,0,1,module: bootcamp triaged module: infra,True
62530,[feature request] `torch.switch` (to mirror `lax.switch`) and `torch.cond` - with jax arithmetic coder as a case study module: nn triaged needs research,2021-07-31 08:26:01+00:00,,0,2,module: nn triaged needs research,False
62508,torch.prod internal asserts when passed a tensor that requires_grad (and a dtype) module: autograd triaged module: assert failure,2021-07-30 21:41:32+00:00,,0,5,module: autograd triaged module: assert failure,True
62500,meta tensor `set_` seems fishy module: internals triaged module: meta tensors,2021-07-30 19:09:48+00:00,,0,1,module: internals triaged module: meta tensors,True
62475,Learning rate scheduler list index out of range module: optimizer triaged module: LrScheduler,2021-07-30 13:25:23+00:00,,0,3,module: optimizer triaged module: LrScheduler,True
62474,Distributed: RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [2048]] is at version 4; expected version 3 instead  oncall: distributed,2021-07-30 11:52:47+00:00,,0,6,oncall: distributed,False
62473,Sparse updates to logits in distributions.Categorical module: sparse module: distributions module: autograd module: memory usage triaged,2021-07-30 09:27:08+00:00,,0,2,module: sparse module: distributions module: autograd module: memory usage triaged,True
62451,torch.Generator ignores the device argument? triaged module: random,2021-07-30 04:56:40+00:00,,0,1,triaged module: random,True
62449,Review replacing test/test_namedtuple_return_api.py with an OpInfo-based test feature module: tests triaged,2021-07-30 03:53:40+00:00,,0,0,feature module: tests triaged,True
62448,Dynamic tensor rematerialization module: internals module: checkpoint feature triaged,2021-07-30 03:48:12+00:00,,0,9,module: internals module: checkpoint feature triaged,True
62446,[docs] Can't get header links to h3 on docs pages triaged enhancement module: doc infra,2021-07-30 00:35:10+00:00,,0,1,triaged enhancement module: doc infra,False
62403,DDP oncall: distributed module: ddp,2021-07-29 14:50:53+00:00,,0,2,oncall: distributed module: ddp,False
62399,Understatement about how running_var is computed in BatchNorm module: docs triaged module: norms and normalization,2021-07-29 14:29:20+00:00,,0,1,module: docs triaged module: norms and normalization,True
62396,"Make interpolation output size compatible with opencv, scikit-image and scipy for floating scale factor triaged enhancement module: interpolation",2021-07-29 12:44:02+00:00,,1,0,triaged enhancement module: interpolation,True
62395,[FX] Does PyTorch want to support for dynamic control flow for torch.fx? triaged module: fx,2021-07-29 11:59:04+00:00,,0,2,triaged module: fx,False
62394,crash in libtorch jit oncall: jit,2021-07-29 11:45:21+00:00,,0,6,oncall: jit,True
62390,Profiler reports profiling overhead & inaccurate numbers for last record_function oncall: profiler,2021-07-29 09:15:03+00:00,,0,4,oncall: profiler,False
62387,Bad performance of stock model on Windows compared to Linux module: performance module: windows module: cpu triaged,2021-07-29 08:43:41+00:00,,0,23,module: performance module: windows module: cpu triaged,True
62381,torch.distributed and subprocess do not work together? oncall: distributed,2021-07-29 06:12:04+00:00,,0,5,oncall: distributed,False
62371,libtorch on Apple m1 module: cpp module: ci triaged module: macos enhancement module: arm,2021-07-29 00:13:38+00:00,,0,5,module: cpp module: ci triaged module: macos enhancement module: arm,True
62358,torch.cholesky_inverse supports tensors with more than 2 dimensions module: docs triaged module: linear algebra,2021-07-28 21:05:03+00:00,,0,1,module: docs triaged module: linear algebra,True
62354,Some way to specify expected failures for OpInfo-based tests module: tests triaged enhancement,2021-07-28 20:24:34+00:00,,0,9,module: tests triaged enhancement,True
62352,[feature request] Expand a given dim triaged enhancement module: viewing and reshaping,2021-07-28 20:18:24+00:00,,0,1,triaged enhancement module: viewing and reshaping,True
62350,NotImplementedError: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. module: cuda triaged module: vision,2021-07-28 19:23:38+00:00,,0,3,module: cuda triaged module: vision,True
62343,maybe_wrap_dim should have an example attached to it module: docs triaged,2021-07-28 18:33:35+00:00,,0,0,module: docs triaged,False
62332,"How to Fix “AssertionError: CUDA unavailable, invalid device 0 requested” module: binaries triaged",2021-07-28 15:05:52+00:00,,0,1,module: binaries triaged,True
62331,broken pipe error  needs reproduction module: crash module: cuda triaged,2021-07-28 14:42:01+00:00,,0,0,needs reproduction module: crash module: cuda triaged,True
62330,Bus error (core dumped) when import torch needs reproduction module: crash module: build triaged,2021-07-28 14:04:16+00:00,,0,1,needs reproduction module: crash module: build triaged,True
62325,[jit]][script] handling of 'void' returns oncall: jit,2021-07-28 12:56:20+00:00,,1,7,oncall: jit,True
62320,[JIT] Support JAX-style statically shaped nonzero to avoid host-device synchronization high priority module: cuda triaged module: advanced indexing needs design,2021-07-28 11:02:50+00:00,,0,5,high priority module: cuda triaged module: advanced indexing needs design,True
62314,dist.destroy_process_group() don't release master_port when there're at least 2 process groups oncall: distributed module: c10d,2021-07-28 08:23:11+00:00,,0,2,oncall: distributed module: c10d,False
62313,"On Triton inference server, the TorchScript .pt file, exported by 1.9.0, serves unsuccessfully oncall: jit",2021-07-28 07:27:23+00:00,,1,4,oncall: jit,False
62308,Error while running FINETUNING TORCHVISION MODELS needs reproduction module: crash triaged module: vision,2021-07-28 05:42:46+00:00,,0,8,needs reproduction module: crash triaged module: vision,True
62300,ProcessGroupGloo CUDA Comm Synchronization Looks Wrong oncall: distributed module: c10d,2021-07-28 01:39:33+00:00,,0,0,oncall: distributed module: c10d,False
62283,c10d and discontiguous tensors with mismatch strides oncall: distributed module: c10d,2021-07-27 18:51:36+00:00,,0,2,oncall: distributed module: c10d,False
62273,Serialization map_location silently ignores xla/mlc/meta (any serialization mechanism that skips storage) module: serialization triaged,2021-07-27 17:09:48+00:00,,0,0,module: serialization triaged,False
62271,Refactor serialization tests to use device parametrization module: tests module: serialization triaged,2021-07-27 17:09:11+00:00,,0,0,module: tests module: serialization triaged,True
62265,`periodic-pytorch-linux-xenial-cuda11.3-cudnn8-py3.6-gcc7-build` started to fail  module: cuda triaged,2021-07-27 15:02:14+00:00,,0,3,module: cuda triaged,True
62258,Adding a new operator shouldn't need to trigger a rebuild of CUDA kernels module: build module: cuda triaged,2021-07-27 13:03:54+00:00,,1,2,module: build module: cuda triaged,True
62255,Support `torch.linalg.trace` triaged module: python array api,2021-07-27 10:17:55+00:00,,0,5,triaged module: python array api,True
62221,Torchscript Optimize for Mobile RuntimeError: Can't deepcopy IValue with tag: Enum oncall: jit,2021-07-26 21:21:36+00:00,,0,5,oncall: jit,False
62182,Dispatch to Python with wrappers triggers Autograd view asserts triaged module: __torch_dispatch__,2021-07-26 13:42:32+00:00,,0,9,triaged module: __torch_dispatch__,True
62178,Wrong implementation of method log_prob in torch.distributions.negative_binomial module: distributions module: docs triaged module: numpy,2021-07-26 09:28:24+00:00,,0,15,module: distributions module: docs triaged module: numpy,True
62171,[feature request] torch.nn.Conv3d on tensors having more than 5 dimensions feature module: nn module: convolution triaged,2021-07-26 02:17:07+00:00,,0,2,feature module: nn module: convolution triaged,True
62168,Error installing from source module: build triaged,2021-07-26 00:39:34+00:00,,0,5,module: build triaged,True
62165,"RuntimeError: xnn_status_success == run_status INTERNAL ASSERT FAILED at ""/pytorch/aten/src/ATen/native/xnnpack/Linear.cpp"":158 module: android oncall: mobile module: ios",2021-07-25 16:44:02+00:00,,1,3,module: android oncall: mobile module: ios,True
62162,Clarify sparse COO tensor coalesce behavior wrt overflow + how to binarize a sparse tensor module: sparse module: docs triaged,2021-07-25 12:37:49+00:00,,0,3,module: sparse module: docs triaged,True
62155,[docs] Change code snippet formatting to be directly copy-pasteable into REPL or into source files module: docs triaged enhancement needs design,2021-07-24 15:30:23+00:00,,0,4,module: docs triaged enhancement needs design,False
62154,AttributeError: module 'torch.jit' has no attribute '_script_if_tracing' oncall: jit,2021-07-24 14:18:50+00:00,,0,4,oncall: jit,True
62153,cuSPARSELt Integration module: sparse module: cuda triaged enhancement,2021-07-24 14:09:03+00:00,,1,10,module: sparse module: cuda triaged enhancement,True
62152,Runtime Error: Expected a 'cuda' device type for generator but found 'cpu' in google colab needs reproduction triaged,2021-07-24 13:22:45+00:00,,0,2,needs reproduction triaged,False
62148,[feature request] Function to broadcast right triaged enhancement needs design module: viewing and reshaping,2021-07-24 12:02:40+00:00,,0,2,triaged enhancement needs design module: viewing and reshaping,False
62147,Tensor subclassing does not support bool input triaged module: boolean tensor module: tensor creation,2021-07-24 09:48:36+00:00,,0,3,triaged module: boolean tensor module: tensor creation,True
62141,Dedupe code in functional optim classes module: bootcamp triaged better-engineering pt_distributed_rampup module: ddp,2021-07-23 23:42:59+00:00,,1,1,module: bootcamp triaged better-engineering pt_distributed_rampup module: ddp,True
62130,"nn.DataParallel not working on A100 with nvidia-driver 470.42.01, Cuda 11.1, Debian 10 module: multi-gpu triaged module: data parallel",2021-07-23 21:40:01+00:00,,0,0,module: multi-gpu triaged module: data parallel,True
62094,"[FR] Support for non-Parameter, non-Buffer objects in state_dict module: nn triaged enhancement",2021-07-23 16:57:42+00:00,,0,20,module: nn triaged enhancement,True
62066,Memory Leak Found in Persistent DataLoader module: dataloader module: memory usage triaged,2021-07-22 23:43:27+00:00,,0,2,module: dataloader module: memory usage triaged,True
62034,cudnn_batch_norm_backward is extremely imprecise for some input shapes module: cudnn triaged,2021-07-22 16:45:33+00:00,,0,2,module: cudnn triaged,True
62032,"Create a boxed fallback / template recipe for Autograd that forwards, but errors on backwards module: internals module: autograd triaged module: dispatch",2021-07-22 15:53:10+00:00,,0,6,module: internals module: autograd triaged module: dispatch,True
62031,Caffe2 tensor to ATen conversion doesn't initialize PyTorch CUDA state caffe2 module: internals module: cuda triaged,2021-07-22 15:13:43+00:00,,0,0,caffe2 module: internals module: cuda triaged,True
62027,`x.to(memory_format=torch.contiguous_format)` does not always return a contiguous tensor module: docs triaged module: memory format needs design,2021-07-22 13:12:23+00:00,,0,22,module: docs triaged module: memory format needs design,True
62026,Should we be upcasting integral types to int64 in torch.sum and torch.prod? triaged module: numpy module: reductions module: python array api,2021-07-22 12:44:28+00:00,,0,3,triaged module: numpy module: reductions module: python array api,True
62021,[FX] Ability to provide autowrap function to Tracer init triaged module: fx,2021-07-22 09:17:02+00:00,,1,0,triaged module: fx,True
62016,ppc64le build fail: invalid conversion from Bfloat16 in functional_base.h:180 triaged module: POWER module: bfloat16,2021-07-22 06:56:02+00:00,,0,0,triaged module: POWER module: bfloat16,True
62011,Enable clang-tidy on all of master triaged,2021-07-22 02:56:41+00:00,,2,0,triaged,True
62009,multi-input and multi-output support for nn.Module module: nn triaged,2021-07-22 02:45:29+00:00,,0,2,module: nn triaged,True
62007,Add Future return from default process group collective communications oncall: distributed module: c10d,2021-07-22 01:38:48+00:00,,0,2,oncall: distributed module: c10d,False
61970,FX incorrectly provides `int` instead of `float` for `value` kwarg of `nn.functional.pad` triaged module: fx,2021-07-21 17:13:28+00:00,,1,5,triaged module: fx,True
61966,Bootcamp Task: Add `prim::to_mkldnn` to convert from aten's nchw to mkldnn's nChw8c directly oncall: jit module: bootcamp jit-backlog,2021-07-21 16:39:41+00:00,,0,3,oncall: jit module: bootcamp jit-backlog,True
61951,Vulkan: error -5 when trying to use it with Nvidia card module: android oncall: mobile module: vulkan,2021-07-21 11:28:30+00:00,,1,1,module: android oncall: mobile module: vulkan,True
61930,NNAPI delegate library cannot be built on MacOS module: build triaged module: macos,2021-07-20 20:06:50+00:00,,0,0,module: build triaged module: macos,True
61923,Calling `torch.save()` twice writes a corrupted file module: serialization triaged,2021-07-20 18:02:02+00:00,,0,0,module: serialization triaged,True
61920,RPC memory leak for CPU high priority triage review oncall: distributed,2021-07-20 17:20:46+00:00,,2,33,high priority triage review oncall: distributed,True
61919,RuntimeError: !ref.requires_grad()INTERNAL ASSERT FAILED oncall: jit triaged,2021-07-20 17:19:44+00:00,,0,1,oncall: jit triaged,True
61909,Support matrix operations between complex and real tensors triaged module: complex module: type promotion module: linear algebra,2021-07-20 15:42:56+00:00,,0,18,triaged module: complex module: type promotion module: linear algebra,True
61901,Reducing over empty dimensions for reductions without identity triaged module: numpy needs design module: reductions,2021-07-20 12:49:51+00:00,,0,4,triaged module: numpy needs design module: reductions,True
61890,The matrix multiplication operator can't get correct results on 3090 !! module: cuda triaged module: tf32,2021-07-20 05:00:28+00:00,,0,4,module: cuda triaged module: tf32,True
61843,Investigate fairscale's use of storage APIs and find alternatives triaged ezyang's list,2021-07-19 14:13:47+00:00,,0,0,triaged ezyang's list,True
61839,died with <Signals.SIGSEGV: 11> oncall: distributed triaged,2021-07-19 12:37:11+00:00,,0,1,oncall: distributed triaged,True
61835,Broadcasting documentation diverges from implementation module: docs module: bootcamp triaged,2021-07-19 10:35:37+00:00,,0,2,module: docs module: bootcamp triaged,True
61827, Bail out if `-std=c++` setting is detected during build module: build triaged,2021-07-19 06:20:35+00:00,,0,3,module: build triaged,True
61825,Improved OpInfo dtype testing module: tests triaged,2021-07-19 05:46:50+00:00,,0,0,module: tests triaged,True
61819,"[discussion] Support other index dtypes for scatter, scatter_reduce and other indexing functions in addition to int64: uint8, int16, int32 (without nya casting reallocations) triaged enhancement module: type promotion module: advanced indexing actionable module: scatter & gather ops",2021-07-18 19:41:32+00:00,,0,3,triaged enhancement module: type promotion module: advanced indexing actionable module: scatter & gather ops,True
61816,Better argument names for torch.atan2 and other math functions triaged module: numpy,2021-07-18 10:57:05+00:00,,0,11,triaged module: numpy,True
61781,Letting `_allgather_base` to support multiple tensors as inputs and outputs oncall: distributed triaged,2021-07-16 19:22:34+00:00,,0,7,oncall: distributed triaged,True
61772,Can I set the algorithm of torch.nn.Conv to CUDNN_CONVOLUTION FWD_ALGO_GEMM by myself? module: cudnn module: cuda module: convolution triaged,2021-07-16 16:58:02+00:00,,0,1,module: cudnn module: cuda module: convolution triaged,True
61765,How to save tensors on mobile (lite interpreter)? oncall: mobile,2021-07-16 09:43:03+00:00,,0,1,oncall: mobile,False
61758,`make_tensor` tracking issue triaged module: testing tracker,2021-07-16 04:39:07+00:00,,0,2,triaged module: testing tracker,True
61730,"Can't specify compiler, leading to SUPPORT_GLIBCXX_USE_C99 failed module: build triaged",2021-07-15 20:29:59+00:00,,0,9,module: build triaged,True
61727,```torch.distributions.Categorical``` unintended  ```log_prob``` gradient w.r.t ```probs``` module: distributions triaged,2021-07-15 20:17:57+00:00,,0,3,module: distributions triaged,True
61718,ATen codegen is always re-running for Makefile generator module: build triaged,2021-07-15 17:46:16+00:00,,2,2,module: build triaged,True
61709,[pytorch android] use Vulkan backend crash module: android oncall: mobile module: vulkan,2021-07-15 09:46:58+00:00,,0,3,module: android oncall: mobile module: vulkan,True
61695,miss header file while using python setup.py install to install pytorch with anaconda env module: build triaged,2021-07-15 04:23:22+00:00,,0,2,module: build triaged,True
61693,"Pytorch Mobile on Android, LAPACK library not found in compilation module: android oncall: mobile",2021-07-15 04:18:41+00:00,,0,4,module: android oncall: mobile,False
61686,[discussion] Remove the need of mandatory super() nn.Module's call module: nn triaged enhancement,2021-07-15 00:47:37+00:00,,0,13,module: nn triaged enhancement,True
61671,TCPStoreTest.testWatchKeyCallback Timed Out oncall: distributed triaged module: flaky-tests module: c10d,2021-07-14 19:11:46+00:00,,1,1,oncall: distributed triaged module: flaky-tests module: c10d,True
61665,Default Collate doesn't work for subtypes of ndarray module: dataloader triaged,2021-07-14 18:24:39+00:00,,1,1,module: dataloader triaged,True
61664,Support adding new keyword-only parameters without breaking FC module: internals triaged module: codegen needs design,2021-07-14 17:09:39+00:00,,0,3,module: internals triaged module: codegen needs design,True
61662,`super().__init__()` usage for `_Joinable` oncall: distributed triaged,2021-07-14 16:39:11+00:00,,0,0,oncall: distributed triaged,True
61661,Return `Future` instead of `Work` in `notify_join_context()` oncall: distributed triaged,2021-07-14 16:31:09+00:00,,0,0,oncall: distributed triaged,True
61660,Pytorch model load failure in Gunicorn with Gevent workers needs reproduction module: nn module: dataloader triaged module: numpy,2021-07-14 16:24:36+00:00,,0,8,needs reproduction module: nn module: dataloader triaged module: numpy,True
61659,Python program locks up when using some innocuous code inside mp.spawn() oncall: distributed triaged,2021-07-14 16:09:27+00:00,,0,8,oncall: distributed triaged,True
61658,Move `torch.cholesky_solve` into `torch.linalg`. triaged enhancement module: linear algebra,2021-07-14 15:55:01+00:00,,0,0,triaged enhancement module: linear algebra,True
61654,Allow creation of pseudo devices for testing purposes module: internals triaged,2021-07-14 15:17:37+00:00,,0,6,module: internals triaged,True
61653,Move `lobpcg` into `torch.linalg` triaged enhancement module: linear algebra,2021-07-14 15:16:25+00:00,,0,2,triaged enhancement module: linear algebra,True
61650,"Move `{svd,pca}_lowrank` into `torch.linalg` triaged enhancement module: linear algebra",2021-07-14 14:43:43+00:00,,0,0,triaged enhancement module: linear algebra,True
61649,Move `tensordot` into `torch.linalg` triaged enhancement module: linear algebra module: python array api,2021-07-14 14:34:53+00:00,,0,0,triaged enhancement module: linear algebra module: python array api,True
61645,`channel_shuffle` output is sometimes aliased with its input module: autograd module: nn triaged,2021-07-14 13:41:16+00:00,,0,0,module: autograd module: nn triaged,True
61636,Enhance Distributed `get_future()` docs oncall: distributed triaged better-engineering pt_distributed_rampup,2021-07-14 07:15:24+00:00,,0,0,oncall: distributed triaged better-engineering pt_distributed_rampup,True
61619,"When 'trapezoid' is called with an empty tensor input, it does not produce an output with requires_grad module: autograd triaged",2021-07-14 00:25:46+00:00,,0,0,module: autograd triaged,True
61614,Possibly flaky test: TestMultiprocessing.test_fd_sharing triaged module: flaky-tests,2021-07-14 00:01:48+00:00,,0,0,triaged module: flaky-tests,True
61591,Add `map_location` arg for c10d collective comms oncall: distributed triaged,2021-07-13 18:20:24+00:00,,0,0,oncall: distributed triaged,False
61585,"Proposal: allow using axis, axes, dim and dims interchangeably  triaged needs design module: ux module: python array api",2021-07-13 17:23:07+00:00,,0,3,triaged needs design module: ux module: python array api,True
61582,Many reduction operators do not support reducing over multiple dimensions triaged module: numpy module: reductions module: python array api,2021-07-13 15:44:56+00:00,,0,9,triaged module: numpy module: reductions module: python array api,True
61574,[nn] Passing dtype to `_stacklevel` argument in `log_softmax` silently works module: bc-breaking module: nn triaged module: deprecation module: functional UX topic: bc breaking,2021-07-13 07:00:18+00:00,,0,2,module: bc-breaking module: nn triaged module: deprecation module: functional UX topic: bc breaking,True
61544,`__torch_function__` always records default kwargs as kwargs triaged module: __torch_function__,2021-07-12 17:56:04+00:00,,0,6,triaged module: __torch_function__,True
61538,The use of epsilon in torch.nn.functional.pairwise_distance leads to incorrect minima of the distances module: nn triaged module: correctness (silent) module: distance functions,2021-07-12 16:25:32+00:00,,0,1,module: nn triaged module: correctness (silent) module: distance functions,True
61528,Function Request: scipy.ndimage.map_coordinates  triaged module: numpy function request module: interpolation,2021-07-12 12:16:36+00:00,,0,0,triaged module: numpy function request module: interpolation,True
61525,"I have graphic card of P4, but torch.cuda.is_available() always shows false module: cuda triaged",2021-07-12 08:22:53+00:00,,0,1,module: cuda triaged,True
61523,F.nll_loss with 16-bit CUDA tensors and reduction=mean produces NaNs module: numerical-stability module: loss triaged module: NaNs and Infs module: reductions,2021-07-12 07:32:06+00:00,,0,3,module: numerical-stability module: loss triaged module: NaNs and Infs module: reductions,True
61516,MemoryError with pip Install even with --no-cache-dir option module: binaries triaged,2021-07-11 13:10:28+00:00,,0,2,module: binaries triaged,True
61515,Buggy page with overlapped tabs module: docs triaged,2021-07-11 11:11:44+00:00,,0,3,module: docs triaged,True
61513,Adding 'differentiable' section to each function docstring module: docs module: autograd triaged,2021-07-11 09:21:12+00:00,,0,3,module: docs module: autograd triaged,False
61510,What is find_package(Torch REQUIRED) doing that a manual include/glob doesnt? module: build triaged,2021-07-10 18:35:09+00:00,,0,1,module: build triaged,True
61509,[feature request] Exact euclidean distance transform triaged function request module: distance functions,2021-07-10 11:47:19+00:00,,0,0,triaged function request module: distance functions,False
61503,UCI Data Sets feature module: dataloader triaged,2021-07-10 03:00:23+00:00,,0,1,feature module: dataloader triaged,True
61492,Replace unbiased parameter in torch.(std|var|std_mean|var_mean) with correction=0 triaged module: deprecation module: reductions module: python array api,2021-07-09 22:47:32+00:00,,1,2,triaged module: deprecation module: reductions module: python array api,True
61490,Deprecate torch.(min|max|median|mode) to only return values and not indices triaged module: numpy module: deprecation needs design module: reductions module: python array api,2021-07-09 22:41:18+00:00,,0,16,triaged module: numpy module: deprecation needs design module: reductions module: python array api,True
61486,Some reduction operators have double signatures triaged module: deprecation module: reductions module: python array api,2021-07-09 22:17:33+00:00,,0,4,triaged module: deprecation module: reductions module: python array api,True
61485,CompositeImplicitAutograd ops should not call out= variants of operators module: autograd triaged,2021-07-09 21:51:08+00:00,,0,0,module: autograd triaged,True
61474,Implement missing torch.nan* operators triaged module: numpy module: NaNs and Infs module: reductions tracker,2021-07-09 19:40:33+00:00,,0,10,triaged module: numpy module: NaNs and Infs module: reductions tracker,True
61470,torch.nn.utils.weight_norm fails with DDP high priority oncall: distributed triaged module: ddp,2021-07-09 18:57:01+00:00,,0,5,high priority oncall: distributed triaged module: ddp,True
61468,Version is set to 1.8.0 in the torch tag v1.8.1 module: build oncall: releng triaged,2021-07-09 18:19:56+00:00,,0,1,module: build oncall: releng triaged,True
61464,Allow `ScriptModule`s to be symbolically traced triaged FX-TorchScript Compatibility module: fx,2021-07-09 16:57:31+00:00,,1,0,triaged FX-TorchScript Compatibility module: fx,True
61457,using DistributedSampler   occur RuntimeError: Expected a 'cuda:0' generator device but found 'cpu' oncall: distributed module: dataloader triaged,2021-07-09 14:24:30+00:00,,0,1,oncall: distributed module: dataloader triaged,True
61453,"Reduce with any(), all(), median() over multiple dimensions triaged enhancement module: numpy module: reductions",2021-07-09 13:19:08+00:00,,0,1,triaged enhancement module: numpy module: reductions,True
61452,New module: Split log softmax with loss feature module: nn triaged,2021-07-09 12:14:51+00:00,,0,2,feature module: nn triaged,True
61417,Reductions tracking issue triaged module: numpy module: reductions tracker,2021-07-08 15:13:54+00:00,,0,0,triaged module: numpy module: reductions tracker,True
61411,"I do have the GPU, but the print(torch.cuda.is_available()) always shows False module: cuda triaged",2021-07-08 13:59:14+00:00,,0,3,module: cuda triaged,True
61410,requires grad get lost during transform. module: autograd triaged module: deprecation module: tensor creation,2021-07-08 12:48:17+00:00,,0,3,module: autograd triaged module: deprecation module: tensor creation,True
61384,Merge fork of upload-artifact-s3 to a single commit triaged better-engineering,2021-07-07 23:04:57+00:00,,0,0,triaged better-engineering,True
61344,Get TensorType's device in python oncall: jit module: bootcamp,2021-07-07 11:44:57+00:00,,0,3,oncall: jit module: bootcamp,False
61343,Feature Request: Remove Optimizer Lazy State Initialization module: optimizer triaged needs research,2021-07-07 11:38:42+00:00,,0,0,module: optimizer triaged needs research,False
61341,inerror: reference to ‘DeviceType’ is ambiguous needs reproduction module: build module: cpp triaged,2021-07-07 09:58:54+00:00,,0,4,needs reproduction module: build module: cpp triaged,True
61309,The class weights implementation is incorrect module: nn module: loss triaged,2021-07-06 21:43:26+00:00,,0,14,module: nn module: loss triaged,True
61292,torch.linspace tensor support triaged enhancement module: numpy,2021-07-06 14:07:10+00:00,,0,3,triaged enhancement module: numpy,True
61289,Zero seed in DistributedSampler oncall: distributed triaged,2021-07-06 13:27:29+00:00,,0,1,oncall: distributed triaged,True
61281,DataLoader with IterativeDataset throws an error when providing a BatchSampler module: dataloader triaged,2021-07-06 10:26:33+00:00,,0,5,module: dataloader triaged,True
61268,Unknown type name '__torch__.torch.classes.metal.Conv2dOpContext' module: convolution module: macos oncall: mobile module: ios,2021-07-06 03:03:33+00:00,,0,1,module: convolution module: macos oncall: mobile module: ios,True
61267,Incorrect trace with MKLDNN (adding scalar) oncall: jit module: mkldnn,2021-07-06 02:22:50+00:00,,0,1,oncall: jit module: mkldnn,True
61261,libtorch_cpu.so is getting too large. oncall: releng triaged,2021-07-05 17:20:56+00:00,,0,3,oncall: releng triaged,True
61260,libtorch_cpu.so: undefined symbol: _ZN3c1010ThreadPool3runESt8functionIFvvEE module: build module: abi triaged,2021-07-05 15:51:20+00:00,,0,1,module: build module: abi triaged,True
61246,"torch.quantization.fx.fuser forget to pass ""additional_fuser_method_mapping"" to fuse method oncall: quantization low priority triaged module: fx",2021-07-05 07:20:34+00:00,,1,2,oncall: quantization low priority triaged module: fx,True
61244,error: invalid conversion in vec256_bfloat16.h module: build triaged,2021-07-05 03:57:22+00:00,,0,9,module: build triaged,True
61243,Pytorch 1.5+ is slower than pytorch 1.3 needs reproduction module: performance triaged,2021-07-05 03:10:41+00:00,,0,8,needs reproduction module: performance triaged,False
61233,[typing] ModuleDict ctor has wrong signature module: typing triaged,2021-07-04 20:27:29+00:00,,0,0,module: typing triaged,True
61231,Document torch.Size and its methods (like torch.Size.numel) module: docs triaged,2021-07-04 19:01:55+00:00,,0,5,module: docs triaged,True
61223,optimize_for_inference does not handle MKLDNN for BatchNorm1d high priority oncall: jit triaged,2021-07-04 13:52:57+00:00,,0,2,high priority oncall: jit triaged,True
61222,The syncbatchnorm can not  be employed in the MODEL that the gradients including in the loss function oncall: distributed module: autograd triaged module: derivatives module: norms and normalization,2021-07-04 13:21:59+00:00,,0,9,oncall: distributed module: autograd triaged module: derivatives module: norms and normalization,True
61221,Unable to add a scalar to an MKLDNN tensor oncall: jit module: mkldnn module: intel,2021-07-04 12:43:05+00:00,,0,9,oncall: jit module: mkldnn module: intel,True
61213,[feature request] Low pass filtering of FFT results triaged enhancement module: fft,2021-07-03 19:24:55+00:00,,0,0,triaged enhancement module: fft,True
61211,[feature request] Do zero-padding in high-frequency modes in `ifft` triaged enhancement module: fft,2021-07-03 19:02:04+00:00,,0,0,triaged enhancement module: fft,True
61210,Jitted function cannot be pickled oncall: jit good first issue OSS contribution wanted,2021-07-03 17:48:21+00:00,,1,9,oncall: jit good first issue OSS contribution wanted,True
61186,possible SummaryWriter memory leak triaged module: tensorboard,2021-07-02 22:35:06+00:00,,0,0,triaged module: tensorboard,True
61182,"Feature Request: torch.functional.interpolate to quietly ignore ""align_corners"" when mode is set to ""nearest"" module: nn triaged enhancement module: interpolation",2021-07-02 20:29:06+00:00,,0,2,module: nn triaged enhancement module: interpolation,True
61173, Increased memory usage with AMP  module: cuda triaged module: amp (automated mixed precision),2021-07-02 12:07:45+00:00,,0,2,module: cuda triaged module: amp (automated mixed precision),True
61167,Torch .pth model export to ONNX returned empty graph module: onnx triaged,2021-07-02 06:42:49+00:00,,1,1,module: onnx triaged,False
61149,`pad(mode='reflect')` beyond `input.size` module: nn triaged enhancement module: padding,2021-07-01 23:29:36+00:00,,0,4,module: nn triaged enhancement module: padding,True
61136,rename(**{}) should throw no error triaged module: named tensor,2021-07-01 19:17:04+00:00,,0,4,triaged module: named tensor,True
61122,Make copy_ use dispatcher module: bootcamp triaged,2021-07-01 14:21:15+00:00,,0,9,module: bootcamp triaged,True
61117,flatten renames in place triaged module: named tensor,2021-07-01 14:05:36+00:00,,0,0,triaged module: named tensor,True
61116,[custom op]RuntimeError: Error compiling objects for extension module: build module: cpp-extensions triaged,2021-07-01 13:24:20+00:00,,0,1,module: build module: cpp-extensions triaged,True
61110,Incorrect exponential calculation on Jetson devices with float32 dtype module: cuda triaged module: arm module: jetson,2021-07-01 09:48:59+00:00,,0,12,module: cuda triaged module: arm module: jetson,True
61079,"[clang-tidy] Add a custom rule for AT_ERROR -> TORCH_CHECK(false, ...) triaged",2021-06-30 23:26:48+00:00,,0,0,triaged,False
61046,Audit use of __ARM_NEON__ define oncall: quantization low priority triaged enhancement,2021-06-30 16:19:25+00:00,,1,3,oncall: quantization low priority triaged enhancement,True
61041,Profile saving is slow oncall: profiler,2021-06-30 13:34:34+00:00,,0,5,oncall: profiler,False
61030,[Docker] Incompatible torchvision for 1.9.0-cuda10.2-cudnn7-runtime tag. triaged module: vision module: docker,2021-06-30 08:01:08+00:00,,0,2,triaged module: vision module: docker,True
60999,Merge seemethere/upload-artifact-s3 and driazati/upload-artifact-s3 triaged better-engineering,2021-06-29 23:14:02+00:00,,0,0,triaged better-engineering,True
60997,Dispatcher doesn't handle ops with an empty list of tensors (e.g. `torch.cat()`) triaged module: dispatch,2021-06-29 22:10:40+00:00,,0,1,triaged module: dispatch,True
60991,[NNC] APIs needed to bridge scheduling gap in reductions NNC,2021-06-29 21:25:58+00:00,,1,0,NNC,False
60983,Automate bumping of `clang-tidy` docker image tag on CI module: ci triaged module: infra,2021-06-29 20:23:44+00:00,,0,2,module: ci triaged module: infra,True
60977,[package] pattern usage can potentially create broken packages triaged oncall: package/deploy imported,2021-06-29 19:37:48+00:00,,0,0,triaged oncall: package/deploy imported,True
60971,jit scripting for the parametrizations oncall: jit module: nn.utils.parametrize,2021-06-29 18:48:11+00:00,,1,3,oncall: jit module: nn.utils.parametrize,True
60956,Naming convention for variants of operations that never short circuit feature triaged module: ux,2021-06-29 14:40:03+00:00,,0,7,feature triaged module: ux,True
60953,Add support for Python's optimized mode to torchscript oncall: jit,2021-06-29 14:28:31+00:00,,0,3,oncall: jit,False
60950,Singular build setting CLANG_CXX_LANGUAGE_STANDARD has different values module: build triaged module: macos,2021-06-29 14:09:11+00:00,,0,0,module: build triaged module: macos,True
60941,Incorrect dtype cast for binary ops w/ mixed ComplexFloat + Double operands triaged module: complex module: type promotion,2021-06-29 10:39:49+00:00,,0,2,triaged module: complex module: type promotion,True
60939,Functional multi_head_attention_forward softmax get nan for fp16 mode oncall: transformer/mha,2021-06-29 09:43:20+00:00,,0,0,oncall: transformer/mha,False
60937,[jit] About traced script module using in c++ oncall: jit,2021-06-29 09:22:37+00:00,,0,9,oncall: jit,True
60936,The usage of get_stoi() in 0.10.0 and stoi[] in previous versions needs reproduction triaged,2021-06-29 09:06:44+00:00,,0,1,needs reproduction triaged,False
60933,jit.load error when changed Folder Name module: serialization triaged,2021-06-29 06:57:34+00:00,,0,3,module: serialization triaged,True
60932,Is Python version of Docker image on DockerHub downgraded? triaged module: docker,2021-06-29 06:12:59+00:00,,0,4,triaged module: docker,True
60930,error for using faster rcnn on c++ & GPU needs reproduction oncall: jit,2021-06-29 05:49:09+00:00,,0,1,needs reproduction oncall: jit,True
60924,[rfc][local lint] Create lint runner as a setup.py target triaged better-engineering,2021-06-29 03:00:01+00:00,,0,1,triaged better-engineering,True
60923,RuntimeError: derivative for im2col_backward is not implemented needs reproduction module: autograd module: nn triaged,2021-06-29 02:24:22+00:00,,0,1,needs reproduction module: autograd module: nn triaged,True
60916,Scipy 1.7.0 may cause some test failures module: distributions module: tests triaged,2021-06-28 23:31:17+00:00,,0,0,module: distributions module: tests triaged,True
60911,[elastic launcher] redirects/tee support for global rank triaged module: elastic,2021-06-28 22:12:30+00:00,,0,0,triaged module: elastic,True
60904,F.max_pool1d docs are really bad module: docs module: nn triaged module: pooling,2021-06-28 21:03:34+00:00,,1,4,module: docs module: nn triaged module: pooling,True
60858,Sparse CSR layout CPU backend tracking issue module: sparse triaged module: mkl tracker,2021-06-28 11:10:44+00:00,,1,0,module: sparse triaged module: mkl tracker,True
60854,Sparse CSR layout GPU backend tracking issue module: sparse module: cuda triaged tracker,2021-06-28 09:15:47+00:00,,1,7,module: sparse module: cuda triaged tracker,True
60848,Fail to build with gcc11 module: build triaged module: xnnpack,2021-06-28 05:16:39+00:00,,0,0,module: build triaged module: xnnpack,True
60847,How to release CPU memory cache in Libtorch JIT ? oncall: jit,2021-06-28 03:40:21+00:00,,0,2,oncall: jit,False
60844,DDP fails if you have multiple forward passes and a single backwards pass with `find_unused_parameters=True` oncall: distributed module: ddp,2021-06-28 02:54:54+00:00,,0,2,oncall: distributed module: ddp,True
60843,PyTorch's last 1 year update became unfriendly due to the lack of examples module: docs triaged,2021-06-28 02:54:06+00:00,,0,1,module: docs triaged,False
60835,"Pytorch V1.1 runs OK, but Pytorch v1.5 to 1.9 run wrong needs reproduction module: autograd triaged module: regression",2021-06-27 17:39:55+00:00,,0,1,needs reproduction module: autograd triaged module: regression,True
60834,complie error when i use python setup.py build on arm computer module: build triaged module: arm,2021-06-27 12:33:42+00:00,,0,1,module: build triaged module: arm,True
60832,State of Torch Named Tensors triaged module: named tensor,2021-06-27 09:15:44+00:00,,0,6,triaged module: named tensor,True
60825,Where OpInfo doesn't handle cases where one of the inputs is a scalar module: tests triaged module: sorting and selection,2021-06-26 20:37:52+00:00,,0,1,module: tests triaged module: sorting and selection,True
60802,Bug in `pytorch/benchmarks/tensorexpr/pt_engine.py` triaged op-bench,2021-06-26 00:13:04+00:00,,0,1,triaged op-bench,True
60796,AccumulateGrad short circuits before calling variable hooks when the incoming grad is None module: autograd triaged,2021-06-25 23:33:27+00:00,,0,0,module: autograd triaged,True
60783,[torch.profiler] double counting CUDA wrapper self-cuda-time oncall: profiler,2021-06-25 20:49:03+00:00,,0,6,oncall: profiler,False
60766,"_foreach_maximum returns ""incompatible type"" mypy error unexpectedly module: typing triaged",2021-06-25 19:21:48+00:00,,0,0,module: typing triaged,True
60748,"Record file/line number when creating test data, and then report it in backtraces associated with this data feature module: tests triaged",2021-06-25 15:44:37+00:00,,0,2,feature module: tests triaged,True
60732,Potential memory leaks reported by static analysis tool oncall: jit,2021-06-25 09:49:48+00:00,,0,1,oncall: jit,False
60727,[ONNX] export affine_grid_generator module: onnx triaged OSS contribution wanted function request onnx-needs-info,2021-06-25 07:36:44+00:00,,0,1,module: onnx triaged OSS contribution wanted function request onnx-needs-info,False
60724,bilinear interpolate is very slow under mixed precision training mode. module: performance module: nn module: cuda triaged module: amp (automated mixed precision) module: interpolation,2021-06-25 06:26:54+00:00,,0,8,module: performance module: nn module: cuda triaged module: amp (automated mixed precision) module: interpolation,True
60674,Turn on -Winfinite-recursion in OSS CI module: build triaged better-engineering,2021-06-24 18:55:47+00:00,,0,0,module: build triaged better-engineering,True
60640,Building fails on a node with libibverbs installed with no Infiniband hardware present on RHEL8. oncall: distributed module: tensorpipe,2021-06-24 09:21:26+00:00,,0,2,oncall: distributed module: tensorpipe,False
60628,pip install nightly torch torchaudio and torchvision together will install 0.3.0 version of torchvsion on Ubuntu high priority module: binaries triaged,2021-06-24 05:27:43+00:00,,0,5,high priority module: binaries triaged,True
60626,Impossible to raise the limit on number of shared memory tensors high priority module: multiprocessing triaged,2021-06-24 04:27:58+00:00,,0,11,high priority module: multiprocessing triaged,True
60594,Install PyTorch on Windows with Conda and other Torch packages with cpuonly fails module: binaries triaged,2021-06-23 22:28:43+00:00,,0,1,module: binaries triaged,True
60561,nn.CrossMapLRN2d is missing from docs module: docs module: nn triaged,2021-06-23 18:33:47+00:00,,0,0,module: docs module: nn triaged,False
60548,Ensure warnings relate to user code with stacklevel triaged better-engineering module: deprecation,2021-06-23 16:02:08+00:00,,1,9,triaged better-engineering module: deprecation,True
60541,torch.cuda.Event(blocking=True) doesn't work module: cuda triaged,2021-06-23 14:08:20+00:00,,0,3,module: cuda triaged,True
60531,Enhanced torch.chunk and torch.split  good first issue triaged enhancement,2021-06-23 11:33:26+00:00,,0,18,good first issue triaged enhancement,True
60520,[docs] torch.log_softmax is undocumented module: docs triaged,2021-06-23 07:32:50+00:00,,0,3,module: docs triaged,False
60518,different gpus to train oncall: distributed,2021-06-23 07:15:53+00:00,,0,5,oncall: distributed,False
60477,ROCm miopenStatusInternalError /MIOpen/src/sqlite_db.cpp:109: open memvfs: unable to open database file needs reproduction module: rocm triaged,2021-06-22 18:18:02+00:00,,0,12,needs reproduction module: rocm triaged,True
60466,PyTorch unfold could be faster module: performance triaged,2021-06-22 16:13:53+00:00,,0,4,module: performance triaged,True
60459,[Mkldnn] has_bf16 check only works on Linux for tests module: tests triaged module: mkldnn,2021-06-22 13:15:25+00:00,,0,4,module: tests triaged module: mkldnn,True
60448,Enable large file support module: build feature triaged,2021-06-22 09:13:07+00:00,,0,0,module: build feature triaged,True
60440,DDP with cuda rpc failed with DistributedOptimizer Adagrad oncall: distributed module: optimizer,2021-06-22 06:09:04+00:00,,0,6,oncall: distributed module: optimizer,False
60431,Implement simple view methods for TensorAccessor/PackedTensorAccessor module: bootcamp triaged,2021-06-22 02:40:43+00:00,,0,2,module: bootcamp triaged,True
60426,"Output of non-inplace, non-CompositeImplicitAutograd op has TensorImpl > 1 or StorageImpl use_count != 1 module: autograd triaged",2021-06-22 00:58:00+00:00,,1,5,module: autograd triaged,True
60401,Need workaround to support multiprocess CUDA tensor sharing on Jetson Platforms module: multiprocessing feature triaged,2021-06-21 21:15:42+00:00,,0,1,module: multiprocessing feature triaged,True
60400,Using inbuilt function name `input` as variable names in examples module: docs triaged,2021-06-21 21:11:48+00:00,,0,0,module: docs triaged,False
60381,broken source link / missing anchors module: docs triaged,2021-06-21 17:39:41+00:00,,0,6,module: docs triaged,False
60372,Docs for `Installing C++ Distributions of PyTorch` needs update module: docs triaged,2021-06-21 15:47:30+00:00,,0,1,module: docs triaged,False
60359,Expose the .pt model file path during TorchScript custom class Serialization/Deserialization oncall: jit,2021-06-21 10:35:37+00:00,,0,2,oncall: jit,False
60354,test_old_models_bc is flaky high priority triage review oncall: jit module: flaky-tests,2021-06-21 08:52:29+00:00,,1,2,high priority triage review oncall: jit module: flaky-tests,True
60343,What changes we need to make in metrics calculation and visualization part when we use Distributed Data Parallel for distributed training oncall: distributed triaged module: ddp,2021-06-20 22:10:22+00:00,,1,2,oncall: distributed triaged module: ddp,True
60341,C++ version 1.9.0 libtorch dynamic load fails -- GCC only needs reproduction module: binaries module: cpp module: abi triaged,2021-06-20 15:43:09+00:00,,0,10,needs reproduction module: binaries module: cpp module: abi triaged,True
60334,USE_SYSTEM_SLEEF: undefined reference to symbol 'Sleef_expd4_u10' module: build triaged module: sleef,2021-06-19 23:07:51+00:00,,0,0,module: build triaged module: sleef,True
60333,Two-element ModuleList results in error in inference_mode when jit'ed oncall: jit,2021-06-19 22:36:00+00:00,,1,4,oncall: jit,True
60329,USE_SYSTEM_CPUINFO: internal symbol `clog_vlog_fatal' isn't defined module: build triaged,2021-06-19 16:35:48+00:00,,0,1,module: build triaged,True
60306,Divergent code is needed to record usage streams on different TensorImpl types module: sparse module: cuda triaged module: vmap,2021-06-18 22:07:42+00:00,,0,4,module: sparse module: cuda triaged module: vmap,True
60295,Optimize torch.einsum module: performance triaged module: linear algebra,2021-06-18 19:49:13+00:00,,0,4,module: performance triaged module: linear algebra,True
60294,Add a NumPy-like `pad` function triaged module: numpy function request module: padding,2021-06-18 19:43:44+00:00,,1,13,triaged module: numpy function request module: padding,True
60288,provide hashsum for downloads module: binaries triaged,2021-06-18 18:10:07+00:00,,0,0,module: binaries triaged,True
60277,Sparse CSR tensor should not accept equal column indices in the same row module: sparse module: error checking triaged,2021-06-18 17:02:25+00:00,,0,2,module: sparse module: error checking triaged,True
60264,Subtensor operations like `tril/triu` to have an option to respect the strides of the input triaged module: memory format module: linear algebra,2021-06-18 12:29:31+00:00,,0,3,triaged module: memory format module: linear algebra,True
60261,Mixed logical indexing / numerical indexing fails. triaged module: advanced indexing,2021-06-18 10:32:14+00:00,,0,0,triaged module: advanced indexing,True
60252,UserWarning: Failed to initialize NumPy: No module named 'numpy' oncall: package/deploy imported,2021-06-18 06:42:41+00:00,,0,1,oncall: package/deploy imported,False
60234,Numerical-reproducibility issue in torch.matmul module: cuda triaged module: numerical-reproducibility,2021-06-18 03:22:20+00:00,,0,15,module: cuda triaged module: numerical-reproducibility,True
60228,cannot build with tensorrt module: build caffe2 module: cuda triaged,2021-06-18 00:41:38+00:00,,0,4,module: build caffe2 module: cuda triaged,True
60223,[testing] SkipInfo should error if `cls_name` and `test_name` combination is not valid module: tests triaged needs design,2021-06-17 23:11:26+00:00,,0,1,module: tests triaged needs design,True
60210,Quantized model using `boolean_dispatch` not picklable oncall: quantization low priority triaged,2021-06-17 21:20:13+00:00,,1,6,oncall: quantization low priority triaged,True
60189,[package] Tutorial not accessible oncall: package/deploy imported,2021-06-17 18:30:21+00:00,,0,1,oncall: package/deploy imported,False
60182,Improvement to CUDA mem leak check module: ci module: tests triaged,2021-06-17 15:52:20+00:00,,0,1,module: ci module: tests triaged,True
60172,[Android] Upgrading to 1.9.0 causes NNAPI model loading to fail module: android oncall: mobile,2021-06-17 09:11:41+00:00,,0,12,module: android oncall: mobile,True
60165,torch.load non backwards compatible on Transformer between 1.8.1 and 1.9.0 module: serialization triaged,2021-06-17 07:50:50+00:00,,0,7,module: serialization triaged,True
60164,Training goes wrong with amp and no_grad since pytorch 1.8 module: cuda triaged module: amp (automated mixed precision),2021-06-17 07:27:06+00:00,,0,3,module: cuda triaged module: amp (automated mixed precision),True
60159,Some ops throw runtime errors when called with complex tensors that require_grad but work when requires_grad=False module: autograd triaged module: complex complex_autograd,2021-06-17 06:12:10+00:00,,0,6,module: autograd triaged module: complex complex_autograd,True
60156,PEP-585 type annotations not supported for class-level annotations oncall: jit triaged,2021-06-17 05:14:10+00:00,,0,4,oncall: jit triaged,True
60153,[oneline docs] adding a source link for non-python functions module: docs triaged,2021-06-17 04:47:54+00:00,,0,2,module: docs triaged,False
60149,backward compatibility - need a way to find out when a certain API was added/modified/etc. module: docs module: internals triaged module: doc infra better-engineering module: deprecation module: codegen,2021-06-17 04:43:48+00:00,,0,15,module: docs module: internals triaged module: doc infra better-engineering module: deprecation module: codegen,True
60146,[torch.profiler] enhancements + corrections triaged enhancement oncall: profiler,2021-06-17 02:49:22+00:00,,3,8,triaged enhancement oncall: profiler,True
60143,"torch.jit.save gives error - RuntimeError: Could not export Python function call 'NMSop'. Remove calls to Python functions before export. Did you forget to add @script or @script_method annotation? If this is a nn.ModuleList, add it to __constants__ oncall: jit",2021-06-17 01:17:19+00:00,,0,3,oncall: jit,True
60109,test_remote_module_py_pickle_not_supported_script is flaky oncall: distributed module: flaky-tests module: rpc,2021-06-16 17:44:11+00:00,,1,1,oncall: distributed module: flaky-tests module: rpc,False
60102,Allow packaging of classes defined in a notebook triaged,2021-06-16 16:43:51+00:00,,0,1,triaged,True
60100,test_forward_async_script is flaky oncall: distributed module: flaky-tests module: rpc,2021-06-16 16:29:07+00:00,,1,1,oncall: distributed module: flaky-tests module: rpc,False
60096,Tutorials that require `pip install` not loading correctly in Colab module: docs triaged,2021-06-16 14:44:09+00:00,,0,4,module: docs triaged,True
60094,bundled libiomp5 causing segfaults in other libraries that use libomp needs reproduction module: binaries triaged module: macos module: openmp,2021-06-16 14:20:56+00:00,,0,12,needs reproduction module: binaries triaged module: macos module: openmp,True
60092,`memory_format` flag for `tensor.new_empty()`.  triaged enhancement module: memory format,2021-06-16 14:17:34+00:00,,0,0,triaged enhancement module: memory format,True
60089,torch.package: TypeError: 'NoneType' object is not iterable oncall: package/deploy imported,2021-06-16 09:26:44+00:00,,0,4,oncall: package/deploy imported,False
60084,Profiler does not contain NCCL kernel high priority triaged module: regression oncall: profiler,2021-06-16 08:04:07+00:00,,3,3,high priority triaged module: regression oncall: profiler,True
60083,caffe_translator.py Unable to convert Power Layer caffe2,2021-06-16 07:46:33+00:00,,0,2,caffe2,False
60081,[Bug] numpy is no longer a required dependency high priority triaged module: numpy,2021-06-16 07:32:19+00:00,,2,10,high priority triaged module: numpy,True
60069,Symbolic trace with *args triaged module: fx,2021-06-16 04:05:13+00:00,,0,4,triaged module: fx,True
60063,Replace native NHWC BN kernel triaged module: memory format,2021-06-16 02:07:27+00:00,,0,4,triaged module: memory format,True
60062,[c10d] Make `broadcast_object_list` accept a device parameter oncall: distributed module: bootcamp,2021-06-16 01:42:31+00:00,,0,0,oncall: distributed module: bootcamp,False
60028,RReLU and PReLU do not propagate input strides to outputs module: nn triaged module: memory format,2021-06-15 17:33:09+00:00,,0,0,module: nn triaged module: memory format,True
60008,Dynamic quantized LSTM does not support output projection now. oncall: quantization low priority triaged,2021-06-15 08:42:28+00:00,,1,1,oncall: quantization low priority triaged,True
59936,Can't perform any operation on Vulkan device - macOS M1 triaged module: macos module: arm module: vulkan,2021-06-14 12:58:42+00:00,,0,3,triaged module: macos module: arm module: vulkan,True
59935,Error /usr/local/lib/libopenblas.so: error adding symbols: File in wrong format while building pytorch for ppc64le module: build triaged module: POWER,2021-06-14 12:03:57+00:00,,0,1,module: build triaged module: POWER,True
59925,Implicit module registration permits silent programming errors in torch.nn.Module module: nn triaged,2021-06-13 22:59:24+00:00,,0,2,module: nn triaged,True
59923,Lazy modules should accept keyword arguments to forward method module: nn triaged actionable,2021-06-13 21:37:34+00:00,,0,3,module: nn triaged actionable,True
59905,[JIT] batch operators in training/inference  oncall: jit,2021-06-12 01:39:46+00:00,,0,2,oncall: jit,True
59896,Autograd engine current graph_task should be in the TLSState module: autograd triaged,2021-06-11 21:27:56+00:00,,0,3,module: autograd triaged,True
59883,"matmul causing cuDNN error: CUDNN_STATUS_INTERNAL_ERROR at future, unrelated Conv2d needs reproduction module: cudnn module: cuda triaged",2021-06-11 17:47:43+00:00,,0,4,needs reproduction module: cudnn module: cuda triaged,True
59875,[ROCm] test_gather_stress_cuda is flaky module: rocm triaged module: flaky-tests,2021-06-11 15:56:27+00:00,,0,1,module: rocm triaged module: flaky-tests,True
59868,Python scalars should be promoted to the same `dtype` as the respective tensor triaged module: type promotion needs design module: python array api,2021-06-11 07:52:01+00:00,,0,6,triaged module: type promotion needs design module: python array api,True
59865,Custom build of c++ libtorch for Android program or dynamic so lib does not reduce program or lib size module: android oncall: mobile,2021-06-11 06:32:19+00:00,,0,2,module: android oncall: mobile,False
59863,Sum issues with FP32 module: numerical-stability module: cpu triaged module: multithreading module: reductions,2021-06-11 05:14:44+00:00,,1,3,module: numerical-stability module: cpu triaged module: multithreading module: reductions,True
59855,pytorch test failed module: autograd module: cuda module: tests triaged,2021-06-11 01:20:34+00:00,,0,5,module: autograd module: cuda module: tests triaged,True
59854,Misreported linting messages module: lint triaged,2021-06-11 01:07:12+00:00,,0,0,module: lint triaged,True
59823,print_regression should handle sharded test properly oncall: distributed better-engineering,2021-06-10 20:34:32+00:00,,0,2,oncall: distributed better-engineering,False
59822,TransformedDistribution's log_prob gradient is inconsistent w.r.t. cache_size module: distributions triaged,2021-06-10 20:24:03+00:00,,0,0,module: distributions triaged,True
59806,Context manager for torch.jit.ignore is not covered by test coverage  oncall: jit,2021-06-10 18:28:00+00:00,,1,1,oncall: jit,False
59787,Remove support for multiple ellipses in slicing triaged module: advanced indexing module: deprecation needs design module: python array api,2021-06-10 11:48:40+00:00,,0,2,triaged module: advanced indexing module: deprecation needs design module: python array api,True
59786,Support negative step sizes for slicing triaged module: python array api,2021-06-10 11:44:07+00:00,,0,6,triaged module: python array api,True
59783,[Feature Request] Deterministic implementation for AdaptiveMaxpool1d. module: nn triaged enhancement module: determinism module: pooling,2021-06-10 10:45:48+00:00,,0,3,module: nn triaged enhancement module: determinism module: pooling,True
59778,CUDA Memory Error: PyTorch doen't allocate memory even though it's available module: cuda module: memory usage triaged,2021-06-10 07:46:21+00:00,,0,0,module: cuda module: memory usage triaged,True
59745,[c10d] Work objects should have a general operator<<  high priority oncall: distributed triaged better-engineering pt_distributed_rampup module: c10d,2021-06-09 21:10:09+00:00,,0,2,high priority oncall: distributed triaged better-engineering pt_distributed_rampup module: c10d,True
59743,m.fallback(torch::CppFunction::makeFromBoxedFunction<&my_fallback>) gives bad error message module: internals module: cpp triaged module: assert failure,2021-06-09 20:50:46+00:00,,0,0,module: internals module: cpp triaged module: assert failure,True
59730,Quantized conv2d with dilation and groups much slower than float32 module: performance oncall: quantization low priority module: convolution triaged,2021-06-09 19:49:51+00:00,,1,10,module: performance oncall: quantization low priority module: convolution triaged,True
59723,Segmentation fault when using CUDA with RNN high priority needs reproduction module: crash module: rnn module: cuda triaged,2021-06-09 18:20:47+00:00,,0,11,high priority needs reproduction module: crash module: rnn module: cuda triaged,True
59692,"cuda streams run sequentially, expected to run parallel module: performance module: cuda triaged",2021-06-09 08:01:38+00:00,,0,18,module: performance module: cuda triaged,True
59690,Inference ran on new thread leak memory on Android module: memory usage module: android oncall: mobile,2021-06-09 07:52:15+00:00,,0,7,module: memory usage module: android oncall: mobile,False
59682,Add a lint rule to recommend TORCH_CHECK(false) over throw std::runtime_error triaged better-engineering,2021-06-09 00:37:22+00:00,,0,1,triaged better-engineering,True
59652,pin_memory mutates its input but this is not reflected in JIT schema oncall: jit,2021-06-08 19:57:47+00:00,,0,0,oncall: jit,False
59645,Tensor.type() does not work with meta tensors module: crash triaged module: meta tensors,2021-06-08 18:42:25+00:00,,0,4,module: crash triaged module: meta tensors,True
59629,Potentially misleading note in documentation for PackedSequence module: docs module: nn module: rnn triaged module: nestedtensor,2021-06-08 12:49:37+00:00,,0,4,module: docs module: nn module: rnn triaged module: nestedtensor,True
59628,what():  result type Float can't be cast to the desired output type Long needs reproduction module: cpp triaged,2021-06-08 11:50:47+00:00,,0,3,needs reproduction module: cpp triaged,True
59627,Example code for is_storage missing module: docs triaged,2021-06-08 11:43:29+00:00,,0,2,module: docs triaged,False
59626,multi scripted model on the same gpu on multi gpu machine needs reproduction module: multi-gpu triaged,2021-06-08 11:24:47+00:00,,0,4,needs reproduction module: multi-gpu triaged,True
59617,torch.split() infer -1 entry from the other split sizes triaged enhancement module: viewing and reshaping,2021-06-08 07:08:21+00:00,,0,0,triaged enhancement module: viewing and reshaping,True
59612,Why Pytroch.distributed does not expose NCCL cuda stream? oncall: distributed feature module: c10d,2021-06-08 02:07:43+00:00,,0,4,oncall: distributed feature module: c10d,False
59592,Build failed with TBB module: build triaged module: tbb,2021-06-07 21:39:39+00:00,,0,1,module: build triaged module: tbb,True
59575,[clang-tidy] check for iteration over unordered data structures triaged better-engineering,2021-06-07 19:36:28+00:00,,0,0,triaged better-engineering,True
59566,[discussion] Make docs consistent for torch.nn Modules module: docs module: nn triaged,2021-06-07 16:40:05+00:00,,0,3,module: docs module: nn triaged,True
59552,test_batch_isend_irecv_nccl fails with NCCL 2.8.3 oncall: distributed,2021-06-07 11:41:52+00:00,,0,2,oncall: distributed,False
59551,"error when trying to call ""torch::jit::load""method, use Metal backend in PyTorch Mobile oncall: mobile module: ios",2021-06-07 11:24:04+00:00,,0,12,oncall: mobile module: ios,True
59546,Multiprocessing model evaluation gets stuck module: multiprocessing triaged,2021-06-07 09:13:30+00:00,,0,0,module: multiprocessing triaged,True
59545,max-sum operation triaged module: linear algebra module: reductions,2021-06-07 08:31:11+00:00,,0,7,triaged module: linear algebra module: reductions,True
59542,Build Pytorch with Aten? module: build triaged,2021-06-07 07:19:21+00:00,,0,1,module: build triaged,True
59531,Failed to enable libuv module: build triaged,2021-06-07 02:26:09+00:00,,0,1,module: build triaged,True
59530,Batch size is hardcoded using torch.jit.trace with LSTMCell oncall: jit,2021-06-07 02:23:23+00:00,,0,11,oncall: jit,False
59528,AttributeError: module 'torch.jit' has no attribute '_script_if_tracing' oncall: jit,2021-06-07 00:40:43+00:00,,0,2,oncall: jit,True
59526,Documentation for torch.finfo doesn't match implementation module: docs triaged module: numpy,2021-06-06 22:02:22+00:00,,1,3,module: docs triaged module: numpy,True
59515,Conv1d with large batch size and half precision in cuda returns incorrect result module: cuda triaged module: correctness (silent),2021-06-05 17:21:40+00:00,,0,1,module: cuda triaged module: correctness (silent),True
59493,ctc loss bug module: autograd module: loss triaged,2021-06-05 00:08:15+00:00,,0,6,module: autograd module: loss triaged,True
59457,Error compiling PyTorch: `/usr/bin/ld: cannot find -lmagma /usr/bin/ld: cannot find -lnvToolsExt` module: build module: cuda triaged,2021-06-04 16:38:10+00:00,,0,3,module: build module: cuda triaged,True
59441,distributed/test_jit_c10d.py fails with RuntimeError: Address already in use oncall: distributed,2021-06-04 12:05:43+00:00,,0,1,oncall: distributed,False
59439,NaN values on torch.nn.functional.conv2d (aarch64) module: nn module: convolution triaged module: arm module: jetson,2021-06-04 11:29:14+00:00,,0,6,module: nn module: convolution triaged module: arm module: jetson,True
59438," DataLoader worker (pid(s) 18056, 20540, 4512) exited unexpectedly module: dataloader triaged",2021-06-04 11:14:45+00:00,,0,1,module: dataloader triaged,True
59437,pytorch/manylinux-cuda102 support for aarch64 module: build triaged module: docker module: arm,2021-06-04 09:08:05+00:00,,0,4,module: build triaged module: docker module: arm,True
59436,Test timeout and assertion failure in distributed/rpc/test_tensorpipe_agent oncall: distributed module: flaky-tests module: rpc,2021-06-04 08:56:23+00:00,,1,1,oncall: distributed module: flaky-tests module: rpc,True
59418,Libtorch segfault in packed GRU evaluation with cuda batch_sizes module: cpp module: nn module: rnn triaged,2021-06-03 22:15:39+00:00,,0,0,module: cpp module: nn module: rnn triaged,True
59386,Energy-efficiency benchmarks? feature triaged,2021-06-03 17:41:05+00:00,,0,2,feature triaged,False
59356,pytorch build failure on arm64 module: build module: cuda triaged module: arm module: jetson,2021-06-03 01:45:28+00:00,,0,3,module: build module: cuda triaged module: arm module: jetson,True
59309,CUDA memory error when using torch.device('cpu')  module: cuda module: memory usage triaged,2021-06-02 16:08:55+00:00,,0,1,module: cuda module: memory usage triaged,True
59263,Ubuntu 20.04 GeForce RTX 3070  / NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 /  GPU Visibility Issue module: binaries module: cuda triaged,2021-06-01 21:13:23+00:00,,0,10,module: binaries module: cuda triaged,True
59262,"Possibly alignment issues on NEON vectorized ops, Jetson platforms triaged module: arm module: jetson",2021-06-01 21:05:29+00:00,,0,6,triaged module: arm module: jetson,True
59257,Optimizer step stuck during distributed training oncall: distributed module: data parallel module: amp (automated mixed precision),2021-06-01 19:18:05+00:00,,0,2,oncall: distributed module: data parallel module: amp (automated mixed precision),True
59256,ProcessGroupGloo creation crashes when world_size > 150 oncall: distributed triaged module: data parallel module: c10d,2021-06-01 19:04:38+00:00,,0,1,oncall: distributed triaged module: data parallel module: c10d,True
59251,Ahead of time C++ Extensions should have an option to build with verbose=False module: cpp-extensions triaged,2021-06-01 15:40:27+00:00,,0,0,module: cpp-extensions triaged,True
59247,Support complex numbers in `at::nan_to_num`. triaged enhancement module: numpy,2021-06-01 14:36:40+00:00,,0,2,triaged enhancement module: numpy,True
59243,Can't install pytorch =1.7 with python 3.8.5 on Raspberry Pi  triaged module: arm,2021-06-01 13:37:32+00:00,,0,2,triaged module: arm,True
59237,Multiplication of `torch.tensor` with `np.array` does the operation with numpy. high priority triaged module: numpy,2021-06-01 08:11:45+00:00,,1,6,high priority triaged module: numpy,True
59218,Clarify BLAS configuration option module: build module: docs triaged module: linear algebra,2021-05-31 14:07:16+00:00,,0,1,module: build module: docs triaged module: linear algebra,True
59216,Add a BlendLoss class in torch.nn and a blend_loss function in torch.nn.functional to allow blending of a list of loss functions module: nn module: loss triaged enhancement,2021-05-31 13:54:57+00:00,,0,3,module: nn module: loss triaged enhancement,False
59196,torch.zeros memory leak : cpu high priority module: memory usage triaged,2021-05-31 07:47:44+00:00,,1,3,high priority module: memory usage triaged,True
59194,Add numerically-stable function for angle between vectors module: numerical-stability triaged enhancement module: distance functions,2021-05-31 06:04:57+00:00,,0,2,module: numerical-stability triaged enhancement module: distance functions,True
59185,Add supports_nnc metadata to OpInfos module: tests triaged NNC,2021-05-30 08:04:09+00:00,,0,0,module: tests triaged NNC,True
59174,[jit] Error: Results of original model and exported/imported version of model differed  oncall: jit,2021-05-29 11:12:29+00:00,,1,0,oncall: jit,True
59168,[CUDA] Add channels_last_3d support for commonly used modules module: cuda triaged module: memory format,2021-05-29 00:31:20+00:00,,0,2,module: cuda triaged module: memory format,True
59153,JIT compiling a ParameterList raises an unnecessary warning oncall: jit,2021-05-28 16:55:52+00:00,,1,7,oncall: jit,True
59140,No index type check of torch.tensor in array manner module: error checking triaged module: advanced indexing,2021-05-28 07:05:20+00:00,,0,0,module: error checking triaged module: advanced indexing,True
59137,[autograd] `fill_` gradgradcheck raises RuntimeError module: autograd triaged,2021-05-28 06:24:02+00:00,,0,2,module: autograd triaged,True
59114,[JIT] OpInfo / Autodiff tests dont seem to actually invoke symbolic backwards  oncall: jit,2021-05-27 23:18:12+00:00,,0,1,oncall: jit,True
59104,Data access pattern in the loop in add_out_dense_sparse_csr_cuda could be pretty bad module: sparse triaged,2021-05-27 20:46:30+00:00,,0,0,module: sparse triaged,True
59101,Use of storage_offset is not needed  in add_out_dense_sparse_csr_cuda module: sparse module: cuda triaged,2021-05-27 20:28:57+00:00,,0,0,module: sparse module: cuda triaged,True
59099,CSR: Relaxing constraints to s_addmm_out_sparse_dense_cuda_worker module: sparse triaged open source,2021-05-27 20:22:01+00:00,,1,0,module: sparse triaged open source,True
59098,`test_lstm` in `quantization.bc.test_backward_compatibility.TestSerialization` fails on Intel Cascade Lake machines oncall: quantization low priority triaged,2021-05-27 19:55:17+00:00,,0,10,oncall: quantization low priority triaged,True
59074,c++ use pybind11 to import torch  free(): invalid pointer module: cpp triaged module: pybind,2021-05-27 08:52:44+00:00,,0,3,module: cpp triaged module: pybind,True
59071,test_bottleneck_cuda fails without error message module: cuda module: tests triaged,2021-05-27 06:42:11+00:00,,0,0,module: cuda module: tests triaged,True
59058,Issue: support auto generation of device check for sparse tensors module: sparse triaged module: codegen,2021-05-27 02:00:21+00:00,,0,6,module: sparse triaged module: codegen,True
58997,[discussion] Should Optimizers be also Modules? module: nn module: optimizer triaged,2021-05-26 16:50:11+00:00,,0,8,module: nn module: optimizer triaged,True
58983,Get stuck while using pytorch.profiler triaged oncall: profiler,2021-05-26 09:49:30+00:00,,3,2,triaged oncall: profiler,False
58982,Can't re-init rpc with a different rank after rpc shutdown oncall: distributed has workaround,2021-05-26 09:44:07+00:00,,0,4,oncall: distributed has workaround,False
58980,libtorch conflict with cxxopts and cause memery leak module: memory usage triaged,2021-05-26 08:02:52+00:00,,0,0,module: memory usage triaged,True
58979,gradient layout contract triaged module: memory format,2021-05-26 07:51:04+00:00,,0,2,triaged module: memory format,True
58973,First-class Java API feature triaged oncall: java,2021-05-26 04:40:03+00:00,,0,9,feature triaged oncall: java,False
58969,Improper use of quantization API for MHA should fail fast oncall: quantization triaged,2021-05-26 02:46:10+00:00,,1,5,oncall: quantization triaged,True
58962,Operating on a shared memory tensor with multiple threads hangs oncall: distributed module: multiprocessing triaged,2021-05-25 23:53:13+00:00,,0,2,oncall: distributed module: multiprocessing triaged,True
58944,Use PYTHONHASHSEED during pytorch build to avoid nondeterminism module: build triaged enhancement module: determinism module: codegen,2021-05-25 20:37:49+00:00,,0,0,module: build triaged enhancement module: determinism module: codegen,True
58930,[local lint] Shellcheck quicklint file filter is incorrect triaged better-engineering,2021-05-25 18:42:23+00:00,,0,0,triaged better-engineering,True
58907,"JIT fails when calling an exported method inside another one, if kwargs with defaults are not passed oncall: jit",2021-05-25 08:54:27+00:00,,1,1,oncall: jit,True
58876,[deploy] Enable `torch.distributed.rpc` Python bindings in deploy oncall: distributed triaged module: rpc module: deploy,2021-05-24 20:22:39+00:00,,1,2,oncall: distributed triaged module: rpc module: deploy,True
58862,[cuDNN v8] Improve cuDNN convolution v8 API error reporting module: cudnn module: convolution triaged,2021-05-24 17:02:34+00:00,,0,0,module: cudnn module: convolution triaged,True
58860,[cuDNN v8] Extend current cuDNN convolution v8 API binding to support conv-bias-activation fusion module: cudnn module: convolution triaged,2021-05-24 16:58:01+00:00,,1,2,module: cudnn module: convolution triaged,True
58859,[cuDNN v8] Extend current cuDNN convolution v8 API binding to support cuDNN benchmark module: cudnn module: convolution triaged,2021-05-24 16:56:43+00:00,,0,0,module: cudnn module: convolution triaged,True
58858,[cuDNN v8] Extend current cuDNN v8 API binding to support convolution backward and transposed convolution forward module: cudnn module: convolution triaged,2021-05-24 16:48:53+00:00,,0,0,module: cudnn module: convolution triaged,True
58857,Call non class methods from `torch::jit::Module` in C++ oncall: jit,2021-05-24 16:32:38+00:00,,0,1,oncall: jit,True
58856,NCCL multi-gpu test intermittently failing after NCCL version upgrade high priority triage review oncall: distributed triaged module: flaky-tests module: nccl,2021-05-24 16:26:44+00:00,,1,2,high priority triage review oncall: distributed triaged module: flaky-tests module: nccl,True
58847,Dataloader Rerunning with num_workers=0 may give better error trace oncall: distributed module: dataloader triaged module: ddp,2021-05-24 14:21:12+00:00,,0,2,oncall: distributed module: dataloader triaged module: ddp,True
58846,Simplification of pruned models feature module: nn triaged,2021-05-24 12:58:20+00:00,,0,14,feature module: nn triaged,True
58845,Use jit in flask oncall: jit,2021-05-24 12:16:10+00:00,,0,7,oncall: jit,False
58841,[bug] torch.topk sometimes supports `float16` and sometimes doesn't triaged enhancement module: half module: sorting and selection,2021-05-24 08:50:13+00:00,,0,1,triaged enhancement module: half module: sorting and selection,True
58836,Latest nightly produces CUDA torchscript that cannot be run oncall: jit,2021-05-24 05:39:55+00:00,,1,2,oncall: jit,False
58833,Foreach Functions Tracking Issue module: performance triaged tracker module: mta,2021-05-24 03:29:33+00:00,,0,13,module: performance triaged tracker module: mta,True
58828,[Feature Pitch] Fast extremal eigensolvers module: sparse feature triaged module: linear algebra,2021-05-23 20:37:16+00:00,,0,15,module: sparse feature triaged module: linear algebra,True
58820,[docs] Tensor.bernoulli_ formatting is hard to read and UX inconsistent with the function variant triaged module: ux,2021-05-23 09:55:45+00:00,,0,1,triaged module: ux,True
58814,TensorIterator for sparse layouts module: sparse feature triaged,2021-05-23 02:57:43+00:00,,0,2,module: sparse feature triaged,True
58813,Single-Process Multi-GPU is not the recommended mode for DDP oncall: distributed triaged module: ddp,2021-05-23 02:40:08+00:00,,0,2,oncall: distributed triaged module: ddp,True
58812,"Transferring tensor to the gpu and converting dtype in a single call to .to() is slower than first transferring, then converting. module: performance module: cuda triaged",2021-05-23 00:30:09+00:00,,0,3,module: performance module: cuda triaged,True
58804,A Null pointer dereference bug oncall: jit,2021-05-22 09:04:35+00:00,,0,1,oncall: jit,True
58791,[nnc] Add more optimization for conditional stmts in loopnest transformations NNC,2021-05-22 01:02:51+00:00,,0,0,NNC,False
58789,Type annotations for torch.jit.* decorators oncall: jit,2021-05-22 00:38:10+00:00,,0,3,oncall: jit,True
58779,C++ Extensions that use ninja should enable colors by default module: cpp-extensions triaged enhancement,2021-05-21 22:05:07+00:00,,0,3,module: cpp-extensions triaged enhancement,True
58772,NNC Lowering bugs found by new OpInfo tests NNC,2021-05-21 19:38:28+00:00,,0,2,NNC,False
58770,MKL csr matmul issue module: sparse triaged module: mkl,2021-05-21 19:16:21+00:00,,0,0,module: sparse triaged module: mkl,True
58765,[codegen] generated inplace/out= wrappers don't have input checks module: error checking triaged module: codegen,2021-05-21 18:01:59+00:00,,1,4,module: error checking triaged module: codegen,True
58758,precision/consistency issue in `linspace` triaged module: numpy module: numerical-reproducibility module: tensor creation,2021-05-21 16:32:01+00:00,,0,3,triaged module: numpy module: numerical-reproducibility module: tensor creation,True
58745,"`torch.(min|max)(..., dim=...)` diverges from array API specification triaged module: deprecation module: reductions module: python array api",2021-05-21 10:26:23+00:00,,0,7,triaged module: deprecation module: reductions module: python array api,True
58744,Using pytorch 1.1 model on pytorch 1.8 environment oncall: jit,2021-05-21 10:21:08+00:00,,0,4,oncall: jit,False
58743,Python Array API Compatibility Tracker high priority triaged module: python array api tracker,2021-05-21 10:08:45+00:00,,0,11,high priority triaged module: python array api tracker,True
58742,Python Array API New Operators Tracker triaged module: python array api tracker,2021-05-21 10:02:37+00:00,,0,3,triaged module: python array api tracker,True
58741,`torch.size()` diverges from array API specification triaged module: deprecation module: python array api,2021-05-21 09:57:46+00:00,,1,3,triaged module: deprecation module: python array api,True
58736,type promotion with 0d-tensors diverges from array API specification triaged module: type promotion module: python array api,2021-05-21 07:41:26+00:00,,0,4,triaged module: type promotion module: python array api,True
58735,CSR construction: safe_get_attr_string suppresses real errors module: sparse triaged open source,2021-05-21 07:40:14+00:00,,0,0,module: sparse triaged open source,True
58734,"Support for `uint16`, `uint32`, and `uint64` triaged module: python array api oncall: pt2",2021-05-21 07:38:15+00:00,,0,16,triaged module: python array api oncall: pt2,False
58720,`pickle_save`/`pickle_load` are undocumented oncall: jit triaged,2021-05-21 00:41:57+00:00,,0,1,oncall: jit triaged,True
58681,Incorrect channel in installation command for stable (1.8.1)+windows+conda+python+cuda 11.1 oncall: releng triaged has workaround,2021-05-20 18:10:34+00:00,,0,0,oncall: releng triaged has workaround,True
58672,Werror should include -Wmissing-prototypes module: build triaged module: build warnings,2021-05-20 15:24:39+00:00,,0,0,module: build triaged module: build warnings,True
58671,torch.load with dill is unable to unserialize from buffer module: serialization triaged,2021-05-20 15:13:47+00:00,,0,2,module: serialization triaged,True
58670,Ability to enabling/disabling cuDNN and cuBLAS API logging in PyTorch API directly module: cudnn feature module: cuda triaged module: cublas,2021-05-20 15:09:44+00:00,,0,0,module: cudnn feature module: cuda triaged module: cublas,True
58657,"Improvement suggestions for the error message ""Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false"" module: error checking triaged module: linear algebra",2021-05-20 12:50:57+00:00,,0,1,module: error checking triaged module: linear algebra,True
58628,[JIT] Concat QKV Linear Layers On Frozen Models oncall: jit,2021-05-20 00:51:09+00:00,,2,9,oncall: jit,True
58626,GPU 0 context created on GPU 1 worker when using pin_memory=True module: dataloader module: cuda triaged,2021-05-20 00:23:19+00:00,,0,7,module: dataloader module: cuda triaged,True
58617,[rfc] Bug developers about release notes for new APIs triaged better-engineering,2021-05-19 21:42:17+00:00,,0,3,triaged better-engineering,True
58596,Some cuda objects take very long time to build module: build module: cuda triaged enhancement,2021-05-19 20:02:30+00:00,,0,6,module: build module: cuda triaged enhancement,True
58591,"link error on BatchNormImplBase<D, Derived>::pretty_print  when using icpc  triaged module: undefined reference",2021-05-19 19:22:50+00:00,,0,0,triaged module: undefined reference,True
58585,Calling benchmark.Timer with default `num_threads=1` disables parallelism permanently module: docs triaged module: macos module: multithreading,2021-05-19 18:34:50+00:00,,0,0,module: docs triaged module: macos module: multithreading,True
58567,ONNX exported EmbeddingBag fails for not strictly increasing offset. module: onnx triaged onnx-triaged,2021-05-19 16:44:00+00:00,,0,2,module: onnx triaged onnx-triaged,True
58555,fx: unable to symbolically trace model with torch.full(Proxy) triaged module: fx,2021-05-19 14:22:29+00:00,,0,1,triaged module: fx,True
58548,Pixel shuffle support for PyTorch Mobile NN-API triaged enhancement module: android oncall: mobile,2021-05-19 09:46:25+00:00,,1,1,triaged enhancement module: android oncall: mobile,True
58533,Migrate wheels to manylinux2010 oncall: releng triaged,2021-05-19 05:33:15+00:00,,1,2,oncall: releng triaged,True
58522,DDP grads dont have parity with local training when grads are undefined oncall: distributed triaged module: ddp,2021-05-19 00:44:00+00:00,,0,3,oncall: distributed triaged module: ddp,True
58516,Unhelpful error message when type annotation is violated. oncall: jit,2021-05-18 22:58:31+00:00,,0,0,oncall: jit,True
58507,OpInfo JIT tests do not work with Tensor kwarg arguments oncall: jit module: tests better-engineering,2021-05-18 21:17:59+00:00,,0,2,oncall: jit module: tests better-engineering,True
58489,Inter-category type promotion has undocumented behavior module: docs triaged module: type promotion,2021-05-18 16:18:31+00:00,,0,8,module: docs triaged module: type promotion,True
58479,Make SavedVariables aware of pyobjects module: autograd triaged,2021-05-18 14:20:33+00:00,,2,0,module: autograd triaged,True
58474,nn.Upsample result mismatch in 1.1.0a0+828a6a3 and 1.9.0 module: cudnn module: cuda triaged,2021-05-18 09:59:18+00:00,,0,1,module: cudnn module: cuda triaged,True
58471,Why the same training code running on different GPUs (V100 vs P100) brings a significiant difference in outputs and thus train loss / model accuracy? triaged module: determinism,2021-05-18 08:29:50+00:00,,0,2,triaged module: determinism,True
58466,`set_per_process_memory_fraction()` does not ensure max used GPU memory below fraction module: cuda triaged,2021-05-18 05:02:02+00:00,,0,4,module: cuda triaged,True
58456,TSAN issues in torch::jit::ProfilingRecord oncall: jit,2021-05-18 01:35:50+00:00,,0,2,oncall: jit,False
58452,Data race in RecordFunction::callbackShouldRun triaged oncall: profiler,2021-05-18 01:27:59+00:00,,0,0,triaged oncall: profiler,True
58447,TSAN issues in at::RecordFunction triaged oncall: profiler,2021-05-18 00:27:05+00:00,,0,0,triaged oncall: profiler,True
58442,TSAN issues in autograd module: autograd triaged,2021-05-17 23:59:53+00:00,,0,5,module: autograd triaged,True
58440,TSAN issues in libkineto triaged oncall: profiler,2021-05-17 23:44:01+00:00,,0,0,triaged oncall: profiler,True
58414,cuDNN v8 API tracking issue high priority module: cudnn triaged tracker,2021-05-17 17:15:22+00:00,,1,0,high priority module: cudnn triaged tracker,True
58402,fx OpInfo tests don't work with nn.functional test names triaged module: fx,2021-05-17 16:13:01+00:00,,1,0,triaged module: fx,True
58395,[DOC] Add info about differentiability of functions module: docs triaged enhancement,2021-05-17 15:23:28+00:00,,0,2,module: docs triaged enhancement,False
58393,"In Android, the memory used by the tensor or model cannot be recycled. Is there any way to solve it? triaged module: android oncall: mobile",2021-05-17 15:06:37+00:00,,1,0,triaged module: android oncall: mobile,True
58392,torch.cuda.amp.autocast nan values when model.eval() needs reproduction triaged module: amp (automated mixed precision),2021-05-17 14:03:41+00:00,,0,2,needs reproduction triaged module: amp (automated mixed precision),True
58386,Better shared memory allocation under Docker module: memory usage triaged enhancement module: docker,2021-05-17 11:17:53+00:00,,0,2,module: memory usage triaged enhancement module: docker,True
58385,"A ""sanitizer"" mode for the CUDA caching allocator? feature module: cuda triaged oncall: profiler",2021-05-17 11:07:42+00:00,,0,0,feature module: cuda triaged oncall: profiler,True
58380,Clarity of error message in einsum regressed in performance improvements module: error checking triaged module: linear algebra,2021-05-17 08:37:27+00:00,,0,2,module: error checking triaged module: linear algebra,True
58371,Tests do not pass even though build successful oncall: distributed triaged,2021-05-16 22:14:47+00:00,,0,5,oncall: distributed triaged,True
58370,Add better python operators annotations for IDE type checking high priority feature module: typing triaged,2021-05-16 22:10:58+00:00,,1,5,high priority feature module: typing triaged,True
58352,Broadcasting multiple tensors to all procs in distributed training oncall: distributed triaged,2021-05-15 16:28:40+00:00,,0,5,oncall: distributed triaged,True
58351,need  sampled softmax loss  function to train model to get user vector and item vector with a large dictionary feature module: loss triaged,2021-05-15 14:25:40+00:00,,0,0,feature module: loss triaged,False
58343,Add the features like Keras feature module: convolution triaged,2021-05-15 03:53:59+00:00,,0,1,feature module: convolution triaged,False
58298,"[JIT] Can't handle creating new objects in compiled methods: ""Class does not have an __init__ function defined"" oncall: jit",2021-05-14 10:15:39+00:00,,0,5,oncall: jit,False
58267,Pytorch build issues on PowerPC module: build triaged module: POWER oncall: profiler,2021-05-13 22:22:38+00:00,,0,5,module: build triaged module: POWER oncall: profiler,True
58238,torchdeploy doesn't prevent Obj from being used on wrong interpreter triaged module: deploy,2021-05-13 15:19:49+00:00,,1,1,triaged module: deploy,True
58228,"""_amp_foreach_non_finite_check_and_unscale_cuda"" not implemented for 'ComplexFloat' module: cuda triaged module: complex needs design module: amp (automated mixed precision)",2021-05-13 13:28:44+00:00,,0,4,module: cuda triaged module: complex needs design module: amp (automated mixed precision),True
58220,"After the Adam optimizer used weight_decay, the model became extremely slow when tested on the CPU.(Time from 7 seconds to 46 seconds) module: performance module: optimizer triaged",2021-05-13 06:06:57+00:00,,0,2,module: performance module: optimizer triaged,True
58218,"Torch.nn.DataParallel training model, the output of the model becomes list type, the number of lists is batch size triaged module: data parallel",2021-05-13 05:41:53+00:00,,0,2,triaged module: data parallel,True
58212,`torch.autograd.Function` subclasses *sometimes* throw away custom subclasses module: autograd triaged actionable,2021-05-13 03:01:25+00:00,,1,10,module: autograd triaged actionable,True
58190,[FX] record_function tracing in FX triaged module: fx,2021-05-12 22:00:10+00:00,,0,2,triaged module: fx,True
58181,torch.split_with_sizes is not documented module: docs triaged,2021-05-12 20:08:04+00:00,,1,10,module: docs triaged,True
58172,Mobile Android: Could not run 'aten::quantize_per_tensor' with arguments from the 'Vulkan' backend triaged module: android oncall: mobile,2021-05-12 19:11:33+00:00,,1,7,triaged module: android oncall: mobile,True
58171,"Error while converting model for NNAPI ""Unsupported node kind ('quantized::batch_norm2d')"" triaged module: android oncall: mobile",2021-05-12 19:03:57+00:00,,1,7,triaged module: android oncall: mobile,True
58167,Allow `Union` as a `Dict` key triaged TSRootCause:TypeRefinement,2021-05-12 18:29:56+00:00,,1,0,triaged TSRootCause:TypeRefinement,False
58155,"""uncorrectable NVLink error"" making the tests fail oncall: distributed module: cuda triaged module: nccl",2021-05-12 16:19:21+00:00,,0,7,oncall: distributed module: cuda triaged module: nccl,True
58139,[docs] Strange torch.unique function signature module: docs module: bootcamp triaged,2021-05-12 14:51:36+00:00,,0,3,module: docs module: bootcamp triaged,False
58136,scatter_add_ 6000-times slower with int64 compared to int32 module: performance module: docs module: cuda triaged module: scatter & gather ops,2021-05-12 13:16:56+00:00,,0,1,module: performance module: docs module: cuda triaged module: scatter & gather ops,True
58135,Segmentation fault need help needs reproduction module: cuda triaged,2021-05-12 13:10:24+00:00,,0,3,needs reproduction module: cuda triaged,True
58128,the same input with different batchsize got different precision output module: nn triaged module: numerical-reproducibility module: batching,2021-05-12 08:30:44+00:00,,0,2,module: nn triaged module: numerical-reproducibility module: batching,True
58127,Save checkpoint error module: optimizer module: serialization triaged,2021-05-12 08:10:12+00:00,,0,2,module: optimizer module: serialization triaged,True
58124,Python API binding code generation does not need to pack TensorOptions for `xxx_like` generators triaged module: codegen,2021-05-12 06:33:10+00:00,,0,0,triaged module: codegen,True
58122,Generate special clear error messages for known common misuses in TorchScript module: bootcamp good first issue triaged OSS contribution wanted,2021-05-12 06:11:02+00:00,,4,16,module: bootcamp good first issue triaged OSS contribution wanted,True
58119,[FX] Revise PyTree support in FX to move pytree logic out of `self.code` triaged module: fx,2021-05-12 05:27:45+00:00,,0,0,triaged module: fx,False
58111,Static graph training fails if forward is called multiple times before backward high priority triage review oncall: distributed triaged module: ddp,2021-05-12 02:20:55+00:00,,0,2,high priority triage review oncall: distributed triaged module: ddp,True
58109,torch.jit.trace memory leak oncall: jit,2021-05-12 02:03:52+00:00,,1,2,oncall: jit,False
58093,"TrilinearBackward takes 98.4% of total computational time, is this to be expected? module: performance module: nn triaged",2021-05-11 22:18:26+00:00,,0,9,module: performance module: nn triaged,True
58091,Follow-up updates to inference mode python bindings triaged inference mode,2021-05-11 21:49:33+00:00,,0,0,triaged inference mode,False
58087,TensorList upfront checks parameter types in argument parsing and throws an error unexpectedly triaged module: codegen,2021-05-11 21:17:09+00:00,,0,0,triaged module: codegen,True
58054,PyTorch master failed to build with glog 0.5.0. module: build triaged,2021-05-11 16:15:41+00:00,,0,16,module: build triaged,True
58051,quantize_per_tensor not symbolically traceable with FX if scale+zp are proxied triaged module: fx,2021-05-11 16:06:12+00:00,,0,2,triaged module: fx,True
58037,linalg.eigh and linalg.cholesky UPLO flag breaks in backward module: autograd triaged module: linear algebra,2021-05-11 11:05:55+00:00,,0,3,module: autograd triaged module: linear algebra,True
58036,[feature request] torch.as_tensor to support any object that NumPy's asarray or array can consume (consume __array_interface__) triaged module: numpy,2021-05-11 10:39:11+00:00,,0,4,triaged module: numpy,True
58034,Add a `descending` flag to `linalg.eigh` and `linalg.svd`  module: docs feature good first issue triaged module: linear algebra,2021-05-11 09:35:53+00:00,,0,10,module: docs feature good first issue triaged module: linear algebra,True
58016,Eliminate potential double device check triaged better-engineering,2021-05-11 05:32:53+00:00,,0,2,triaged better-engineering,True
58015,Support a dist.group_like API oncall: distributed module: bootcamp triaged pt_distributed_rampup module: c10d,2021-05-11 05:05:49+00:00,,1,4,oncall: distributed module: bootcamp triaged pt_distributed_rampup module: c10d,True
58005,torch.distributed.nn.all_reduce incorrectly scales the gradient high priority oncall: distributed triaged,2021-05-11 00:28:20+00:00,,0,27,high priority oncall: distributed triaged,True
57987,`type: ignore` everything triaged better-engineering,2021-05-10 21:30:54+00:00,,0,3,triaged better-engineering,True
57960,Use the same CUDA stream for all RPCs within the same dist autograd context oncall: distributed feature triaged module: rpc,2021-05-10 17:21:12+00:00,,0,13,oncall: distributed feature triaged module: rpc,True
57957,Add a few tests to make sure new dispatch keys for backends are added properly. triaged module: dispatch,2021-05-10 16:56:51+00:00,,0,0,triaged module: dispatch,True
57949,[structured] Clarify if NoNamesGuard is actually needed in impls or not triaged module: structured kernels,2021-05-10 14:28:13+00:00,,0,0,triaged module: structured kernels,True
57947,torch.lerp to support argument type promotion / broadcasting similar to torch.where triaged module: type promotion module: interpolation,2021-05-10 13:30:41+00:00,,0,4,triaged module: type promotion module: interpolation,True
57929,[JIT] provide a pass for alias reduction oncall: jit,2021-05-10 02:30:47+00:00,,0,2,oncall: jit,False
57921,Backward pass for a nn.Conv2d with half-precision on Quadro RTX 8000 leads to CUDNN_STATUS_INTERNAL_ERROR module: cudnn triaged,2021-05-09 18:04:38+00:00,,0,1,module: cudnn triaged,True
57911,Circular padding in Convolution layers should not only be wrap for once. module: nn module: convolution triaged enhancement module: padding,2021-05-09 02:21:19+00:00,,0,0,module: nn module: convolution triaged enhancement module: padding,True
57894,"JIT ""optimizations"" take way too long  oncall: jit",2021-05-08 12:03:14+00:00,,0,2,oncall: jit,False
57893,Android API to run model on Vulkan backend oncall: mobile,2021-05-08 10:54:57+00:00,,1,6,oncall: mobile,False
57887,[Mobile] Valid scale ratio with invalid error oncall: mobile,2021-05-08 07:01:00+00:00,,0,1,oncall: mobile,True
57843,Torch.save doesn't make new file module: serialization triaged,2021-05-07 20:08:08+00:00,,0,1,module: serialization triaged,True
57815,copy.deepcopy fails on spectral-norm layers after the first forward module: nn triaged,2021-05-07 10:28:24+00:00,,0,11,module: nn triaged,True
57796,BatchNorm grad calculation is imprecise needs reproduction module: numerical-stability module: autograd triaged,2021-05-07 03:39:35+00:00,,2,10,needs reproduction module: numerical-stability module: autograd triaged,True
57794,CUDA error: an illegal memory access was encountered needs reproduction module: cuda triaged,2021-05-07 02:23:34+00:00,,0,7,needs reproduction module: cuda triaged,True
57793,"LibTorch inference script model is slower than PyTorch, and the speed is not stable. Why? oncall: jit",2021-05-07 02:14:20+00:00,,0,2,oncall: jit,False
57787,Add CI automation tests for Infiniband for DDP oncall: distributed triaged,2021-05-07 00:16:09+00:00,,0,0,oncall: distributed triaged,True
57776,[rfc] Build a debug tool to help users find which parameters are unused during DDP training oncall: distributed triaged,2021-05-06 22:11:24+00:00,,1,2,oncall: distributed triaged,True
57773,BFloat16 CUDA GEMM ops unsupported on Nvidia P100 (SM_60) on CUDA 11.3 module: cuda triaged module: cublas module: bfloat16,2021-05-06 21:45:41+00:00,,0,0,module: cuda triaged module: cublas module: bfloat16,True
57759,Show commit hash for master branch docs on pytorch.org triaged enhancement module: doc infra,2021-05-06 18:14:23+00:00,,0,0,triaged enhancement module: doc infra,False
57742,[structured] Meta-impl split implies redundant dtype tests triaged better-engineering,2021-05-06 15:28:03+00:00,,0,0,triaged better-engineering,True
57739,object with __torch_functon__ does not work with jit function  oncall: jit,2021-05-06 14:51:22+00:00,,0,1,oncall: jit,False
57724,Incorporate Posit Support triaged needs research,2021-05-06 11:55:32+00:00,,0,18,triaged needs research,False
57721,nn.TransformerEncoder cannot deal with large negative value even when this value is masked by src_key_padding_mask oncall: transformer/mha,2021-05-06 11:19:47+00:00,,0,1,oncall: transformer/mha,False
57717,TorchScript JIT Compiler Can't Recognize Repeated Branches (Scripting) oncall: jit,2021-05-06 09:22:49+00:00,,0,6,oncall: jit,False
57716,Unknown builtin op: torchvision::nms in LibTorch oncall: jit module: serialization module: vision,2021-05-06 08:53:26+00:00,,0,2,oncall: jit module: serialization module: vision,True
57712,[Docs] nn.init._calculate_fan_in_and_fan_out() module: docs triaged,2021-05-06 06:30:54+00:00,,0,5,module: docs triaged,False
57691,Unify `std::getenv` usages triaged better-engineering,2021-05-05 23:04:05+00:00,,0,0,triaged better-engineering,True
57690,torch.cdist returns high diagonal values with CUDA module: numerical-stability triaged module: tf32,2021-05-05 22:26:00+00:00,,0,5,module: numerical-stability triaged module: tf32,True
57646,"get_future() should be documented once it is enabled for gloo, mpi backends oncall: distributed triaged",2021-05-05 16:55:24+00:00,,1,0,oncall: distributed triaged,True
57639,"After tracing, the device of the named buffer is still `cuda` although it was moved to `cpu ` oncall: jit",2021-05-05 15:08:37+00:00,,1,4,oncall: jit,False
57631,Pruners' compute_mask returns tensors with dtypes that are not consistent with each other module: nn triaged module: pruning,2021-05-05 14:23:15+00:00,,0,0,module: nn triaged module: pruning,True
57617,complex128 autograd failures on PPC triaged module: complex module: POWER complex_autograd,2021-05-05 13:11:09+00:00,,0,0,triaged module: complex module: POWER complex_autograd,True
57611,[perf] 10x improvement on element-wise operations with manual broadcast module: performance triaged module: TensorIterator,2021-05-05 08:03:00+00:00,,0,0,module: performance triaged module: TensorIterator,True
57610,[perf] 10x improvement when doing `x.sum(-1)` manually module: performance triaged module: reductions,2021-05-05 07:54:42+00:00,,0,0,module: performance triaged module: reductions,True
57593,[RFC] Provide an API for Structural Performance Tips in DDP oncall: distributed feature triaged module: ddp,2021-05-05 06:18:57+00:00,,0,4,oncall: distributed feature triaged module: ddp,True
57592,`jit.trace` fails to capture single-element tensor passed into op requiring scalar-type argument oncall: jit,2021-05-05 05:59:24+00:00,,0,0,oncall: jit,True
57559,[JIT] Investigate cloning/copying object before tracing it oncall: jit,2021-05-04 18:45:18+00:00,,0,2,oncall: jit,False
57554,Delete max_pool2d_with_indices_backward.grad_input? module: autograd triaged module: deprecation,2021-05-04 17:35:22+00:00,,0,1,module: autograd triaged module: deprecation,True
57547,Add distributed testing for CUDA aware MPI oncall: distributed triaged module: c10d,2021-05-04 15:22:57+00:00,,0,0,oncall: distributed triaged module: c10d,True
57534,`torch.view_as_complex()` does not work when `storage_offset` is odd triaged module: complex,2021-05-04 09:32:34+00:00,,0,7,triaged module: complex,True
57533,test_grid_sample (from TestNN) fails on POWER module: tests triaged module: POWER,2021-05-04 08:39:10+00:00,,0,1,module: tests triaged module: POWER,True
57507,Cleanup usage of IS_PYTHON_3_9_PLUS in autograd engine module: autograd triaged module: pybind,2021-05-03 22:18:05+00:00,,0,0,module: autograd triaged module: pybind,True
57503,[new feature] Adaptive gradient clipping module: autograd module: nn triaged,2021-05-03 22:10:09+00:00,,0,1,module: autograd module: nn triaged,True
57502,[structured] Fail if CompositeExplicitAutograd is in dispatch table of structured_delegate function module: autograd triaged,2021-05-03 21:59:11+00:00,,0,0,module: autograd triaged,True
57495,Please remove this assertion - it triggers on valid use cases triaged,2021-05-03 21:25:01+00:00,,0,1,triaged,True
57491,setting  all seed still get different result.  needs reproduction triaged module: random module: determinism,2021-05-03 20:44:54+00:00,,0,1,needs reproduction triaged module: random module: determinism,True
57476,Profiler creates sequence numbers that are off by 1 between the forward and backwards pass triaged oncall: profiler,2021-05-03 17:50:04+00:00,,0,1,triaged oncall: profiler,True
57475,CUDA RPC stream synchronization does not work with @rpc.functions.async_execution oncall: distributed triaged module: rpc,2021-05-03 17:39:34+00:00,,1,0,oncall: distributed triaged module: rpc,True
57457,Valgrind Shows Potential Memory Leaks oncall: jit module: autograd,2021-05-03 08:14:10+00:00,,0,12,oncall: jit module: autograd,False
57447,`torch._dirichlet_grad` returns `nan` value on cuda device module: autograd module: cuda triaged module: NaNs and Infs,2021-05-02 21:55:18+00:00,,0,3,module: autograd module: cuda triaged module: NaNs and Infs,True
57440,"I have just wasted 3 solid days trying to build PyTorch, without success module: build triaged",2021-05-02 17:06:40+00:00,,0,0,module: build triaged,True
57423,internal compiler error compiling PyTorch master on Fedora 33 module: build triaged shadow review,2021-05-01 23:29:33+00:00,,0,3,module: build triaged shadow review,True
57418,Poor torch.cat performance in the quantized Unet oncall: quantization low priority triaged,2021-05-01 18:02:56+00:00,,1,0,oncall: quantization low priority triaged,True
57415,torch 1.8.1+cuda crashes when setting breakpoints. triaged,2021-05-01 10:49:29+00:00,,0,2,triaged,True
57406,nn.Module custom setattr leads very confusing behaviors module: nn triaged enhancement,2021-05-01 01:29:23+00:00,,0,5,module: nn triaged enhancement,True
57404,[lint] Make sure that all CMake options have a corresponding output in the summary triaged better-engineering,2021-05-01 00:04:04+00:00,,0,0,triaged better-engineering,True
57392,Simplified API for custom inplace & view kernel  triaged enhancement module: custom-operators module: dispatch,2021-04-30 21:50:35+00:00,,0,0,triaged enhancement module: custom-operators module: dispatch,True
57380,Asserts all tensor are defined in dispatch wrapper triaged enhancement,2021-04-30 19:52:54+00:00,,0,0,triaged enhancement,True
57370,gen_proto_typestubs_helper.py is slow module: build triaged,2021-04-30 17:41:15+00:00,,0,1,module: build triaged,True
57369,Delay errors of inference tensor to backward pass? triaged enhancement module: dispatch inference mode,2021-04-30 17:19:44+00:00,,0,0,triaged enhancement module: dispatch inference mode,True
57359,torch.return_types does not exist triaged enhancement,2021-04-30 15:20:48+00:00,,0,0,triaged enhancement,True
57358,Missing complex autograd support for some operators triaged module: complex complex_autograd,2021-04-30 14:55:27+00:00,,0,0,triaged module: complex complex_autograd,True
57326,Renaming Autograd dispatch keys to ADCreateGraph triaged enhancement module: dispatch,2021-04-30 00:42:45+00:00,,0,0,triaged enhancement module: dispatch,True
57313,[jit] pickling custom classes may lead to invalid pickle files oncall: jit,2021-04-29 22:21:03+00:00,,0,2,oncall: jit,True
57301,Avoid code repeat in create sample inputs for sort/msort module: tests triaged module: sorting and selection,2021-04-29 20:47:03+00:00,,1,0,module: tests triaged module: sorting and selection,True
57278,Attributes consistency in Sampler and DistributedSampler oncall: distributed triaged,2021-04-29 14:54:33+00:00,,0,7,oncall: distributed triaged,True
57272,Consistent treatment of non-differentiability in linear algebra operations module: docs triaged module: linear algebra,2021-04-29 10:03:49+00:00,,0,2,module: docs triaged module: linear algebra,True
57271,RuntimeError after optimize_for_mobile module: android oncall: mobile,2021-04-29 09:49:22+00:00,,0,10,module: android oncall: mobile,False
57230,Implementation of Self Attention vs Encoder Decoder Attention Causing Early Evaluation for PyTorch/XLA triaged oncall: transformer/mha,2021-04-29 02:32:48+00:00,,0,3,triaged oncall: transformer/mha,False
57185,Segmentation fault with ITIMER_REAL triaged module: macos,2021-04-28 20:46:01+00:00,,0,8,triaged module: macos,True
57178,MaxPool2D Returns Wrong Shape With Ceil_Mode high priority module: docs triaged module: pooling,2021-04-28 19:35:05+00:00,,0,7,high priority module: docs triaged module: pooling,True
57163,Segmentation fault when using add_graph from tensorboard in combination with try except in forward pass  oncall: visualization,2021-04-28 17:48:44+00:00,,0,0,oncall: visualization,True
57158,Increasing RTT when using rpc_async in a PS architecture setup oncall: distributed triaged,2021-04-28 17:04:46+00:00,,1,7,oncall: distributed triaged,True
57157,Using register_full_backward_hook with target module for intermediate activation module: docs module: autograd module: nn triaged,2021-04-28 17:03:36+00:00,,0,4,module: docs module: autograd module: nn triaged,True
57136,Intermittent segmentation fault when using rpc_async/rpc_sync with CUDA tensors oncall: distributed triaged,2021-04-28 15:06:00+00:00,,1,5,oncall: distributed triaged,True
57128,test_eig_with_eigvec_cuda_float64 is flaky on ROCm module: rocm triaged module: flaky-tests,2021-04-28 14:29:23+00:00,,0,1,module: rocm triaged module: flaky-tests,True
57124,Allow `ScriptFunction`s to be symbolically traced triaged FX-TorchScript Compatibility module: fx,2021-04-28 12:55:24+00:00,,1,1,triaged FX-TorchScript Compatibility module: fx,True
57123,LibTorch ships with two identical library files: libdnnl and libmlkdnn module: build triaged,2021-04-28 12:55:21+00:00,,0,0,module: build triaged,True
57121,"einsum ""jk,ijkl->il"" is ~16x slower than numpy module: performance triaged module: numpy module: linear algebra",2021-04-28 12:28:23+00:00,,0,15,module: performance triaged module: numpy module: linear algebra,True
57118,How to view VLOG information module: logging triaged,2021-04-28 11:16:12+00:00,,0,7,module: logging triaged,True
57117,MultiheadAttention.out_proj.weight is not explicitly initialized oncall: transformer/mha,2021-04-28 10:53:38+00:00,,0,0,oncall: transformer/mha,False
57112,:attr: does not behave as expected. module: docs triaged module: linear algebra,2021-04-28 09:39:40+00:00,,0,1,module: docs triaged module: linear algebra,True
57108,Compilation failure in caffe2/core/plan_executor_test.cc with GCC 5.4 module: build caffe2 triaged,2021-04-28 08:04:40+00:00,,0,2,module: build caffe2 triaged,True
57095,[FR] unflatten support empty shape if unflattened dim has size 1 triaged module: viewing and reshaping,2021-04-28 05:21:13+00:00,,0,2,triaged module: viewing and reshaping,True
57065,Static Linking Pytorch triaged module: static linking,2021-04-27 21:38:28+00:00,,0,1,triaged module: static linking,True
57059,Get rid of weak_intrusive_ptr module: internals triaged,2021-04-27 20:35:54+00:00,,0,1,module: internals triaged,True
57035,`typing.List` need to be on the same line as the variable declaration? oncall: jit,2021-04-27 16:49:17+00:00,,1,0,oncall: jit,False
57018,Attempting to concatenate scalar tensors throws a runtime error triaged module: numpy,2021-04-27 14:46:14+00:00,,0,7,triaged module: numpy,True
57005,about torch install triaged,2021-04-27 10:48:38+00:00,,0,1,triaged,True
56979,"apex-0.1-py3.6-linux-x86_64.egg/apex/amp/wrap.py"", line 28, in wrapper     return orig_fn(*new_args, **kwargs) RuntimeError: CUDA error: an illegal memory access was encountered module: cuda triaged module: amp (automated mixed precision)",2021-04-27 03:09:02+00:00,,0,2,module: cuda triaged module: amp (automated mixed precision),True
56978,Do padding of weight and activation tensors to match optimized backend implementation oncall: quantization low priority triaged,2021-04-27 03:06:02+00:00,,1,3,oncall: quantization low priority triaged,True
56975,scatter does not accept scalar src= module: docs triaged module: numpy module: scatter & gather ops,2021-04-27 02:21:25+00:00,,0,3,module: docs triaged module: numpy module: scatter & gather ops,True
56969,[JIT / NNC] Incomplete Ops Coverage oncall: jit,2021-04-26 23:41:51+00:00,,0,5,oncall: jit,False
56938,remove the dependence on node names in fx graph mode quant  triaged module: fx,2021-04-26 18:38:31+00:00,,1,0,triaged module: fx,True
56928,torch-1.8.1 wheel verification fails with distlib needs reproduction module: binaries triaged,2021-04-26 17:52:39+00:00,,0,5,needs reproduction module: binaries triaged,True
56926,CUDA 11 tracking issue module: binaries module: cuda triaged tracker,2021-04-26 17:24:26+00:00,,0,1,module: binaries module: cuda triaged tracker,True
56921,torch.multiprocessing implement SyncManager module: multiprocessing triaged,2021-04-26 15:25:35+00:00,,0,0,module: multiprocessing triaged,True
56901,Static prebuilt libraries contain shared libraries module: binaries triaged module: static linking,2021-04-26 07:07:54+00:00,,0,2,module: binaries triaged module: static linking,True
56896,masked_select is x3 slower than reshaping and index_select module: performance triaged module: advanced indexing module: viewing and reshaping,2021-04-26 03:00:30+00:00,,0,0,module: performance triaged module: advanced indexing module: viewing and reshaping,True
56891,Batched SVD_LOWRANK being much slower than loop implementation (both CPU and GPU)  module: performance triaged module: linear algebra,2021-04-25 18:29:25+00:00,,0,7,module: performance triaged module: linear algebra,True
56881,[FR] integral discrete distributions should support dtype=int64 module: distributions triaged enhancement,2021-04-25 07:34:24+00:00,,0,0,module: distributions triaged enhancement,True
56862,Non-symbolic FX tracer triaged module: fx,2021-04-24 14:16:50+00:00,,0,11,triaged module: fx,True
56843,Add clipnorm parameter to optimizers module: optimizer triaged,2021-04-24 00:43:47+00:00,,0,0,module: optimizer triaged,False
56838,Comprehensively test NCCL's `get_future()` API oncall: distributed triaged better-engineering pt_distributed_rampup,2021-04-23 23:07:39+00:00,,0,1,oncall: distributed triaged better-engineering pt_distributed_rampup,True
56836,Remove single device constraint from ProcessGroupNCCL profiling  triaged module: nccl better-engineering pt_distributed_rampup module: c10d,2021-04-23 22:59:20+00:00,,1,0,triaged module: nccl better-engineering pt_distributed_rampup module: c10d,True
56809,Autograd engine worker thread initialization fails with python 3.9 debug build triaged,2021-04-23 19:30:00+00:00,,0,3,triaged,True
56805,Implement random SeedSequence triaged module: random,2021-04-23 18:55:03+00:00,,0,0,triaged module: random,True
56794,No tensor operations allowed inside at::parallel_for high priority triaged module: multithreading,2021-04-23 17:55:24+00:00,,1,5,high priority triaged module: multithreading,True
56781,Named Tensors in C++ Is Undocumented module: cpp triaged module: named tensor,2021-04-23 14:53:54+00:00,,0,0,module: cpp triaged module: named tensor,True
56778,can't find user's fuser method in quantization.fx.fusion_patterns.py when run fx graph qat. triaged module: fx,2021-04-23 14:09:20+00:00,,0,0,triaged module: fx,True
56777,Error when building PyTorch from source in CentOS Linux 7 module: build triaged module: tensorpipe,2021-04-23 13:36:28+00:00,,0,8,module: build triaged module: tensorpipe,True
56774,Support `expand_dims` triaged module: numpy function request module: python array api,2021-04-23 10:06:03+00:00,,0,4,triaged module: numpy function request module: python array api,True
56772,The multi-fc losses calculating in DistributedDataParallel. oncall: distributed triaged,2021-04-23 09:10:19+00:00,,0,1,oncall: distributed triaged,True
56771,Problems with initial communication between GPUs needs reproduction module: cuda triaged,2021-04-23 08:47:24+00:00,,0,7,needs reproduction module: cuda triaged,True
56764,Change make_reduction to reflect input resizing. triaged better-engineering module: reductions,2021-04-23 05:29:08+00:00,,0,0,triaged better-engineering module: reductions,True
56747,Matrix multiplication broken on PyTorch 1.8.1 with CUDA 11.1 and Nvidia GTX 1080 Ti high priority triaged module: cublas,2021-04-22 23:17:56+00:00,,0,20,high priority triaged module: cublas,True
56710,Hard fail build when there is no CUDA but `USE_CUDA=1` module: build triaged,2021-04-22 17:40:14+00:00,,0,1,module: build triaged,True
56707,[TorchScript JIT] `dtype` propagation pass (meta-issue) triaged,2021-04-22 16:28:23+00:00,,1,0,triaged,True
56703,[Pytorch Mobile] Error running build_pytorch_android.sh module: android oncall: mobile,2021-04-22 15:33:19+00:00,,1,4,module: android oncall: mobile,True
56698,Convolution2D may have a numerical error. module: numerical-stability triaged,2021-04-22 14:46:32+00:00,,0,3,module: numerical-stability triaged,True
56697,Avoid no-op suggest_memory_format call in SparseCsrTensorImpl::resize_as_sparse_csr_tensor_ module: sparse triaged open source better-engineering,2021-04-22 14:25:28+00:00,,0,0,module: sparse triaged open source better-engineering,True
56694,PyTorch 1.8.0 / 1.8.1 CUDA memory corruption on oddly sized tesnor sections of strided tensor module: cuda triaged,2021-04-22 14:08:24+00:00,,0,2,module: cuda triaged,True
56691,[structured] at::cpu_unchecked functions triaged module: structured kernels,2021-04-22 13:59:40+00:00,,0,0,triaged module: structured kernels,True
56688,Torch FX does not work with torchvision model triaged module: fx,2021-04-22 11:07:39+00:00,,1,1,triaged module: fx,True
56678,Segmentation fault on Pytorch 1.8.1 + cuda 11.1 on  GTX 1050 Ti  high priority needs reproduction module: crash module: cuda triaged,2021-04-22 07:28:57+00:00,,1,4,high priority needs reproduction module: crash module: cuda triaged,True
56671,{h / v / d}split methods are missing triaged module: numpy,2021-04-22 05:59:17+00:00,,0,0,triaged module: numpy,True
56660,consistency jit tests gives error for gradient operator oncall: jit,2021-04-22 02:44:39+00:00,,0,8,oncall: jit,True
56634,[package] Module name reported in error message does not always match what is needed to extern/mock it triaged,2021-04-21 21:41:20+00:00,,0,0,triaged,True
56623,new torch.profiler slow for large number of events? triaged oncall: profiler,2021-04-21 20:46:58+00:00,,1,1,triaged oncall: profiler,True
56595,"TorchScript breaks (""internal assert failed"") on `torch.set_grad_enabled` oncall: jit",2021-04-21 17:20:45+00:00,,1,3,oncall: jit,False
56586,"Support multi-dim reductions for torch.prod, torch.all, torch.any triaged enhancement module: reductions",2021-04-21 14:47:02+00:00,,0,2,triaged enhancement module: reductions,True
56583,"All thread stack trace dumps interleave output in distributed tests, giving garbled output oncall: distributed triaged",2021-04-21 14:40:06+00:00,,0,0,oncall: distributed triaged,True
56571,Python static variable access for TorchScript's custom C++ classes oncall: jit,2021-04-21 07:50:08+00:00,,2,1,oncall: jit,False
56542,CrossEntropyLoss target shape expectation is inconsistent with BCEWithLogits triaged module: shape checking,2021-04-20 21:49:26+00:00,,0,0,triaged module: shape checking,True
56525,Can't deepcopy memory format objects triaged module: memory format,2021-04-20 19:52:05+00:00,,0,1,triaged module: memory format,True
56522,[cpp op] TORCH_LIBRARY schema doesn't respect keyword only module: cpp-extensions module: cpp triaged,2021-04-20 19:38:56+00:00,,0,0,module: cpp-extensions module: cpp triaged,True
56500,Marking variable not required in backward calculation if it is not needed feature module: autograd triaged actionable,2021-04-20 17:49:45+00:00,,0,13,feature module: autograd triaged actionable,True
56489,[torchdeploy] ar: _bz2module.o: No such file or directory  triaged module: deploy,2021-04-20 16:29:19+00:00,,0,6,triaged module: deploy,True
56485, Sparse tensor CSR layout for CUDA module: sparse module: cuda triaged,2021-04-20 15:29:06+00:00,,0,0,module: sparse module: cuda triaged,True
56483,Add compute_residuals flag for torch.linalg.lstsq triaged enhancement module: numpy module: linear algebra,2021-04-20 15:01:19+00:00,,0,3,triaged enhancement module: numpy module: linear algebra,True
56482,unsafe_reclaim_from_nonowning is not that unsafe module: internals module: bootcamp triaged,2021-04-20 14:36:25+00:00,,0,0,module: internals module: bootcamp triaged,True
56480,shared torch.tensor with multiprocesses using python Queue cause coredump  module: multiprocessing triaged shadow review,2021-04-20 13:49:51+00:00,,0,4,module: multiprocessing triaged shadow review,True
56465,Provide a pkg-config file module: build feature triaged,2021-04-20 11:24:53+00:00,,0,0,module: build feature triaged,True
56464,Scatter tests missing when passing a reduction operation. triaged module: scatter & gather ops,2021-04-20 10:31:20+00:00,,1,0,triaged module: scatter & gather ops,True
56460,"I meet an error assert key in deserialized_objects when I torch.load(pthname),and pth file is trained on multi gpu oncall: distributed module: serialization triaged module: ddp",2021-04-20 08:32:57+00:00,,0,1,oncall: distributed module: serialization triaged module: ddp,True
56440,Raise exception when torch.clamp min value underflows the input tensor's dtype module: error checking triaged,2021-04-20 02:03:37+00:00,,0,3,module: error checking triaged,True
56417,Building with USE_TENSORPIPE=0 causes errors on import torch for MacOS oncall: distributed module: tensorpipe,2021-04-19 21:11:09+00:00,,0,3,oncall: distributed module: tensorpipe,False
56399,Figure out what mojo FB common/process/StackTrace.h has that we don't module: internals triaged better-engineering,2021-04-19 19:19:03+00:00,,0,0,module: internals triaged better-engineering,True
56397,Symbolicate crashes from releases triaged,2021-04-19 18:48:29+00:00,,0,0,triaged,True
56396,Tests in CI are run from the test/ directory module: docs module: tests triaged,2021-04-19 18:48:26+00:00,,1,2,module: docs module: tests triaged,True
56390,Distributed tests don't always check the exit code of worker processes oncall: distributed module: flaky-tests module: ddp,2021-04-19 18:22:48+00:00,,1,4,oncall: distributed module: flaky-tests module: ddp,True
56370,"Acquiring ""is_grad_enabled()"" inside an autograd function  feature module: autograd triaged module: custom-operators actionable",2021-04-19 11:11:59+00:00,,0,5,feature module: autograd triaged module: custom-operators actionable,True
56360,Support for stereo audio data in from torch.utils.tensorboard.SummaryWriter feature triaged module: tensorboard module: data,2021-04-19 07:50:15+00:00,,0,0,feature triaged module: tensorboard module: data,True
56357,[NNC] All NNC APIs should return bool flag NNC,2021-04-19 06:58:31+00:00,,1,0,NNC,False
56356,"Resolved: Only add type promotion support to unary pwise, binary pwise, and reduction operations triaged module: type promotion module: ux",2021-04-19 06:43:08+00:00,,0,6,triaged module: type promotion module: ux,True
56350,logcumsumexp (and maybe other cum* ops) has divergent CUDA and CPU out= behavior module: cuda triaged module: safe resize,2021-04-19 05:06:58+00:00,,0,2,module: cuda triaged module: safe resize,True
56340,log_softmax(x) != x - logsumexp(x) module: numerical-stability triaged,2021-04-18 13:52:52+00:00,,0,2,module: numerical-stability triaged,True
56333,Unable to pip install PyTorch on M1 Mac - Errors module: build triaged module: macos module: arm,2021-04-18 00:15:45+00:00,,0,7,module: build triaged module: macos module: arm,True
56330,Type checking doesn't match actual type in error message module: error checking triaged,2021-04-17 13:26:46+00:00,,0,0,module: error checking triaged,True
56329,Change `other` to `src` in torch.Tensor.scatter_add_ module: docs triaged,2021-04-17 09:33:08+00:00,,0,3,module: docs triaged,True
56328,Why does output_padding have constraints in ConvTranspose1d? module: docs triaged,2021-04-17 09:26:22+00:00,,0,0,module: docs triaged,True
56314,test_variant_consistency_jit fails for torch.tensordot with dtype float32 with error INTERNAL ASSERT FAILED oncall: jit module: tests,2021-04-17 00:54:43+00:00,,0,2,oncall: jit module: tests,True
56300,torch.where with input Tensor and other Scalar raises type mismatch error triaged module: type promotion,2021-04-16 21:34:14+00:00,,0,2,triaged module: type promotion,True
56297,Audit destructors of classes bound using pybind11::class_ to see if they can block; such cases can deadlock triaged module: pybind,2021-04-16 20:57:38+00:00,,0,2,triaged module: pybind,True
56285,qnnpack uses deprecated pthreadpool APIs triaged,2021-04-16 18:34:23+00:00,,0,0,triaged,True
56275,cumprod gradgradcheck fails in fast_mode=True module: autograd module: tests triaged,2021-04-16 16:14:41+00:00,,0,0,module: autograd module: tests triaged,True
56263,Reflect padding_mode should be supported for Conv3d module: docs module: convolution triaged,2021-04-16 12:27:27+00:00,,0,0,module: docs module: convolution triaged,True
56262,torch.nn.quantized.functional.conv_transpose1d/2d/3d support module: docs oncall: quantization feature low priority triaged,2021-04-16 11:47:46+00:00,,1,2,module: docs oncall: quantization feature low priority triaged,True
56246,Silent incorrect running with zero padding for Conv1d module: error checking module: convolution triaged module: padding,2021-04-16 09:48:13+00:00,,0,0,module: error checking module: convolution triaged module: padding,True
56244,Cuda RPC error when using then() oncall: distributed module: tensorpipe,2021-04-16 08:34:10+00:00,,0,6,oncall: distributed module: tensorpipe,True
56237,optim.Adadelta: local variable 'lr' referenced before assignment module: optimizer triaged,2021-04-16 06:21:31+00:00,,0,1,module: optimizer triaged,False
56235,`test_cholesky_solve` gradgradcheck fails sometimes module: autograd module: tests triaged module: linear algebra module: magma,2021-04-16 05:19:58+00:00,,0,1,module: autograd module: tests triaged module: linear algebra module: magma,True
56231,Half precision support for torch.sparse.mm feature triaged module: half,2021-04-16 04:33:30+00:00,,0,0,feature triaged module: half,True
56224,Error message regarding Padding of Conv2d needs improving module: convolution triaged module: padding,2021-04-16 02:55:21+00:00,,0,0,module: convolution triaged module: padding,True
56202,Tolerance for non-determinism operators in gradcheck module: autograd module: tests triaged module: determinism,2021-04-15 22:17:44+00:00,,0,0,module: autograd module: tests triaged module: determinism,True
56191,[FX] Issues with names of functions in user packages module: bootcamp triaged module: fx,2021-04-15 21:09:07+00:00,,1,0,module: bootcamp triaged module: fx,True
56187,AVX512 and Vec512 feature triaged module: vectorization,2021-04-15 20:41:57+00:00,,0,94,feature triaged module: vectorization,True
56183,"scripting LSTM module generates a script that has no .code, no .graph, and cannot be executed.  oncall: jit triaged",2021-04-15 19:51:53+00:00,,1,5,oncall: jit triaged,True
56177,torch.gradient  not throwing error when spacing to be equal to 0 module: autograd low priority triaged,2021-04-15 18:43:40+00:00,,0,0,module: autograd low priority triaged,True
56161,"[rfc] Add a ""core only"" build flag module: build triaged enhancement better-engineering",2021-04-15 17:54:24+00:00,,0,2,module: build triaged enhancement better-engineering,True
56159,Addition/subtraction/multiplication/division of bool variables are not supported in JIT oncall: jit,2021-04-15 17:29:31+00:00,,1,1,oncall: jit,False
56144,libtorch static linking results in undefined references to onnx_torch and caffe2::EmbeddingLookup triaged module: undefined reference,2021-04-15 13:56:50+00:00,,0,4,triaged module: undefined reference,True
56136,"Please add a tab of ""all"" packages in `Docs` option module: docs triaged",2021-04-15 09:55:39+00:00,,0,1,module: docs triaged,False
56127,torch.profiler does not work out of the box on nightly triaged oncall: profiler,2021-04-15 07:30:04+00:00,,0,3,triaged oncall: profiler,True
56126,AttributeError: module 'torch._C' has no attribute 'ComplexDoubleStorageBase' module: build triaged,2021-04-15 07:15:52+00:00,,0,2,module: build triaged,True
56111,ProcessGroupGlooTest and other distributed tests don't capture stdout/stderr in test UI view oncall: distributed,2021-04-15 03:14:12+00:00,,0,1,oncall: distributed,False
56110,caffe2/utils/signal_handler.cc is failing to symbolize libtorch_cpu.so module: build triaged,2021-04-15 03:08:17+00:00,,0,3,module: build triaged,True
56100,[package] implicit externing can miss cases where the stdlib depends on further stdlib modules triaged,2021-04-15 00:07:53+00:00,,0,3,triaged,True
56096,[JIT] Merge/reconcile `__jit_unused_properties` and `__jit_ignored_attributes__` oncall: jit,2021-04-14 23:39:15+00:00,,1,1,oncall: jit,False
56090,Converting float16->bool causes an internal compiler error with llvm NNC,2021-04-14 23:04:03+00:00,,1,1,NNC,True
56067,A more flexible torch.hub search strategy feature triaged module: hub,2021-04-14 21:22:38+00:00,,0,0,feature triaged module: hub,True
56064,Optionally include padding_idx items in the EmbeddingBag reduction feature module: nn triaged module: embedding,2021-04-14 20:59:48+00:00,,0,0,feature module: nn triaged module: embedding,True
56038,[Meta] PyTorch features build/test matrix module: build module: ci triaged,2021-04-14 18:05:23+00:00,,0,1,module: build module: ci triaged,True
56030,Add Dirichlet Multinomial to PyTorch Distributions module: distributions triaged function request,2021-04-14 16:04:59+00:00,,0,3,module: distributions triaged function request,True
56027,Add an extensibility point for ThreadLocalState triaged function request,2021-04-14 15:58:38+00:00,,0,3,triaged function request,False
56024,Support for mkldnn + ddp oncall: distributed module: mkldnn,2021-04-14 15:46:03+00:00,,0,0,oncall: distributed module: mkldnn,False
56023,[Opinfo] Better ErrorMsg for test_out with wrong shape module: tests triaged,2021-04-14 15:39:22+00:00,,0,1,module: tests triaged,True
56019,when building apk with libtorch needs reproduction module: build triaged,2021-04-14 15:14:48+00:00,,0,1,needs reproduction module: build triaged,True
56012,Failure in complex CUDA numerics tests for sigmoid triaged module: complex,2021-04-14 12:06:01+00:00,,0,0,triaged module: complex,True
56010,[tensorboard] Expected data-type of images in tensorboard SummaryWriter oncall: visualization,2021-04-14 11:34:15+00:00,,0,0,oncall: visualization,False
56008,Segmentation fault when using torch.profiler high priority needs reproduction module: crash triaged oncall: profiler,2021-04-14 10:07:36+00:00,,1,6,high priority needs reproduction module: crash triaged oncall: profiler,True
56007,NNAPI support for torch.cat([...]) when dim=-1 enhancement oncall: mobile,2021-04-14 10:02:23+00:00,,0,0,enhancement oncall: mobile,False
56006,Unable to get a Vulkan tensor using `to('vulkan')` oncall: mobile module: vulkan,2021-04-14 09:21:56+00:00,,0,1,oncall: mobile module: vulkan,False
55999,[JIT][ProfilingExecutor] A reliable and consistent API/protocol to query & propagate profiling data oncall: jit,2021-04-14 07:04:41+00:00,,1,1,oncall: jit,True
55981,"LazyEmbedding, an embedding layer with a dynamically sized vocabulary feature module: nn triaged module: embedding",2021-04-14 01:40:36+00:00,,0,13,feature module: nn triaged module: embedding,True
55967,"Cannot init, destroy, and then re-init process groups oncall: distributed triaged",2021-04-13 22:37:53+00:00,,1,2,oncall: distributed triaged,True
55964,Finish deprecating torch.range triaged module: deprecation,2021-04-13 22:17:40+00:00,,0,0,triaged module: deprecation,True
55953,Deprecations tracking issue high priority triaged module: deprecation tracker,2021-04-13 20:46:00+00:00,,1,2,high priority triaged module: deprecation tracker,True
55952,Compiler warnings tracking triaged module: build warnings,2021-04-13 20:41:59+00:00,,0,2,triaged module: build warnings,True
55951,Update linspace and logspace to throw an error when steps is not provided triaged module: deprecation module: tensor creation,2021-04-13 20:40:41+00:00,,0,0,triaged module: deprecation module: tensor creation,True
55948,Deprecate torch.stft returning real-valued tensors and torch.istft accepting real-valued inputs triaged module: deprecation module: fft,2021-04-13 20:31:09+00:00,,1,3,triaged module: deprecation module: fft,True
55945,Quantizable LSTMCell does not work correctly. oncall: quantization low priority triaged,2021-04-13 19:58:01+00:00,,1,7,oncall: quantization low priority triaged,True
55944,Sparse-sparse matrix multiplication only works with torch.sparse.mm() module: sparse triaged module: ux,2021-04-13 19:54:49+00:00,,0,1,module: sparse triaged module: ux,True
55941,[Tracking] Remove unneeded BC duplicates from native_functions.yaml triaged better-engineering,2021-04-13 19:03:06+00:00,,0,0,triaged better-engineering,True
55937,test_stream_event_nogil: Is the test making a wrong assumption? in progress module: rocm module: tests triaged actionable,2021-04-13 18:31:11+00:00,,0,4,in progress module: rocm module: tests triaged actionable,True
55933,[NNC] Replace ComputeAt NNC,2021-04-13 17:53:42+00:00,,1,18,NNC,False
55912,Support float_qparams in quantized_clone oncall: quantization low priority triaged,2021-04-13 15:37:21+00:00,,1,0,oncall: quantization low priority triaged,True
55910,Allow variable intermediate hidden dimensions for stacked RNN/LSTM layers module: nn module: rnn triaged enhancement,2021-04-13 15:11:33+00:00,,0,2,module: nn module: rnn triaged enhancement,True
55907,test_variant_consistency_eager_addbmm fails on both cpu and cuda module: tests triaged module: linear algebra,2021-04-13 14:55:21+00:00,,0,0,module: tests triaged module: linear algebra,True
55905,Attempt to use jited `torch.isnan` hit internal assert oncall: jit module: crash module: ci,2021-04-13 14:18:49+00:00,,1,5,oncall: jit module: crash module: ci,True
55902,Warn users when cross entropy is called after softmax module: nn module: loss triaged,2021-04-13 13:48:28+00:00,,0,4,module: nn module: loss triaged,True
55898,How should I warmup JIT properly ? oncall: jit,2021-04-13 13:14:14+00:00,,1,2,oncall: jit,False
55884,[Bug] RuntimeError: could not create a primitive on Xeon triaged module: backend module: intel,2021-04-13 06:36:10+00:00,,0,3,triaged module: backend module: intel,True
55879,Vulkan backend on desktop platforms triaged module: backend module: vulkan,2021-04-13 05:11:57+00:00,,0,10,triaged module: backend module: vulkan,True
55876,[rfc] Trigger callback when backwards begins for DDP with custom autograd function oncall: distributed triaged module: ddp,2021-04-13 04:33:29+00:00,,0,1,oncall: distributed triaged module: ddp,True
55869,We should perhaps prevent linking against LibTorch DLLs with the wrong configuration (debug vs release) module: windows triaged,2021-04-13 03:02:09+00:00,,0,4,module: windows triaged,True
55865,Don't define __assert_fail on systems using `musl-dev` headers module: build triaged,2021-04-13 02:09:59+00:00,,0,0,module: build triaged,True
55804,Add torch.nn.Conv2D correctness test module: nn module: tests triaged enhancement,2021-04-12 17:19:11+00:00,,1,1,module: nn module: tests triaged enhancement,True
55790,Update contributing.md docs review guidelines module: docs triaged,2021-04-12 15:45:15+00:00,,0,2,module: docs triaged,False
55784,pybind11 Tensor type caster forces reference count bump triaged module: pybind,2021-04-12 14:25:51+00:00,,0,3,triaged module: pybind,True
55782,YoloV4 Pytorch model inference fails on NNAPI Android oncall: mobile,2021-04-12 12:45:44+00:00,,0,0,oncall: mobile,False
55779,Request setNumThreads API for Java enhancement oncall: mobile,2021-04-12 09:10:17+00:00,,0,0,enhancement oncall: mobile,False
55777,torch.load is very slow with gzip.open module: serialization triaged has workaround,2021-04-12 08:52:13+00:00,,0,1,module: serialization triaged has workaround,True
55764,clang format on OS X ssl verification failure module: lint triaged,2021-04-12 00:52:35+00:00,,0,0,module: lint triaged,True
55757,Synchronize RRef.to_here() CUDA Streams properly when the profiler is enabled triaged module: rpc,2021-04-11 20:52:05+00:00,,0,1,triaged module: rpc,True
55756,Cuda OOM after several steps for DataParallel model module: cuda module: memory usage triaged,2021-04-11 19:16:53+00:00,,0,6,module: cuda module: memory usage triaged,True
55755,"Some float16 inputs to CPU matmul are supported, others aren't triaged module: half module: linear algebra",2021-04-11 16:57:32+00:00,,0,4,triaged module: half module: linear algebra,True
55752,"[question] Influence of divisibility of B, C, T by 16 on Conv1d (and Conv2d perf) with CuDNN, including presence of padding module: performance module: docs triaged",2021-04-11 15:24:29+00:00,,0,2,module: performance module: docs triaged,True
55751,[docs] algolia docsearch in pytorch docs triaged module: doc infra,2021-04-11 12:23:57+00:00,,0,0,triaged module: doc infra,False
55749,Fail to introduce torch::jit::Module as a parameter of a cusomized operator. oncall: jit,2021-04-11 09:39:15+00:00,,0,3,oncall: jit,True
55742,[JIT] TorchScript to represent typed tensor annotations in IR graph oncall: jit,2021-04-10 20:12:35+00:00,,1,5,oncall: jit,True
55736,RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)` module: cuda triaged,2021-04-10 08:16:35+00:00,,0,11,module: cuda triaged,True
55696,[TensorExpr] IRSimplifier fails to optimize useless stores NNC,2021-04-09 18:29:25+00:00,,0,0,NNC,False
55694,symbolic tracing does not support patterns with multiple arguments? triaged module: fx,2021-04-09 17:59:43+00:00,,0,0,triaged module: fx,True
55669,Segmentation fault when loss.backward() module: crash module: cuda triaged,2021-04-09 10:33:20+00:00,,0,5,module: crash module: cuda triaged,True
55668,GraphModule.to_folder generates code with syntax error (imports in the middle of a class) [torch.fx] triaged module: fx,2021-04-09 09:28:39+00:00,,1,3,triaged module: fx,True
55663,torch.nn.functional.log_softmax on Arm CPU gives partly NaN results needs reproduction triaged module: arm,2021-04-09 07:48:46+00:00,,0,3,needs reproduction triaged module: arm,True
55661,"RuntimeError: ""max_cuda"" not implemented for 'ComplexFloat' triaged module: complex enhancement",2021-04-09 07:33:20+00:00,,0,8,triaged module: complex enhancement,True
55655,Performance debugging / warning mode module: performance triaged enhancement,2021-04-09 03:10:05+00:00,,0,8,module: performance triaged enhancement,True
55643,at::globalContext().hasCUDA()'s forked tongue tells lies module: multiprocessing module: cuda triaged,2021-04-08 21:39:18+00:00,,0,2,module: multiprocessing module: cuda triaged,True
55629,torch.arctanh not implemented for torch.float16 triaged module: half,2021-04-08 18:25:41+00:00,,0,5,triaged module: half,True
55611,ATen/native/sparse headers not in LibTorch or pypi package module: binaries triaged,2021-04-08 14:11:29+00:00,,0,0,module: binaries triaged,True
55609,requires_grad does not get propagated properly when using the JIT compiler module: autograd triaged,2021-04-08 13:35:03+00:00,,1,8,module: autograd triaged,True
55607,pytorch inference lead to memory leak in cpu module: memory usage triaged,2021-04-08 12:08:04+00:00,,0,4,module: memory usage triaged,True
55603,Faster Inference time for first .pt model prediction oncall: jit,2021-04-08 08:39:20+00:00,,1,8,oncall: jit,False
55597,[ONNX] Incorrect handling of tuple multiplication with zero module: onnx triaged onnx-triaged,2021-04-08 06:16:02+00:00,,0,4,module: onnx triaged onnx-triaged,True
55585,"[amp scaler] unable to prevent ""scheduler before optimizer step"" warning module: optimizer triaged module: amp (automated mixed precision)",2021-04-08 03:18:23+00:00,,0,2,module: optimizer triaged module: amp (automated mixed precision),True
55583,Improve test runtime of distributed tests. oncall: distributed module: tests triaged better-engineering,2021-04-08 02:54:06+00:00,,0,2,oncall: distributed module: tests triaged better-engineering,True
55577,Source location range issue for dictionary  oncall: jit module: bootcamp good first issue OSS contribution wanted,2021-04-08 00:34:33+00:00,,1,29,oncall: jit module: bootcamp good first issue OSS contribution wanted,True
55571,Update TCPStore.wait() error message to be more friendly oncall: distributed triaged,2021-04-07 23:20:46+00:00,,1,1,oncall: distributed triaged,True
55557,Gradcheck failure for `torchaudio.functional.phase_vocoder` module: autograd triaged module: correctness (silent),2021-04-07 20:42:51+00:00,,1,2,module: autograd triaged module: correctness (silent),True
55549,`embedding_bag` has unexpected behavior when given `offsets` that are not monotonically increasing module: nn triaged module: embedding,2021-04-07 19:55:19+00:00,,1,1,module: nn triaged module: embedding,True
55544,[FX] See if we can type-annotate attribute accesses on proxy triaged module: fx,2021-04-07 19:44:08+00:00,,0,0,triaged module: fx,True
55541,`torch.jit.interface` does not understand dunder methods (e.g. `__call__`) oncall: jit,2021-04-07 19:39:02+00:00,,1,1,oncall: jit,False
55464,Support quantized functional linear/conv operators with 32b output oncall: quantization feature low priority triaged,2021-04-07 15:46:10+00:00,,0,4,oncall: quantization feature low priority triaged,True
55461,Cross-compiling for cortexa9 processor module: build triaged,2021-04-07 14:54:33+00:00,,0,0,module: build triaged,True
55457,[feature request] TupleSequential module or an option for Sequential for unpacking tuples (i.e. support varargs) module: nn triaged,2021-04-07 13:19:09+00:00,,0,4,module: nn triaged,False
55454,Include the cuda headers in the libtorch pre-built packages module: binaries module: build module: cuda triaged,2021-04-07 10:50:45+00:00,,0,3,module: binaries module: build module: cuda triaged,True
55453,C++ sparse_coo_tensor ignores TensorOptions argument module: sparse triaged module: tensor creation,2021-04-07 10:38:27+00:00,,0,4,module: sparse triaged module: tensor creation,True
55450,[DOC] Incorrect image for the confusion matrix module: docs module: rnn triaged,2021-04-07 07:54:34+00:00,,0,2,module: docs module: rnn triaged,True
55448,"Errors occur when operating broadcast with error log: Got completion with error 12, opcode 1, len 32547, vendor err 129 oncall: distributed triaged",2021-04-07 06:48:02+00:00,,0,2,oncall: distributed triaged,True
55442,More AVX2 vectorization support for half (float16) triaged module: vectorization module: half,2021-04-07 04:57:32+00:00,,0,6,triaged module: vectorization module: half,True
55440,Have the ability to disable `__torch_function__` dispatch for torch.nn.functional functions module: nn triaged module: __torch_function__,2021-04-07 03:07:56+00:00,,0,1,module: nn triaged module: __torch_function__,True
55437,Error building docker image: No module named 'typing_extensions' module: build triaged module: docker,2021-04-07 00:34:19+00:00,,0,4,module: build triaged module: docker,True
55401,[FX][testing] Add symbolic tracing tests for torchaudio triaged module: fx,2021-04-06 18:17:57+00:00,,0,0,triaged module: fx,True
55400,[FX][testing] Add symbolic tracing tests for torchtext triaged module: fx,2021-04-06 18:16:40+00:00,,0,0,triaged module: fx,True
55399,[FX][testing] Test symbolic tracing of detectron2 triaged module: fx,2021-04-06 18:13:34+00:00,,0,1,triaged module: fx,True
55397,[FX][testing] Run symbolic tracing tests over torch benchmark in CI triaged module: fx,2021-04-06 18:07:38+00:00,,0,0,triaged module: fx,True
55395,[FX][testing] Test tracing into all the standard `torch.nn.functional` instances triaged module: fx,2021-04-06 17:59:09+00:00,,1,0,triaged module: fx,True
55394,[FX][testing] Test tracing into all the standard `nn.Module` instances triaged module: fx,2021-04-06 17:58:00+00:00,,1,0,triaged module: fx,True
55375,`DataParallel` (`broadcast_coalesced`) with complex tensors yield real views high priority oncall: distributed triaged module: complex module: data parallel,2021-04-06 13:32:17+00:00,,0,11,high priority oncall: distributed triaged module: complex module: data parallel,True
55374,[RFC] Extend Autocast to CPU/CUDA with BF16 data type feature triaged module: bfloat16 module: amp (automated mixed precision),2021-04-06 12:12:38+00:00,,0,39,feature triaged module: bfloat16 module: amp (automated mixed precision),True
55366,bool_tensor.sum(dtype=torch.int32) creates int32-copy of the original int8 tensor  triaged module: boolean tensor module: reductions,2021-04-06 10:00:00+00:00,,0,5,triaged module: boolean tensor module: reductions,True
55363,it is lack of compatibility of CUDA_HOME (or other env) module: cpp-extensions module: cuda triaged,2021-04-06 09:17:44+00:00,,0,5,module: cpp-extensions module: cuda triaged,True
55356,torch.allclose does not allow different types for comparison triaged module: numpy module: type promotion,2021-04-06 05:42:05+00:00,,0,4,triaged module: numpy module: type promotion,True
55355,simple matrix multiplication yields wrong result on Ampere (3080) triaged module: cublas module: correctness (silent) module: tf32,2021-04-06 04:18:53+00:00,,0,2,triaged module: cublas module: correctness (silent) module: tf32,True
55340,[RFC] Plan to deduplicate test_c10d and distributed_test  oncall: distributed triaged better-engineering,2021-04-05 22:42:11+00:00,,1,6,oncall: distributed triaged better-engineering,True
55331,Tests should be runnable without run_test.py triaged module: testing,2021-04-05 21:49:36+00:00,,0,1,triaged module: testing,True
55330,CI not surfacing some failures (-Werror?) on PR module: ci triaged,2021-04-05 21:42:16+00:00,,0,3,module: ci triaged,True
55329,Split test/cpp_extensions/setup.py per orthogonal extension module and/or do Python build from ninja too module: build module: cpp-extensions triaged,2021-04-05 21:36:23+00:00,,0,0,module: build module: cpp-extensions triaged,True
55304,nn.DataParallel should raise error when provided with list of tensors triaged module: data parallel,2021-04-05 15:24:37+00:00,,0,0,triaged module: data parallel,True
55299,[feature] `torch.polygamma` : Support Tensor for argument `n` triaged function request module: special,2021-04-05 10:29:29+00:00,,0,0,triaged function request module: special,False
55297,SIGSEGV at at::is_vulkan_available() invocation on Android triaged module: android,2021-04-05 06:54:22+00:00,,0,0,triaged module: android,True
55296,Build errors with USE_VULKAN=ON when cross-compiling for Android module: build triaged module: android,2021-04-05 06:38:58+00:00,,0,1,module: build triaged module: android,True
55289,Feature Request: Add a rounding mode to round triaged module: numpy function request,2021-04-04 17:22:32+00:00,,0,3,triaged module: numpy function request,False
55286,endpoint=False for torch.linspace and torch.logspace triaged enhancement module: numpy module: tensor creation,2021-04-04 14:15:01+00:00,,0,1,triaged enhancement module: numpy module: tensor creation,True
55282,Poor error message when trying to jit a function instead of a module (RuntimeError: Cannot insert a Tensor that requires grad as a constant.) oncall: jit,2021-04-04 01:13:04+00:00,,1,0,oncall: jit,True
55279,[Feature Pitch] Full-batch optimization toolkit module: optimizer triaged,2021-04-03 17:41:00+00:00,,0,8,module: optimizer triaged,False
55277,Small model occupies too much GPU in CUDA11.1 + Torch1.8.1 but is normal in Torch 1.6 + CUDA10.1 module: performance module: cuda module: memory usage triaged,2021-04-03 16:34:50+00:00,,0,4,module: performance module: cuda module: memory usage triaged,True
55274,[Feature Request] PowerNorm feature triaged,2021-04-03 10:20:30+00:00,,0,0,feature triaged,False
55267,Improve CUDA extension building experience module: binaries module: cuda triaged enhancement,2021-04-03 01:48:36+00:00,,1,26,module: binaries module: cuda triaged enhancement,True
55261,Batched multi_dot / chain_matmul + let it accept a tensor instead of tuple triaged enhancement module: linear algebra,2021-04-02 22:59:40+00:00,,0,4,triaged enhancement module: linear algebra,True
55260,Cuda memory leak check is somehow unstable with repeat_test_for_types module: memory usage triaged module: testing,2021-04-02 22:45:01+00:00,,0,0,module: memory usage triaged module: testing,True
55230,Profile Optionals and Optional[Tensor] specifically oncall: jit triaged,2021-04-02 16:11:19+00:00,,0,7,oncall: jit triaged,True
55228,Requesting a feature to calculate attention weights. triaged oncall: transformer/mha,2021-04-02 15:04:58+00:00,,0,2,triaged oncall: transformer/mha,False
55211,[TorchScript] Can we support type refinement by Python type annotation? oncall: jit,2021-04-02 04:38:49+00:00,,1,3,oncall: jit,False
55207,[RFC] Model Sharding for distributed training oncall: distributed triaged,2021-04-02 03:33:51+00:00,,0,17,oncall: distributed triaged,True
55198,[FX] to_folder breaks with qualified type name module: bootcamp triaged module: fx,2021-04-02 01:01:38+00:00,,1,1,module: bootcamp triaged module: fx,True
55194,Tracing `len(tensor)` gets static constant value in result graph oncall: jit,2021-04-02 00:03:04+00:00,,0,0,oncall: jit,False
55192,"Consider adding meta:1, meta:2 devices triaged enhancement module: structured kernels module: meta tensors",2021-04-01 23:39:01+00:00,,0,0,triaged enhancement module: structured kernels module: meta tensors,True
55191,"`torch.tensor(..., device='meta')` doesn't work module: bootcamp triaged",2021-04-01 23:38:05+00:00,,0,5,module: bootcamp triaged,True
55174,Isolate CPU tests from GPU tests module: tests triaged actionable,2021-04-01 21:05:58+00:00,,0,7,module: tests triaged actionable,True
55161,Internal assert failed: `iter.device(arg).is_cuda()` module: error checking triaged,2021-04-01 17:13:45+00:00,,0,2,module: error checking triaged,True
55159,Better syntax for OpInfo module: tests triaged better-engineering,2021-04-01 16:40:02+00:00,,0,4,module: tests triaged better-engineering,True
55156,multiprocessing function cannot pass cuda objects use when calling inside from a DDP process oncall: distributed triaged module: ddp,2021-04-01 16:21:19+00:00,,0,0,oncall: distributed triaged module: ddp,True
55152,Better handling of OpInfo.sample_inputs high priority module: tests triaged better-engineering needs design ezyang's list,2021-04-01 15:42:10+00:00,,0,7,high priority module: tests triaged better-engineering needs design ezyang's list,True
55144,Parameterless model still has extra inputs when exported to ONNX + insufficient checking of argument export_params module: onnx triaged onnx-triaged,2021-04-01 12:50:45+00:00,,0,9,module: onnx triaged onnx-triaged,True
55143,Add suport to torch.gather for negative indices triaged enhancement module: advanced indexing,2021-04-01 12:37:20+00:00,,0,0,triaged enhancement module: advanced indexing,True
55135,LR scheduler step() behaviour with and without epoch parameter  module: optimizer triaged,2021-04-01 08:11:57+00:00,,0,0,module: optimizer triaged,True
55122,Cusolver handle may decrease MAGMA performance on GPU module: performance module: cuda triaged module: linear algebra module: magma,2021-04-01 02:45:10+00:00,,0,6,module: performance module: cuda triaged module: linear algebra module: magma,True
55120,Why isn't pip finding the correct pytorch dependency? module: binaries triaged,2021-04-01 01:25:59+00:00,,0,0,module: binaries triaged,True
55104,Alternative backend fallback like mechanism which has higher precedence than operator-specific composite implementations module: internals triaged,2021-03-31 20:45:54+00:00,,0,0,module: internals triaged,True
55095,Python dispatch key and backend fallback triaged module: __torch_function__,2021-03-31 19:51:48+00:00,,0,4,triaged module: __torch_function__,True
55093,Make it possible to skip only one hop of __torch_function__ override triaged module: __torch_function__,2021-03-31 19:33:52+00:00,,1,21,triaged module: __torch_function__,True
55083,[JIT][Testing] Split up test_jit.py and test_jit_py3.jit oncall: jit triaged,2021-03-31 18:44:39+00:00,,0,0,oncall: jit triaged,True
55072,[JIT][Testing] Replace use of `assertRaisesRegex` with `assertRaisesRegexWithHilight` oncall: jit,2021-03-31 18:30:04+00:00,,0,0,oncall: jit,False
55056,[FR] Safe softmax module: nn triaged enhancement,2021-03-31 15:25:59+00:00,,0,5,module: nn triaged enhancement,True
55045,16 bit accumulation for Convolution module: convolution triaged enhancement,2021-03-31 13:36:15+00:00,,0,0,module: convolution triaged enhancement,True
55037,High CPU using torch.stack/torch.cat on Windows module: performance module: windows triaged,2021-03-31 09:13:11+00:00,,0,12,module: performance module: windows triaged,True
55035,DataLoader performance drop on 4-channel images? needs reproduction module: dataloader triaged,2021-03-31 09:01:41+00:00,,0,1,needs reproduction module: dataloader triaged,True
55030,Add Minimal Gated Unit (MGU) module: nn triaged enhancement,2021-03-31 07:42:08+00:00,,0,1,module: nn triaged enhancement,True
55005,[TensorExpr] Provide a safe API to request the N-th loop for a given Buf. module: bootcamp triaged NNC,2021-03-30 22:55:22+00:00,,0,0,module: bootcamp triaged NNC,True
54983,Accept objects with `__float__` wherever regular `float`s are accepted feature triaged module: half,2021-03-30 19:44:51+00:00,,0,0,feature triaged module: half,True
54982,[RFC] A PyTorch Tensor Shape DSL For Symbolic Shape Inference oncall: jit,2021-03-30 19:12:47+00:00,,0,7,oncall: jit,False
54979,test_quantize_fx.py test_resnet_18_dpp test failure oncall: quantization low priority triaged module: fx,2021-03-30 18:48:27+00:00,,1,2,oncall: quantization low priority triaged module: fx,True
54975,CUBLAS_STATUS_EXECUTION_FAILED error on torch >= 1.8.0 and CUDA 11.1 needs reproduction module: cuda triaged,2021-03-30 18:36:12+00:00,,0,6,needs reproduction module: cuda triaged,True
54971,[1.8.x] Build from source with USE_VULKAN fails (requires C++20 instead of C++14) module: build triaged,2021-03-30 17:53:25+00:00,,0,1,module: build triaged,True
54947,Results surprisingly differ on different backends triaged module: numerical-reproducibility,2021-03-30 09:15:33+00:00,,0,3,triaged module: numerical-reproducibility,True
54943,Structured kernels have increased TensorIterator overhead. module: performance module: bootcamp triaged,2021-03-30 07:00:58+00:00,,0,0,module: performance module: bootcamp triaged,True
54942,Optimizations to TORCH_CHECK change inlining behavior. module: performance module: bootcamp triaged,2021-03-30 07:00:54+00:00,,0,1,module: performance module: bootcamp triaged,True
54941,Regression in Python arg parser performance. module: performance module: bootcamp triaged,2021-03-30 07:00:51+00:00,,1,2,module: performance module: bootcamp triaged,True
54938,[tests] `cumprod` OpInfo tests take long time to run (around 1min) module: ci module: tests triaged,2021-03-30 06:12:03+00:00,,0,0,module: ci module: tests triaged,True
54937,[JIT] export fast-transformer to TorchScript oncall: jit,2021-03-30 03:35:14+00:00,,0,1,oncall: jit,False
54913,FX tracer doesn't support returning a new instance of `object` subclass from module forward triaged module: fx,2021-03-29 21:08:40+00:00,,1,3,triaged module: fx,True
54912,[autocast] DataParallel in a single process - possibly outdated docs oncall: distributed,2021-03-29 20:45:22+00:00,,0,2,oncall: distributed,False
54906,Out variants for Convolution ops module: convolution triaged,2021-03-29 19:47:51+00:00,,0,6,module: convolution triaged,True
54905,Allow CUDA build without requiring a physical GPU device. module: build module: cuda triaged enhancement,2021-03-29 19:42:45+00:00,,0,3,module: build module: cuda triaged enhancement,True
54899,[fx] forward references in annotations do not produce correct source code module: bootcamp triaged module: fx,2021-03-29 18:40:44+00:00,,0,0,module: bootcamp triaged module: fx,True
54897,Default generated meta functions for inplace operations don't report errors triaged,2021-03-29 18:31:12+00:00,,0,0,triaged,True
54881,Often the documentation does not match the actual signature of the function module: docs triaged module: __torch_function__,2021-03-29 16:03:43+00:00,,0,2,module: docs triaged module: __torch_function__,True
54874,Get a thread safe copy of torch::nn::Sequential object module: cpp module: nn triaged,2021-03-29 14:03:41+00:00,,0,6,module: cpp module: nn triaged,True
54871,Not possible to save dataloader in C++ module: dataloader triaged enhancement,2021-03-29 10:45:14+00:00,,0,2,module: dataloader triaged enhancement,True
54861,Occured error in loss.backward() when using sparse=True in Embedding layer module: sparse triaged module: embedding,2021-03-29 06:19:12+00:00,,0,0,module: sparse triaged module: embedding,True
54843,Inconsistent behaviour with `weight` argument in CrossEntropyLoss and BCEWithLogitsLoss module: loss triaged,2021-03-28 08:38:46+00:00,,0,0,module: loss triaged,True
54841,[testing] test_reference_numerics_extremal_clamp_cpu_bfloat16 fails on ci build with GCC5.4 and Python 3.6 module: tests triaged module: bfloat16,2021-03-28 06:26:51+00:00,,0,0,module: tests triaged module: bfloat16,True
54829,c++ libtorch config mismatch with python version module: build triaged,2021-03-27 14:06:19+00:00,,0,0,module: build triaged,True
54827,Import error needs reproduction triaged,2021-03-27 09:50:56+00:00,,0,2,needs reproduction triaged,True
54823,"Fixing pytorch distributed training (DDP) code send a SIGKILL signal on its own on most version of pytorch 1.6.0, 1.7.0, 1.7.1? oncall: distributed triaged",2021-03-27 03:06:17+00:00,,0,9,oncall: distributed triaged,False
54821,Decouple TorchScript compiler from operator library oncall: jit,2021-03-27 02:30:20+00:00,,0,3,oncall: jit,False
54816,Per channel weight observer for ConvTranspose oncall: quantization low priority triaged,2021-03-27 00:31:12+00:00,,1,12,oncall: quantization low priority triaged,True
54815,TensorExpr `LoopNest.get_loops_for` misbehaved after loop distribution transformation module: cpp triaged,2021-03-27 00:19:59+00:00,,0,0,module: cpp triaged,True
54802,[nnc] simplify is sometimes broken after computeAt NNC,2021-03-26 22:19:51+00:00,,0,0,NNC,False
54800,[package] Detection of dependencies introduced by torch.ops.load_library  triaged,2021-03-26 21:23:50+00:00,,1,1,triaged,True
54799,"Vertices=torch.matmul(vertices.unsqueeze(0), rotations_init), RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemmStridedBatched in CentOS needs reproduction module: cuda triaged module: numpy",2021-03-26 21:21:37+00:00,,0,13,needs reproduction module: cuda triaged module: numpy,True
54785,[FX] Cannot symbolically trace class method triaged module: fx,2021-03-26 17:54:44+00:00,,0,2,triaged module: fx,True
54774,"RuntimeError: ""log2"" ""_vml_cpu"" not implemented for 'Half' triaged module: numpy module: half",2021-03-26 16:04:49+00:00,,0,2,triaged module: numpy module: half,True
54766,Add `start_dim` and `end_dim` functionality for common reduction operations. feature triaged module: reductions,2021-03-26 09:09:42+00:00,,0,3,feature triaged module: reductions,False
54757,[nnc] computeAt and index flatten broken in presence of if statements NNC,2021-03-26 06:14:07+00:00,,0,1,NNC,False
54753,torch.sqrt for negative values should either return complex tensors or clearly throw a domain error/warning triaged module: complex module: numpy,2021-03-26 05:10:51+00:00,,0,5,triaged module: complex module: numpy,True
54743,[Feature request] Add batched matrix support for torch.diag triaged enhancement module: numpy module: batching module: linear algebra,2021-03-26 00:03:14+00:00,,0,4,triaged enhancement module: numpy module: batching module: linear algebra,True
54728,JIT prim ops v.s. dispatcher oncall: jit module: dispatch oncall: mobile,2021-03-25 20:43:42+00:00,,0,3,oncall: jit module: dispatch oncall: mobile,True
54716,Segmentation fault in PyTorch dataloader needs reproduction module: dataloader triaged,2021-03-25 18:44:25+00:00,,0,6,needs reproduction module: dataloader triaged,True
54713,Test test_large_cumprod_cuda_float16 gets killed due to (probably) OOM  module: memory usage module: tests triaged,2021-03-25 18:36:30+00:00,,0,2,module: memory usage module: tests triaged,True
54677,[docs] use sphinx link/reference checking features module: docs triaged module: doc infra better-engineering,2021-03-25 12:28:06+00:00,,0,4,module: docs triaged module: doc infra better-engineering,False
54655,qsize not implemented error module: multiprocessing triaged module: macos,2021-03-25 04:23:33+00:00,,0,4,module: multiprocessing triaged module: macos,True
54646,Optional enhanced logging for operator calls for backend implementor debugging triaged module: backend,2021-03-25 00:58:34+00:00,,0,6,triaged module: backend,True
54638,torch.jit.trace() fails on a GCN with sparse inputs and dense layers oncall: jit,2021-03-25 00:03:12+00:00,,0,0,oncall: jit,True
54630,torch-tensor-repr gdb macro doesn't seem to work in opt mode triaged,2021-03-24 22:19:22+00:00,,0,5,triaged,True
54629,"""Skipped!"" tests should have more descriptive skipped reason module: tests triaged enhancement",2021-03-24 21:29:06+00:00,,0,1,module: tests triaged enhancement,False
54625,"[torchscript] nn.Embedding(_weight), tensor.masked_fill_, and torch.autograd.grad causes wrong gradient oncall: jit",2021-03-24 20:45:07+00:00,,0,1,oncall: jit,False
54622,torch.pow returns incorrect value for 0^0j triaged module: complex module: numpy,2021-03-24 20:26:21+00:00,,0,1,triaged module: complex module: numpy,True
54620,"torch.pow(tensor, tensor) throws RuntimeError for dtype bool  triaged module: numpy",2021-03-24 20:05:41+00:00,,0,1,triaged module: numpy,True
54614,Dispatcher TLS to bypass loads of dispatch key from tensor arguments module: performance triaged enhancement module: dispatch,2021-03-24 18:50:57+00:00,,0,5,module: performance triaged enhancement module: dispatch,True
54612,Exceptions thrown in JIT interpreter always are translated into RuntimeError oncall: jit,2021-03-24 18:36:59+00:00,,1,6,oncall: jit,True
54602,if cannot be symbolically traced triaged module: fx,2021-03-24 17:20:25+00:00,,0,2,triaged module: fx,True
54599,Better documentation for `torch.jit.isinstance` oncall: jit module: docs docs-hackathon,2021-03-24 16:19:38+00:00,,1,0,oncall: jit module: docs docs-hackathon,False
54598,Type refinement with `isinstance` only works with generics oncall: jit TSRootCause:TypeRefinement,2021-03-24 16:05:05+00:00,,1,1,oncall: jit TSRootCause:TypeRefinement,False
54593,Multiple refinement with `isinstance` doesn't work oncall: jit TSRootCause:TypeRefinement,2021-03-24 15:28:14+00:00,,1,1,oncall: jit TSRootCause:TypeRefinement,False
54591,Various issues in derivatives.yaml module: autograd triaged actionable,2021-03-24 15:18:38+00:00,,0,0,module: autograd triaged actionable,True
54588,Make searchsorted and bucketize API consistent module: docs triaged module: numpy module: deprecation module: ux,2021-03-24 12:48:12+00:00,,0,5,module: docs triaged module: numpy module: deprecation module: ux,True
54578,cublasSgemmStridedBatched failure when calling grad of grad needs reproduction module: crash module: cuda triaged,2021-03-24 07:16:59+00:00,,0,5,needs reproduction module: crash module: cuda triaged,True
54577,Question when I use Pytorch Mobile needs reproduction triaged oncall: mobile,2021-03-24 07:13:57+00:00,,0,1,needs reproduction triaged oncall: mobile,False
54574,"Add OpInfo metadata for ""is_torch_functional"" and a skip for these ops in TestOperatorSignatures.test_get_torch_func_signature_exhaustive module: tests triaged module: fx",2021-03-24 05:57:03+00:00,,0,1,module: tests triaged module: fx,True
54566,Support docstring definition in torch/library.h operator definition triaged enhancement,2021-03-24 03:24:05+00:00,,0,0,triaged enhancement,False
54564,why single pytorch process is displayed on each GPUs with memory 0MB module: cuda triaged,2021-03-24 03:06:46+00:00,,0,2,module: cuda triaged,True
54546,Let backends specify a schema version when registering kernels module: internals triaged module: backend module: codegen,2021-03-23 21:42:19+00:00,,0,0,module: internals triaged module: backend module: codegen,True
54535,[ROCm] test failures during 4.1 upgrade high priority module: dependency bug module: rocm triaged,2021-03-23 19:59:50+00:00,,0,5,high priority module: dependency bug module: rocm triaged,True
54524,Store created by dist.new_group doesn't appear to respect timeout high priority triage review oncall: distributed triaged better-engineering,2021-03-23 18:07:15+00:00,,1,4,high priority triage review oncall: distributed triaged better-engineering,True
54503,torch.fx may have problem for annotations triaged module: fx,2021-03-23 14:33:38+00:00,,1,0,triaged module: fx,True
54499,Caught an integer overflow or wraparound in torch.nn.MaxPool1d triaged module: pooling,2021-03-23 12:32:09+00:00,,0,1,triaged module: pooling,True
54497,Caffe2 has undeclared dependencies caffe2 triaged,2021-03-23 11:44:22+00:00,,0,0,caffe2 triaged,True
54487,"RuntimeError: iter.device(arg).is_cuda() INTERNAL ASSERT FAILED at ""/opt/conda/conda-bld/pytorch_1607370116979/work/aten/src/ATen/native/cuda/Loops.cuh"":94, please report a bug to PyTorch.  needs reproduction module: cuda triaged module: TensorIterator",2021-03-23 06:04:25+00:00,,0,5,needs reproduction module: cuda triaged module: TensorIterator,True
54445,add tests to cover DDP on mixed dense-sparse models oncall: distributed triaged module: ddp,2021-03-22 20:21:39+00:00,,1,0,oncall: distributed triaged module: ddp,True
54440,BERT model from torchbenchmark fails inference after freezing. triaged jit-backlog,2021-03-22 18:20:56+00:00,,1,0,triaged jit-backlog,True
54435,[JIT] prim::ModuleContainerIndex doesnt work with freezing oncall: jit days,2021-03-22 17:25:56+00:00,,1,0,oncall: jit days,True
54418,Some Numba tests are failing high priority module: ci module: tests triaged module: numba,2021-03-22 14:40:14+00:00,,0,2,high priority module: ci module: tests triaged module: numba,True
54408,It is strange that PyTorch is slow on RTX 3090 module: performance module: cuda triaged,2021-03-22 09:57:25+00:00,,0,6,module: performance module: cuda triaged,True
54407,test_clamp fails on ppc64le oncall: jit triaged module: POWER NNC,2021-03-22 09:47:15+00:00,,0,4,oncall: jit triaged module: POWER NNC,True
54394,Half Normal Log_Prob not defined for 0  module: numerical-stability module: distributions triaged module: random module: half,2021-03-21 18:56:20+00:00,,0,1,module: numerical-stability module: distributions triaged module: random module: half,True
54393,[docs] torch.optim.lr_scheduler module: docs module: optimizer triaged,2021-03-21 17:56:51+00:00,,0,0,module: docs module: optimizer triaged,False
54389,"torch.lerp and torch._foreach_lerp should support uint8 inputs (for vision), int16 (for audio), int32/int64 (for generality)  without (up)casting to float for  and/or dtype argument  triaged enhancement module: interpolation",2021-03-21 12:31:57+00:00,,0,4,triaged enhancement module: interpolation,True
54388,"in WINDOWS, CUDA Out of Memory error but CUDA memory is almost empty module: windows module: cuda module: memory usage triaged",2021-03-21 12:27:16+00:00,,0,2,module: windows module: cuda module: memory usage triaged,True
54387,[docs] torch.lerp has unmatching signature and explained parameter names module: docs triaged module: interpolation,2021-03-21 12:24:41+00:00,,0,3,module: docs triaged module: interpolation,False
54375,[proposal] Integrate autograd graph visualizer for displaying crashed locations during detect_anomaly feature module: autograd triaged,2021-03-20 19:59:52+00:00,,0,6,feature module: autograd triaged,True
54368,RMSProp documentation is confusing module: docs module: optimizer triaged enhancement,2021-03-20 03:53:07+00:00,,0,1,module: docs module: optimizer triaged enhancement,True
54366,[package] Heuristics for extern-ing common dependencies triaged,2021-03-20 01:06:46+00:00,,0,0,triaged,True
54365,Support `Generic` Type in TorchScript oncall: jit weeks,2021-03-20 00:53:05+00:00,,0,0,oncall: jit weeks,False
54364,[package] Clear and minimal interface for what a package provides triaged,2021-03-20 00:52:50+00:00,,0,0,triaged,True
54362,[package] Buck integration for dependency analysis triaged,2021-03-20 00:14:53+00:00,,0,0,triaged,True
54356,Generate delegates for all non-structured kernels triaged module: dispatch,2021-03-19 22:26:06+00:00,,0,0,triaged module: dispatch,True
54351,Numpy.float64 vs native python float breaks DDP triaged module: ddp,2021-03-19 20:53:18+00:00,,0,0,triaged module: ddp,True
54348,[JIT] TorchScript does not correctly handle `torch.` namespace functions called with `kwargs` oncall: jit days,2021-03-19 20:06:21+00:00,,1,5,oncall: jit days,False
54346,Missing tests for gradcheck module: autograd module: tests triaged,2021-03-19 19:48:47+00:00,,1,0,module: autograd module: tests triaged,True
54340,RuntimeError: polar does not support automatic differentiation for outputs with complex dtype. triaged complex_autograd,2021-03-19 17:42:01+00:00,,0,0,triaged complex_autograd,True
54332,Unit testing failures when porting PyTorch wheel module: binaries module: build triaged,2021-03-19 16:13:42+00:00,,0,19,module: binaries module: build triaged,True
54325,build broken when USE_TENSORPIPE=OFF and USE_DISTRIBUTED=ON oncall: distributed module: build triaged module: tensorpipe,2021-03-19 15:26:01+00:00,,0,5,oncall: distributed module: build triaged module: tensorpipe,True
54309,[discussion] Support torch.matmul: strided x sparse in addition to sparse x strided module: sparse triaged,2021-03-19 08:42:38+00:00,,0,1,module: sparse triaged,True
54307,cuDNN error upon the backward method with a sliced tensor output from nn.GRU. module: cudnn module: rnn module: cuda triaged,2021-03-19 05:36:13+00:00,,0,0,module: cudnn module: rnn module: cuda triaged,True
54283,Split VariableTypeManual.cpp triaged module: dispatch,2021-03-18 22:45:32+00:00,,1,0,triaged module: dispatch,True
54282,test_randperm is failing on CPU-only build module: tests triaged,2021-03-18 22:44:51+00:00,,0,3,module: tests triaged,True
54279,[Static Runtime] static_runtime_benchmark compile-time error module: build triaged,2021-03-18 22:09:27+00:00,,1,0,module: build triaged,True
54278,[Static Runtime] StaticRuntime.EmbeddingBag test case broken module: tests triaged,2021-03-18 22:05:14+00:00,,1,0,module: tests triaged,True
54269,SummaryWriter add_image() docs do not state that it is expecting images in a certain format triaged module: tensorboard oncall: visualization,2021-03-18 19:53:17+00:00,,0,0,triaged module: tensorboard oncall: visualization,True
54267,"Profiler with Kineto has ""orphan+childless"" function events (on P100) triaged oncall: profiler",2021-03-18 18:58:45+00:00,,0,6,triaged oncall: profiler,False
54232,DistributedDataParallel: `DDP(model)` hangs on non-master node oncall: distributed triaged module: ddp,2021-03-18 02:52:50+00:00,,0,12,oncall: distributed triaged module: ddp,True
54216,Add inplace variant for torch.minimum / torch.maximum module: autograd triaged function request,2021-03-17 22:04:36+00:00,,0,3,module: autograd triaged function request,False
54213,"[docs] torch.clamp / torch.clip do support None as placeholder of min/max, but this is not documented module: docs triaged",2021-03-17 21:36:34+00:00,,0,1,module: docs triaged,False
54209,[docs] Incomplete examples for torch.Generator module: docs triaged,2021-03-17 21:18:52+00:00,,0,1,module: docs triaged,False
54201,Provide half-away-from-zero rounding mode on Tensor::round module: cpp triaged enhancement function request,2021-03-17 20:05:00+00:00,,0,1,module: cpp triaged enhancement function request,True
54192,OSS Mobile Errors Point to fburls triaged enhancement better-engineering,2021-03-17 17:51:54+00:00,,0,0,triaged enhancement better-engineering,True
54149,OSS build doesn't hard fail on nodiscard triaged module: build warnings better-engineering,2021-03-17 14:36:02+00:00,,0,0,triaged module: build warnings better-engineering,True
54147,Implement Truly Parallel Ensemble Layers feature module: nn triaged module: batching,2021-03-17 14:17:11+00:00,,0,10,feature module: nn triaged module: batching,True
54146,Conversion of Pytorch model to Torchscript for model LaneATT(Lane Detection) oncall: jit,2021-03-17 14:04:42+00:00,,0,1,oncall: jit,False
54140,[Torchscript] Error detected in torch::jit::(anonymous namespace)::DifferentiableGraphBackward on custom RNN model oncall: jit,2021-03-17 11:30:16+00:00,,1,7,oncall: jit,True
54139,as_tensor returns CPU copy of a CUDA buffer object implementing CUDA Array Interface module: cuda triaged module: numba,2021-03-17 11:11:38+00:00,,0,0,module: cuda triaged module: numba,True
54138,Support Array Interface (__array_interface__ attribute) triaged enhancement module: numpy,2021-03-17 10:39:49+00:00,,0,2,triaged enhancement module: numpy,True
54137,Add additive angular margin loss feature module: loss triaged,2021-03-17 10:00:13+00:00,,0,1,feature module: loss triaged,False
54135,torch.kron of a transposed input error module: error checking triaged function request,2021-03-17 09:35:10+00:00,,0,3,module: error checking triaged function request,True
54133,[TorchScript] kind_.is_prim() INTERNAL ASSERT FAILED oncall: jit,2021-03-17 08:27:34+00:00,,1,3,oncall: jit,False
54116,torch.put is divergent from np.put triaged module: numpy,2021-03-17 00:21:23+00:00,,0,1,triaged module: numpy,True
54104,TorchVision and TorchAudio wheels for AArch64 absent from https://download.pytorch.org/whl/torch_stable.html module: build triaged module: vision module: arm,2021-03-16 21:57:50+00:00,,0,7,module: build triaged module: vision module: arm,True
54096,cdist skip for manual backward is kind of janky triaged module: dispatch better-engineering module: distance functions,2021-03-16 21:46:47+00:00,,0,0,triaged module: dispatch better-engineering module: distance functions,True
54082,Dispatch table for linalg_norm is fishy triaged module: dispatch better-engineering module: norms and normalization,2021-03-16 19:13:14+00:00,,0,5,triaged module: dispatch better-engineering module: norms and normalization,True
54077,[cuda extension] tutorial should mention device guard module: cpp-extensions module: docs triaged,2021-03-16 18:22:28+00:00,,0,1,module: cpp-extensions module: docs triaged,False
54071,[FX] Add guards in files in `test/fx` to make sure they're not run directly triaged module: fx,2021-03-16 17:18:21+00:00,,1,0,triaged module: fx,True
54062,Many advanced indexing operations have untested large tensor branches module: tests triaged module: advanced indexing,2021-03-16 11:23:06+00:00,,0,1,module: tests triaged module: advanced indexing,True
54058,c++ convert from std::vector<Tensor> to c10::List<optional<Tensor>> module: docs module: cpp triaged,2021-03-16 09:48:12+00:00,,0,0,module: docs module: cpp triaged,True
54055,Wrong initialisation gain for SNNs/SELU module: nn triaged module: initialization,2021-03-16 09:08:41+00:00,,0,1,module: nn triaged module: initialization,True
54047,RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED in pytorch lightning module: cudnn module: cuda triaged,2021-03-16 03:25:29+00:00,,0,3,module: cudnn module: cuda triaged,True
54035,Web support? feature triaged module: language binding,2021-03-15 22:34:47+00:00,,0,3,feature triaged module: language binding,True
54030,[FR] Configurable gain value for kaiming_normal_ / kaiming_uniform_ in torch.nn.init module: nn triaged enhancement module: initialization,2021-03-15 21:21:02+00:00,,0,2,module: nn triaged enhancement module: initialization,True
54021,Support scatter/gather for NCCL backend triaged module: c10d,2021-03-15 20:10:53+00:00,,0,0,triaged module: c10d,True
54006,enable parallel test execution for GPU tests module: ci module: tests triaged,2021-03-15 16:08:17+00:00,,0,0,module: ci module: tests triaged,True
54001,[codegen] Change public C++ to accept out/mut arguments by const& triaged module: codegen,2021-03-15 14:14:35+00:00,,0,0,triaged module: codegen,True
53983,"hope to support something like  ""torch::manual_seed_for_mulit_thread"" module: cpp triaged enhancement module: random",2021-03-14 14:42:49+00:00,,0,0,module: cpp triaged enhancement module: random,True
53982,[Feature Request] Optionally specify batch dimension in DataLoader and collate_fn module: dataloader triaged enhancement,2021-03-14 09:52:26+00:00,,0,1,module: dataloader triaged enhancement,True
53957,Is pytorch 1.8.0 incompatible with cuda 11.2 or what is the reason for this error? module: cuda triaged,2021-03-13 06:02:04+00:00,,0,10,module: cuda triaged,True
53952,[Pipe] supporting None and non-Tensors in forward's input/output  triaged pipeline parallelism,2021-03-13 03:51:50+00:00,,1,16,triaged pipeline parallelism,True
53937,Only tensor can be proxied in builtin operators with `__torch_function__` triaged module: __torch_function__ module: fx,2021-03-12 23:11:30+00:00,,0,7,triaged module: __torch_function__ module: fx,True
53935,SubgraphRewriter doesn't handle tensor constants in replacement graph  triaged module: fx,2021-03-12 23:10:42+00:00,,1,1,triaged module: fx,True
53921,[JIT] TorchScript infers incorrect type for `or` in non-boolean context oncall: jit,2021-03-12 20:55:56+00:00,,1,2,oncall: jit,True
53910,There should be a CI build with OpenBLAS in addition to MKL module: ci triaged,2021-03-12 18:12:12+00:00,,0,1,module: ci triaged,True
53904,[JIT] Support `torch.ones([])` oncall: jit,2021-03-12 16:44:24+00:00,,2,2,oncall: jit,False
53903,[torch.vmap] Support second order grads module: autograd triaged enhancement module: vmap,2021-03-12 16:33:02+00:00,,0,14,module: autograd triaged enhancement module: vmap,True
53900,Have the possibility to reduce a tensor with median on more than one specified dimension triaged enhancement module: numpy has workaround module: reductions,2021-03-12 16:12:21+00:00,,0,2,triaged enhancement module: numpy has workaround module: reductions,True
53895,"terminate called after throwing an instance of 'c10::Error'   what():  isTuple() INTERNAL ASSERT FAILED at ""/home/wenda/libtorch/include/ATen/core/ivalue_inl.h"":927, please report a bug to PyTorch. Expected Tuple but got GenericList needs reproduction module: crash module: cpp triaged",2021-03-12 14:30:10+00:00,,0,13,needs reproduction module: crash module: cpp triaged,True
53891,Check Archive.org for Failed Dataset Downloads module: dependency bug triaged enhancement,2021-03-12 12:59:16+00:00,,0,5,module: dependency bug triaged enhancement,True
53890,GoogleNet pretrained/scratch bug? triaged module: vision,2021-03-12 11:52:17+00:00,,0,0,triaged module: vision,True
53879,Linear algebra GPU library function bug tracking issue [magma/cusolver/cublas] module: cuda triaged module: linear algebra,2021-03-12 06:12:10+00:00,,0,2,module: cuda triaged module: linear algebra,True
53878,[JIT] TorchScript errors out when assigning a value to an existing variable that's defined in outer block oncall: jit,2021-03-12 05:49:22+00:00,,1,1,oncall: jit,False
53877,[JIT] Bad interaction between if-else-style and assert-style type refinement oncall: jit,2021-03-12 05:37:25+00:00,,1,0,oncall: jit,True
53867,[JIT][Static Runtime] Memory optimization for output tensors oncall: jit,2021-03-12 02:17:32+00:00,,1,0,oncall: jit,False
53866,Add comprehensive subgroup testing to torch.distributed oncall: distributed triaged module: c10d module: ddp,2021-03-12 02:00:49+00:00,,0,0,oncall: distributed triaged module: c10d module: ddp,True
53862,"After the source code is compiled, an error will be reported fvcore oncall: jit",2021-03-12 01:36:01+00:00,,1,9,oncall: jit,True
53836,Lowering MHA C++/Python to ATen triage review module: nn oncall: transformer/mha,2021-03-11 19:04:59+00:00,,0,1,triage review module: nn oncall: transformer/mha,True
53824,[JIT] torch.jit.optimized_execution(True) greatly slows down some operations in PyTorch 1.8.0  oncall: jit,2021-03-11 16:41:53+00:00,,0,1,oncall: jit,True
53796,foreach ops in autograd  high priority module: autograd triaged,2021-03-11 06:21:12+00:00,,0,1,high priority module: autograd triaged,True
53794,[fx] GraphModule __init__ when provided a Module w/ inconsistent buffer handling triaged module: fx,2021-03-11 06:02:03+00:00,,0,0,triaged module: fx,True
53785,Support for one-hot of dtypes besides torch.int64 module: nn triaged enhancement,2021-03-11 03:15:37+00:00,,0,3,module: nn triaged enhancement,True
53758,ZeroDivisionError: float division by zero in Adam (bias_correction1 is zero) module: numerical-stability module: optimizer triaged,2021-03-10 22:28:42+00:00,,0,1,module: numerical-stability module: optimizer triaged,True
53753,TorchScript cannot handle declared module attributes of Python types oncall: jit,2021-03-10 21:51:50+00:00,,2,2,oncall: jit,True
53744,TorchScript divison by zero fails to error out  oncall: jit,2021-03-10 21:14:04+00:00,,1,8,oncall: jit,True
53732,[FX] Allow customizing code generation for certain Nodes triaged module: fx,2021-03-10 19:19:45+00:00,,0,0,triaged module: fx,True
53721,PyTorch on Embedded Hardware module: cuda triaged,2021-03-10 18:00:00+00:00,,0,1,module: cuda triaged,True
53712,Poor support of `Optimizer.add_param_group` module: optimizer triaged,2021-03-10 14:38:24+00:00,,0,2,module: optimizer triaged,True
53710,Ninja recompiles certain unchanged source files in cpp extension since torch 1.8.0 module: build triaged,2021-03-10 14:15:39+00:00,,0,0,module: build triaged,True
53708,torch.load with Exception module: serialization triaged,2021-03-10 13:23:48+00:00,,0,2,module: serialization triaged,True
53707,[RFC] Add common device runtime abstraction feature triaged,2021-03-10 10:24:32+00:00,,0,8,feature triaged,False
53704,`test_reductions` ignoring some tests module: tests triaged module: reductions,2021-03-10 09:51:17+00:00,,0,1,module: tests triaged module: reductions,True
53696,Support GPU/CPU communication in RPC oncall: distributed triaged module: rpc module: tensorpipe,2021-03-10 08:12:00+00:00,,0,3,oncall: distributed triaged module: rpc module: tensorpipe,True
53678,[FX] Regression from 1.8: FX can no longer trace functions where the first element of an int list is a Proxy triaged module: fx,2021-03-10 02:13:32+00:00,,0,2,triaged module: fx,True
53666,torch.autograd.functional.jacobian/hessian allow tensor in addition to function feature module: autograd triaged,2021-03-09 23:48:41+00:00,,0,5,feature module: autograd triaged,False
53658,torch.distributed with NCCL can hang if the first operation is a barrier oncall: distributed triaged module: c10d,2021-03-09 22:20:50+00:00,,0,7,oncall: distributed triaged module: c10d,True
53649,vmap gradgradcheck test fails for unfold operation module: autograd triaged module: vmap,2021-03-09 21:14:21+00:00,,0,0,module: autograd triaged module: vmap,True
53642,torch.unsafe_chunk and torch.unsafe_split 's documentation does not render on the website. module: docs triaged module: viewing and reshaping,2021-03-09 20:39:56+00:00,,0,3,module: docs triaged module: viewing and reshaping,True
53627,test_metal.py must skip when not compiled with metal support module: tests triaged,2021-03-09 18:32:43+00:00,,0,1,module: tests triaged,False
53625,torch.arange() issue oncall: distributed module: multiprocessing triaged module: regression,2021-03-09 18:27:35+00:00,,0,0,oncall: distributed module: multiprocessing triaged module: regression,True
53623,test/distributed/test_c10d.py::DistributedDataParallelTest only passes if run before other test cases oncall: distributed triaged,2021-03-09 17:27:10+00:00,,1,6,oncall: distributed triaged,True
53622,Properly design manual_cpp_binding (make it less error prone) module: cpp triaged module: language binding,2021-03-09 17:23:50+00:00,,0,0,module: cpp triaged module: language binding,True
53584,How to delete Module from GPU? (libtorch C++) module: cpp-extensions module: cpp triaged,2021-03-09 02:55:03+00:00,,0,6,module: cpp-extensions module: cpp triaged,True
53534,[FX] Ability to wrap functions in other modules for symbolic tracing triaged module: fx,2021-03-08 18:36:48+00:00,,0,8,triaged module: fx,True
53510,Improve memory format testing module: tests triaged module: memory format,2021-03-08 16:26:39+00:00,,0,1,module: tests triaged module: memory format,True
53491,Very slow backward speed when using gather with small-range indices module: performance triaged,2021-03-08 04:00:05+00:00,,0,0,module: performance triaged,True
53477,Show methods of the class in the right sidebar module: docs triaged,2021-03-07 16:20:56+00:00,,0,1,module: docs triaged,True
53473,RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)` triaged module: cublas,2021-03-07 10:32:07+00:00,,0,5,triaged module: cublas,True
53472,After upgrade to 1.8.0 memory leak in libTorch after using torch::jit::load() oncall: jit oncall: mobile,2021-03-07 10:14:22+00:00,,0,1,oncall: jit oncall: mobile,True
53464,libtorch reports bad_alloc when used together with darknet needs reproduction triaged,2021-03-07 04:29:56+00:00,,0,3,needs reproduction triaged,True
53463,[Feature Request] return type support for `torch.norm` and tensor iteration in for loop triaged enhancement,2021-03-07 03:11:35+00:00,,0,0,triaged enhancement,False
53462,Second order derivatives for F.grid_sample() triaged enhancement,2021-03-06 22:45:05+00:00,,0,1,triaged enhancement,True
53451,Model loaded from full model state checkpoint does not run on multiple GPUs (device mismatch) triage review oncall: distributed module: nn module: cuda module: serialization,2021-03-06 12:40:50+00:00,,0,9,triage review oncall: distributed module: nn module: cuda module: serialization,True
53442,IRSimplifier performs unsafe transformation on mod: m*n%m -> 0 triaged NNC,2021-03-06 06:43:13+00:00,,0,7,triaged NNC,True
53441,"Conjugate gradient Descent, and Linear operator are not present in pytorch. module: sparse triaged module: complex module: linear algebra complex_autograd function request module: lazy",2021-03-06 05:26:41+00:00,,0,12,module: sparse triaged module: complex module: linear algebra complex_autograd function request module: lazy,True
53440,Add complex support for torch.unique triaged module: complex module: numpy complex_autograd function request module: python array api,2021-03-06 03:49:24+00:00,,0,2,triaged module: complex module: numpy complex_autograd function request module: python array api,True
53438,Pipe should automatically build appropriate partitions. triaged pipeline parallelism,2021-03-06 02:49:46+00:00,,2,1,triaged pipeline parallelism,True
53427,Backward compute time in DDP logging may not be accurate  oncall: distributed triaged,2021-03-05 23:54:36+00:00,,0,1,oncall: distributed triaged,True
53421,[bug][JIT] Error when tracing model with partial hooks in PyTorch v1.8.0 oncall: jit,2021-03-05 23:12:51+00:00,,0,5,oncall: jit,True
53414,[JIT] Metaprogram non-type based expression trees oncall: jit,2021-03-05 22:12:58+00:00,,1,0,oncall: jit,False
53407,"torch.matmul doesn't handle zero-sized inputs in some cases, leading to batched grad failures module: autograd module: tests triaged module: vmap",2021-03-05 21:18:49+00:00,,0,5,module: autograd module: tests triaged module: vmap,True
53405,When I ran the 'Get Started with Data Data Parallel' tutorial the code got 'RuntimeError '!There are no hints triaged module: data parallel,2021-03-05 20:39:57+00:00,,0,0,triaged module: data parallel,True
53392,[bug] GaussianNLLLoss: Variance not broadcast to input shape module: docs module: nn triaged,2021-03-05 18:57:11+00:00,,0,5,module: docs module: nn triaged,True
53391,torch.empty_strided doesn't test if strides are negative module: error checking triaged,2021-03-05 18:56:24+00:00,,0,0,module: error checking triaged,True
53386,Default pytest is truncating stacktrace (somehow!!!) triaged,2021-03-05 18:19:19+00:00,,0,2,triaged,True
53365,Pytorch mobile Vulkan API usage triaged oncall: mobile,2021-03-05 15:48:55+00:00,,0,1,triaged oncall: mobile,False
53360,[bug] torch.cumsum: functional and method variant promotes all ints to Long but inplace don't module: docs triaged module: type promotion,2021-03-05 14:00:45+00:00,,0,1,module: docs triaged module: type promotion,True
53358,[bug] torch.cumsum: behaviour for `bool` input module: cuda module: cpu triaged module: boolean tensor,2021-03-05 13:38:46+00:00,,0,2,module: cuda module: cpu triaged module: boolean tensor,True
53352,OpInfo mechanism to test for nondeterminism module: tests triaged module: determinism,2021-03-05 10:59:17+00:00,,1,1,module: tests triaged module: determinism,True
53345,CUDA error: an illegal memory access was encountered when updating model weights using GradScaler module: nn module: cuda triaged module: amp (automated mixed precision),2021-03-05 06:54:30+00:00,,0,1,module: nn module: cuda triaged module: amp (automated mixed precision),True
53344,[JIT] raise unittest.SkipTest and self.skipTest cannot be used to skip tests in JitTestCase oncall: jit,2021-03-05 06:28:12+00:00,,0,0,oncall: jit,False
53337,Should we remind users not to use the dataset  on GPU when it's the argument of DataLoader? module: dataloader triaged,2021-03-05 04:16:25+00:00,,0,1,module: dataloader triaged,True
53284,Find a good namespace home for torch._assert_async triaged module: numpy module: testing,2021-03-04 18:57:18+00:00,,0,0,triaged module: numpy module: testing,True
53264,How to load trained . torch from conversion to .mlmodel oncall: mobile,2021-03-04 14:27:11+00:00,,0,4,oncall: mobile,False
53263,Different gradients in torch.matmul depending on input shape module: numerical-stability triaged,2021-03-04 14:13:05+00:00,,0,9,module: numerical-stability triaged,True
53261,Dimension argument names in torch.diag_embed/diagonal vs. transpose/transpose_ module: docs triaged module: numpy,2021-03-04 12:32:44+00:00,,0,2,module: docs triaged module: numpy,True
53257,Inconsistent `out=` behaviour on advanced indexing operations. triaged module: complex enhancement module: advanced indexing,2021-03-04 11:02:26+00:00,,0,0,triaged module: complex enhancement module: advanced indexing,True
53256,`index_copy_`  test fails on PyTorch/XLA module: tests triaged module: xla,2021-03-04 10:36:48+00:00,,1,0,module: tests triaged module: xla,True
53194,Relative performance of histc vs bincount module: performance triaged module: sorting and selection,2021-03-03 18:03:12+00:00,,0,0,module: performance triaged module: sorting and selection,True
53193,[nnc] Prevent user from scheduling after prepareForCodegen triaged NNC,2021-03-03 17:44:08+00:00,,0,1,triaged NNC,True
53191,RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR module: cudnn module: convolution triaged,2021-03-03 17:09:43+00:00,,0,6,module: cudnn module: convolution triaged,True
53185,[NIT] Unpickler persistent_load monkey patch should fall back to original persistent_load module: serialization triaged,2021-03-03 15:14:06+00:00,,0,0,module: serialization triaged,True
53178,Exceedingly different F.conv2d outputs on cuda/cpu high priority module: cudnn module: cuda module: convolution triaged module: correctness (silent),2021-03-03 12:21:45+00:00,,0,9,high priority module: cudnn module: cuda module: convolution triaged module: correctness (silent),True
53157,doxygen and pytorch documentation module: docs module: cpp triaged,2021-03-03 01:00:41+00:00,,0,4,module: docs module: cpp triaged,True
53141,Structured kernels for operators that don't have out= variants triaged module: structured kernels,2021-03-02 21:46:46+00:00,,0,1,triaged module: structured kernels,True
53140,[JIT] Document __jit_ignored_attributes__ oncall: jit,2021-03-02 20:57:53+00:00,,1,0,oncall: jit,False
53130,Lint rule to forbid bare assert() in cuda module: cuda module: lint triaged,2021-03-02 19:36:22+00:00,,0,0,module: cuda module: lint triaged,True
53124,Deprecate and remove torch.set_default_tensor_type triaged module: deprecation,2021-03-02 18:29:03+00:00,,0,1,triaged module: deprecation,True
53121,[fx] Handle models with Optional inputs triaged module: fx,2021-03-02 18:12:27+00:00,,1,2,triaged module: fx,True
53103,ModuleList not recognised as reversible by mypy module: nn module: typing triaged,2021-03-02 09:32:00+00:00,,0,3,module: nn module: typing triaged,True
53095,Reset mask for torch.cumsum? triaged enhancement module: numpy module: reductions,2021-03-02 07:25:10+00:00,,0,6,triaged enhancement module: numpy module: reductions,True
53094,group conv in amp too slower module: performance module: cuda triaged module: amp (automated mixed precision),2021-03-02 07:21:11+00:00,,0,6,module: performance module: cuda triaged module: amp (automated mixed precision),True
53090,'Tensor' object has no attribute 'astype' triaged module: numpy function request,2021-03-02 06:05:08+00:00,,0,3,triaged module: numpy function request,True
53072,[NNC] Simplify after each transformation triaged NNC,2021-03-02 00:35:26+00:00,,0,1,triaged NNC,True
53026,Ensure that nn.MHA and functional multi-head attention is supported by FX graph mode quantization oncall: quantization low priority triaged module: fx,2021-03-01 17:38:00+00:00,,1,1,oncall: quantization low priority triaged module: fx,True
53023,[codegen] Make it easier to codegen call to API triaged enhancement module: codegen,2021-03-01 16:41:31+00:00,,0,0,triaged enhancement module: codegen,True
53022,[TorchScript Usability] Instance attribute annotation does not seem to take effect oncall: jit,2021-03-01 15:55:19+00:00,,1,0,oncall: jit,False
53017,[TorchScript Usability] Optional type does not support `x != None` oncall: jit,2021-03-01 15:29:53+00:00,,1,0,oncall: jit,False
52989,lr_scheduler _triangular2_scale_fn calculation is overflowed needs reproduction module: optimizer triaged,2021-03-01 01:17:58+00:00,,0,2,needs reproduction module: optimizer triaged,True
52984,Tracker: pytest-related test improvements module: tests triaged tracker,2021-02-28 20:13:26+00:00,,1,12,module: tests triaged tracker,True
52973,Add numerically stable methods to torch.distributions (e.g. logcdf) module: distributions triaged,2021-02-27 17:50:54+00:00,,0,9,module: distributions triaged,True
52970,Wrong link in readme for the android nightly version module: docs module: android oncall: mobile,2021-02-27 11:45:27+00:00,,0,1,module: docs module: android oncall: mobile,False
52940,[JIT] Provide a way to access a flattened optimized graph from Profiling Executor oncall: jit module: bootcamp triaged,2021-02-26 20:20:30+00:00,,0,1,oncall: jit module: bootcamp triaged,True
52936,profile pure C++ process  module: cpp triaged enhancement oncall: profiler,2021-02-26 19:18:45+00:00,,0,2,module: cpp triaged enhancement oncall: profiler,True
52920,[TorchScript Usability]  More intuitive error message when passing wrong types to `torch.jit.script()` oncall: jit,2021-02-26 16:52:56+00:00,,0,1,oncall: jit,True
52915,Broadcasting behaviour for linear algebra solvers triaged module: linear algebra,2021-02-26 14:59:44+00:00,,0,2,triaged module: linear algebra,True
52914,Export to ONNX with all tensor shapes included module: onnx triaged onnx-triaged,2021-02-26 13:56:22+00:00,,0,10,module: onnx triaged onnx-triaged,True
52913,Create a standalone python execution viewer like livepython feature module: logging triaged,2021-02-26 12:18:11+00:00,,0,1,feature module: logging triaged,True
52911,LR Scheduler load_state_dict does not properly update scaling module: optimizer triaged,2021-02-26 08:27:27+00:00,,0,0,module: optimizer triaged,True
52851,"`embedding_bag(..., include_last_offset=True)` should always error if `offsets[-1] != input.size()` module: nn triaged module: embedding",2021-02-25 18:21:44+00:00,,1,4,module: nn triaged module: embedding,True
52850,How to skip the images in a custom dataset and deal with None values? module: dataloader triaged,2021-02-25 18:04:33+00:00,,0,1,module: dataloader triaged,True
52845,[feature request] F.interpolate_as module: nn triaged function request module: interpolation,2021-02-25 16:39:57+00:00,,0,0,module: nn triaged function request module: interpolation,True
52821,[nnc] Compute expressions with > 4 axes is pretty tedious oncall: jit NNC,2021-02-25 05:00:04+00:00,,0,0,oncall: jit NNC,False
52800,Create CI test checker to ensure # of test ran is the same as # of test reported in xmlrunner module: ci triaged enhancement,2021-02-25 00:49:56+00:00,,1,1,module: ci triaged enhancement,True
52795, Add format indexed argument support in JIT oncall: jit,2021-02-25 00:14:41+00:00,,1,0,oncall: jit,False
52788,distributed/optim/test_zero_redundancy_optimizer uses unittest.main() instead of common_utils.run_tests() oncall: distributed,2021-02-24 23:08:20+00:00,,1,3,oncall: distributed,False
52777,[nnc] Placeholder::load braced init list fails to deduce oncall: jit NNC,2021-02-24 22:11:07+00:00,,1,1,oncall: jit NNC,False
52772,[nnc] Optimize 1x1 convolutions oncall: jit NNC,2021-02-24 21:57:26+00:00,,0,0,oncall: jit NNC,False
52768,[nnc] Optimize tiny convolutions oncall: jit NNC,2021-02-24 21:50:34+00:00,,1,0,oncall: jit NNC,False
52767,[nnc] Grouped convolutions triaged NNC,2021-02-24 21:43:27+00:00,,1,0,triaged NNC,True
52762,Basic feature missing module: autograd triaged,2021-02-24 19:07:50+00:00,,0,3,module: autograd triaged,True
52753,Enable support for dynamic shapes oncall: jit,2021-02-24 17:29:57+00:00,,2,1,oncall: jit,False
52747,[proposal] Make torchvision and other libraries' docs searchable via main docs search module: docs triaged module: doc infra,2021-02-24 15:32:38+00:00,,0,0,module: docs triaged module: doc infra,False
52745,Sort torch.topk() output following input index order triaged enhancement,2021-02-24 13:53:38+00:00,,0,3,triaged enhancement,True
52744,Gradient checkpointing support in C++ API module: cpp feature triaged enhancement,2021-02-24 12:33:35+00:00,,0,0,module: cpp feature triaged enhancement,True
52743,torch.searchsorted issues module: docs triaged module: numpy module: sorting and selection,2021-02-24 12:26:21+00:00,,0,4,module: docs triaged module: numpy module: sorting and selection,True
52741,torch.jit.script is swallowing a rfloordiv warning oncall: jit,2021-02-24 09:57:45+00:00,,1,5,oncall: jit,False
52738,[OpInfo] Improvements for sparse ops tests module: sparse module: tests triaged,2021-02-24 09:46:16+00:00,,0,0,module: sparse module: tests triaged,True
52722,Creating torch tensor as a function of index value triaged module: numpy function request,2021-02-24 03:55:59+00:00,,0,2,triaged module: numpy function request,True
52720,[JIT] Finish migrating torch._C._jit_ bindings to torch._C._jit oncall: jit,2021-02-24 03:02:04+00:00,,0,0,oncall: jit,False
52718,Discrepancy between CPU->GPU and GPU->CPU data transfer speeds module: performance module: cuda triaged,2021-02-24 02:26:28+00:00,,0,2,module: performance module: cuda triaged,True
52717,Fix `torch.norm`'s backward to match forward for ord=+/-inf module: autograd triaged module: linear algebra,2021-02-24 01:51:16+00:00,,1,0,module: autograd triaged module: linear algebra,True
52712,Refactor `torch.linalg.norm` triaged module: linear algebra,2021-02-24 01:35:53+00:00,,1,0,triaged module: linear algebra,True
52694,[TensorExpr] Loop Transformations: `compute_inline` should use `Let` for inlined expressions triaged NNC,2021-02-23 20:16:30+00:00,,2,0,triaged NNC,True
52688,[BE] Consolidating scripts/ and tools/ folder module: docs triaged,2021-02-23 19:08:59+00:00,,0,0,module: docs triaged,True
52680,Add Quantized{CPU|CUDA} support to Structured Kernels module: internals triaged module: structured kernels,2021-02-23 17:30:19+00:00,,0,2,module: internals triaged module: structured kernels,True
52675,[feature request] Introduce TORCH_USE_CUDA=OFF runtime environment variable to faithfully/completely disable cuda (torch.cuda.is_available() <- False and torch.cuda.get_device_count() <- 0) and driver loading/interaction module: cuda triaged enhancement small,2021-02-23 15:29:21+00:00,,0,15,module: cuda triaged enhancement small,True
52673,Add custom types to PyTorch feature triaged,2021-02-23 14:57:21+00:00,,0,2,feature triaged,False
52668,Independent axis support for torch.trace triaged enhancement,2021-02-23 13:55:00+00:00,,0,1,triaged enhancement,False
52666,caffe2 load onnx model error:IndexError: Input 475 is undefined! needs reproduction caffe2 triaged,2021-02-23 12:43:23+00:00,,0,3,needs reproduction caffe2 triaged,False
52663,code linking to libtorch cannot use thrust/cub functions high priority module: cpp module: cuda triaged,2021-02-23 09:58:32+00:00,,0,8,high priority module: cpp module: cuda triaged,True
52655,Optimize `einsum` in TorchScript profile guided optimization module: performance oncall: jit feature module: linear algebra,2021-02-23 04:43:00+00:00,,0,3,module: performance oncall: jit feature module: linear algebra,False
52648,Fix `torch.linalg.vector_norm`'s backward to avoid needing `at::abs()` call in forward for ord=+/-inf  triaged module: linear algebra,2021-02-23 01:51:26+00:00,,1,0,triaged module: linear algebra,True
52644,[FR] torch.load's map_location supports int  module: cuda triaged enhancement,2021-02-23 01:13:45+00:00,,0,0,module: cuda triaged enhancement,True
52633,CUDA `linalg.norm` matrix order +/-2 and nuclear norm for extreme values may be incorrect high priority module: cuda triaged module: NaNs and Infs module: linear algebra,2021-02-22 22:20:04+00:00,,1,4,high priority module: cuda triaged module: NaNs and Infs module: linear algebra,True
52625,Add slicing to distributions module: distributions triaged enhancement,2021-02-22 21:50:25+00:00,,0,2,module: distributions triaged enhancement,True
52611,[Profiler] LegacyEvent does not respect device triaged oncall: profiler,2021-02-22 19:21:18+00:00,,1,1,triaged oncall: profiler,False
52607,Enable timed barrier in c10d oncall: distributed module: c10d,2021-02-22 18:44:49+00:00,,0,0,oncall: distributed module: c10d,False
52605,Torchaudio 0.8.0.dev nightly requires torch 1.9.0.dev which is missing module: build triaged,2021-02-22 17:50:06+00:00,,0,2,module: build triaged,True
52599,torch::jit::Node::isNondeterministic() does not include aten::feature_dropout or inline kinds oncall: jit,2021-02-22 14:47:13+00:00,,1,3,oncall: jit,True
52598,CUDA error: no kernel image is available for execution on the device When convert cpu tensor to gpu tensor module: build module: cuda triaged,2021-02-22 12:14:04+00:00,,0,1,module: build module: cuda triaged,True
52597,Custom scatter function for DataParallel triaged enhancement module: data parallel,2021-02-22 10:37:07+00:00,,0,2,triaged enhancement module: data parallel,True
52596,pickle is a security issue module: pickle module: serialization triaged module: hub topic: security,2021-02-22 09:40:22+00:00,,1,10,module: pickle module: serialization triaged module: hub topic: security,True
52589,[JIT-autodiff] batch_norm isn't differentiable oncall: jit,2021-02-22 05:41:42+00:00,,1,6,oncall: jit,False
52583,[TensorExpr] Add IR Verifier module: bootcamp triaged NNC,2021-02-22 03:40:50+00:00,,0,1,module: bootcamp triaged NNC,True
52578,version1.7.0 is ~1.3x slower than 1.4.0 on ResNet18 module: performance triaged,2021-02-22 01:40:29+00:00,,0,3,module: performance triaged,True
52575,Error: ‘str’ is not a member of ‘c10’; did you mean ‘c10::aten::str’? while using libtorch module: cpp triaged,2021-02-21 23:18:22+00:00,,0,2,module: cpp triaged,True
52570,"RTX 2080s performs better than RTX 3080 in Semantic Segmentation inference process(Libtorch,win10),why? module: performance module: cuda triaged",2021-02-21 14:22:39+00:00,,0,7,module: performance module: cuda triaged,True
52569,Omitting mutability annotations in op schemas can lead to subtle errors  module: internals triaged,2021-02-21 10:19:15+00:00,,0,0,module: internals triaged,True
52564,Submodules not rewritten during AST rewrite triaged module: fx,2021-02-21 01:38:15+00:00,,1,0,triaged module: fx,True
52553,Does pytorch have the inverse activation function of nn.LeakyReLU? feature triaged function request,2021-02-20 14:12:49+00:00,,0,1,feature triaged function request,False
52552,Cumulative integration? triaged module: numpy function request,2021-02-20 12:16:27+00:00,,0,4,triaged module: numpy function request,True
52538,Helper methods not preserved during AST rewrite triaged module: fx,2021-02-19 23:36:26+00:00,,1,0,triaged module: fx,True
52517,[doc] JIT-compatible c++ custom ops oncall: jit module: docs docs-hackathon,2021-02-19 20:04:09+00:00,,0,4,oncall: jit module: docs docs-hackathon,False
52515,torch.bmm incorrect with pytorch 1.7.1 and cuda 11 needs reproduction module: cuda triaged module: cublas,2021-02-19 19:48:33+00:00,,0,2,needs reproduction module: cuda triaged module: cublas,True
52495,link failed when using custom build pytorch on Android triaged module: undefined reference module: android module: arm,2021-02-19 08:52:27+00:00,,0,4,triaged module: undefined reference module: android module: arm,True
52485,Dispatcher documentation needs update module: docs triaged module: dispatch,2021-02-19 05:33:23+00:00,,0,1,module: docs triaged module: dispatch,True
52478,"Could you please increase the wheel compiled Linux (aarch64) GPU, thank you very much! triaged module: arm",2021-02-19 02:18:17+00:00,,0,2,triaged module: arm,True
52471,all reduce hangs and does not throw exception when CUDA_VISIBLE_DEVICES is not set properly using the NCCL backend oncall: distributed module: nccl,2021-02-18 23:49:14+00:00,,0,10,oncall: distributed module: nccl,True
52465,[nnc][perf] Performance decrease with CPU fusion on `freeze(script(pytorch_mobilenet_v3))` module: performance triaged NNC,2021-02-18 21:44:32+00:00,,1,2,module: performance triaged NNC,True
52457,Add autograd tests to verify correctness for R -> C cases module: tests triaged module: complex complex_autograd,2021-02-18 20:21:38+00:00,,0,1,module: tests triaged module: complex complex_autograd,True
52439,[proposal] Way to export/display the convolution algorithm found with cudnn.benchmark = True + allow to override algo choice with conv/matmul module/function algo/hint arguments module: cudnn feature module: convolution triaged,2021-02-18 15:01:34+00:00,,0,7,module: cudnn feature module: convolution triaged,True
52434,[NNC] NNC usability list oncall: jit triaged NNC,2021-02-18 09:14:21+00:00,,0,4,oncall: jit triaged NNC,True
52415,torch.autograd.Function doesn't support non-Tensor outputs module: autograd triaged,2021-02-18 01:35:26+00:00,,1,0,module: autograd triaged,True
52390,autograd.functional.vjp with function that mutates inputs in-place can cause confusion module: autograd triaged,2021-02-17 22:02:11+00:00,,0,0,module: autograd triaged,True
52363,RuntimeError: Can't redefine method: forward on class: __torch__.torch.nn.quantized.modules.conv.Conv1d (of Python compilation unit at: 0x5571a223a770) oncall: jit,2021-02-17 18:23:56+00:00,,0,13,oncall: jit,False
52334,Normal_like operator triaged enhancement module: numpy,2021-02-17 00:21:31+00:00,,0,4,triaged enhancement module: numpy,True
52332,Reciprocals of complex tensors with infinities are different from NumPy. triaged module: complex module: numpy module: NaNs and Infs,2021-02-16 23:57:01+00:00,,0,6,triaged module: complex module: numpy module: NaNs and Infs,True
52318,"TorchConfig.cmake entries of if(ON) are not accepted by CMake as ""true"", to be replaced by if(1) module: build triaged",2021-02-16 19:28:06+00:00,,0,3,module: build triaged,True
52312,TorchScript can't wrap Python constants in current module oncall: jit,2021-02-16 18:58:57+00:00,,1,4,oncall: jit,False
52310,Implementation of many complex functions is fast but inaccurate in libc++ module: numerical-stability triaged module: complex enhancement module: numpy,2021-02-16 17:58:47+00:00,,0,1,module: numerical-stability triaged module: complex enhancement module: numpy,True
52309,[JIT] __getitem__ on scripted ModuleDict returns unscripted module oncall: jit days,2021-02-16 17:51:39+00:00,,0,1,oncall: jit days,True
52307,Model produces same outputs for different inputs when using GPU needs reproduction module: cuda triaged,2021-02-16 17:31:02+00:00,,0,5,needs reproduction module: cuda triaged,True
52298,[FR} Implement Skellam distribution module: distributions triaged,2021-02-16 15:17:51+00:00,,0,0,module: distributions triaged,True
52295,Printing a Tensor that is being vmap'ed over in C++ raises an error triaged module: vmap,2021-02-16 11:45:37+00:00,,0,4,triaged module: vmap,True
52291,"""LayerNormKernelImpl"" not implemented for 'Half' - CPU module: nn module: cpu triaged enhancement module: half",2021-02-16 06:45:17+00:00,,0,3,module: nn module: cpu triaged enhancement module: half,True
52289,DataParallel module fails to handle data with size not divisible by number of GPUs triaged module: data parallel,2021-02-16 01:40:55+00:00,,0,2,triaged module: data parallel,True
52279,[docs] Official apex -> torch.cuda.amp migration guide module: docs feature triaged module: amp (automated mixed precision),2021-02-15 19:16:22+00:00,,0,3,module: docs feature triaged module: amp (automated mixed precision),True
52274,Port unary elementwise ops to structured kernels triaged module: structured kernels,2021-02-15 14:31:57+00:00,,1,1,triaged module: structured kernels,True
52271,torchscript should work with type: ignore comments from mypy even if they are not at the end of the line oncall: jit days,2021-02-15 12:28:04+00:00,,1,9,oncall: jit days,False
52266,GPU (+CPU/RAM?) self-check script or instructions in core module: cudnn module: cuda triaged,2021-02-15 07:58:52+00:00,,0,0,module: cudnn module: cuda triaged,True
52265,'TypeError: expected CPU (got CUDA)' when placing a CUDA tensor on a class that is inheriting torch.Tensor triaged module: tensor creation,2021-02-14 21:19:17+00:00,,0,5,triaged module: tensor creation,True
52262,tolist called on torch scalar does not return a list => proposal to support new arg force = True to provide a work around module: docs triaged module: numpy,2021-02-14 10:14:41+00:00,,0,7,module: docs triaged module: numpy,True
52260,Unlucky batch->device split by DataParallel causes an exception triaged module: data parallel,2021-02-14 05:39:38+00:00,,0,1,triaged module: data parallel,True
52256,[docs] Bad docs SEO module: docs triaged module: doc infra,2021-02-13 22:48:26+00:00,,0,27,module: docs triaged module: doc infra,False
52241,CTCLoss gradient is incorrect module: docs module: autograd module: loss triaged,2021-02-12 21:55:23+00:00,,0,21,module: docs module: autograd module: loss triaged,True
52236,CosineAnnealingWarmRestarts LR scheduler fails when lash_epoch != -1 module: optimizer triaged,2021-02-12 21:18:23+00:00,,0,1,module: optimizer triaged,True
52230,Removing mutation on block input should print any meaning message. module: onnx triaged onnx-triaged,2021-02-12 20:02:56+00:00,,0,1,module: onnx triaged onnx-triaged,True
52214,`hasattr` check not supported on objects other than `self` oncall: jit weeks,2021-02-12 17:15:16+00:00,,0,1,oncall: jit weeks,False
52211,Conv3D error : CUDNN_STATUS_INTERNAL_ERROR module: cudnn module: convolution triaged,2021-02-12 16:32:59+00:00,,0,4,module: cudnn module: convolution triaged,True
52208,C++ standard library confusion module: build triaged,2021-02-12 12:59:27+00:00,,0,2,module: build triaged,True
52207,Wasserstein metric module: nn triaged needs research,2021-02-12 12:15:51+00:00,,0,3,module: nn triaged needs research,True
52206,from_numpy function rounds float values on Jetson NX triaged module: correctness (silent) module: arm module: jetson,2021-02-12 12:13:11+00:00,,0,3,triaged module: correctness (silent) module: arm module: jetson,True
52205,Allow F.pad(mode = 'reflect') when shape == pad module: nn triaged enhancement module: numpy module: padding,2021-02-12 11:47:47+00:00,,0,1,module: nn triaged enhancement module: numpy module: padding,True
52181,"`torch.load(..., weights_only=True)` currently raises a Deprecation warning + [proposal] `weights_only=True` should become default for safe legacy-loading pickles high priority feature module: serialization triaged module: hub topic: security",2021-02-12 01:32:12+00:00,,1,57,high priority feature module: serialization triaged module: hub topic: security,True
52168,CMake errors building project that uses PyTorch module: build triaged,2021-02-11 22:38:49+00:00,,0,1,module: build triaged,True
52147,Pointer passed where number is expected for PYTORCH_CUDA_FUSER_JIT_OPT_LEVEL leading to crash oncall: jit,2021-02-11 17:04:53+00:00,,0,0,oncall: jit,True
52145,Trying to initialise CUDA twice in a process with no visible devices hangs the process and terminal permanently module: cuda triaged module: deadlock,2021-02-11 16:10:23+00:00,,0,1,module: cuda triaged module: deadlock,True
52144,Concatenation of Java tensors (Android for mobile) oncall: mobile,2021-02-11 16:03:01+00:00,,0,0,oncall: mobile,False
52143,[proposal] Parameter dim for F.linear (and maybe nn.Linear) module: nn triaged enhancement,2021-02-11 15:43:53+00:00,,0,3,module: nn triaged enhancement,True
52134,Build using Py_LIMITED_API and then build wheels with the stable ABI abi3 tag module: binaries module: performance oncall: releng triaged,2021-02-11 10:59:53+00:00,,0,3,module: binaries module: performance oncall: releng triaged,True
52096,"Unable to build: subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '4']' returned non-zero exit status 1. module: build triaged",2021-02-10 21:15:02+00:00,,0,6,module: build triaged,True
52058,at::numeric_limits is misleading module: cuda module: rocm triaged better-engineering actionable,2021-02-10 16:39:46+00:00,,0,1,module: cuda module: rocm triaged better-engineering actionable,True
52052,common_jit.check_against_reference() should check gradients w.r.t. weights oncall: jit module: autograd triaged days,2021-02-10 16:02:21+00:00,,0,0,oncall: jit module: autograd triaged days,True
52047,Detect OpenMP Loop and this application may hang warnings oncall: jit module: tests module: multithreading days,2021-02-10 14:18:51+00:00,,0,4,oncall: jit module: tests module: multithreading days,True
52041,device (+dtype) arguments for torch.stack / torch.cat module: performance module: cuda triaged enhancement,2021-02-10 12:02:19+00:00,,0,3,module: performance module: cuda triaged enhancement,True
52040,torch.distributed.TCPStore doesn't work with dual IPv4/IPv6 network interface oncall: distributed triaged,2021-02-10 11:29:18+00:00,,1,3,oncall: distributed triaged,True
52036,Automatic splitting of batch during model feed-forwarding to avoid memory errors caused by the too big batch_size feature module: memory usage triaged needs research,2021-02-10 10:09:23+00:00,,0,2,feature module: memory usage triaged needs research,True
52022,[FX] Interpreter doesn't GC values triaged module: fx,2021-02-10 04:16:55+00:00,,1,0,triaged module: fx,True
52011,CPU eval BatchNorm2d is not threaded module: performance module: nn triaged module: norms and normalization,2021-02-10 00:31:32+00:00,,0,1,module: performance module: nn triaged module: norms and normalization,True
52005,Implementation of Normalized Euclidean Distance (NED) in pytorch module: nn triaged function request module: distance functions,2021-02-09 23:53:35+00:00,,0,6,module: nn triaged function request module: distance functions,False
51997,[FX] Modules called with constants should return constants and not proxies. triaged module: fx,2021-02-09 22:21:31+00:00,,1,0,triaged module: fx,True
51994,Allow Custom Classes to register a handler for .to operations  oncall: jit feature module: custom-operators weeks,2021-02-09 21:30:46+00:00,,0,0,oncall: jit feature module: custom-operators weeks,False
51969,Support backward hooks in JIT  oncall: jit weeks,2021-02-09 15:59:44+00:00,,0,4,oncall: jit weeks,False
51965,Feature Request:Fast Cosine Transform triaged module: fft function request,2021-02-09 14:52:01+00:00,,0,4,triaged module: fft function request,False
51962,PyTorch for scientific computing feature triaged needs research needs design,2021-02-09 13:44:25+00:00,,0,4,feature triaged needs research needs design,False
51950,test_jit_cuda_archflags fails if current GPU is newer than nvcc oncall: jit,2021-02-09 08:52:33+00:00,,0,0,oncall: jit,False
51942,Overwritten method has wrong self when JIT'ed oncall: jit,2021-02-09 05:23:54+00:00,,1,4,oncall: jit,True
51941,RuntimeError: torch.dtype passed as int from JIT to ignored function oncall: jit weeks,2021-02-09 05:10:35+00:00,,0,3,oncall: jit weeks,False
51933,Performance bugs of transpose2d on A100 module: performance module: cudnn triaged,2021-02-09 02:49:54+00:00,,0,15,module: performance module: cudnn triaged,True
51932,[Pipe] Memory-Efficient Model Construction  oncall: distributed pipeline parallelism,2021-02-09 02:45:11+00:00,,0,3,oncall: distributed pipeline parallelism,False
51931,[Pipe] Tied layers oncall: distributed pipeline parallelism,2021-02-09 02:45:06+00:00,,0,8,oncall: distributed pipeline parallelism,False
51911,torch.nn.functional.grid_sample outputs NaN module: nn triaged module: NaNs and Infs,2021-02-08 22:50:13+00:00,,0,3,module: nn triaged module: NaNs and Infs,True
51906,torch.nn.functional.binary_cross_entropy(_with_logits) outputs NaN module: nn triaged module: NaNs and Infs,2021-02-08 22:05:48+00:00,,0,1,module: nn triaged module: NaNs and Infs,True
51896,Can't reassign a child module instance to a lambda module: nn triaged,2021-02-08 20:07:01+00:00,,0,14,module: nn triaged,True
51892,[cuda jit extentions] global shared destination conflicts with virtual envs oncall: jit,2021-02-08 19:17:52+00:00,,0,2,oncall: jit,True
51885,JIT Module forward pre-hooks do not support default arguments oncall: jit,2021-02-08 17:53:29+00:00,,0,1,oncall: jit,True
51872,Tensor.nonzero tries to allocate huge amount of memory for tensors on GPU with num_elements close to INT_MAX module: dependency bug module: cuda module: memory usage triaged,2021-02-08 14:17:19+00:00,,0,2,module: dependency bug module: cuda module: memory usage triaged,True
51871,Tensor.nonzero fails on GPU for tensors containing more than INT_MAX elements module: cuda triaged enhancement,2021-02-08 14:06:59+00:00,,0,2,module: cuda triaged enhancement,True
51868,discrepancies between TorchConfigVersion.cmake to git branch module: build triaged,2021-02-08 07:43:32+00:00,,0,2,module: build triaged,True
51863,TestVectorizedMemoryAccess.CopyKernel starting to fail after driver to 460.39 high priority module: cuda module: ci triaged,2021-02-07 17:26:32+00:00,,0,5,high priority module: cuda module: ci triaged,True
51856,Assignment target is transposed when using jit.script and avanced indexing oncall: jit good first issue OSS contribution wanted days,2021-02-07 11:13:42+00:00,,1,3,oncall: jit good first issue OSS contribution wanted days,True
51855,"Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /pytorch/caffe2/serialize/inline_container.cc:132) triaged module: assert failure",2021-02-07 10:01:14+00:00,,0,2,triaged module: assert failure,True
51849,.numpy() array failes to keep original storage around triaged module: numpy module: tensor creation,2021-02-07 04:38:39+00:00,,0,7,triaged module: numpy module: tensor creation,True
51836,TypeError: can't pickle _thread.lock objects when using spawn / forkserver module: pickle triaged better-engineering,2021-02-06 17:42:03+00:00,,0,1,module: pickle triaged better-engineering,True
51835,methods decorated with torch.jit._overload_method are not accessible oncall: jit,2021-02-06 17:28:35+00:00,,1,1,oncall: jit,True
51808,[jit] Keyword-arg expansion for constants oncall: jit,2021-02-05 21:55:02+00:00,,0,4,oncall: jit,False
51803,torch.fx.symbolic_trace fails on torch.arange with input-dependent size triaged module: fx,2021-02-05 20:00:31+00:00,,0,6,triaged module: fx,True
51794,"optim.Optimizer should copy ""params"" before modifying them module: optimizer triaged",2021-02-05 18:12:58+00:00,,0,3,module: optimizer triaged,False
51782,[discussion] [python] Generic way for generating module-wrappers for stateless parametrized functions feature module: nn triaged better-engineering,2021-02-05 15:05:50+00:00,,0,6,feature module: nn triaged better-engineering,True
51781,Libtorch lock thread module: abi triaged,2021-02-05 13:08:37+00:00,,0,0,module: abi triaged,True
51780,Proposal for export TensorBoard tracing files triaged module: tensorboard,2021-02-05 11:54:27+00:00,,0,0,triaged module: tensorboard,True
51778,Assertion on the existence of RRef user-facing methods' docstrings oncall: distributed triaged better-engineering module: rpc,2021-02-05 11:01:50+00:00,,0,3,oncall: distributed triaged better-engineering module: rpc,True
51777,Analytical metric package module: distributions feature triaged module: data,2021-02-05 10:54:14+00:00,,0,15,module: distributions feature triaged module: data,True
51776,BatchNorm3d error : CUDNN_STATUS_NOT_SUPPORTED module: cudnn triaged,2021-02-05 09:38:51+00:00,,0,6,module: cudnn triaged,True
51732,torch.nn.functional.ctc_loss crash(segfault)  module: crash module: nn module: loss triaged,2021-02-04 18:15:43+00:00,,0,0,module: crash module: nn module: loss triaged,True
51720,Support for 64-bit (ILP64) LAPACK module: build triaged module: linear algebra,2021-02-04 15:26:51+00:00,,0,5,module: build triaged module: linear algebra,True
51714,Inconsistent result between NumPy and PyTorch for 0^(z) when Re(z) < 0 triaged module: complex module: NaNs and Infs,2021-02-04 09:21:51+00:00,,0,4,triaged module: complex module: NaNs and Infs,True
51711,Weird behavior in Conv2d padding when changed to 'reflect' module: docs module: nn module: convolution triaged module: padding,2021-02-04 07:56:26+00:00,,0,2,module: docs module: nn module: convolution triaged module: padding,True
51673,[JIT/Futures] Future callbacks/then() APIs should throw the correct exception oncall: jit module: bootcamp days,2021-02-03 22:37:00+00:00,,2,2,oncall: jit module: bootcamp days,True
51663,Torch DataLoader fails to reraise error from sqlalchemy. module: dataloader triaged,2021-02-03 19:50:36+00:00,,0,2,module: dataloader triaged,True
51644,"Segmentation Fault: Garbage collector, cuda memory module: cuda triaged",2021-02-03 16:53:23+00:00,,0,3,module: cuda triaged,True
51623,[docs] FractionalMaxPool2d : _random_samples arg is present in signature but undocumented. module: docs module: nn triaged module: pooling,2021-02-03 07:16:33+00:00,,0,6,module: docs module: nn triaged module: pooling,True
51622,"`fractional_max_pool{2, 3}d` inconsistent between CUDA and CPU triaged module: pooling",2021-02-03 07:10:28+00:00,,0,4,triaged module: pooling,True
51616,[RPC] rref.get_type() should have default return type of future oncall: distributed module: rpc,2021-02-03 05:31:32+00:00,,0,0,oncall: distributed module: rpc,True
51575,[doc] being able to switch version while keeping the same doc module: docs triaged enhancement module: doc infra better-engineering,2021-02-02 18:27:04+00:00,,1,4,module: docs triaged enhancement module: doc infra better-engineering,False
51574,[Pipe] conversion to Sequential: various complex scenarios oncall: distributed pipeline parallelism,2021-02-02 18:18:09+00:00,,0,10,oncall: distributed pipeline parallelism,False
51563,[TorchScript Usability] Type-checker too strict for Optional type initialization pattern oncall: jit,2021-02-02 15:29:24+00:00,,1,0,oncall: jit,True
51558,torch.reshape fails to keep the memory format triaged module: memory format module: viewing and reshaping,2021-02-02 13:44:49+00:00,,0,3,triaged module: memory format module: viewing and reshaping,True
51553,ONNX export failed on ATen operator _thnn_fused_gru_cell module: onnx triaged onnx-needs-info,2021-02-02 10:45:52+00:00,,0,2,module: onnx triaged onnx-needs-info,False
51535,Why isn't there a pip-installable PyTorch for Raspberry Pi? module: binaries feature triaged module: arm,2021-02-02 05:56:09+00:00,,0,4,module: binaries feature triaged module: arm,True
51511,Python-3.9: Importing numba and torch triggers internal error module: build triaged,2021-02-02 00:51:12+00:00,,0,1,module: build triaged,True
51509,Massive Performance bottlenecks in some of the Reduce operations. module: performance triaged module: TensorIterator module: reductions,2021-02-02 00:26:33+00:00,,0,5,module: performance triaged module: TensorIterator module: reductions,True
51493,Add branch predictor hints to prefer `Context::deterministicAlgorithms() == false` module: performance triaged module: determinism,2021-02-01 20:55:13+00:00,,1,0,module: performance triaged module: determinism,True
51471,Make TorchBind object appear in state_dict feature module: nn module: serialization triaged,2021-02-01 17:57:06+00:00,,0,1,feature module: nn module: serialization triaged,True
51465,torch::jit::Module cannot be forward declared and used into a `unique_ptr`  oncall: jit,2021-02-01 17:23:45+00:00,,0,0,oncall: jit,True
51456,Optional tensor type feature triaged,2021-02-01 15:26:58+00:00,,0,0,feature triaged,True
51455,"[docs] Improve documentation for LayerNorm, GroupNorm, etc (+ add python reference impl) module: docs module: nn triaged module: norms and normalization",2021-02-01 14:12:57+00:00,,0,11,module: docs module: nn triaged module: norms and normalization,False
51451,Add support for conan package manager triaged enhancement,2021-02-01 12:49:13+00:00,,0,0,triaged enhancement,False
51450,torch.median slower than torch.sort on cpu module: performance module: cpu triaged module: sorting and selection module: reductions,2021-02-01 10:58:14+00:00,,0,4,module: performance module: cpu triaged module: sorting and selection module: reductions,True
51445,Saving embedding vectors as TSV for Tensorboard is not efficient oncall: visualization,2021-02-01 06:55:41+00:00,,0,2,oncall: visualization,False
51440,Support unity 3d feature triaged,2021-02-01 02:48:09+00:00,,0,1,feature triaged,False
51429,"dist.init_process_group('nccl', ...) hangs in some combinations of pytorch+python+cuda version oncall: distributed triaged",2021-01-31 14:09:09+00:00,,0,5,oncall: distributed triaged,True
51426,Different results on RTX 2060s vs GTX 1060 with gpt model triaged module: numerical-reproducibility,2021-01-31 05:39:34+00:00,,0,0,triaged module: numerical-reproducibility,True
51418,Pytorch num_worker>0 code worked first time and then it never worked with same setting again module: dataloader triaged,2021-01-30 19:44:24+00:00,,0,0,module: dataloader triaged,True
51410,Caffe2 RNN Cell Names not rectified caffe2 triaged,2021-01-30 12:17:05+00:00,,0,0,caffe2 triaged,True
51409,C++ load model error oncall: jit,2021-01-30 11:16:26+00:00,,0,1,oncall: jit,True
51407,Additon of levenberg-marquardt optimizer in TORCH.OPTIM module: optimizer triaged enhancement,2021-01-30 10:20:47+00:00,,0,0,module: optimizer triaged enhancement,False
51406,[numpy] `round` and `trunc` not supported for Integral Type while Python and NumPy supports them triaged module: numpy,2021-01-30 10:19:28+00:00,,0,2,triaged module: numpy,True
51393,max_pool2d CPU forward performance is poor module: performance triaged module: pooling,2021-01-30 01:04:04+00:00,,0,2,module: performance triaged module: pooling,True
51385,DataParallel copies the model onto GPUs sequentially module: performance triaged enhancement module: data parallel,2021-01-29 23:03:17+00:00,,0,5,module: performance triaged enhancement module: data parallel,True
51379,TorchScript class compilation failure with pybind11-2.6.2 oncall: jit,2021-01-29 22:19:16+00:00,,0,6,oncall: jit,True
51376,Huggingface model not being torch scriptable oncall: jit,2021-01-29 22:00:10+00:00,,0,5,oncall: jit,False
51351,Run time error module: dependency bug module: cuda triaged module: cublas module: wsl,2021-01-29 15:03:43+00:00,,1,11,module: dependency bug module: cuda triaged module: cublas module: wsl,True
51332,Cant compile from source 1.8.0a0 because 'magma_v2.h' file not found and undefined references to `magma_*' high priority module: build module: cuda triaged module: linear algebra module: magma,2021-01-29 05:01:26+00:00,,1,7,high priority module: build module: cuda triaged module: linear algebra module: magma,True
51328,Support torch::jit::script::Module converting to torch::nn::Module. oncall: jit weeks,2021-01-29 02:34:35+00:00,,0,3,oncall: jit weeks,True
51320,"Pytorch not working properly (I don't know how to summarize it, see below) module: nn triaged",2021-01-29 00:17:31+00:00,,0,6,module: nn triaged,True
51302,"""make install"" skips some required libraries for static build module: build triaged",2021-01-28 20:57:35+00:00,,0,0,module: build triaged,True
51284,result_type doesn't take dtypes and doesn't match numpy triaged module: numpy function request,2021-01-28 14:33:32+00:00,,0,20,triaged module: numpy function request,True
51282,Bind tensor.unflatten to torch.unflatten namespace (torch.flatten exists but not torch.unflatten) triaged,2021-01-28 11:39:59+00:00,,0,1,triaged,True
51280,"[ux] Proposal to have t() === transpose(-1, -2), since batches are very frequent triaged module: numpy module: ux",2021-01-28 11:33:55+00:00,,0,8,triaged module: numpy module: ux,True
51279,Profiler is stuck for DDP training triaged oncall: profiler,2021-01-28 11:17:18+00:00,,0,3,triaged oncall: profiler,True
51275,Error of running needs reproduction triaged,2021-01-28 08:58:45+00:00,,0,2,needs reproduction triaged,True
51256,[search] duplicated entries in results module: docs triaged,2021-01-28 01:06:40+00:00,,1,4,module: docs triaged,True
51231,macOS error building 1.7.1 from source on Catalina 10.15.7 module: build triaged module: macos,2021-01-27 19:40:49+00:00,,0,16,module: build triaged module: macos,True
51229,std::type_index doesn't provide reliable equality for getCustomClassTypeMap module: internals triaged,2021-01-27 19:33:31+00:00,,0,2,module: internals triaged,True
51224,`torch.cuda.device` should look like it is inherited from `torch.device` module: cuda triaged enhancement,2021-01-27 18:49:02+00:00,,1,1,module: cuda triaged enhancement,True
51210,"Complex->Real cast is a warning, calling `real` or `imag` on non-complex tensors is an Error. triaged module: complex module: ux",2021-01-27 13:21:26+00:00,,0,4,triaged module: complex module: ux,True
51207,Initialize NCCL backend with MPI oncall: distributed triaged module: mpi enhancement module: nccl,2021-01-27 11:42:28+00:00,,0,6,oncall: distributed triaged module: mpi enhancement module: nccl,True
51199,[numpy] torch.ceil and torch.floor don't support integer inputs while Numpy does triaged module: numpy,2021-01-27 09:25:17+00:00,,1,2,triaged module: numpy,True
51197,"[docs] use `versionadded`, `versionchanged` and `deprecated` directive module: docs feature triaged",2021-01-27 09:09:40+00:00,,0,5,module: docs feature triaged,False
51156,[Feature Request] Support tensor creation from objects that implement the __array__ interface triaged module: numpy needs design function request,2021-01-26 23:40:14+00:00,,1,11,triaged module: numpy needs design function request,False
51154,[JIT] Rough edges found while exploring expressions supported by TorchScript oncall: jit,2021-01-26 23:39:19+00:00,,0,0,oncall: jit,False
51152,Make it clearer when a tensor was quantized symmetrically oncall: quantization low priority triaged,2021-01-26 23:36:30+00:00,,1,3,oncall: quantization low priority triaged,True
51151,-fno-omit-frame-pointer by default in our builds module: build triaged enhancement,2021-01-26 23:33:42+00:00,,0,9,module: build triaged enhancement,True
51140,[TorchScript Usability] Non-intuitive error messages for user errors  oncall: jit,2021-01-26 20:40:27+00:00,,1,0,oncall: jit,True
51138,[TorchScript Usability] tensor printout format differs from eager mode oncall: jit good first issue OSS contribution wanted TSUsability TSRootCause:PyTorchParityGap,2021-01-26 20:33:49+00:00,,1,6,oncall: jit good first issue OSS contribution wanted TSUsability TSRootCause:PyTorchParityGap,True
51137,[TorchScript Usability] Earlier type checking needed for interface type oncall: jit,2021-01-26 20:29:23+00:00,,0,0,oncall: jit,True
51136,[TorchScript Usability] error message when invoking __init__ of module types inside TorchScript oncall: jit,2021-01-26 20:25:38+00:00,,1,1,oncall: jit,True
51135,[TorchScript Usability] Behaviors of used-defined subclasses of torch.tensor are silently ignored oncall: jit TSRootCause:InvalidCustomClass TSUsability,2021-01-26 20:17:52+00:00,,1,1,oncall: jit TSRootCause:InvalidCustomClass TSUsability,False
51134,segmentation fault in torch.nn.ReplicationPad3d/2d when padding is large module: crash module: nn triaged module: padding,2021-01-26 20:14:42+00:00,,0,0,module: crash module: nn triaged module: padding,True
51117,TestLinalgCPU.test_cholesky_solve_autograd_cpu_float64 fails on different random seed module: autograd triaged module: linear algebra module: correctness (silent),2021-01-26 17:40:05+00:00,,0,3,module: autograd triaged module: linear algebra module: correctness (silent),True
51112,"torch.tensor(x) fails inconsistently, assumes 0 index exists in 'Series' triaged module: tensor creation",2021-01-26 16:58:18+00:00,,0,3,triaged module: tensor creation,True
51084,dyld: Symbol not found: _PyBaseObject_Type   Referenced from module: build triaged module: macos module: static linking,2021-01-26 04:22:24+00:00,,0,0,module: build triaged module: macos module: static linking,True
51083,apex internal assert failed module: dependency bug triaged module: assert failure module: regression,2021-01-26 04:19:58+00:00,,0,2,module: dependency bug triaged module: assert failure module: regression,True
51075,RFC: Private CUDA memory pools feature module: cuda triaged module: cuda graphs,2021-01-26 01:51:23+00:00,,1,13,feature module: cuda triaged module: cuda graphs,True
51074, error: ‘PyFrameObject {aka struct _frame}’ has no member named ‘f_lasti’ module: build triaged,2021-01-26 01:20:14+00:00,,0,1,module: build triaged,True
51062,test_profiler_with_remote_builtin (__main__.TensorPipeRpcTestWithSpawn) is flaky oncall: distributed triaged module: flaky-tests module: rpc,2021-01-25 23:36:22+00:00,,1,0,oncall: distributed triaged module: flaky-tests module: rpc,False
51055,Native code assert or hang on process exit with a torch.jit.script model on multi-GPU triage review oncall: jit,2021-01-25 22:04:09+00:00,,0,14,triage review oncall: jit,True
51054,Class-based structured kernels instruction count regression module: performance module: internals triaged module: structured kernels,2021-01-25 21:06:27+00:00,,0,2,module: performance module: internals triaged module: structured kernels,True
51044,CuDNN 8 with benchmark=True takes minutes to execute for certain configurations module: performance module: cudnn triaged,2021-01-25 17:35:33+00:00,,0,2,module: performance module: cudnn triaged,True
51039,Status of pip wheels with _GLIBCXX_USE_CXX11_ABI=1 high priority module: binaries module: cpp module: abi triaged needs design,2021-01-25 16:24:49+00:00,,0,22,high priority module: binaries module: cpp module: abi triaged needs design,True
51038,The pooling code is ignoring sliding windows that start within the right padding triaged module: pooling,2021-01-25 16:17:36+00:00,,0,1,triaged module: pooling,True
51020,arm64-v8a not compiling due to libpytorch_jni.so module: build triaged module: android module: arm,2021-01-25 01:33:54+00:00,,0,29,module: build triaged module: android module: arm,True
51018,len of ModuleList is incorrect for jitted models oncall: jit,2021-01-24 23:44:03+00:00,,0,3,oncall: jit,True
51015,/usr/local/include/google/protobuf/stubs/pbconfig.h:3:37: note: expanded from macro 'GOOGLE_PROTOBUF_HASH_MAP_H' module: build triaged module: macos,2021-01-24 18:08:26+00:00,,0,0,module: build triaged module: macos,True
51014,[pipeline] gpu util + peak mem reporting to tune partitions and chunks  oncall: distributed feature triaged pipeline parallelism,2021-01-24 17:59:28+00:00,,0,7,oncall: distributed feature triaged pipeline parallelism,True
51011,DataLoader is slow in spawned processes module: performance module: multiprocessing module: dataloader triaged,2021-01-24 13:40:57+00:00,,0,3,module: performance module: multiprocessing module: dataloader triaged,True
51008,Suggestion: Assimilate activation images to PyTorch brand color module: docs triaged,2021-01-24 10:04:38+00:00,,0,0,module: docs triaged,False
51003,Proposing new features: Developing Echo State Network layers new-layer feature triaged shadow review module: embedding,2021-01-24 05:42:44+00:00,,0,1,new-layer feature triaged shadow review module: embedding,False
51002,Consider adding context manager support for torch.set_printoptions module: printing triaged enhancement,2021-01-24 05:41:50+00:00,,0,0,module: printing triaged enhancement,True
50987,Scalar/Tensor arg type for op schemas triaged module: codegen,2021-01-23 09:29:36+00:00,,0,7,triaged module: codegen,True
50971,Failing to trace Huggingface models using Trace API from Torchscript oncall: jit,2021-01-22 23:10:33+00:00,,0,5,oncall: jit,False
50962,Add option for Convolutions to operate over NTC and NHWC tensors feature triaged module: memory format,2021-01-22 21:22:13+00:00,,0,5,feature triaged module: memory format,True
50953,Dispatch-less structured wrapper / composite / alias kernels module: internals triaged module: structured kernels,2021-01-22 18:48:34+00:00,,0,22,module: internals triaged module: structured kernels,True
50944,libpytorch macos build: static library eigen_blas_LIBRARY-NOTFOUND not found module: build triaged,2021-01-22 13:16:45+00:00,,0,0,module: build triaged,True
50943,Exporting opnames fails when using custom classes oncall: mobile,2021-01-22 13:03:06+00:00,,0,3,oncall: mobile,False
50941,Questions about amp module: cuda triaged module: amp (automated mixed precision),2021-01-22 11:34:53+00:00,,0,1,module: cuda triaged module: amp (automated mixed precision),True
50940,SequentialSampler getting slower as time passing by module: dataloader triaged,2021-01-22 11:28:07+00:00,,0,8,module: dataloader triaged,True
50939,dataloader bug need help module: dataloader triaged,2021-01-22 10:41:54+00:00,,0,4,module: dataloader triaged,True
50933,[JIT] Type refinement failure in if-elif-else structure oncall: jit TSRootCause:TypeRefinement TSUsability,2021-01-22 06:54:36+00:00,,0,1,oncall: jit TSRootCause:TypeRefinement TSUsability,True
50925,THCCachingAllocator::cuda_free_mutex has no effect module: cuda triaged module: nccl,2021-01-22 01:40:11+00:00,,0,0,module: cuda triaged module: nccl,True
50890,"torch.unique(x, dim=1, return_inverse=True) returns inverse for only the last sub-tensor along dim module: docs triaged module: numpy module: sorting and selection",2021-01-21 17:55:54+00:00,,0,3,module: docs triaged module: numpy module: sorting and selection,True
50888,Does pytorch support cuda 11.1 module: build triaged,2021-01-21 16:35:38+00:00,,0,3,module: build triaged,True
50879,Unexpected slow dropout in stacked RNN/LSTM/GRU high priority module: cudnn module: rnn triaged,2021-01-21 14:28:18+00:00,,0,5,high priority module: cudnn module: rnn triaged,True
50873,Segmentation Fault when importing torch on macOS Big Sur high priority needs reproduction module: binaries triaged module: macos module: openmp,2021-01-21 10:01:41+00:00,,0,15,high priority needs reproduction module: binaries triaged module: macos module: openmp,True
50853,[nnc][perf] CPU fuser needs to support intra-op parallelism triaged NNC,2021-01-20 23:22:19+00:00,,1,4,triaged NNC,False
50852,[TorchScript Performance Deep-dive] problems discovered in TS performance deep-dive module: performance triaged,2021-01-20 23:20:01+00:00,,4,1,module: performance triaged,True
50835,Insufficient shared memory (shm) while training module: memory usage triaged,2021-01-20 20:16:06+00:00,,0,2,module: memory usage triaged,True
50834,Precision of scalars is lost in script mode for binary operations with tensors oncall: jit,2021-01-20 20:11:12+00:00,,1,4,oncall: jit,False
50831,Dataloader Prefetch data to GPU by cudaMemPrefetchAsync module: dataloader triaged enhancement,2021-01-20 19:50:12+00:00,,0,1,module: dataloader triaged enhancement,True
50830,Pull upstream cmake's change in FindCUDA/select_compute_arch module: build module: cuda triaged,2021-01-20 19:14:07+00:00,,1,0,module: build module: cuda triaged,True
50829,pin_memory leads to GPU memory being used but it doesn't initialize CUDA module: cuda triaged,2021-01-20 19:13:49+00:00,,0,2,module: cuda triaged,True
50820,NCCL_BLOCKING_WAIT=1 makes training extremely slow (but if not set then OOM on one device will hang training) oncall: distributed triaged,2021-01-20 17:28:26+00:00,,0,6,oncall: distributed triaged,True
50804,Segmentation fault encountered when using nn.MultiheadAttention with v1.7.1 module: crash triaged module: regression oncall: transformer/mha,2021-01-20 11:43:49+00:00,,0,3,module: crash triaged module: regression oncall: transformer/mha,True
50803,pytorch DDP hangs at .backward() call needs reproduction triaged module: deadlock module: ddp,2021-01-20 11:13:24+00:00,,0,2,needs reproduction triaged module: deadlock module: ddp,True
50802,Link to `torch.einsum` in `torch.tensordot` module: docs triaged,2021-01-20 11:02:32+00:00,,0,2,module: docs triaged,True
50789,Implement torch.pow for float16 and bfloat16 on CPU triaged module: half function request,2021-01-20 03:30:51+00:00,,0,4,triaged module: half function request,True
50779,pytorch is built without _GLIBCXX_USE_CXX11_ABI and can cause std::regex crashes (probably) needs reproduction module: build triaged,2021-01-20 01:25:27+00:00,,0,9,needs reproduction module: build triaged,True
50772,[TorchScript Usability] Incorrect default types oncall: jit,2021-01-19 22:26:05+00:00,,1,0,oncall: jit,True
50758,"Improve error message for ""Could not run 'aten::record_stream' with arguments from the 'CPU' backend. ""  triaged enhancement module: dispatch",2021-01-19 20:05:38+00:00,,1,0,triaged enhancement module: dispatch,True
50743,Batched grad coverage rollup module: autograd triaged module: vmap,2021-01-19 17:31:07+00:00,,0,3,module: autograd triaged module: vmap,True
50737,Adding visualizations for indexing and other functions module: docs triaged module: advanced indexing module: scatter & gather ops,2021-01-19 16:47:25+00:00,,0,3,module: docs triaged module: advanced indexing module: scatter & gather ops,True
50734,[TorchScript] Document TorchScript IR oncall: jit,2021-01-19 15:40:07+00:00,,2,1,oncall: jit,False
50723,[PyTorch Mobile] Can't use Vulkan backend triaged module: android oncall: mobile module: vulkan,2021-01-19 13:06:30+00:00,,0,5,triaged module: android oncall: mobile module: vulkan,True
50718,dependency_links is deprecated  module: build module: docs triaged,2021-01-19 10:17:45+00:00,,0,8,module: build module: docs triaged,True
50714,[PyTorch Mobile] Android speed benchmark binary crashes when using vulkan oncall: mobile,2021-01-19 07:54:48+00:00,,0,7,oncall: mobile,False
50712,ignore_index for nn_mse_loss module: nn module: loss triaged needs design function request,2021-01-19 06:40:31+00:00,,0,1,module: nn module: loss triaged needs design function request,True
50701,Multiple ProcessGroup C++ Tests are flaky oncall: distributed triaged module: flaky-tests module: c10d,2021-01-18 21:15:53+00:00,,1,0,oncall: distributed triaged module: flaky-tests module: c10d,True
50694,CosineAnnealingWarmRestarts lacks verbose functionality module: optimizer triaged,2021-01-18 20:04:54+00:00,,0,0,module: optimizer triaged,True
50689,RandomSampler is very slow with huge dataset module: dataloader triaged,2021-01-18 15:15:41+00:00,,0,13,module: dataloader triaged,True
50688,[feature request] `torch.scan` (also port `lax.fori_loop` / `lax.while_loop` / `lax.associative_scan` and hopefully parallelized associative scans) feature triaged module: numpy needs design module: functional UX oncall: pt2 module: functorch module: pt2-dispatcher,2021-01-18 14:21:46+00:00,,1,38,feature triaged module: numpy needs design module: functional UX oncall: pt2 module: functorch module: pt2-dispatcher,False
50678,[PyTorch Mobile] NNAPI prototype installer no longer available oncall: mobile,2021-01-18 07:19:11+00:00,,0,1,oncall: mobile,False
50669,Launching two processes causes hanging module: multiprocessing triaged,2021-01-17 19:58:43+00:00,,0,9,module: multiprocessing triaged,True
50650,build from source fail in jeston tx2 Python3.8 module: build triaged module: arm,2021-01-16 06:17:58+00:00,,0,2,module: build triaged module: arm,True
50636,Training slowdown from 1.6 to 1.7.1 module: performance triaged module: __torch_function__,2021-01-16 00:20:31+00:00,,0,1,module: performance triaged module: __torch_function__,True
50616,Use unified type for distributions.constraint API module: distributions triaged,2021-01-15 21:58:12+00:00,,0,1,module: distributions triaged,True
50613,Discourage slow gradchecks module: autograd module: tests triaged,2021-01-15 21:16:12+00:00,,0,3,module: autograd module: tests triaged,True
50566,Enhance supported fill value type for constant pad in functional.pad module: nn triaged enhancement needs design module: padding,2021-01-15 03:14:30+00:00,,0,13,module: nn triaged enhancement needs design module: padding,True
50552,test_fn_grad_fft_fftn_cpu_complex128 and test_fn_grad_fft_rfftn_cpu_float64 are failing under TSAN module: tests triaged module: sanitizers module: fft,2021-01-14 21:13:02+00:00,,0,0,module: tests triaged module: sanitizers module: fft,True
50549,Channels last doesn't improve speed when using SyncBatchNorm module: performance triaged module: memory format,2021-01-14 20:24:55+00:00,,0,4,module: performance triaged module: memory format,True
50533,Non-avairable nightly version for converting models to NNAPI oncall: mobile,2021-01-14 11:41:27+00:00,,0,13,oncall: mobile,False
50531,"PyTorch mobile perf recipes, fails when building android  module: android oncall: mobile",2021-01-14 10:45:44+00:00,,0,1,module: android oncall: mobile,False
50530,Unable to compile CUDAExtension with Pytorch 1.5 or above module: bc-breaking module: cpp-extensions module: cuda triaged topic: bc breaking,2021-01-14 09:07:31+00:00,,0,3,module: bc-breaking module: cpp-extensions module: cuda triaged topic: bc breaking,True
50503,Huge file sizes for libtorch and dependencies? module: binaries module: build triaged,2021-01-13 21:45:27+00:00,,0,2,module: binaries module: build triaged,True
50473,Make use of eps for numerical stability more consistent module: numerical-stability triaged,2021-01-13 13:08:25+00:00,,0,0,module: numerical-stability triaged,True
50471,linker error when trying to use Metal backend in PyTorch Mobile module: build triaged module: arm,2021-01-13 12:10:19+00:00,,0,2,module: build triaged module: arm,True
50468,[RFC] Make RRef proxy APIs non-blocking triaged module: rpc,2021-01-13 10:10:17+00:00,,0,2,triaged module: rpc,True
50463,creating a dummy_input param for Pytorch model to Onnx conversion module: onnx triaged,2021-01-13 07:24:36+00:00,,0,1,module: onnx triaged,False
50462,Standalone OSS RPC benchmark oncall: distributed triaged better-engineering module: rpc,2021-01-13 07:00:47+00:00,,0,0,oncall: distributed triaged better-engineering module: rpc,True
50444,[TorchScript] Rough edges discovered during type system rewrite oncall: jit,2021-01-12 20:20:11+00:00,,1,0,oncall: jit,False
50425,libtorch gpu set id bug module: cpp module: cuda triaged,2021-01-12 09:22:48+00:00,,0,1,module: cpp module: cuda triaged,True
50402,ValueError: optimizer got an empty parameter list module: optimizer triaged,2021-01-11 23:09:39+00:00,,0,2,module: optimizer triaged,False
50384,Static hooks serialization: moving the hook definitions outside of module definitions in the archive oncall: jit,2021-01-11 19:03:12+00:00,,0,0,oncall: jit,False
50382,out variant of many loss functions are not consistent with non-out variant when reduction is not none module: cpp module: nn module: loss triaged module: reductions,2021-01-11 18:36:39+00:00,,1,3,module: cpp module: nn module: loss triaged module: reductions,True
50375,Provide a set of C++ foreach APIs that will take tensor pointers as an input module: cpp triaged,2021-01-11 16:07:11+00:00,,1,3,module: cpp triaged,True
50366,Optimizer Closure: Enable skip by returning None loss. module: optimizer triaged,2021-01-11 10:13:36+00:00,,0,0,module: optimizer triaged,False
50360,error: conversion from ‘std::vector<at::Tensor>’ to non-scalar type ‘at::Tensor’ requested triaged module: numpy,2021-01-11 04:01:33+00:00,,0,19,triaged module: numpy,True
50354,for CNN in fp16 execution time depends on input scale module: performance module: nn module: cuda triaged module: half,2021-01-10 18:58:05+00:00,,0,13,module: performance module: nn module: cuda triaged module: half,True
50352,torch.tril_indices is incompatible with np.tril_indices triaged module: numpy module: deprecation module: ux,2021-01-10 15:21:11+00:00,,0,4,triaged module: numpy module: deprecation module: ux,True
50346,how to save weights when using RPC framework oncall: distributed triaged module: rpc,2021-01-10 08:26:37+00:00,,0,3,oncall: distributed triaged module: rpc,True
50345,torch.special tracking issue triaged module: numpy module: special tracker,2021-01-10 08:02:14+00:00,,1,15,triaged module: numpy module: special tracker,True
50344,NumPy Compatibility tracking issue high priority triaged module: numpy tracker,2021-01-10 07:45:08+00:00,,0,0,high priority triaged module: numpy tracker,True
50343,RFC: identify analogous NumPy operators when documenting PyTorch operators module: docs triaged module: numpy,2021-01-10 07:44:49+00:00,,1,1,module: docs triaged module: numpy,True
50342,torch.any and torch.all map uint8 -> uint8 but should map uint8 -> bool module: bc-breaking triaged module: numpy module: reductions topic: bc breaking,2021-01-10 07:41:01+00:00,,0,0,module: bc-breaking triaged module: numpy module: reductions topic: bc breaking,True
50341,Interpolation tracking issue triaged module: numpy module: interpolation tracker,2021-01-10 07:17:12+00:00,,0,5,triaged module: numpy module: interpolation tracker,True
50340,Function request: scipy.interpolate.InterpolatedUnivariateSpline triaged module: numpy function request module: interpolation,2021-01-10 07:01:06+00:00,,0,0,triaged module: numpy function request module: interpolation,True
50339,Function request: scipy.interpolate.griddata triaged module: numpy function request module: interpolation,2021-01-10 06:59:07+00:00,,0,8,triaged module: numpy function request module: interpolation,True
50338,Function request: scipy.interpolate.RegularGridInterpolator triaged module: numpy function request module: interpolation,2021-01-10 06:57:40+00:00,,0,0,triaged module: numpy function request module: interpolation,True
50337,Function request: scipy.interpolate.RectBivariateSpline triaged module: numpy function request module: interpolation,2021-01-10 06:55:45+00:00,,0,0,triaged module: numpy function request module: interpolation,True
50336,Function Request: scipy.ndimage.zoom triaged module: numpy function request module: interpolation,2021-01-10 06:53:43+00:00,,0,1,triaged module: numpy function request module: interpolation,True
50335,Function Request: scipy.interpolate.interp1d triaged module: numpy function request module: interpolation,2021-01-10 06:51:48+00:00,,0,2,triaged module: numpy function request module: interpolation,True
50334,Function Request: np.interp triaged module: numpy function request module: interpolation,2021-01-10 06:49:49+00:00,,0,4,triaged module: numpy function request module: interpolation,True
50333,Function Request: scipy.stats.pearsonr triaged module: numpy function request,2021-01-10 06:26:32+00:00,,0,1,triaged module: numpy function request,True
50332,torch.Tensor.random_ is divergent from NumPy's np.random.random triaged module: numpy module: random module: deprecation,2021-01-10 06:12:24+00:00,,0,3,triaged module: numpy module: random module: deprecation,True
50328,RTX3090 performs no better than 1080ti module: performance triaged,2021-01-10 02:26:44+00:00,,0,12,module: performance triaged,True
50323,running a job on multiple gpus with qsub  oncall: distributed triaged,2021-01-09 16:10:12+00:00,,0,8,oncall: distributed triaged,True
50303,Way to solve GPU Host Thread Contention? oncall: distributed triaged,2021-01-09 00:19:11+00:00,,0,2,oncall: distributed triaged,True
50292,Implement dunder getattribute instead of dunder getattr for nn.Module module: bc-breaking module: nn triaged topic: bc breaking,2021-01-08 21:55:56+00:00,,0,2,module: bc-breaking module: nn triaged topic: bc breaking,True
50282,Add MultiScheduler module: optimizer triaged enhancement,2021-01-08 16:27:28+00:00,,0,2,module: optimizer triaged enhancement,False
50276,torch.meshgrid is divergent from np.meshgrid triaged module: numpy module: deprecation,2021-01-08 14:41:14+00:00,,0,21,triaged module: numpy module: deprecation,True
50275,torch.transpose is divergent from np.transpose triaged module: numpy module: deprecation,2021-01-08 14:27:06+00:00,,0,1,triaged module: numpy module: deprecation,True
50274,torch.equal is divergent from np.equal triaged module: numpy module: deprecation module: testing,2021-01-08 14:19:33+00:00,,1,1,triaged module: numpy module: deprecation module: testing,True
50273,torch.cross is divergent from np.cross triaged module: numpy module: deprecation,2021-01-08 14:11:48+00:00,,0,5,triaged module: numpy module: deprecation,True
50268,CUDA error when using `binary_cross_entropy_with_logits` module: cuda module: memory usage triaged,2021-01-08 10:16:12+00:00,,0,10,module: cuda module: memory usage triaged,True
50261,ModuleNotFoundError: No module named '__torch__' needs reproduction module: multiprocessing module: serialization triaged,2021-01-08 09:05:01+00:00,,0,2,needs reproduction module: multiprocessing module: serialization triaged,True
50260,FusedKernelCPU failed to delete generated dll files on Windows  oncall: jit module: windows,2021-01-08 08:42:13+00:00,,0,3,oncall: jit module: windows,True
50258,More general MultiHeadAttention and Transformer modules triaged enhancement oncall: transformer/mha,2021-01-08 07:38:44+00:00,,0,4,triaged enhancement oncall: transformer/mha,True
50231,"Tracing with autocast failed with error: ""Cannot insert a Tensor that requires grad as a constant"" oncall: jit module: amp (automated mixed precision)",2021-01-07 21:31:21+00:00,,0,1,oncall: jit module: amp (automated mixed precision),True
50219,`requires_grad_` in `no_grad` context returns incorrect value with tensor subclasses module: autograd triaged,2021-01-07 19:03:46+00:00,,0,10,module: autograd triaged,True
50213,torch.float_power out= and inplace variant errors on non-matching output dtype instead of casting triaged module: type promotion,2021-01-07 17:32:33+00:00,,0,1,triaged module: type promotion,True
50210,"Pytorch 1.7.1 hangs with multi-gpu, while Pytorch 1.6.0 works correctly triaged module: deadlock module: data parallel",2021-01-07 16:03:35+00:00,,0,6,triaged module: deadlock module: data parallel,True
50206,Build Fail with vulkan on ARM64 and wayland drivers module: build triaged,2021-01-07 15:53:33+00:00,,0,1,module: build triaged,True
50204,Find the joint eigenvalue of two matrices.  feature triaged needs research module: linear algebra,2021-01-07 15:46:07+00:00,,0,8,feature triaged needs research module: linear algebra,True
50201,Excessive memory usage caused by Samplers storing lists of indices module: dataloader module: memory usage triaged,2021-01-07 14:22:19+00:00,,0,3,module: dataloader module: memory usage triaged,True
50198,deterministic implementation for adaptive_avg_pool2d_backward_cuda  module: cuda triaged module: determinism module: pooling function request,2021-01-07 11:22:49+00:00,,0,4,module: cuda triaged module: determinism module: pooling function request,True
50192,GRU and LSTM fail for seq_len = 0 module: rnn triaged,2021-01-07 06:55:58+00:00,,1,12,module: rnn triaged,True
50190,Calling emptyCache in Conv_v7.cpp causes performance degradation module: cudnn module: convolution triaged,2021-01-07 05:32:21+00:00,,0,0,module: cudnn module: convolution triaged,True
50185,CUDACachingAllocator is not GC-aware module: cuda module: memory usage triaged,2021-01-07 02:27:46+00:00,,0,1,module: cuda module: memory usage triaged,True
50177,[FX] Static specialization of callsites (e.g. chunk) triaged module: fx,2021-01-07 00:37:52+00:00,,0,0,triaged module: fx,True
50170,Automated Mixed Precision not documented to work with nn.DataParallel triaged module: data parallel,2021-01-06 23:33:22+00:00,,0,0,triaged module: data parallel,True
50143,FX call_function does not work if target is a class member function triaged module: fx,2021-01-06 15:49:35+00:00,,0,4,triaged module: fx,True
50136,Inconsistent Variable Naming in FindTorch.cmake module: build module: cpp triaged,2021-01-06 09:42:01+00:00,,0,0,module: build module: cpp triaged,True
50134,"nn.InstanceNorm should warn user if input channel is inconsistent with num_features, even when affine=False module: nn triaged needs design module: norms and normalization",2021-01-06 09:02:40+00:00,,0,1,module: nn triaged needs design module: norms and normalization,True
50127,JIT does not correctly compile custom classes derived from torch.Tensor oncall: jit,2021-01-06 05:21:18+00:00,,0,0,oncall: jit,True
50122,[proposal] Pseudo-functions to support common gradient patch use-cases like replacing inf / nan or clipping / gradient reversal module: autograd triaged needs design,2021-01-05 23:26:30+00:00,,0,6,module: autograd triaged needs design,True
50118,torch.where scalar/tensor documentation is unclear and not formatted module: docs triaged module: sorting and selection,2021-01-05 22:52:49+00:00,,1,3,module: docs triaged module: sorting and selection,True
50112,need a clear guide for when and how to use torch.cuda.set_device() module: docs module: cuda triaged needs design,2021-01-05 22:11:26+00:00,,0,4,module: docs module: cuda triaged needs design,True
50098,Errors when coercing complex numbers of various sizes triaged module: complex module: half,2021-01-05 16:13:31+00:00,,0,6,triaged module: complex module: half,True
50097,Cannot print 32-bit complex tensors module: docs module: printing triaged module: complex module: half,2021-01-05 15:46:38+00:00,,0,4,module: docs module: printing triaged module: complex module: half,True
50094,Broken LAPACK links in the documentation module: docs triaged module: linear algebra,2021-01-05 14:15:48+00:00,,0,1,module: docs triaged module: linear algebra,True
50092,batch_isend_irecv: the receiving end cannot receive large tensors from the sending end correctly oncall: distributed triaged,2021-01-05 10:26:28+00:00,,1,3,oncall: distributed triaged,True
50090,Torch native_layer_norm OP out-of-bounds access triaged module: norms and normalization,2021-01-05 08:19:35+00:00,,0,1,triaged module: norms and normalization,True
50076,resize_ documentation does not match implementation when memory_format is given module: docs triaged,2021-01-04 23:29:10+00:00,,0,6,module: docs triaged,False
50068,Magma functions that don't have queue argument create cublas handles for each call module: performance triaged module: linear algebra,2021-01-04 21:38:25+00:00,,0,3,module: performance triaged module: linear algebra,True
50051,pytorch_windows_vs2019_py36_cuda11.1_test1 intermittently fails high priority module: windows module: cuda module: ci triaged,2021-01-04 18:43:08+00:00,,1,7,high priority module: windows module: cuda module: ci triaged,True
50038,Torch  _remove_batch_dim OP out-of-bounds access module: crash triaged module: vmap,2021-01-04 11:46:43+00:00,,0,5,module: crash triaged module: vmap,True
50037,Torch quantized_lstm_cell op out-of-bounds access module: crash oncall: quantization low priority triaged,2021-01-04 10:35:26+00:00,,1,3,module: crash oncall: quantization low priority triaged,True
50036,channels_last format convolution is slower than normal NCHW module: performance module: cuda triaged module: memory format,2021-01-04 08:18:14+00:00,,0,6,module: performance module: cuda triaged module: memory format,True
50034,Multinomial without replacement produces samples that have zero probability module: distributions triaged module: numpy module: random module: ux,2021-01-04 07:05:29+00:00,,0,8,module: distributions triaged module: numpy module: random module: ux,True
50031,Inconsistent function name between stub and implementation in `torch.optim.swa_utils` module: typing triaged,2021-01-04 04:48:50+00:00,,0,1,module: typing triaged,False
50013,torch.Tensor.repeat is divergent from np.repeat high priority triaged module: numpy module: deprecation,2021-01-03 12:04:06+00:00,,0,3,high priority triaged module: numpy module: deprecation,True
50012,torch.split is divergent from np.split high priority triaged module: numpy module: deprecation,2021-01-03 11:53:42+00:00,,0,2,high priority triaged module: numpy module: deprecation,True
50010,torch.var and torch.std are not compatible with np.var and np.std high priority triaged module: numpy module: deprecation module: reductions,2021-01-03 10:46:29+00:00,,1,7,high priority triaged module: numpy module: deprecation module: reductions,True
49992,"[docs] nn.modules pages should mention corresponding functional versions, e.g. for nn.Hardshrink module: docs module: nn triaged small",2021-01-01 13:06:22+00:00,,0,2,module: docs module: nn triaged small,True
49961,rfc: automating the switching of inputs to the device of the params oncall: distributed feature module: cuda module: rpc needs design module: ux,2020-12-30 03:40:54+00:00,,0,18,oncall: distributed feature module: cuda module: rpc needs design module: ux,False
49949,"RFC: Add a ""see also"" section to operators' documentation module: docs triaged",2020-12-29 21:16:42+00:00,,0,3,module: docs triaged,False
49935,[FX] intermediate types of empty lists/dicts not preserved during torch.fx tracing oncall: jit TSRootCause:DefaultTypes TSUsability,2020-12-29 19:55:30+00:00,,1,5,oncall: jit TSRootCause:DefaultTypes TSUsability,False
49911,[RFC] Speed up python function and arg serialization in RPC APIs oncall: distributed triaged module: rpc,2020-12-29 01:02:54+00:00,,0,5,oncall: distributed triaged module: rpc,True
49909,Test functionality to detect extra cross device synchronizations module: cuda module: tests triaged,2020-12-29 00:04:29+00:00,,0,0,module: cuda module: tests triaged,True
49900,A significant overhead when running fastrnns with autograd.profiler module: performance triaged oncall: profiler,2020-12-28 21:02:06+00:00,,1,0,module: performance triaged oncall: profiler,True
49891,Support DDP find_unused_parameters=True mode when combined with Pipe oncall: distributed triaged module: rpc module: ddp,2020-12-28 16:19:06+00:00,,1,2,oncall: distributed triaged module: rpc module: ddp,True
49890,GEMM with int8 datatype throws RuntimeError on GPU triaged module: linear algebra needs design function request,2020-12-28 15:23:18+00:00,,0,13,triaged module: linear algebra needs design function request,True
49885,LR scheduler `get_lr()` bug module: optimizer triaged,2020-12-28 10:39:19+00:00,,0,4,module: optimizer triaged,True
49882,nll_loss2d: t >= 0 && t < n_classes assertion is not checked when using GPU tensors and reduction='none' module: nn module: loss module: cuda good first issue module: error checking triaged,2020-12-28 08:19:21+00:00,,0,7,module: nn module: loss module: cuda good first issue module: error checking triaged,True
49877,The same tensor requires more memory on RTX3090 high priority module: cuda module: memory usage triaged,2020-12-27 14:11:09+00:00,,0,5,high priority module: cuda module: memory usage triaged,True
49874,Accessibility Coding Needed feature triaged module: doc infra,2020-12-27 07:14:05+00:00,,0,1,feature triaged module: doc infra,False
49874,Accessibility Coding Needed feature triaged module: doc infra,2020-12-27 07:14:05+00:00,,0,1,feature triaged module: doc infra,False
49873,Accessibility of Website for Low Vision Users feature triaged module: doc infra,2020-12-27 07:13:09+00:00,,0,1,feature triaged module: doc infra,False
49870,Tensorboard Error needs reproduction triaged,2020-12-26 20:37:41+00:00,,0,1,needs reproduction triaged,True
49865,torch.solve on Jetson is slower than humans module: build triaged module: linear algebra,2020-12-26 08:05:45+00:00,,0,3,module: build triaged module: linear algebra,True
49856,Pytorch1.7.1 Dataloader bugs on win10 module: windows module: dataloader triaged,2020-12-25 18:08:05+00:00,,1,11,module: windows module: dataloader triaged,True
49852,Advanced Indexing does not trace correctly for tensor shape that has leading 1s oncall: jit module: advanced indexing days,2020-12-25 12:35:03+00:00,,1,0,oncall: jit module: advanced indexing days,True
49851,[Feature Request] SuperLoss (NeurIPS 2020) module: nn module: loss triaged function request,2020-12-25 12:30:19+00:00,,0,4,module: nn module: loss triaged function request,True
49849,[dataloader] RuntimeError: Too many open files when yielding integers module: dataloader triaged,2020-12-25 09:38:50+00:00,,0,1,module: dataloader triaged,True
49846,torch init_process_group always hangs awaiting response (this tag is deprecated) oncall: distributed triaged,2020-12-25 07:11:07+00:00,,0,2,awaiting response (this tag is deprecated) oncall: distributed triaged,True
49844,BCEWithLogitsLoss gives out nan with -inf logits module: loss triaged module: NaNs and Infs,2020-12-25 05:31:00+00:00,,0,8,module: loss triaged module: NaNs and Infs,True
49832,RuntimeError: CUDA error: unspecified launch failure needs reproduction module: cuda triaged,2020-12-24 14:09:44+00:00,,0,2,needs reproduction module: cuda triaged,True
49825,Ensure inplace views are properly handled module: autograd triaged better-engineering,2020-12-24 09:49:24+00:00,,0,6,module: autograd triaged better-engineering,True
49823,Why trace method used insted of script method in graph creation? oncall: visualization,2020-12-24 08:35:06+00:00,,0,0,oncall: visualization,False
49820,[JIT] Don't support math.exp and math.log in the Script oncall: jit days TSRootCause:TypeChecking TSUsability,2020-12-24 05:27:10+00:00,,0,0,oncall: jit days TSRootCause:TypeChecking TSUsability,False
49814,from_blob segfaults when given CUDA pointer module: crash module: cpp module: cuda triaged,2020-12-24 00:49:50+00:00,,0,4,module: crash module: cpp module: cuda triaged,True
49810,Undefined reference to `cv::imread` module: build triaged has workaround,2020-12-23 19:00:12+00:00,,0,2,module: build triaged has workaround,True
49795,ninja: build stopped: subcommand failed. module: build triaged,2020-12-23 09:37:15+00:00,,0,3,module: build triaged,True
49791,Caching for autoregressive decoding of Transformer module: performance feature module: nn triaged oncall: transformer/mha,2020-12-23 07:54:46+00:00,,0,2,module: performance feature module: nn triaged oncall: transformer/mha,True
49784,Compatibility with Raspberry Pi module: build feature triaged,2020-12-23 03:29:03+00:00,,0,0,module: build feature triaged,True
49782,TorchScript model's output different on Mobile and PC oncall: mobile,2020-12-23 03:20:30+00:00,,0,6,oncall: mobile,False
49764,Second order derivative errors with torch.jit.script decorator after a few iterations oncall: jit weeks,2020-12-22 22:55:42+00:00,,1,1,oncall: jit weeks,False
49758,c10::scalar_to_tensor(...) uses should be audited for performance and type promotion impact high priority module: performance module: cuda triaged,2020-12-22 22:13:56+00:00,,1,6,high priority module: performance module: cuda triaged,True
49752,IndexError: Input _features.0.weight is undefined! caffe2 triaged module: vision,2020-12-22 20:39:24+00:00,,0,3,caffe2 triaged module: vision,True
49727,RandomSampler generator created in every iteration module: dataloader triaged,2020-12-22 07:26:40+00:00,,1,1,module: dataloader triaged,True
49726,Wrong error is raised for property of nn.Module (again) module: nn triaged,2020-12-22 06:34:15+00:00,,0,10,module: nn triaged,True
49724,cure unnecessary NaN values that arise from ±∞ arguments module: numerical-stability module: bootcamp module: autograd triaged module: NaNs and Infs,2020-12-22 05:24:51+00:00,,0,25,module: numerical-stability module: bootcamp module: autograd triaged module: NaNs and Infs,True
49723,Flattening nn.Parameters while maintaining gradients from neural network forward pass module: nn module: optimizer triaged enhancement,2020-12-22 04:07:24+00:00,,0,3,module: nn module: optimizer triaged enhancement,True
49716,Two independent future chains in PowerSGD cannot be kicked off asynchronously oncall: distributed triaged,2020-12-22 00:23:55+00:00,,0,2,oncall: distributed triaged,True
49678,That define multiple models can influence the convolution parameter update. needs reproduction module: nn triaged,2020-12-21 13:01:37+00:00,,0,1,needs reproduction module: nn triaged,True
49677,"File ""setup.py"", line 773:subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '8']' returned non-zero exit status 2 module: build module: cuda triaged",2020-12-21 11:39:08+00:00,,0,4,module: build module: cuda triaged,True
49673,Is there any upper-level API to disable the detection of the weight’s version module: autograd triaged,2020-12-21 10:01:53+00:00,,0,3,module: autograd triaged,False
49672,Model passed through queue hangs while loading in sub process  module: multiprocessing triaged,2020-12-21 09:39:33+00:00,,0,1,module: multiprocessing triaged,True
49669,"While training the model, GPU util reaches 100% but no progress happends. needs reproduction module: cuda triaged module: deadlock",2020-12-21 08:40:55+00:00,,0,5,needs reproduction module: cuda triaged module: deadlock,True
49666,Use rlibm for faster and more accurate floating point operations triaged enhancement module: bfloat16,2020-12-21 04:42:14+00:00,,0,3,triaged enhancement module: bfloat16,True
49653,"torch.cuda.amp, example with 20% memory increase compared to apex/amp triaged module: amp (automated mixed precision)",2020-12-20 11:58:25+00:00,,0,0,triaged module: amp (automated mixed precision),True
49650,Annotating submodule with correct type fails scripting oncall: jit TSRootCause:InvalidCustomClass TSUsability,2020-12-20 05:38:34+00:00,,1,0,oncall: jit TSRootCause:InvalidCustomClass TSUsability,True
49643,Tracing doesn't work with spectral normalization oncall: jit weeks,2020-12-19 14:16:38+00:00,,0,0,oncall: jit weeks,False
49642,Turn deprecation warnings into errors in CI module: ci module: tests triaged,2020-12-19 13:34:03+00:00,,0,4,module: ci module: tests triaged,True
49630,Bug in CosineAnnealingWarmRestarts module: optimizer triaged,2020-12-19 03:06:04+00:00,,0,5,module: optimizer triaged,True
49618,Support scope in fx Node triaged module: fx,2020-12-18 22:49:28+00:00,,0,0,triaged module: fx,True
49614,Improve error on missing/problematic Module member type annotation oncall: jit days TSRootCause:TypeChecking TSUsability,2020-12-18 20:08:13+00:00,,0,2,oncall: jit days TSRootCause:TypeChecking TSUsability,True
49585,[JIT] Serialization forward compatibility tests oncall: jit weeks,2020-12-18 04:05:19+00:00,,1,0,oncall: jit weeks,True
49569,Update jit/OVERVIEW.md oncall: jit weeks,2020-12-17 22:07:13+00:00,,2,1,oncall: jit weeks,False
49562,"Batched gradient computation w/ vmap, feature rollup triaged actionable module: vmap",2020-12-17 20:05:27+00:00,,0,0,triaged actionable module: vmap,False
49555,Gunicorn preload flag not working with PyTorch library triaged oncall: transformer/mha,2020-12-17 18:50:55+00:00,,0,5,triaged oncall: transformer/mha,False
49539,Improve AssertionError for torch.nn.functional.pad 'replicate' module: bootcamp module: nn triaged enhancement better-engineering,2020-12-17 15:14:59+00:00,,0,3,module: bootcamp module: nn triaged enhancement better-engineering,True
49538,jit tracer doesn't work with unflatten layer oncall: jit weeks,2020-12-17 14:48:46+00:00,,1,17,oncall: jit weeks,True
49536,Pytorch mobile Vulkan API bug during call to ReLU oncall: mobile oncall: java,2020-12-17 09:46:39+00:00,,0,1,oncall: mobile oncall: java,True
49520,Segfault in torch.bincount module: crash triaged,2020-12-17 00:50:14+00:00,,0,1,module: crash triaged,True
49503,Enable torch.nn.modules.pooling typechecks during CI module: typing triaged,2020-12-16 21:53:56+00:00,,1,0,module: typing triaged,True
49491,Bug report: INTERNAL ASSERT FAILED needs reproduction triaged,2020-12-16 20:56:53+00:00,,0,2,needs reproduction triaged,True
49481,rank-0 gpu consume too high memory oncall: distributed triaged,2020-12-16 19:14:25+00:00,,0,3,oncall: distributed triaged,True
49480,Internal symbols are leaking from torch.nn.functional triaged,2020-12-16 19:11:44+00:00,,0,8,triaged,True
49477,Callbacks may not be automatically synchronized in a single NCCL future chain oncall: distributed triaged,2020-12-16 18:56:18+00:00,,0,1,oncall: distributed triaged,True
49476,Use METH_FASTCALL protocol in Python bindings feature triaged module: pybind,2020-12-16 18:29:47+00:00,,0,2,feature triaged module: pybind,True
49475,Write link in README.md module: docs triaged,2020-12-16 18:22:41+00:00,,0,1,module: docs triaged,True
49473,Pytorch 1.5+: Kernel dies / Segmentation Fault with torch.cuda.mermory_allocated() and torch.cuda.mermory_reserved() module: cuda triaged module: regression,2020-12-16 18:00:17+00:00,,0,0,module: cuda triaged module: regression,True
49460,Libtorch: Segmentation fault when running torch::jit::load oncall: jit weeks,2020-12-16 15:17:05+00:00,,0,24,oncall: jit weeks,True
49459, This error occurs occasionally during the run module: crash module: cudnn triaged,2020-12-16 13:42:46+00:00,,0,5,module: crash module: cudnn triaged,True
49456,Impossible to run tests target with LibTorch as dependency inside a cocoapods. triaged module: ios,2020-12-16 10:15:05+00:00,,0,3,triaged module: ios,True
49445,[doc] how to prevent pytorch-nightly from being replaced by a released version on pip install module: binaries module: docs oncall: releng triaged,2020-12-16 02:42:27+00:00,,0,8,module: binaries module: docs oncall: releng triaged,False
49444,[RFC] Add JIT graph fuser for oneDNN Graph API oncall: jit,2020-12-16 02:06:08+00:00,,1,12,oncall: jit,False
49440,[RFC] DataLoader architecture updates and TarDataset implementation feature module: dataloader triaged,2020-12-16 00:34:47+00:00,,0,50,feature module: dataloader triaged,True
49431,clip_grad_norm_ performance regression module: performance triaged module: norms and normalization,2020-12-15 22:09:04+00:00,,0,4,module: performance triaged module: norms and normalization,True
49427,Inserting named tensor into other fails triaged enhancement module: named tensor,2020-12-15 21:21:30+00:00,,0,4,triaged enhancement module: named tensor,True
49426,[doc] missing torch.pdist entry module: docs module: nn triaged small module: distance functions,2020-12-15 20:43:57+00:00,,0,4,module: docs module: nn triaged small module: distance functions,True
49416,A few years after #701 and PyTorch is still using implicit __all__ imports. module: typing triaged module: pybind better-engineering module: codegen,2020-12-15 19:06:05+00:00,,0,13,module: typing triaged module: pybind better-engineering module: codegen,True
49409,Add global gradcheck setting module: autograd triaged better-engineering,2020-12-15 17:48:57+00:00,,1,8,module: autograd triaged better-engineering,True
49406,Bitcode enable for iOS module: binaries triaged module: ios,2020-12-15 16:02:51+00:00,,1,13,module: binaries triaged module: ios,True
49405,PyTorch 1.7.1 on (macOS/python 3.9/conda) links libtorch_global_deps.dylib with libomp.dylib instead of libiomp5.dylib module: binaries module: build triaged module: macos,2020-12-15 15:57:37+00:00,,1,6,module: binaries module: build triaged module: macos,True
49398,Can we add try-except for list/slice indices even when auto_collation is True? triaged enhancement module: data,2020-12-15 07:31:12+00:00,,0,2,triaged enhancement module: data,True
49394,Apparent Memory Leak with torch.as_tensor module: memory usage triaged module: numpy quansight-nack module: tensor creation,2020-12-15 06:19:09+00:00,,0,23,module: memory usage triaged module: numpy quansight-nack module: tensor creation,True
49392,Uninitialized variable was not detected in ASAN CI config high priority module: ci triaged,2020-12-15 05:44:50+00:00,,0,0,high priority module: ci triaged,True
49377,[JIT-DEBUG] Suppress GRAPH dump in profile-graph-executor oncall: jit,2020-12-15 00:22:50+00:00,,1,0,oncall: jit,False
49372,[jit] Support `torch.Tensor.numel()` and `torch.Size.numel()` properly in tracing oncall: jit,2020-12-14 23:12:22+00:00,,1,5,oncall: jit,False
49370,"PyEval_SaveThread: the function must be called with the GIL held, but the GIL is released triaged module: pybind",2020-12-14 23:01:28+00:00,,0,5,triaged module: pybind,True
49347,[pre-commit hook] fails on `BLK100 Black would make changes.` oncall: distributed triaged,2020-12-14 19:24:23+00:00,,0,4,oncall: distributed triaged,True
49338,a time series library  feature triaged needs research,2020-12-14 17:43:45+00:00,,0,6,feature triaged needs research,False
49332,global_pruning costs memory after model is trained module: cuda module: memory usage triaged module: pruning,2020-12-14 15:47:04+00:00,,0,1,module: cuda module: memory usage triaged module: pruning,True
49321,Backward hangs with DDP during training. oncall: distributed triaged module: ddp,2020-12-14 10:17:27+00:00,,0,4,oncall: distributed triaged module: ddp,True
49320,how could I print the log in source code module: cpp module: logging triaged,2020-12-14 10:15:03+00:00,,0,5,module: cpp module: logging triaged,True
49300,is this a typo in optimizer.pyi ? it says `statue` instead of `state` module: optimizer triaged,2020-12-13 19:25:21+00:00,,0,0,module: optimizer triaged,True
49298,[question] How hard would it be to implement 4-bit precision training? module: internals triaged,2020-12-13 18:04:02+00:00,,0,5,module: internals triaged,True
49291,add inverse cdf for Chi-square Distribution  module: distributions feature triaged,2020-12-13 09:19:34+00:00,,0,2,module: distributions feature triaged,True
49284,TensorBoard SummaryWriter to remote storage unreasonably slow triaged module: tensorboard,2020-12-12 14:43:12+00:00,,0,0,triaged module: tensorboard,True
49282,Bug when combining AMP with channels_last memory format module: cuda triaged module: memory format module: amp (automated mixed precision),2020-12-12 10:20:39+00:00,,0,7,module: cuda triaged module: memory format module: amp (automated mixed precision),True
49278,Does Pytorch have the function that can obtain sub-matrix according to index?  feature triaged module: advanced indexing,2020-12-12 05:57:27+00:00,,0,1,feature triaged module: advanced indexing,False
49265,[FX] Update `placeholder` docs to better reflect the role of `node` and `target` triaged module: fx,2020-12-11 21:56:25+00:00,,0,0,triaged module: fx,True
49261,[TorchScipt] Custom Objects C++ API oncall: jit,2020-12-11 21:35:52+00:00,,0,1,oncall: jit,False
49253,Differentiate between objects with a true `Tensor` type and objects with a default `Tensor` type oncall: jit,2020-12-11 19:36:04+00:00,,1,2,oncall: jit,True
49252,Methods that solve systems of linear equations are memory-inefficient (batch-wise broadcasting) module: memory usage triaged module: linear algebra,2020-12-11 19:34:31+00:00,,0,3,module: memory usage triaged module: linear algebra,True
49227,Looking for a more convenient way to getter value of the upper/lower triangular matrix into 1D module: bc-breaking triaged module: numpy needs design topic: bc breaking,2020-12-11 10:03:48+00:00,,0,6,module: bc-breaking triaged module: numpy needs design topic: bc breaking,True
49224,Torch abs op crashes on -128 int8 tensor with ASAN module: error checking triaged,2020-12-11 09:18:37+00:00,,0,2,module: error checking triaged,True
49217,Bug in profiler on CI machines when profiling NCCL distributed calls oncall: distributed triaged module: c10d,2020-12-11 04:42:27+00:00,,0,0,oncall: distributed triaged module: c10d,True
49207,Provide a mechanism to limit the workspace size of cudnn convolution module: cudnn module: convolution triaged,2020-12-11 00:36:01+00:00,,0,3,module: cudnn module: convolution triaged,True
49180,torch.utils.data.DistributedSampler allow uneven inputs module: dataloader triaged enhancement module: data,2020-12-10 19:30:34+00:00,,0,2,module: dataloader triaged enhancement module: data,True
49178,[NNC] Some ops have type promotion logic which adds extra casts & does compute in different dtype than eager module: bootcamp triaged NNC,2020-12-10 19:05:59+00:00,,0,1,module: bootcamp triaged NNC,True
49177,Investigate `torch.linalg.norm` performance module: performance triaged module: linear algebra,2020-12-10 18:29:22+00:00,,1,2,module: performance triaged module: linear algebra,True
49166,Implement fill_out on complex triaged enhancement,2020-12-10 15:37:53+00:00,,0,0,triaged enhancement,True
49157,softmax fail needs reproduction oncall: jit module: autograd module: nn module: viewing and reshaping,2020-12-10 10:38:54+00:00,,1,4,needs reproduction oncall: jit module: autograd module: nn module: viewing and reshaping,True
49139,"Hello everyone, why does libtorch half precision float 16 leak memory?大家好，为什么libtorch半精度float16会内存泄漏？ module: memory usage triaged",2020-12-10 04:00:45+00:00,,0,11,module: memory usage triaged,True
49133,Unknown failure for test_reference_numerics_sinc_cpu_complex64  triaged module: complex module: trigonometric functions,2020-12-10 01:37:58+00:00,,0,1,triaged module: complex module: trigonometric functions,True
49110,[RFC] Integrate DeepSpeed CUDA kernels feature triaged,2020-12-09 20:52:17+00:00,,1,0,feature triaged,True
49101,[RFC] RemoteTensor: Use Remote Devices as Local Devices oncall: distributed feature triaged module: rpc,2020-12-09 18:52:11+00:00,,0,9,oncall: distributed feature triaged module: rpc,False
49095,NCCL Error when training with 2x 3090s. module: multi-gpu module: cuda triaged,2020-12-09 16:30:12+00:00,,0,27,module: multi-gpu module: cuda triaged,True
49082,swa_utils.bn_update is too opinionated in how it calls the model feature module: optimizer triaged,2020-12-09 11:54:17+00:00,,0,0,feature module: optimizer triaged,True
49076,Add List Type Data through add_hparam method in torch.utils.tensorboard.SummaryWriter feature triaged,2020-12-09 08:50:16+00:00,,0,1,feature triaged,True
49038,Make the  forward function of a nn.Module and/or a certain function to work with only float32 when used with autocast module: nn triaged module: amp (automated mixed precision),2020-12-08 20:53:59+00:00,,0,8,module: nn triaged module: amp (automated mixed precision),True
49023,Automatically rerun tests with CUDA_LAUNCH_BLOCKING=1 when they fail with CUDA errors in CI module: cuda module: tests triaged,2020-12-08 16:01:20+00:00,,0,4,module: cuda module: tests triaged,True
49016,Discrete Cosine Transform feature triaged,2020-12-08 13:09:19+00:00,,0,3,feature triaged,False
48998,Torch1.2 can't be downloaded! triaged,2020-12-08 03:49:01+00:00,,0,1,triaged,True
48984,Scripting silently ignores methods of class derived from `NamedTuple` oncall: jit,2020-12-08 01:21:36+00:00,,1,5,oncall: jit,False
48972,non-negative least squares solver feature request feature triaged,2020-12-08 00:16:57+00:00,,0,0,feature triaged,False
48959,[Poll] Support DistributedDataParallel (DDP) in PyTorch C++ API (libtorch) oncall: distributed feature triaged module: ddp,2020-12-07 23:01:32+00:00,,0,21,oncall: distributed feature triaged module: ddp,True
48945,CUDA error: illegal memory access Conv3d module: cudnn module: convolution triaged,2020-12-07 20:07:54+00:00,,0,1,module: cudnn module: convolution triaged,True
48932,Unknown builtin op: torchvision::nms when loading scripted FasterRCNN oncall: jit,2020-12-07 15:17:27+00:00,,0,11,oncall: jit,False
48929,Domain Specific Batch Normalization triaged enhancement,2020-12-07 13:32:29+00:00,,0,0,triaged enhancement,True
48928,"When multiple GPUs run multiple processes, it is found that any process not running in GPU 0 will have some more memory (such as 200m) in GPU 0. What is the cause of this?（多个GPU跑多进程时候，发现只要不在0号GPU跑的进程都会在0号GPU多出一些内存(如200M)，请问这是什么情况导致的？） oncall: distributed",2020-12-07 11:28:55+00:00,,0,8,oncall: distributed,True
48900,addmm with out= argument returns incorrect result high priority module: docs triaged module: partial aliasing module: linear algebra module: correctness (silent) module: structured kernels,2020-12-06 17:36:49+00:00,,0,9,high priority module: docs triaged module: partial aliasing module: linear algebra module: correctness (silent) module: structured kernels,True
48892,is_non_overlapping_and_dense() does not error out for sparse tensors  module: sparse triaged,2020-12-05 21:58:24+00:00,,0,4,module: sparse triaged,True
48878,Libtorch:  nvrtc: error: invalid value for --gpu-architecture (-arch) triaged,2020-12-05 05:46:54+00:00,,0,2,triaged,True
48869,Tracing fails sanity check  oncall: jit days,2020-12-04 23:14:25+00:00,,1,1,oncall: jit days,False
48845,memory_format kwarg doesn't work on most factory functions module: bootcamp triaged enhancement module: memory format,2020-12-04 18:05:39+00:00,,0,1,module: bootcamp triaged enhancement module: memory format,True
48832,cudnn cannot be pickled by cloudpickle module: cudnn module: cuda module: serialization triaged,2020-12-04 10:36:35+00:00,,0,7,module: cudnn module: cuda module: serialization triaged,True
48830,"Please add ""dim"" feature for function ""torch.masked_select"" triaged module: numpy needs design module: sorting and selection function request",2020-12-04 09:13:17+00:00,,0,8,triaged module: numpy needs design module: sorting and selection function request,False
48813,inception_v3 is not symbolically traceable triaged module: fx,2020-12-03 23:26:42+00:00,,0,0,triaged module: fx,True
48793,Weight_decay in torch.Adam module: docs module: optimizer triaged,2020-12-03 17:06:20+00:00,,0,5,module: docs module: optimizer triaged,True
48747,[bug][libtorch] torch::load seeks to the start of the input stream before reading module: serialization triaged needs design,2020-12-02 21:51:43+00:00,,0,1,module: serialization triaged needs design,True
48714,RTX 3090 setup vs 2x RTX 2080TI setup slower? Help.. module: performance triaged module: data parallel,2020-12-02 15:02:35+00:00,,0,17,module: performance triaged module: data parallel,True
48710,RuntimeError: CUDNN_STATUS_BAD_PARAM when testing environment oncall: distributed,2020-12-02 12:17:48+00:00,,0,2,oncall: distributed,False
48706,test_inverse setup is flaky using MKL>=2020.1 on certain CPUs and fails on CUDA module: cuda triaged module: mkl shadow review,2020-12-02 10:57:28+00:00,,0,4,module: cuda triaged module: mkl shadow review,True
48703,Caffe2 upsample do not support float times output_size oncall: mobile,2020-12-02 08:59:53+00:00,,0,0,oncall: mobile,False
48702,provide example for distributed training with iterative dataloaders  oncall: distributed module: dataloader triaged,2020-12-02 08:17:04+00:00,,0,2,oncall: distributed module: dataloader triaged,True
48684,Remove remaining native:: references from non-native ATen module: internals triaged,2020-12-02 01:49:36+00:00,,0,0,module: internals triaged,True
48667,Operator registration doesn't work with noexcept functions on some compilers (c10::guts::is_function_type rejects noexcept) module: internals triaged,2020-12-01 20:44:53+00:00,,0,0,module: internals triaged,True
48650,[feature request] Keyword-only device argument (and maybe dtype) for torch.meshgrid module: internals feature triaged,2020-12-01 13:17:40+00:00,,0,0,module: internals feature triaged,False
48649,[docs] A single additional summary page for diag* methods module: docs triaged,2020-12-01 11:59:14+00:00,,0,1,module: docs triaged,False
48648,Confused on project website pointer. module: docs triaged,2020-12-01 11:11:02+00:00,,0,1,module: docs triaged,True
48645,Can sublist a nn.Sequential subclass module: nn triaged,2020-12-01 08:20:22+00:00,,0,5,module: nn triaged,True
48641,"[bug] `torch.{sinh, cosh}`: Incorrect values for vectorized path module: cpu triaged module: sleef module: NaNs and Infs module: vectorization module: trigonometric functions",2020-12-01 06:27:32+00:00,,0,5,module: cpu triaged module: sleef module: NaNs and Infs module: vectorization module: trigonometric functions,True
48628,Add docs on PyTorch - NumPy interaction module: docs triaged module: numpy,2020-12-01 00:04:53+00:00,,1,3,module: docs triaged module: numpy,False
48626,Missing super().__init__() call in nn.Module module: nn triaged,2020-11-30 23:28:24+00:00,,0,4,module: nn triaged,True
48621,[FR] loss reduction argument accepts None module: loss triaged,2020-11-30 22:26:38+00:00,,0,0,module: loss triaged,True
48605,[NNC] Bugs Exposed in Binary Op Testing module: bootcamp triaged NNC,2020-11-30 19:24:27+00:00,,2,3,module: bootcamp triaged NNC,True
48587,[feature request] Give `torch.cholesky` an optional fallback to test whether the tensor is positive definite triaged module: linear algebra,2020-11-30 15:13:56+00:00,,0,1,triaged module: linear algebra,True
48580,torch.save() fails when attempting to save to mounted drives needs reproduction module: serialization triaged,2020-11-30 13:25:57+00:00,,0,8,needs reproduction module: serialization triaged,True
48579,test_fs_pool fails needs reproduction module: multiprocessing module: tests triaged,2020-11-30 12:37:24+00:00,,0,3,needs reproduction module: multiprocessing module: tests triaged,True
48559,[FR] torch.load should support loading directly to pinned/shared memory high priority module: multiprocessing feature module: serialization triaged,2020-11-29 16:46:07+00:00,,0,2,high priority module: multiprocessing feature module: serialization triaged,True
48558,[FR] tensor ctors should support directly creating in shared memory high priority feature triaged module: tensor creation,2020-11-29 16:41:33+00:00,,0,3,high priority feature triaged module: tensor creation,True
48555,segment fault for Image model training by four GPUs needs reproduction module: crash triaged,2020-11-29 10:22:48+00:00,,0,5,needs reproduction module: crash triaged,True
48549,JIT script has wrong signature for .norm oncall: jit days TSUsability TSRootCause:PyTorchParityGap,2020-11-29 06:57:42+00:00,,1,2,oncall: jit days TSUsability TSRootCause:PyTorchParityGap,False
48548,at::size documentation conflict module: docs module: cpp triaged,2020-11-29 04:13:14+00:00,,0,2,module: docs module: cpp triaged,True
48526,CARU: A Content-Adaptive Recurrent Unit for the Transition of Hidden State in NLP feature module: nn module: rnn triaged,2020-11-28 04:04:11+00:00,,0,1,feature module: nn module: rnn triaged,True
48525,TorchScript input tensor size oncall: jit,2020-11-28 03:03:46+00:00,,2,1,oncall: jit,False
48518,C++ optimizer check for duplicate parameters module: optimizer triaged,2020-11-27 14:57:55+00:00,,0,0,module: optimizer triaged,True
48511,Ambiguous RuntimeError raised when device of input PackedSequence does not match with nn.LSTM's device module: nn module: rnn triaged,2020-11-27 06:50:36+00:00,,0,1,module: nn module: rnn triaged,True
48494,Inconsistent type of property stride of Conv1d and MaxPool1d module: nn triaged needs design module: ux,2020-11-26 17:21:49+00:00,,0,2,module: nn triaged needs design module: ux,True
48491,SegmentationFault when pytorch is installed from source. module: crash module: build triaged,2020-11-26 15:55:55+00:00,,0,4,module: crash module: build triaged,True
48486,[complex] torch.abs: does not match numpy  triaged module: complex module: numpy module: NaNs and Infs,2020-11-26 10:34:18+00:00,,1,1,triaged module: complex module: numpy module: NaNs and Infs,True
48485,Add wheels for all cuda versions on pypi module: binaries triaged,2020-11-26 10:34:15+00:00,,0,1,module: binaries triaged,True
48483,Vulkan Api Backend Build Error with GCC module: build triaged,2020-11-26 09:47:47+00:00,,0,6,module: build triaged,True
48478,Callbacks of Futures shouldn't wait inline on another future oncall: distributed module: ddp,2020-11-26 08:50:00+00:00,,0,1,oncall: distributed module: ddp,False
48466,[master] new default signature: op=<ReduceOp.SUM: 0>from `op=<ReduceOp.SUM> oncall: distributed module: docs triaged,2020-11-26 00:12:30+00:00,,0,1,oncall: distributed module: docs triaged,True
48465,anchored direct links to functions disappeared in 1.7.0 docs module: docs triaged module: doc infra,2020-11-25 23:58:54+00:00,,0,4,module: docs triaged module: doc infra,False
48457,compile error immintrin.h module: build triaged,2020-11-25 17:34:00+00:00,,0,4,module: build triaged,True
48452,After updating to Xcode 12 and LibTorch to 1.7.0. Facing issue when running unit test. oncall: jit oncall: mobile module: ios module: arm,2020-11-25 12:34:16+00:00,,1,8,oncall: jit oncall: mobile module: ios module: arm,True
48450,"Qt 5.14.2, libtorch1.7.0 cuda10.2，Error after using header file triaged module: static linking",2020-11-25 11:25:42+00:00,,0,3,triaged module: static linking,True
48439,cudnn convolution modifies the input Tensor metadata inplace when it tries to `.resize_()` it module: cudnn module: convolution triaged module: memory format,2020-11-25 07:16:16+00:00,,1,10,module: cudnn module: convolution triaged module: memory format,True
48419,pin_memory=True in DataLoader converts a tuple to list automatically module: dataloader triaged,2020-11-24 13:08:51+00:00,,0,1,module: dataloader triaged,True
48410,PyTorch Mobile speed_benchmark_torch crashes on mobile optimized model oncall: mobile,2020-11-24 04:33:08+00:00,,0,0,oncall: mobile,False
48408,TorchScript: Optional sized lists cannot be called with optional scalar values oncall: jit days,2020-11-24 01:40:47+00:00,,1,0,oncall: jit days,False
48400,conv_transpose3d returns different result when the input and kernel are mkldnn tensors high priority module: dependency bug module: convolution triaged module: mkldnn,2020-11-23 19:00:17+00:00,,1,2,high priority module: dependency bug module: convolution triaged module: mkldnn,True
48389,"Add parameter ""half_pixel_center =False"" to the Bilinear function triaged enhancement module: interpolation",2020-11-23 11:49:35+00:00,,0,2,triaged enhancement module: interpolation,False
48387,"Error occurred when trying to call ""torch::jit::load""method for loading "".pt"" file on IOS oncall: jit oncall: mobile",2020-11-23 10:55:55+00:00,,0,8,oncall: jit oncall: mobile,True
48382,hangs indefinitely at os.waitpid() high priority module: docs module: multiprocessing module: error checking triaged module: deadlock,2020-11-23 02:25:34+00:00,,0,8,high priority module: docs module: multiprocessing module: error checking triaged module: deadlock,True
48377,Management of Gpu memory to avoid memory errors feature module: memory usage triaged,2020-11-22 16:18:06+00:00,,0,3,feature module: memory usage triaged,True
48366,Add Go to high level interface feature triaged,2020-11-21 19:39:21+00:00,,0,1,feature triaged,True
48365,Custom exception for out of memory high priority feature triaged better-engineering,2020-11-21 16:40:09+00:00,,1,7,high priority feature triaged better-engineering,True
48353,[FR] bool tensor should support basic arithmetics triaged module: numpy module: type promotion module: boolean tensor,2020-11-21 02:37:10+00:00,,0,4,triaged module: numpy module: type promotion module: boolean tensor,True
48338,torch.float128 datatype feature triaged module: numpy,2020-11-20 22:35:44+00:00,,0,1,feature triaged module: numpy,True
48325,[JIT] Constant Propagation Shouldn't Run on Values that Escape the Graph oncall: jit days,2020-11-20 19:16:22+00:00,,1,1,oncall: jit days,True
48307,Provide torchscript support for staticmethods for classes derived from Enum oncall: jit,2020-11-20 13:18:00+00:00,,1,1,oncall: jit,False
48306,[doc] example for pairwise distance matrix module: docs module: nn triaged module: distance functions,2020-11-20 11:33:27+00:00,,0,4,module: docs module: nn triaged module: distance functions,False
48305,[RFC] CUDA-aware future for distributed oncall: distributed triaged module: rpc module: c10d,2020-11-20 10:00:06+00:00,,0,3,oncall: distributed triaged module: rpc module: c10d,True
48291,pytorch is not linked with support for cuda devices module: build module: cuda triaged,2020-11-20 02:45:08+00:00,,0,3,module: build module: cuda triaged,True
48287,"[doc] Error ""You've reached a dead end"" when opening torch.__config__ in the docs module: docs triaged module: doc infra",2020-11-20 01:18:54+00:00,,0,9,module: docs triaged module: doc infra,True
48281,Batched inplace mm changes stride when out size is correct triaged module: memory format module: linear algebra module: correctness (silent),2020-11-20 00:00:59+00:00,,0,6,triaged module: memory format module: linear algebra module: correctness (silent),True
48279,"Pytorch streams API don't execute concurrently, However Same code in CUDA does.  module: performance module: cuda triaged",2020-11-19 23:55:16+00:00,,0,12,module: performance module: cuda triaged,True
48273,torch.multinomial example is incorrect module: distributions module: docs triaged,2020-11-19 21:01:12+00:00,,0,5,module: distributions module: docs triaged,True
48251,torch.eye(d) is slow and hogs cpu for d >= 182 module: performance good first issue module: cpu triaged module: multithreading,2020-11-19 10:34:50+00:00,,0,7,module: performance good first issue module: cpu triaged module: multithreading,True
48246,[RFC] XPU device for PyTorch feature triaged module: intel,2020-11-19 08:36:34+00:00,,0,16,feature triaged module: intel,False
48245,"RuntimeError: ""threshold_cpu"" not implemented for 'Half' module: cpu triaged enhancement module: half",2020-11-19 08:32:51+00:00,,0,5,module: cpu triaged enhancement module: half,True
48243,torch.save writes file even when pickle fails module: serialization triaged,2020-11-19 07:14:06+00:00,,0,3,module: serialization triaged,True
48235,Softplus differs depending on number of elements in tensor module: nn triaged,2020-11-19 04:33:32+00:00,,0,3,module: nn triaged,True
48227,Modifying values() tensor of COO tensor requiring grad throws an odd error message module: sparse module: autograd triaged,2020-11-19 00:35:46+00:00,,0,2,module: sparse module: autograd triaged,True
48209,[FX] Write an inference optimization pass triaged module: fx,2020-11-18 22:30:07+00:00,,0,0,triaged module: fx,False
48203,Document torch.distributed.destroy_process_group() oncall: distributed module: docs triaged module: c10d,2020-11-18 21:27:45+00:00,,0,2,oncall: distributed module: docs triaged module: c10d,True
48202,"""Found no NVIDIA driver"" produces too much log module: cuda triaged",2020-11-18 21:13:13+00:00,,0,5,module: cuda triaged,True
48191,[JIT] Contained Module Attributes should be recursively compiled oncall: jit,2020-11-18 18:32:10+00:00,,1,1,oncall: jit,True
48177,Support for oneDNN / MKL-DNN on AArch64 triaged module: mkldnn module: arm,2020-11-18 15:06:02+00:00,,0,5,triaged module: mkldnn module: arm,True
48167,[libtorch] Build shared library with libtorch and compile a static library triaged module: static linking,2020-11-18 09:04:29+00:00,,0,1,triaged module: static linking,True
48163,nn.functional.interpolate backward in fp16 is extremely slow module: performance module: nn module: cuda triaged module: half module: interpolation,2020-11-18 07:04:32+00:00,,0,5,module: performance module: nn module: cuda triaged module: half module: interpolation,True
48153,More robust list comprehension  oncall: jit triaged weeks,2020-11-18 03:47:32+00:00,,1,2,oncall: jit triaged weeks,True
48108,JIT should not force users to write ugly code oncall: jit TSUsability,2020-11-17 18:12:42+00:00,,0,12,oncall: jit TSUsability,False
48106,quantization - document get_default_qconfig oncall: quantization triaged,2020-11-17 17:28:06+00:00,,1,3,oncall: quantization triaged,True
48094,[JIT][PE] Profiling Executor should re-profile `DifferentiableGraph` oncall: jit,2020-11-17 13:37:36+00:00,,0,0,oncall: jit,True
48090,[JIT] Constant folding on no-op `aten::mul` oncall: jit,2020-11-17 12:47:02+00:00,,0,0,oncall: jit,True
48089,[JIT] Optimization pass in profiling executor to fold away conditional branches oncall: jit,2020-11-17 12:22:31+00:00,,0,0,oncall: jit,False
48088,[dataloader] Worker threads to print the signal they received before they die module: dataloader triaged enhancement,2020-11-17 11:11:08+00:00,,0,0,module: dataloader triaged enhancement,True
48083,CPU Tensor with Python MP Freezing in Docker Container module: multiprocessing triaged module: docker,2020-11-17 08:12:36+00:00,,0,2,module: multiprocessing triaged module: docker,True
48082,Microsoft C++ exception: torch::jit::ErrorReport at memory location 0x0000001C0C3BBB40. oncall: jit,2020-11-17 06:51:03+00:00,,0,13,oncall: jit,False
48066,[JIT] Saving and Loading JIT Model Does Not Preserve __dir__ oncall: jit days,2020-11-17 00:11:20+00:00,,0,2,oncall: jit days,True
48064,FAILED: test_api/CMakeFiles/test_api.dir/dataloader.cpp.o  module: build triaged,2020-11-17 00:06:34+00:00,,0,4,module: build triaged,True
48054,RuntimeError: NYI: Named tensors are not supported with the tracer oncall: jit,2020-11-16 22:34:04+00:00,,0,1,oncall: jit,False
48045,"[NNC] Improve ""UNSUPPORTED DTYPE"" error messages  oncall: jit module: bootcamp NNC",2020-11-16 21:23:55+00:00,,0,0,oncall: jit module: bootcamp NNC,True
48033,Missing -fopenmp when used in torchvision module: build module: cpp-extensions triaged enhancement module: build warnings module: openmp,2020-11-16 19:57:36+00:00,,0,1,module: build module: cpp-extensions triaged enhancement module: build warnings module: openmp,True
48010,[complex] torch.{exp}: does not match numpy triaged module: complex module: numpy module: correctness (silent),2020-11-16 09:22:17+00:00,,1,6,triaged module: complex module: numpy module: correctness (silent),True
48000,Inconsistent complex results with NumPy when computing non-positive power of 0 triaged module: complex module: numpy module: NaNs and Infs,2020-11-16 06:19:48+00:00,,0,2,triaged module: complex module: numpy module: NaNs and Infs,True
47999,Torch DDP seems not prefer IPv6 over IPv4? high priority triage review oncall: distributed triaged,2020-11-16 05:21:11+00:00,,1,6,high priority triage review oncall: distributed triaged,True
47996,GPU Vendor-Agnosticism via Vulkan feature module: cuda module: rocm triaged module: vulkan,2020-11-16 03:19:15+00:00,,0,0,feature module: cuda module: rocm triaged module: vulkan,True
47993,"build pytorch from source on ubuntu, building error from fbgemm::SparseAdaGradSignature module: build module: collect_env.py triaged",2020-11-16 01:00:12+00:00,,0,31,module: build module: collect_env.py triaged,True
47990,Is nn.Conv2d equivalent with Unfold + Matrix Multiplication + Fold ? needs reproduction module: convolution triaged module: correctness (silent),2020-11-15 22:12:00+00:00,,0,10,needs reproduction module: convolution triaged module: correctness (silent),True
47988,Support calls to super().function() by implementing (poor man's) inheritance  oncall: jit TSRootCause:DynamicBehaviors,2020-11-15 21:19:58+00:00,,1,4,oncall: jit TSRootCause:DynamicBehaviors,False
47964,TorchScript is unable to distinguish int and ScalarType oncall: jit weeks TSRootCause:DefaultTypes TSUsability,2020-11-14 06:29:32+00:00,,1,14,oncall: jit weeks TSRootCause:DefaultTypes TSUsability,False
47954,[Feature] torch.bitwise_where triaged enhancement shadow review,2020-11-14 01:10:29+00:00,,0,3,triaged enhancement shadow review,False
47953,Linear algebra GPU backend tracking issue [magma/cusolver/cublas] high priority module: performance module: cuda triaged module: linear algebra,2020-11-14 00:51:02+00:00,,0,9,high priority module: performance module: cuda triaged module: linear algebra,True
47931,Unknown DispatchKey when indexing tensor with named dimensions triaged module: dispatch module: named tensor,2020-11-13 19:52:22+00:00,,0,1,triaged module: dispatch module: named tensor,True
47918,Function request: Sparse matrix inverse module: sparse triaged module: linear algebra function request,2020-11-13 16:22:49+00:00,,0,5,module: sparse triaged module: linear algebra function request,True
47917,LibTorch cannot load PyTorch exported model module: docs module: serialization triaged,2020-11-13 15:45:40+00:00,,0,15,module: docs module: serialization triaged,True
47916,Hash mismatch for METADATA file module: binaries triaged,2020-11-13 15:36:05+00:00,,0,3,module: binaries triaged,True
47915,sparse filter layers (more specifically convolutions) module: sparse triaged needs research function request,2020-11-13 15:17:20+00:00,,0,1,module: sparse triaged needs research function request,True
47912,Code becomes more than x20 slower after upgrading torch version module: performance triaged,2020-11-13 14:18:50+00:00,,0,3,module: performance triaged,True
47908,The speed of pytorch with cudatoolkit 11.0 is slower than cudatoolkit 10.2 module: performance module: cuda triaged,2020-11-13 08:38:08+00:00,,0,54,module: performance module: cuda triaged,True
47906,amp for custom op triaged module: custom-operators module: amp (automated mixed precision),2020-11-13 07:32:15+00:00,,0,4,triaged module: custom-operators module: amp (automated mixed precision),True
47904,Memory leak in nn.MaxPool2d layer when run on iOS module: memory usage triaged module: ios,2020-11-13 06:34:39+00:00,,0,0,module: memory usage triaged module: ios,True
47901,"Get forward output, grad, hessian all at once module: autograd triaged enhancement",2020-11-13 06:22:02+00:00,,0,4,module: autograd triaged enhancement,True
47887,"ONNX export of a scripted submodule fails with ""Modules that are called during a trace must be registered as submodules of the thing being traced"" oncall: jit module: onnx onnx-triaged",2020-11-13 01:09:04+00:00,,0,19,oncall: jit module: onnx onnx-triaged,True
47885,NCCL 2.7.8 errors on PyTorch distributed process group creation oncall: distributed triaged,2020-11-12 22:40:55+00:00,,0,23,oncall: distributed triaged,True
47883,`.modules()` is not callable in TorchScript when any submodule is a module interface type (i.e. a class decorated by `@torch.jit.interface`) oncall: jit,2020-11-12 22:23:32+00:00,,0,2,oncall: jit,True
47857,QAT with DDP should have documentation oncall: quantization triaged,2020-11-12 16:29:12+00:00,,1,3,oncall: quantization triaged,True
47844,Incorrect output loss value under specific CUDA version high priority module: dependency bug module: numerical-stability module: cuda triaged module: linear algebra module: tf32,2020-11-12 09:32:46+00:00,,0,10,high priority module: dependency bug module: numerical-stability module: cuda triaged module: linear algebra module: tf32,True
47841,Factorial & Binomial Coefficient triaged module: numpy function request,2020-11-12 08:37:37+00:00,,0,1,triaged module: numpy function request,True
47832,Error during distributed training needs reproduction oncall: distributed triaged,2020-11-12 06:02:33+00:00,,0,3,needs reproduction oncall: distributed triaged,True
47824,"DDP error, when I use ""if"" branch in forward function. module: autograd triaged module: ddp",2020-11-12 03:30:26+00:00,,0,3,module: autograd triaged module: ddp,True
47778,Eliminate redundant device guards in generic dispatch key kernel wrappers module: performance triaged module: dispatch,2020-11-11 20:33:22+00:00,,0,0,module: performance triaged module: dispatch,True
47753," Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2 module: serialization triaged",2020-11-11 15:09:41+00:00,,0,9,module: serialization triaged,True
47751,Make libtorch modular module: binaries module: build triaged module: selective build,2020-11-11 13:52:28+00:00,,0,3,module: binaries module: build triaged module: selective build,True
47743,Cudnn header files should be copied into build package as well  module: windows triaged windows-triaged,2020-11-11 09:44:45+00:00,,0,27,module: windows triaged windows-triaged,True
47739,Conda package should install libtorch in the standard location module: binaries module: build triaged,2020-11-11 08:44:30+00:00,,0,0,module: binaries module: build triaged,True
47702,GPU acceleration for Apple's M1 chip? module: performance triaged,2020-11-10 22:05:54+00:00,,0,183,module: performance triaged,True
47699,DCNv2 Layers module: nn triaged function request,2020-11-10 20:44:45+00:00,,0,3,module: nn triaged function request,True
47692,pyre not able to infer the type of torch.jit.script oncall: jit,2020-11-10 19:32:04+00:00,,1,2,oncall: jit,True
47688,ARM Mac 16-core Neural Engine feature triaged,2020-11-10 19:00:32+00:00,,0,18,feature triaged,True
47682,Common dunder methods fail when called directly oncall: jit days,2020-11-10 17:07:35+00:00,,1,1,oncall: jit days,False
47681,How to install Pytorch on AIX7.2 without internet access? module: build triaged,2020-11-10 17:02:24+00:00,,0,1,module: build triaged,True
47669,Pytorch 1.7.0 with cuda 11.1.1 and cudnn 8.0.5 module: cuda oncall: releng module: ci triaged enhancement,2020-11-10 12:23:20+00:00,,0,20,module: cuda oncall: releng module: ci triaged enhancement,True
47650,DataParallel support for scripted modules in C++ oncall: jit triaged,2020-11-10 04:46:46+00:00,,1,2,oncall: jit triaged,True
47649,error: reference to __host__ function 'parallel_for<thrust::cuda_cub::for_each_f...' in __host__ __device__ function module: build module: cuda triaged,2020-11-10 03:50:43+00:00,,0,3,module: build module: cuda triaged,True
47627,Allow add_embedding to have dict for metadata triaged module: tensorboard,2020-11-09 21:44:09+00:00,,0,1,triaged module: tensorboard,True
47610,Input dimension check for `torch.gather` module: error checking triaged,2020-11-09 18:15:47+00:00,,0,1,module: error checking triaged,True
47609,Error while trying to load a pretrained ResNet34 VTN (Video Transformer Network) model in Android triaged oncall: mobile,2020-11-09 17:45:32+00:00,,0,0,triaged oncall: mobile,True
47591,PyTorch1.3.1 Can not using namespace torch::indexing module: cpp triaged,2020-11-09 06:42:52+00:00,,0,4,module: cpp triaged,True
47588,Can we apply Weight normalization hook to a method other than `forward`? module: nn triaged enhancement module: norms and normalization,2020-11-09 03:14:04+00:00,,0,1,module: nn triaged enhancement module: norms and normalization,True
47582,torch.nn.Module.apply cyclical references unbounded module: nn triaged,2020-11-08 20:08:21+00:00,,0,4,module: nn triaged,True
47568,Failed to get generated_cpp list module: build triaged,2020-11-08 07:59:23+00:00,,0,3,module: build triaged,True
47565,Keyword-only function not allowed with TorchScript oncall: jit module: bootcamp OSS contribution wanted days,2020-11-08 01:32:45+00:00,,1,2,oncall: jit module: bootcamp OSS contribution wanted days,False
47563,Handling multiple large-scale datasets efficiently  module: dataloader triaged,2020-11-07 18:58:00+00:00,,0,5,module: dataloader triaged,True
47560,Minumul LR is never reached in  module: optimizer triaged,2020-11-07 15:39:37+00:00,,0,1,module: optimizer triaged,True
47557,Ninja-built CUDAExtension build process ignores changed #include dependencies module: build triaged,2020-11-07 12:53:22+00:00,,0,0,module: build triaged,True
47546,Whats Pytorch's policy on adding support for a wider range of hardwares for training and inference?  triaged module: backend,2020-11-07 04:39:28+00:00,,0,3,triaged module: backend,False
47535,per channel observer to work for weights of groupwise conv transpose oncall: quantization low priority triaged,2020-11-06 23:32:41+00:00,,0,4,oncall: quantization low priority triaged,True
47513,ReduceLROnPlateau fails for negative input module: docs module: optimizer triaged,2020-11-06 19:35:25+00:00,,0,5,module: docs module: optimizer triaged,True
47502,Hackable python-only autograd engine module: autograd triaged,2020-11-06 16:33:23+00:00,,0,6,module: autograd triaged,True
47481,[fx] scripting a model with tensor list as input fails triaged module: fx,2020-11-06 00:55:11+00:00,,0,1,triaged module: fx,True
47476,Reciprocal and reciprocal square root instructions are too inaccurate on ARM64 triaged module: correctness (silent) module: arm,2020-11-06 00:20:41+00:00,,0,2,triaged module: correctness (silent) module: arm,True
47447,"thread blocked when moving a tensor from GPU to CPU, by calling the function .cpu() in pytorch. This kind of block can be stop by any window event like mouse moving/clicking or keyboard pressing. module: cuda module: cpu triaged",2020-11-05 16:46:46+00:00,,0,0,module: cuda module: cpu triaged,True
47439,how to use torch.utils.checkpoint + gru with variable length sequence? module: rnn triaged,2020-11-05 13:25:06+00:00,,0,3,module: rnn triaged,True
47433,unbalanced gpu memory when using DistributedDataParallel oncall: distributed triaged,2020-11-05 08:22:38+00:00,,0,5,oncall: distributed triaged,True
47422,[Discussion] Use the unicode variant of the Windows API module: windows module: internals triaged,2020-11-05 04:11:50+00:00,,0,21,module: windows module: internals triaged,True
47378,torch.vmap giving INTERNAL ASSERT FAILED error triaged module: vmap,2020-11-04 19:08:19+00:00,,0,13,triaged module: vmap,True
47363,FX quantization: we should preserve original model class name through the quantization passes oncall: quantization low priority triaged,2020-11-04 17:20:27+00:00,,1,7,oncall: quantization low priority triaged,True
47357,Conv1D formula in docs is wrong module: docs module: nn triaged,2020-11-04 14:01:40+00:00,,0,5,module: docs module: nn triaged,True
47351,no member named 'beta1' in 'torch::optim::AdamOptions' module: docs module: optimizer triaged actionable,2020-11-04 09:32:49+00:00,,0,5,module: docs module: optimizer triaged actionable,False
47342,My demo app uses other package name will crash but org.pytorch.demo oncall: mobile,2020-11-04 06:47:05+00:00,,0,2,oncall: mobile,True
47341,ConvTranspose1d groups=channels is very slow!!! module: cudnn module: convolution triaged,2020-11-04 06:41:49+00:00,,0,5,module: cudnn module: convolution triaged,True
47336,[jit] Properly support slicing of ModuleList oncall: jit,2020-11-04 03:42:11+00:00,,0,2,oncall: jit,False
47335,[jit] support length inference for list comprehension oncall: jit,2020-11-04 03:36:34+00:00,,0,2,oncall: jit,False
47311,Quantized modules should properly implement __getstate__ and __setstate__ (copy.deepcopy doesn't work on quantized model) oncall: quantization triaged,2020-11-03 20:30:45+00:00,,0,5,oncall: quantization triaged,True
47307,[Feature request] Stochastic Frank-Wolfe optimizer feature module: optimizer triaged,2020-11-03 20:05:00+00:00,,0,1,feature module: optimizer triaged,False
47291,ProcessGroupNCCL NCCL lib version mismatch module: binaries oncall: distributed triaged,2020-11-03 17:09:47+00:00,,0,7,module: binaries oncall: distributed triaged,True
47281,Error: invalid use of register module: build triaged,2020-11-03 15:18:51+00:00,,0,4,module: build triaged,True
47265,[Feature Request] tensorboard add_graph support for multiple method oncall: visualization,2020-11-03 07:13:10+00:00,,0,0,oncall: visualization,False
47260,DDP doesn't work with retain_graph = True high priority triage review oncall: distributed triaged,2020-11-03 04:03:44+00:00,,1,6,high priority triage review oncall: distributed triaged,True
47218,[JIT] Alias Annotations for Native Schema List Ops Is Subtly Wrong high priority oncall: jit days weeks,2020-11-02 19:52:37+00:00,,0,2,high priority oncall: jit days weeks,True
47217,RuntimeError: Exporting the operator quantize_per_tensor to ONNX opset version 10 is not supported. module: onnx feature triaged onnx-triaged onnx-needs-info,2020-11-02 19:50:31+00:00,,0,33,module: onnx feature triaged onnx-triaged onnx-needs-info,False
47208,[RFC] Add test execution time analysis CI workflow module: ci triaged,2020-11-02 18:39:18+00:00,,0,0,module: ci triaged,True
47201,torch.save has a maximum size regardless of RAM module: docs module: serialization triaged function request,2020-11-02 17:49:56+00:00,,0,3,module: docs module: serialization triaged function request,True
47177,LayerNorm on Android will cause very high cpu usage oncall: mobile,2020-11-02 02:05:37+00:00,,0,0,oncall: mobile,False
47175,[feature request] Show warning if optimizer.zero_grad() was not called before optimizer.step() module: nn module: optimizer triaged module: ux,2020-11-01 21:00:24+00:00,,0,4,module: nn module: optimizer triaged module: ux,True
47163,Grad strides do not match bucket view strides. oncall: distributed module: memory format module: ddp,2020-11-01 09:48:49+00:00,,0,14,oncall: distributed module: memory format module: ddp,True
47162,Custon C++ classes are not traceable  oncall: jit weeks,2020-11-01 05:57:41+00:00,,1,1,oncall: jit weeks,False
47161,segfault during shutdown with torch1.7 module: dependency bug module: crash triaged module: pybind module: third_party,2020-10-31 21:50:33+00:00,,0,3,module: dependency bug module: crash triaged module: pybind module: third_party,True
47149,Allow all torch.nn modules to accept arbitrary batch dimensions module: nn triaged enhancement module: batching module: ux,2020-10-31 07:41:52+00:00,,0,6,module: nn triaged enhancement module: batching module: ux,True
47128,AMP doesn't gracefully handle optimizers for disabled regions triaged module: amp (automated mixed precision),2020-10-30 18:38:28+00:00,,0,4,triaged module: amp (automated mixed precision),False
47117,ctx.save_for_backward doesn't save torch.Tensor subclasses fully module: docs module: autograd triaged enhancement needs design module: __torch_function__,2020-10-30 14:43:00+00:00,,0,26,module: docs module: autograd triaged enhancement needs design module: __torch_function__,True
47116,trying to Installing pytorch on python3.9 via pip results in an non-descript error high priority module: binaries oncall: releng triaged,2020-10-30 13:04:51+00:00,,0,13,high priority module: binaries oncall: releng triaged,True
47111,THCudaShutdown should be called before THCState_free module: cuda module: memory usage triaged,2020-10-30 09:25:13+00:00,,0,0,module: cuda module: memory usage triaged,True
47109,Pytorch 1.7/Cuda 11.1 binaries module: binaries module: cuda triaged,2020-10-30 08:43:01+00:00,,0,22,module: binaries module: cuda triaged,True
47056,Training slows down and memory usage increases when upgrading from PyTorch 1.6 to 1.7 module: performance triaged module: regression module: amp (automated mixed precision),2020-10-29 15:55:44+00:00,,0,15,module: performance triaged module: regression module: amp (automated mixed precision),True
47055,"[types] torch.utils.data.{Dataset, Sampler} are not Sized triaged enhancement needs research module: data",2020-10-29 15:27:53+00:00,,0,18,triaged enhancement needs research module: data,True
47052,Activation functions for complex tensors module: nn triaged module: complex function request,2020-10-29 14:55:35+00:00,,0,20,module: nn triaged module: complex function request,True
47051,Tensor subclasses lose type when pickling module: serialization triaged enhancement module: __torch_function__,2020-10-29 13:45:54+00:00,,0,7,module: serialization triaged enhancement module: __torch_function__,True
47037,"Change ""_next_index()"" in DataLoader to a public and stable API module: dataloader triaged needs research",2020-10-29 02:39:22+00:00,,0,0,module: dataloader triaged needs research,True
47028,torch.autograd.backward() fails to sync with other stream module: autograd triaged,2020-10-28 22:51:49+00:00,,1,7,module: autograd triaged,True
47027,Fix the way imports are done to be more correct for static type checkers module: typing triaged enhancement,2020-10-28 22:43:42+00:00,,1,31,module: typing triaged enhancement,True
47010,[JIT] Tracing turns None Device Input into a Constant oncall: jit weeks,2020-10-28 19:42:15+00:00,,0,0,oncall: jit weeks,False
46999,"torch.sum(tensor(2.), dim=0) (and probably other reduction functions) doesn't make sense module: docs triaged module: reductions",2020-10-28 17:26:03+00:00,,0,4,module: docs triaged module: reductions,True
46995,max_pool2d always fails on native Android app oncall: mobile,2020-10-28 16:56:08+00:00,,0,7,oncall: mobile,False
46980,torch.acos not supported for sparse layout module: sparse triaged function request,2020-10-28 10:41:48+00:00,,0,2,module: sparse triaged function request,False
46979,[JIT] scripting torchaudio.transforms.MFCC() error  oncall: mobile,2020-10-28 09:58:07+00:00,,0,1,oncall: mobile,True
46971,Yolov5 detect.py(ingerence) Error:Torch.nn.modules.module.ModuleAttributeError: 'Hardswish' object has no attribute 'inplace' module: nn module: pickle triaged,2020-10-28 07:15:10+00:00,,0,12,module: nn module: pickle triaged,True
46948,Record shaping assertions and use them for tracing / scripting optimization and codegen oncall: jit module: codegen,2020-10-27 22:03:28+00:00,,0,4,oncall: jit module: codegen,False
46947,Add Optimistic Mirror Descent (OMD) module: optimizer triaged,2020-10-27 21:51:11+00:00,,0,0,module: optimizer triaged,False
46946,Changing order of field assignments breaks TorchScript classes oncall: jit,2020-10-27 21:31:07+00:00,,1,2,oncall: jit,False
46944,Export TorchScript Classes as TorchScript code oncall: jit,2020-10-27 21:23:33+00:00,,1,3,oncall: jit,False
46929,Pooling code does not allow sliding window starting in right padded region triaged module: pooling,2020-10-27 17:38:33+00:00,,0,0,triaged module: pooling,True
46918,Jacobians computed by autograd.functional.jacobian with compute_graph sometimes set requires_grad True module: autograd triaged,2020-10-27 15:08:15+00:00,,0,9,module: autograd triaged,True
46907,SummaryWriter deletes data automatically when there are too many module: tensorboard oncall: visualization,2020-10-27 11:07:01+00:00,,0,5,module: tensorboard oncall: visualization,False
46904,Authentication for RPC oncall: distributed triaged module: rpc module: tensorpipe,2020-10-27 08:54:48+00:00,,0,5,oncall: distributed triaged module: rpc module: tensorpipe,True
46902,How to use clang as a cuda compiler instead of nvcc? module: build triaged enhancement,2020-10-27 05:52:23+00:00,,0,1,module: build triaged enhancement,True
46899,CUDA out of memory when using torch.load module: cuda triaged,2020-10-27 03:03:27+00:00,,0,0,module: cuda triaged,True
46886,Support deleting a parameter/buffer by name feature module: nn triaged module: ux,2020-10-26 23:12:05+00:00,,0,6,feature module: nn triaged module: ux,True
46869,[META] DO NOT DELETE THIS LABEL triaged open source fb-exported Merged,2020-10-26 19:33:36+00:00,,0,1,triaged open source fb-exported Merged,True
46829,Mixing Numpy's arrays and PyTorch tensors triaged module: numpy needs design module: ux,2020-10-26 00:39:36+00:00,,0,10,triaged module: numpy needs design module: ux,True
46822,C++ Optimizer: remove warning on Optimizer::size method module: optimizer triaged better-engineering,2020-10-25 14:27:18+00:00,,0,1,module: optimizer triaged better-engineering,True
46820,simple v *=  v_scale error module: autograd triaged actionable,2020-10-25 13:29:37+00:00,,0,8,module: autograd triaged actionable,True
46819,Specify input dimensions where they are not obvious module: docs triaged,2020-10-25 13:24:35+00:00,,0,1,module: docs triaged,True
46809,Including AdaBound in the list of Optimizers.  module: optimizer triaged enhancement,2020-10-24 08:19:12+00:00,,0,2,module: optimizer triaged enhancement,False
46807,[cudatoolkit 11.0] segfaults module: build module: cuda triaged,2020-10-24 06:37:02+00:00,,0,17,module: build module: cuda triaged,True
46806,Compile Pytorch with MAGMA Issue needs reproduction module: build triaged,2020-10-24 06:01:35+00:00,,0,2,needs reproduction module: build triaged,True
46803,About 2 minor bug fixes on CUDA macOSX 10.13.6 module: build module: cuda triaged module: macos module: nnpack,2020-10-24 04:33:25+00:00,,0,6,module: build module: cuda triaged module: macos module: nnpack,True
46802,"DataLoader gives ""Broken pipe"" error on Linux platform module: multiprocessing module: dataloader triaged",2020-10-24 04:13:24+00:00,,0,6,module: multiprocessing module: dataloader triaged,True
46787,Clearer error messages for 'different devices' error in the RNN module triaged better-engineering,2020-10-23 21:17:48+00:00,,0,0,triaged better-engineering,True
46766,Unable to compile from code v1.6.0 module: build triaged,2020-10-23 14:40:13+00:00,,0,1,module: build triaged,True
46760,How to define a new data type in  native_functions.yaml? module: internals triaged,2020-10-23 09:04:27+00:00,,0,1,module: internals triaged,True
46753,Memory leak when creating new tensors inside nn.DataParallel on multiple GPUs oncall: distributed triaged,2020-10-23 02:12:05+00:00,,0,2,oncall: distributed triaged,True
46749,Quantization - we need a better solution for tracking quantization backend settings in a model oncall: quantization triaged,2020-10-23 00:41:49+00:00,,1,33,oncall: quantization triaged,True
46739,torch.utils.tensorboard.SummaryWriter.add_embedding fails for some label_img sizes triaged module: tensorboard,2020-10-22 20:54:22+00:00,,0,3,triaged module: tensorboard,True
46724,PROBLEM WITH INSTALLATION PYTORCH FROM SOURCE module: build triaged,2020-10-22 17:44:07+00:00,,0,1,module: build triaged,True
46712,The documentation for c10::Dict is completely empty. module: docs module: internals module: cpp triaged,2020-10-22 14:55:18+00:00,,0,4,module: docs module: internals module: cpp triaged,True
46711,A random split function that return the datasets following specific target (label) distribution. feature triaged module: data,2020-10-22 13:41:52+00:00,,0,0,feature triaged module: data,True
46705,torch.eye not supported for sparse layout module: sparse triaged function request,2020-10-22 10:13:50+00:00,,1,1,module: sparse triaged function request,False
46659,Excluding image processing operators due to no opencv module: build triaged,2020-10-21 17:26:54+00:00,,0,0,module: build triaged,True
46654,Converting object detector model to TorchScript oncall: jit,2020-10-21 14:53:48+00:00,,0,1,oncall: jit,False
46653,Future returned by RPC should print a warning message on destruction if it's not waited oncall: distributed module: bootcamp triaged enhancement module: rpc,2020-10-21 14:52:32+00:00,,0,1,oncall: distributed module: bootcamp triaged enhancement module: rpc,True
46644,Trying to compile PyTorch for sm_30 fails with `error: identifier “__ldg” is undefined` module: build module: cuda triaged,2020-10-21 07:05:31+00:00,,0,5,module: build module: cuda triaged,True
46643,Autograd support for the tensor multiplication of sparse tensors module: sparse module: autograd triaged function request,2020-10-21 06:41:31+00:00,,0,1,module: sparse module: autograd triaged function request,True
46642,Loss functions for complex tensors  module: nn triaged module: complex complex_autograd,2020-10-21 05:19:31+00:00,,0,3,module: nn triaged module: complex complex_autograd,True
46631,error in bazel build //... module: build triaged module: bazel,2020-10-21 00:53:12+00:00,,0,2,module: build triaged module: bazel,True
46604,torch.concat doesn't raise an error in a quantized model oncall: quantization low priority triaged,2020-10-20 18:52:05+00:00,,1,1,oncall: quantization low priority triaged,True
46597,Suggestion: link individual APIs in docs with tutorials that use them module: docs triaged,2020-10-20 17:24:15+00:00,,2,0,module: docs triaged,False
46574,Mismatch between the documentation and implementation of caffe2 PReLU operator caffe2 module: docs,2020-10-20 05:57:08+00:00,,0,0,caffe2 module: docs,False
46564,Clean up request_callback_no_python.cpp triaged better-engineering module: rpc,2020-10-19 22:36:14+00:00,,2,5,triaged better-engineering module: rpc,True
46559,Exception raised in rpc_async context is silently handled oncall: distributed triaged module: rpc,2020-10-19 20:33:47+00:00,,0,1,oncall: distributed triaged module: rpc,True
46544,PyTorch NaN behavior and API design triaged module: numpy module: NaNs and Infs needs design module: ux,2020-10-19 17:44:02+00:00,,0,8,triaged module: numpy module: NaNs and Infs needs design module: ux,True
46531,`torch.igamma` error and gives wrong results on float64 ROCm high priority module: rocm triaged module: correctness (silent),2020-10-19 10:07:13+00:00,,0,0,high priority module: rocm triaged module: correctness (silent),True
46530,[Request] Batched Dataset->DataLoader interface feature module: dataloader triaged,2020-10-19 09:39:36+00:00,,0,0,feature module: dataloader triaged,True
46522,ModuleNotFoundError when installing PyTorch via pip on aarch64 environment module: binaries triaged module: arm,2020-10-18 15:46:02+00:00,,0,14,module: binaries triaged module: arm,True
46516,Batchnorm support for tracking buffer statistics when using gradient accumulation  triaged enhancement needs research module: norms and normalization,2020-10-17 15:13:11+00:00,,0,2,triaged enhancement needs research module: norms and normalization,True
46505,Cannot script when a static method or cross module function calls custom op oncall: jit module: cpp-extensions internals module: torchbind,2020-10-17 01:27:27+00:00,,1,2,oncall: jit module: cpp-extensions internals module: torchbind,True
46491,cannot call rpc.init_rpc twice within a single process high priority oncall: distributed triaged module: rpc,2020-10-16 21:00:28+00:00,,0,6,high priority oncall: distributed triaged module: rpc,True
46468,Advanced indexing: allow combining Boolean & integer index triaged module: numpy module: advanced indexing function request,2020-10-16 14:33:11+00:00,,0,1,triaged module: numpy module: advanced indexing function request,True
46464,[META] Assert that expected libraries appear in libtorch triaged module: static linking enhancement,2020-10-16 14:09:01+00:00,,0,0,triaged module: static linking enhancement,True
46460,C++ API: Training and inference of torchscript modules on multiple GPU oncall: jit,2020-10-16 13:18:51+00:00,,1,0,oncall: jit,True
46428,send_object/recv_object APIs for c10d oncall: distributed triaged module: c10d,2020-10-15 22:01:35+00:00,,1,0,oncall: distributed triaged module: c10d,True
46409,Custom ops get stuck in multiprocess data loader under certain environments module: dataloader triaged module: deadlock module: custom-operators,2020-10-15 18:51:50+00:00,,0,10,module: dataloader triaged module: deadlock module: custom-operators,True
46400,Make Undefined reprensentable in DispatchKeySet.  triaged enhancement,2020-10-15 16:31:52+00:00,,1,0,triaged enhancement,True
46386,"GPU memory leak when registering a forward hook with ""self"" access module: cuda module: memory usage triaged has workaround",2020-10-15 09:19:34+00:00,,0,2,module: cuda module: memory usage triaged has workaround,True
46382,"Code asserts when register new aten operations' implementation thru ""c10::RegisterOperators::op"" API.  module: internals triaged",2020-10-15 07:24:21+00:00,,0,6,module: internals triaged,True
46381,[Feature] Imbalanced batch scattering in th.nn.DataParallel triaged enhancement module: data parallel,2020-10-15 07:23:10+00:00,,0,1,triaged enhancement module: data parallel,True
46379,"Reduce ""reserved"" memory by PyTorch. module: cuda module: memory usage triaged",2020-10-15 06:44:31+00:00,,0,2,module: cuda module: memory usage triaged,True
46375,JIT: error in LSTM with `flatten_parameters` oncall: jit,2020-10-15 02:54:19+00:00,,0,4,oncall: jit,True
46374,Complex Number support for torch.nn modules module: nn triaged module: complex,2020-10-15 02:45:45+00:00,,0,13,module: nn triaged module: complex,True
46368,[JIT] Inconsistent behavior of torchScript parser in python and c++ oncall: jit,2020-10-15 01:04:32+00:00,,1,1,oncall: jit,True
46367,[chore] Test pybind11 2.6.0 (RC 2?) module: build module: ci triaged enhancement,2020-10-15 00:23:41+00:00,,0,20,module: build module: ci triaged enhancement,True
46350,torch.distributions.half_normal.HalfNormal.cdf returns negative values module: distributions triaged,2020-10-14 22:14:34+00:00,,0,0,module: distributions triaged,True
46334,A race problem of JIT cpp extensions in distributed setting oncall: jit days,2020-10-14 19:09:00+00:00,,0,0,oncall: jit days,True
46321,Add distributed examples into PyTorch CI tests oncall: distributed module: bootcamp module: tests triaged,2020-10-14 16:08:28+00:00,,1,3,oncall: distributed module: bootcamp module: tests triaged,True
46319,Remove header declarations for CPUType/TypeDefault module: internals triaged,2020-10-14 14:33:27+00:00,,0,0,module: internals triaged,True
46311,"Compile error when use c10::RegisterOperators::Options::kernel with the return type ""std::tuple<Tensor&, Tensor&>"" module: internals triaged",2020-10-14 07:22:35+00:00,,1,2,module: internals triaged,True
46291,ProcessGroup::Work API changes module: bc-breaking feature triaged module: c10d topic: bc breaking,2020-10-13 22:18:10+00:00,,0,0,module: bc-breaking feature triaged module: c10d topic: bc breaking,True
46276,test_distributed_* does not show error details from the subprocess oncall: distributed module: tests triaged better-engineering pt_distributed_rampup,2020-10-13 20:13:40+00:00,,0,1,oncall: distributed module: tests triaged better-engineering pt_distributed_rampup,True
46272,test_distributed_* does not work with run_test.py -i option oncall: distributed module: tests triaged better-engineering pt_distributed_rampup,2020-10-13 19:53:35+00:00,,0,0,oncall: distributed module: tests triaged better-engineering pt_distributed_rampup,True
46254,Enable named tensor inputs to `torch.linalg.norm` triaged enhancement module: named tensor,2020-10-13 16:51:58+00:00,,1,1,triaged enhancement module: named tensor,True
46253,Add short guide for updating `torch.norm` calls to `torch.linalg.norm` module: docs triaged,2020-10-13 16:49:40+00:00,,1,16,module: docs triaged,False
46248,"""distributed"" NCCL tests fail when having more than 3 GPUs oncall: distributed module: tests triaged module: nccl module: ddp",2020-10-13 14:41:51+00:00,,0,12,oncall: distributed module: tests triaged module: nccl module: ddp,True
46241,ubuntu16.04+vscode compile libtorch error module: build triaged,2020-10-13 12:01:44+00:00,,0,2,module: build triaged,True
46240,"Support ""symmetric"" reflection padding module: nn triaged module: numpy function request module: padding module: tensorflow",2020-10-13 11:55:07+00:00,,0,11,module: nn triaged module: numpy function request module: padding module: tensorflow,True
46225,torch.mode when input has nans module: docs triaged module: numpy module: NaNs and Infs module: sorting and selection module: reductions,2020-10-13 00:20:33+00:00,,0,2,module: docs triaged module: numpy module: NaNs and Infs module: sorting and selection module: reductions,True
46222,Use better tempfile creation mechanism to avoid skip windows test module: tests triaged,2020-10-12 22:52:13+00:00,,0,8,module: tests triaged,True
46210,[FX]Add support for len() on Proxy triaged module: fx,2020-10-12 20:03:53+00:00,,1,1,triaged module: fx,False
46202,pytorch not uploading data to tensorboard page triaged module: tensorboard,2020-10-12 18:03:00+00:00,,0,3,triaged module: tensorboard,True
46187,Better support for operators that return (named) tuples of tensors triaged module: numpy module: deprecation module: ux,2020-10-12 16:00:46+00:00,,0,34,triaged module: numpy module: deprecation module: ux,True
46184,redesign of Dropout feature module: nn triaged,2020-10-12 15:21:17+00:00,,0,1,feature module: nn triaged,True
46178,"torch.cuda.amp!  when I use @autocast() on DCN(DeformConv), the error ""RuntimeError:expect scalar type Float but Half "" module: cuda triaged module: amp (automated mixed precision)",2020-10-12 11:12:06+00:00,,0,2,module: cuda triaged module: amp (automated mixed precision),True
46176,BucketSampler for easy variable-length input batching feature module: dataloader triaged,2020-10-12 09:00:26+00:00,,0,0,feature module: dataloader triaged,True
46169,Squared 2-norm pdist (as available in SciPy / Faiss) triaged module: numpy function request module: norms and normalization module: distance functions,2020-10-11 21:57:52+00:00,,0,3,triaged module: numpy function request module: norms and normalization module: distance functions,True
46168,[discussion] In-place gradient (grad_input) computation for better memory utilisation module: autograd module: memory usage triaged needs research,2020-10-11 21:25:43+00:00,,0,18,module: autograd module: memory usage triaged needs research,True
46166,Error with DistributedDataParallel with specific model high priority triage review oncall: distributed triaged,2020-10-11 19:39:39+00:00,,0,5,high priority triage review oncall: distributed triaged,True
46164,Deprecate spmm and dsmm functions module: sparse triaged open source module: linear algebra module: deprecation,2020-10-11 17:06:07+00:00,,0,1,module: sparse triaged open source module: linear algebra module: deprecation,True
46158,No improvement gain between sm_86 (cuda 11.1) and sm_80 (cuda 11.0) on 3090 or 3080 GPUs. module: cudnn module: cuda triaged,2020-10-11 11:24:16+00:00,,0,35,module: cudnn module: cuda triaged,True
46157,Ability to disable cusolver module: build module: cuda triaged module: linear algebra,2020-10-11 10:47:25+00:00,,0,6,module: build module: cuda triaged module: linear algebra,True
46155,Complex backward returns NaN values high priority module: cuda triaged module: complex complex_autograd,2020-10-11 10:05:13+00:00,,0,16,high priority module: cuda triaged module: complex complex_autograd,True
46153,Getting this error while installing torch and torchvision! needs reproduction triaged,2020-10-11 07:11:35+00:00,,0,4,needs reproduction triaged,True
46149,seeding does not work when I initialize a linear model even if I do not use it in the code module: nn triaged,2020-10-10 22:06:53+00:00,,0,2,module: nn triaged,True
46139,"Policy CMP0012   is not set, CMake, Building For C++ With PyTorch CUDA module: build triaged better-engineering",2020-10-10 10:54:48+00:00,,0,2,module: build triaged better-engineering,True
46102,test_cat_cuda (__main__.TestTensorExprFuser) fails module: tests triaged shadow review,2020-10-09 18:36:59+00:00,,1,8,module: tests triaged shadow review,True
46093,"Spawn wrapper should catch BaseException, not Exception module: multiprocessing triaged better-engineering",2020-10-09 15:50:51+00:00,,0,0,module: multiprocessing triaged better-engineering,True
46084,"In PyTorch Tutorials, RuntimeError: CUDA error: out of memory happen module: memory usage triaged",2020-10-09 08:17:58+00:00,,0,2,module: memory usage triaged,True
46076,NCCL watchdog thread should log warnings about long-running GPU operations instead of silently hanging module: bootcamp triaged pt_distributed_rampup module: c10d,2020-10-09 02:23:07+00:00,,1,1,module: bootcamp triaged pt_distributed_rampup module: c10d,True
46066,TestXNNPACKConv1dTransformPass.test_conv1d_with_relu_fc takes 2+ min to finsh module: ci module: tests triaged,2020-10-08 23:37:58+00:00,,0,0,module: ci module: tests triaged,True
46065,TestDataLoader.test_proper_exit takes 2.5min to finish module: dataloader module: ci module: tests triaged,2020-10-08 23:34:26+00:00,,0,2,module: dataloader module: ci module: tests triaged,True
46061,Support undispatched ops in codegen triaged module: codegen,2020-10-08 22:41:39+00:00,,0,0,triaged module: codegen,True
46035,"Error installing on source: ""src.cxx:1:10: fatal error: glog/stl_logging.h: No such file or directory"" module: build triaged",2020-10-08 15:52:10+00:00,,0,3,module: build triaged,True
46031,Custom attention triaged oncall: transformer/mha,2020-10-08 10:26:13+00:00,,0,0,triaged oncall: transformer/mha,True
46027,Padding mode for ConvTransposeNd module: convolution triaged function request,2020-10-08 09:24:50+00:00,,0,2,module: convolution triaged function request,True
46024,Reproducibility breaks down with weighted Cross Entropy loss module: loss module: cuda triaged module: numerical-reproducibility module: determinism,2020-10-08 07:38:58+00:00,,0,11,module: loss module: cuda triaged module: numerical-reproducibility module: determinism,True
45996,Backpropagation for sparse matrix indexing is problematic (colab provided) module: sparse triaged,2020-10-07 21:50:59+00:00,,0,3,module: sparse triaged,True
45943,Improve distributed documentation for NCCL_BLOCKING_WAIT  oncall: distributed triaged pt_distributed_rampup module: ddp,2020-10-07 01:59:07+00:00,,0,3,oncall: distributed triaged pt_distributed_rampup module: ddp,True
45910,with torch.cuda.amp.autocast() get out of memory error when using with torch.no_grad() during validation module: docs module: autograd triaged module: amp (automated mixed precision),2020-10-06 17:29:48+00:00,,1,7,module: docs module: autograd triaged module: amp (automated mixed precision),True
45901,Add support for reading the whole file in from_file module: serialization triaged enhancement module: numpy,2020-10-06 15:22:57+00:00,,0,2,module: serialization triaged enhancement module: numpy,True
45897,Unify matrix multiplications operations module: sparse triaged open source module: linear algebra,2020-10-06 13:31:48+00:00,,0,13,module: sparse triaged open source module: linear algebra,True
45855,Do we have plan to offer C++ binding for prune related features.  module: cpp triaged enhancement module: pruning,2020-10-05 18:00:44+00:00,,0,2,module: cpp triaged enhancement module: pruning,True
45851,The same function can have different signatures in  the torch and torch.nn.functional namespaces module: nn triaged,2020-10-05 17:25:04+00:00,,0,2,module: nn triaged,True
45840,Error with DataParallel and dataclass triaged module: data parallel,2020-10-05 14:46:49+00:00,,0,0,triaged module: data parallel,True
45833,Support torch.mean for BoolTensors and other integer tensor inputs (without manual upcasting and hopefully without hidden upcasting) triaged module: numpy module: type promotion module: reductions,2020-10-05 11:23:56+00:00,,0,3,triaged module: numpy module: type promotion module: reductions,True
45823,"Traceback for ""Warning: Mixed memory format inputs detected while calling the operator."" triaged module: memory format",2020-10-04 19:32:31+00:00,,1,7,triaged module: memory format,True
45808,Support for Locality Sensitive Hashing Optimizations triaged needs research,2020-10-03 19:30:42+00:00,,0,0,triaged needs research,False
45769,F.conv2d() causes RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR  module: cudnn triaged,2020-10-02 20:30:31+00:00,,0,15,module: cudnn triaged,True
45767,`defineMethodsInModule` no longer exists module: docs triaged,2020-10-02 20:17:16+00:00,,0,0,module: docs triaged,True
45764,resize_(0) is very expensive module: performance triaged,2020-10-02 19:51:26+00:00,,0,3,module: performance triaged,True
45760,Complex Number support for distributed oncall: distributed module: bootcamp triaged module: complex pt_distributed_rampup module: c10d,2020-10-02 18:57:52+00:00,,0,6,oncall: distributed module: bootcamp triaged module: complex pt_distributed_rampup module: c10d,True
45745,Native CUDA-Aware MPI support for MVAPICH2 and other MPI libraries oncall: distributed module: cuda needs research,2020-10-02 16:59:47+00:00,,0,4,oncall: distributed module: cuda needs research,False
45728,Low shared memory module: dataloader triaged,2020-10-02 07:26:50+00:00,,0,5,module: dataloader triaged,True
45717,Better documentation of vec256 API module: docs triaged module: vectorization internals,2020-10-02 02:47:54+00:00,,0,0,module: docs triaged module: vectorization internals,True
45690,Complex and real results do not agree when computing reciprocal or pow(-1) of 0 triaged module: complex,2020-10-01 20:00:14+00:00,,0,8,triaged module: complex,True
45685,"[fx] symbolic trace ""is None"" and ""is not None"" checks triaged module: fx",2020-10-01 18:42:28+00:00,,0,9,triaged module: fx,True
45683,[FX] Configurability of nested module representation triaged module: fx,2020-10-01 18:22:54+00:00,,0,0,triaged module: fx,True
45682,[FX] Allow customization of the behavior of Proxy triaged module: fx,2020-10-01 17:53:04+00:00,,1,0,triaged module: fx,True
45635,fx graph mode quantization tutorials oncall: quantization low priority triaged,2020-10-01 00:29:25+00:00,,1,5,oncall: quantization low priority triaged,True
45578,Why is RTX3080 slower than RTX2020-Ti? module: performance module: cuda triaged,2020-09-30 16:53:50+00:00,,0,16,module: performance module: cuda triaged,True
45573,Build Failed Ubuntu Cuda. CUDAHooks.cpp:97:15: error: ‘struct cudaPointerAttributes’ has no member named ‘type’ return attr.type == cudaMemoryTypeHost; module: build triaged,2020-09-30 15:57:43+00:00,,0,2,module: build triaged,True
45569,Distutils Error in torch.hub Load() triaged module: hub,2020-09-30 14:06:55+00:00,,0,2,triaged module: hub,True
45565,Resolving overloads and default arguments from within __torch_function__ feature triaged module: __torch_function__,2020-09-30 10:48:28+00:00,,1,9,feature triaged module: __torch_function__,True
45560,Android: allow preallocation of output buffers feature triaged module: android oncall: mobile,2020-09-30 07:34:41+00:00,,0,0,feature triaged module: android oncall: mobile,True
45549,Can't initialize NCCL/GLOO process group if default process group is MPI high priority triage review oncall: distributed module: bootcamp triaged module: nccl pt_distributed_rampup,2020-09-30 01:56:09+00:00,,1,0,high priority triage review oncall: distributed module: bootcamp triaged module: nccl pt_distributed_rampup,True
45540,Why does NLLLoss and CrossEntropyLoss require type long? feature triaged,2020-09-29 23:27:14+00:00,,0,0,feature triaged,True
45534,"[FX] Make tracer return a Graph, not a GraphModule triaged module: fx",2020-09-29 21:40:55+00:00,,1,0,triaged module: fx,True
45524,[JIT] JIT Autograd is saving more results than are necessary for bprop like the result of RELU when it is an intermediate oncall: jit,2020-09-29 19:27:32+00:00,,1,5,oncall: jit,True
45503,Tensorboard makes logger handlers (except the 1st one) DISAPPEAR ! triaged module: tensorboard,2020-09-29 14:56:13+00:00,,0,11,triaged module: tensorboard,True
45499,"torch.nonzero(t, as_tuple=...) does not work with the JIT because the as_tuple signatures are not exposed properly oncall: jit triaged module: numpy module: python array api",2020-09-29 10:45:15+00:00,,0,20,oncall: jit triaged module: numpy module: python array api,True
45493,Build LibTorch for cpu using OpenBLAS: Had to manually remove a path in Caffe2Targets.cmake module: build triaged,2020-09-29 08:57:12+00:00,,0,3,module: build triaged,True
45483,Extra arguments included in the doc where they are not actually presented in the source code module: docs module: optimizer triaged,2020-09-29 04:25:41+00:00,,0,0,module: docs module: optimizer triaged,False
45481,Combining add_scalar with add_hparams with different frequencey  triaged module: tensorboard,2020-09-29 03:33:03+00:00,,0,0,triaged module: tensorboard,True
45465,torchscript model size increases when loading and saving it again oncall: jit,2020-09-28 21:42:36+00:00,,0,3,oncall: jit,False
45459,Backward for sparse tensor item select does not work module: sparse module: autograd triaged,2020-09-28 20:37:38+00:00,,0,2,module: sparse module: autograd triaged,True
45448,CuDNN version not found module: build module: cudnn triaged,2020-09-28 18:50:41+00:00,,0,2,module: build module: cudnn triaged,True
45447,Retire usages of CUDA_tensor_apply helpers in ATen module: internals module: cuda triaged better-engineering,2020-09-28 18:19:10+00:00,,0,3,module: internals module: cuda triaged better-engineering,True
45444,Using operator[] in GenericPackedTensorAccessor impossible in cpu code in .cu files (tensor on cpu) module: build triaged,2020-09-28 16:34:39+00:00,,0,0,module: build triaged,True
45428,Use libtorch for online inference: an illegal memory access was encountered awaiting response (this tag is deprecated) module: cuda triaged,2020-09-28 12:17:55+00:00,,0,2,awaiting response (this tag is deprecated) module: cuda triaged,True
45414,Module __call__ typing module: typing triaged,2020-09-28 04:15:09+00:00,,0,2,module: typing triaged,True
45412,Autocast but there are no kernels registered for this dispatch key. The operator is p�8ߎU Aborted (core dumped) triaged module: amp (automated mixed precision),2020-09-28 04:12:52+00:00,,0,1,triaged module: amp (automated mixed precision),True
45399,Load ExtraFilesMap with Java library  triaged enhancement oncall: java,2020-09-27 19:46:20+00:00,,0,0,triaged enhancement oncall: java,False
45398,build_android.sh oncall: mobile,2020-09-27 19:32:04+00:00,,0,1,oncall: mobile,False
45352,torch.nn.functional.one_hot should gracefully skip negative and out-of-range indices module: nn triaged enhancement,2020-09-25 19:09:10+00:00,,0,8,module: nn triaged enhancement,True
45327,Tensorboard summary_iterator module: tensorboard oncall: visualization,2020-09-25 05:30:33+00:00,,0,1,module: tensorboard oncall: visualization,False
45321,Memory leak when using share_memory_ on cuda device module: memory usage triaged,2020-09-25 01:08:45+00:00,,0,0,module: memory usage triaged,True
45308,Blocked version of Cholesky backward module: autograd triaged module: linear algebra,2020-09-24 21:38:02+00:00,,0,0,module: autograd triaged module: linear algebra,True
45295,Adaptive usage of memory during training inference feature module: nn module: cuda triaged needs research,2020-09-24 19:51:41+00:00,,0,5,feature module: nn module: cuda triaged needs research,True
45273,"Training fast with small dataset, slow with large dataset needs reproduction module: performance triaged module: data parallel module: amp (automated mixed precision)",2020-09-24 15:33:50+00:00,,0,8,needs reproduction module: performance triaged module: data parallel module: amp (automated mixed precision),True
45242,Functional interface for optimizers module: cpp module: optimizer triaged,2020-09-23 23:30:01+00:00,,0,6,module: cpp module: optimizer triaged,True
45220,FX should preserve type annotations and not break TorchScript-ability oncall: jit triaged module: fx,2020-09-23 19:07:35+00:00,,1,2,oncall: jit triaged module: fx,True
45208,Adding @torch.no_grad() to forward () causes undefined value in torch.jit.script()  oncall: jit,2020-09-23 17:12:30+00:00,,0,3,oncall: jit,True
45203,Staged backend boxed fallback (per-operator precomputation / precompute) triaged module: dispatch,2020-09-23 14:48:10+00:00,,0,0,triaged module: dispatch,True
45199,JIT doesn't support rfloordiv with scalars correctly oncall: jit days TSUsability TSRootCause:PyTorchParityGap,2020-09-23 11:10:35+00:00,,1,2,oncall: jit days TSUsability TSRootCause:PyTorchParityGap,False
45198,DataLoader with cv2 and some numpy/cv2 import order causes workers to not work high priority needs reproduction module: dataloader triaged module: multithreading module: regression,2020-09-23 10:58:16+00:00,,0,12,high priority needs reproduction module: dataloader triaged module: multithreading module: regression,True
45160,"In profiler, record_function event's total CPU time can be less than the contained ops triaged oncall: profiler module: correctness (silent)",2020-09-22 20:29:33+00:00,,0,1,triaged oncall: profiler module: correctness (silent),True
45139,Mention accessor/data_ptr for raw memory access in Libtorch index API document and discuss performance implications module: performance module: cpp triaged,2020-09-22 17:04:32+00:00,,0,8,module: performance module: cpp triaged,True
45136,Multiple torch.load in one file high priority module: serialization triaged module: regression,2020-09-22 16:23:53+00:00,,0,13,high priority module: serialization triaged module: regression,True
45133,[Feature Request] Add support for Hidden Markov Models in torch.distributions module: distributions feature triaged needs research,2020-09-22 15:28:25+00:00,,0,4,module: distributions feature triaged needs research,False
45129,Inconsistent wheel name in https://download.pytorch.org/whl/torch_stable.html module: build triaged,2020-09-22 14:37:37+00:00,,0,1,module: build triaged,True
45125,Boolean indexing of an ndarray with a torch.tensor mask breaks for size=1  triaged module: numpy module: advanced indexing,2020-09-22 14:03:32+00:00,,0,7,triaged module: numpy module: advanced indexing,True
45115,Standardized Distributions module: distributions triaged,2020-09-22 11:23:30+00:00,,0,11,module: distributions triaged,True
45111,(prototype) Graph Mode Dynamic Quantization on BERT failure on quantize_dynamic_jit(...) call triaged oncall: mobile,2020-09-22 08:13:24+00:00,,0,7,triaged oncall: mobile,True
45100,Pytorch report INTERNAL ASSERT FAILED at ..\\torch\\csrc\\jit\\ir.cpp:1529 when use torch.jit.script to convert to model oncall: jit,2020-09-22 01:40:19+00:00,,0,3,oncall: jit,True
45073,Compilation errors on power-pc module: build triaged module: POWER,2020-09-21 17:33:25+00:00,,0,10,module: build triaged module: POWER,True
45053,about benchmark issue module: cudnn triaged,2020-09-21 03:59:56+00:00,,0,5,module: cudnn triaged,True
45051,Custom Datatypes in Tensors triaged module: numpy needs research,2020-09-21 00:06:41+00:00,,0,6,triaged module: numpy needs research,True
45044,torch rpc cannot handle UnsupportedNodeError exception oncall: distributed triaged module: rpc,2020-09-20 07:09:51+00:00,,0,1,oncall: distributed triaged module: rpc,True
45042,"torch.distributed launch.py is hanged.  (pid, sts) = os.waitpid(self.pid, wait_flags) oncall: distributed triaged",2020-09-20 03:40:08+00:00,,0,6,oncall: distributed triaged,True
45012,torch.jit saves nonpersistent buffers oncall: jit days,2020-09-19 03:01:25+00:00,,0,0,oncall: jit days,False
45009,Median / quantile / mode / rank / percentile pooling module: performance triaged module: sorting and selection module: pooling,2020-09-19 01:48:47+00:00,,0,1,module: performance triaged module: sorting and selection module: pooling,True
44992,[jit] cann't pass class objects across boundary of tracing/scripting oncall: jit weeks,2020-09-18 22:42:52+00:00,,0,0,oncall: jit weeks,False
44991,"Unite/unify tensor.unfold with F.unfold and make them more performant (zero-copy or little-copy with stride tricks, as in NumPy) module: nn triaged module: ux",2020-09-18 22:26:44+00:00,,0,6,module: nn triaged module: ux,True
44989,"Support unfold for integral types (long, byte etc) tensors feature module: nn triaged actionable",2020-09-18 22:18:23+00:00,,0,9,feature module: nn triaged actionable,True
44969,Tensordot does not support Bool triaged enhancement module: numpy module: boolean tensor,2020-09-18 19:44:22+00:00,,0,1,triaged enhancement module: numpy module: boolean tensor,True
44968,[feature request] dtype argument for torch.sign triaged enhancement module: numpy,2020-09-18 19:35:48+00:00,,0,2,triaged enhancement module: numpy,True
44954,Optimization with constraints for torch.optim feature module: optimizer triaged needs research,2020-09-18 16:10:14+00:00,,1,4,feature module: optimizer triaged needs research,False
44951,copy.deepcopy not working properly for jit.TopLevelTracedModule oncall: jit days,2020-09-18 14:57:30+00:00,,0,3,oncall: jit days,True
44948,Clarify use of GLog/GFlags module: dependency bug module: build triaged small module: build warnings better-engineering,2020-09-18 12:06:35+00:00,,0,2,module: dependency bug module: build triaged small module: build warnings better-engineering,True
44945,Fuse softmax and masking in MultiheadAttention feature triaged needs research oncall: transformer/mha,2020-09-18 10:27:35+00:00,,0,4,feature triaged needs research oncall: transformer/mha,False
44943,In pytorch 1.6。Run model with input no contiguous tensor will become very slow. module: performance module: cuda triaged,2020-09-18 09:32:27+00:00,,0,5,module: performance module: cuda triaged,True
44938,DataParallel on CPU triaged enhancement module: data parallel,2020-09-18 07:44:03+00:00,,0,3,triaged enhancement module: data parallel,True
44931,Issue while writing scalars o tensorboard using writer.add_scalars(....) triaged module: tensorboard,2020-09-18 03:47:48+00:00,,0,4,triaged module: tensorboard,True
44917,Incorrect documentation of SGD momentum module: docs module: optimizer triaged,2020-09-17 23:09:20+00:00,,0,2,module: docs module: optimizer triaged,True
44901,for loop can't be symbolically traced triaged module: fx,2020-09-17 19:10:36+00:00,,0,2,triaged module: fx,True
44898,Don't query current device on stream construction module: bootcamp triaged,2020-09-17 18:13:58+00:00,,0,1,module: bootcamp triaged,True
44878,"[ONNX] Support Tuple(List(Tensors),...) in input/ouput module: onnx feature triaged",2020-09-17 14:42:13+00:00,,0,8,module: onnx feature triaged,False
44869,Incorrect processing of autograd profiler outputs in torch.utils.bottleneck module: autograd triaged oncall: profiler,2020-09-17 08:08:01+00:00,,0,0,module: autograd triaged oncall: profiler,True
44855,RuntimeError: Caught RuntimeError in replica 1 on device 1. needs reproduction triaged module: data parallel,2020-09-17 01:31:16+00:00,,0,1,needs reproduction triaged module: data parallel,True
44851,fx: Python range function used on tensor shape is not symbolically traceable triaged module: fx,2020-09-17 00:05:52+00:00,,0,4,triaged module: fx,True
44827,[RFC] Pipeline Parallelism in PyTorch oncall: distributed triaged needs research module: rpc,2020-09-16 21:29:08+00:00,,1,1,oncall: distributed triaged needs research module: rpc,True
44822,ONNX export failed on ATen operator rfft because torch.onnx.symbolic_opset9.rfft does not exist module: onnx triaged enhancement,2020-09-16 20:53:11+00:00,,0,6,module: onnx triaged enhancement,False
44817,ComplexHelper.h contains non-inline functions triaged module: complex,2020-09-16 20:33:31+00:00,,0,0,triaged module: complex,True
44814,LNK2019 error after static compilation of Libtorch module: build triaged,2020-09-16 20:23:32+00:00,,0,13,module: build triaged,True
44809,Simple functions shouldn't go through dispatcher triaged module: dispatch better-engineering,2020-09-16 19:48:56+00:00,,1,2,triaged module: dispatch better-engineering,True
44800,torchscript continue training oncall: jit,2020-09-16 17:59:57+00:00,,0,1,oncall: jit,False
44784,Unreliable CPU times in torch.autograd.profiler.profile(use_cuda=True) when using CUDA module: cuda triaged oncall: profiler,2020-09-16 14:14:45+00:00,,0,1,module: cuda triaged oncall: profiler,True
44777,Hybrid Memory module: cuda module: memory usage triaged needs research,2020-09-16 07:29:27+00:00,,0,2,module: cuda module: memory usage triaged needs research,True
44771,Timed out RRef can still be used in subsequent RPCs triaged module: rpc,2020-09-16 03:09:14+00:00,,0,1,triaged module: rpc,True
44768,[JIT] function decorated with @torch.no_grad() can not be exported when another function is called inside it. oncall: jit days,2020-09-16 01:54:02+00:00,,1,4,oncall: jit days,False
44741,cblas_gemv is not being used for gemv on complex on CPU triaged module: complex,2020-09-15 22:07:29+00:00,,0,1,triaged module: complex,True
44738,[tools.codegen] Remove byte-for-byte compatibility code triaged module: codegen,2020-09-15 21:23:38+00:00,,1,1,triaged module: codegen,True
44726,Pytorch Installation from source fails (GCC-8.4/CUDA-10.2/RHEL7) and (GCC-7.5/CUDA-10.2/RHEL7) module: build triaged,2020-09-15 18:29:02+00:00,,0,3,module: build triaged,True
44724,16-bit input + AMP breaks AdaptiveLogSoftmaxWithLoss triaged module: amp (automated mixed precision),2020-09-15 18:06:47+00:00,,1,4,triaged module: amp (automated mixed precision),True
44721,Get rid of copy_from module: internals triaged,2020-09-15 17:19:31+00:00,,0,0,module: internals triaged,True
44719,404 Page not found - inference_api.md & management_api.md triaged module: doc infra,2020-09-15 16:43:52+00:00,,0,2,triaged module: doc infra,True
44710,AMP support for libtorch/c++ triaged enhancement module: amp (automated mixed precision),2020-09-15 14:17:17+00:00,,1,3,triaged enhancement module: amp (automated mixed precision),True
44704,Error message concerning dtype GRU not identical between devices module: error checking triaged,2020-09-15 10:10:00+00:00,,0,1,module: error checking triaged,True
44676,Make quantized::prepack_fp16 op just do prepacking oncall: quantization low priority triaged,2020-09-15 00:21:36+00:00,,0,1,oncall: quantization low priority triaged,True
44673,[RFC] Integrate profiler with torch.distributed APIs for profiling of distributed models oncall: distributed triaged oncall: profiler,2020-09-14 23:33:22+00:00,,0,0,oncall: distributed triaged oncall: profiler,True
44657,Use macro to define `DispatchKey::toString` method triaged module: dispatch,2020-09-14 21:07:41+00:00,,0,0,triaged module: dispatch,True
44634,torch.sparse improvements - tracking issue high priority module: sparse triaged,2020-09-14 13:03:14+00:00,,1,7,high priority module: sparse triaged,True
44631,Multi-process Dataloader and multi-parameter exceptions module: dataloader triaged enhancement,2020-09-14 09:14:46+00:00,,0,3,module: dataloader triaged enhancement,True
44630,No pytorch_jni in StartLocal's zip oncall: mobile,2020-09-14 09:03:15+00:00,,0,1,oncall: mobile,False
44624,CUDA RuntimeError unrecoverably bricks session module: cuda triaged,2020-09-14 01:56:57+00:00,,0,3,module: cuda triaged,True
44611,MacOS CPU torch.tan and torch.tanh do not compute some values properly module: cpu triaged module: complex module: macos,2020-09-13 10:18:19+00:00,,0,4,module: cpu triaged module: complex module: macos,True
44608,building torchscript extension results in INVALID TYPE: Only int64_t and bool are supported as an integral argument type  custom_class oncall: jit,2020-09-13 06:29:21+00:00,,0,3,oncall: jit,True
44593,Docs include the verbose arg of `CosineAnnealingWarmRestarts` but the actual docstring in code doesn't module: docs triaged module: doc infra,2020-09-12 15:10:39+00:00,,0,6,module: docs triaged module: doc infra,False
44591,[Feature] Fused Matmul & Min/Max/Sum/Prod module: performance triaged enhancement needs research module: linear algebra,2020-09-12 09:30:49+00:00,,0,1,module: performance triaged enhancement needs research module: linear algebra,True
44552,torch.norm does not broadcast for dim > 2 module: docs triaged,2020-09-11 14:53:08+00:00,,0,6,module: docs triaged,False
44551,PyTorch 1.6 DataParallel causes CUDNN_STATUS_BAD_PARAM in backward pass module: cudnn module: cuda triaged module: data parallel,2020-09-11 14:20:19+00:00,,0,11,module: cudnn module: cuda triaged module: data parallel,True
44547, floating point exception (core dumped) in training 10^4 steps module: cuda triaged,2020-09-11 10:49:52+00:00,,0,4,module: cuda triaged,True
44545,too large data in Queue cause dead lock in Multiprocessing  module: multiprocessing triaged,2020-09-11 09:02:56+00:00,,0,1,module: multiprocessing triaged,True
44544,torch.distributed server back-connects to clients on random ports leading to firewall problems oncall: distributed triaged,2020-09-11 08:39:43+00:00,,0,5,oncall: distributed triaged,True
44533,“doxygenfunction: Unable to resolve multiple matches for function ...” in C++ documentation module: docs module: cpp triaged,2020-09-11 03:05:28+00:00,,1,2,module: docs module: cpp triaged,True
44528,DataLoader consumes extremely large shared memory (shm) in its initialization. module: dataloader triaged,2020-09-11 02:08:56+00:00,,1,1,module: dataloader triaged,True
44511,"LR scheduler throws warning when using scaler.step instead of optimizer.step, and when saving optimizer state triaged module: amp (automated mixed precision)",2020-09-10 22:11:30+00:00,,0,8,triaged module: amp (automated mixed precision),True
44504,FP16 inference latency after sleeping module: performance triaged,2020-09-10 21:16:05+00:00,,0,5,module: performance triaged,True
44503,TestJitGeneratedModule.test_nn_CTCLoss_lengths_intlists fails if not skipped oncall: jit,2020-09-10 21:12:26+00:00,,1,2,oncall: jit,False
44489,for_each doesn't support integer input to float output type promotion triaged,2020-09-10 18:38:10+00:00,,0,2,triaged,True
44473,RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input. module: cudnn module: cuda module: convolution triaged,2020-09-10 15:41:48+00:00,,0,10,module: cudnn module: cuda module: convolution triaged,True
44469,Error in the converting to pytorch mobile.. oncall: mobile,2020-09-10 13:48:08+00:00,,0,5,oncall: mobile,True
44446,[jit] class type optional type annotation not working properly  oncall: jit,2020-09-10 01:30:00+00:00,,1,0,oncall: jit,True
44428,Support for CUDA matrix multiplication on long (and other integer) tensors high priority module: cuda triaged module: cublas module: linear algebra needs design function request,2020-09-09 21:30:32+00:00,,0,16,high priority module: cuda triaged module: cublas module: linear algebra needs design function request,True
44421,[tools.codegen] Rename api.legacy_dispatcher to api.native module: internals triaged,2020-09-09 20:46:17+00:00,,0,0,module: internals triaged,True
44415,Batchnorm2d in PT1.6 couldn't turn off track_running_stats module: docs module: nn triaged,2020-09-09 19:59:16+00:00,,0,7,module: docs module: nn triaged,True
44403,Investigate SyncBatchNorm cleanup with NCCL Async Error Handling oncall: distributed triaged better-engineering,2020-09-09 18:14:06+00:00,,1,0,oncall: distributed triaged better-engineering,True
44380,Support for NVIDIA UVM technology feature module: cuda triaged,2020-09-09 14:10:49+00:00,,0,2,feature module: cuda triaged,True
44362,Libtorch CMakeList.txt config help module: build triaged,2020-09-09 05:15:41+00:00,,0,1,module: build triaged,True
44343,Asynchronous Execution on CPU feature module: cpu triaged,2020-09-08 23:15:41+00:00,,0,4,feature module: cpu triaged,True
44341,Consider cache effects in Timer module: bootcamp triaged enhancement,2020-09-08 23:01:22+00:00,,1,4,module: bootcamp triaged enhancement,True
44316,[FX] Schema normalization tooling  module: internals triaged module: fx,2020-09-08 18:23:21+00:00,,0,5,module: internals triaged module: fx,True
44313,`Undefined symbol: torch::kNearest` when building App iOS triaged oncall: mobile module: ios,2020-09-08 17:55:49+00:00,,0,0,triaged oncall: mobile module: ios,True
44295,Request: Add PyTorch version to state dicts module: serialization triaged enhancement,2020-09-08 04:25:05+00:00,,0,3,module: serialization triaged enhancement,False
44282,PyTorch wheel's own OpenMP library clashing with system-wide OpenMP library at runtime triaged module: openmp,2020-09-07 15:38:54+00:00,,0,4,triaged module: openmp,True
44279,How to use CUDA Dynamic Parallelism in PyTorch CPP extension? module: cuda triaged,2020-09-07 10:55:28+00:00,,0,2,module: cuda triaged,True
44274,Slower speeds when using half(). module: performance triaged,2020-09-07 07:33:38+00:00,,0,2,module: performance triaged,True
44255,Allow to use system zstd feature triaged,2020-09-06 00:13:15+00:00,,0,0,feature triaged,False
44248,Additional clutter memory allocated on GPU 0 when training on GPU 1+ module: cuda triaged,2020-09-05 09:46:19+00:00,,0,4,module: cuda triaged,True
44245,torch.mv with sparse matrix gives internal assert on cuda module: cuda triaged module: assert failure,2020-09-05 08:46:16+00:00,,0,2,module: cuda triaged module: assert failure,True
44185,caffe2: Python 3 deprecation warnings about inspect.getargspec caffe2 triaged,2020-09-04 08:46:37+00:00,,0,0,caffe2 triaged,True
44182,Support keep stride for neg with requires_grad=False module: memory usage triaged enhancement module: numpy,2020-09-04 07:55:08+00:00,,0,6,module: memory usage triaged enhancement module: numpy,True
44179,REINFORCEMENT LEARNING (DQN) TUTORIAL is not working anymore due to matplotlib.pyplot.imshow() module: docs triaged,2020-09-04 07:33:31+00:00,,0,0,module: docs triaged,True
44172,build from scratch failed because of the expansion of macro module: build triaged,2020-09-04 03:12:54+00:00,,0,2,module: build triaged,True
44169,Deadlock with RPC and dist.barrier() for TensorPipeAgent and NCCL. triaged module: nccl module: rpc module: tensorpipe,2020-09-04 02:23:15+00:00,,0,0,triaged module: nccl module: rpc module: tensorpipe,True
44159,"[Feature Request] Add an option to run GPU tests only, and skip all CPU tests feature module: tests triaged",2020-09-03 22:46:17+00:00,,0,6,feature module: tests triaged,False
44156,CUDA memory leak in multi-processing module: performance module: cuda module: memory usage triaged,2020-09-03 22:27:43+00:00,,0,9,module: performance module: cuda module: memory usage triaged,True
44103,Item seems to affect the backward process of DataParallel module: cuda triaged module: data parallel,2020-09-03 08:28:00+00:00,,0,3,module: cuda triaged module: data parallel,True
44084,[RFC] Manage CUDA Stream in TensorPipe RPC Agent feature triaged module: rpc module: tensorpipe,2020-09-03 01:00:23+00:00,,0,5,feature triaged module: rpc module: tensorpipe,True
44077,[jit] Better type refinement for class attributes which are class types oncall: jit weeks months TSRootCause:TypeRefinement TSUsability,2020-09-02 23:30:24+00:00,,0,2,oncall: jit weeks months TSRootCause:TypeRefinement TSUsability,True
44049,[FX] Can't symbolically trace callsites to ScriptFunction triaged module: fx,2020-09-02 19:11:30+00:00,,0,2,triaged module: fx,True
44027,Immutable (read-only) tensors triaged,2020-09-02 15:18:46+00:00,,0,9,triaged,True
44026,[Feature] Einsum like ShapeGuard feature triaged shadow review,2020-09-02 14:11:36+00:00,,0,5,feature triaged shadow review,False
44024,"When the pytorch training data reaches a certain epoch, the memory remains unchanged? needs reproduction triaged",2020-09-02 09:13:44+00:00,,0,4,needs reproduction triaged,False
43964,Make it so that leading underscore operators are truly private and can be changed without worry for BC triaged,2020-09-01 17:42:20+00:00,,0,0,triaged,True
43949,bytes(byte_tensor) gives strange error + [feature request] support memoryview(tensor) module: error checking triaged module: advanced indexing,2020-09-01 10:49:49+00:00,,0,10,module: error checking triaged module: advanced indexing,True
43947,torch.cuda.synchronize Influence distributed training module: performance module: cudnn module: cuda triaged module: data parallel,2020-09-01 10:20:15+00:00,,0,4,module: performance module: cudnn module: cuda triaged module: data parallel,True
43904,Documentation and `torch.sparse` alias for `torch.bmm` sparse-dense module: sparse triaged,2020-08-31 20:15:08+00:00,,0,1,module: sparse triaged,True
43876,Insert at specific index/key in nn.Sequential module: nn triaged enhancement,2020-08-31 08:49:16+00:00,,0,5,module: nn triaged enhancement,True
43873,PackedSequence objects that are created within jit.script are not PackedSequence oncall: jit TSRootCause:InvalidCustomClass TSUsability,2020-08-31 07:11:04+00:00,,1,4,oncall: jit TSRootCause:InvalidCustomClass TSUsability,True
43869,"In profiler, calling `key_averages()` unexpectedly changes CPU time returned for profiled events triaged oncall: profiler",2020-08-31 02:21:03+00:00,,0,0,triaged oncall: profiler,True
43868,"In profiler, recorded block's total time can be less than the operators within the block triaged oncall: profiler",2020-08-31 01:54:39+00:00,,0,0,triaged oncall: profiler,True
43867,torch.bincount beyond 1d arrays triaged enhancement,2020-08-30 23:49:31+00:00,,0,1,triaged enhancement,True
43865,"Allow custom kwargs for forward method of nn.TransformerEncoderLayer, nn.TransformerDecoderLayer. feature triaged oncall: transformer/mha",2020-08-30 22:30:45+00:00,,0,2,feature triaged oncall: transformer/mha,False
43859,Multi-machine multi-gpu training won't start on CNGrid needs reproduction oncall: distributed triaged,2020-08-30 14:46:16+00:00,,0,2,needs reproduction oncall: distributed triaged,True
43816,Move randperm() to DistributionTemplates triaged module: random csprng,2020-08-29 00:16:41+00:00,,0,0,triaged module: random csprng,True
43815,Buffers are moved to another device by copy and not in-place triaged,2020-08-29 00:14:24+00:00,,1,5,triaged,False
43795,[ONNX] shape node constant folded tensor with dynamic shape module: onnx triaged,2020-08-28 19:48:36+00:00,,0,8,module: onnx triaged,False
43765,JIT fails sanity checks during tracing torch.rand_like oncall: jit,2020-08-28 13:02:52+00:00,,0,1,oncall: jit,True
43760,Error : lib/libstdc++.so.6: version `CXXABI_1.3.11 not found module: binaries module: build triaged,2020-08-28 09:30:57+00:00,,0,4,module: binaries module: build triaged,True
43758,OSError: /lib64/libc.so.6: version `GLIBC_2.14' not found  module: binaries triaged,2020-08-28 07:41:48+00:00,,0,1,module: binaries triaged,True
43754,"Using Dataparallel with multi input error.  Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0 triaged module: data parallel",2020-08-28 00:54:00+00:00,,0,1,triaged module: data parallel,True
43707,torch.optim mentions legacy Variable module: docs triaged,2020-08-27 15:36:02+00:00,,0,0,module: docs triaged,False
43706,checkpoint restore of optimizers changes dtype of Floating-point state module: optimizer triaged,2020-08-27 15:29:57+00:00,,0,2,module: optimizer triaged,False
43692,named tensor INTERNAL ASSERT FAILED when indexing with a list. triaged module: named tensor,2020-08-27 07:16:40+00:00,,0,1,triaged module: named tensor,True
43686,Static Linking of PyTorch didn't statically link CUDA module: binaries triaged module: static linking,2020-08-27 03:12:27+00:00,,0,1,module: binaries triaged module: static linking,True
43685,Syncbatchnorm and DDP  oncall: distributed module: cuda triaged,2020-08-27 02:33:59+00:00,,0,4,oncall: distributed module: cuda triaged,True
43679,[FX] List unpacking is broken triaged module: fx,2020-08-27 00:44:24+00:00,,0,4,triaged module: fx,True
43677,"Some largeCUDATensorTest fails with OOM when running with the entire test suit, but not when running standalone module: tests triaged",2020-08-27 00:01:59+00:00,,0,1,module: tests triaged,True
43672,Make `torch.Generator` picklable feature module: serialization triaged,2020-08-26 23:07:02+00:00,,0,1,feature module: serialization triaged,True
43663,test_doc_template is not working correctly module: tests triaged,2020-08-26 21:50:27+00:00,,0,0,module: tests triaged,True
43659,Exploring supporting some cases of `__bool__` on Proxy triaged module: fx,2020-08-26 21:11:35+00:00,,0,0,triaged module: fx,True
43646,c10::string_view pybind11 custom type caster. triaged enhancement module: pybind,2020-08-26 19:14:56+00:00,,0,1,triaged enhancement module: pybind,True
43638,[FR] hub uses default github branch triaged enhancement module: hub,2020-08-26 18:00:07+00:00,,0,7,triaged enhancement module: hub,True
43623,[FR] Raise an exception when constructing non-empty index and empty values sparse tensor module: sparse module: cuda triaged,2020-08-26 11:17:03+00:00,,1,2,module: sparse module: cuda triaged,True
43617,Type hints from _VariableFunctions and elsewhere clash module: typing triaged,2020-08-26 07:46:48+00:00,,0,1,module: typing triaged,True
43604,ImportError: libtorch_cpu.so: cannot open shared object file: No such file or directory module: build triaged,2020-08-26 01:10:07+00:00,,0,3,module: build triaged,True
43579,"For the same complex dtype and same value, comparing a PyTorch tensor with a NumPy array results in False triaged module: complex",2020-08-25 21:57:44+00:00,,1,8,triaged module: complex,True
43567,`torch.svd()` CUDA gives incorrect results when input contains `nan` triaged module: NaNs and Infs module: linear algebra,2020-08-25 19:14:15+00:00,,0,3,triaged module: NaNs and Infs module: linear algebra,True
43561,[RFC] RPC BENCHMARK triaged module: rpc,2020-08-25 18:14:14+00:00,,1,1,triaged module: rpc,True
43556,FP16 gives NaN loss when using pre-trained model module: cuda triaged module: amp (automated mixed precision),2020-08-25 16:37:14+00:00,,0,3,module: cuda triaged module: amp (automated mixed precision),True
43548,JIT tracing incorrectly records some slice bounds high priority triage review oncall: jit,2020-08-25 11:32:31+00:00,,1,4,high priority triage review oncall: jit,True
43546,"NCCL WARN Your program may be hanging, this may be caused by a collective mismatch around rank 2. Please check your collective calls at and around this rank. You can use NCCL_DEBUG=INFO and NCCL_DEBUG_SUBSYS=COLL to see the collective logs oncall: distributed module: cuda triaged module: nccl",2020-08-25 10:34:29+00:00,,0,8,oncall: distributed module: cuda triaged module: nccl,True
43544,AMP much worse performance with groupped Conv2d than fp32  module: dependency bug module: performance module: cuda triaged module: amp (automated mixed precision),2020-08-25 09:33:13+00:00,,0,10,module: dependency bug module: performance module: cuda triaged module: amp (automated mixed precision),True
43536,"How do I debug ""RuntimeError: trying to initialize the default process group twice!"" oncall: distributed triaged",2020-08-25 02:55:51+00:00,,0,7,oncall: distributed triaged,True
43520,[jit] cannot use functools.partial with TorchScript oncall: jit months,2020-08-24 21:41:17+00:00,,0,6,oncall: jit months,False
43510,Make busy waiting time in DDP (kSynchronizeBusyWaitMillis) a configurable environment variable oncall: distributed triaged,2020-08-24 19:23:10+00:00,,1,2,oncall: distributed triaged,True
43503,torch.quantization.quantize_dynamic document refers `module` as a parameter  module: docs triaged,2020-08-24 16:35:45+00:00,,0,0,module: docs triaged,True
43502,Useless Exception traces when DataSet timing out module: bootcamp module: dataloader triaged,2020-08-24 16:23:12+00:00,,0,6,module: bootcamp module: dataloader triaged,True
43501,[jit] Shape hints for TorchScript oncall: jit,2020-08-24 16:10:41+00:00,,1,5,oncall: jit,False
43490,Implement a set_printoptions method in libtorch  module: cpp triaged enhancement,2020-08-24 12:43:13+00:00,,0,0,module: cpp triaged enhancement,True
43488,"c10::Error: Couldn’t find an operator for aten::dropout(Tensor input, float p, bool train) oncall: mobile",2020-08-24 08:54:38+00:00,,0,4,oncall: mobile,False
43486,scatter not place inputs into every device  triaged,2020-08-24 07:43:44+00:00,,0,2,triaged,False
43484,pybind11_object_dealloc error module: crash triaged module: pybind,2020-08-24 03:13:11+00:00,,0,1,module: crash triaged module: pybind,True
43467,Data loader struct pack issue(overflow)? module: multiprocessing module: dataloader triaged,2020-08-23 07:16:55+00:00,,0,5,module: multiprocessing module: dataloader triaged,True
43465,Memory Profiler: Not profiled memory deallocation in outside of an autograd function triaged oncall: profiler,2020-08-23 06:15:18+00:00,,0,2,triaged oncall: profiler,False
43459,Trying to build PyTorch from source with LLD 8 fails module: build triaged module: static linking,2020-08-23 00:11:57+00:00,,0,2,module: build triaged module: static linking,True
43453,CUDAGuard might not create CUcontext module: cuda triaged,2020-08-22 12:14:11+00:00,,0,2,module: cuda triaged,True
43450,Training Large Neural Networks with Constant Memory using a New Execution Algorithm module: nn module: memory usage triaged needs research,2020-08-22 07:39:36+00:00,,0,4,module: nn module: memory usage triaged needs research,True
43429,Inference performance regression caused by hacky_wrapper_for_legacy_signatures module: performance module: internals triaged module: dispatch,2020-08-21 20:54:42+00:00,,0,14,module: performance module: internals triaged module: dispatch,True
43411,Add support of random state generator objects to nn.init module module: nn triaged enhancement module: initialization,2020-08-21 18:13:46+00:00,,1,2,module: nn triaged enhancement module: initialization,True
43409,Switch C10_EXPORT_CAFFE2_OP_TO_C10 to new operator registration API caffe2 module: bootcamp,2020-08-21 18:01:24+00:00,,0,0,caffe2 module: bootcamp,True
43402,ModuleDict does not preserve order of initializing dictionary module: nn triaged,2020-08-21 12:31:13+00:00,,0,1,module: nn triaged,True
43388,"Abort message: ‘terminating with uncaught exception of type c10::Error: _ivalue_ INTERNAL ASSERT FAILED at ../torch/csrc/jit/api/object.cpp:19, please report a bug to PyTorch. (_ivalue at ../torch/csrc/jit/api/object.cpp:19) oncall: jit oncall: mobile",2020-08-21 02:50:40+00:00,,0,4,oncall: jit oncall: mobile,True
43385,OffsetCalculator.cuh(and THCIntegerDivider.cuh) should be available with PyTorch cpu-only binaries module: internals triaged better-engineering csprng,2020-08-21 01:49:43+00:00,,0,0,module: internals triaged better-engineering csprng,True
43378,[JIT] Using Any type variable inside can produce incorrect results due to type unification problems oncall: jit weeks,2020-08-21 00:50:06+00:00,,1,0,oncall: jit weeks,True
43369,"torch.load(.., map_location='cpu') fails when unserializing cuda tensors on a cpu-only device serialized with pickle module: pickle module: serialization triaged",2020-08-20 23:46:01+00:00,,0,4,module: pickle module: serialization triaged,True
43352,"Remove warning, and update documentation. module: docs module: optimizer triaged",2020-08-20 20:28:09+00:00,,0,2,module: docs module: optimizer triaged,True
43347,backward for dense+sparse does not work module: sparse module: autograd triaged,2020-08-20 19:01:16+00:00,,0,6,module: sparse module: autograd triaged,True
43328,torch.utils.tensorboard.SummaryWriter.add_histogram() execution time explodes with each epoch. oncall: visualization,2020-08-20 11:33:49+00:00,,0,0,oncall: visualization,False
43325,CUDA error: an illegal memory access was encountered (laugh_kernel at ...../cuda/CUDALoops.cuh:112) module: cuda triaged module: cublas,2020-08-20 09:20:48+00:00,,0,2,module: cuda triaged module: cublas,True
43323,Set SummaryWriter step globally oncall: visualization,2020-08-20 08:08:48+00:00,,0,0,oncall: visualization,False
43281,Cannot find CUDA devices when the machine stays idle for a while module: cuda triaged,2020-08-19 19:07:10+00:00,,0,1,module: cuda triaged,True
43261,OSError: could not find class definition when export torchscript module oncall: jit,2020-08-19 09:18:52+00:00,,0,2,oncall: jit,True
43259,RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. oncall: distributed triaged module: data parallel,2020-08-19 08:52:26+00:00,,0,31,oncall: distributed triaged module: data parallel,True
43255,Adding entropy function analogous to SciPy feature triaged module: numpy,2020-08-19 05:42:54+00:00,,0,2,feature triaged module: numpy,True
43254,Vec256<int64_t> does not handle LONG_MAX on minimum module: cpu triaged module: NaNs and Infs module: vectorization module: reductions,2020-08-19 04:58:44+00:00,,0,3,module: cpu triaged module: NaNs and Infs module: vectorization module: reductions,True
43253,Reduce number of stack frames used up by dispatcher module: internals module: bootcamp triaged,2020-08-19 04:43:40+00:00,,0,3,module: internals module: bootcamp triaged,True
43250,Support for Multi-Categorical in torch.distributions module: distributions feature triaged,2020-08-19 03:16:27+00:00,,0,6,module: distributions feature triaged,True
43249,[JIT] Cannot iterate over nn.ModuleList in JITted code when accessed through one level of indirection oncall: jit days,2020-08-19 02:48:15+00:00,,0,3,oncall: jit days,True
43245,mp.spawn 'args' are not clear module: docs module: multiprocessing triaged,2020-08-19 01:56:59+00:00,,0,0,module: docs module: multiprocessing triaged,True
43238,Expose counters for torch.distributed to report communication overhead within a block. oncall: distributed triaged,2020-08-19 00:28:24+00:00,,1,2,oncall: distributed triaged,True
43237,[jit] remove shape/device specialization from tracing after ONNX done with its own shape inference oncall: jit weeks,2020-08-18 23:52:57+00:00,,0,1,oncall: jit weeks,False
43200,Support controlling output delay for CTC loss. module: loss triaged enhancement,2020-08-18 09:53:46+00:00,,0,1,module: loss triaged enhancement,True
43196,model trace error caffe2 triaged,2020-08-18 09:22:56+00:00,,0,3,caffe2 triaged,True
43195,one_hot tensors are channels_last but marked as contiguous triaged module: memory format,2020-08-18 09:11:11+00:00,,0,5,triaged module: memory format,True
43166,Do not call nullptr deleter in at::fromDLPack (dlpack) module: cpp module: bootcamp triaged,2020-08-17 22:08:26+00:00,,0,4,module: cpp module: bootcamp triaged,True
43140,"Pytorch ""Import torch"" lead to ""core dump"" module: binaries module: protobuf triaged",2020-08-17 11:12:35+00:00,,0,32,module: binaries module: protobuf triaged,True
43134,[jit] need a better way to handle mix CPU/GPU (Inference/Training) for tracing oncall: jit,2020-08-17 06:27:49+00:00,,0,2,oncall: jit,True
43128,Accessing elements of tensor with multi-dimensional index results `IndexError` triaged module: numpy module: advanced indexing,2020-08-16 15:18:18+00:00,,0,1,triaged module: numpy module: advanced indexing,True
43127,`onnxifi_load` symbol undefined in `libtorch_cpu.a` library built from source triaged,2020-08-16 13:38:57+00:00,,0,2,triaged,False
43125,"Non-conforming variable identifiers in JIT code errors / printouts: e.g. ""Tensor 0"" oncall: jit",2020-08-16 09:46:13+00:00,,1,4,oncall: jit,True
43123,Add CutMix transform to torchvision.transforms triaged module: vision,2020-08-16 06:46:48+00:00,,0,1,triaged module: vision,True
43120,"When modified the model python file, the pytorch will raise the KeyError of this file triaged module: data parallel",2020-08-16 04:16:47+00:00,,0,5,triaged module: data parallel,True
43119,Tracing can leak CUDA memory when using tensor constants oncall: jit months,2020-08-16 03:40:07+00:00,,1,4,oncall: jit months,False
43116,Accessing tensor by element is super slow module: performance triaged,2020-08-15 23:41:52+00:00,,0,1,module: performance triaged,True
43115,torch.multinomial with replacement=True produces inaccurate results for large number of categories module: numerical-stability module: distributions triaged,2020-08-15 22:38:22+00:00,,0,0,module: numerical-stability module: distributions triaged,True
43114,"out kwargs are sometimes inconsistent with returned named tuple field name (and in some cases, cannot be made consistent) triaged module: codegen",2020-08-15 22:17:51+00:00,,0,3,triaged module: codegen,True
43113,Missing caffe2_pybind11_state in pip install after cmake. module: build triaged module: pybind,2020-08-15 21:59:48+00:00,,0,0,module: build triaged module: pybind,True
43109,[docs] LR schedulers' get_last_lr missing in docs online module: docs triaged,2020-08-15 16:50:13+00:00,,0,1,module: docs triaged,False
43101,New function for CTC decoding module: nn triaged enhancement needs design,2020-08-15 07:45:23+00:00,,0,6,module: nn triaged enhancement needs design,True
43091,Enable torch.optim typechecks during CI  module: typing triaged,2020-08-15 00:22:05+00:00,,0,0,module: typing triaged,True
43072,LSTM::permute_hidden breaks Liskov substitution principle module: rnn module: typing triaged,2020-08-14 20:24:09+00:00,,0,1,module: rnn module: typing triaged,True
43064,torch.dot throws an error for input tensors of different dtypes triaged enhancement module: numpy module: type promotion,2020-08-14 17:09:05+00:00,,0,3,triaged enhancement module: numpy module: type promotion,True
43040,undefined symbol: FLAGS_caffe2_keep_on_shrink caffe2 triaged,2020-08-14 01:03:25+00:00,,0,2,caffe2 triaged,True
43033,No vectorization for int8 and uint8 triaged module: vectorization better-engineering,2020-08-13 22:32:06+00:00,,0,3,triaged module: vectorization better-engineering,True
43030,BUILD_PYTHON switched to OFF at second run of CMake module: build triaged,2020-08-13 21:31:38+00:00,,0,0,module: build triaged,True
43012,Implement map-style caching DataSet as PyTorch build-in DataSet. module: bootcamp module: dataloader triaged,2020-08-13 14:45:21+00:00,,0,5,module: bootcamp module: dataloader triaged,True
43005,`torch.distributions.Categorical` crashes with illegal instruction module: crash module: distributions triaged,2020-08-13 13:00:24+00:00,,0,6,module: crash module: distributions triaged,True
42971,Enable torch.quantization.fuse_modules typechecks during CI  module: typing triaged,2020-08-13 04:56:32+00:00,,0,0,module: typing triaged,True
42969,Enable torch.testing._internal typechecks during CI  module: typing triaged,2020-08-13 04:54:56+00:00,,1,2,module: typing triaged,True
42959,[feature request] Faster specialized int16->float32 conversions to match speed with NumPy module: performance module: cpu triaged,2020-08-13 01:17:10+00:00,,0,23,module: performance module: cpu triaged,True
42950,TorchBind C++ Enum Class triaged enhancement module: torchbind,2020-08-12 22:55:27+00:00,,0,1,triaged enhancement module: torchbind,True
42949,Verify TorchBind works with nested class type triaged enhancement module: torchbind,2020-08-12 22:54:19+00:00,,0,0,triaged enhancement module: torchbind,True
42948,[jit] TorchBind std::chrono::milliseconds  oncall: jit,2020-08-12 22:51:57+00:00,,0,1,oncall: jit,False
42947,[jit] std::exception_ptr can't be torchbind/convert to ivalue oncall: jit,2020-08-12 22:50:43+00:00,,0,3,oncall: jit,True
42919,Legacy Python2 and early Python3 leftovers triaged better-engineering shadow review OSS contribution wanted,2020-08-12 15:41:44+00:00,,0,4,triaged better-engineering shadow review OSS contribution wanted,True
42916,Build problems caffe2 -- pytorch from source & CUDA 11.0 module: build module: cuda triaged,2020-08-12 14:16:39+00:00,,0,3,module: build module: cuda triaged,True
42912,"Add typing information into variable declarations for JIT script "".code"" output oncall: jit feature triaged days",2020-08-12 13:17:08+00:00,,0,0,oncall: jit feature triaged days,True
42911,Converting from .pth to .onnx failed on 3d-input. Support 3d-conv? module: onnx triaged onnx-needs-info,2020-08-12 13:14:35+00:00,,0,6,module: onnx triaged onnx-needs-info,False
42902,Avoid dynamic isCustomClassRegistered() checks in kernel call paths triaged module: dispatch,2020-08-12 06:32:58+00:00,,0,1,triaged module: dispatch,True
42885,Unable to call `super` method with TorchScript triage review oncall: jit TSRootCause:DynamicBehaviors TSUsability TSRootCause:ModuleInheritance,2020-08-11 22:24:54+00:00,,0,8,triage review oncall: jit TSRootCause:DynamicBehaviors TSUsability TSRootCause:ModuleInheritance,True
42882,Support for defining/saving custom operators with dynamic schema oncall: jit triaged,2020-08-11 22:14:11+00:00,,1,1,oncall: jit triaged,True
42879,Tensorpipe requires setting both GLOO_SOCKET_IFNAME and TP_SOCKET_IFNAME oncall: distributed triaged,2020-08-11 20:31:44+00:00,,0,5,oncall: distributed triaged,True
42863,TensorOptions extensibility has rusted shut triaged module: codegen,2020-08-11 16:28:04+00:00,,0,3,triaged module: codegen,True
42861,Can not get pytorch working with tensorboard triaged module: tensorboard,2020-08-11 16:09:45+00:00,,0,2,triaged module: tensorboard,True
42858,"prelu_backward, hardshrink_backward shouldn't be a method triaged module: dispatch",2020-08-11 15:17:26+00:00,,0,0,triaged module: dispatch,True
42849,[RFC] DeepSpeed + PT Distributed Integration oncall: distributed feature triaged,2020-08-11 03:24:09+00:00,,0,20,oncall: distributed feature triaged,True
42848,Add support for user defined types in serialization in libtorch module: cpp feature module: serialization triaged,2020-08-11 02:14:34+00:00,,0,0,module: cpp feature module: serialization triaged,True
42847,Get test_jit.py below 10k lines oncall: jit module: tests enhancement better-engineering,2020-08-11 01:54:13+00:00,,0,5,oncall: jit module: tests enhancement better-engineering,False
42843,Documentation mistake of Adam in v1.6.0? module: docs module: optimizer triaged,2020-08-11 01:17:41+00:00,,0,7,module: docs module: optimizer triaged,True
42838,"RuntimeError: ""triangular_solve_cuda"" not implemented for 'Half' module: cuda triaged module: half module: amp (automated mixed precision)",2020-08-11 00:02:09+00:00,,1,5,module: cuda triaged module: half module: amp (automated mixed precision),True
42830,Undocumented parameters in the Variables section of the recurrent bidirectional layers regarding the backward pass module: docs triaged,2020-08-10 20:36:01+00:00,,0,0,module: docs triaged,True
42829,torch/library.h doc rendering issue module: docs triaged,2020-08-10 20:22:42+00:00,,0,0,module: docs triaged,True
42818,Can't registered boxed kernel for operator that doesn't support boxing triaged module: dispatch,2020-08-10 18:08:14+00:00,,0,4,triaged module: dispatch,True
42815,Add a guide for what to do when you think there's a memory leak high priority module: docs module: memory usage triaged,2020-08-10 17:19:13+00:00,,0,1,high priority module: docs module: memory usage triaged,True
42812,checkpoint_sequential breaks backpropagation module: checkpoint module: autograd triaged,2020-08-10 16:52:11+00:00,,0,2,module: checkpoint module: autograd triaged,True
42807,build error redefinition of ‘struct c10::complex<T>’ with release v1.6.0 module: build triaged,2020-08-10 12:08:17+00:00,,0,3,module: build triaged,True
42805,Unbuffered operation triaged enhancement module: numpy function request,2020-08-10 09:41:13+00:00,,0,1,triaged enhancement module: numpy function request,True
42800,RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:27 for spectral norm triaged module: cublas,2020-08-10 08:01:39+00:00,,0,4,triaged module: cublas,True
42793,distributed training with c10d does not work with layerdrop in pytorch > 1.4 high priority oncall: distributed triaged,2020-08-09 23:09:21+00:00,,1,14,high priority oncall: distributed triaged,True
42791,Build fail on Ubuntu 18.04: caffe2/CMakeFiles/init_test.dir/build.make:106: recipe for target 'bin/init_test' failed module: build triaged,2020-08-09 12:56:00+00:00,,0,3,module: build triaged,True
42784,libtorch 1.5 crashes when used on macs when using torch::max without AVX support module: crash module: cpu triaged module: macos,2020-08-08 20:19:35+00:00,,0,1,module: crash module: cpu triaged module: macos,True
42779,[quant] Quantized AdaptivePool3d is much slower for ChannelsLast3d. module: performance oncall: quantization low priority triaged module: memory format,2020-08-08 14:19:05+00:00,,0,8,module: performance oncall: quantization low priority triaged module: memory format,True
42774,Missing API reference for SWALR and AveragedModel module: docs triaged,2020-08-08 05:35:15+00:00,,0,0,module: docs triaged,True
42729,Mismatch in docs and behavior of align_corners for nn.functional.interpolate module: docs module: nn triaged,2020-08-07 10:47:19+00:00,,1,5,module: docs module: nn triaged,True
42722,How to build libtorch static libraries on Windows? module: binaries module: build module: windows triaged windows-triaged,2020-08-07 02:37:24+00:00,,0,13,module: binaries module: build module: windows triaged windows-triaged,True
42705,torch.distributed.rpc package not work well with generator and lambda triaged module: rpc,2020-08-06 22:08:29+00:00,,0,1,triaged module: rpc,True
42689,How can I specify NumPy while building PyTorch? module: build triaged,2020-08-06 18:31:24+00:00,,0,0,module: build triaged,True
42673,"Unexpected behavior of ""to"" method inside a torch.jit.script decorated function oncall: jit days TSRootCause:DefaultTypes TSUsability",2020-08-06 16:05:41+00:00,,2,7,oncall: jit days TSRootCause:DefaultTypes TSUsability,True
42671,[discussion] Support __round__ magic triaged enhancement module: numpy,2020-08-06 15:58:41+00:00,,0,3,triaged enhancement module: numpy,True
42665,Optimizer support via Libtorch C++ on Android module: cpp module: optimizer triaged oncall: mobile,2020-08-06 11:06:22+00:00,,0,7,module: cpp module: optimizer triaged oncall: mobile,True
42663,Why Conv3D is slower than Conv2D when its flops is smaller than Conv2D module: performance module: cudnn module: convolution triaged,2020-08-06 07:42:11+00:00,,0,4,module: performance module: cudnn module: convolution triaged,True
42655,"When one distributed test fails in CI, the next one can fail spuriously oncall: distributed module: ci module: tests triaged",2020-08-06 03:56:10+00:00,,0,0,oncall: distributed module: ci module: tests triaged,True
42654,RuntimeError: each element in list of batch should be of equal size  module: dataloader triaged,2020-08-06 03:10:56+00:00,,0,25,module: dataloader triaged,True
42653,onnx export failed when output size are not factor of input size for adaptive_avg_pool2d module: onnx triaged onnx-triaged,2020-08-06 03:09:38+00:00,,0,27,module: onnx triaged onnx-triaged,True
42652,Add non_blocking=True copy of torch.Storage triaged needs research,2020-08-06 02:41:13+00:00,,0,5,triaged needs research,False
42625,F.grid_sample produces weird results on single-pixel images module: nn triaged,2020-08-05 19:47:01+00:00,,0,2,module: nn triaged,True
42597,Logsigmoid as chaining operation triaged function request,2020-08-05 12:50:47+00:00,,0,6,triaged function request,True
42594,support LSTM for quantization aware training oncall: quantization low priority triaged,2020-08-05 11:24:40+00:00,,0,3,oncall: quantization low priority triaged,True
42592,Lower performance when forwarding JIT model with libTorch 1.6 compared with 1.5 oncall: jit,2020-08-05 10:15:18+00:00,,0,2,oncall: jit,True
42581,Build script complains that gcc is < 6 even though gcc version is 8 needs reproduction triaged,2020-08-05 03:37:51+00:00,,0,6,needs reproduction triaged,True
42545,CuDNN RNN bindings are basically all deprecated in cudnn 8 module: cudnn module: rnn triaged module: deprecation,2020-08-04 17:04:19+00:00,,0,2,module: cudnn module: rnn triaged module: deprecation,True
42544,torch rpc lose device information between rpc calls triaged module: rpc,2020-08-04 16:53:56+00:00,,0,6,triaged module: rpc,True
42539,Docs for Tensor.copy_ non_blocking should mention pinned memory module: docs triaged,2020-08-04 15:15:12+00:00,,0,0,module: docs triaged,False
42529,renorm dim argument is extremely confusing module: docs triaged better-engineering module: ux,2020-08-04 14:24:37+00:00,,0,0,module: docs triaged better-engineering module: ux,True
42526,Python3 segfaults or doesn't load torch after installation module: build triaged,2020-08-04 12:24:04+00:00,,0,4,module: build triaged,True
42502,[proposal] batch mode for randperm triaged enhancement module: random,2020-08-04 02:09:34+00:00,,0,4,triaged enhancement module: random,True
42498,pytorch tests failed module: tests triaged,2020-08-03 23:26:13+00:00,,0,3,module: tests triaged,True
42487,Support recursive data type in TorchScript high priority oncall: jit triaged weeks,2020-08-03 21:07:35+00:00,,1,2,high priority oncall: jit triaged weeks,True
42481,[Macos][CircleCI] Test failed in test_dataloader.py module: ci triaged module: macos,2020-08-03 20:20:03+00:00,,0,1,module: ci triaged module: macos,True
42478,Using hooks with `DataParallel` gets `autograd` error triaged module: data parallel,2020-08-03 19:12:54+00:00,,0,2,triaged module: data parallel,True
42460,Training with AMP gives overflow warning module: cuda triaged module: amp (automated mixed precision),2020-08-03 15:49:26+00:00,,0,1,module: cuda triaged module: amp (automated mixed precision),True
42459,[JIT] Runtime error when backpropagating through input gradient oncall: jit triaged,2020-08-03 15:40:02+00:00,,1,3,oncall: jit triaged,True
42452,`device-side assert triggered` - when probability > 1.0 module: crash module: cuda triaged better-engineering,2020-08-03 13:05:45+00:00,,0,1,module: crash module: cuda triaged better-engineering,True
42447,OneCycleLR argument `pct_start` used as a proportion not percentage module: docs module: optimizer triaged,2020-08-03 10:36:52+00:00,,0,0,module: docs module: optimizer triaged,True
42444,torch.random.randperm stuck in multiprocess module: multiprocessing triaged module: random,2020-08-03 09:47:52+00:00,,0,7,module: multiprocessing triaged module: random,True
42440,[feature request].Time Series Algorithms feature triaged,2020-08-03 08:51:40+00:00,,0,0,feature triaged,False
42439,How to use libtorch on Jetson TX2 module: binaries triaged module: arm,2020-08-03 06:14:11+00:00,,0,3,module: binaries triaged module: arm,True
42428,KeyError: xxxxxxxxxx when calling optimizer.state_dict() module: optimizer triaged,2020-08-02 20:25:21+00:00,,0,6,module: optimizer triaged,True
42427,./scripts/build_android.sh need support nn::module nn::Functional nn::Linear feature triaged module: android oncall: mobile,2020-08-02 19:48:49+00:00,,0,1,feature triaged module: android oncall: mobile,True
42407,torch.distributions.multinomial.Multinomial cannot be used in batch module: distributions feature triaged,2020-08-01 13:51:03+00:00,,0,11,module: distributions feature triaged,True
42405,slow data loading in VisionDataset  - need to allow batch loading. module: performance feature module: dataloader triaged module: vision,2020-08-01 12:33:07+00:00,,0,6,module: performance feature module: dataloader triaged module: vision,True
42402,Reading data speed slower than tensorflow module: performance module: dataloader triaged,2020-08-01 06:50:18+00:00,,0,8,module: performance module: dataloader triaged,True
42379,TensorBoard Summaries Written with `SummaryWriter.add_scalars()` are not Purged  module: tensorboard oncall: visualization,2020-07-31 19:32:33+00:00,,0,0,module: tensorboard oncall: visualization,False
42368,RFC: torch.vmap triaged module: vmap,2020-07-31 15:53:04+00:00,,1,41,triaged module: vmap,True
42366,Stop implementing JIT stack as a std::vector oncall: jit triaged weeks,2020-07-31 15:27:20+00:00,,0,4,oncall: jit triaged weeks,True
42362,libpytorch_jni is not provided in conda builds module: build triaged,2020-07-31 12:14:54+00:00,,0,0,module: build triaged,True
42355,nn::Sequential Link error in Android version module: android oncall: mobile,2020-07-31 07:38:44+00:00,,0,1,module: android oncall: mobile,True
42350,An error occurred deploying CRNN using libtorch oncall: jit module: rnn module: cuda triaged,2020-07-31 02:07:42+00:00,,1,4,oncall: jit module: rnn module: cuda triaged,True
42345,Generate unique `worker_id` for each node in the RPC framework for the store. triaged module: rpc,2020-07-31 00:37:58+00:00,,0,1,triaged module: rpc,True
42336,Audit quantization functions to ensure proper argument sizes oncall: quantization low priority triaged,2020-07-30 22:35:13+00:00,,0,3,oncall: quantization low priority triaged,True
42332,RuntimeError: ProcessGroupNCCL does not support recv (torch-nightly) oncall: distributed feature triaged module: nccl,2020-07-30 20:47:31+00:00,,0,5,oncall: distributed feature triaged module: nccl,True
42317,Instance norm annotation is incorrect high priority triage review oncall: jit days,2020-07-30 16:34:16+00:00,,0,0,high priority triage review oncall: jit days,True
42316,Proper testing of `nn.Module` loading backward compatibility module: nn module: tests module: serialization triaged,2020-07-30 16:06:23+00:00,,0,0,module: nn module: tests module: serialization triaged,True
42311,Inconsistent results when trying to enable Tensor Cores on NVIDIA T4 module: cudnn triaged module: memory format module: amp (automated mixed precision),2020-07-30 15:25:19+00:00,,0,8,module: cudnn triaged module: memory format module: amp (automated mixed precision),True
42308,Not able to run quantized model on android oncall: mobile,2020-07-30 14:31:45+00:00,,0,3,oncall: mobile,False
42303,frame_length reference in pytorch.stft documentation module: docs triaged,2020-07-30 12:44:30+00:00,,0,2,module: docs triaged,False
42301,Is it possible that one-version pytorch supports two gpu (GTX TITAN X and TITAN) using at the same time? module: cuda triaged,2020-07-30 12:23:49+00:00,,0,1,module: cuda triaged,True
42295,KeyError: 'track_running_stats' in batchnorm.extra_repr module: printing module: serialization triaged,2020-07-30 07:43:22+00:00,,0,1,module: printing module: serialization triaged,True
42267,assertRaisesRegxWithHighlight isn't triggered correctly oncall: jit,2020-07-30 00:25:50+00:00,,1,0,oncall: jit,False
42258,Cannot load certain function from dumped Torchscript file high priority triage review oncall: jit days,2020-07-29 22:11:06+00:00,,1,11,high priority triage review oncall: jit days,True
42246,"`torch.tensor([1, 2], dtype=torch.int).fmod(torch.tensor(0, dtype=torch.float))` leads to ""RuntimeError: result type Float can't be cast to the desired output type Int"" triaged module: type promotion",2020-07-29 19:33:56+00:00,,0,2,triaged module: type promotion,True
42228,[docs] Bitwise ops miss arguments module: docs triaged,2020-07-29 09:03:12+00:00,,0,1,module: docs triaged,False
42223,Implementing packbits feature triaged module: numpy,2020-07-29 05:48:09+00:00,,0,20,feature triaged module: numpy,True
42207,torchscript does not support unicode strings oncall: jit months,2020-07-29 00:43:09+00:00,,0,4,oncall: jit months,False
42188,Eliminate warning when cloning a tensor using `torch.tensor(x)` module: docs triaged,2020-07-28 20:25:23+00:00,,0,6,module: docs triaged,True
42183,Skip all reduce globally unused parameters in DDP oncall: distributed triaged enhancement module: data parallel,2020-07-28 19:11:48+00:00,,0,5,oncall: distributed triaged enhancement module: data parallel,True
42177,record_function no longer appears in the docs module: docs triaged,2020-07-28 17:33:56+00:00,,1,3,module: docs triaged,False
42173,Exclusive flag in torch.cumsum triaged function request,2020-07-28 15:50:32+00:00,,0,1,triaged function request,False
42170,Tracing a[b] records converting b to fixed device type of a oncall: jit,2020-07-28 14:24:26+00:00,,0,6,oncall: jit,False
42159,[docs] Add example of interop with DLPack created in plain C module: docs triaged enhancement,2020-07-28 12:24:28+00:00,,0,0,module: docs triaged enhancement,False
42156,"RuntimeError: test_autograd failed! Build success, test failed on IBM POWER9 module: autograd triaged module: POWER",2020-07-28 11:34:41+00:00,,0,0,module: autograd triaged module: POWER,True
42155,BusError memory-mapped tensor module: internals triaged,2020-07-28 07:50:50+00:00,,0,13,module: internals triaged,True
42145,Support Python type() in TorchScript oncall: jit,2020-07-28 00:11:31+00:00,,0,3,oncall: jit,False
42112,Function names in title should not be capitalized in documentation module: docs triaged,2020-07-27 16:12:50+00:00,,0,0,module: docs triaged,False
42109,Slow `index_add_` on `torch.long` tensors  module: performance module: cuda triaged,2020-07-27 15:21:53+00:00,,0,6,module: performance module: cuda triaged,True
42095,[RFC] Add Windows support to torch.distributed package oncall: distributed module: windows triaged windows-triaged,2020-07-27 04:51:00+00:00,,1,23,oncall: distributed module: windows triaged windows-triaged,True
42094,Setting threads number to the number of default by torch.set_num_threads is faster than not setting it triaged module: multithreading,2020-07-27 04:04:58+00:00,,1,1,triaged module: multithreading,True
42081,Document optimizations enabled by JIT scripting and tracing (TorchScript) oncall: jit days,2020-07-26 12:55:01+00:00,,0,0,oncall: jit days,False
42080,To have single cuda context across multiple processes module: cuda triaged,2020-07-26 11:24:27+00:00,,0,17,module: cuda triaged,True
42074,Memory usage of torch.nn.functional.interpolate increased with v1.5.0 when run on numpy input module: nn module: cuda module: memory usage triaged module: numpy,2020-07-26 01:48:59+00:00,,0,2,module: nn module: cuda module: memory usage triaged module: numpy,True
42053,Function request: np.copy (alias of clone?) triaged module: numpy function request,2020-07-25 07:15:16+00:00,,0,11,triaged module: numpy function request,True
42049,"Code hangs when using `set_start_method('spawn', force=True)` in `torch.multiprocessing.pool` module: multiprocessing triaged",2020-07-25 05:42:09+00:00,,0,1,module: multiprocessing triaged,True
42039,Segmentation fault (core dumped) when running optimize_for_mobile oncall: mobile,2020-07-24 22:01:24+00:00,,0,3,oncall: mobile,True
42015,Support additional arguments in nn.Identity.forward module: nn triaged,2020-07-24 16:03:01+00:00,,0,4,module: nn triaged,True
42012,EMNIST looks different to MNIST triaged module: vision,2020-07-24 15:15:31+00:00,,0,1,triaged module: vision,True
41999,PyBind11 submodule required even when pybind11_PREFER_third_party=OFF module: build triaged,2020-07-24 10:14:18+00:00,,0,1,module: build triaged,True
41970,torch.random.fork tries to initialize cuda even when no cuda devices are available module: cuda triaged module: random,2020-07-24 02:32:16+00:00,,0,1,module: cuda triaged module: random,True
41955,can not use nn::Functional(torch::softmax(-1)) in Sequential module: cpp module: nn triaged,2020-07-23 22:41:09+00:00,,0,1,module: cpp module: nn triaged,True
41950,[docs] Clarify behavior of torch.cuda.device_count() when torch compiled without CUDA (cpu-only) or when CUDA is not available module: docs module: cuda triaged,2020-07-23 21:50:59+00:00,,0,0,module: docs module: cuda triaged,True
41938,Installing pytorch from source on Power9 (PPC64LE) + CUDA 10.2 + RHLE7  module: build triaged module: POWER,2020-07-23 19:29:47+00:00,,0,1,module: build triaged module: POWER,True
41926,torch.exp() cannot be modified by an inplace operation module: docs module: autograd triaged,2020-07-23 14:51:17+00:00,,0,3,module: docs module: autograd triaged,True
41921,Failure to compile Eigen on Power with clang module: build triaged module: third_party,2020-07-23 13:15:14+00:00,,0,0,module: build triaged module: third_party,True
41920,Compilation on Power fails with clang due to vec_xl module: build triaged module: POWER,2020-07-23 13:03:09+00:00,,0,0,module: build triaged module: POWER,True
41918,ASAN build breaks when using third_party/protobuf module: build module: protobuf triaged module: POWER,2020-07-23 12:07:37+00:00,,0,1,module: build module: protobuf triaged module: POWER,True
41916,ASAN build broken when using USE_ASAN=1 module: build triaged module: POWER,2020-07-23 12:02:49+00:00,,0,4,module: build triaged module: POWER,True
41915,Automating the various Installation processes for Linux module: build feature triaged,2020-07-23 11:18:53+00:00,,0,0,module: build feature triaged,True
41909,Dropout in TorchScript under torch.no_grad() produces incoherent results oncall: jit,2020-07-23 05:43:16+00:00,,1,0,oncall: jit,False
41904,Inconsistencies on ModelLayer caffe2-op triaged better-engineering,2020-07-23 03:47:40+00:00,,0,2,caffe2-op triaged better-engineering,True
41903,[C++] adding type checking or type casting to torch::PackedTensorAccessor indexing module: cpp triaged enhancement,2020-07-23 03:47:01+00:00,,0,0,module: cpp triaged enhancement,True
41869,TorchScript pack_padded_sequence and  pad_packed_sequence run time error oncall: jit,2020-07-22 20:29:37+00:00,,0,13,oncall: jit,True
41852,Difference in inference time between CUDA 10.0 & 10.2 module: performance module: cudnn module: cuda triaged,2020-07-22 14:21:46+00:00,,0,12,module: performance module: cudnn module: cuda triaged,True
41844,test_bottleneck_cuda fails on Power module: tests triaged module: POWER,2020-07-22 07:52:58+00:00,,0,0,module: tests triaged module: POWER,True
41839,"[Feature Request] Add `to`, `cpu`, and `cuda` method to optimizer feature module: optimizer triaged",2020-07-22 06:00:16+00:00,,0,4,feature module: optimizer triaged,False
41833,"Error occurs when adding ""max_unpool2d"" to onnx module: onnx triaged enhancement",2020-07-22 01:51:56+00:00,,0,10,module: onnx triaged enhancement,True
41823,bitwise_or / bitwise_and /... reductions across dim (+ multidim / keepdim for consistency) triaged enhancement,2020-07-21 22:40:49+00:00,,0,1,triaged enhancement,True
41816,install path can set prefix install dir  module: build triaged,2020-07-21 21:27:17+00:00,,0,2,module: build triaged,True
41803,out-variant for tensor.bitwise_and (exists for torch.bitwise_and) + bitwise_friends triaged enhancement module: numpy needs research module: ux function request,2020-07-21 16:18:10+00:00,,0,2,triaged enhancement module: numpy needs research module: ux function request,True
41801,Caffe2 GPU Inference not working on pytorch nightly version caffe2 triaged,2020-07-21 16:01:24+00:00,,0,0,caffe2 triaged,True
41797,[C++] Libtorch error in Release build only  module: abi triaged,2020-07-21 13:09:13+00:00,,0,6,module: abi triaged,True
41793,General reduction mode selection for in-place and out-variants for wider range (hopefully all) of ops triaged function request module: reductions,2020-07-21 09:36:53+00:00,,0,0,triaged function request module: reductions,True
41695,Improve overload resolution order rules for `invokeOperatorFromPython` high priority oncall: jit,2020-07-20 21:28:43+00:00,,1,1,high priority oncall: jit,True
41694,Torch.multiprocessing.spawn can deadlock module: multiprocessing triaged,2020-07-20 21:21:17+00:00,,0,0,module: multiprocessing triaged,True
41683,'torch::jit::script::ErrorReport' from 'torch::jit::load('modelpath') in Visual Studio 2017 oncall: jit,2020-07-20 16:24:36+00:00,,1,5,oncall: jit,True
41681,TypeError: cannot create 'generator' instances triaged module: data parallel,2020-07-20 16:00:41+00:00,,0,3,triaged module: data parallel,True
41673,"../caffe2/perfkernels/common_avx2.cc:17:2: error: #error ( ""You found a build system error: __AVX2__ is defined (via e.g. -mavx2) "" ""but CAFFE2_PERF_WITH_AVX2 is not defined.""); module: build triaged",2020-07-20 13:46:40+00:00,,0,2,module: build triaged,True
41667,[Feature Request] named tensor support for `tensordot`(tensor contract) triaged enhancement module: named tensor,2020-07-20 08:18:40+00:00,,0,1,triaged enhancement module: named tensor,True
41661,libgcc_s.so.1 must be installed for pthread_cancel to work high priority module: binaries triaged,2020-07-20 03:36:31+00:00,,0,40,high priority module: binaries triaged,True
41660,Potential bug in net_printer caffe2,2020-07-20 02:36:37+00:00,,0,0,caffe2,True
41653,Enable and disable autograd profiler without context manager mechanics triaged enhancement oncall: profiler,2020-07-19 16:14:36+00:00,,0,0,triaged enhancement oncall: profiler,False
41639,Error with minimal hogwild test (multiprocessing shared memory) module: multiprocessing triaged,2020-07-18 23:51:31+00:00,,0,1,module: multiprocessing triaged,True
41625,Bring Python torch.slice bindings back or add step argument to torch.narrow triaged enhancement needs research function request,2020-07-18 11:20:14+00:00,,0,9,triaged enhancement needs research function request,False
41623,Can we have a way to reset a scheduler back to epoch -1 module: optimizer triaged enhancement,2020-07-18 09:08:53+00:00,,1,1,module: optimizer triaged enhancement,True
41622,New Feature : A very fast algorithm for computing matrix rank triaged enhancement module: numpy module: linear algebra,2020-07-18 06:01:23+00:00,,0,5,triaged enhancement module: numpy module: linear algebra,True
41571,"torch.cuda.BoolTensor uses 8 bits per element, not 1 bit as reported by element_size() module: docs triaged",2020-07-17 01:18:54+00:00,,0,11,module: docs triaged,True
41561,[RPC] Should we support users _not_ calling rpc.shutdown()? triaged module: rpc,2020-07-16 21:10:54+00:00,,0,13,triaged module: rpc,True
41559,momentum in BatchNorm module: docs module: nn triaged needs research,2020-07-16 21:06:22+00:00,,0,6,module: docs module: nn triaged needs research,True
41556,Implement LSH Optimizations for Enhanced CPU-Only Performance module: performance feature module: cpu triaged needs research,2020-07-16 20:45:28+00:00,,0,0,module: performance feature module: cpu triaged needs research,False
41531,Test failure in test_shared_allgather_nccl: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:537 oncall: distributed triaged module: POWER,2020-07-16 13:12:18+00:00,,0,4,oncall: distributed triaged module: POWER,True
41530,Parallel computation of the diagonal of a Jacobian feature module: autograd triaged,2020-07-16 12:35:57+00:00,,0,0,feature module: autograd triaged,True
41528,"torch.nn.functional.grid_sample()is doing bilinear interpolation when the input is 5D, i think the mode should add 'trilinear' module: nn triaged",2020-07-16 08:12:57+00:00,,0,9,module: nn triaged,True
41525,c++ indexing vs python module: cpp triaged,2020-07-16 06:47:50+00:00,,0,2,module: cpp triaged,True
41513,Python object None check not supported oncall: jit days TSUsability TSRootCause:UnsupportedConstructs,2020-07-16 00:46:00+00:00,,1,1,oncall: jit days TSUsability TSRootCause:UnsupportedConstructs,False
41512,Tensor.new_tensor is not supported oncall: jit days TSUsability TSRootCause:BetterEngineering,2020-07-16 00:42:51+00:00,,0,2,oncall: jit days TSUsability TSRootCause:BetterEngineering,False
41508,nn.MultiheadAttention causes gradients to become NaN under some use cases high priority module: nn triaged module: NaNs and Infs oncall: transformer/mha,2020-07-16 00:03:06+00:00,,0,69,high priority module: nn triaged module: NaNs and Infs oncall: transformer/mha,True
41499,Helping test example code blocks in the docs module: docs feature module: tests triaged,2020-07-15 21:39:01+00:00,,1,9,module: docs feature module: tests triaged,False
41492,Make torch.iinfo/torch.finfo torchscriptable triage review oncall: jit days,2020-07-15 19:20:30+00:00,,0,1,triage review oncall: jit days,True
41486,"LSTMs leak memory in CPU PyTorch 1.5.1, 1.6, and 1.7 on Linux high priority module: rnn module: cpu module: memory usage triaged",2020-07-15 18:02:16+00:00,,0,33,high priority module: rnn module: cpu module: memory usage triaged,True
41485,GLOO infiniband with PyTorch oncall: distributed feature triaged,2020-07-15 17:42:06+00:00,,0,1,oncall: distributed feature triaged,False
41478,Implement backend fallback for Tracer module: internals triaged,2020-07-15 16:32:17+00:00,,0,2,module: internals triaged,True
41433,[JIT][to-backend] `selective_to_backend` infra oncall: jit,2020-07-14 23:31:43+00:00,,0,0,oncall: jit,False
41428,Add a done() API to torch.futures.Future and ProcessGroup::Work oncall: distributed triaged,2020-07-14 21:25:45+00:00,,0,1,oncall: distributed triaged,True
41419,[jit] Support NamedTuple in tracing oncall: jit module: bootcamp days,2020-07-14 17:46:42+00:00,,0,0,oncall: jit module: bootcamp days,True
41418,[jit] support `rpc_remote` and `rpc_sync` oncall: jit days,2020-07-14 17:45:13+00:00,,0,1,oncall: jit days,False
41417,[jit] support class polymorphism oncall: jit months TSRootCause:DynamicBehaviors TSUsability TSRootCause:UnsupportedConstructs,2020-07-14 17:42:31+00:00,,1,6,oncall: jit months TSRootCause:DynamicBehaviors TSUsability TSRootCause:UnsupportedConstructs,False
41416,[jit] support for generators and `yield` oncall: jit months TSUsability TSRootCause:UnsupportedConstructs,2020-07-14 17:40:49+00:00,,0,3,oncall: jit months TSUsability TSRootCause:UnsupportedConstructs,False
41415,[jit] `set` builtin support oncall: jit weeks TSUsability TSRootCause:UnsupportedConstructs,2020-07-14 17:37:40+00:00,,0,0,oncall: jit weeks TSUsability TSRootCause:UnsupportedConstructs,False
41400,test_jit.py fails on Power oncall: jit triaged,2020-07-14 13:43:04+00:00,,1,4,oncall: jit triaged,True
41399,Bottleneck when publishing the model using flask about 3 times slower. module: performance module: cpu triaged,2020-07-14 12:45:47+00:00,,0,1,module: performance module: cpu triaged,True
41392,"About the description of the mathematical formula of nn.RNN, I think it is wrong module: docs triaged",2020-07-14 09:25:12+00:00,,0,3,module: docs triaged,True
41383,Large overhead (7 microseconds) for PyTorch operation module: performance triaged,2020-07-14 03:57:35+00:00,,0,3,module: performance triaged,True
41369,[RFC] Device Placement API for RPC  feature triaged module: rpc module: tensorpipe,2020-07-13 22:32:07+00:00,,0,7,feature triaged module: rpc module: tensorpipe,True
41353,[jit] support for torch.distributed primitives oncall: jit weeks,2020-07-13 17:58:01+00:00,,0,13,oncall: jit weeks,False
41337,Distributed tests fail with pytest oncall: distributed triaged,2020-07-13 13:24:18+00:00,,1,4,oncall: distributed triaged,True
41329,Reference the randomness issue in DataLoader & Dataset documentation. module: docs module: dataloader triaged,2020-07-13 06:16:24+00:00,,1,4,module: docs module: dataloader triaged,True
41327,torch.nn.parallel.scatter_gather.gather can't gather outputs that are dataclasses oncall: distributed triaged module: data parallel,2020-07-13 06:15:32+00:00,,0,4,oncall: distributed triaged module: data parallel,True
41325,torch.combinations() - Tried to allocate 7869836414.81 GiB module: cuda module: memory usage triaged,2020-07-12 22:44:50+00:00,,0,1,module: cuda module: memory usage triaged,True
41323,[discussion] Comparison operator chaining triaged enhancement module: boolean tensor,2020-07-12 14:41:50+00:00,,0,1,triaged enhancement module: boolean tensor,True
41322,SGD documentatiuon detail on g_{t+1} module: optimizer triaged,2020-07-12 14:23:22+00:00,,0,2,module: optimizer triaged,True
41311,ConvTranspose1d layer behaviour under different channel numbers module: docs module: convolution triaged,2020-07-11 19:55:54+00:00,,0,0,module: docs module: convolution triaged,True
41292,"[RFC, Tracker] DataLoader improvements module: dataloader triaged",2020-07-11 00:23:22+00:00,,0,19,module: dataloader triaged,True
41291,Return list of tuples from custom operator oncall: jit,2020-07-11 00:15:54+00:00,,1,1,oncall: jit,False
41289,List[Any] support in TorchScript high priority triage review oncall: jit weeks,2020-07-11 00:09:39+00:00,,0,5,high priority triage review oncall: jit weeks,True
41284,add a reparameterized version of inverse Gaussian distribution module: distributions triaged enhancement,2020-07-10 23:06:11+00:00,,0,1,module: distributions triaged enhancement,True
41282,[docs] Strange arg names of torch.bmm/mm and tensor.bmm/mm module: docs triaged enhancement,2020-07-10 22:25:06+00:00,,0,0,module: docs triaged enhancement,False
41256,Add nn.functional.swish alias for nn.functional.silu module: nn triaged,2020-07-10 15:43:44+00:00,,0,4,module: nn triaged,True
41246,torch.abs(complex) is divergent from NumPy on vectorized NaN values  triaged module: complex module: NaNs and Infs,2020-07-10 12:23:32+00:00,,0,0,triaged module: complex module: NaNs and Infs,True
41245,torch.sign is divergent from numpy.sign on NaN triaged module: numpy,2020-07-10 12:04:57+00:00,,0,1,triaged module: numpy,True
41244,torch.tan(complex) on CUDA doesn't handle nonfinite values properly module: cuda triaged module: complex,2020-07-10 12:00:25+00:00,,0,0,module: cuda triaged module: complex,True
41243,[discussion] Unite nn modules nn.Something*d to just nn.Something with appropriate options module: nn triaged,2020-07-10 11:43:10+00:00,,0,6,module: nn triaged,True
41242,"test_backward_deadlock fails with ""Directory not empty"" oncall: jit module: POWER",2020-07-10 11:14:54+00:00,,1,8,oncall: jit module: POWER,True
41236,"""out of scope"" variable `cmd` used oncall: distributed triaged better-engineering",2020-07-10 09:27:14+00:00,,0,1,oncall: distributed triaged better-engineering,True
41232,Cleanup git branches triaged small better-engineering actionable,2020-07-10 07:49:48+00:00,,1,15,triaged small better-engineering actionable,True
41226,"The name ""Contiguous"" in `torch::MemoryFormat::Contiguous` can mislead users into assuming contiguous memory high priority triaged module: memory format",2020-07-10 04:42:09+00:00,,1,4,high priority triaged module: memory format,True
41213,libTorch cpp docs missing for Tensor::item() module: docs module: cpp triaged,2020-07-09 23:24:12+00:00,,0,0,module: docs module: cpp triaged,True
41187,test_upsampling_not_recompute_scale_factor fails with Eigen/OpenBLAS module: tests triaged,2020-07-09 16:00:18+00:00,,0,4,module: tests triaged,True
41186,test_autograd failures on Power module: autograd module: cuda module: tests triaged module: POWER module: linear algebra,2020-07-09 15:51:18+00:00,,0,8,module: autograd module: cuda module: tests triaged module: POWER module: linear algebra,True
41182,DLPack cannot accept empty CUDA tensor module: cuda triaged,2020-07-09 13:23:11+00:00,,0,1,module: cuda triaged,True
41178,Failure to build on Power9 due to FXDIV_SOURCE_DIR not being set module: build triaged module: POWER,2020-07-09 07:37:01+00:00,,0,5,module: build triaged module: POWER,True
41177,The new version of the libtorch become slow module: performance oncall: jit module: cpp triaged,2020-07-09 07:22:45+00:00,,0,24,module: performance oncall: jit module: cpp triaged,True
41164,Pytorch.js would be nice! feature triaged,2020-07-08 23:58:31+00:00,,0,3,feature triaged,False
41162,Advanced indexing gradient is extremely slow when there are many duplicate indices module: performance triaged has workaround,2020-07-08 23:55:07+00:00,,0,9,module: performance triaged has workaround,True
41149,eager mode quantization should remove qconfig in the (non-leaf module of) quantized model oncall: quantization low priority triaged,2020-07-08 21:51:37+00:00,,1,1,oncall: quantization low priority triaged,True
41148,Source code for some /_modules/.../.../ not displaying on https://pytorch.org/docs/master/_modules/ module: docs triaged,2020-07-08 21:48:30+00:00,,0,0,module: docs triaged,False
41142,Support for More Loss Functions feature module: loss triaged,2020-07-08 20:46:42+00:00,,0,3,feature module: loss triaged,False
41141,Sum of two tuples becomes a list in torchscript oncall: jit,2020-07-08 19:47:52+00:00,,0,1,oncall: jit,False
41135,libtorch cmake should PUBLIC ally advertise it as requiring C++14 module: build triaged,2020-07-08 18:27:09+00:00,,1,0,module: build triaged,True
41128,Memory bug for backward on torch.sparse.mm?  module: sparse module: autograd triaged,2020-07-08 17:23:27+00:00,,0,10,module: sparse module: autograd triaged,True
41119,TORCH_CUDA_ARCH_LIST deprecation warning module: build triaged enhancement,2020-07-08 14:37:10+00:00,,1,1,module: build triaged enhancement,True
41106,Missing Code for Audio Classification Tutorial module: docs triaged,2020-07-08 00:00:29+00:00,,0,4,module: docs triaged,False
41101,DDP should provide an option for not touching grad of locally unused params oncall: distributed feature triaged,2020-07-07 23:00:40+00:00,,1,26,oncall: distributed feature triaged,True
41099,max_pool2d always compute indices even when it's not required module: nn triaged enhancement module: pooling,2020-07-07 22:50:44+00:00,,0,0,module: nn triaged enhancement module: pooling,True
41081,Add a function to convert SyncBatchNorm layers back to BatchNorm Layers module: bootcamp feature module: nn triaged,2020-07-07 18:39:22+00:00,,0,9,module: bootcamp feature module: nn triaged,True
41070,Cannot define v_dim in MultiheadAttention module: nn triaged oncall: transformer/mha,2020-07-07 12:12:33+00:00,,0,1,module: nn triaged oncall: transformer/mha,True
41046,Human-readable names and operations for TorchScript model graphs triage review oncall: jit TSUsability TSRootCause:PoorIRVisibility,2020-07-06 22:52:57+00:00,,0,0,triage review oncall: jit TSUsability TSRootCause:PoorIRVisibility,True
41027,out= resizing (and restriding) behavior is confusing module: bc-breaking triaged module: numpy module: safe resize topic: bc breaking,2020-07-06 18:59:19+00:00,,0,16,module: bc-breaking triaged module: numpy module: safe resize topic: bc breaking,True
41006,Simplify Adam Optimizer module: optimizer triaged enhancement,2020-07-06 10:58:04+00:00,,0,2,module: optimizer triaged enhancement,False
40997,Is this a bug? The values calculated according to the document isn't equal to the values calculated by framework module: optimizer triaged,2020-07-06 02:16:27+00:00,,0,0,module: optimizer triaged,True
40989,Difference between allocated and reserved CUDA memory module: memory usage triaged,2020-07-05 15:26:40+00:00,,0,2,module: memory usage triaged,True
40988,'_mm256_extract_epi64' was not declared in this scope when compiling on Debian 32-bit module: build triaged,2020-07-05 14:34:55+00:00,,0,8,module: build triaged,True
40978,[FR] NCCL and bool type triaged module: nccl small module: data parallel,2020-07-04 06:00:29+00:00,,0,0,triaged module: nccl small module: data parallel,True
40974,Add a launching script for RPC feature triaged module: rpc,2020-07-04 01:13:51+00:00,,0,0,feature triaged module: rpc,True
40972,len of dataloader when using iterable dataset does not reflect batch size module: nn module: dataloader triaged,2020-07-03 18:29:33+00:00,,0,2,module: nn module: dataloader triaged,True
40971,The values calculated according to the document isn't equal to the values calculated by framework module: docs module: optimizer triaged,2020-07-03 17:36:40+00:00,,0,3,module: docs module: optimizer triaged,True
40969,Regarding graphs page on site module: docs triaged,2020-07-03 16:27:00+00:00,,0,3,module: docs triaged,True
40968,Pytorch 1.4 compilation hangs on AMD Epyc module: build triaged module: vectorization,2020-07-03 15:30:26+00:00,,0,7,module: build triaged module: vectorization,True
40967,Inconsistent behaviour when parameter appears multiple times in parameter list module: docs module: optimizer triaged,2020-07-03 15:06:49+00:00,,0,12,module: docs module: optimizer triaged,True
40959,Will the model run slower when deployed using libtorch ? module: performance module: cpp triaged,2020-07-03 02:20:48+00:00,,0,4,module: performance module: cpp triaged,True
40936,[RFC] [RPC] Automatic retries of all requests in TensorPipe agent triaged module: rpc module: tensorpipe,2020-07-02 19:34:34+00:00,,0,5,triaged module: rpc module: tensorpipe,True
40932,MultiheadAttention set(-inf) cause 'Nan' in loss computing triaged module: NaNs and Infs oncall: transformer/mha,2020-07-02 18:57:50+00:00,,0,2,triaged module: NaNs and Infs oncall: transformer/mha,True
40916,Vectorized torch.eig() module: performance triaged enhancement module: vectorization,2020-07-02 14:17:01+00:00,,0,0,module: performance triaged enhancement module: vectorization,True
40914,Inconsistent handling of torch.Size.__add__ oncall: jit weeks TSUsability TSRootCause:PyTorchParityGap,2020-07-02 13:33:28+00:00,,0,4,oncall: jit weeks TSUsability TSRootCause:PyTorchParityGap,False
40906,will you support SRU on mobile? module: android oncall: mobile,2020-07-02 09:29:33+00:00,,0,1,module: android oncall: mobile,False
40891,[FR] torch.floor/ceil should support output int dtype triaged enhancement module: type promotion,2020-07-02 00:07:29+00:00,,0,1,triaged enhancement module: type promotion,True
40886,Improve error messaging when using dictionaries as inputs to a trace oncall: jit triaged enhancement,2020-07-01 21:38:19+00:00,,0,0,oncall: jit triaged enhancement,True
40882,JIT fuser result of dropout doesn't fully match eager mode  oncall: jit triaged,2020-07-01 20:35:47+00:00,,1,0,oncall: jit triaged,True
40880,Allow TLS to keep distributed autograd context alive feature triaged module: rpc,2020-07-01 19:21:12+00:00,,0,0,feature triaged module: rpc,True
40870,[JIT] cannot statically infer the expected size of a list in this context for positional arguments to Tensor.view() oncall: jit weeks TSUsability TSRootCause:UnsupportedConstructs,2020-07-01 17:55:43+00:00,,0,2,oncall: jit weeks TSUsability TSRootCause:UnsupportedConstructs,False
40867,[JIT] Infer type of argument foo = None to be Optional[Tensor] high priority triage review oncall: jit,2020-07-01 17:25:54+00:00,,0,4,high priority triage review oncall: jit,True
40859,Option to allow loading state dict with mismatching shapes. module: nn module: serialization triaged enhancement,2020-07-01 13:37:31+00:00,,0,35,module: nn module: serialization triaged enhancement,True
40854,Build pytorch with cuda11 module: build module: cuda triaged,2020-07-01 07:42:43+00:00,,0,3,module: build module: cuda triaged,True
40843,emit_nvtx context manager is very slow module: performance module: autograd triaged oncall: profiler,2020-07-01 00:55:27+00:00,,0,9,module: performance module: autograd triaged oncall: profiler,True
40790,Encoding dimension argument for F.one_hot module: bootcamp triaged enhancement module: memory format,2020-06-30 17:01:52+00:00,,0,2,module: bootcamp triaged enhancement module: memory format,True
40770,Incremental version of pca_lowrank feature module: cuda triaged function request,2020-06-30 12:56:44+00:00,,0,5,feature module: cuda triaged function request,True
40763,Enhance supported types of functional.pad  module: nn triaged enhancement module: numpy,2020-06-30 09:26:56+00:00,,1,2,module: nn triaged enhancement module: numpy,True
40761,Sparse Convolutional support module: sparse triaged,2020-06-30 06:45:06+00:00,,0,1,module: sparse triaged,True
40759,Quantization support for F.Softmax oncall: quantization low priority triaged,2020-06-30 04:37:19+00:00,,0,6,oncall: quantization low priority triaged,True
40756,RuntimeError: rois.device().is_cpu() ASSERT FAILED at /vision/torchvision/csrc/cpu/ROIAlign_cpu.cpp:386 triaged module: vision,2020-06-30 02:13:28+00:00,,0,1,triaged module: vision,True
40736,torch::autograd::Function should set AutoNonVariableTypeMode when running forward module: autograd triaged,2020-06-29 22:37:05+00:00,,0,8,module: autograd triaged,True
40729,Custom c++ extension build process doesn't preserve color from compiler module: cpp-extensions module: cpp triaged better-engineering,2020-06-29 21:46:04+00:00,,0,2,module: cpp-extensions module: cpp triaged better-engineering,True
40696,"Trace model with floor, Inconsistent performance oncall: jit days",2020-06-29 11:57:21+00:00,,0,4,oncall: jit days,False
40687,libtorch: macros in logging_is_not_google_glog.h have very common names like CHECK or LOG module: logging triaged better-engineering,2020-06-28 21:47:38+00:00,,1,5,module: logging triaged better-engineering,True
40679,[JIT] Tracing BCE loss throws error when using weight oncall: jit triaged,2020-06-28 10:36:17+00:00,,1,3,oncall: jit triaged,True
40667,Segmentation fault in forward pass using DataParallel and multiple GPUs module: cuda triaged module: data parallel,2020-06-27 22:16:07+00:00,,0,3,module: cuda triaged module: data parallel,True
40659,[typing] overly restrictive List[int] module: typing triaged,2020-06-27 05:01:39+00:00,,0,7,module: typing triaged,True
40635,[JIT] Recursive compilation doesn't apply for types used in type expressions but not value expressions oncall: jit triaged,2020-06-26 19:01:01+00:00,,1,1,oncall: jit triaged,True
40633,RuntimeError: broken pipe from NCCL oncall: distributed triaged module: nccl,2020-06-26 18:20:09+00:00,,0,4,oncall: distributed triaged module: nccl,True
40613,Suppress scientific notation in libtorch module: cpp feature triaged,2020-06-26 08:16:27+00:00,,0,3,module: cpp feature triaged,True
40590,Inconsistent behavior between numpy.exp and torch.exp on CPU for complex numbers triaged module: complex module: numpy,2020-06-25 21:43:54+00:00,,0,4,triaged module: complex module: numpy,True
40581,Libtorch C++ multiple GPU performance slower than single GPU module: performance module: cpp triaged module: data parallel,2020-06-25 19:45:20+00:00,,0,19,module: performance module: cpp triaged module: data parallel,True
40570,Generalized CPU vector reductions module: performance module: cpu triaged module: reductions,2020-06-25 17:31:52+00:00,,0,1,module: performance module: cpu triaged module: reductions,True
40568,Converting NumPy dtype to Torch dtype when using `as_tensor` triaged module: numpy,2020-06-25 17:20:20+00:00,,0,6,triaged module: numpy,True
40566,"If all parameters are unused by forward pass in a process, backward will not work with DDP. oncall: distributed triaged",2020-06-25 16:53:44+00:00,,0,0,oncall: distributed triaged,True
40564,Stop registering kernels that use DispatchStub as catch all module: internals triaged,2020-06-25 16:30:16+00:00,,0,0,module: internals triaged,True
40563,"Compiling PyTorch with 11.0, V11.0.167 module: build triaged",2020-06-25 15:52:00+00:00,,0,1,module: build triaged,True
40561,Cannot manually assign a tensor to .grad from TorchScript triage review oncall: jit days,2020-06-25 14:26:57+00:00,,0,7,triage review oncall: jit days,True
40550,RemoteModule enhancements module: bootcamp triaged enhancement module: rpc pt_distributed_rampup,2020-06-25 02:46:09+00:00,,0,0,module: bootcamp triaged enhancement module: rpc pt_distributed_rampup,True
40548,"Embedding with DataParallel can return ""incomplete"" results module: cuda triaged module: data parallel",2020-06-25 01:30:14+00:00,,0,0,module: cuda triaged module: data parallel,True
40507,SyncBatchNorm for JIT and a list of not supported operations oncall: jit triaged enhancement,2020-06-24 15:21:39+00:00,,0,0,oncall: jit triaged enhancement,True
40500,Not able to launch tensorboard using pytorch  module: tensorboard oncall: visualization,2020-06-24 11:21:08+00:00,,0,8,module: tensorboard oncall: visualization,False
40497,Mixed precision causes NaN loss triaged module: NaNs and Infs module: amp (automated mixed precision),2020-06-24 09:33:21+00:00,,0,87,triaged module: NaNs and Infs module: amp (automated mixed precision),True
40492,Learning rate change is not applied at designated iteration with a scheduler module: optimizer triaged,2020-06-24 05:31:33+00:00,,0,2,module: optimizer triaged,True
40480,torch.autograd.functional.* for models module: autograd triaged enhancement,2020-06-24 01:36:53+00:00,,0,1,module: autograd triaged enhancement,True
40471,please add 'tensor.astype(dtype_string)' syntax for numpy interoperability triaged enhancement module: numpy,2020-06-23 22:54:15+00:00,,0,7,triaged enhancement module: numpy,True
40470,jit's default dtype is different in sandcastle and test_jit.py oncall: jit module: tests triaged,2020-06-23 22:53:19+00:00,,0,1,oncall: jit module: tests triaged,True
40457,DataParallel with Torch 1.5 high priority triaged module: regression module: data parallel,2020-06-23 19:39:01+00:00,,0,10,high priority triaged module: regression module: data parallel,True
40456,Print values (but not strings) when STRIP_ERROR_MESSAGES is defined module: internals module: bootcamp triaged,2020-06-23 19:20:23+00:00,,0,1,module: internals module: bootcamp triaged,True
40441,"[feature request] batch_apply, a general-purpose device-agnostic batch iterator triaged module: batching",2020-06-23 17:16:32+00:00,,0,6,triaged module: batching,True
40427,"Can we make torch.inverse FP16? - RuntimeError: ""inverse_cuda"" not implemented for 'Half' module: cuda triaged enhancement has workaround module: linear algebra module: amp (automated mixed precision)",2020-06-23 10:09:51+00:00,,0,4,module: cuda triaged enhancement has workaround module: linear algebra module: amp (automated mixed precision),True
40425,Segment Fault after use cusolverDnDestroy() with torch1.5 module: dependency bug needs reproduction module: crash module: cuda triaged,2020-06-23 08:46:41+00:00,,0,9,module: dependency bug needs reproduction module: crash module: cuda triaged,True
40419,Add a `like` argument to creation ops triaged module: tensor creation function request,2020-06-23 05:15:17+00:00,,0,6,triaged module: tensor creation function request,True
40417,Missing explanation in torch.utils.tensorboard.add_histogram() module: docs triaged module: tensorboard,2020-06-23 04:18:26+00:00,,0,0,module: docs triaged module: tensorboard,True
40403,Cannot re-initialize CUDA in forked subprocess oncall: distributed module: dataloader module: cuda triaged,2020-06-22 22:57:44+00:00,,0,39,oncall: distributed module: dataloader module: cuda triaged,True
40400,New ProcessGroups created with dist.new_group may leak memory oncall: distributed module: memory usage triaged better-engineering,2020-06-22 22:13:07+00:00,,0,0,oncall: distributed module: memory usage triaged better-engineering,True
40375,Add batched torch.combinations triaged enhancement module: batching function request,2020-06-22 16:43:26+00:00,,0,1,triaged enhancement module: batching function request,False
40373,[discussion] Expressing tensor dimension semantics / constraints through typing / constraints blocks. Constraints block could be scripted/traced and help for tracing/script execution and codegen module: internals feature triaged,2020-06-22 12:26:39+00:00,,0,27,module: internals feature triaged,True
40355,Tracking output dimensions of the convolutional layers module: internals triaged,2020-06-21 15:47:03+00:00,,0,6,module: internals triaged,True
40319,Too many labels in the repo triaged module: infra,2020-06-19 23:12:16+00:00,,0,3,triaged module: infra,True
40316,Refactor the adaptive avg pool code oncall: quantization good first issue low priority triaged module: pooling,2020-06-19 23:10:35+00:00,,1,15,oncall: quantization good first issue low priority triaged module: pooling,True
40295,from torch._C import default_generator ImportError: cannot import name 'default_generator' module: build module: rocm triaged,2020-06-19 19:24:20+00:00,,0,7,module: build module: rocm triaged,True
40278,Observer in Quantization throws Warning oncall: quantization low priority triaged,2020-06-19 09:20:59+00:00,,1,7,oncall: quantization low priority triaged,True
40275,Conv3d with specific kernel size outputs inconsistent results between FP16 and FP32 in V100 GPU module: dependency bug module: cudnn module: cuda module: convolution triaged,2020-06-19 09:09:06+00:00,,0,12,module: dependency bug module: cudnn module: cuda module: convolution triaged,True
40269,caffe2 error during compilation of PyTorch with ROCm on Archlinux caffe2,2020-06-19 05:10:22+00:00,,0,3,caffe2,True
40266,CUDA error: out of memory when running tensorpipe test_cuda triaged module: rpc module: tensorpipe,2020-06-19 02:38:39+00:00,,0,0,triaged module: rpc module: tensorpipe,True
40250,Include expanded TensorOptions version of op in at:: namespace triaged module: codegen,2020-06-18 22:32:05+00:00,,1,0,triaged module: codegen,True
40237,Move torch cpp Errors to c10::Error module: internals module: error checking triaged enhancement better-engineering,2020-06-18 20:23:54+00:00,,0,0,module: internals module: error checking triaged enhancement better-engineering,True
40215,Move all torch.Tensor methods to codegen module: internals triaged module: pybind,2020-06-18 09:19:42+00:00,,0,6,module: internals triaged module: pybind,True
40208,C++ API for torch.autograd.functional.jacobian module: cpp module: autograd triaged,2020-06-18 01:59:46+00:00,,0,18,module: cpp module: autograd triaged,True
40147,SyncBatchNorm doesn't work when I set track_running_stats False oncall: distributed triaged,2020-06-17 04:35:50+00:00,,0,8,oncall: distributed triaged,True
40137,Improve RPC test debugability high priority triaged better-engineering module: rpc,2020-06-17 02:46:59+00:00,,0,7,high priority triaged better-engineering module: rpc,True
40134,About torch.backends.cudnn.deterministic issue module: cudnn triaged,2020-06-17 02:05:50+00:00,,0,8,module: cudnn triaged,True
40133,Why does Libtorch use more memory than Pytorch does in Python？ needs reproduction module: memory usage triaged,2020-06-17 01:20:43+00:00,,0,3,needs reproduction module: memory usage triaged,True
40114,ppc64le: test cpp_extensions/rng_extension.cpp failure (without altivec override) triaged module: random module: POWER,2020-06-16 19:44:50+00:00,,1,1,triaged module: random module: POWER,True
40107,[RFC] Add a RPC context manager that collects/waits for all RPC futures created in scope module: bootcamp feature triaged module: rpc,2020-06-16 18:46:22+00:00,,1,6,module: bootcamp feature triaged module: rpc,True
40098,Updating learning rate with Libtorch 1.5 and optimiser options module: cpp module: optimizer triaged,2020-06-16 15:53:58+00:00,,0,3,module: cpp module: optimizer triaged,True
40095,Bump up NCCL to 2.7.3 oncall: distributed triaged module: nccl,2020-06-16 15:09:00+00:00,,0,6,oncall: distributed triaged module: nccl,True
40087,Make Scaling in BatchNorm optional  triaged enhancement,2020-06-16 10:22:47+00:00,,0,5,triaged enhancement,True
40086,TestListwiseL2rOps::test_lambda_rank_loss fails caffe2 module: tests triaged,2020-06-16 10:20:51+00:00,,0,3,caffe2 module: tests triaged,True
40071,[JIT][to-backend] Finalize the PyTorchBackendInterface API/naming for external sharing oncall: jit triaged,2020-06-16 01:23:41+00:00,,0,0,oncall: jit triaged,True
40070,[JIT][to-backend] Support type refinement for container types in the generated code oncall: jit triaged,2020-06-16 01:19:44+00:00,,1,0,oncall: jit triaged,True
40052,Simplify checks that generator has next normal sample cache methods in normal_distribution triaged module: random,2020-06-15 20:31:02+00:00,,1,0,triaged module: random,True
40034,Doing optimizing compilations in a separate thread triaged jit-backlog,2020-06-15 17:35:17+00:00,,1,0,triaged jit-backlog,True
40030,matchTensor tests. Add tests to make sure that `isSubtypeOf` and `matchTensor` return equivalent results triaged jit-backlog,2020-06-15 16:06:45+00:00,,0,0,triaged jit-backlog,True
40027,[Android Pytorch] TorchScript traced model returns inconsistent output tensors on each run oncall: jit triaged oncall: mobile,2020-06-15 15:10:38+00:00,,0,17,oncall: jit triaged oncall: mobile,True
40024,torch::jit::load -> Unhandled exception  oncall: jit triaged,2020-06-15 14:11:21+00:00,,0,9,oncall: jit triaged,True
40020,Caffe2 operation switches current CUDA stream caffe2 module: cuda triaged,2020-06-15 08:55:23+00:00,,0,2,caffe2 module: cuda triaged,True
39993,How do I derive weights for CrossEntropy Loss on my custom dataset? triaged module: vision,2020-06-13 15:47:15+00:00,,0,0,triaged module: vision,True
39990,libtorch 1.5 macos crash when loading on some mac module: build module: cpp triaged module: mkldnn,2020-06-13 10:37:50+00:00,,1,5,module: build module: cpp triaged module: mkldnn,True
39972,Port old registration API to new one triaged better-engineering,2020-06-12 23:38:26+00:00,,1,0,triaged better-engineering,True
39959,Different max_pool2d cpp signatures due to indices.  triaged better-engineering module: pooling,2020-06-12 20:19:53+00:00,,0,0,triaged better-engineering module: pooling,True
39947,[Caffe2]  compilation in c++ undefined reference to `caffe2 module: build caffe2 triaged,2020-06-12 16:08:57+00:00,,0,4,module: build caffe2 triaged,True
39917, build QT program use libtorch-cxx11-abi-shared-with-deps-1.5.0+cu101 ok with CPU but error with cuda GPU module: build triaged,2020-06-12 02:21:58+00:00,,0,8,module: build triaged,True
39864,Parallelize arguments serde for RPC with TorchScript functions.  module: performance oncall: jit triaged module: rpc,2020-06-11 16:13:51+00:00,,0,0,module: performance oncall: jit triaged module: rpc,True
39852,Shared file-system initialization in pytorch distributed is slow  module: performance oncall: distributed triaged,2020-06-11 11:47:28+00:00,,0,2,module: performance oncall: distributed triaged,True
39836,Feedback to split certain doc pages into sub-topics module: docs triaged,2020-06-11 00:43:58+00:00,,0,4,module: docs triaged,False
39832,Torch Distributed Asynch docs with invalid ENV args needs reproduction module: docs triaged module: rpc,2020-06-11 00:28:44+00:00,,0,1,needs reproduction module: docs triaged module: rpc,False
39820,I would like to install pytorch-nightly from requirements.txt triaged has workaround,2020-06-10 22:26:45+00:00,,0,6,triaged has workaround,False
39806,Simplify layers of optionals in `VaryingShape` and `Stride` oncall: jit triaged jit-backlog,2020-06-10 21:30:51+00:00,,1,0,oncall: jit triaged jit-backlog,True
39799,Tensor.size() API not correctly documented? module: docs triaged,2020-06-10 19:32:42+00:00,,0,1,module: docs triaged,True
39780,LRP-based explainability feature triaged needs research,2020-06-10 14:01:22+00:00,,0,1,feature triaged needs research,False
39765,Support mainstream pruning techniques  feature triaged needs research module: pruning,2020-06-10 01:01:44+00:00,,0,11,feature triaged needs research module: pruning,False
39757,max_unpool2d and max_unpool3d cpp signature should be similar triaged enhancement module: pooling,2020-06-09 21:32:33+00:00,,1,10,triaged enhancement module: pooling,True
39745,[JIT] exported dunder methods are ignored oncall: jit triaged,2020-06-09 18:40:14+00:00,,0,0,oncall: jit triaged,True
39724,pickle_save on mobile (no longer works with 1.5.0 release) oncall: mobile,2020-06-09 13:58:37+00:00,,0,2,oncall: mobile,False
39719,User Objects aren't recursively scripted as nn.Module attributes oncall: jit triaged,2020-06-09 09:26:17+00:00,,0,5,oncall: jit triaged,True
39716,Do not modify global random state module: docs triaged module: random,2020-06-09 08:20:01+00:00,,0,11,module: docs triaged module: random,True
39706,[distributed] calling nccl reduce with inconsistent dst hangs oncall: distributed triaged module: nccl module: deadlock,2020-06-09 05:18:22+00:00,,0,1,oncall: distributed triaged module: nccl module: deadlock,True
39705,[doc] [distributed] example of specifying url etc in url oncall: distributed module: docs triaged,2020-06-09 05:14:45+00:00,,0,0,oncall: distributed module: docs triaged,False
39690,[Quantization] Output tensor type is lost after serializing and loading back a quantized model oncall: jit,2020-06-09 01:16:29+00:00,,0,9,oncall: jit,True
39682,Run `clang-tidy` on the `aten` folder? module: lint triaged module: build warnings better-engineering,2020-06-08 22:38:25+00:00,,0,0,module: lint triaged module: build warnings better-engineering,True
39680,CUDA cannot be found module: build module: cuda triaged module: arm,2020-06-08 21:48:03+00:00,,0,13,module: build module: cuda triaged module: arm,True
39674,[BatchNorm] Add boolean flags to choose the stats for normalization module: nn triaged function request module: norms and normalization,2020-06-08 20:09:30+00:00,,0,22,module: nn triaged function request module: norms and normalization,True
39671,TestTorchDeviceTypeCPU.test_float_to_int_conversion_finite_cpu_uint8 is broken on PowerPC module: tests triaged module: POWER,2020-06-08 19:01:25+00:00,,0,1,module: tests triaged module: POWER,True
39670,Force JIT to do type inference even when mypy annotated oncall: jit module: typing triaged,2020-06-08 18:56:15+00:00,,1,3,oncall: jit module: typing triaged,True
39665,Better err msg for tensor ctor from sequence triaged enhancement better-engineering,2020-06-08 16:50:03+00:00,,0,0,triaged enhancement better-engineering,True
39662,[RFC][distributed] RFC: c10d ProcessGroup extension and C++ API change oncall: distributed triaged,2020-06-08 16:12:49+00:00,,1,6,oncall: distributed triaged,True
39656,Broadcasting for torch.cross feature triaged module: numpy,2020-06-08 13:53:48+00:00,,0,4,feature triaged module: numpy,True
39634,Installation from source fails on macOS (No CUDA) module: build module: cuda triaged module: macos,2020-06-07 05:06:48+00:00,,0,4,module: build module: cuda triaged module: macos,True
39632,F.affine_grid dispatch async issue module: performance module: nn module: cuda triaged,2020-06-07 00:14:15+00:00,,0,2,module: performance module: nn module: cuda triaged,True
39625,All keys matched successfully missing when loading state dict on optimizers module: optimizer triaged,2020-06-06 09:56:56+00:00,,0,2,module: optimizer triaged,False
39619,Switching from CPU build to CUDA build in conda environments is a bit tricky module: docs triaged,2020-06-06 06:18:17+00:00,,0,1,module: docs triaged,True
39603,friend constexpr in templated struct loses constexpr-ness in nvcc triaged,2020-06-05 21:34:12+00:00,,0,2,triaged,True
39570,Dataloader._shutdown_workers hangs module: multiprocessing module: dataloader triaged,2020-06-05 10:19:29+00:00,,0,14,module: multiprocessing module: dataloader triaged,True
39569,[JIT] OrderedDict doesn't support custom objects triage review oncall: jit,2020-06-05 10:09:38+00:00,,0,1,triage review oncall: jit,True
39539,[JIT] Print out mutation in IR Dumps triage review oncall: jit,2020-06-04 21:38:47+00:00,,0,0,triage review oncall: jit,True
39537,Segfault = docker + tensorboard + pytorch triaged module: tensorboard,2020-06-04 21:24:35+00:00,,0,0,triaged module: tensorboard,True
39524,test_nn_module_tests should run less tests module: nn module: tests triaged,2020-06-04 15:59:40+00:00,,0,0,module: nn module: tests triaged,True
39522,Valgrind leak checking flags losses in libtorch module: cpp triaged,2020-06-04 15:10:38+00:00,,0,7,module: cpp triaged,True
39503,pytest suppresses stderr from Python startup by default module: logging module: tests triaged,2020-06-04 01:36:40+00:00,,0,2,module: logging module: tests triaged,True
39495,Misannotation of layer_norm parameters causes internal assert failure oncall: quantization low priority triaged,2020-06-04 00:19:14+00:00,,0,4,oncall: quantization low priority triaged,True
39463,JIT test suite has dependencies across tests oncall: jit triaged,2020-06-03 19:12:25+00:00,,0,2,oncall: jit triaged,True
39444,Torch hub: object has no attribute nms triaged module: docker,2020-06-03 12:43:24+00:00,,0,3,triaged module: docker,True
39443,PyTorch multiprocessing.spawn seems slow with list of tensors module: performance module: multiprocessing triaged,2020-06-03 12:07:26+00:00,,0,1,module: performance module: multiprocessing triaged,True
39437,error when specifying sparse=True in embedding module: sparse triaged,2020-06-03 08:32:56+00:00,,0,5,module: sparse triaged,True
39435,BCEWithLogitsLoss() not equal to BCELoss() with sigmoid() module: loss triaged,2020-06-03 07:19:59+00:00,,0,3,module: loss triaged,True
39426,Format issue in `torch.quantization.add_quant_dequant`  documentation parameter section module: docs triaged,2020-06-03 02:00:45+00:00,,1,1,module: docs triaged,True
39421,torch::jit::script::Module::to(torch::kDouble) also casts buffers oncall: jit triaged,2020-06-02 23:40:10+00:00,,0,4,oncall: jit triaged,True
39411,Allow to_here to interrupt blocking wait in the case of rpc.remote timeout feature triaged module: rpc pt_distributed_rampup,2020-06-02 20:34:14+00:00,,1,1,feature triaged module: rpc pt_distributed_rampup,True
39399,Update RPC doc  to recommend async user functions for non-blocking server execution module: docs triaged module: rpc,2020-06-02 15:05:02+00:00,,1,0,module: docs triaged module: rpc,False
39395,Morphological operations feature triaged,2020-06-02 11:12:37+00:00,,0,1,feature triaged,True
39388,"RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:514, unhandled system error, NCCL version 2.4.8 module: crash triaged module: nccl",2020-06-02 03:09:13+00:00,,0,11,module: crash triaged module: nccl,True
39383,Pickling a Tensor or a Storage is not deterministic module: pickle triaged,2020-06-02 01:26:29+00:00,,0,1,module: pickle triaged,True
39370,Draw quantized tensors in tensorboard triaged enhancement module: tensorboard,2020-06-01 23:37:32+00:00,,0,2,triaged enhancement module: tensorboard,True
39365,Attributes not removed by freeze module oncall: jit triaged,2020-06-01 22:51:15+00:00,,0,3,oncall: jit triaged,True
39362,"The conversion of Inplace aten::append to outplace is incomplete, creating incorrect subgraphs oncall: jit triaged",2020-06-01 22:42:39+00:00,,0,6,oncall: jit triaged,True
39358,SyncBatchNorm doesn't scale well across multiple nodes on large data sizes module: performance oncall: distributed triaged,2020-06-01 21:58:10+00:00,,0,3,module: performance oncall: distributed triaged,True
39357,Enable scripting modules with recursive functions oncall: jit feature triaged,2020-06-01 21:55:35+00:00,,0,3,oncall: jit feature triaged,True
39356,Torch script to support dictionary with keys of type tuple oncall: jit triaged,2020-06-01 21:49:32+00:00,,0,8,oncall: jit triaged,True
39355,Clean up GIL that used to guard deleted RRef destructions  triaged better-engineering module: rpc,2020-06-01 21:37:06+00:00,,1,0,triaged better-engineering module: rpc,True
39351,Clean up RPC request callback implementation high priority triage review oncall: distributed triaged better-engineering module: rpc,2020-06-01 20:34:00+00:00,,0,4,high priority triage review oncall: distributed triaged better-engineering module: rpc,True
39317,send a Tensor to Cuda very slow module: performance module: cuda triaged,2020-06-01 04:50:33+00:00,,0,2,module: performance module: cuda triaged,True
39310,Make torch.cross `dim` parameter work intuitively module: bc-breaking triaged module: numpy topic: bc breaking,2020-05-31 14:34:21+00:00,,0,4,module: bc-breaking triaged module: numpy topic: bc breaking,True
39309,[JIT] nn.ModuleList loses None objects inside it after scripting triage review oncall: jit,2020-05-31 13:09:53+00:00,,0,1,triage review oncall: jit,True
39305,Compilation errors module: build triaged module: mkldnn,2020-05-30 23:19:03+00:00,,0,1,module: build triaged module: mkldnn,True
39298,Caffe 2 android app not working with proguard and minifyEnabled caffe2,2020-05-30 06:23:36+00:00,,0,1,caffe2,False
39292,Tensorboard，graph in pytorch 1.4 is more complicated than pytorch 1.1？ module: tensorboard oncall: visualization,2020-05-30 02:42:03+00:00,,0,0,module: tensorboard oncall: visualization,False
39280,Let future.wait() take in an optional timeout triaged enhancement module: rpc,2020-05-29 23:09:12+00:00,,0,0,triaged enhancement module: rpc,True
39279,Differentiable Optimizers feature module: nn module: optimizer triaged,2020-05-29 23:08:25+00:00,,0,10,feature module: nn module: optimizer triaged,False
39270,Current implementation of c10::complex does not support being used in shared memory triaged module: complex,2020-05-29 20:07:09+00:00,,1,1,triaged module: complex,True
39260,Naming inconsistency: padding_mode vs pad_mode + F.conv* docs ops miss padding_mode arg at all triaged module: ux,2020-05-29 18:15:23+00:00,,0,1,triaged module: ux,False
39250,SummaryWriter `add_hparams` should support adding new hyperparameters  enhancement oncall: visualization,2020-05-29 15:46:17+00:00,,0,7,enhancement oncall: visualization,False
39245,pytorch-crf model to onnx conversion module: onnx triaged,2020-05-29 14:26:26+00:00,,0,10,module: onnx triaged,False
39242,Add numerically stable log1mexp = log(1 - exp(-|x|)) function module: numerical-stability triaged enhancement function request,2020-05-29 13:30:02+00:00,,0,5,module: numerical-stability triaged enhancement function request,False
39240,[mobile] Running on armeabi-v7a is inconsistent with arm64-v8a oncall: mobile,2020-05-29 11:50:01+00:00,,0,2,oncall: mobile,False
39226,Try Address Sanitizer in MSVC builds module: windows triaged,2020-05-29 07:27:06+00:00,,1,7,module: windows triaged,True
39224,Negative stride values in `as_strided` module: internals module: error checking triaged module: memory format,2020-05-29 05:45:00+00:00,,0,0,module: internals module: error checking triaged module: memory format,True
39178,Add support for rsample to MixtureSameFamily Distribution module: distributions triaged enhancement,2020-05-28 17:02:44+00:00,,0,9,module: distributions triaged enhancement,True
39134,Crossentropy inconsistent results depending on tensor order module: nn triaged module: determinism,2020-05-28 08:13:28+00:00,,0,4,module: nn triaged module: determinism,True
39130,`torch.backends` undocumented module: docs triaged,2020-05-28 06:13:46+00:00,,0,0,module: docs triaged,True
39128,"Problems implementing complex support for acosh, asinh, tanh triaged module: complex",2020-05-28 04:47:38+00:00,,0,8,triaged module: complex,True
39124,[TensorPipe] Errors in pipeWrite should clear out the future in pendingResponseMessage triaged module: rpc module: tensorpipe,2020-05-28 02:05:21+00:00,,0,0,triaged module: rpc module: tensorpipe,True
39123,The MacOS compiler is generating illegal instruction for the division of c10::complex triaged module: complex,2020-05-28 02:03:20+00:00,,0,7,triaged module: complex,True
39101,Build issue when installing pytorch with USE_FFMPEG=1 USE_OPENCV=1  module: build triaged,2020-05-27 21:48:38+00:00,,0,0,module: build triaged,True
39082,[TensorPipe] Avoid wrapping the future message in order to do atomic test-and-set triaged module: tensorpipe,2020-05-27 19:26:03+00:00,,1,4,triaged module: tensorpipe,False
39064,Add cpack support to CMakeLists.txt module: build module: cpp triaged,2020-05-27 13:33:26+00:00,,0,4,module: build module: cpp triaged,True
39061,Some @slowtests are never run in CI high priority module: tests triaged quansight-nack,2020-05-27 10:27:22+00:00,,0,2,high priority module: tests triaged quansight-nack,True
39058,Failure when loading quantized pre-trained weights partially oncall: quantization low priority triaged,2020-05-27 09:26:49+00:00,,0,0,oncall: quantization low priority triaged,True
39050,PyTorch Issue w/ GPU needs reproduction module: cuda triaged,2020-05-27 05:43:36+00:00,,0,2,needs reproduction module: cuda triaged,True
39022,Cap DDP total number of buckets oncall: distributed triaged,2020-05-26 19:01:19+00:00,,0,3,oncall: distributed triaged,True
39009,Missing OneCycleLR and MultiplicativeLR in lr_scheduler.pyi module: typing triaged,2020-05-26 15:45:47+00:00,,1,5,module: typing triaged,True
38997,[doc] document cuda.nccl module: docs module: cuda triaged module: nccl,2020-05-26 03:54:45+00:00,,0,0,module: docs module: cuda triaged module: nccl,True
38996,[FR] API consistency for cuda.comm and distributed oncall: distributed module: docs module: cuda triaged,2020-05-26 03:53:38+00:00,,0,0,oncall: distributed module: docs module: cuda triaged,True
38995,[FR] [distributed] coalesced primitives oncall: distributed triaged,2020-05-26 03:48:18+00:00,,0,11,oncall: distributed triaged,True
38980,runtime error while using default arguments in add_graph() function triaged module: tensorboard oncall: visualization,2020-05-25 11:31:58+00:00,,0,0,triaged module: tensorboard oncall: visualization,True
38975,Use general ATen dispatch mechanism module: internals triaged,2020-05-25 07:45:12+00:00,,0,4,module: internals triaged,True
38974,ATen registrable operator list module: internals triaged,2020-05-25 07:31:10+00:00,,0,12,module: internals triaged,True
38973,ATen operator API versioning module: internals triaged,2020-05-25 06:54:33+00:00,,0,10,module: internals triaged,True
38964,[JIT] jit can not recognize the imported function oncall: jit triaged,2020-05-24 15:09:23+00:00,,1,5,oncall: jit triaged,True
38955,fold variation feature module: nn triaged function request,2020-05-23 14:15:18+00:00,,0,5,feature module: nn triaged function request,False
38948,torch.lobpcg always breaks for autograd module: autograd triaged module: linear algebra,2020-05-23 04:38:20+00:00,,1,67,module: autograd triaged module: linear algebra,True
38940,[FR] Support SyncBatchNorm in DataParallel triaged enhancement module: nccl module: data parallel,2020-05-22 20:32:38+00:00,,0,0,triaged enhancement module: nccl module: data parallel,True
38936,[FR] DataParallel arg rename device_ids->devices triaged enhancement module: data parallel,2020-05-22 18:20:03+00:00,,0,0,triaged enhancement module: data parallel,True
38911,[FR] cuda.comm.broadcast/reduce_add support `out=` oncall: distributed feature module: cuda triaged,2020-05-22 05:35:55+00:00,,0,1,oncall: distributed feature module: cuda triaged,True
38910,Memory Leak with Docker GPU high priority module: cudnn module: cuda module: memory usage triaged module: regression,2020-05-22 04:27:09+00:00,,0,13,high priority module: cudnn module: cuda module: memory usage triaged module: regression,True
38876,Override `__call__` instead of `forward` module: typing triaged,2020-05-21 16:20:15+00:00,,0,1,module: typing triaged,True
38863,[JIT] named_(parameters | buffers) do not support recursive iteration.  oncall: jit triaged,2020-05-21 09:20:33+00:00,,1,1,oncall: jit triaged,True
38855,Can not compile GridSamplerKernel.cpp with gcc-9.3 module: build triaged,2020-05-21 03:55:06+00:00,,0,0,module: build triaged,True
38854,Can't compile QuantizedOpKernels.cpp using gcc-9.3 module: build triaged,2020-05-21 03:52:43+00:00,,0,0,module: build triaged,True
38853,Weight decay in AdamW module: optimizer triaged,2020-05-21 03:50:44+00:00,,0,3,module: optimizer triaged,True
38788,Convergence issues when using pytorch's native AMP triaged module: amp (automated mixed precision),2020-05-20 16:35:33+00:00,,0,5,triaged module: amp (automated mixed precision),True
38782,SPMG in DDP does not have gradient computation and communication overlap? oncall: distributed triaged,2020-05-20 15:10:15+00:00,,0,1,oncall: distributed triaged,True
38767,weight_decay in Adam is not an L2 Penalty module: optimizer triaged,2020-05-20 08:45:01+00:00,,0,1,module: optimizer triaged,True
38765,LibTorch 1.5.0 not supporting GLIBC < 2.23 module: build module: cpp triaged,2020-05-20 07:34:44+00:00,,0,5,module: build module: cpp triaged,True
38752,test_float_to_int_conversion_finite_cpu_int16 is failing on MacOS high priority module: tests triaged module: numpy,2020-05-20 01:59:53+00:00,,1,3,high priority module: tests triaged module: numpy,True
38744,[jit] `jit.annotate` and multiple compilation of functions with different types triage review oncall: jit,2020-05-19 23:20:54+00:00,,0,1,triage review oncall: jit,True
38737,equivalent of tensorflow's embedding_lookup_sparse feature triaged,2020-05-19 20:46:47+00:00,,0,2,feature triaged,False
38720,DOC: add documentation for undocumented classes and methods module: docs triaged,2020-05-19 14:17:03+00:00,,0,0,module: docs triaged,False
38719,Building NVCC (Device) failed when building from source module: build triaged,2020-05-19 13:13:59+00:00,,0,3,module: build triaged,True
38709,NaN Loss for FasterRCNN on Multiclass Object Detection on Custom Dataset COCO triaged module: vision,2020-05-19 05:51:10+00:00,,0,0,triaged module: vision,True
38703,torch.as_tensor(np_array) is sometimes much faster than torch.tensor(np_array) module: performance triaged module: numpy,2020-05-19 03:59:28+00:00,,0,1,module: performance triaged module: numpy,True
38684,Add Plackett-Luce distribution feature triaged,2020-05-18 22:40:07+00:00,,0,2,feature triaged,False
38677,TestCase.assertEqual does not distinguish Python builtin types and single-element Tensor module: tests triaged,2020-05-18 21:23:32+00:00,,1,1,module: tests triaged,True
38671,Add CUDA callback to Python API feature module: cuda triaged module: rpc,2020-05-18 18:38:43+00:00,,0,1,feature module: cuda triaged module: rpc,True
38662,Unify CPU/CUDA exponential transformation formula triaged module: random,2020-05-18 17:06:07+00:00,,0,1,triaged module: random,True
38650,Implement torch.erf for complex dtypes triaged module: complex,2020-05-18 15:02:09+00:00,,0,1,triaged module: complex,True
38643,torch while loading weighs found runtime error on storage has wrong size: expected 4254413747647032608 got 1024  module: serialization triaged,2020-05-18 10:24:13+00:00,,0,2,module: serialization triaged,True
38642,Synchronization problem in torch.distributed with MPI on CUDA  oncall: distributed triaged,2020-05-18 10:19:20+00:00,,0,5,oncall: distributed triaged,True
38640,Maxunpool seems to give a weird error message triaged,2020-05-18 09:51:46+00:00,,0,1,triaged,True
38639,Difference between using a python list and nn.ModuleList module: docs module: nn triaged,2020-05-18 09:43:44+00:00,,0,0,module: docs module: nn triaged,True
38630,Torchvision error TypeError: _resolve_type_from_object() triaged module: vision,2020-05-17 20:22:23+00:00,,0,3,triaged module: vision,True
38624,Wheels not manylinux1 compliant module: binaries triaged,2020-05-17 14:49:48+00:00,,0,6,module: binaries triaged,True
38623,Providing CUDA tensor to model on CPU causes a crash module: crash module: rnn module: cuda triaged,2020-05-17 11:17:35+00:00,,0,6,module: crash module: rnn module: cuda triaged,True
38622,Clarification for usage of negative loss with optim.lr_scheduler.ReduceLROnPlateau module: docs triaged,2020-05-17 10:44:53+00:00,,0,2,module: docs triaged,False
38619,"Can you add NMS,RoIAlign,RoIPool for libtorch? triaged module: vision",2020-05-17 03:52:57+00:00,,0,5,triaged module: vision,True
38616,RuntimeError: num_gpus <= 16 INTERNAL ASSERT FAILED  module: multi-gpu module: cuda triaged,2020-05-17 02:09:59+00:00,,0,19,module: multi-gpu module: cuda triaged,True
38614,`SummaryWriter.add_graph` borks with simple example oncall: visualization,2020-05-16 21:43:51+00:00,,0,0,oncall: visualization,False
38613,Investigate using log1p instead of log in transformation functions(TransformationHelper.h) module: distributions triaged module: random,2020-05-16 21:08:16+00:00,,0,0,module: distributions triaged module: random,True
38612,Investigate exponential distribution improvements module: distributions triaged module: random,2020-05-16 21:04:54+00:00,,0,0,module: distributions triaged module: random,True
38611,Investigate using -cospi(u) / sinpi(u) instead of tan(pi * (u - 0.5)) in transformation::cauchy module: distributions triaged module: random,2020-05-16 21:00:18+00:00,,0,0,module: distributions triaged module: random,True
38595,torch.norm p/ord parameter documentation is wrong module: docs triaged,2020-05-16 05:55:22+00:00,,0,1,module: docs triaged,False
38569,[quantization] Version support for quantization BC tests oncall: quantization low priority triaged,2020-05-15 19:36:21+00:00,,1,0,oncall: quantization low priority triaged,True
38564,tensorboard projector mode with custom metadata_header with only one label name triaged module: tensorboard,2020-05-15 18:27:22+00:00,,0,0,triaged module: tensorboard,True
38553,"When I use cuda(), wg = th.matmul(extra_obs, extra_obs.transpose(-2, -1)) take a mistake triaged module: cublas",2020-05-15 16:43:26+00:00,,0,1,triaged module: cublas,True
38550,Returning a tensor instead of a list in split and chunk feature triaged has workaround shadow review,2020-05-15 14:55:02+00:00,,0,3,feature triaged has workaround shadow review,True
38548,Add Distributed LR Scheduler to RPC module: bootcamp feature triaged module: rpc,2020-05-15 14:34:33+00:00,,0,0,module: bootcamp feature triaged module: rpc,True
38543,Make rebuildBucket() to be async in c10d reducer oncall: distributed triaged module: data parallel,2020-05-15 07:55:38+00:00,,0,0,oncall: distributed triaged module: data parallel,True
38541,Doc update regarding predictability of experiments using Seeds and Workers module: docs triaged module: determinism,2020-05-15 06:39:59+00:00,,1,3,module: docs triaged module: determinism,False
38536,"build libtorch problem: Configuring incomplete, errors occurred!  _mm256_abs_epi16 module: build triaged",2020-05-15 03:51:10+00:00,,0,2,module: build triaged,True
38510,[DISCUSSION] RPC server-side ThreadLocalState oncall: jit triaged module: multithreading module: rpc,2020-05-14 22:07:02+00:00,,0,0,oncall: jit triaged module: multithreading module: rpc,True
38487,expected scalar type Half but found Float with torch.cuda.amp and torch.nn.DataParallel triaged module: data parallel module: amp (automated mixed precision),2020-05-14 17:07:09+00:00,,0,13,triaged module: data parallel module: amp (automated mixed precision),True
38475,TopK implementation slower than a custom divide and conquer implementation module: performance module: cuda good first issue module: cpu triaged module: sorting and selection,2020-05-14 13:57:43+00:00,,0,7,module: performance module: cuda good first issue module: cpu triaged module: sorting and selection,True
38474,Scale parameter downcasted and rounded down in pytorch.distributions.Normal module: distributions triaged,2020-05-14 13:45:00+00:00,,0,5,module: distributions triaged,True
38473,Python builtin function next() is currently not supported in Torchscript oncall: jit triaged,2020-05-14 12:38:36+00:00,,0,1,oncall: jit triaged,False
38472,[torch.jit.trace] torch.jit.trace fixed batch size CNN  oncall: jit triaged,2020-05-14 12:03:33+00:00,,0,3,oncall: jit triaged,True
38471,Immediate mode API (with functional flavor) for optimizers module: optimizer triaged enhancement,2020-05-14 11:33:26+00:00,,0,1,module: optimizer triaged enhancement,False
38470,[Feature] Option to have zeros/ones/full output tensor with zero strides triaged enhancement module: tensor creation function request,2020-05-14 10:47:18+00:00,,0,1,triaged enhancement module: tensor creation function request,True
38457,Named Tensor and Indexing triaged module: named tensor,2020-05-14 05:58:30+00:00,,1,2,triaged module: named tensor,True
38443,JitTest.testAutogradProfiler is broken in test_misc.cpp oncall: jit module: tests triaged oncall: profiler,2020-05-13 22:08:11+00:00,,0,8,oncall: jit module: tests triaged oncall: profiler,True
38437,[JIT] torch.tensor needs a Tensor overload oncall: jit module: bootcamp triaged small,2020-05-13 21:37:27+00:00,,1,1,oncall: jit module: bootcamp triaged small,True
38419,[RFC] Add tar-based IterableDataset implementation to PyTorch feature module: dataloader triaged,2020-05-13 18:54:24+00:00,,1,31,feature module: dataloader triaged,False
38412,No MKL Compatible Conda installation for PyTorch 1.5 module: binaries triaged module: mkl,2020-05-13 18:17:34+00:00,,0,27,module: binaries triaged module: mkl,True
38394,4D tensor support for torch.nn.functionnal.fold() (col2im) module: nn triaged function request,2020-05-13 12:30:16+00:00,,0,5,module: nn triaged function request,True
38354,"Can I add more ""Project Documentation"" on the PYPI webpage?  module: build module: docs triaged small",2020-05-12 21:06:32+00:00,,0,3,module: build module: docs triaged small,True
38337,"Clicking ""Click here to view docs for latest stable release."" on some pages leads to nowhere module: docs triaged",2020-05-12 17:59:11+00:00,,0,0,module: docs triaged,False
38328,Provide an issubdtype API triaged module: numpy module: type promotion,2020-05-12 15:33:55+00:00,,0,0,triaged module: numpy module: type promotion,True
38323,Support alternate casting rules triaged module: numpy module: type promotion,2020-05-12 15:02:11+00:00,,0,0,triaged module: numpy module: type promotion,True
38319,c10/macros/cmake_macros.h not exists triaged oncall: mobile,2020-05-12 14:12:42+00:00,,0,1,triaged oncall: mobile,True
38312,Status of support for training on mobile oncall: mobile,2020-05-12 09:49:17+00:00,,0,5,oncall: mobile,False
38310,Compiling errors when trying to cross-compile the C++ API for RTOS (QNX) module: build module: cpp triaged,2020-05-12 09:12:52+00:00,,0,4,module: build module: cpp triaged,True
38273,Adding model metadata in TorchScript model file triage review oncall: jit module: serialization,2020-05-11 21:49:34+00:00,,0,1,triage review oncall: jit module: serialization,True
38272,"PyTorch's rot90 returns a new tensor, inconsistent with NumPy's returning a view triaged module: numpy",2020-05-11 21:31:54+00:00,,0,2,triaged module: numpy,True
38271,"PyTorch's flip returns a new tensor, but NumPy's flip returns a view triaged module: numpy",2020-05-11 21:30:57+00:00,,0,3,triaged module: numpy,True
38245,Make torch.rpc accept store as optional parameter feature triaged module: rpc,2020-05-11 17:19:30+00:00,,0,1,feature triaged module: rpc,True
38241,SIGXCPU at test_cholesky_solve with AMD EPYC 7742 64-Core Processor module: tests triaged,2020-05-11 17:01:15+00:00,,0,1,module: tests triaged,True
38240,RuntimeError: arg_types.size() == param_names.size() - (moduleSelf_ ? 1 : 0) INTERNAL ASSERT FAILED oncall: jit module: typing triaged,2020-05-11 16:22:59+00:00,,0,2,oncall: jit module: typing triaged,True
38239,Failed to link torch_library using cmake module: build module: cpp triaged,2020-05-11 16:21:32+00:00,,0,6,module: build module: cpp triaged,True
38233,When TorchScripted module has bad type annotation you get bad error message oncall: jit module: bootcamp triaged,2020-05-11 15:14:57+00:00,,0,4,oncall: jit module: bootcamp triaged,True
38231,Python autograd engine threads never terminate in Python 3.5-3.8 module: crash module: autograd module: ci triaged,2020-05-11 14:50:41+00:00,,1,4,module: crash module: autograd module: ci triaged,True
38228,pybind11::gil_scoped_release: crash on exit with daemon threads high priority module: dependency bug module: crash triaged,2020-05-11 14:38:29+00:00,,1,13,high priority module: dependency bug module: crash triaged,True
38212,"Please help, building failing with MAGMA support module: build triaged",2020-05-11 05:47:47+00:00,,0,2,module: build triaged,True
38208,nn.Module Abstract Class attribute overrides Child. triaged,2020-05-10 17:32:58+00:00,,0,2,triaged,True
38206,Loops are very slow compared to tensorflow triage review oncall: jit,2020-05-10 12:32:50+00:00,,1,6,triage review oncall: jit,True
38205,RESOLVED: Disable zero-dim CUDA tensors interacting with CUDA tensors on other devices module: bc-breaking module: cuda triaged enhancement,2020-05-10 09:51:21+00:00,,0,2,module: bc-breaking module: cuda triaged enhancement,True
38204,missing dependency `protobuf` for caffe2 module: build triaged,2020-05-10 08:10:15+00:00,,0,10,module: build triaged,True
38202,IndexError reports the wrong dimension when fancy indexing module: error checking triaged,2020-05-10 00:39:49+00:00,,0,1,module: error checking triaged,True
38197,Why no `torch.randperm_like`? triaged enhancement module: tensor creation,2020-05-09 17:27:17+00:00,,0,3,triaged enhancement module: tensor creation,True
38193,Can't Build Pytorch0.4.1 From Source module: build triaged,2020-05-09 11:52:35+00:00,,0,6,module: build triaged,True
38190,test_ReplicationPad3d (test_nn.TestNN) takes too long to run module: tests triaged,2020-05-09 07:45:20+00:00,,0,0,module: tests triaged,True
38189,test_LocalResponseNorm_3d_custom_params (test_nn.TestNN) takes too long to run module: tests triaged,2020-05-09 07:44:44+00:00,,0,0,module: tests triaged,True
38188,test_interpolate_nearest_scale_3d in test_nn takes too long to run module: tests triaged,2020-05-09 07:43:56+00:00,,0,0,module: tests triaged,True
38185,Feature Request: Sampled softmax loss feature module: nn module: loss triaged,2020-05-09 06:20:47+00:00,,0,1,feature module: nn module: loss triaged,False
38184,Feature Request: Log uniform candidate sampler feature module: nn triaged,2020-05-09 06:18:25+00:00,,0,0,feature module: nn triaged,False
38177,valgrind says libtorch has memory leak module: memory usage triaged,2020-05-09 02:36:58+00:00,,0,1,module: memory usage triaged,True
38170,"OneCycleLR  raises ""Tried to step { step_num + 1 } times"" after the value is more than expected. triaged",2020-05-08 23:40:59+00:00,,0,1,triaged,True
38155,"Python script name with ""profile.py"" will run twice needs reproduction triaged",2020-05-08 21:32:59+00:00,,0,7,needs reproduction triaged,False
38142,[JIT] Aliased Python references to script::Object escape IValue reference counting oncall: jit triaged,2020-05-08 19:05:26+00:00,,0,0,oncall: jit triaged,True
38138,"TorchScript to support == None, != None oncall: jit triaged",2020-05-08 18:48:44+00:00,,0,2,oncall: jit triaged,True
38135,test_cpp_warnings_have_python_context_cpu fails under some build configurations module: tests triaged,2020-05-08 18:33:34+00:00,,0,2,module: tests triaged,True
38122,Pybind11 cpp extensions broken with pytorch v1.5.0 high priority module: build module: docs module: cpp triaged module: regression,2020-05-08 15:56:21+00:00,,1,21,high priority module: build module: docs module: cpp triaged module: regression,True
38118,Add helpers to save/load RPC-based models feature triaged module: rpc,2020-05-08 14:35:34+00:00,,0,0,feature triaged module: rpc,True
38090,ImportError: libtorch_cpu.so: cannot open shared object file: No such file or directory module: build triaged,2020-05-08 00:24:05+00:00,,0,12,module: build triaged,True
38057,DNNL's backward pass much slower when using nn.grad.conv2d_input and nn.grad.conv2d_weight module: autograd module: nn triaged module: mkldnn,2020-05-07 21:42:49+00:00,,0,5,module: autograd module: nn triaged module: mkldnn,True
38051,Div by zero error not triggered and inf not returned when dividing by 0 for some dtypes module: numerical-stability triaged module: type promotion,2020-05-07 21:21:02+00:00,,1,8,module: numerical-stability triaged module: type promotion,True
38035,Quantile Regression Loss module: nn triaged OSS contribution wanted,2020-05-07 19:06:44+00:00,,0,3,module: nn triaged OSS contribution wanted,True
38034,Support Slicing of ModuleList during JIT model tracing/scripting  oncall: jit module: bootcamp triaged large medium,2020-05-07 19:04:37+00:00,,1,5,oncall: jit module: bootcamp triaged large medium,True
38019,"torch.cuda.nccl.init_rank does not handle ""uid"" properly, causing runtime error module: cuda triaged module: nccl",2020-05-07 16:22:44+00:00,,0,1,module: cuda triaged module: nccl,True
38012,Formatting of topic and sub-topic pages module: docs triaged module: doc infra,2020-05-07 12:32:19+00:00,,0,0,module: docs triaged module: doc infra,False
38010,Split more pages into sub-topics module: docs triaged module: doc infra open source,2020-05-07 12:22:01+00:00,,0,1,module: docs triaged module: doc infra open source,False
38009,Resnet Model always predicting same label triaged module: vision,2020-05-07 11:40:55+00:00,,0,2,triaged module: vision,True
38006,Inconsistent Documentation about Optimizer.step(closure) module: docs module: optimizer triaged,2020-05-07 09:30:27+00:00,,0,7,module: docs module: optimizer triaged,True
38000,"New padding size format for F.pad, allowing named tensors and more clear syntax overall feature module: nn triaged",2020-05-07 05:59:42+00:00,,0,3,feature module: nn triaged,True
37995,unsqueeze support for named tensors triaged enhancement module: named tensor,2020-05-07 04:48:08+00:00,,0,2,triaged enhancement module: named tensor,True
37988,JIT compilation of NamedTuple Containing a NamedTuple fails oncall: jit triaged,2020-05-07 01:22:56+00:00,,0,1,oncall: jit triaged,True
37987,Special methods on torchscript custom class oncall: jit triaged,2020-05-07 01:20:42+00:00,,1,1,oncall: jit triaged,True
37985,torch.save incompatible with lzma file module: serialization triaged,2020-05-07 00:20:17+00:00,,0,9,module: serialization triaged,True
37928,CUDA sources are not cached with sccache module: build module: windows module: cuda module: ci triaged module: regression,2020-05-06 14:43:04+00:00,,1,13,module: build module: windows module: cuda module: ci triaged module: regression,True
37895,torch.addmv can't take as input tensors with different dtypes triaged module: type promotion,2020-05-06 00:24:14+00:00,,0,3,triaged module: type promotion,True
37883,Support @property decorator in TorchScript triage review oncall: jit,2020-05-05 21:27:07+00:00,,0,6,triage review oncall: jit,True
37880,"[RFC] Don't install CI dependencies in build scripts, install them in underlying docker images module: ci triaged",2020-05-05 21:19:49+00:00,,0,3,module: ci triaged,True
37863,Deprecate type() and type_as() call triaged module: deprecation module: ux,2020-05-05 19:30:58+00:00,,1,0,triaged module: deprecation module: ux,True
37860,autograd engine callbacks don't respect non-default cuda streams module: autograd module: cuda triaged,2020-05-05 19:25:45+00:00,,0,13,module: autograd module: cuda triaged,True
37847,llvmlite version issue when upgrading CI docker image to python 3.8 module: ci triaged,2020-05-05 18:35:05+00:00,,0,0,module: ci triaged,True
37837,[DRAFT] Channels Last + AMP support plan for 1.6 release triaged module: memory format,2020-05-05 17:00:59+00:00,,1,3,triaged module: memory format,True
37814,DistributedDataParallel does not support Modules that take no inputs. triaged module: data parallel,2020-05-05 06:02:18+00:00,,0,2,triaged module: data parallel,True
37809,"Add Numpy-like ""order"" argument to reshape feature triaged module: numpy has workaround",2020-05-05 03:42:25+00:00,,0,2,feature triaged module: numpy has workaround,True
37782,Guard Gloo and TensorPipe related code in RPC with #ifdef module: bootcamp triaged module: rpc module: tensorpipe,2020-05-04 18:46:29+00:00,,0,0,module: bootcamp triaged module: rpc module: tensorpipe,True
37766,CMake Documentation Issue module: docs triaged,2020-05-04 15:19:11+00:00,,0,5,module: docs triaged,True
37762,Add docstring to `torch/__init__.pyi`? module: docs triaged enhancement module: doc infra,2020-05-04 11:27:13+00:00,,0,10,module: docs triaged enhancement module: doc infra,False
37751,Documentation of _CtxMethodMixin: must be tensors? module: docs module: autograd triaged,2020-05-04 07:31:07+00:00,,1,5,module: docs module: autograd triaged,True
37740,torch Summary writer does not display torchvision.io.read_video output module: tensorboard oncall: visualization,2020-05-03 19:44:12+00:00,,0,0,module: tensorboard oncall: visualization,False
37734,torch.cdist returns inconsistent result module: numerical-stability module: docs triaged module: distance functions,2020-05-03 15:37:48+00:00,,0,16,module: numerical-stability module: docs triaged module: distance functions,True
37731,Variational Dropout In RNN module: rnn triaged enhancement,2020-05-03 10:54:27+00:00,,0,0,module: rnn triaged enhancement,True
37713,Builtin FusedLayerNorm is slower than apex one module: performance module: cuda triaged,2020-05-02 07:52:25+00:00,,0,4,module: performance module: cuda triaged,True
37684,"Add Compound key, make custom ops default to it (but keep internal users using CatchAll) triaged internals",2020-05-01 19:08:35+00:00,,0,0,triaged internals,True
37682,LSTMCell consumes x1.5 more memory on CUDA on pytorch >=1.3 comparing to pytorch 1.2 module: rnn module: cuda module: memory usage triaged,2020-05-01 18:58:30+00:00,,0,2,module: rnn module: cuda module: memory usage triaged,True
37664,Remove everything from a GPU (including drivers) module: cuda triaged enhancement,2020-05-01 14:38:14+00:00,,0,3,module: cuda triaged enhancement,True
37629,logging_is_not_google_glog.h:24:11: error: 'const int ERROR' redeclared as different kind of symbol  (v1.3.0) module: build triaged,2020-04-30 23:24:22+00:00,,0,3,module: build triaged,True
37598,NCCL fails to find cuda include dir oncall: distributed triaged module: nccl,2020-04-30 18:56:47+00:00,,0,1,oncall: distributed triaged module: nccl,True
37588,"test_qnnpack_sigmoid failing on old CPU, built pytorch 1.5.0 module: binaries module: build triaged",2020-04-30 17:19:25+00:00,,0,8,module: binaries module: build triaged,True
37585,Unable to use torch.det() inside nn.DataParallel with multiple gpus module: cuda triaged module: data parallel module: linear algebra,2020-04-30 16:31:35+00:00,,0,3,module: cuda triaged module: data parallel module: linear algebra,True
37556,"torch.cartesian_prod(*tensors) error when you have tensors with [x,y] triaged module: linear algebra",2020-04-30 02:05:20+00:00,,0,1,triaged module: linear algebra,True
37554,Add name to Class Parameter() oncall: distributed module: nn triaged enhancement module: rpc,2020-04-30 01:28:30+00:00,,0,7,oncall: distributed module: nn triaged enhancement module: rpc,True
37549,Are there any differences in kernel memory between RX2080's and Quadro RTX4000?   module: cuda module: memory usage triaged,2020-04-29 23:58:39+00:00,,0,2,module: cuda module: memory usage triaged,True
37529,Implement generic function scheduler in c10/util triaged module: rpc,2020-04-29 20:37:33+00:00,,1,0,triaged module: rpc,True
37512,Improve visibility in test suite timings module: ci module: tests triaged better-engineering,2020-04-29 18:45:32+00:00,,0,5,module: ci module: tests triaged better-engineering,True
37510,"Cannot build pytorch with linker arguments in C{,XX}FLAGS module: build triaged",2020-04-29 17:18:35+00:00,,0,0,module: build triaged,True
37488,undefined reference to pthreadpool_compute* module: build triaged module: xnnpack,2020-04-29 06:22:37+00:00,,0,8,module: build triaged module: xnnpack,True
37487,Log-linear version of cumsum and cumprod module: performance feature triaged,2020-04-29 06:12:10+00:00,,0,6,module: performance feature triaged,True
37484,No speedup from channels_last with DataParallel module: performance triaged module: data parallel module: memory format,2020-04-29 04:53:59+00:00,,1,2,module: performance triaged module: data parallel module: memory format,True
37457,[RuntimeError] Tensor creation using storage fails module: error checking triaged,2020-04-28 20:44:39+00:00,,0,1,module: error checking triaged,True
37449,RuntimeError: CUDA error: an illegal memory access was encountered with channels_last high priority module: dependency bug module: binaries module: cudnn module: cuda triaged,2020-04-28 19:35:49+00:00,,0,12,high priority module: dependency bug module: binaries module: cudnn module: cuda triaged,True
37444,Multi-Process Single-GPU is bad oncall: distributed triaged module: data parallel,2020-04-28 18:54:57+00:00,,0,10,oncall: distributed triaged module: data parallel,True
37443,torch.clamp_max clamp_min shouldn't be there triaged better-engineering,2020-04-28 18:54:53+00:00,,0,2,triaged better-engineering,True
37442,Tensor.is_distributed not documented oncall: distributed module: docs,2020-04-28 18:52:14+00:00,,0,1,oncall: distributed module: docs,False
37441,Tensor.as_strided_ is not documented module: docs triaged,2020-04-28 18:51:18+00:00,,0,2,module: docs triaged,True
37440,Tensor.is_same_size not documented module: docs triaged,2020-04-28 18:49:30+00:00,,0,0,module: docs triaged,True
37434,rsub incorrectly exposed in torch triaged better-engineering,2020-04-28 18:35:14+00:00,,0,0,triaged better-engineering,True
37416,[docs] Unclear return type of torch.randint and extra comma in arg spec module: docs triaged,2020-04-28 15:30:25+00:00,,0,0,module: docs triaged,False
37411,"Questions about ""torch.utils.tensorboard.add_graph"": Could I use it to see network graph's compute time and memory?  module: docs triaged module: tensorboard",2020-04-28 10:18:31+00:00,,0,2,module: docs triaged module: tensorboard,True
37410,Reset a `torch.optim.Optimizer` module: optimizer triaged,2020-04-28 09:35:50+00:00,,0,11,module: optimizer triaged,False
37409,Compatibility of subset dataset with disabled batch sampling module: dataloader triaged,2020-04-28 09:32:27+00:00,,0,4,module: dataloader triaged,True
37408,torch.cdist() implementation without using contiguous() calls module: performance triaged enhancement module: distance functions,2020-04-28 09:31:22+00:00,,0,4,module: performance triaged enhancement module: distance functions,True
37406,3D grouped & depthwise convolution very slow on backward pass module: cudnn module: cuda triaged,2020-04-28 08:13:45+00:00,,0,1,module: cudnn module: cuda triaged,True
37387,The pytorch's graph is lack of common names for nodes module: bootcamp feature module: tensorboard oncall: visualization days,2020-04-28 02:44:47+00:00,,2,3,module: bootcamp feature module: tensorboard oncall: visualization days,False
37386,Add BufferDict container feature module: nn triaged,2020-04-28 01:14:36+00:00,,0,12,feature module: nn triaged,False
37384,Error running trace on Pytorch Crowd Counting model oncall: jit triaged,2020-04-28 00:46:25+00:00,,0,15,oncall: jit triaged,True
37373,Move bernoulli_() to DistributionTemplates triaged module: random,2020-04-27 21:56:24+00:00,,1,2,triaged module: random,True
37363,"cdist docs imply only one batch dimension is possible, but actually arbitrarily many are allowed module: docs triaged module: distance functions",2020-04-27 21:14:26+00:00,,0,0,module: docs triaged module: distance functions,False
37354,Efficient handling special gradient values in the autograd module: performance feature module: autograd triaged,2020-04-27 18:59:45+00:00,,0,9,module: performance feature module: autograd triaged,True
37344,[JIT] Not support for MaskRCNNPredictor needs reproduction oncall: jit triaged,2020-04-27 14:38:15+00:00,,0,8,needs reproduction oncall: jit triaged,True
37343,qnnpack's quantized-add gives wrong result oncall: quantization triaged oncall: mobile,2020-04-27 12:40:40+00:00,,2,4,oncall: quantization triaged oncall: mobile,True
37334,Why do we not use TorchScript to build graph for tensorboard feature triaged module: tensorboard oncall: visualization,2020-04-27 03:39:57+00:00,,0,2,feature triaged module: tensorboard oncall: visualization,True
37332,Concerning default confiugration for distribution packages module: binaries module: build triaged,2020-04-27 03:20:39+00:00,,0,8,module: binaries module: build triaged,True
37324,Multiprocessing: model shared between processes hangs during copy.deepcopy module: multiprocessing module: cuda triaged,2020-04-26 21:17:55+00:00,,0,0,module: multiprocessing module: cuda triaged,True
37314,equal_nan keyword not implemented for complex torch.isclose triaged module: complex module: numpy,2020-04-26 08:43:44+00:00,,0,0,triaged module: complex module: numpy,True
37300,Data caching module a la `Sampler` feature triaged,2020-04-25 13:20:08+00:00,,0,0,feature triaged,True
37282,python setup.py install error module: build triaged,2020-04-25 04:35:59+00:00,,0,1,module: build triaged,True
37261,"intermittent failures of ""test_remote_script_module"" high priority triage review oncall: jit module: flaky-tests",2020-04-24 21:31:06+00:00,,0,2,high priority triage review oncall: jit module: flaky-tests,True
37250,Inconsistency between GPU memory usage in torch.cuda.memory_summary and nvidia-smi module: cuda module: memory usage triaged,2020-04-24 19:36:57+00:00,,0,5,module: cuda module: memory usage triaged,True
37246,`max_norm` parameter on nn.Embedding will fail inside nn.DataParallel module: nn triaged module: data parallel,2020-04-24 18:32:03+00:00,,0,7,module: nn triaged module: data parallel,True
37208,RuntimeError: Tried to instantiate class __file__.__file__ but it does not exist! Ensure that it is registered via torch::jit::class_ needs reproduction oncall: jit triaged,2020-04-24 03:19:23+00:00,,0,2,needs reproduction oncall: jit triaged,True
37204,Make it harder to make SIOF bugs for torchbind classes referenced by schemas triaged better-engineering,2020-04-24 02:49:36+00:00,,0,0,triaged better-engineering,True
37162,torch.distributions bug in RelaxedOneHotCategorical.log_prob  module: distributions triaged,2020-04-23 18:30:26+00:00,,0,0,module: distributions triaged,True
37160,Enhanced operator context when reporting errors triaged enhancement module: shape checking,2020-04-23 18:17:50+00:00,,0,1,triaged enhancement module: shape checking,True
37155,Per-cluster biases in AdaptiveLogSoftmaxWithLoss feature module: nn triaged needs research,2020-04-23 17:22:06+00:00,,1,1,feature module: nn triaged needs research,True
37153,CPU out of bound memory access in CUDA reduction kernel config module: cuda triaged module: TensorIterator,2020-04-23 17:04:23+00:00,,0,7,module: cuda triaged module: TensorIterator,True
37149,Cuda profiler + DataParallel + manual profiling start = strange profiling overhead pattern oncall: distributed triaged,2020-04-23 14:39:38+00:00,,0,2,oncall: distributed triaged,True
37142,Propagation of channels-last layout leads to massive slowdowns in 1.5 compared to 1.4 module: performance module: internals triaged module: memory format,2020-04-23 12:02:14+00:00,,2,13,module: performance module: internals triaged module: memory format,True
37136,[Design][RFC] RemoteModule API Design oncall: jit triaged module: rpc,2020-04-23 08:03:15+00:00,,1,0,oncall: jit triaged module: rpc,True
37113,pip install torch==1.4.0 is broken when using CUDA 10.1 module: build triaged,2020-04-22 23:01:17+00:00,,0,6,module: build triaged,True
37104,[JIT] Use of global value creates confusing error message oncall: jit triaged small,2020-04-22 21:17:04+00:00,,0,2,oncall: jit triaged small,True
37103,Legacy fuser doesn't do remainder consistently with `aten::remainder` oncall: jit triaged,2020-04-22 21:07:18+00:00,,0,2,oncall: jit triaged,True
37092,nn.Bilinear cannot be used inside nn.Sequential feature module: nn triaged,2020-04-22 18:29:31+00:00,,0,4,feature module: nn triaged,True
37079,Result parameters from Single-Process Multi-GPU DDP training on RNN do not match local training oncall: distributed triaged,2020-04-22 15:57:16+00:00,,0,4,oncall: distributed triaged,True
37067,can't wrap two models in the same class needs reproduction triaged,2020-04-22 13:00:28+00:00,,0,1,needs reproduction triaged,True
37066,Saved model behaves differently on same data needs reproduction module: serialization triaged,2020-04-22 12:59:46+00:00,,0,1,needs reproduction module: serialization triaged,True
37065,Deprecated mask fill mask type can causes pages and pages of repeated messages triaged,2020-04-22 12:54:12+00:00,,1,1,triaged,True
37063,Misleading documentation in torch.nn.functional.fold module: docs module: nn triaged,2020-04-22 10:53:54+00:00,,0,5,module: docs module: nn triaged,True
37057,CUDA error: device-side assert triggered @ model.cuda() needs reproduction module: cuda triaged,2020-04-22 06:12:04+00:00,,0,1,needs reproduction module: cuda triaged,True
37048,copy_ slowness module: performance module: cuda triaged,2020-04-22 03:05:40+00:00,,0,4,module: performance module: cuda triaged,True
37016,Guidance on implementing a new backend oncall: mobile,2020-04-21 18:30:20+00:00,,0,7,oncall: mobile,False
37015,Converting to Torch Script: cpp_module does not match nn_module high priority triage review oncall: jit triaged,2020-04-21 18:27:03+00:00,,1,4,high priority triage review oncall: jit triaged,True
37010,Make it an error to def() an operator multiple times module: bootcamp triaged,2020-04-21 17:34:20+00:00,,0,2,module: bootcamp triaged,True
37004,"RuntimeError: NCCL error in ProcessGroupNCCL.cpp:290, unhandled system error module: dependency bug oncall: distributed triaged module: nccl",2020-04-21 16:24:25+00:00,,0,2,module: dependency bug oncall: distributed triaged module: nccl,True
37002,[RFC] Modularize DistributedDataParallel oncall: distributed feature triaged,2020-04-21 16:09:12+00:00,,0,7,oncall: distributed feature triaged,False
36999,Add a CI configuration to test USE_DISTRIBUTED=0 oncall: distributed module: ci module: tests triaged,2020-04-21 14:08:13+00:00,,0,1,oncall: distributed module: ci module: tests triaged,True
36990,[docs] Explain active_bytes in torch.cuda.memory_stats and Cuda Memory Management module: docs triaged,2020-04-21 08:28:28+00:00,,0,4,module: docs triaged,True
36989,pytorch latest update(1.4) broke CosineAnnealingWarmRestarts: T_cur is not define module: optimizer triaged,2020-04-21 08:04:37+00:00,,0,3,module: optimizer triaged,True
36931,Pull hacked twins out of prim ops oncall: jit triaged,2020-04-20 16:09:10+00:00,,1,0,oncall: jit triaged,True
36926,JIT string ops and other miscellaneous ops probably shouldn't be in aten namespace triage review oncall: jit triaged,2020-04-20 15:01:58+00:00,,0,2,triage review oncall: jit triaged,True
36922,Multiprocess DataLoader with DLPack conversion sometimes corrupts memory module: multiprocessing module: serialization triaged module: xnnpack,2020-04-20 14:49:48+00:00,,0,6,module: multiprocessing module: serialization triaged module: xnnpack,True
36917,"jit trace failed due to ""failed to differentiate `prim::ListConstruct`"" oncall: jit triaged",2020-04-20 08:18:10+00:00,,0,2,oncall: jit triaged,True
36914,Problem with c10/utils/variant.h module: cpp triaged,2020-04-20 06:58:10+00:00,,0,1,module: cpp triaged,True
36910,jit.script leads to RuntimeError: attribute lookup is not defined on python value of type in some cases triage review oncall: jit triaged small,2020-04-20 02:39:08+00:00,,0,4,triage review oncall: jit triaged small,True
36891,pytorch and c++ inference disagree module: cpp triaged,2020-04-19 07:53:47+00:00,,0,1,module: cpp triaged,True
36874,Memory leak upon exception for interactive consoles module: memory usage triaged,2020-04-18 18:49:43+00:00,,0,2,module: memory usage triaged,True
36871,Stochasticity for DistributedDataParallel on CPU but not on GPU high priority needs reproduction oncall: distributed triaged module: determinism module: data parallel,2020-04-18 17:31:21+00:00,,0,31,high priority needs reproduction oncall: distributed triaged module: determinism module: data parallel,True
36804,c++: error: unrecognized command line option '-Wthread-safety' module: build triaged,2020-04-17 15:49:49+00:00,,0,3,module: build triaged,True
36754,Clearer guidance on when to define an operator as a method on torchbind'ed class versus standalone function triaged,2020-04-16 22:14:53+00:00,,0,5,triaged,True
36753,When you try to register a kernel that make boxed from unboxed functor doesn't support you get a horrible error message triaged,2020-04-16 21:57:04+00:00,,0,0,triaged,True
36748,return_index option for torch.unique triaged enhancement module: numpy function request,2020-04-16 20:07:43+00:00,,0,23,triaged enhancement module: numpy function request,True
36739,Custom class type name is very wordy oncall: jit triaged,2020-04-16 19:16:54+00:00,,0,1,oncall: jit triaged,True
36733,Custom Generators don't work in JIT triaged,2020-04-16 18:35:54+00:00,,0,0,triaged,True
36721,[1.4.1] Cuda build fails needs reproduction module: build triaged,2020-04-16 12:11:50+00:00,,0,10,needs reproduction module: build triaged,True
36718,Support alpha channel in tensorboard.add_figure module: tensorboard oncall: visualization,2020-04-16 08:10:54+00:00,,0,5,module: tensorboard oncall: visualization,False
36713,variable name N_ conflicts with an internationalization macro in glib oncall: jit module: build triaged,2020-04-16 04:32:34+00:00,,0,4,oncall: jit module: build triaged,True
36706,Reduction for `torch.int8` is super slow on CUDA module: performance module: cuda triaged module: TensorIterator,2020-04-16 02:41:52+00:00,,0,7,module: performance module: cuda triaged module: TensorIterator,True
36650,state_dict and load_state_dict methods for DataLoader and Sampler to continue training at specific epoch and batch feature module: dataloader triaged,2020-04-15 11:22:04+00:00,,0,15,feature module: dataloader triaged,True
36649,"RuntimeError: Expected cuda::check_device({sparse_, r_, t, dense}) to be true, but got false.   needs reproduction triaged module: data parallel",2020-04-15 09:25:11+00:00,,0,3,needs reproduction triaged module: data parallel,True
36638,Poor elmenetwise_kernel performance becomes critical on small mini-batch sizes module: performance module: cuda triaged module: TensorIterator,2020-04-15 02:57:18+00:00,,0,2,module: performance module: cuda triaged module: TensorIterator,True
36603,TorchScript Support for Named Tensors triage review oncall: jit triaged,2020-04-14 19:59:18+00:00,,0,0,triage review oncall: jit triaged,True
36577,Add load_state_dict and state_dict() in C++ module: cpp triaged,2020-04-14 11:52:24+00:00,,0,7,module: cpp triaged,True
36575,One confusion about the CompilationUnit destructuring process in torch/jit/__init__.py  oncall: jit triaged,2020-04-14 11:25:41+00:00,,0,2,oncall: jit triaged,True
36572,[1.4.1] cmake3 not found module: build triaged,2020-04-14 09:45:06+00:00,,0,3,module: build triaged,True
36570,Libtorch build error when setting both `USE_GLOO` and `USE_SYSTEM_NCCL` to `ON` oncall: distributed module: build module: docs triaged module: nccl,2020-04-14 08:54:00+00:00,,0,8,oncall: distributed module: build module: docs triaged module: nccl,True
36560,Allow `__array__` to automatically detach and move to CPU feature module: cuda triaged module: numpy,2020-04-14 05:03:29+00:00,,0,15,feature module: cuda triaged module: numpy,True
36553,Typecasting issue in MSELoss module: loss triaged module: type promotion,2020-04-14 02:06:07+00:00,,0,1,module: loss triaged module: type promotion,True
36524,"Drop _stacklevel from argspecs of F.softmax, F.softmin, F.log_softmax (for implicit dim has been long deprecated) module: docs triaged",2020-04-13 22:21:54+00:00,,0,4,module: docs triaged,True
36519,Restructure test_c10d.py and test_distributed.py oncall: distributed triaged better-engineering,2020-04-13 21:33:05+00:00,,0,0,oncall: distributed triaged better-engineering,True
36517,XNNPACK operators are not actually registered under xnnpack namespace triaged better-engineering module: xnnpack,2020-04-13 21:30:58+00:00,,1,4,triaged better-engineering module: xnnpack,True
36516,DDP should divide bucket contents by the number of global replicas instead of world size oncall: distributed triaged,2020-04-13 21:30:40+00:00,,0,0,oncall: distributed triaged,True
36508,Quantized _out functions don't follow same conventions as other out functions in the codebase oncall: quantization low priority triaged better-engineering,2020-04-13 20:31:56+00:00,,1,6,oncall: quantization low priority triaged better-engineering,True
36469,Wrong results for multiplication of non-finite complex numbers with real numbers triaged module: complex module: numpy,2020-04-13 01:54:08+00:00,,0,1,triaged module: complex module: numpy,True
36444,Comparison ops for Complex Tensors triaged module: complex module: numpy,2020-04-12 00:12:51+00:00,,0,6,triaged module: complex module: numpy,True
36437,Issue when linking C++ code with libtorch_cpu: cuda not detected module: build module: cpp triaged,2020-04-11 16:20:57+00:00,,0,5,module: build module: cpp triaged,True
36436,"After `create_graph=True`, calculating `backward()` on sparse Tensor fails triaged enhancement",2020-04-11 15:44:36+00:00,,0,0,triaged enhancement,True
36426,Any reference to LPPool2d module: docs triaged,2020-04-11 04:59:51+00:00,,0,1,module: docs triaged,True
36407,Unable to build wheel from RC3 module: build triaged,2020-04-10 19:48:57+00:00,,0,0,module: build triaged,True
36386,Docs of distributed module do not include the full documentation for torch.distributed.launch module: docs triaged,2020-04-10 13:50:06+00:00,,0,2,module: docs triaged,True
36380,Population Count Op feature triaged,2020-04-10 09:42:48+00:00,,0,1,feature triaged,False
36378,CMake targets wrongly forward unknown options to NVCC (v1.5+) module: build triaged,2020-04-10 08:59:16+00:00,,0,3,module: build triaged,True
36370,Jit doesn't match schema like eager mode does oncall: jit triaged,2020-04-10 06:48:32+00:00,,1,0,oncall: jit triaged,True
36356,Memory leak issue still exists in CI  module: build module: ci triaged,2020-04-10 01:06:01+00:00,,0,1,module: build module: ci triaged,True
36333,Illegal memory access / CUDNN_STATUS_MAPPING_ERROR on Quadro 8000 module: cudnn triaged,2020-04-09 19:46:10+00:00,,0,13,module: cudnn triaged,True
36317,Updating a ModelDict instance with another ModelDict instance generates error. module: nn triaged,2020-04-09 14:58:20+00:00,,0,2,module: nn triaged,True
36314,[discussion] Refactor spectral_norm to use the newly merged lowrank solvers and proposal for Linear Algebra Cookbook page triaged module: linear algebra,2020-04-09 13:55:14+00:00,,0,1,triaged module: linear algebra,True
36309,"torch.ceil, torch.floor should accept a dtype argument triaged module: numpy module: ux",2020-04-09 11:38:07+00:00,,0,3,triaged module: numpy module: ux,True
36307,[docs] Matrix transpose missing in formula in nn.functional.bilinear docs module: docs triaged,2020-04-09 11:10:45+00:00,,0,4,module: docs triaged,False
36217, Rectify docs for MultiLabelSoftMarginLoss module: docs module: nn triaged,2020-04-08 09:57:32+00:00,,0,3,module: docs module: nn triaged,True
36211,[JIT] support self.named_buffers and self.named_parameters in TorchScript triage review oncall: jit triaged enhancement,2020-04-08 05:31:14+00:00,,1,5,triage review oncall: jit triaged enhancement,True
36182,## 🐛 Bug: QNNPACK tests failing on master on Nexus 6 oncall: mobile,2020-04-07 23:33:04+00:00,,0,0,oncall: mobile,False
36175,New dtype ComplexPolarFloat (phasor) low priority triaged module: complex enhancement,2020-04-07 21:54:35+00:00,,0,7,low priority triaged module: complex enhancement,True
36160,Decouple DDP from CUDA oncall: distributed feature module: cuda triaged,2020-04-07 20:03:05+00:00,,0,2,oncall: distributed feature module: cuda triaged,True
36152,Seg Fault: import vaex with torch module: binaries module: crash triaged,2020-04-07 17:37:19+00:00,,0,5,module: binaries module: crash triaged,True
36140,Inconsistent ProcessGroupMPI work data structure for send/recv and collectives module: performance oncall: distributed triaged better-engineering,2020-04-07 14:28:12+00:00,,0,0,module: performance oncall: distributed triaged better-engineering,True
36125,build Pytorch-1.5.0-rc1 from source fail module: build triaged,2020-04-07 03:28:51+00:00,,0,3,module: build triaged,True
36108,Add a flow_sample function to sample optical flows and displacement fields module: nn triaged enhancement,2020-04-06 22:46:15+00:00,,1,1,module: nn triaged enhancement,False
36107,Allow grid_sample to accept pixel units (absolute coordinates) proposal accepted module: nn triaged module: vision,2020-04-06 22:43:55+00:00,,1,2,proposal accepted module: nn triaged module: vision,True
36099,"In AutogradContext, get_saved_variables() should be renamed to get_saved_tensors() module: cpp triaged",2020-04-06 20:57:46+00:00,,0,0,module: cpp triaged,True
36091,[quantization] torch.quantized_lstm and torch.quantized_gru not documented module: docs oncall: quantization low priority triaged,2020-04-06 19:43:14+00:00,,1,0,module: docs oncall: quantization low priority triaged,True
36066,Add Specific Warning/Error For Unsupported GPU or Systems module: cuda triaged enhancement,2020-04-06 14:48:56+00:00,,0,4,module: cuda triaged enhancement,True
36062,RuntimeError CUDA error despite CUDA available and GPU supported module: binaries module: cuda module: error checking triaged,2020-04-06 14:11:07+00:00,,0,2,module: binaries module: cuda module: error checking triaged,True
36061,[JIT] support list of nn.Module in torchscript triage review oncall: jit triaged enhancement,2020-04-06 13:56:16+00:00,,0,16,triage review oncall: jit triaged enhancement,True
36059,Some tutorials cannot be found from both side panel and tutorial welcome page module: docs triaged,2020-04-06 07:21:37+00:00,,0,1,module: docs triaged,False
36058,Half type promotion with Numpy arrays is incorrect triaged module: numpy,2020-04-06 06:31:56+00:00,,1,4,triaged module: numpy,True
36041,[DISCUSSION] Better user experience for debugging on Windows module: build module: windows triaged,2020-04-05 13:35:32+00:00,,0,4,module: build module: windows triaged,True
36040,[JIT] Huge delay (1274s vs 0.031s) when running scripted model high priority oncall: jit triaged,2020-04-05 13:28:02+00:00,,1,5,high priority oncall: jit triaged,True
36036,torch.multinomial is misnamed.  module: distributions triaged module: ux,2020-04-05 05:28:33+00:00,,0,4,module: distributions triaged module: ux,True
36028,About tensorboard in pytorch record graph in different state triaged oncall: visualization,2020-04-04 14:54:19+00:00,,0,0,triaged oncall: visualization,False
36004,C++ tensor print doesn't show requires_grad and grad_fn like Python tensor print module: printing module: cpp triaged,2020-04-04 00:33:37+00:00,,0,2,module: printing module: cpp triaged,True
35998,JITed GRU too slow high priority triage review oncall: jit triaged,2020-04-03 22:56:33+00:00,,2,7,high priority triage review oncall: jit triaged,True
35990,"[JIT] Tensor method API behavior discrepancy, Tensor.detach(..) needs reproduction oncall: jit triaged",2020-04-03 22:10:48+00:00,,0,1,needs reproduction oncall: jit triaged,True
35964,TensorOptions shouldn't provide default values triaged better-engineering,2020-04-03 18:56:20+00:00,,1,0,triaged better-engineering,True
35944,"Copy activations from one parts to another part in tensor, but report error triaged",2020-04-03 09:51:33+00:00,,0,1,triaged,True
35942,Build PyTorch-1.4.0 from source failed module: build triaged,2020-04-03 09:26:00+00:00,,0,9,module: build triaged,True
35937,MKLDNN_conv2d 2X slower than the native TH implementation module: performance module: cpu triaged module: mkldnn,2020-04-03 06:15:17+00:00,,2,10,module: performance module: cpu triaged module: mkldnn,True
35881,Error when loading jit traced FasterRCNN model in C++ oncall: jit triaged,2020-04-02 14:51:04+00:00,,1,23,oncall: jit triaged,True
35872,torch.autograd.set_detect_anomaly(True) does not exist in C++? module: cpp triaged,2020-04-02 09:01:18+00:00,,1,3,module: cpp triaged,True
35870,CUDNN_STATUS_INTERNAL_ERROR with GPU RTX 8000 needs reproduction module: cudnn module: cuda triaged,2020-04-02 07:27:23+00:00,,0,17,needs reproduction module: cudnn module: cuda triaged,True
35832,Linking error after marking an op `manual_kernel_registration: True` module: build triaged,2020-04-01 19:48:59+00:00,,0,1,module: build triaged,True
35820,[JIT] Tracer bakes List[Tensor] attribute as constant oncall: jit triaged,2020-04-01 17:58:40+00:00,,0,0,oncall: jit triaged,True
35802,Hook on input tensor not called when using autograd.grad() module: autograd triaged actionable,2020-04-01 10:58:46+00:00,,1,2,module: autograd triaged actionable,True
35801,Add KFAC optimizer module: optimizer triaged needs research,2020-04-01 10:08:56+00:00,,1,3,module: optimizer triaged needs research,False
35798,Dropout on sparse tensors module: sparse module: nn triaged enhancement,2020-04-01 06:25:00+00:00,,0,4,module: sparse module: nn triaged enhancement,True
35797,Backward function causes device error in C++ when changing module's device repeatly. module: cpp module: autograd triaged,2020-04-01 06:20:40+00:00,,0,3,module: cpp module: autograd triaged,True
35792,Python 3 build PyTorch got core dump needs reproduction module: crash triaged,2020-04-01 03:31:06+00:00,,0,1,needs reproduction module: crash triaged,True
35788,[jit] Structural typing shouldn't work oncall: jit triaged,2020-04-01 00:22:11+00:00,,1,0,oncall: jit triaged,True
35775,Unexpected behaviour for affine_grid and grid_sample with 3D inputs module: numerical-stability module: nn triaged,2020-03-31 20:44:46+00:00,,0,6,module: numerical-stability module: nn triaged,True
35770,BatchNormFuncOptions object cant be printed in C++ module: cpp module: nn triaged,2020-03-31 18:34:26+00:00,,0,0,module: cpp module: nn triaged,True
35766,Clarify tensor storage communication behavior in RPC module: docs feature triaged module: rpc,2020-03-31 17:44:48+00:00,,0,4,module: docs feature triaged module: rpc,True
35759,how to do 3d data augmentation in parallel on the gpu? module: dataloader module: cuda triaged,2020-03-31 15:25:22+00:00,,0,1,module: dataloader module: cuda triaged,True
35758,UserRRef should store error if it sees any and prevent subsequent usage feature triaged module: rpc,2020-03-31 14:58:46+00:00,,0,0,feature triaged module: rpc,True
35755,Expected object of scalar type Double but got scalar type Float for argument #2 'mat2' in call to _th_mm triaged,2020-03-31 09:21:17+00:00,,0,1,triaged,True
35754,pytorch 1.4.0 hangs when using with CUDA   >=  10.1 needs reproduction module: cuda triaged,2020-03-31 08:41:19+00:00,,0,8,needs reproduction module: cuda triaged,True
35749,Cannot JIT functions with custom backwards (e.g. swish) oncall: jit module: autograd triaged,2020-03-31 05:54:16+00:00,,0,1,oncall: jit module: autograd triaged,True
35736,"Simple C++ custom autograd function code throws error ""CUDA error: driver shutting down"" module: cpp module: autograd triaged",2020-03-31 01:36:48+00:00,,0,12,module: cpp module: autograd triaged,True
35723,LibTorch API on Mobile module: cpp triaged oncall: mobile,2020-03-30 22:32:06+00:00,,0,1,module: cpp triaged oncall: mobile,True
35713,libtorch for Windows. MNIST example does no work. module: cpp triaged,2020-03-30 21:50:41+00:00,,0,13,module: cpp triaged,True
35681,TensorBoard add_scalars throws error when dict has keys of type int triaged,2020-03-30 16:46:21+00:00,,0,1,triaged,True
35678,USE_AVX/USE_AVX2 does not affect __AVX2__ macro defition triaged module: build warnings internals,2020-03-30 16:04:40+00:00,,0,1,triaged module: build warnings internals,True
35671,allgather_coalesced for tensors of different types seems to be broken oncall: distributed triaged,2020-03-30 13:35:00+00:00,,1,1,oncall: distributed triaged,True
35666,[discussion] Generic solutions for too-small-epsilon in FP16 training module: docs module: autograd triaged enhancement module: half,2020-03-30 10:46:09+00:00,,0,9,module: docs module: autograd triaged enhancement module: half,True
35661,"nn.LSTM gives nondeterministic results with dropout and multiple layers, OR cuDNN version mismatch module: cudnn module: cuda triaged module: determinism",2020-03-30 10:16:24+00:00,,0,6,module: cudnn module: cuda triaged module: determinism,True
35655,Wrong conv2d output on GPU when kernel has many zeros module: dependency bug module: numerical-stability module: cuda module: convolution triaged,2020-03-30 04:23:05+00:00,,0,3,module: dependency bug module: numerical-stability module: cuda module: convolution triaged,True
35651,Caffe2 utility_ops_gpu_test fails on Windows module: tests triaged,2020-03-30 01:19:25+00:00,,0,0,module: tests triaged,True
35648,Caffe2 ReshapeOpGPUTest crashes on Windows module: tests triaged,2020-03-29 21:19:58+00:00,,0,0,module: tests triaged,True
35642,Caching Support for class Dataset module: dataloader triaged enhancement,2020-03-29 15:22:43+00:00,,0,8,module: dataloader triaged enhancement,True
35641,"Dimension reducing variants of bitwise operations (bitwise_or, bitwise_and, bitwise_xor) triaged function request module: reductions",2020-03-29 08:48:25+00:00,,0,7,triaged function request module: reductions,True
35640,How do you change Adam learning rate since the latest commits ? module: cpp triaged,2020-03-29 08:45:52+00:00,,0,3,module: cpp triaged,True
35636,Caffe2 generate_proposals_op_gpu_test crashes on Windows module: tests triaged,2020-03-29 06:01:40+00:00,,0,1,module: tests triaged,True
35633,Integration of Large Model Support in PyTorch module: internals feature low priority module: memory usage triaged,2020-03-29 03:13:38+00:00,,0,20,module: internals feature low priority module: memory usage triaged,True
35609,Randomly error reports awaiting response (this tag is deprecated) triaged,2020-03-28 18:42:18+00:00,,0,1,awaiting response (this tag is deprecated) triaged,True
35606,"Could not find any similar ops to ""foo..."" in the `Libtorch` triage review oncall: jit triaged",2020-03-28 17:27:32+00:00,,0,1,triage review oncall: jit triaged,True
35603,Performance bug with convolutions with weights and inputs of similar spatial size module: performance module: cudnn module: convolution triaged,2020-03-28 15:36:32+00:00,,0,4,module: performance module: cudnn module: convolution triaged,True
35600,Increased memory usage in repetitive torch.jit.trace calls high priority oncall: jit triaged,2020-03-28 10:47:19+00:00,,1,9,high priority oncall: jit triaged,True
35597,libtorch_global_deps.so not found. module: build triaged module: regression,2020-03-28 06:32:27+00:00,,0,1,module: build triaged module: regression,True
35553,caffe2 `DEPTHWISE3x3.Conv` test is broken caffe2 module: tests triaged,2020-03-27 17:49:02+00:00,,0,1,caffe2 module: tests triaged,True
35535,Apple Review Rejected. ITMS-90338: Non-public API usage In Pytorch For iOS triaged module: ios,2020-03-27 13:09:37+00:00,,0,3,triaged module: ios,True
35534,Fail to load torch script model oncall: jit triaged,2020-03-27 13:06:30+00:00,,0,11,oncall: jit triaged,True
35529,Support bool input tensors for argmax / argmin / sort / topk and other functions triaged enhancement module: reductions,2020-03-27 11:38:50+00:00,,0,2,triaged enhancement module: reductions,True
35527,torch.bernoulli and torch.rand and torch.randint to support dtype=torch.bool kwarg (and operation of tensor.uniform_ / tensor.random_ on bool tensors) module: distributions triaged,2020-03-27 11:34:13+00:00,,0,13,module: distributions triaged,True
35526,RuntimeError during converting Reformer model to TorchScript triage review oncall: jit triaged,2020-03-27 11:19:34+00:00,,1,2,triage review oncall: jit triaged,True
35520,TorchScript can't use lists in conditionals oncall: jit triaged enhancement,2020-03-27 02:11:09+00:00,,0,2,oncall: jit triaged enhancement,True
35499,Support tensor.cumsum() for 1-dim tensors triaged module: numpy,2020-03-26 21:00:30+00:00,,0,3,triaged module: numpy,True
35472,Multiprocessing map gets stuck if doing inference on loaded model module: multiprocessing triaged,2020-03-26 17:23:13+00:00,,0,17,module: multiprocessing triaged,True
35458,Shared-QK transformer for the transformer (nn.activation.MultiheadAttention) module in PyTorch? triaged needs research oncall: transformer/mha,2020-03-26 06:18:28+00:00,,1,7,triaged needs research oncall: transformer/mha,True
35446,[JIT] dropout fails on legacy mode oncall: jit triaged,2020-03-26 02:04:16+00:00,,0,10,oncall: jit triaged,True
35441,"torch::normal only supports (double, double), but at::normal supports (double, double) / (double, Tensor) / (Tensor, double) / (Tensor, Tensor) module: cpp triaged",2020-03-26 01:00:45+00:00,,0,2,module: cpp triaged,True
35418,PyTorch / libtorch executables fail when built against libcuda stub library high priority module: binaries module: build triaged,2020-03-25 20:48:04+00:00,,0,8,high priority module: binaries module: build triaged,True
35414,"MacOS Error: subprocess.CalledProcessError: Command '['cmake', '--build', '.', '--target', 'install', '--config', 'Release', '--', '-j', '12']' returned non-zero exit status 1. module: build module: cuda triaged module: macos",2020-03-25 20:40:16+00:00,,0,3,module: build module: cuda triaged module: macos,True
35408,torch.jit.script works for x*sigmoid(x) but not for x*sin(x) oncall: jit triaged,2020-03-25 19:57:31+00:00,,0,1,oncall: jit triaged,True
35381,runtime error when loding heavy dataset needs reproduction triaged,2020-03-25 15:31:19+00:00,,0,1,needs reproduction triaged,True
35372,How to support single-process-multiple-devices in DistributedDataParallel other than CUDA device oncall: distributed,2020-03-25 08:52:08+00:00,,0,2,oncall: distributed,False
35363,NCCL version upgrade for PyTorch module: build triaged,2020-03-25 05:01:58+00:00,,0,3,module: build triaged,True
35312,Method/constructor which takes as input angle and magnitude and returns a complex tensor triaged module: complex,2020-03-24 18:40:16+00:00,,0,10,triaged module: complex,True
35294,ONNX export support for sparse tensors module: onnx triaged onnx-triaged,2020-03-24 13:26:20+00:00,,0,5,module: onnx triaged onnx-triaged,True
35290,Why C++ version libtorch so slow oncall: jit triaged,2020-03-24 11:45:35+00:00,,0,5,oncall: jit triaged,True
35266,Using profiler to profile distributed autograd code can lead to misleading results triaged module: rpc oncall: profiler,2020-03-24 01:18:05+00:00,,0,0,triaged module: rpc oncall: profiler,True
35234,Raspberry Pi Zero build fails module: build triaged,2020-03-23 20:50:13+00:00,,0,2,module: build triaged,True
35215,Get hold of Backward graph from a C++ module. oncall: jit,2020-03-23 16:41:23+00:00,,0,11,oncall: jit,False
35208,The BatchNorm error in `DataParallel` module: nn triaged module: data parallel,2020-03-23 12:57:42+00:00,,0,1,module: nn triaged module: data parallel,True
35180,hasSpecialCase INTERNAL ASSERT FAILED: We don't have an op for aten::to but it isn't a special case. oncall: jit triaged,2020-03-22 09:03:11+00:00,,0,9,oncall: jit triaged,True
35160,Documentation doesn't cover MWE using launch.py script todo oncall: distributed triaged,2020-03-21 17:20:31+00:00,,1,4,todo oncall: distributed triaged,True
35151,Segfault when using misaligned data pointer (from joblib) module: crash triaged module: numpy,2020-03-21 05:14:01+00:00,,0,3,module: crash triaged module: numpy,True
35128,Autograd Engine leaks reentrant threadpool threads on deletion module: autograd triaged better-engineering,2020-03-20 22:28:55+00:00,,0,1,module: autograd triaged better-engineering,True
35118,Customize batch size based on  gpu id triaged enhancement module: data parallel,2020-03-20 20:53:27+00:00,,0,2,triaged enhancement module: data parallel,True
35116,JIT does not support class instance attribute type annotation oncall: jit triaged enhancement days,2020-03-20 19:36:42+00:00,,0,2,oncall: jit triaged enhancement days,True
35108,"zip(list, tuple) throws an non-actionable error message oncall: jit triaged days TSRootCause:TypeChecking TSUsability",2020-03-20 18:33:30+00:00,,1,1,oncall: jit triaged days TSRootCause:TypeChecking TSUsability,True
35097,undefined symbols when using  libtorch and ITK module: build triaged,2020-03-20 09:42:58+00:00,,0,0,module: build triaged,True
35095,"tensorboard add_graph 's ""operator_export_type"" module: tensorboard oncall: visualization",2020-03-20 08:35:26+00:00,,0,1,module: tensorboard oncall: visualization,False
35078,Generator C++ API should match Python API module: cpp triaged module: random,2020-03-20 00:26:56+00:00,,0,0,module: cpp triaged module: random,True
35074,"[JIT] If a python function type comment is referring to a wrong type, JIT frontend gives a not helpful error message oncall: jit triaged",2020-03-19 23:51:47+00:00,,0,0,oncall: jit triaged,True
35071,Options for printing the shape with print(tensor) module: printing triaged,2020-03-19 22:57:10+00:00,,0,1,module: printing triaged,True
35041,Make operator registrations truly commutative using priority triaged module: dispatch,2020-03-19 16:37:00+00:00,,0,3,triaged module: dispatch,True
35040,Deprecate and remove RegisterOperators triaged module: dispatch,2020-03-19 16:33:16+00:00,,0,0,triaged module: dispatch,True
35038,Rename Dispatcher::findSchema to Dispatcher::findOperator triaged module: dispatch,2020-03-19 16:23:37+00:00,,0,0,triaged module: dispatch,True
35035,How install old version pytorch 1.2.0 from source? module: build triaged,2020-03-19 15:11:42+00:00,,0,2,module: build triaged,True
35026,Refactor record_function_ops.cpp to not use cpp_custom_type_hack oncall: jit triaged oncall: profiler,2020-03-19 05:13:48+00:00,,1,0,oncall: jit triaged oncall: profiler,True
34998,Better examples in functional autograd functions module: docs module: autograd triaged,2020-03-18 22:22:02+00:00,,0,1,module: docs module: autograd triaged,False
34977,`conv2d` is slow with specific shapes of channels_last tensors module: performance module: cudnn triaged module: memory format,2020-03-18 20:27:27+00:00,,0,4,module: performance module: cudnn triaged module: memory format,True
34953,Graphic tool to view the backward(Gradient Graph) and forward graph in Pytorch  triaged oncall: visualization,2020-03-18 14:06:45+00:00,,0,6,triaged oncall: visualization,False
34951,"[feature request] Sparse (hybrid sparse-dense) output option for topk, min, max module: sparse triaged",2020-03-18 12:22:26+00:00,,0,4,module: sparse triaged,True
34949,Equality operator for torch.distribution.* module: distributions triaged enhancement,2020-03-18 08:37:24+00:00,,0,9,module: distributions triaged enhancement,True
34948,TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function needs reproduction oncall: jit triaged,2020-03-18 08:14:53+00:00,,0,16,needs reproduction oncall: jit triaged,True
34941,Issues with DataParallel on Multiple GPUs triaged module: data parallel,2020-03-18 03:45:46+00:00,,0,4,triaged module: data parallel,True
34939,Enable OpaqueTensor to possess Storage then allow it to view from CPUTensor module: internals triaged module: mkldnn,2020-03-18 02:54:14+00:00,,0,27,module: internals triaged module: mkldnn,True
34937,[JIT]torch.jit.export invaild for pytorch1.4 needs reproduction oncall: jit triaged,2020-03-18 02:13:18+00:00,,0,2,needs reproduction oncall: jit triaged,True
34881,libtorch memory leak module: cpp module: memory usage triaged,2020-03-17 13:38:47+00:00,,0,0,module: cpp module: memory usage triaged,True
34880,Expanded tensors don't work as registered buffers module: nn module: error checking triaged module: partial aliasing,2020-03-17 12:37:26+00:00,,0,8,module: nn module: error checking triaged module: partial aliasing,True
34879,Addition of Siamese loss or contrastive loss function feature module: loss triaged,2020-03-17 10:50:58+00:00,,0,11,feature module: loss triaged,False
34878,Set the number of CUDAStream feature module: cuda triaged,2020-03-17 10:07:15+00:00,,0,4,feature module: cuda triaged,True
34874,Extend nn.functional.softmax for arbitrary dimensions module: nn triaged function request,2020-03-17 09:02:12+00:00,,0,6,module: nn triaged function request,True
34870,Flaky test test_ctc_loss_cuda on Windows high priority module: autograd module: cuda triaged module: flaky-tests,2020-03-17 08:19:59+00:00,,0,2,high priority module: autograd module: cuda triaged module: flaky-tests,True
34851,[jit] `__hash__` magic method is missing oncall: jit low priority,2020-03-17 00:41:28+00:00,,0,3,oncall: jit low priority,True
34839,[torch.jit.script] Allow `range` to index into Tensor oncall: jit triaged enhancement,2020-03-16 21:34:39+00:00,,0,2,oncall: jit triaged enhancement,True
34837,[torch.jit.script] Support tensor indexing after an Ellipsis oncall: jit triaged enhancement,2020-03-16 21:24:23+00:00,,0,2,oncall: jit triaged enhancement,True
34833,NO_CUDA and NO_DISTRIBUTED referenced in docs module: docs triaged,2020-03-16 20:09:48+00:00,,0,2,module: docs triaged,False
34814,[JIT] Tensor.add_ incorrect schema matching on overloads oncall: jit triaged,2020-03-16 17:01:46+00:00,,1,0,oncall: jit triaged,True
34812,"Semantic differences between forward and backward of upsampling2d/3d in channels last (NHWC, NDHWC) format. triaged module: memory format",2020-03-16 16:48:44+00:00,,0,1,triaged module: memory format,True
34799,Is it possible to run an object detection android app by using Pytorch Mobile? triaged module: android,2020-03-16 08:03:12+00:00,,0,4,triaged module: android,True
34788,keepdim for Tensor.select? triaged function request module: viewing and reshaping,2020-03-15 20:08:25+00:00,,0,1,triaged function request module: viewing and reshaping,True
34772,"Unsupported ONNX op (Upsample 3D, bicubic) contrary to documentation module: nn triaged module: vision",2020-03-15 00:56:40+00:00,,0,5,module: nn triaged module: vision,True
34765,Add ability to return a copy to Module's state_dict member function. module: nn triaged enhancement,2020-03-14 14:51:54+00:00,,0,1,module: nn triaged enhancement,True
34760,incomplete implementation doubt triaged oncall: transformer/mha,2020-03-14 04:29:08+00:00,,0,6,triaged oncall: transformer/mha,False
34754,General Purpose Faulty RPC Agent oncall: distributed triaged better-engineering module: rpc,2020-03-14 01:13:48+00:00,,0,1,oncall: distributed triaged better-engineering module: rpc,True
34731,Small output value mismatch after convert to onnx model module: onnx triaged onnx-needs-info,2020-03-13 19:21:44+00:00,,0,1,module: onnx triaged onnx-needs-info,False
34728,CPU softmax performance poor when dim is not the last dimension module: performance module: cpu triaged,2020-03-13 19:18:18+00:00,,0,0,module: performance module: cpu triaged,True
34715,Maybe gpu_kernel shouldn't ASSERT_HOST_DEVICE_LAMBDA triaged,2020-03-13 13:53:08+00:00,,0,10,triaged,True
34713,torch.jit.script report error when using index to subscript nn.ModuleList triage review oncall: jit,2020-03-13 12:16:53+00:00,,0,2,triage review oncall: jit,True
34707,Pytorch report INTERNAL ASSERT FAILED at ..\\torch\\csrc\\jit\\ir.cpp:1529 when use torch.jit.script to convert to model needs reproduction oncall: jit triaged,2020-03-13 08:36:22+00:00,,0,3,needs reproduction oncall: jit triaged,True
34704,RuntimeError: derivative for grid_sampler_2d_backward is not implemented triaged module: interpolation,2020-03-13 06:42:12+00:00,,0,38,triaged module: interpolation,True
34682,Pytorch not compatible with react native android oncall: mobile,2020-03-12 21:58:03+00:00,,0,4,oncall: mobile,False
34675,[feature request]Support dilation parameter for unfold2d_* function (slow cpu maxpool2d #28733) module: performance module: nn triaged module: mkldnn module: pooling function request,2020-03-12 20:48:32+00:00,,0,10,module: performance module: nn triaged module: mkldnn module: pooling function request,True
34660,"Add ""strict"" flag to ignore missing parameters in Optimizer.load_state_dict module: optimizer triaged enhancement",2020-03-12 18:05:52+00:00,,1,5,module: optimizer triaged enhancement,False
34651,Support passing memoryview to torch.as_tensor triaged enhancement,2020-03-12 16:55:07+00:00,,0,3,triaged enhancement,False
34649,Confusing error message of tensor constructor when passing a storage triaged,2020-03-12 16:25:54+00:00,,0,3,triaged,True
34646,"Support creating a CPU tensor from ctypes pointer in Python / from_blob(ptr, shape, strides, dtype) module: docs module: memory usage triaged enhancement module: numpy",2020-03-12 12:40:46+00:00,,0,6,module: docs module: memory usage triaged enhancement module: numpy,True
34621,A better way to show users all build options module: build triaged,2020-03-11 21:37:34+00:00,,1,1,module: build triaged,True
34611,Eigen version for PyTorch ? module: build triaged,2020-03-11 20:18:09+00:00,,0,5,module: build triaged,True
34606,Multiple CMake target errors ever since commit 0e52627358 module: build triaged,2020-03-11 19:53:04+00:00,,0,18,module: build triaged,True
34576,Significant speed difference between P100 and V100 module: performance module: cuda triaged module: cublas,2020-03-11 03:38:02+00:00,,0,8,module: performance module: cuda triaged module: cublas,True
34573,Restructure `multi_head_attention_forward` triaged oncall: transformer/mha,2020-03-11 02:50:20+00:00,,0,31,triaged oncall: transformer/mha,True
34565,Backward operations not decorated with stashed seq marker triaged oncall: profiler,2020-03-11 00:13:12+00:00,,0,0,triaged oncall: profiler,True
34544,Strange behaviour of F.interpolate with bicubic mode. module: nn triaged,2020-03-10 18:24:57+00:00,,0,4,module: nn triaged,True
34516,Expose chunk_sizes for DataParallel module: nn triaged enhancement module: data parallel,2020-03-09 23:48:04+00:00,,0,0,module: nn triaged enhancement module: data parallel,True
34483,JIT: Tracing faster than scripting oncall: jit triaged,2020-03-09 14:40:01+00:00,,0,2,oncall: jit triaged,True
34453,[Feature request] Query padding mask for nn.MultiheadAttention module: nn triaged oncall: transformer/mha,2020-03-08 15:15:38+00:00,,0,4,module: nn triaged oncall: transformer/mha,True
34451,Mac build from source error  module: build triaged oncall: mobile module: xnnpack,2020-03-08 12:00:40+00:00,,0,12,module: build triaged oncall: mobile module: xnnpack,True
34412,error in pytorch Docs: https://pytorch.org/docs/stable/distributions.html module: docs triaged,2020-03-07 04:02:18+00:00,,0,2,module: docs triaged,True
34409,PyTorch build with Ideep support? module: build triaged module: mkldnn,2020-03-07 01:07:38+00:00,,0,7,module: build triaged module: mkldnn,True
34397,TorchScript docs have broken links oncall: jit triaged,2020-03-06 21:49:07+00:00,,0,0,oncall: jit triaged,False
34375,.detach() behaves differently for dense tensors vs sparse tensors module: sparse triaged,2020-03-06 17:28:16+00:00,,1,8,module: sparse triaged,True
34367,Better testing of the autograd engine module: autograd triaged better-engineering,2020-03-06 15:10:08+00:00,,4,1,module: autograd triaged better-engineering,True
34361,[JIT legacy executor] device propagation regression oncall: jit triaged,2020-03-06 08:32:32+00:00,,0,6,oncall: jit triaged,True
34329,[jit] Hook support tracking high priority triage review oncall: jit triaged,2020-03-05 23:55:37+00:00,,1,3,high priority triage review oncall: jit triaged,True
34323,PyTorch GPU memory allocation module: cuda triaged,2020-03-05 22:23:01+00:00,,0,2,module: cuda triaged,True
34306,[docs] Missing docs online for conv_tbc module: docs triaged,2020-03-05 15:47:07+00:00,,0,3,module: docs triaged,False
34300,Inconsistent semantics of converting inf/-inf to long module: docs triaged,2020-03-05 12:40:53+00:00,,0,4,module: docs triaged,True
34279,`torch.norm` is 113x slower than `torch.sqrt(a**2 + b**2)` module: performance triaged module: linear algebra,2020-03-05 00:43:09+00:00,,0,2,module: performance triaged module: linear algebra,True
34257,CustomFromMask pruning stores a copy of the user-provided mask module: nn triaged module: pruning,2020-03-04 20:20:54+00:00,,0,3,module: nn triaged module: pruning,True
34242,RuntimeError with torch.unique: radix_sort: failed on 2nd step: invalid argument module: cuda triaged module: sorting and selection,2020-03-04 19:00:26+00:00,,0,14,module: cuda triaged module: sorting and selection,True
34210,[docs] Unclear arg spec for torch.full module: docs triaged module: numpy,2020-03-04 12:02:23+00:00,,0,0,module: docs triaged module: numpy,True
34207,Support vars/__dict__ on torch.return_types tuples e.g. topk triaged enhancement,2020-03-04 09:53:42+00:00,,0,7,triaged enhancement,False
34172,END_HANDLE_TH_ERRORS_PYBIND prevents pybind11 Exception translation triaged module: pybind,2020-03-03 23:21:23+00:00,,0,4,triaged module: pybind,True
34130,ExceptionWrapper cannot handle exceptions with more than one argument module: dataloader module: error checking triaged,2020-03-03 14:09:24+00:00,,0,5,module: dataloader module: error checking triaged,True
34126,torch::jit::script::Object::attr should throw AttributeError instead of RuntimeError oncall: jit triaged,2020-03-03 10:47:22+00:00,,0,1,oncall: jit triaged,True
34116,autograd with TorchScript does not match finite differences high priority triage review oncall: jit,2020-03-03 02:58:03+00:00,,0,1,high priority triage review oncall: jit,True
34086,Enable profiler tracing tests on windows feature triaged,2020-03-02 20:47:38+00:00,,0,0,feature triaged,False
34082,Autograd deep copy avoidance optimization unsound in the presence of views module: autograd triaged,2020-03-02 20:29:46+00:00,,0,4,module: autograd triaged,True
34080,RPC API Changes for TensorPipes triaged module: rpc,2020-03-02 20:21:19+00:00,,0,0,triaged module: rpc,True
34068,[C++ API Parity] Incorrect documentation for optim initialization in serialization docs module: docs module: cpp module: optimizer triaged,2020-03-02 17:29:14+00:00,,0,0,module: docs module: cpp module: optimizer triaged,True
34064,pip --requirement installs incorrect CUDA version module: dependency bug module: binaries oncall: releng triaged,2020-03-02 14:49:18+00:00,,0,4,module: dependency bug module: binaries oncall: releng triaged,True
34063,[docs] Unclear description of indices arg in torch.index_put_ module: docs triaged,2020-03-02 14:27:59+00:00,,0,1,module: docs triaged,False
34058,libtorch.so file size is very large high priority module: binaries module: cpp triaged oncall: mobile quansight-nack,2020-03-02 10:19:16+00:00,,0,17,high priority module: binaries module: cpp triaged oncall: mobile quansight-nack,True
34052,Model loaded in C++ runtime is not thread safe module: cpp triaged module: sanitizers,2020-03-02 06:01:34+00:00,,0,4,module: cpp triaged module: sanitizers,True
34044,Torchscript Inference with DLL Creation oncall: jit,2020-03-01 15:35:23+00:00,,0,1,oncall: jit,False
34001,_broadcast_coalesced_reshape doesn't respect zero-size tensor triaged small module: data parallel,2020-02-29 04:49:26+00:00,,0,0,triaged small module: data parallel,True
33993,ModuleList doesn't support slicing in TorchScript oncall: jit,2020-02-29 01:24:43+00:00,,0,0,oncall: jit,False
33991,TensorPipes RPC Agent Default Args/Result Device Mapping triaged module: rpc,2020-02-29 00:30:07+00:00,,0,0,triaged module: rpc,True
33990,TensorPipes RPC Agent Multiple Placement Retries triaged module: rpc,2020-02-29 00:28:00+00:00,,0,0,triaged module: rpc,True
33989,TensorPipes RPC Agent Message Acknowledgements triaged module: rpc,2020-02-29 00:06:14+00:00,,0,0,triaged module: rpc,True
33983,TensorPipes RPC Agent Listener shortcut triaged module: rpc,2020-02-28 23:23:14+00:00,,0,0,triaged module: rpc,True
33982,Initialize TensorPipe RPC Agent Transport triaged module: rpc,2020-02-28 23:21:30+00:00,,0,0,triaged module: rpc,True
33979,TensorPipes RPC Agent CUDA Support module: cuda triaged module: rpc,2020-02-28 23:13:54+00:00,,0,0,module: cuda triaged module: rpc,True
33898,Docs for uniform_ don't make any sense module: distributions triaged,2020-02-27 20:55:53+00:00,,0,8,module: distributions triaged,True
33890,Libtorch segfault when used with libqpOASES  module: build triaged,2020-02-27 18:58:37+00:00,,0,1,module: build triaged,True
33887,[jit] Python arg parser / TorchScript incompatibilities oncall: jit triaged,2020-02-27 18:47:17+00:00,,0,0,oncall: jit triaged,True
33876,Torchscript incompatible with torch.cat for tensor lists oncall: jit triaged,2020-02-27 16:39:30+00:00,,0,1,oncall: jit triaged,True
33867,cuDNN batchnorm with non-contiguous running mean silently discards updates module: cudnn triaged,2020-02-27 15:21:43+00:00,,1,5,module: cudnn triaged,True
33864,Memory leak in embedding layer and LSTM module: memory usage triaged,2020-02-27 13:03:11+00:00,,0,0,module: memory usage triaged,True
33859,Possibility to support int4 data type feature triaged,2020-02-27 05:33:36+00:00,,0,1,feature triaged,True
33855,Unclear output for Pytorch Profiler module: docs module: autograd triaged,2020-02-27 03:53:47+00:00,,0,6,module: docs module: autograd triaged,True
33844,[jit] Returning different types with `Any` segfaults oncall: jit triaged,2020-02-26 23:44:04+00:00,,0,0,oncall: jit triaged,True
33811,"Models saved in C++ LibTorch with torch::save, cannot be loaded in python using torch.load module: cpp module: serialization triaged",2020-02-26 11:52:16+00:00,,0,7,module: cpp module: serialization triaged,True
33810,Wrongly detected: Division of ints in TorchScript uses Python 3 true division semantics oncall: jit,2020-02-26 11:32:04+00:00,,0,0,oncall: jit,False
33800,Can't get module gradient in autograd.Function's custom backward when DataParallel is used triaged module: data parallel,2020-02-26 02:31:06+00:00,,0,2,triaged module: data parallel,True
33799,Add `start_process_in_context` to `torch.multiprocessing` module: multiprocessing triaged enhancement,2020-02-26 01:51:28+00:00,,0,4,module: multiprocessing triaged enhancement,True
33787,MSE Loss Implementation triaged,2020-02-25 22:58:43+00:00,,0,4,triaged,False
33785,Deepcopy fails with nn.parallel.replicate triaged module: data parallel,2020-02-25 22:30:02+00:00,,0,5,triaged module: data parallel,True
33782,[JIT] BroadcastingList annotations don't work with ignore'd functions oncall: jit triaged,2020-02-25 22:08:37+00:00,,0,0,oncall: jit triaged,True
33738,Support pipelining the backward pass and optimizer.step() for distributed autograd. oncall: distributed triaged module: rpc,2020-02-25 01:45:57+00:00,,0,0,oncall: distributed triaged module: rpc,True
33725,Tensor.random_ is not implemented for bfloat16 on CPU(but implemented on CUDA) module: cpu triaged module: random,2020-02-24 22:12:38+00:00,,1,0,module: cpu triaged module: random,True
33692,JIT tracing check fails with boolean tensor modifications triage review oncall: jit actionable,2020-02-24 13:12:27+00:00,,0,3,triage review oncall: jit actionable,True
33691,Some module has incorrect scope in complex naming situations in tensorboard graph export triaged module: tensorboard,2020-02-24 10:39:07+00:00,,0,0,triaged module: tensorboard,True
33684,Decouple Lifetime of Local RRef and RPC oncall: distributed triaged,2020-02-24 02:32:51+00:00,,1,0,oncall: distributed triaged,True
33672,[dev] `RecursiveScriptModule` does not expose `jit.ignore`d methods oncall: jit triaged,2020-02-23 14:16:25+00:00,,0,6,oncall: jit triaged,True
33670,Some module info is missing in nested graph for tensorboard triaged module: tensorboard,2020-02-23 10:54:11+00:00,,0,0,triaged module: tensorboard,True
33668,"/usr/bin/x86_64-linux-gnu-ld: warning: libcusparse.so.10.0, needed by /pytorch_master/build/lib/libtorch_cuda.so, not found (try using -rpath or -rpath-link) module: build module: cuda triaged",2020-02-23 02:02:07+00:00,,0,5,module: build module: cuda triaged,True
33664,torch.rand() not having same values on using torch.manual_seed(0) triaged module: random,2020-02-22 17:51:03+00:00,,0,1,triaged module: random,True
33653,master build error caffe2,2020-02-22 07:33:34+00:00,,0,0,caffe2,True
33631,bitmapToFloat32Tensor() 1 channel Tensor [feature] [mobile] oncall: mobile oncall: java,2020-02-21 20:58:12+00:00,,0,2,oncall: mobile oncall: java,False
33628,Remove .data subset1 for fixathon module: autograd triaged enhancement better-engineering actionable fixathon,2020-02-21 19:59:53+00:00,,0,0,module: autograd triaged enhancement better-engineering actionable fixathon,True
33611,Resnet 32x32 in caffe2 generates ResNet with wrong number of layers caffe2,2020-02-21 05:05:14+00:00,,0,0,caffe2,False
33571,empty_sparse shouldn't be called with memory layout but is module: sparse triaged module: memory format module: tensor creation,2020-02-20 19:48:11+00:00,,0,0,module: sparse triaged module: memory format module: tensor creation,True
33520,Reverse Cumulative Sum triaged function request,2020-02-19 22:40:22+00:00,,0,5,triaged function request,False
33514,"Make setter non-optional, e.g., TensorOptions::device(optional<Device>) -> device(Device), and add a device_opt setter module: cpp triaged",2020-02-19 20:34:13+00:00,,0,0,module: cpp triaged,False
33501,"When a Node fails to resolve to an Operator, print out the types of arguments, and all ""close matches"" in known operators oncall: jit triaged",2020-02-19 18:12:02+00:00,,0,0,oncall: jit triaged,True
33491,jit.trace checker fails for LSTM  oncall: jit,2020-02-19 11:56:49+00:00,,0,1,oncall: jit,True
33482,Distributed Data Parallel for computation graphs that make RPCs in forward() oncall: distributed triaged module: rpc,2020-02-19 05:28:17+00:00,,1,0,oncall: distributed triaged module: rpc,True
33477,"Add 32-bit CI (e.g., Raspberry PI CI) module: ci triaged",2020-02-19 02:09:08+00:00,,0,0,module: ci triaged,False
33476,Make ArrayRef::size() return int64_t rather than size_t module: internals triaged,2020-02-19 02:05:51+00:00,,0,0,module: internals triaged,True
33463,torch._C.Node.scopeName() missing in pytorch 1.4 oncall: jit triaged,2020-02-18 21:44:40+00:00,,0,3,oncall: jit triaged,True
33436,It seems nn.Sequential.add_module() could take duplicate names module: nn triaged,2020-02-18 08:20:19+00:00,,0,2,module: nn triaged,True
33413,IterableDataset with num_workers > 0 and drop_last=True drops more instances than expected feature module: dataloader triaged,2020-02-17 06:21:31+00:00,,0,1,feature module: dataloader triaged,True
33396,PyTorch 1.4.0 does not support using `Module` or `ModuleList` in attribute annotations in ScriptModule oncall: jit triaged,2020-02-16 09:12:42+00:00,,0,1,oncall: jit triaged,True
33390,It is not good to separate the steps of modules making and forward computation feature triaged,2020-02-16 02:40:14+00:00,,0,1,feature triaged,True
33388,Sobol engine generates out-of-bounds samples after drawing too many samples triaged module: random,2020-02-16 01:19:41+00:00,,0,5,triaged module: random,True
33385,Illegal instruction: 4 - OSX 10.13.6 install from source module: build triaged module: macos,2020-02-15 20:53:32+00:00,,0,6,module: build triaged module: macos,True
33373,torch.clamp_ not inplace during backward module: autograd module: memory usage triaged,2020-02-15 08:25:37+00:00,,0,6,module: autograd module: memory usage triaged,True
33367,Expose `internal::GRAIN_SIZE` through Python API. triaged enhancement module: multithreading,2020-02-15 00:23:14+00:00,,0,4,triaged enhancement module: multithreading,True
33363,Make it easier to add new messages in RPC layer triaged better-engineering module: rpc,2020-02-14 23:26:35+00:00,,0,0,triaged better-engineering module: rpc,True
33360,Dropout of attention weights in function F.multi_head_attention_forward() breaks sum-to-1 constraint module: nn triaged,2020-02-14 22:04:12+00:00,,0,3,module: nn triaged,True
33354,Long torchscript warmup times can be problematic for production serving oncall: jit triaged,2020-02-14 19:37:23+00:00,,0,11,oncall: jit triaged,True
33353,[jit] `Node` callstack is incorrect oncall: jit triaged,2020-02-14 19:32:34+00:00,,0,0,oncall: jit triaged,True
33342, malloc(): memory corruption (fast) triaged module: data parallel,2020-02-14 12:15:44+00:00,,0,0,triaged module: data parallel,True
33341,how-to-adjust-learning-rate using libtorch triaged,2020-02-14 11:25:57+00:00,,0,0,triaged,True
33327,Creating Torch tensors slows OpenCV video reading a lot module: performance triaged module: multithreading,2020-02-14 01:28:48+00:00,,1,4,module: performance triaged module: multithreading,True
33303,torch::var_out and dimnames module: cpp triaged module: named tensor,2020-02-13 18:09:13+00:00,,0,0,module: cpp triaged module: named tensor,True
33301,Don't take TensorOptions by reference module: cpp triaged,2020-02-13 17:53:34+00:00,,0,0,module: cpp triaged,True
33298,[feature request] torch.expand to not require unsqueeze and match -1 to existing dimensions if tensor.shape.count(-1) == tensor.ndim triaged enhancement,2020-02-13 17:01:10+00:00,,0,8,triaged enhancement,True
33296,Training got stuck due to timeout from dataloader module: performance module: dataloader triaged,2020-02-13 16:38:03+00:00,,1,26,module: performance module: dataloader triaged,True
33285,iOS libtorch superpoint model bug module: macos oncall: mobile module: ios shadow review,2020-02-13 08:12:35+00:00,,1,0,module: macos oncall: mobile module: ios shadow review,True
33281,"Uninitialised value was created by a stack allocation, reported by valgrind triage review oncall: jit triaged",2020-02-13 03:41:44+00:00,,0,2,triage review oncall: jit triaged,True
33249,Core dumps being created when running test_c10d.py and test_multiprocessing_spawn.py module: tests triaged,2020-02-12 20:54:47+00:00,,0,1,module: tests triaged,True
33248,PyTorch 1.4.0 CUDA initialization error with CPU-only (multiprocessing) on Python 3.7.5 module: autograd module: cuda triaged,2020-02-12 19:42:13+00:00,,0,11,module: autograd module: cuda triaged,True
33241,"[feature request] Add ""groups"" argument to nn.Fold and nn.Unfold feature module: nn triaged",2020-02-12 17:06:39+00:00,,0,0,feature module: nn triaged,False
33227,[v1.5] Python/C++ API parity master tracking task module: cpp triaged,2020-02-12 03:39:46+00:00,,0,0,module: cpp triaged,True
33181,[feature request] [dataloader] Introduce Dataset.__collate__ module: dataloader triaged enhancement needs research,2020-02-11 11:35:55+00:00,,0,20,module: dataloader triaged enhancement needs research,True
33180,Loading pretrained model caffe2,2020-02-11 09:42:22+00:00,,0,1,caffe2,False
33166,TensorIterator does not work with different input/output types triaged enhancement module: vectorization module: TensorIterator,2020-02-11 00:46:52+00:00,,1,2,triaged enhancement module: vectorization module: TensorIterator,True
33132,Support multiple-build-type generators for CMake module: windows triaged,2020-02-09 19:21:58+00:00,,1,4,module: windows triaged,True
33129,Python package using CMake module: build low priority triaged enhancement,2020-02-09 07:36:38+00:00,,1,5,module: build low priority triaged enhancement,True
33124,QNNPACK: GNU aarch64 assembler does not support 4s on neon mov oncall: mobile,2020-02-08 20:40:15+00:00,,0,3,oncall: mobile,False
33122,[feature request] make torch.multinomial behaviour compliant with rnn output dimension module: distributions module: bc-breaking feature triaged,2020-02-08 11:25:37+00:00,,0,3,module: distributions module: bc-breaking feature triaged,True
33115,[RFC] Add ability to get all remote parameters when constructing DistributedOptimizer. triaged module: rpc,2020-02-07 23:37:57+00:00,,0,0,triaged module: rpc,True
33114,Move the custom pass execution back to the beginning of runNondiffOptimization oncall: jit triaged,2020-02-07 22:47:30+00:00,,1,8,oncall: jit triaged,True
33110,InlineAutodiffSubgraphs in JIT inlines non-differentiable custom groups unexpectedly. oncall: jit module: custom-operators actionable,2020-02-07 22:29:30+00:00,,0,4,oncall: jit module: custom-operators actionable,True
33089,test_baddbmm_cpu_float32 fails locally for me when built with DEBUG=1 module: tests triaged small,2020-02-07 15:35:38+00:00,,0,2,module: tests triaged small,True
33086,F.max_pool*d/F.min_pool*d should support integer dtypes and bool tensors triaged module: pooling function request,2020-02-07 13:52:24+00:00,,0,3,triaged module: pooling function request,True
33085,not able to import *  from fastai.vision  in Google collab triaged,2020-02-07 10:03:08+00:00,,1,9,triaged,True
33081,DataParallel gives different gradients when using LSTMs triaged module: determinism module: data parallel,2020-02-07 05:39:20+00:00,,0,1,triaged module: determinism module: data parallel,True
33055,[docs] Strange order of items in docs contents in left pane module: docs triaged,2020-02-06 19:46:50+00:00,,3,4,module: docs triaged,False
33051,Recover from CUDA runtime error module: cuda triaged module: third_party,2020-02-06 17:33:57+00:00,,1,2,module: cuda triaged module: third_party,True
33049,Warning when link libtorch and opencv4.2.0 together module: build triaged better-engineering,2020-02-06 16:21:22+00:00,,0,3,module: build triaged better-engineering,True
33047,torch.nn.functional import grid_sample needs reproduction triaged,2020-02-06 15:35:41+00:00,,0,3,needs reproduction triaged,True
33041,bytearray(tensor) behaves very differently from bytearray(tensor.numpy()) module: printing triaged module: numpy,2020-02-06 11:33:03+00:00,,0,3,module: printing triaged module: numpy,True
33040,[debatable] Better infer dtype in torch.as_tensor triaged module: tensor creation,2020-02-06 11:23:24+00:00,,0,5,triaged module: tensor creation,True
33035,Support batch linear transformation module: nn triaged module: batching function request,2020-02-06 00:47:48+00:00,,0,0,module: nn triaged module: batching function request,True
33034,Scripting fails to preserve attribute aliasing oncall: jit triaged,2020-02-06 00:43:02+00:00,,0,1,oncall: jit triaged,True
33029,Torch not compiled with CUDA enabled awaiting response (this tag is deprecated) module: binaries module: cuda triaged,2020-02-05 23:16:40+00:00,,0,1,awaiting response (this tag is deprecated) module: binaries module: cuda triaged,True
33007,Upper/Lower attributes for named dimensions for proper Ricci notation and to generalize matrix operations triaged module: named tensor,2020-02-05 14:18:57+00:00,,0,0,triaged module: named tensor,True
33006,matmul: no warning when contracting differently named dimensions triaged module: named tensor,2020-02-05 13:46:55+00:00,,0,0,triaged module: named tensor,True
32998,torch batchwise max with indices triaged module: batching function request module: reductions,2020-02-05 01:57:32+00:00,,0,4,triaged module: batching function request module: reductions,True
32988,[jit] Dict set item type mismatch error doesn't say the type was inferred oncall: jit triaged,2020-02-04 23:06:40+00:00,,0,1,oncall: jit triaged,True
32976,[JIT] pytorch 1.4 breaks torch.jit.script(LSTM/GRU) triage review oncall: jit triaged has workaround,2020-02-04 17:41:29+00:00,,0,7,triage review oncall: jit triaged has workaround,True
32972,"which pytorch do i install for running this project in my windows.link for this is ::  https://github.com/xiaojunxu/SQLNet  .IN this they have used python 2.7 ,but i am unable to install pytorch on python 2.7 environment .help me with this module: binaries triaged",2020-02-04 14:07:15+00:00,,0,3,module: binaries triaged,True
32937,Operation Registration Error triaged module: dispatch,2020-02-03 19:12:27+00:00,,0,5,triaged module: dispatch,True
32924,Connect timeout feature do not work in DDP with TCPStore oncall: distributed triaged,2020-02-03 02:34:23+00:00,,0,2,oncall: distributed triaged,True
32919,"Torchscript used to work, but now it fails with VariableTensorId error oncall: jit triaged module: android oncall: mobile",2020-02-03 00:46:13+00:00,,1,6,oncall: jit triaged module: android oncall: mobile,True
32918,Interpolate in the “bicubic” mode with the same shape outputs zeros from second sample onwards high priority triaged,2020-02-02 17:26:01+00:00,,1,1,high priority triaged,True
32916, from torch._C import * (ImportError: DLL load failed) triaged,2020-02-02 13:51:15+00:00,,0,15,triaged,True
32912,Optional seq_len argument to torch.nn.utils.rnn.pad_sequence triaged function request module: padding,2020-02-01 19:24:29+00:00,,1,1,triaged function request module: padding,True
32909,Some questions Concerning Intel's DNNL(MKLDNN) support in Pytorch (adding support for Intel Processors GPUs) by transitioning to DNNL triaged module: mkldnn,2020-02-01 11:09:18+00:00,,0,4,triaged module: mkldnn,True
32901,QNNPACK linear doesn't preserve dimensions oncall: mobile,2020-02-01 04:18:54+00:00,,0,1,oncall: mobile,False
32884,[RFC] Nested scopes in autograd profiler should support RPC calls properly. module: autograd triaged module: rpc,2020-01-31 22:47:17+00:00,,0,5,module: autograd triaged module: rpc,True
32881,CUDNN_STATUS_EXECUTION_FAILED module: cudnn module: cuda triaged,2020-01-31 22:32:06+00:00,,0,13,module: cudnn module: cuda triaged,True
32872,Confusing error messages of tensor.scatter_ on both CPU and CUDA module: docs module: error checking triaged module: scatter & gather ops,2020-01-31 18:22:55+00:00,,0,0,module: docs module: error checking triaged module: scatter & gather ops,True
32868,__cuda_array_interface__ conversion does not support readonly arrays module: internals module: cuda triaged module: numba,2020-01-31 15:10:04+00:00,,0,1,module: internals module: cuda triaged module: numba,True
32867,"[feature request] np.packbits / np.unpackbits, general BitTensors (maybe can be just tensors with dtype torch.bits8 or have a new dtype torch.bits introduced) and bit packed tensors utilities for saving memory / accesses, support for BitTensors wherever BoolTensors are used high priority feature triaged module: boolean tensor",2020-01-31 14:35:47+00:00,,0,60,high priority feature triaged module: boolean tensor,True
32864,[docs] Missing docs for torch.__version__ and torch.version module: docs triaged small,2020-01-31 10:46:50+00:00,,0,6,module: docs triaged small,False
32851,Silent failing of batch_sampler when the data points are lists of tensors. module: dataloader triaged module: nestedtensor,2020-01-31 01:15:02+00:00,,0,3,module: dataloader triaged module: nestedtensor,True
32840,Can we add support for Enum in scripted models? oncall: jit triaged,2020-01-30 21:28:06+00:00,,0,1,oncall: jit triaged,True
32826,MKLDNN doesnt work and is slower than normal cpu mode triaged module: mkldnn,2020-01-30 13:17:16+00:00,,0,10,triaged module: mkldnn,True
32822,Error tracing custom autograd.Function oncall: jit triaged,2020-01-30 07:54:17+00:00,,0,6,oncall: jit triaged,True
32784,Implement Backend-Agnostic RPC functionality in RpcAgent triaged better-engineering module: rpc,2020-01-29 20:03:05+00:00,,1,1,triaged better-engineering module: rpc,True
32783,Comments Separating Class Methods from Different Classes in C++ Files triaged better-engineering,2020-01-29 19:53:57+00:00,,1,0,triaged better-engineering,True
32770,Error while trying to build pytorch from source in conda environment module: build triaged,2020-01-29 14:35:47+00:00,,1,6,module: build triaged,True
32767,Documentation is not loaded by IDEs module: docs triaged,2020-01-29 11:41:29+00:00,,0,0,module: docs triaged,True
32754,JIT performance discrepancies module: performance oncall: jit triaged,2020-01-29 01:51:54+00:00,,1,1,module: performance oncall: jit triaged,True
32752,RPC mock mode for unit tests. triaged module: rpc,2020-01-29 01:29:43+00:00,,0,0,triaged module: rpc,True
32751,PyTorch freezes on second call to scripted densenet model from torchvision oncall: jit triaged module: vision,2020-01-29 01:10:53+00:00,,0,3,oncall: jit triaged module: vision,True
32706,Torch serialization does not restore tensors properly when custom __reduce__ is defined module: serialization triaged,2020-01-28 18:18:31+00:00,,0,3,module: serialization triaged,True
32698,Add Julia Bindings to Torch backend  feature low priority triaged module: language binding,2020-01-28 13:05:13+00:00,,0,6,feature low priority triaged module: language binding,True
32695,torch.tensordot has inconsistent signature with torch script oncall: jit triaged,2020-01-28 12:44:05+00:00,,0,1,oncall: jit triaged,True
32694,ninja: build stopped: subcommand failed. module: build triaged,2020-01-28 12:35:08+00:00,,0,11,module: build triaged,True
32690,How to customize build torchscript model to be used in end devices codebase oncall: jit triaged oncall: mobile,2020-01-28 10:11:07+00:00,,0,5,oncall: jit triaged oncall: mobile,True
32687,Building PyTorch ignores my current Python version module: build triaged,2020-01-28 04:50:53+00:00,,0,6,module: build triaged,True
32662,Second-order gradient cause segfault over time needs reproduction triaged,2020-01-27 20:00:35+00:00,,0,5,needs reproduction triaged,True
32651,[Tensorboard] Problem with subfolders from SummaryWriter triaged module: tensorboard,2020-01-27 15:35:42+00:00,,0,19,triaged module: tensorboard,True
32641,changed format of trace graph in torch 1.4.0 oncall: jit triaged,2020-01-27 04:53:21+00:00,,0,18,oncall: jit triaged,True
32638,TORCH_CUDA_API export failure on torch::cuda::nccl::detail::throw_nccl_error(ncclResult_t) high priority module: binaries oncall: releng triaged module: nccl quansight-nack,2020-01-27 00:44:24+00:00,,0,6,high priority module: binaries oncall: releng triaged module: nccl quansight-nack,True
32614,Logical AND and OR for Tensors in C++ API. module: cpp triaged function request,2020-01-25 08:15:31+00:00,,1,3,module: cpp triaged function request,True
32590,[FYI] MultiheadAttention / Transformer proposal accepted module: nn triaged oncall: transformer/mha,2020-01-24 20:06:25+00:00,,1,17,proposal accepted module: nn triaged oncall: transformer/mha,True
32589,User-friendly handling of types and devices triaged module: numpy module: ux,2020-01-24 19:08:16+00:00,,0,10,triaged module: numpy module: ux,True
32584,Segmentation Fault (core dumped) with 1.4.0 needs reproduction triaged,2020-01-24 17:10:13+00:00,,0,8,needs reproduction triaged,True
32580,SGD optimizer with deprecation warning module: optimizer triaged module: deprecation,2020-01-24 16:48:32+00:00,,0,0,module: optimizer triaged module: deprecation,True
32576,Document different gradient wrt to jax when nesting module: docs module: autograd triaged,2020-01-24 11:32:41+00:00,,0,13,module: docs module: autograd triaged,True
32575,Interrupting a DDP worker while using the CUDA MPS server causes CUDA to hang until reboot oncall: distributed module: cuda triaged,2020-01-24 10:00:47+00:00,,0,9,oncall: distributed module: cuda triaged,True
32570,[feature request] Out-variant and dtype argument for torch.argmax / torch.argmin / torch.argsort (and friends) module: memory usage triaged module: sorting and selection function request module: reductions,2020-01-24 03:07:30+00:00,,0,5,module: memory usage triaged module: sorting and selection function request module: reductions,True
32565,Reduce RPC branches for Python/Built-inOp/TorchScript oncall: jit triaged module: rpc,2020-01-24 00:09:08+00:00,,0,0,oncall: jit triaged module: rpc,True
32552,MultivariateNormal.rsample: use eigen-decomposition when Cholesky fails module: distributions triaged enhancement module: linear algebra,2020-01-23 21:02:58+00:00,,0,5,module: distributions triaged enhancement module: linear algebra,True
32549,"""Tried to register multiple operators with the same name and the same overload name"" error is confusing triaged module: dispatch better-engineering",2020-01-23 20:01:37+00:00,,0,3,triaged module: dispatch better-engineering,True
32544,[jit] Use `typing.get_type_hints` instead of parsing types manually oncall: jit triaged,2020-01-23 19:02:19+00:00,,0,2,oncall: jit triaged,True
32531,Batched torch.eig() and gradient of torch.eig() for real eigenvalues module: autograd triaged module: batching module: linear algebra,2020-01-23 14:15:45+00:00,,1,1,module: autograd triaged module: batching module: linear algebra,True
32529,addition of attention based techniques to pytorch module: convolution triaged oncall: transformer/mha function request,2020-01-23 09:12:57+00:00,,1,3,module: convolution triaged oncall: transformer/mha function request,False
32528,Bug in add_param - functionality for tensorboard (scatter matrix view) module: tensorboard oncall: visualization,2020-01-23 08:09:50+00:00,,0,1,module: tensorboard oncall: visualization,True
32524,Pytorch 1.4 does not detect gpu module: cuda triaged,2020-01-23 04:59:56+00:00,,0,3,module: cuda triaged,True
32511,TorchScript C++ API Tracking Issue oncall: jit triaged,2020-01-22 21:41:31+00:00,,0,0,oncall: jit triaged,True
32510,Tensor.random_ is not implemented for bool on CUDA(but implemented on CPU) module: cuda triaged module: random,2020-01-22 21:27:40+00:00,,1,0,module: cuda triaged module: random,True
32504,LibTorch operates very slowly on data blobs from GPU module: performance module: cuda triaged,2020-01-22 20:23:52+00:00,,0,0,module: performance module: cuda triaged,True
32497,Don't unnecesarily send cleanup dist autograd context RPCs to other nodes module: bootcamp triaged better-engineering module: rpc,2020-01-22 18:22:55+00:00,,1,0,module: bootcamp triaged better-engineering module: rpc,True
32490,"RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch. oncall: distributed triaged",2020-01-22 14:09:01+00:00,,1,2,oncall: distributed triaged,True
32488,TracerWarning When Using Tensor Size in Torchscript Trace oncall: jit triaged,2020-01-22 12:25:25+00:00,,0,2,oncall: jit triaged,True
32463,[JIT] Make `torch.jit.script` work on all objects which we can represent as IValues oncall: jit triaged,2020-01-21 22:59:21+00:00,,0,0,oncall: jit triaged,True
32452,Numba Enhancement Proposal (NBEP) 7: External Memory Management Plugins feature triaged,2020-01-21 20:35:02+00:00,,0,0,feature triaged,False
32444,DataParallel does not work with sparse parameters triaged module: data parallel,2020-01-21 18:07:19+00:00,,0,11,triaged module: data parallel,True
32443,Segmentation fault in lazyInitCUDA -> CUDAHooks::initCUDA -> THCMagma_init -> magma_init needs reproduction module: cuda triaged module: third_party,2020-01-21 18:04:34+00:00,,0,1,needs reproduction module: cuda triaged module: third_party,True
32427,[C++] Don't use DeprecatedTypeProperties in torch::utils::reorder_tensors_like module: internals triaged,2020-01-20 19:02:32+00:00,,0,0,module: internals triaged,True
32421,Segmentation fault in C++ API torch::from_blob(...).clone() module: cpp triaged,2020-01-20 13:07:41+00:00,,0,0,module: cpp triaged,True
32419,MAGMA libraries module: binaries module: build triaged,2020-01-20 06:39:00+00:00,,0,0,module: binaries module: build triaged,True
32416,Is there a way to use SYSTEM_INSTALLED 3rdparty libraries? module: build triaged,2020-01-20 04:22:09+00:00,,0,0,module: build triaged,True
32415,DistributedStreamSampler: support stream sampler in distributed setting oncall: distributed module: cpp triaged,2020-01-20 03:05:10+00:00,,0,0,oncall: distributed module: cpp triaged,True
32407,Build without MKL is not possible when MKL is installed module: build triaged module: mkl,2020-01-19 18:32:01+00:00,,0,9,module: build triaged module: mkl,True
32406,KL divergence for diagonal Gaussian distributions module: distributions feature triaged,2020-01-19 15:47:51+00:00,,0,0,module: distributions feature triaged,True
32403,How to accelerate the compiling of pytorch  module: build triaged,2020-01-19 13:42:14+00:00,,0,0,module: build triaged,True
32402,torchvision triaged module: vision,2020-01-19 08:05:32+00:00,,0,0,triaged module: vision,True
32390,[JIT] Compile group of functions/modules oncall: jit triaged,2020-01-18 13:47:07+00:00,,0,4,oncall: jit triaged,True
32375,Make _rpc_sync_torchscript and _rpc_async_torchscript work with autograd profiler oncall: jit triaged,2020-01-17 22:34:48+00:00,,0,0,oncall: jit triaged,True
32372,Migrate processGroup::async_work to use Future oncall: distributed triaged,2020-01-17 21:24:05+00:00,,0,0,oncall: distributed triaged,True
32370,Backward `Functional.conv3d` is slow when cuDNN is enabled module: cudnn module: cuda triaged,2020-01-17 20:23:34+00:00,,0,3,module: cudnn module: cuda triaged,True
32365,[jit] Calling `torch.jit.script` on `@staticmethod`s which in turn call other `@staticmethod`s results in a `TypeError` oncall: jit triaged,2020-01-17 19:10:13+00:00,,0,1,oncall: jit triaged,True
32363,Possible ProcessGroup::Work::abort correctness issue oncall: distributed triaged module: rpc,2020-01-17 18:50:56+00:00,,0,0,oncall: distributed triaged module: rpc,True
32358,torch.bartlett_window not jitable oncall: jit triaged,2020-01-17 17:15:39+00:00,,0,5,oncall: jit triaged,True
32357,Pytorch JIT Compilation Does Not Finish (Infinite Loop?) for Deeper Models oncall: jit triaged,2020-01-17 16:57:38+00:00,,0,1,oncall: jit triaged,True
32351,"Problem with multiprocessing, custom __getstate__ with Tensors and forkserver module: multiprocessing module: serialization triaged",2020-01-17 14:32:43+00:00,,0,4,module: multiprocessing module: serialization triaged,True
32337,Save LocalResponceNorm as a single aten node when convert a pytorch model to jit model oncall: jit triaged,2020-01-17 02:41:46+00:00,,0,0,oncall: jit triaged,True
32322,deadlock when using mp.spawn multiprocessing module: multiprocessing triaged,2020-01-16 21:55:31+00:00,,0,13,module: multiprocessing triaged,True
32306,[FR] bincount along arbitrary dimension triaged module: sorting and selection function request,2020-01-16 18:15:14+00:00,,0,3,triaged module: sorting and selection function request,True
32305,[FR] histc should (optionally) return a long tensor triaged module: sorting and selection function request,2020-01-16 18:14:36+00:00,,0,5,triaged module: sorting and selection function request,True
32300,[jit] Various problems calling `@staticmethod`s in 1.4.0 oncall: jit triaged,2020-01-16 17:38:12+00:00,,0,4,oncall: jit triaged,True
32293,Truncated normal distribution module: distributions feature triaged,2020-01-16 15:39:20+00:00,,0,14,module: distributions feature triaged,True
32288,Allow range in dim argument of reducing operations such as sum triaged module: ux function request module: reductions,2020-01-16 13:41:34+00:00,,0,0,triaged module: ux function request module: reductions,True
32287,Python 3.8 Windows JIT test failure oncall: jit triaged,2020-01-16 13:24:42+00:00,,0,1,oncall: jit triaged,True
32269,Support strategy to train large model that exceeds GPU mem and DRAM mem feature triaged module: rpc,2020-01-16 05:11:49+00:00,,0,3,feature triaged module: rpc,True
32268,Numpy array functionality in torchscript. oncall: jit triaged,2020-01-16 04:45:31+00:00,,0,4,oncall: jit triaged,True
32267,the results when using opset10 and when using opset11 are different. module: onnx triaged onnx-needs-info,2020-01-16 04:07:22+00:00,,0,9,module: onnx triaged onnx-needs-info,False
32264,torch.nn.functional.threshold not work with LongTensor module: nn triaged enhancement,2020-01-16 03:18:14+00:00,,0,0,module: nn triaged enhancement,True
32262,models after prepare/prepare_qat contains `qconfig` which prevents pickling oncall: quantization low priority triaged,2020-01-16 02:16:12+00:00,,1,3,oncall: quantization low priority triaged,True
32253,[TorchScript] Device comparison implemented in eq but not ne oncall: jit triaged,2020-01-16 00:25:05+00:00,,0,0,oncall: jit triaged,False
32237,PyTorch C++ API docs only tracks master branch module: cpp triaged module: doc infra,2020-01-15 20:13:22+00:00,,0,0,module: cpp triaged module: doc infra,False
32227," 0 INTERNAL ASSERT FAILED at /pytorch/c10/util/intrusive_ptr.h:348, please report a bug to PyTorch. triaged module: assert failure",2020-01-15 16:18:21+00:00,,0,3,triaged module: assert failure,True
32219,[Feature proposal] improved algorithms for checkpointing feature triaged,2020-01-15 11:03:50+00:00,,0,0,feature triaged,False
32214,Pytorch compilation error module: build triaged,2020-01-15 08:36:37+00:00,,0,1,module: build triaged,True
32210,"dist.send/recv ""IndexError: map::at"" error when bool tensors are used with mpi backend triaged module: boolean tensor",2020-01-15 07:28:39+00:00,,0,0,triaged module: boolean tensor,True
32196,pytorch_linux_xenial_cuda10_1_cudnn7_py3_slow_test triggered on all PRs module: ci triaged,2020-01-14 23:43:20+00:00,,0,3,module: ci triaged,True
32186,Can't import torch on latest master module: build triaged,2020-01-14 22:14:00+00:00,,0,6,module: build triaged,True
32167,Support in-place pinning of memory triaged,2020-01-14 12:50:18+00:00,,0,3,triaged,False
32166,"C++ randint returns float32, python returns int64 module: cpp triaged",2020-01-14 10:42:49+00:00,,0,4,module: cpp triaged,True
32165,Serialization inconsistency with pickling tensors breaks caching module: serialization triaged,2020-01-14 10:17:59+00:00,,0,0,module: serialization triaged,True
32162,Using Tensor.to(device) after distributed all_reduce intermittently causes deadlock with NCCL  oncall: distributed triaged module: nccl,2020-01-14 08:10:42+00:00,,0,7,oncall: distributed triaged module: nccl,True
32150,pin_memory may change the type of instance returned by collate_fn. module: dataloader triaged,2020-01-14 01:33:24+00:00,,0,3,module: dataloader triaged,True
32141,We should change DeprecationWarnings to UserWarnings in 27361 module: docs triaged,2020-01-13 23:36:16+00:00,,0,1,module: docs triaged,True
32137,torch.nn.functional.normalize epsilon too small for half precision module: numerical-stability module: nn triaged module: half module: norms and normalization,2020-01-13 22:26:48+00:00,,0,2,module: numerical-stability module: nn triaged module: half module: norms and normalization,True
32132,Slighty out of tolerance for `test_mv` and `test_cholesky_solve_batched_cuda_float64` module: tests triaged,2020-01-13 21:57:47+00:00,,0,2,module: tests triaged,True
32130,Idempotency Keys for RPC Retry triaged module: rpc,2020-01-13 21:36:09+00:00,,1,1,triaged module: rpc,True
32113,Inconsistent linking flags result in error when building lib/libc10_cuda.so module: build module: cuda triaged,2020-01-13 19:24:19+00:00,,0,1,module: build module: cuda triaged,True
32105,TensorRT: CheckDims() need adjustment for EXPLICIT_BATCH? caffe2 triaged,2020-01-13 16:54:20+00:00,,0,1,caffe2 triaged,True
32101,Improve cuda OOM message module: docs module: bootcamp module: cuda triaged,2020-01-13 15:06:16+00:00,,0,11,module: docs module: bootcamp module: cuda triaged,True
32098,torch.gt ge lt le triaged module: type promotion,2020-01-13 08:26:15+00:00,,0,3,triaged module: type promotion,True
32097,logsumexp with subtraction feature triaged module: reductions,2020-01-13 06:50:18+00:00,,0,3,feature triaged module: reductions,True
32094,logical_not for Boolean tensor has peculiar behavior triaged module: boolean tensor,2020-01-13 05:24:56+00:00,,1,3,triaged module: boolean tensor,True
32088,Batch convolutional layer with 5d weight tensor that is not contiguous feature triaged,2020-01-12 13:45:02+00:00,,0,0,feature triaged,True
32087,Out-of-date link to pytorch ci dockerfiles in jenkins readme module: docs triaged,2020-01-12 04:06:45+00:00,,0,0,module: docs triaged,False
32083,build pytorch from source fauled: undefined reference to `cusparseGetErrorString(cusparseStatus_t)' module: build triaged,2020-01-11 11:24:38+00:00,,0,15,module: build triaged,True
32078,Reduce degrees of freedom in strides so that they unambiguously specify layout permutation triaged enhancement,2020-01-10 23:15:16+00:00,,1,6,triaged enhancement,True
32047,Sobol point implementation triaged module: random,2020-01-10 17:06:56+00:00,,0,16,triaged module: random,True
32039,Internal assert failure for user IValue unwrapping oncall: jit triaged,2020-01-10 13:34:42+00:00,,0,3,oncall: jit triaged,True
32036,Internal assert failed using multiple GPUs with DataParallel module: cuda triaged module: assert failure module: data parallel,2020-01-10 12:06:48+00:00,,0,3,module: cuda triaged module: assert failure module: data parallel,True
32034,Extend `ConcatDataset` to return dataset index module: dataloader triaged enhancement small,2020-01-10 10:15:44+00:00,,0,9,module: dataloader triaged enhancement small,True
32033,run detectron test test_spatial_narrow_as_op.py fails caffe2,2020-01-10 10:06:29+00:00,,0,0,caffe2,False
32030,Dockerfile for people to quick-start contributing triaged enhancement module: docker,2020-01-10 06:34:16+00:00,,1,16,triaged enhancement module: docker,True
32023,Inaccurate batched GRU results on CPU module: nn module: rnn module: cpu triaged module: numerical-reproducibility,2020-01-10 01:36:50+00:00,,0,0,module: nn module: rnn module: cpu triaged module: numerical-reproducibility,True
32021,[discussion] Relax optimizer constructor constraints for simplicity module: optimizer triaged enhancement,2020-01-10 00:45:46+00:00,,0,3,module: optimizer triaged enhancement,False
32018,DistributedDataParallel non-floating point dtype parameter with requires_grad=False oncall: distributed triaged,2020-01-10 00:17:11+00:00,,0,9,oncall: distributed triaged,True
32005,gradients inside gradient checkpoint module: checkpoint module: autograd triaged enhancement,2020-01-09 20:38:00+00:00,,0,2,module: checkpoint module: autograd triaged enhancement,True
31994,Trying to disable cuda to run torch on OSX run it on CPU module: build module: cuda triaged,2020-01-09 17:10:40+00:00,,0,5,module: build module: cuda triaged,True
31980,repeat_interleave Performance Issue module: performance good first issue triaged module: tensor creation,2020-01-09 04:57:29+00:00,,0,8,module: performance good first issue triaged module: tensor creation,True
31979,Reuse spawned subprocesses in RPC tests triaged better-engineering module: rpc,2020-01-09 04:24:08+00:00,,0,1,triaged better-engineering module: rpc,True
31953,JIT tests are linked directly into libtorch and register operators even when unused oncall: jit triaged,2020-01-08 18:36:10+00:00,,0,4,oncall: jit triaged,True
31945,"Function request: logerfc, logerfcx special functions module: numerical-stability triaged module: numpy function request module: special",2020-01-08 16:09:34+00:00,,0,8,module: numerical-stability triaged module: numpy function request module: special,True
31943,error when building pytorch 1.1.0 from source module: build triaged,2020-01-08 09:31:32+00:00,,0,6,module: build triaged,True
31942,Support sparse inputs for torch.block_diag module: sparse triaged module: tensor creation function request,2020-01-08 09:19:02+00:00,,0,4,module: sparse triaged module: tensor creation function request,True
31937,RPC and dist_autograd should respect no_grad mode feature triaged module: rpc,2020-01-08 02:56:34+00:00,,0,2,feature triaged module: rpc,True
31916,I'm not able to build pytorch with tensorrt (current master) module: build triaged,2020-01-07 17:41:49+00:00,,0,3,module: build triaged,True
31913,`index_add_` with multidimensional index triaged module: advanced indexing function request,2020-01-07 08:12:52+00:00,,1,5,triaged module: advanced indexing function request,True
31907,Compile libtorch by source code failed. module: binaries module: cpp triaged,2020-01-07 02:04:13+00:00,,0,2,module: binaries module: cpp triaged,True
31895,"torch.masked_select out argument can easily be misused, because output shape is dynamically computed high priority triaged module: numpy module: safe resize",2020-01-06 21:19:00+00:00,,0,11,high priority triaged module: numpy module: safe resize,True
31883,"The dependency target ""nccl_external"" of target ""gloo_cuda"" does not exist. oncall: distributed module: build triaged",2020-01-06 13:09:38+00:00,,0,14,oncall: distributed module: build triaged,True
31882,Example cmakelists for custom cuda operator? oncall: jit triaged,2020-01-06 06:20:56+00:00,,0,0,oncall: jit triaged,True
31881,[Feature Request] reduce CUDA runtime size by selectively compiling PyTorch GPU kernels high priority module: binaries module: build module: cuda triaged,2020-01-06 03:14:37+00:00,,0,10,high priority module: binaries module: build module: cuda triaged,True
31867,The model training time is increasing between runs if the same DataLoader reused to train multiple models. module: dataloader triaged,2020-01-05 05:35:10+00:00,,0,0,module: dataloader triaged,True
31866,Negative indices in chunk could cause Out of Range access on loss.backward in JIT oncall: jit triaged,2020-01-05 03:52:10+00:00,,0,3,oncall: jit triaged,True
31856,Fix softplus clampling issues using logsigmoid module: nn triaged,2020-01-04 16:20:14+00:00,,0,2,module: nn triaged,True
31842,Support rpc/remote torch script call with script class/module name and class/module method name oncall: jit triaged module: rpc,2020-01-03 20:11:52+00:00,,0,0,oncall: jit triaged module: rpc,True
31837,logsumexp: two little-impact perf suggestions module: performance module: cpu triaged module: reductions,2020-01-03 19:44:59+00:00,,0,5,module: performance module: cpu triaged module: reductions,True
31822,Libtorch's files conflict with glog's file? module: build triaged,2020-01-03 08:56:13+00:00,,0,2,module: build triaged,True
31818,How to distinguish different layers in hook？ module: nn triaged,2020-01-03 03:48:13+00:00,,0,3,module: nn triaged,True
31815,ImportError: /home/xx/anaconda3/lib/python3.7/site-packages/torch/lib/libtorch_python.so: undefined symbol: _ZNK5torch3jit5Graph8toStringE oncall: jit triaged,2020-01-03 01:20:47+00:00,,0,1,oncall: jit triaged,True
31799,Inaccurate ValueError reporting in nn/functional.py module: nn triaged,2020-01-02 22:30:10+00:00,,0,0,module: nn triaged,True
31788,CI test should use PR commit instead of pulling the latest master module: ci triaged,2020-01-02 18:32:00+00:00,,0,2,module: ci triaged,True
31779,torch.poisson returns floating point tensor module: distributions triaged,2020-01-02 17:25:49+00:00,,1,8,module: distributions triaged,True
31776,Documentation for `scatter` incorrectly states that index values must be unique module: bc-breaking module: docs triaged module: scatter & gather ops,2020-01-02 16:23:17+00:00,,0,6,module: bc-breaking module: docs triaged module: scatter & gather ops,True
31772,Support Python builtins on iterators in JIT oncall: jit triaged,2020-01-02 14:18:09+00:00,,0,3,oncall: jit triaged,True
31759,Allow using digits in names of named tensors triaged enhancement module: named tensor,2020-01-01 12:22:24+00:00,,0,4,triaged enhancement module: named tensor,True
31758,DataLoader: Segmentation Fault (core dumped) high priority needs reproduction module: crash module: dataloader triaged,2020-01-01 12:15:03+00:00,,0,31,high priority needs reproduction module: crash module: dataloader triaged,True
31752,Need a launch utility for Distributed RPC framework. triaged better-engineering module: rpc,2019-12-31 21:39:06+00:00,,0,2,triaged better-engineering module: rpc,True
31742,sparse tensor eliminate_zeros module: sparse feature triaged,2019-12-31 02:28:56+00:00,,0,11,module: sparse feature triaged,True
31710,AttributeError: module 'torch.distributed' has no attribute 'init_process_group' on torch 1.3 aarch64 oncall: distributed triaged,2019-12-30 19:50:33+00:00,,0,14,oncall: distributed triaged,True
31708,Computing dot product of columns sliced from large matrix causes illegal memory access in CUDA module: dependency bug module: cuda triaged module: 64-bit module: cublas,2019-12-30 17:36:42+00:00,,1,14,module: dependency bug module: cuda triaged module: 64-bit module: cublas,True
31697,the cmake problem with build from source ? module: binaries triaged,2019-12-30 07:18:03+00:00,,0,1,module: binaries triaged,True
31689,DataLoader does not consider default floating point type module: dataloader triaged,2019-12-29 11:23:32+00:00,,0,3,module: dataloader triaged,True
31688,Parallelization: more balanced work distribution among workers module: cpp feature triaged module: data parallel,2019-12-29 09:24:06+00:00,,0,3,module: cpp feature triaged module: data parallel,True
31685,What is the significance of torchvision._is_tracing()?  triaged module: vision,2019-12-29 04:07:08+00:00,,0,0,triaged module: vision,True
31679,broadcast randperm with dim specification triaged function request,2019-12-28 19:35:58+00:00,,0,0,triaged function request,False
31660,Cuda error 59 : device-side assert triggered needs reproduction module: cuda triaged,2019-12-27 18:14:45+00:00,,0,4,needs reproduction module: cuda triaged,True
31657,Use of Sequence collections for abstract classes in Dataset feature module: dataloader triaged,2019-12-27 14:41:25+00:00,,0,0,feature module: dataloader triaged,True
31655,pytorch out of memory when calculation squared difference of unfold module: memory usage triaged,2019-12-27 11:20:37+00:00,,0,1,module: memory usage triaged,False
31651,"update embedding at indices, other than those passed as input, in the case of sparse tensors module: sparse module: nn triaged enhancement",2019-12-27 07:19:42+00:00,,0,5,module: sparse module: nn triaged enhancement,True
31615,'torch.load' report 'bad pickle data' module: serialization triaged,2019-12-26 02:02:43+00:00,,0,14,module: serialization triaged,True
31606,No auto-suggest capacity for Transformer triaged enhancement,2019-12-25 08:04:39+00:00,,0,2,triaged enhancement,True
31603,Typo in `torch.utils.tensorboard.add_image` oncall: visualization,2019-12-25 07:44:16+00:00,,0,0,oncall: visualization,False
31598,ATen not compiled with MKL support module: build triaged module: mkl,2019-12-24 16:12:55+00:00,,0,4,module: build triaged module: mkl,True
31596,pytorch forward hangs in multiprocess environment module: multiprocessing triaged,2019-12-24 12:55:25+00:00,,0,0,module: multiprocessing triaged,True
31579,Option to apply weights to gradients when using DistributedDataParallel oncall: distributed feature module: autograd triaged module: data parallel,2019-12-23 21:22:14+00:00,,0,5,oncall: distributed feature module: autograd triaged module: data parallel,True
31574,attribute and register_buffer are not the same on gpu  oncall: jit triaged,2019-12-23 16:52:15+00:00,,0,0,oncall: jit triaged,True
31571,Bug in ForkingPickler for multiprocessing spawn context for shared storages on Linux module: multiprocessing triaged,2019-12-23 14:44:33+00:00,,0,3,module: multiprocessing triaged,True
31570,Can't pin storage memory module: docs module: cuda triaged,2019-12-23 14:31:06+00:00,,0,2,module: docs module: cuda triaged,True
31565,Force libtorch to use CUDA context oncall: jit triaged,2019-12-23 09:23:57+00:00,,0,1,oncall: jit triaged,True
31558,More dynamic PyTorch APIs high priority triaged module: ux,2019-12-22 17:57:37+00:00,,1,15,high priority triaged module: ux,True
31557,[docs] F.ctc_loss docs to warn clearly about invalid inf-causing inputs; zero_infinity to become enabled by default module: docs triaged,2019-12-22 15:28:23+00:00,,0,13,module: docs triaged,False
31553,allow setting different batch size splits for data_parallel.py and distributed.py oncall: distributed feature triaged module: data parallel,2019-12-22 00:02:27+00:00,,0,4,oncall: distributed feature triaged module: data parallel,False
31551,TorchBind broken on rocm oncall: jit triaged,2019-12-21 21:33:37+00:00,,0,5,oncall: jit triaged,True
31548,"c++ PReLUFuncOptions declared, not used or valid triaged",2019-12-21 19:53:41+00:00,,0,1,triaged,True
31546,[Feature Request] Make torch.solve output NaN for singular matrix triaged module: linear algebra,2019-12-21 17:09:48+00:00,,0,12,triaged module: linear algebra,True
31545,IndexExpressions (or slice) for jit.script functions oncall: jit triaged,2019-12-21 16:29:52+00:00,,0,3,oncall: jit triaged,True
31528,cuCtxGetDevice error and seg fault with DDP and OpenMPI oncall: distributed module: cuda triaged,2019-12-20 22:17:22+00:00,,0,6,oncall: distributed module: cuda triaged,True
31487,Disable PSIMD? Why Pytorch is trying to download PSimd when PSIMD_SOURCE_DIR is defined? module: build triaged,2019-12-19 18:32:39+00:00,,0,1,module: build triaged,False
31478,DataParallel has different tensor copy behavior if batch size = 1 triaged module: data parallel,2019-12-19 17:10:03+00:00,,0,3,triaged module: data parallel,True
31474,Slow clip_grad_norm_ because of .item() calls when run on device module: performance module: cuda triaged module: norms and normalization,2019-12-19 16:06:23+00:00,,0,4,module: performance module: cuda triaged module: norms and normalization,True
31471,Mnasnet0_5 first layer shape incorrect triaged module: vision,2019-12-19 15:01:56+00:00,,0,3,triaged module: vision,True
31467,Distributed hangs on process termination with world_size=1 oncall: distributed module: bootcamp triaged,2019-12-19 10:46:36+00:00,,0,1,oncall: distributed module: bootcamp triaged,True
31461,Make RRef.to_here() non-blocking triaged module: rpc,2019-12-19 05:56:28+00:00,,1,2,triaged module: rpc,True
31460,DataParallel doesn't properly handle kwargs module: nn triaged module: data parallel,2019-12-19 05:14:24+00:00,,0,17,module: nn triaged module: data parallel,True
31458,Integrate `torch.xxx` and `Tensor.xxx` triaged module: ux,2019-12-19 03:07:22+00:00,,0,2,triaged module: ux,True
31430,[jit] `del` with slices doesn't work oncall: jit triaged,2019-12-18 18:59:30+00:00,,0,0,oncall: jit triaged,True
31425,cuDNN convolution does not handle empty input tensor module: cudnn module: cuda triaged small,2019-12-18 18:14:11+00:00,,0,4,module: cudnn module: cuda triaged small,True
31423,Add option in LSTM layer to access all cell states of all time steps feature module: nn module: rnn triaged,2019-12-18 16:21:39+00:00,,0,7,feature module: nn module: rnn triaged,True
31422,Support DataParallel with PackedSequence oncall: distributed module: bootcamp triaged enhancement module: data parallel,2019-12-18 14:17:35+00:00,,0,5,oncall: distributed module: bootcamp triaged enhancement module: data parallel,True
31394,torch.scatter_logsumexp triaged function request module: scatter & gather ops,2019-12-17 23:44:25+00:00,,0,4,triaged function request module: scatter & gather ops,True
31391,"Error ""builtin cannot be used as a value"" when add Python snippets in C++ oncall: jit low priority triaged",2019-12-17 22:59:23+00:00,,0,2,oncall: jit low priority triaged,True
31386,[jit] Python type hints in TorchScript classes don't work oncall: jit triaged,2019-12-17 21:38:16+00:00,,0,0,oncall: jit triaged,True
31368,TestMomentumSGD.test_fp16momentum_sgd (caffe2) is flaky high priority triage review caffe2 module: flaky-tests,2019-12-17 16:53:12+00:00,,0,0,high priority triage review caffe2 module: flaky-tests,True
31367,`clip_grad_norm` allows negative `max_norm` values module: nn triaged enhancement,2019-12-17 16:51:10+00:00,,0,0,module: nn triaged enhancement,True
31362,sklearn and pytorch incompatibility issue needs reproduction module: crash triaged,2019-12-17 14:04:27+00:00,,0,12,needs reproduction module: crash triaged,True
31360,DDP/MP not yielding nontrivial speedup module: performance module: multiprocessing triaged module: data parallel,2019-12-17 12:50:43+00:00,,0,1,module: performance module: multiprocessing triaged module: data parallel,True
31356,Optimizing DLRM for CPU module: performance feature module: cpu triaged,2019-12-17 04:46:48+00:00,,0,0,module: performance feature module: cpu triaged,True
31353,Memory management is inefficient which limits performance module: performance module: cuda module: memory usage triaged,2019-12-17 03:29:09+00:00,,0,2,module: performance module: cuda module: memory usage triaged,True
31321,Ability to download docs HTML for offline use module: docs triaged,2019-12-16 18:45:39+00:00,,0,2,module: docs triaged,False
31316,quantization - Missing operations needed for object detection oncall: quantization triaged,2019-12-16 15:13:17+00:00,,0,8,oncall: quantization triaged,True
31311,Why Keras behave better and faster than Pytorch under the same network configuration? module: performance module: windows triaged,2019-12-16 11:26:17+00:00,,0,3,module: performance module: windows triaged,True
31308,[mac] Failure to import torch triaged module: macos,2019-12-16 06:40:50+00:00,,0,2,triaged module: macos,True
31300,torch runtime error when manual link libmkldnn.so module: build triaged module: mkldnn,2019-12-15 07:43:19+00:00,,0,2,module: build triaged module: mkldnn,True
31289,Intel OMP multiprocessing assertion failure: Assertion failure at z_Linux_util.cpp(2338) module: dependency bug module: multiprocessing triaged,2019-12-14 15:37:57+00:00,,0,2,module: dependency bug module: multiprocessing triaged,True
31277,nn.MultiHeadAttention with different similarity measures triaged oncall: transformer/mha function request,2019-12-14 01:30:32+00:00,,0,2,triaged oncall: transformer/mha function request,False
31264,Failed to config caffe2_rocksdb in cmake module: build triaged,2019-12-13 21:09:02+00:00,,0,6,module: build triaged,True
31261,pinned memory requires DeviceGuard in multi-process envs module: docs module: multiprocessing module: cuda triaged,2019-12-13 19:22:36+00:00,,0,2,module: docs module: multiprocessing module: cuda triaged,True
31259,Script method can't call a scripted function when it is decorated with `@torch.no_grad` oncall: jit triaged,2019-12-13 19:04:53+00:00,,0,1,oncall: jit triaged,True
31252,[feature request] Better handling for CUDA Out of Memory module: cuda triaged,2019-12-13 14:16:43+00:00,,0,9,module: cuda triaged,True
31232,Default shuffle behavior of DistributedSampler oncall: distributed module: dataloader triaged,2019-12-13 03:21:19+00:00,,0,2,oncall: distributed module: dataloader triaged,True
31228,[RPC] Support nn.Module pickling with share memory triaged module: rpc,2019-12-13 02:05:30+00:00,,0,1,triaged module: rpc,True
31202,Request for Lint Pass to Detect Modification on Parameters/Attributes during TorchScript Inference oncall: jit feature triaged,2019-12-12 18:51:33+00:00,,0,1,oncall: jit feature triaged,False
31191,[JIT] Slice with optional not supported oncall: jit triaged,2019-12-12 16:02:54+00:00,,0,0,oncall: jit triaged,True
31185,Retain Subgraph or Save Intermediate Grad support? module: autograd triaged,2019-12-12 13:13:23+00:00,,0,5,module: autograd triaged,True
31178,torch::nn::functional::interpolate crash module: cpp triaged,2019-12-12 07:45:53+00:00,,0,5,module: cpp triaged,True
31141,[JIT] tensor(device=...) and tensor.to(device = ...) does not work properly in traced functions and modules oncall: jit triaged,2019-12-11 22:50:28+00:00,,0,6,oncall: jit triaged,True
31129,[jit] Python objects as arguments are not mutated oncall: jit low priority triaged,2019-12-11 19:08:20+00:00,,0,0,oncall: jit low priority triaged,True
31110,torch.nn.Softplus threshold argument bug? module: nn triaged,2019-12-11 13:17:48+00:00,,0,6,module: nn triaged,True
31107,Add torch.version.nccl oncall: distributed module: bootcamp triaged enhancement module: nccl,2019-12-11 11:47:38+00:00,,0,2,oncall: distributed module: bootcamp triaged enhancement module: nccl,True
31085,Common lookup of generic types across full script and mobile parsers feature triaged,2019-12-11 00:43:07+00:00,,1,0,feature triaged,True
31050,float[] unsupported in native_functions.yaml triaged enhancement,2019-12-10 16:16:40+00:00,,0,0,triaged enhancement,True
31048,MathJax too small in Firefox module: docs triaged,2019-12-10 15:37:41+00:00,,0,0,module: docs triaged,True
31038,Pytorch 1.3.0 on RTX cards: CUDA error: an illegal memory access was encountered module: cuda module: memory usage triaged,2019-12-10 07:59:39+00:00,,0,5,module: cuda module: memory usage triaged,True
31007,"GPU version of minimal example for libtorch fails with ""no kernel image is available..."" module: cuda triaged",2019-12-09 22:50:12+00:00,,0,3,module: cuda triaged,True
31004,The inference speed of the torch compiled manually is slower than the torch build from official binaries? needs reproduction module: binaries module: performance triaged,2019-12-09 22:29:20+00:00,,0,4,needs reproduction module: binaries module: performance triaged,True
30987,Remove `.data` module: autograd triaged enhancement better-engineering actionable,2019-12-09 19:47:58+00:00,,0,20,module: autograd triaged enhancement better-engineering actionable,True
30968,Categorical.sample too slow module: distributions triaged,2019-12-09 17:01:02+00:00,,0,4,module: distributions triaged,True
30965,JIT breaks with postponed annotations oncall: jit triaged,2019-12-09 14:54:56+00:00,,0,2,oncall: jit triaged,True
30962,How can I add masks to parameters module: nn triaged,2019-12-09 12:50:11+00:00,,0,1,module: nn triaged,True
30955,error C3203: “templated_iterator”: 未专用化的 类 模板 不能用作 模板 变量，该变量属于 模板 参数“_Ty1”，应为 real 类型 needs reproduction module: internals triaged,2019-12-09 06:08:50+00:00,,0,1,needs reproduction module: internals triaged,True
30953,false CHECK FAILED at ../aten/src/ATen/core/function_schema_inl.h module: internals triaged,2019-12-09 05:41:05+00:00,,0,0,module: internals triaged,True
30947,Wrong initialization with kaiming_uniform_ module: nn triaged,2019-12-09 00:02:52+00:00,,0,0,module: nn triaged,True
30934,Spurious negative output in convolution of positive tensors module: docs module: convolution triaged,2019-12-07 16:11:34+00:00,,0,5,module: docs module: convolution triaged,True
30929,How to set not to build libtorch_cpu.so and libmkl_*.so dependencies? module: build triaged module: mkl,2019-12-07 04:08:13+00:00,,0,2,module: build triaged module: mkl,True
30903,Don't ship protoc in wheels module: binaries triaged,2019-12-06 20:27:22+00:00,,0,0,module: binaries triaged,True
30900,CUDA error: initialization error (multiprocessing) with Python 3.7  needs reproduction module: multiprocessing triaged,2019-12-06 20:04:52+00:00,,0,7,needs reproduction module: multiprocessing triaged,True
30899,Adding max_norm constraint to an Embedding layer leads to an error module: nn triaged,2019-12-06 20:00:51+00:00,,0,0,module: nn triaged,True
30896,"FATAL_ERROR ""Failed to determine the source files for the regular expression backend"" module: build triaged module: third_party",2019-12-06 19:42:00+00:00,,0,4,module: build triaged module: third_party,True
30873,ReduceLROnPlateau detects a plateau during a steady decrease after a spike module: optimizer triaged,2019-12-06 16:54:35+00:00,,0,5,module: optimizer triaged,True
30867,save model definition file feature module: serialization triaged,2019-12-06 10:58:58+00:00,,0,1,feature module: serialization triaged,True
30863,"[TensorBoard] Installed tensorboard 1.14.0, TestTensorBoardWriter.test_writer failed. oncall: visualization",2019-12-06 08:26:08+00:00,,0,0,oncall: visualization,False
30830,Torch getting stuck transfering model to GPU in multiple GPU setting needs reproduction module: multiprocessing module: cuda triaged,2019-12-05 18:26:46+00:00,,0,4,needs reproduction module: multiprocessing module: cuda triaged,True
30812,Tensorboard graph failing to nest modules oncall: visualization,2019-12-05 09:56:07+00:00,,0,5,oncall: visualization,False
30807,Sending sparse tensors over RPC not yet supported module: serialization triaged module: rpc,2019-12-05 07:26:12+00:00,,1,10,module: serialization triaged module: rpc,True
30803,Pytorch openmp thread number tuning option for CPU trainning module: performance module: cpu triaged enhancement module: multithreading,2019-12-05 06:39:28+00:00,,2,4,module: performance module: cpu triaged enhancement module: multithreading,True
30788,Master Task: JIT/C++ Parity With Pytorch Python API oncall: jit triaged,2019-12-05 02:29:23+00:00,,0,0,oncall: jit triaged,True
30786,C++ / JIT Parity for ops in `torch/functional.py` and `torch/tensor.py` oncall: jit triaged TSUsability TSRootCause:PyTorchParityGap,2019-12-05 02:21:18+00:00,,0,0,oncall: jit triaged TSUsability TSRootCause:PyTorchParityGap,True
30780,Remove Ops bound in Python Layer for Legacy Reasons module: internals triaged module: pybind,2019-12-05 00:33:25+00:00,,0,0,module: internals triaged module: pybind,True
30774,Natively Declarable Fast-path Functions  high priority module: internals triaged module: dispatch,2019-12-05 00:08:10+00:00,,0,12,high priority module: internals triaged module: dispatch,True
30762,Page for `torch.__config__` 404s module: docs triaged,2019-12-04 23:03:04+00:00,,0,0,module: docs triaged,True
30759,`TestDocCoverage.test_torch` error messages could be clearer module: docs triaged,2019-12-04 22:51:46+00:00,,0,0,module: docs triaged,True
30754,"JIT, nn.utils.weight_norm and {save,load}_state_dict produce wrong results oncall: jit triaged",2019-12-04 21:29:41+00:00,,0,4,oncall: jit triaged,True
30744,Issues in linking libtorch c++  module: build triaged,2019-12-04 18:59:03+00:00,,0,2,module: build triaged,True
30723,Distributed Package asynchronous send/receive not working as expected (Gloo) oncall: distributed triaged,2019-12-04 11:02:01+00:00,,0,11,oncall: distributed triaged,True
30718,Method to broadcast parameters/buffers of DDP model oncall: distributed triaged enhancement,2019-12-04 07:37:50+00:00,,0,4,oncall: distributed triaged enhancement,True
30711,Sorting in embedding_dense_backward_cuda takes very long time module: performance module: cuda triaged module: embedding,2019-12-04 03:15:07+00:00,,0,10,module: performance module: cuda triaged module: embedding,True
30702,[FR] multidim squeeze and flatten triaged function request module: viewing and reshaping,2019-12-04 00:27:32+00:00,,0,6,triaged function request module: viewing and reshaping,True
30658,"Dedicated inverse AdaptiveMaxPool1d operation (e.g., AdaptiveMaxUnpool1d) module: nn triaged module: vision module: pooling function request",2019-12-03 13:33:42+00:00,,0,2,module: nn triaged module: vision module: pooling function request,True
30651,[TensorBoard] The different order of import SummaryWriter may cause Segmentation fault needs reproduction oncall: visualization,2019-12-03 06:44:03+00:00,,0,6,needs reproduction oncall: visualization,True
30635,Cannot use torch.jit.script with nn.DataParallel oncall: jit triaged module: data parallel,2019-12-02 23:58:59+00:00,,0,2,oncall: jit triaged module: data parallel,True
30633,Remove RPC internal helper that overrides the default pickler triaged module: rpc,2019-12-02 23:12:14+00:00,,1,1,triaged module: rpc,True
30611,Refactor/consolidate code for generating test tensors module: tests triaged module: type promotion,2019-12-02 16:40:43+00:00,,0,0,module: tests triaged module: type promotion,True
30596,Overlapping strides not supported by cublas triaged module: cublas,2019-12-02 01:45:59+00:00,,0,2,triaged module: cublas,True
30589,weight norm missing p= module: nn triaged enhancement,2019-12-01 18:36:40+00:00,,0,4,module: nn triaged enhancement,True
30586,"run ./android/run_tests.sh --warning-mode all   ,  show error. triaged module: android oncall: mobile",2019-12-01 12:01:11+00:00,,1,4,triaged module: android oncall: mobile,True
30574,`index_select` with multidimensional `index` feature triaged module: advanced indexing,2019-11-29 22:10:42+00:00,,0,9,feature triaged module: advanced indexing,True
30569,"Assertion `index >= -sizes[i] && index < sizes[i] && ""index out of bounds""` failed. module: cuda module: error checking triaged",2019-11-29 14:11:05+00:00,,0,5,module: cuda module: error checking triaged,True
30565,`F.interpolate` returns unexpected result when dealing with output size `1` module: nn triaged module: vision module: interpolation,2019-11-29 07:44:21+00:00,,0,13,module: nn triaged module: vision module: interpolation,True
30563,[ONNX] Support affine_grid_generator module: onnx triaged enhancement onnx-triaged,2019-11-29 02:21:18+00:00,,1,13,module: onnx triaged enhancement onnx-triaged,False
30535,[FR] nn.init.* accept None as input tensor module: nn triaged enhancement,2019-11-27 21:30:23+00:00,,0,0,module: nn triaged enhancement,True
30533,Document memory characteristics of in-place ops module: sparse module: docs triaged enhancement module: type promotion,2019-11-27 20:42:52+00:00,,0,1,module: sparse module: docs triaged enhancement module: type promotion,True
30532,Version 1.3 no longer supporting Tesla K40m? module: binaries module: docs module: cuda triaged,2019-11-27 20:31:28+00:00,,0,78,module: binaries module: docs module: cuda triaged,True
30528,Warn about driver CUDA mismatch in torch.cuda.is_available() module: cuda triaged enhancement,2019-11-27 19:21:09+00:00,,0,3,module: cuda triaged enhancement,True
30525,[Tensorboard] nothing appears in the projector tab needs reproduction module: tensorboard oncall: visualization,2019-11-27 19:06:03+00:00,,0,12,needs reproduction module: tensorboard oncall: visualization,False
30507,"LibTorch, Error in 'xxx': free(): invalid pointer needs reproduction module: cpp triaged",2019-11-27 09:13:41+00:00,,0,12,needs reproduction module: cpp triaged,True
30505,Adding a function to change the process_group in DistributedDataParallel oncall: distributed triaged,2019-11-27 08:08:52+00:00,,0,1,oncall: distributed triaged,True
30489,Torchscript Precision Issue with PyTorch/vision pretrained model inception v3 oncall: jit triaged,2019-11-26 22:43:09+00:00,,0,0,oncall: jit triaged,True
30469,Add single input/output tensor scatter/gather to ProcessGroup base class oncall: distributed triaged enhancement,2019-11-26 18:22:22+00:00,,0,0,oncall: distributed triaged enhancement,True
30463,cuda support module: build triaged,2019-11-26 14:28:48+00:00,,0,1,module: build triaged,True
30461,`torch.multiprocessing.spawn` fails when `join=False` module: multiprocessing triaged,2019-11-26 11:27:46+00:00,,0,1,module: multiprocessing triaged,True
30459,[TensorBoard] Graph with objects other than torch.nn.Module can not be visualized. triaged module: tensorboard,2019-11-26 09:08:34+00:00,,0,28,triaged module: tensorboard,True
30458,add method to make tensor constant for debug purposes module: autograd triaged,2019-11-26 09:03:23+00:00,,0,8,module: autograd triaged,True
30443,Expose bhistc to python triaged enhancement,2019-11-26 02:19:28+00:00,,0,0,triaged enhancement,True
30440,Redundant counter in batchnorm impl module: nn triaged,2019-11-26 01:12:17+00:00,,0,2,module: nn triaged,True
30421,[jit] Module references aren't preserved oncall: jit triaged,2019-11-25 21:48:38+00:00,,0,0,oncall: jit triaged,True
30418,More detailed information about TensorType in error messages triaged enhancement,2019-11-25 21:34:09+00:00,,1,0,triaged enhancement,True
30413,nn.functional should maintain API parity with nn where possible module: nn triaged enhancement,2019-11-25 19:33:58+00:00,,0,0,module: nn triaged enhancement,True
30405,Codegen refactoring master task triaged,2019-11-25 15:20:59+00:00,,1,10,triaged,True
30404,affine_grid CUDA / cuDNN support for Half removed in 1.3.x module: cudnn module: cuda triaged,2019-11-25 14:39:55+00:00,,0,7,module: cudnn module: cuda triaged,True
30403,last updated timestamp module: docs triaged enhancement,2019-11-25 14:37:24+00:00,,0,0,module: docs triaged enhancement,False
30402,There is no support for `weight_decay`/`momentum` in SGD for sparse tensors. module: sparse triaged enhancement,2019-11-25 13:56:06+00:00,,0,7,module: sparse triaged enhancement,True
30401,Unexpected difference torch.multiprocessing.manager.queue and torch.multiprocessing.queue module: multiprocessing module: cuda triaged,2019-11-25 12:17:08+00:00,,0,5,module: multiprocessing module: cuda triaged,True
30399,Conv2d: Inconsistent results on Raspberry Pi 3B  module: convolution triaged,2019-11-25 09:59:45+00:00,,0,0,module: convolution triaged,True
30388,Increasing memory usage on CPU module: performance module: cpu module: memory usage triaged,2019-11-24 18:34:50+00:00,,0,8,module: performance module: cpu module: memory usage triaged,True
30387,[docs] Missing docs for Storage.from_buffer module: docs triaged,2019-11-24 17:27:26+00:00,,0,5,module: docs triaged,False
30386,Exponentiated gradient descent feature module: optimizer triaged,2019-11-24 17:20:07+00:00,,1,4,feature module: optimizer triaged,True
30384,Docs missing link to torch.__config__ module: docs triaged,2019-11-24 11:19:21+00:00,,0,0,module: docs triaged,False
30380,Add Google Colab as an option on the 'Get Started' page. module: docs triaged enhancement,2019-11-24 07:09:19+00:00,,0,4,module: docs triaged enhancement,False
30373,Refactor (a bit) `torch.hub(.load)` triaged enhancement module: hub,2019-11-23 17:39:59+00:00,,0,1,triaged enhancement module: hub,True
30365,TorchScript Performance: 150x gap between TorchScript and Native Python triage review oncall: jit triaged,2019-11-23 02:36:50+00:00,,0,13,triage review oncall: jit triaged,True
30291,[feature request] A way to restore/assign tensor _version feature module: autograd triaged,2019-11-22 04:33:27+00:00,,0,4,feature module: autograd triaged,False
30263,pairwise_dist eps argument is confusing module: numerical-stability module: bc-breaking module: docs triaged,2019-11-21 21:38:08+00:00,,0,4,module: numerical-stability module: bc-breaking module: docs triaged,True
30248,Add scripts for comprehensive benchmark TensorIterator module: performance feature triaged,2019-11-21 19:46:05+00:00,,1,0,module: performance feature triaged,True
30246,CUDA masked_select uses way too much memory module: performance module: cuda triaged,2019-11-21 19:22:26+00:00,,0,4,module: performance module: cuda triaged,True
30226,Wrong substitution of aten::to  oncall: jit triaged oncall: mobile,2019-11-21 08:49:48+00:00,,0,4,oncall: jit triaged oncall: mobile,True
30225,c10:Error: could not unlink the shared memory file module: multiprocessing module: dataloader triaged better-engineering,2019-11-21 08:04:28+00:00,,1,1,module: multiprocessing module: dataloader triaged better-engineering,True
30214,Add support for fusion of relu6 with conv and bn for quantization oncall: quantization triaged,2019-11-21 03:20:35+00:00,,1,4,oncall: quantization triaged,False
30182,[quantization][graph mode] SubgraphRewriter discards SourceRanges oncall: jit triaged,2019-11-20 21:12:23+00:00,,1,1,oncall: jit triaged,True
30162,[docs] Newline / whitespace / comma missing in formula for PoissonNLLLoss module: docs triaged,2019-11-20 17:24:46+00:00,,1,1,module: docs triaged,False
30161,Support struct that is initializable / mutable in CPP oncall: jit triaged,2019-11-20 17:17:58+00:00,,0,1,oncall: jit triaged,True
30155,Transform caffe2 to trt failed caffe2 caffe2-op triaged,2019-11-20 13:38:10+00:00,,0,0,caffe2 caffe2-op triaged,True
30150,I have  reconstructed LSTM model and tested by mnist data but the loss is not changed (loss=2.3) module: rnn triaged,2019-11-20 09:07:19+00:00,,1,5,module: rnn triaged,True
30138,Unable to register custom JIT Operator with AliasAnalysisKind::CONSERVATIVE  oncall: jit triaged,2019-11-20 02:40:36+00:00,,1,5,oncall: jit triaged,True
30049,CPU Memory Leak for JIT ScriptModule on DataParallel oncall: jit triaged,2019-11-18 22:51:58+00:00,,0,5,oncall: jit triaged,True
29987,Move the attributes of a module to the given device oncall: jit triaged,2019-11-18 04:17:53+00:00,,0,4,oncall: jit triaged,True
29981,[feature request] Multivariate normal CDF module: distributions feature triaged,2019-11-17 10:30:43+00:00,,0,5,module: distributions feature triaged,False
29973,Indexing into tensor order of magnitude slower than numpy high priority module: performance module: autograd triaged module: advanced indexing,2019-11-16 19:52:58+00:00,,0,36,high priority module: performance module: autograd triaged module: advanced indexing,True
29961,Add support for integer matrix multiplication (particularly for dtype = torch.int8 ) feature module: cuda triaged,2019-11-16 05:29:36+00:00,,0,3,feature module: cuda triaged,True
29891,Get wrong precision when multi nodes run in docker needs reproduction oncall: distributed triaged,2019-11-15 06:58:10+00:00,,0,1,needs reproduction oncall: distributed triaged,True
29863, n-dimensional non-constant padding functional module: nn triaged enhancement module: padding,2019-11-15 00:37:23+00:00,,0,1,module: nn triaged enhancement module: padding,True
29860,torch.save/load shows raw path on the pickle_module arg module: docs module: serialization triaged,2019-11-15 00:16:23+00:00,,0,0,module: docs module: serialization triaged,True
29844,Fold DispatchStub into c10 dispatcher triaged module: dispatch,2019-11-14 21:44:46+00:00,,0,11,triaged module: dispatch,True
29843,torch.distributions.normal.Normal is not JIT supported triage review oncall: jit feature triaged,2019-11-14 21:44:09+00:00,,0,19,triage review oncall: jit feature triaged,True
29822,[jit] Traced `cat` on GPU doesn't support negative indexing oncall: jit triaged,2019-11-14 17:52:49+00:00,,0,0,oncall: jit triaged,True
29816,AdamSparse fails to run module: sparse triaged,2019-11-14 15:53:27+00:00,,0,1,module: sparse triaged,True
29814,SGD fails on sparse matrix module: sparse module: optimizer triaged,2019-11-14 15:40:01+00:00,,0,11,module: sparse module: optimizer triaged,True
29804,Parallel data loader performance degradation for IterableDataset with num_workers > 1 (but not for Dataset). module: dataloader triaged,2019-11-14 11:52:21+00:00,,1,2,module: dataloader triaged,True
29800,unsqueeze has 'out=' option documented but not implemented(?) module: docs triaged,2019-11-14 10:28:06+00:00,,0,1,module: docs triaged,False
29799,android run build_pytorch_android.sh error triaged oncall: mobile,2019-11-14 10:18:57+00:00,,1,4,triaged oncall: mobile,True
29763,Add doc coverage testing for quantization.rst module: docs oncall: quantization triaged,2019-11-13 22:58:27+00:00,,1,0,module: docs oncall: quantization triaged,False
29758,[doc] Tensor.mean: dtype kwarg is not documented module: docs triaged module: reductions,2019-11-13 22:01:16+00:00,,0,1,module: docs triaged module: reductions,True
29754,[FR] general nll_loss and cross_entropy along arbitrary dimension module: loss triaged,2019-11-13 21:37:35+00:00,,0,1,module: loss triaged,True
29750,[jit] Printing the graph doesn't include function calls oncall: jit triaged,2019-11-13 21:21:51+00:00,,0,0,oncall: jit triaged,True
29739,"[build] gcc 7.4 needs CMAKE_CXX_FLAGS=""-std=gnu++11"" module: dependency bug module: build triaged",2019-11-13 18:39:19+00:00,,0,1,module: dependency bug module: build triaged,True
29734,Tensor.nbytes() returns itemsize * numel for sparse tensors module: sparse module: autograd triaged actionable fixathon,2019-11-13 17:57:40+00:00,,1,4,module: sparse module: autograd triaged actionable fixathon,True
29722,Slow (20-50x) RNN tutorial/example when torch is installed using pip comp. to conda installation module: binaries module: performance module: rnn triaged module: mkl,2019-11-13 10:45:36+00:00,,0,17,module: binaries module: performance module: rnn triaged module: mkl,True
29720,Provide a mechanism to set global state per test in thread-safe manner module: tests triaged enhancement,2019-11-13 09:03:33+00:00,,0,0,module: tests triaged enhancement,True
29717,CPU and CUDA error messages are divergent in type promotion module: tests triaged module: type promotion,2019-11-13 08:56:09+00:00,,0,1,module: tests triaged module: type promotion,True
29713,[FR] F.pad support syntax sugars for specifying the padding amount module: convolution triaged enhancement,2019-11-13 08:13:14+00:00,,0,0,module: convolution triaged enhancement,True
29710,Reliable way to identify RuntimeErrors (CUDA) module: cuda triaged enhancement,2019-11-13 06:29:54+00:00,,0,1,module: cuda triaged enhancement,True
29697,MultiStepLR does not return good lr after load_state_dict module: optimizer triaged,2019-11-13 01:23:40+00:00,,0,32,module: optimizer triaged,True
29692,[FR] add generator= kwarg support for torch.randn and torch.rand triaged enhancement module: random,2019-11-13 00:21:50+00:00,,0,3,triaged enhancement module: random,False
29662,[jit] Dict construction fails at runtime oncall: jit triaged,2019-11-12 19:28:20+00:00,,0,0,oncall: jit triaged,True
29657,Improved detection of repeated observers oncall: quantization low priority triaged,2019-11-12 17:54:32+00:00,,0,0,oncall: quantization low priority triaged,True
29647,[ONNX] Exported ONNX module with for loop + scatter operation on tensor seems to be incorrect module: onnx triaged onnx-triaged,2019-11-12 11:31:55+00:00,,0,5,module: onnx triaged onnx-triaged,True
29642,error: ‘struct torch::jit::RegisterOperators’ has no member named ‘op’ oncall: jit triaged,2019-11-12 10:14:14+00:00,,0,4,oncall: jit triaged,True
29637,Compiled functions can't take variable number of arguments oncall: jit feature triaged,2019-11-12 06:04:23+00:00,,0,10,oncall: jit feature triaged,True
29597,[jit] Script class attributes aren't automatically added oncall: jit triaged,2019-11-11 22:55:42+00:00,,0,1,oncall: jit triaged,True
29554,[feature request] Print some measure of fragmentation at CUDA out-of-memory module: cuda triaged enhancement,2019-11-11 17:45:56+00:00,,0,1,module: cuda triaged enhancement,True
29553,`print` uses lots of GPU memory module: printing module: cuda triaged enhancement,2019-11-11 17:42:08+00:00,,0,1,module: printing module: cuda triaged enhancement,True
29548,Dispatch key reorganization module: internals triaged,2019-11-11 15:51:33+00:00,,0,5,module: internals triaged,True
29545,Tensorboard GPU Problems module: cuda triaged module: tensorboard,2019-11-11 11:27:05+00:00,,1,3,module: cuda triaged module: tensorboard,True
29528,Half precision cdist module: cuda triaged module: half function request module: distance functions,2019-11-10 20:45:10+00:00,,0,1,module: cuda triaged module: half function request module: distance functions,True
29522,Indexing with torch tensors and NumPy arrays is different triaged module: numpy,2019-11-10 12:02:06+00:00,,0,3,triaged module: numpy,True
29516,RuntimeError: CUDA error: invalid device ordinal module: cuda triaged,2019-11-10 07:02:38+00:00,,0,4,module: cuda triaged,True
29510,torch.stack: bad shape error message module: docs module: error checking triaged,2019-11-09 21:34:26+00:00,,0,3,module: docs module: error checking triaged,True
29498,"""malloc(): memory corruption (fast)"", action=3) at malloc.c needs reproduction module: crash triaged",2019-11-09 01:40:22+00:00,,0,3,needs reproduction module: crash triaged,True
29429,NLLLoss reduce=True returning nan in float16 module: nn triaged,2019-11-08 01:10:37+00:00,,0,3,module: nn triaged,True
29419,"IValue can't be constructed from one of `int`, `long`, or `long long`. oncall: jit triaged",2019-11-07 23:25:08+00:00,,0,0,oncall: jit triaged,True
29402,[RFC] RPC timeout triaged module: rpc,2019-11-07 19:42:26+00:00,,0,3,triaged module: rpc,True
29398,[feature request] torch.kthvalue to support a new argument largest triaged module: sorting and selection,2019-11-07 19:01:05+00:00,,0,3,triaged module: sorting and selection,True
29378,Support out= parameters with autograd feature module: autograd module: nn triaged,2019-11-07 17:01:21+00:00,,0,16,feature module: autograd module: nn triaged,True
29377,[jit] Saving a `ScriptFunction` to a buffer doesn't work oncall: jit triaged,2019-11-07 16:58:19+00:00,,0,0,oncall: jit triaged,True
29373,backward_compatibility_check_test doesn't play well with reverts module: ci triaged,2019-11-07 14:42:23+00:00,,2,2,module: ci triaged,True
29372,torch.std() returns nan for single item tensors. triaged module: numpy small,2019-11-07 13:52:19+00:00,,0,10,triaged module: numpy small,True
29369,nn.Conv(n)d constructor doesn't check for the number of kernel dimensions module: nn module: convolution triaged,2019-11-07 12:00:03+00:00,,0,1,module: nn module: convolution triaged,True
29254,Add support for ivalue float scalars oncall: jit triaged,2019-11-06 00:34:25+00:00,,1,0,oncall: jit triaged,False
29235,Redo our library structure module: build triaged,2019-11-05 20:11:26+00:00,,0,5,module: build triaged,True
29202,Add instructions for building torch.distributed on macOS oncall: distributed module: docs triaged,2019-11-05 15:58:24+00:00,,0,0,oncall: distributed module: docs triaged,True
29198,RuntimeError: CUDA out of memory with available GPU memory module: cuda module: memory usage triaged,2019-11-05 14:33:30+00:00,,0,2,module: cuda module: memory usage triaged,True
29190,How to run two different jit models in two GPUs respectively in one scrip? oncall: jit triaged,2019-11-05 10:09:34+00:00,,0,1,oncall: jit triaged,True
29177,[FR] trace_module traces both eval and train graph oncall: jit triaged enhancement,2019-11-05 03:04:56+00:00,,0,0,oncall: jit triaged enhancement,True
29172,Modules without copying  in multiprocess module: multiprocessing triaged,2019-11-05 01:59:16+00:00,,0,3,module: multiprocessing triaged,True
29137,"torch.sum(tensor, dim=()) is different from np.sum(arr, axis=()) high priority module: bc-breaking triaged module: numpy module: TensorIterator module: deprecation module: reductions",2019-11-04 18:37:41+00:00,,1,37,high priority module: bc-breaking triaged module: numpy module: TensorIterator module: deprecation module: reductions,True
29128,JIT should respect SKIP_PYTHON_BINDINGS and SKIP_PYTHON_BINDINGS_SIGNATURES triage review oncall: jit triaged,2019-11-04 16:45:45+00:00,,1,1,triage review oncall: jit triaged,True
29125,_compared_saved_loaded doesn't work with torch.tensor constants triage review oncall: jit triaged,2019-11-04 16:20:09+00:00,,0,0,triage review oncall: jit triaged,True
29116,torch.masked_fill missing out argument feature triaged module: ux,2019-11-04 14:21:53+00:00,,0,4,feature triaged module: ux,True
29115,"crash when call dist.new_group(ranks=local_ranks, backend='gloo') oncall: distributed triaged",2019-11-04 13:48:49+00:00,,0,1,oncall: distributed triaged,True
29111,First element in data passed to `torch.*Tensor` constructors cannot be a tensor triaged module: nestedtensor,2019-11-04 10:07:11+00:00,,0,3,triaged module: nestedtensor,True
29108,a retrained and saved  jit module could not be reload. oncall: jit triaged,2019-11-04 06:44:47+00:00,,1,8,oncall: jit triaged,True
29094,"""Unknown type constructor"" error in TorchScript oncall: jit triaged jit-backlog",2019-11-02 20:32:17+00:00,,0,2,oncall: jit triaged jit-backlog,True
29093,error: 'SO_REUSEPORT' was not declared in this scope module: build triaged module: third_party,2019-11-02 19:26:51+00:00,,0,16,module: build triaged module: third_party,True
29092,[FR] script returns subclass of the original module class triage review oncall: jit triaged TSUsability TSRootCause:ModuleInheritance,2019-11-02 15:25:12+00:00,,0,3,triage review oncall: jit triaged TSUsability TSRootCause:ModuleInheritance,True
29063,Trace of torch.tensor is impressively convoluted oncall: jit triaged,2019-11-01 21:23:21+00:00,,0,2,oncall: jit triaged,True
29057,Docker issue for Pytorch 1.3  module: binaries module: build triaged module: docker,2019-11-01 19:33:30+00:00,,0,0,module: binaries module: build triaged module: docker,True
29044,Add Wishart and inverse-Wishart distributions module: distributions feature triaged,2019-11-01 17:47:04+00:00,,0,13,module: distributions feature triaged,True
29042,Support clang+cuda builds module: build triaged enhancement,2019-11-01 17:15:46+00:00,,0,7,module: build triaged enhancement,True
29033,Move allgather_coalesced functionality to comm.cpp. oncall: distributed triaged,2019-11-01 14:48:14+00:00,,0,1,oncall: distributed triaged,False
29028,Inconsistent documentation for in-place functions at https://pytorch.org/docs/stable/torch.html module: docs triaged,2019-11-01 13:43:34+00:00,,0,7,module: docs triaged,False
29026,RuntimeError: !t.is_cuda() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/sparse/SparseTensorMath.cpp:591 module: sparse triaged module: assert failure,2019-11-01 09:25:10+00:00,,0,3,module: sparse triaged module: assert failure,True
29023,Named Tensors: Slicing based on name triaged module: advanced indexing module: named tensor,2019-11-01 08:00:15+00:00,,0,3,triaged module: advanced indexing module: named tensor,True
29021,Named Tensors: size of a named dimension module: docs triaged module: named tensor,2019-11-01 07:56:00+00:00,,0,1,module: docs triaged module: named tensor,True
29010,torch.stack does not yet support named tensors  feature triaged module: named tensor,2019-10-31 23:47:18+00:00,,0,2,feature triaged module: named tensor,False
28993,"torch.mean(x, dims=[]) has incorrect gradient in 1.2 high priority module: autograd triaged quansight-nack module: reductions",2019-10-31 20:58:17+00:00,,0,13,high priority module: autograd triaged quansight-nack module: reductions,True
28980,Unsafe use of `at::parallel_for` in current codebase high priority module: performance module: internals module: autograd triaged module: multithreading,2019-10-31 19:36:42+00:00,,1,9,high priority module: performance module: internals module: autograd triaged module: multithreading,True
28962,[feature request] Some activations modules missing inplace argument module: nn triaged enhancement,2019-10-31 15:21:52+00:00,,0,0,module: nn triaged enhancement,True
28956,RuntimeError: CUDA error: unknown error needs reproduction module: cuda triaged,2019-10-31 10:20:20+00:00,,0,2,needs reproduction module: cuda triaged,True
28950,ConnectionResetError when using dataLoader with pin_memory=True needs reproduction module: dataloader triaged,2019-10-31 05:46:53+00:00,,0,12,needs reproduction module: dataloader triaged,True
28943,The `@` operation uses too much memory on GPU needs reproduction module: memory usage triaged,2019-10-31 02:57:16+00:00,,0,3,needs reproduction module: memory usage triaged,True
28938,Cannot select version in the tutorials page module: docs triaged,2019-10-30 23:45:18+00:00,,0,1,module: docs triaged,True
28930,Add support for multidimensional input to `at::tensor` module: cpp triaged,2019-10-30 22:07:58+00:00,,0,0,module: cpp triaged,True
28929,torch::tensor(scalar) behaves differently from at::tensor(scalar) module: cpp triaged module: pybind,2019-10-30 22:04:47+00:00,,0,1,module: cpp triaged module: pybind,True
28925,"Provide rpc, remote and dist autograd C++ APIs and register them as Prim::ops module: cpp module: autograd triaged enhancement module: rpc",2019-10-30 21:44:25+00:00,,0,0,module: cpp module: autograd triaged enhancement module: rpc,True
28906,Font used in documentation is not always sharp module: docs triaged,2019-10-30 19:57:30+00:00,,0,4,module: docs triaged,True
28884,Why attn_mask is not 3D tensor in nn.MultiheadAttention? module: nn triaged enhancement oncall: transformer/mha,2019-10-30 15:39:18+00:00,,0,0,module: nn triaged enhancement oncall: transformer/mha,True
28882,Support RRef[T].__call__(*args) which invokes T.__call__(*args) on owner triaged module: rpc,2019-10-30 15:29:49+00:00,,1,6,triaged module: rpc,True
28863,Command for downloading torch 1.12.0 CUDA 10 linked to cu92 version module: binaries triaged,2019-10-30 01:10:38+00:00,,0,4,module: binaries triaged,True
28859,Remove bce_with_logits in derivatives.yaml triaged enhancement better-engineering,2019-10-29 22:54:46+00:00,,1,2,triaged enhancement better-engineering,True
28845,Implement operator<< for bfloat16 triaged module: bfloat16,2019-10-29 19:57:57+00:00,,1,0,triaged module: bfloat16,True
28829,torch.cuda.close request to be able to reset communications feature module: cuda triaged,2019-10-29 11:14:17+00:00,,0,3,feature module: cuda triaged,True
28794,torch.from_file not documented module: docs triaged,2019-10-28 17:20:40+00:00,,0,2,module: docs triaged,True
28791,[CUDA-MEMCHECK] TestTorchDeviceTypeCUDA.test_pin_memory_from_constructor_cuda fails module: cuda triaged,2019-10-28 17:06:02+00:00,,0,2,module: cuda triaged,True
28786,RPC couldn't match torch.ones with requires_grad=True high priority triage review oncall: distributed module: internals triaged better-engineering module: rpc,2019-10-28 16:24:28+00:00,,1,2,high priority triage review oncall: distributed module: internals triaged better-engineering module: rpc,True
28780,'import torch` fails with Illegal instruction module: build triaged,2019-10-28 05:37:18+00:00,,0,0,module: build triaged,True
28777,cudnn.determinstic=True causes dilated convolution to be >10x slower module: cudnn triaged,2019-10-28 04:34:16+00:00,,0,9,module: cudnn triaged,True
28761,torch.tensor() is very slow when it is passed an h5py Dataset. module: performance triaged module: tensor creation,2019-10-27 19:25:05+00:00,,0,7,module: performance triaged module: tensor creation,True
28757,[feature request] Docs for fuse_conv_bn_eval module: docs triaged,2019-10-27 16:59:33+00:00,,0,3,module: docs triaged,False
28756,cmake allows both MKL and MKLDNN to be OFF;  aten/src/ATen/CMakeLists.txt then ignores c++ sources module: build triaged,2019-10-27 16:24:00+00:00,,0,2,module: build triaged,True
28754,ABI backwards compatibility module: abi triaged,2019-10-27 13:31:34+00:00,,0,3,module: abi triaged,True
28746,super().__init__() not called in torch.nn.Module.__init__ module: nn triaged,2019-10-26 15:03:24+00:00,,0,2,module: nn triaged,True
28745,Problem when installing Pytorch from source on CentOS 7.4 module: build triaged,2019-10-26 14:49:12+00:00,,0,2,module: build triaged,True
28743,Sampler for IterableDataset module: dataloader triaged,2019-10-26 12:50:25+00:00,,0,10,module: dataloader triaged,True
28742,torch.as_tensor fails to create named tensors triaged module: named tensor module: tensor creation,2019-10-26 09:22:53+00:00,,1,1,triaged module: named tensor module: tensor creation,True
28733,CPU MaxPool2d is very slow module: performance module: bootcamp module: cpu triaged module: pooling,2019-10-26 01:21:36+00:00,,0,6,module: performance module: bootcamp module: cpu triaged module: pooling,True
28721,[docs] Improve docs of nn.MultiheadAttention module: docs triaged,2019-10-25 23:20:53+00:00,,0,3,module: docs triaged,False
28657,[docs] Unclear input/output format for TransformerEncoderLayer module: nn triaged oncall: transformer/mha,2019-10-25 16:15:30+00:00,,0,9,module: nn triaged oncall: transformer/mha,True
28655,Cmake fails due to bad python call module: build triaged,2019-10-25 15:25:20+00:00,,0,0,module: build triaged,True
28648,Can't successfully install pytorch with python3.6 on my pi 4  module: build triaged has workaround,2019-10-25 11:05:42+00:00,,0,14,module: build triaged has workaround,True
28619,Channels Last (NHWC) support plan. triaged module: memory format,2019-10-24 21:21:22+00:00,,2,11,triaged module: memory format,True
28594,[BUG] Can't Deepcopy module with weightnorm module: nn triaged,2019-10-24 15:30:23+00:00,,0,12,module: nn triaged,True
28577,`Tensor.__reversed__` breaks protocol for reversible objects module: internals triaged module: pybind,2019-10-24 10:39:31+00:00,,0,2,module: internals triaged module: pybind,True
28558,We should suggest using as_strided_ instead of set_ module: docs triaged,2019-10-23 23:05:04+00:00,,0,0,module: docs triaged,True
28552,Doing rendezvous twice can cause hangs oncall: distributed triaged,2019-10-23 21:50:59+00:00,,0,6,oncall: distributed triaged,True
28549,Expose DifferentiableGraphBackward to python oncall: jit feature module: autograd triaged,2019-10-23 21:27:50+00:00,,1,6,oncall: jit feature module: autograd triaged,True
28532,TestTorch.test_c10_layer_norm fails if you run it on a build of PyTorch with BUILD_CAFFE2_OPS=0 triaged module: dispatch,2019-10-23 18:41:57+00:00,,0,1,triaged module: dispatch,True
28520,Unified management of thread local variables high priority module: performance module: internals feature triaged module: multithreading,2019-10-23 17:36:04+00:00,,0,12,high priority module: performance module: internals feature triaged module: multithreading,True
28519,[jit] C++ Documentation oncall: jit triaged jit-backlog,2019-10-23 17:20:07+00:00,,0,1,oncall: jit triaged jit-backlog,True
28517,[JIT] __repr__ support for ScriptModules oncall: jit low priority triaged jit-backlog,2019-10-23 17:07:57+00:00,,0,0,oncall: jit low priority triaged jit-backlog,True
28515,Get rid of libc10.so module: build triaged,2019-10-23 16:48:56+00:00,,0,2,module: build triaged,True
28497,Problem when installing pytorch 1.4 from source on Centos 6.3 module: build triaged,2019-10-23 10:05:04+00:00,,0,2,module: build triaged,True
28444,Problem installing from source on CentOS 6.5 module: build triaged module: nccl,2019-10-22 20:21:48+00:00,,0,1,module: build triaged module: nccl,True
28441,gradient of Dirichlet.log_prob gives nan module: distributions triaged,2019-10-22 19:58:52+00:00,,0,5,module: distributions triaged,True
28440,Python/C++ API Parity: torch.optim optimizers module: cpp triaged,2019-10-22 19:29:29+00:00,,1,7,module: cpp triaged,True
28417,Patch: Fix for using `clang` to compile CUDA module: build module: cuda triaged,2019-10-22 09:26:59+00:00,,0,4,module: build module: cuda triaged,True
28390,torch.multinominal ignores elements from cumulative distribution module: cpu triaged module: random,2019-10-21 22:17:27+00:00,,0,0,module: cpu triaged module: random,True
28341,LinearOperator Abstraction / Structure-Exploiting LazyTensors for Linear Algebra feature triaged module: linear algebra module: lazy,2019-10-20 00:00:23+00:00,,0,7,feature triaged module: linear algebra module: lazy,True
28329,No in-place version of where() triaged OSS contribution wanted actionable module: sorting and selection function request,2019-10-19 06:00:04+00:00,,0,15,triaged OSS contribution wanted actionable module: sorting and selection function request,True
28320,Unable to install pytorch with cuda 10.0 using conda module: build triaged,2019-10-18 22:51:31+00:00,,0,9,module: build triaged,True
28319,Seg-fault in LayerNormKernelImpl needs reproduction module: crash module: cuda triaged,2019-10-18 22:35:06+00:00,,0,2,needs reproduction module: crash module: cuda triaged,True
28318,`torch.nn.Module._load_state_dict` catch-all error message can be misleading triaged,2019-10-18 22:21:00+00:00,,0,2,triaged,True
28308,jit script fails with `AttributeError: 'str' object has no attribute 'lineno'` oncall: jit triaged,2019-10-18 21:00:21+00:00,,0,3,oncall: jit triaged,True
28307,[jit] scripted module and user defined methods oncall: jit triaged,2019-10-18 20:33:44+00:00,,0,2,oncall: jit triaged,True
28275,nn.parallel.replicate in v1.1+ is much slower than v1.0 module: performance triaged module: data parallel,2019-10-18 04:59:48+00:00,,0,7,module: performance triaged module: data parallel,True
28267,Problem with jit TorchScript while copying data between GRUs triage review oncall: jit triaged,2019-10-18 00:41:01+00:00,,0,4,triage review oncall: jit triaged,True
28258,[jit] TorchScript classes don't work in notebooks oncall: jit triaged,2019-10-17 22:38:27+00:00,,0,2,oncall: jit triaged,True
28249,Add scopes to autograd profiler triaged,2019-10-17 20:53:02+00:00,,0,10,triaged,False
28245,PyTorch RPC should expose critical metrics to the application. feature triaged module: rpc,2019-10-17 20:38:42+00:00,,1,5,feature triaged module: rpc,True
28244,ProcessGroupMPI reports incorrect world size oncall: distributed triaged,2019-10-17 20:26:05+00:00,,1,4,oncall: distributed triaged,True
28239,support class annotations in __init__ oncall: jit triaged jit-backlog,2019-10-17 19:07:37+00:00,,0,0,oncall: jit triaged jit-backlog,True
28233,torch native functions cannot be used with inspect.signature triaged module: pybind module: language binding,2019-10-17 17:40:37+00:00,,0,5,triaged module: pybind module: language binding,True
28224,Allow to disable polling for CUDA synchronization high priority module: performance module: cuda triaged enhancement,2019-10-17 15:51:42+00:00,,0,19,high priority module: performance module: cuda triaged enhancement,True
28223,Convert manually bound `cuda` `cpu` `byte` `float` operators to native_functions triaged module: ux,2019-10-17 15:09:45+00:00,,1,1,triaged module: ux,True
28221,TorchScript doesn't support torch.channels_last or any other memory format constants oncall: jit triaged module: memory format,2019-10-17 15:05:17+00:00,,1,0,oncall: jit triaged module: memory format,True
28220,TorchScript custom ops like `cuda` `byte` etc. doesn't support memory_format argument oncall: jit triaged,2019-10-17 15:04:18+00:00,,1,0,oncall: jit triaged,True
28218,"index_sub, index_mul and index_div triaged enhancement OSS contribution wanted",2019-10-17 14:07:55+00:00,,0,4,triaged enhancement OSS contribution wanted,True
28214,reflective padding for 5D tensor triaged function request module: padding,2019-10-17 12:42:14+00:00,,0,0,triaged function request module: padding,True
28206,torch.utils.tensorboard.SummaryWriter.add_graph do not support non-tensor inputs oncall: visualization,2019-10-17 05:24:14+00:00,,0,7,oncall: visualization,False
28194,New Stochastic Optimization Algorithms in Pytorch feature module: optimizer triaged,2019-10-17 01:52:03+00:00,,0,2,feature module: optimizer triaged,False
28191,[JIT] Allow user to provide aliasing information on input tensors and model parameters triage review oncall: jit triaged,2019-10-17 01:30:37+00:00,,0,2,triage review oncall: jit triaged,True
28146,Named tensor: align_to align_as  error messages triaged module: named tensor,2019-10-16 21:34:00+00:00,,0,2,triaged module: named tensor,True
28142,Named Tensor: Support -1 in `unflatten`. triaged module: named tensor,2019-10-16 21:29:25+00:00,,0,0,triaged module: named tensor,True
28119,Provide function similar to cdist that returns dist_p^p triaged function request module: distance functions,2019-10-16 18:39:10+00:00,,0,11,triaged function request module: distance functions,False
28110,Stable docs show dead __config__ section link module: docs triaged,2019-10-16 16:50:01+00:00,,0,2,module: docs triaged,False
28093,Move options logic in _like functions from parsing layer to implementations triaged,2019-10-16 15:54:09+00:00,,1,3,triaged,False
28090,[discussion] Smarter version of torch.reshape (can avoid realloc in some cases) triaged function request module: viewing and reshaping,2019-10-16 15:07:09+00:00,,0,8,triaged function request module: viewing and reshaping,True
28055,Traced resnet101 leaks memory during `forward` module: memory usage triaged,2019-10-15 23:56:09+00:00,,0,3,module: memory usage triaged,True
28035,Easy way to create previews of the docs website after any changes triaged module: doc infra,2019-10-15 20:54:56+00:00,,0,1,triaged module: doc infra,False
28033,TestTorch.test_doc should be in TestDocCoverage module: docs triaged module: doc infra,2019-10-15 20:50:04+00:00,,0,6,module: docs triaged module: doc infra,True
27975,"Sending CUDA tensors via queue between processes, memory of Consumer process grows infinitely  module: multiprocessing module: cuda module: memory usage triaged",2019-10-15 13:16:52+00:00,,0,1,module: multiprocessing module: cuda module: memory usage triaged,True
27970,can't load model on cuda after call cudaDeviceReset functions oncall: jit triaged,2019-10-15 09:47:22+00:00,,0,9,oncall: jit triaged,True
27958,how to use libtorch library in cuda file with nvcc compiler(c++)? module: cpp triaged,2019-10-15 03:35:07+00:00,,0,2,module: cpp triaged,True
27955,Expand Pytorch C10D backend to dynamic load third party communication library oncall: distributed feature triaged,2019-10-15 02:34:23+00:00,,0,1,oncall: distributed feature triaged,False
27936,Specifying `pos_weight` in F.binary_cross_entropy_with_logits leads to RuntimeError: class size not match module: nn module: error checking triaged,2019-10-14 21:28:35+00:00,,0,0,module: nn module: error checking triaged,True
27926,`num_batches_tracked` update in `_BatchNorm` forward should be a single scalar update on host regardless of the residence of the layer module: performance module: nn module: cuda triaged enhancement,2019-10-14 20:46:36+00:00,,0,1,module: performance module: nn module: cuda triaged enhancement,True
27843,Be able to build torch.distributed documentation easier oncall: distributed triaged module: doc infra,2019-10-14 14:04:28+00:00,,0,7,oncall: distributed triaged module: doc infra,True
27840,caffe2 install VS2019 CUDA 10.1 lib\\torch.lib : fatal error LNK1248: image size (10028FA9F) exceeds maximum allowable size (FFFFFFFF)  caffe2,2019-10-14 10:55:22+00:00,,0,35,caffe2,True
27839,RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 24 and 195 in dimension 0 at /opt/conda/conda-bld/pytorch_1565272271120/work/aten/src/TH/generic/THTensor.cpp:689 oncall: jit module: nn triaged,2019-10-14 10:21:36+00:00,,0,5,oncall: jit module: nn triaged,True
27815,Improve the error message when trying to install in a 32-bit Python environment triaged enhancement,2019-10-12 19:35:01+00:00,,0,15,triaged enhancement,True
27785,docs for torch.cuda.reset_max_memory_reserved don't exist module: docs module: cuda triaged,2019-10-11 22:05:55+00:00,,0,5,module: docs module: cuda triaged,True
27780,torch.cuda.default_generators documentation referenced but don't exist. high priority module: docs module: cuda triaged,2019-10-11 21:18:46+00:00,,1,5,high priority module: docs module: cuda triaged,True
27766,Test `make html-stable` target in CI triaged module: doc infra,2019-10-11 18:54:35+00:00,,0,0,triaged module: doc infra,True
27752,Version number is still duplicated in a bunch of places module: binaries triaged better-engineering,2019-10-11 17:11:34+00:00,,0,3,module: binaries triaged better-engineering,True
27740,Scripting torchvision.models.detection.maskrcnn_resnet50_fpn triaged module: vision,2019-10-11 10:28:58+00:00,,0,2,triaged module: vision,True
27708,Documentation makefile should include torchvision module: docs triaged module: doc infra,2019-10-10 17:41:47+00:00,,1,0,module: docs triaged module: doc infra,True
27694,Deployment training model at C + + end needs reproduction oncall: jit module: docs module: cpp triaged,2019-10-10 10:47:43+00:00,,1,5,needs reproduction oncall: jit module: docs module: cpp triaged,True
27687,`nn.Sequential.__setattr__` appends to the execution list module: docs low priority triaged,2019-10-10 07:55:04+00:00,,0,4,module: docs low priority triaged,True
27682,I can't set gpu is 1 it always use gpu 0 needs reproduction module: cpp module: cuda low priority triaged,2019-10-10 06:16:32+00:00,,0,6,needs reproduction module: cpp module: cuda low priority triaged,True
27671,Avoid RTTI in DistEngine triaged better-engineering module: rpc,2019-10-10 00:37:00+00:00,,1,4,triaged better-engineering module: rpc,True
27647,🚀 Graceful RPCAgent termination in multi-driver scenario high priority triaged better-engineering module: rpc,2019-10-09 23:16:24+00:00,,1,4,high priority triaged better-engineering module: rpc,True
27642,Test re-entrant backward works with torch.distributed.autograd.backward() module: autograd triaged module: rpc,2019-10-09 23:05:11+00:00,,1,0,module: autograd triaged module: rpc,True
27641,torch.distributed.autograd.backward() should populate .grad field on Tensors by default. module: autograd triaged module: rpc,2019-10-09 23:03:31+00:00,,1,0,module: autograd triaged module: rpc,True
27617,[feature request] [dataloader] Pad variable-sized tensors in default_collate module: dataloader triaged enhancement,2019-10-09 17:54:23+00:00,,0,3,module: dataloader triaged enhancement,True
27614,scatter_add allows index tensor that doesn't match input size in forward pass but fails on backward pass high priority module: crash triaged,2019-10-09 17:31:01+00:00,,1,29,high priority module: crash triaged,True
27610,[FR][RFC] Build a serving framework to host and serve trained PyTorch models feature triaged,2019-10-09 16:29:17+00:00,,0,27,feature triaged,False
27608,Unify warning logging mechanism module: internals triaged enhancement better-engineering,2019-10-09 16:21:05+00:00,,0,0,module: internals triaged enhancement better-engineering,True
27588,RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR module: cudnn module: cuda triaged,2019-10-09 02:34:40+00:00,,0,27,module: cudnn module: cuda triaged,True
27579,Support Anomaly detection for distributed autograd. module: autograd triaged module: rpc,2019-10-08 22:59:41+00:00,,1,1,module: autograd triaged module: rpc,True
27542,Make topk sort stable triaged enhancement module: determinism OSS contribution wanted module: sorting and selection,2019-10-08 13:47:15+00:00,,0,18,triaged enhancement module: determinism OSS contribution wanted module: sorting and selection,True
27540,CTCLoss cuda backend large batch handling takes up to 1.8x more memory module: loss module: cuda module: memory usage triaged enhancement,2019-10-08 12:52:52+00:00,,0,1,module: loss module: cuda module: memory usage triaged enhancement,True
27522,[feature request] Reduction (torch.add / torch.logaddexp / torch.max / torch.min / torch.mean) of several tensors without extra copies/allocations / memory accesses } TensorList inputs support triaged function request,2019-10-08 03:11:57+00:00,,0,41,triaged function request,False
27517,Allow explicit gradients in torch.distributed.autograd.backward() API triaged module: rpc,2019-10-08 00:52:31+00:00,,1,1,triaged module: rpc,True
27505,[JIT] ndimension not supported oncall: jit triaged,2019-10-07 22:04:31+00:00,,0,1,oncall: jit triaged,True
27504,[JIT] List Comprehensions With Ifs not Supported  oncall: jit triaged,2019-10-07 21:57:43+00:00,,0,1,oncall: jit triaged,True
27497,TSAN failure related to mkldnn oncall: jit triaged module: mkldnn,2019-10-07 21:18:15+00:00,,0,2,oncall: jit triaged module: mkldnn,True
27479,[JIT] Figure out how to easily investigate memory usage issues issues oncall: jit triaged,2019-10-07 18:24:08+00:00,,2,5,oncall: jit triaged,True
27475,[JIT] script::Module API parity with nnmodule oncall: jit triaged,2019-10-07 18:05:54+00:00,,0,0,oncall: jit triaged,True
27466,batch_norm_elemt_cuda_template does not use its argument epsilon module: bc-breaking module: nn module: cuda triaged,2019-10-07 17:23:09+00:00,,0,7,module: bc-breaking module: nn module: cuda triaged,True
27421,[dataloader] Sampler abstract constructor API minor proposal module: dataloader triaged,2019-10-05 11:51:39+00:00,,0,2,module: dataloader triaged,True
27409,Installer not setting rpath for MAGMA (OS X w/ GPU) module: build module: cuda triaged module: macos,2019-10-04 22:53:19+00:00,,0,19,module: build module: cuda triaged module: macos,True
27406,[jit] String frontend doesn't support default arg values oncall: jit triaged,2019-10-04 21:36:31+00:00,,0,0,oncall: jit triaged,True
27394,Re-enable test_EmbeddingBag_per_sample_weights_and_no_offsets_cuda module: nn module: ci triaged module: flaky-tests,2019-10-04 19:29:14+00:00,,0,0,module: nn module: ci triaged module: flaky-tests,True
27376,[ONNX export] UpSample with scale_factor should map to UpSample op with scale  module: onnx triaged enhancement onnx-triaged,2019-10-04 17:33:06+00:00,,0,16,module: onnx triaged enhancement onnx-triaged,True
27343,[jit] `ScriptFunction`s are loaded as `ScriptModule`s oncall: jit triaged,2019-10-03 23:20:40+00:00,,1,3,oncall: jit triaged,True
27336,Allow one inferred axis in torch.split  triaged enhancement,2019-10-03 22:34:02+00:00,,0,1,triaged enhancement,False
27312,torch.Tensor.mean erroneously documented as sometimes returning a tuple module: docs triaged module: reductions,2019-10-03 19:56:10+00:00,,0,0,module: docs triaged module: reductions,True
27300,"Tensorboard add image with boxes, labels, and confidence scores. triaged module: tensorboard",2019-10-03 18:20:23+00:00,,0,2,triaged module: tensorboard,True
27297,CUDA error: device-side assert triggered(insert_events at /pytorch/c10/cuda/CUDACachingAllocator.cpp:569) module: cuda triaged,2019-10-03 16:31:03+00:00,,0,2,module: cuda triaged,True
27276,Add in-place view (view_) triaged function request module: viewing and reshaping,2019-10-03 04:40:16+00:00,,0,1,triaged function request module: viewing and reshaping,False
27218,pytorch data_parallel oom on gpu:0 module: memory usage triaged module: data parallel,2019-10-02 15:44:14+00:00,,0,1,module: memory usage triaged module: data parallel,True
27175,Easier way to create tensors with names triaged enhancement module: named tensor,2019-10-01 21:56:35+00:00,,0,12,triaged enhancement module: named tensor,True
27147,Stop binding in-place methods to `torch.*` triaged module: ux,2019-10-01 17:57:52+00:00,,0,0,triaged module: ux,True
27144,torch::jit::script::Module has no zero_grad() oncall: jit triaged jit-backlog,2019-10-01 17:22:48+00:00,,0,4,oncall: jit triaged jit-backlog,True
27138,CUDAPytorchToCaffe2.MutualResizes is flaky caffe2-op module: cuda module: tests triaged module: flaky-tests,2019-10-01 15:48:48+00:00,,0,0,caffe2-op module: cuda module: tests triaged module: flaky-tests,True
27099,Support implicit RRef type conversion triaged module: rpc,2019-09-30 21:19:22+00:00,,1,0,triaged module: rpc,True
27097,[c10] c10 dispatch doesn't support tracing of scalars module: internals triaged module: dispatch,2019-09-30 21:10:15+00:00,,0,19,module: internals triaged module: dispatch,True
27094,[jit] Document what types can be traced oncall: jit triaged jit-backlog,2019-09-30 20:32:43+00:00,,0,0,oncall: jit triaged jit-backlog,True
27072,RNG for torch.randn_like triaged enhancement module: random,2019-09-30 17:54:41+00:00,,0,9,triaged enhancement module: random,True
27055,torch::NoGradGuard no_grad get wrong  when I use batchsize!=1 module: cpp triaged,2019-09-30 08:16:01+00:00,,0,0,module: cpp triaged,True
27046,The Gather problem in DataParallel: dimension are not matched. triaged module: data parallel,2019-09-30 04:01:04+00:00,,0,2,triaged module: data parallel,True
27034,RuntimeError:[enforce fail at context.h:48] option.device_type() ==PROTO_CPU. 1vs0 caffe2 triaged,2019-09-29 13:12:45+00:00,,0,0,caffe2 triaged,True
27030,Tensorboard logging image WITH LABEL triaged module: tensorboard,2019-09-29 07:08:45+00:00,,0,3,triaged module: tensorboard,True
27024,Is it an incompleted dst tensor synchronization in CUDA device to device copy ? module: cuda triaged,2019-09-29 02:24:09+00:00,,0,0,module: cuda triaged,True
27021,Cmake warnings during build triaged module: build warnings,2019-09-28 21:52:25+00:00,,0,0,triaged module: build warnings,True
27015,Missing bin and include when building with torchvision on CentOS module: build module: cpp triaged module: vision,2019-09-28 08:16:04+00:00,,0,1,module: build module: cpp triaged module: vision,True
27003,[JIT] Script doesn't preserve builtin torch named tuples upon python return oncall: jit triaged jit-backlog,2019-09-27 22:37:02+00:00,,0,0,oncall: jit triaged jit-backlog,True
26990,torch.quantized.modules.floatFunctional -- add a little color commentary to the doc module: docs triaged quantization_release_1.3,2019-09-27 20:35:57+00:00,,2,1,module: docs triaged quantization_release_1.3,False
26967,Lint rule to test for creation of tensor in native/ without options() module: build module: lint triaged better-engineering,2019-09-27 14:25:47+00:00,,0,0,module: build module: lint triaged better-engineering,True
26964,AnyValueTest.CorrectlyAccessesIntWhenCorrectType UBSAN failure: owncast of address 0x60300105d750 which does not point to an object of type 'Holder<const int>' Sep 27 00:01:03 0x60300105d750: note: object is of type 'torch::nn::AnyModule::Value::Holder<int>' module: build module: cpp triaged,2019-09-27 13:34:59+00:00,,0,0,module: build module: cpp triaged,True
26957,Batched Dataloader feature module: dataloader triaged,2019-09-27 05:54:56+00:00,,0,9,feature module: dataloader triaged,True
26950,conv2d Memory usage is too large； pytorch 1.1.0 module: dependency bug module: cudnn module: memory usage module: convolution triaged,2019-09-27 03:23:44+00:00,,0,1,module: dependency bug module: cudnn module: memory usage module: convolution triaged,True
26889,Statically checked tensor shapes module: internals feature triaged,2019-09-26 12:41:48+00:00,,0,27,module: internals feature triaged,True
26863,[BUG Report]Integrate libtorch to ffmpeg but memory leak happened! module: build triaged module: vision,2019-09-26 03:01:14+00:00,,0,1,module: build triaged module: vision,True
26802,Google Summer of Code triaged,2019-09-25 14:06:32+00:00,,0,1,triaged,True
26796,setup.py install error module: build triaged,2019-09-25 11:13:25+00:00,,0,0,module: build triaged,True
26790,Importing tensorboard jams CUDA device selection triaged module: tensorboard,2019-09-25 07:37:27+00:00,,1,3,triaged module: tensorboard,True
26781,Matrix corresponding to convolution by a 2D kernel (convmtx2) feature module: nn triaged,2019-09-25 05:28:53+00:00,,0,15,feature module: nn triaged,True
26759,[RFC] RRef Protocol triaged module: rpc,2019-09-24 22:09:15+00:00,,0,5,triaged module: rpc,True
26744,[jit] Default args don't work with TorchScript classes oncall: jit triaged jit-backlog,2019-09-24 20:05:14+00:00,,0,0,oncall: jit triaged jit-backlog,True
26730,[jit] Bad error when instantiating TorchScript class with incorrect types oncall: jit triaged jit-backlog,2019-09-24 17:35:46+00:00,,0,0,oncall: jit triaged jit-backlog,True
26691,Support FPGA Xilinx triaged module: backend,2019-09-23 23:59:28+00:00,,0,11,triaged module: backend,True
26659,torch::nn::Sequential not compatible with torch::nn::RNN module: cpp module: nn triaged,2019-09-23 20:24:45+00:00,,0,3,module: cpp module: nn triaged,True
26567,Make `torch.save` serialize a zip file module: serialization triaged enhancement,2019-09-20 21:20:08+00:00,,0,4,module: serialization triaged enhancement,True
26551,No way to disable mse_loss broadcasting warning module: nn triaged enhancement,2019-09-20 17:39:30+00:00,,0,2,module: nn triaged enhancement,True
26534,MKLDNN+AMD BLIS path for PyTorch  feature module: cpu triaged module: mkldnn,2019-09-20 09:01:12+00:00,,0,13,feature module: cpu triaged module: mkldnn,True
26527,Multilinear map module: nn triaged function request,2019-09-20 06:29:35+00:00,,0,6,module: nn triaged function request,True
26456,[libtorch]Same model in CUDA and CPU got different result? module: cpp module: nn triaged,2019-09-19 07:04:18+00:00,,0,17,module: cpp module: nn triaged,True
26447,parallel_for may hang when called in main process and then on daemon process module: multiprocessing triaged module: deadlock,2019-09-19 01:25:13+00:00,,0,12,module: multiprocessing triaged module: deadlock,True
26446,Avoid sending zero grads over the wire in distributed autograd backward pass triaged module: rpc,2019-09-19 00:24:47+00:00,,1,0,triaged module: rpc,True
26428,Remove TensorOptions logic from generated code module: internals triaged,2019-09-18 20:58:43+00:00,,1,24,module: internals triaged,True
26409,Generated file not getting cleaned up by clean module: build triaged,2019-09-18 16:37:49+00:00,,0,0,module: build triaged,True
26400,RuntimeError: tensor.ndimension() == static_cast<int64_t>(expected_size.size()) INTERNAL ASSERT FAILED needs reproduction module: multi-gpu module: cuda triaged,2019-09-18 09:04:58+00:00,,0,6,needs reproduction module: multi-gpu module: cuda triaged,True
26396,NNPACK condition should be changed (ARM processors) module: convolution triaged enhancement module: nnpack,2019-09-18 07:51:33+00:00,,0,1,module: convolution triaged enhancement module: nnpack,True
26379,DistAutogradContext should be cleaned up in case of node failures. module: autograd module: memory usage triaged enhancement module: rpc,2019-09-18 00:04:00+00:00,,1,0,module: autograd module: memory usage triaged enhancement module: rpc,True
26354,torch.tensor / torch.as_tensor not working with list of tensors module: error checking triaged module: numpy module: tensor creation,2019-09-17 18:13:17+00:00,,0,10,module: error checking triaged module: numpy module: tensor creation,True
26345,The performance of multiplication of two matrices is different between window and linux module: performance module: windows triaged,2019-09-17 12:24:33+00:00,,1,2,module: performance module: windows triaged,True
26344,How to get rid of zombie processes using torch.multiprocessing.Pool? module: dependency bug oncall: distributed module: multiprocessing triaged,2019-09-17 11:55:41+00:00,,0,3,module: dependency bug oncall: distributed module: multiprocessing triaged,True
26340,CPU version of PyTorch on PyPI module: build feature oncall: releng module: cpu triaged,2019-09-17 08:10:19+00:00,,0,58,module: build feature oncall: releng module: cpu triaged,True
26338,Behavior of F.dropout in eval mode module: docs low priority triaged,2019-09-17 06:57:22+00:00,,0,2,module: docs low priority triaged,True
26334,"Supporting ""cdf"" for Student-T distribution module: distributions low priority triaged enhancement",2019-09-17 04:09:00+00:00,,0,7,module: distributions low priority triaged enhancement,True
26288,Inplace and out arguments for BatchNorm (and other norm layers: InstanceNorm / LayerNorm / GroupNorm ...) module: performance triaged function request module: norms and normalization,2019-09-16 15:46:26+00:00,,0,57,module: performance triaged function request module: norms and normalization,True
26258,"""git describe"" shows incorrect version 1.0 instead of 1.2 module: build triaged",2019-09-15 22:12:29+00:00,,1,2,module: build triaged,True
26228,Higher dimension support for `MultiLableSoftMarginLoss` module: nn module: loss triaged enhancement,2019-09-14 02:33:49+00:00,,0,0,module: nn module: loss triaged enhancement,True
26218,Default adam epsilon to 1e-7 when on fp16 module: numerical-stability module: optimizer triaged enhancement module: half,2019-09-13 23:02:07+00:00,,0,11,module: numerical-stability module: optimizer triaged enhancement module: half,True
26207,Dispatch Tracing/Debugging triaged enhancement internals module: dispatch,2019-09-13 21:50:23+00:00,,1,3,triaged enhancement internals module: dispatch,True
26165,Memory leak in multithreading environment when loading checkpoint high priority needs reproduction module: multiprocessing module: memory usage triaged module: multithreading,2019-09-13 12:52:11+00:00,,1,7,high priority needs reproduction module: multiprocessing module: memory usage triaged module: multithreading,True
26157,Tests for pytorch_macos_10_13_cuda9_2_cudnn7_py3_build fail module: ci triaged module: macos,2019-09-13 07:45:36+00:00,,0,1,module: ci triaged module: macos,True
26139,c10 List API hard to use module: internals triaged,2019-09-12 23:07:21+00:00,,0,4,module: internals triaged,True
26136,There should be gating around BFloat16 module: performance triaged enhancement module: bfloat16,2019-09-12 22:37:02+00:00,,0,0,module: performance triaged enhancement module: bfloat16,True
26120,Process fails with assertion error in magma-cuda100 module: dependency bug needs reproduction module: crash triaged module: linear algebra,2019-09-12 20:29:33+00:00,,0,22,module: dependency bug needs reproduction module: crash triaged module: linear algebra,True
26119,Make GloballyUniqueId a common type for both rpc and dist autograd triaged better-engineering module: rpc,2019-09-12 20:24:09+00:00,,1,3,triaged better-engineering module: rpc,True
26117,Tracing non-constant shapes is broken oncall: jit triaged,2019-09-12 20:17:41+00:00,,0,0,oncall: jit triaged,True
26110,Access data_ptr in RNN.cpp module: rnn triaged,2019-09-12 18:30:32+00:00,,0,9,module: rnn triaged,True
26109,Support the AVX512 runtime dispatch feature low priority triaged,2019-09-12 18:24:12+00:00,,1,6,feature low priority triaged,True
26097,[RFC] TensorBoard extensions and improvements for PyTorch triaged module: tensorboard,2019-09-12 15:13:16+00:00,,0,1,triaged module: tensorboard,True
26086,[Feature Request] Trace / Script C++ models oncall: jit module: cpp triaged,2019-09-12 06:43:45+00:00,,0,7,oncall: jit module: cpp triaged,True
26072,DataLoader workers fail to die module: dataloader triaged module: deadlock,2019-09-12 00:15:49+00:00,,0,5,module: dataloader triaged module: deadlock,True
26058,sccache stats can cause whole build to fail module: build triaged,2019-09-11 22:06:27+00:00,,0,0,module: build triaged,True
25991,Why doc building isn't failing us for referring to a non-existent method? module: docs triaged,2019-09-11 12:39:23+00:00,,0,2,module: docs triaged,True
25979,[C++] `Module::pretty_print` is broken module: cpp module: nn triaged,2019-09-11 05:13:23+00:00,,0,7,module: cpp module: nn triaged,True
25978,Provide a way to select SVD algorithm in PyTorch? triaged module: linear algebra function request,2019-09-11 05:04:10+00:00,,0,5,triaged module: linear algebra function request,True
25941,TestAutograd.test_deep_reentrant fails with SIGBUS on macOS module: autograd module: tests triaged module: macos,2019-09-10 20:05:52+00:00,,0,4,module: autograd module: tests triaged module: macos,True
25904,Python hang after using torch.exp() needs reproduction triaged module: deadlock,2019-09-10 07:50:40+00:00,,0,6,needs reproduction triaged module: deadlock,True
25892,load_state_dict on CPU first module: serialization triaged,2019-09-10 02:47:12+00:00,,0,1,module: serialization triaged,True
25888,Forward/backward hooks for C++ torch::nn modules module: cpp module: autograd triaged,2019-09-10 00:31:38+00:00,,0,29,module: cpp module: autograd triaged,True
25883,Python/C++ API Parity: torch.nn modules and functional module: cpp module: nn good first issue triaged,2019-09-09 22:35:40+00:00,,1,109,module: cpp module: nn good first issue triaged,True
25880,Incorrect lable read with ImageInput Op of Caffe2 caffe2 triaged,2019-09-09 22:09:18+00:00,,0,0,caffe2 triaged,True
25821,Conv2D 2x~20x slower than Tensorflow when channel count is small module: performance module: cudnn module: cuda module: convolution triaged,2019-09-07 15:53:03+00:00,,0,5,module: performance module: cudnn module: cuda module: convolution triaged,True
25820,Pytorch master can not build with computer capability 3.0 under Mac OS X 10.13.16 with Nvidia GT 750m module: cuda low priority triaged module: macos,2019-09-07 13:39:26+00:00,,0,2,module: cuda low priority triaged module: macos,True
25814,(PyTorch1.1 and 1.2) RuntimeError: Can't detach views in-place. Use detach() instead module: autograd module: optimizer triaged enhancement,2019-09-07 06:42:08+00:00,,0,8,module: autograd module: optimizer triaged enhancement,True
25803,[jit] `random` module support oncall: jit triaged jit-backlog,2019-09-06 21:16:10+00:00,,0,6,oncall: jit triaged jit-backlog,True
25785,[Feature request] modified Cholesky decomposition high priority triaged enhancement module: linear algebra,2019-09-06 17:50:47+00:00,,1,10,high priority triaged enhancement module: linear algebra,True
25783,Detaching a distribution's `log_prob` to block gradients only w.r.t its parameters module: distributions triaged,2019-09-06 17:36:14+00:00,,0,13,module: distributions triaged,True
25767,"Model parallel with DDP get `Socket Timeout` error when using NCCL, while GLOO works fine oncall: distributed triaged",2019-09-06 13:40:15+00:00,,0,27,oncall: distributed triaged,True
25752,torch.cuda.empty_cache() write data to gpu0 module: cuda triaged,2019-09-06 03:30:48+00:00,,0,3,module: cuda triaged,True
25747,Int32 overflow in bincount indexing module: cuda triaged module: 64-bit,2019-09-06 00:54:15+00:00,,0,0,module: cuda triaged module: 64-bit,True
25743,Custom sampler for Seq2Seq models to avoid padding module: dataloader triaged,2019-09-06 00:11:08+00:00,,0,6,module: dataloader triaged,True
25714,Backtrace prints many <unknown function> module: build triaged better-engineering,2019-09-05 16:48:01+00:00,,0,2,module: build triaged better-engineering,True
25691,[dataloader] Hang because of too many open files (and probably some process dead) module: dataloader triaged,2019-09-05 06:22:42+00:00,,0,4,module: dataloader triaged,True
25661,finfo operator not bound into JIT oncall: jit triaged jit-backlog,2019-09-04 18:59:35+00:00,,1,4,oncall: jit triaged jit-backlog,True
25648,"Torch.jit.trace unexpected error with `torch.cat(…, dim=-1)`  oncall: jit triaged",2019-09-04 15:03:55+00:00,,0,1,oncall: jit triaged,True
25646,libtorch forward memory leak module: cpp module: memory usage triaged,2019-09-04 14:30:47+00:00,,0,9,module: cpp module: memory usage triaged,True
25641,[distributed] all_gather on a List of Tensors directly oncall: distributed module: bootcamp triaged enhancement,2019-09-04 07:48:53+00:00,,1,5,oncall: distributed module: bootcamp triaged enhancement,True
25635,[C++] Support negative index in `torch::TensorAccessor::size()` module: cpp triaged,2019-09-04 05:42:45+00:00,,0,3,module: cpp triaged,True
25601,Avoid non-POD data in thread_local module: performance triaged module: multithreading better-engineering,2019-09-03 20:53:09+00:00,,0,0,module: performance triaged module: multithreading better-engineering,True
25591,IValue pickle does not work properly if an empty tensor table is not provided oncall: jit triaged jit-backlog,2019-09-03 17:49:40+00:00,,0,7,oncall: jit triaged jit-backlog,True
25570,Remote memory access similar to MPI one-sided in pytorch oncall: distributed feature triaged module: mpi,2019-09-03 03:30:46+00:00,,0,5,oncall: distributed feature triaged module: mpi,True
25552,Usage of DDP on a module that doesn't require gradients oncall: distributed triaged enhancement,2019-09-02 12:13:28+00:00,,0,1,oncall: distributed triaged enhancement,True
25548,The inference speed of the torch c++ dynamic library compiled manually is slower than the torch library officially provided module: binaries module: performance module: cuda triaged,2019-09-02 06:48:52+00:00,,0,13,module: binaries module: performance module: cuda triaged,True
25535,[Proposal] Pin Windows SDK and MSVC compiler versions in LibTorch module: build module: windows triaged enhancement,2019-09-01 11:14:53+00:00,,0,1,module: build module: windows triaged enhancement,True
25522,[dataloader] Problem in exception reraise mechanism module: dataloader triaged,2019-08-31 12:51:36+00:00,,0,2,module: dataloader triaged,True
25520,pytorch c++ api cannot call operator() on torch::nn::Sequential module: cpp module: nn triaged,2019-08-31 08:50:33+00:00,,0,0,module: cpp module: nn triaged,True
25518,Assign torch.cuda.FloatTensor to List tensor module: crash module: cuda triaged has workaround,2019-08-31 05:28:20+00:00,,0,3,module: crash module: cuda triaged has workaround,True
25514,Multithreaded backpropagation with custom autograd.Functions feature triaged module: data parallel,2019-08-31 00:53:30+00:00,,0,1,feature triaged module: data parallel,True
25492,clang-tidy job merges with master which can lead to hard to understand errors module: ci triaged,2019-08-30 20:14:52+00:00,,0,2,module: ci triaged,True
25481,[feature request] symmetric matrix square root triaged module: linear algebra function request,2019-08-30 18:19:26+00:00,,0,44,triaged module: linear algebra function request,True
25480,Vectorize bool operations triaged enhancement module: boolean tensor,2019-08-30 17:37:27+00:00,,0,0,triaged enhancement module: boolean tensor,True
25478,Delete TensorOptions::operator== module: internals triaged enhancement small,2019-08-30 16:16:06+00:00,,1,3,module: internals triaged enhancement small,True
25460,Build link not right module: build module: cpp triaged,2019-08-30 10:03:29+00:00,,0,11,module: build module: cpp triaged,True
25417,Add a mode to check input tensor sizes in allreduce_coalesced oncall: distributed feature triaged,2019-08-29 19:33:42+00:00,,0,1,oncall: distributed feature triaged,False
25416,Add GPU support to c10d allreduce_coalesced  oncall: distributed feature triaged,2019-08-29 19:28:14+00:00,,1,0,oncall: distributed feature triaged,False
25410,[jit] NamedTuples don't respect `__new__` oncall: jit triaged jit-backlog,2019-08-29 18:33:48+00:00,,0,4,oncall: jit triaged jit-backlog,True
25387,[DRAFT] Auto-casting in JIT - Automatic mixed precision oncall: jit triaged,2019-08-29 12:08:22+00:00,,0,17,oncall: jit triaged,True
25381,regarding builtin_function_or_method feature low priority triaged,2019-08-29 08:11:08+00:00,,0,1,feature low priority triaged,True
25329,[jit] Bad error for incorrect container type oncall: jit triaged jit-backlog,2019-08-28 17:57:35+00:00,,0,5,oncall: jit triaged jit-backlog,True
25314,ConstQuantizerPtr is misnamed oncall: quantization triaged,2019-08-28 15:33:34+00:00,,1,2,oncall: quantization triaged,True
25310,Handling of packed_sequence by activation functions and linear layers module: nn module: rnn triaged,2019-08-28 14:47:28+00:00,,0,5,module: nn module: rnn triaged,True
25297,torch.as_tensor(bytearray(...)) seems to leak memory module: dataloader module: cuda module: memory usage triaged,2019-08-28 08:37:57+00:00,,0,9,module: dataloader module: cuda module: memory usage triaged,True
25267,JIT leaks memory when I change the max sequence length oncall: jit triaged has workaround,2019-08-27 19:56:39+00:00,,0,17,oncall: jit triaged has workaround,True
25206,Ability to tell whether a tensor might be changed in TH/Aten impl high priority module: internals triaged,2019-08-26 23:13:41+00:00,,1,8,high priority module: internals triaged,True
25190,Error in python3: double free or corruption (fasttop) needs reproduction module: cudnn module: multiprocessing module: dataloader module: cuda triaged quansight-nack,2019-08-26 19:00:55+00:00,,0,13,needs reproduction module: cudnn module: multiprocessing module: dataloader module: cuda triaged quansight-nack,True
25174,BCEWithLogitsLoss expects wrong shape of weight (#classes instead of batch size) module: nn module: loss triaged,2019-08-26 11:57:29+00:00,,0,1,module: nn module: loss triaged,True
25162,Incorrect Validation Accuracy Due to Distributed Sampler oncall: distributed module: dataloader triaged,2019-08-25 16:45:10+00:00,,1,10,oncall: distributed module: dataloader triaged,True
25156,euclidean distance between two tensors triaged function request module: distance functions,2019-08-25 05:57:18+00:00,,0,6,triaged function request module: distance functions,True
25150,Problems with install python from source module: build triaged,2019-08-24 19:19:17+00:00,,0,10,module: build triaged,True
25137,Serialization does not work for quantized modules module: serialization triaged quantization_release_1.3,2019-08-24 01:25:57+00:00,,0,1,module: serialization triaged quantization_release_1.3,True
25132,Feature request: Fix dimension convention for masks in transformer feature module: nn triaged oncall: transformer/mha,2019-08-23 22:12:29+00:00,,0,3,feature module: nn triaged oncall: transformer/mha,False
25112,[RFC] NestedTensor - 0.0.2 triaged,2019-08-23 18:52:25+00:00,,0,0,triaged,True
25104,[FR] Dropout modules/functions should take in generator= module: nn triaged enhancement module: random,2019-08-23 16:36:37+00:00,,0,1,module: nn triaged enhancement module: random,True
25092,No way to correctly reset weights of a model with spectral norm module: nn triaged has workaround,2019-08-23 10:30:36+00:00,,0,9,module: nn triaged has workaround,True
25091,JavaScript (Web Assembly) target for trained models triaged enhancement,2019-08-23 10:07:34+00:00,,0,5,triaged enhancement,True
25071,Generator objects should not always use the same seed triaged module: random,2019-08-23 01:24:45+00:00,,0,1,triaged module: random,True
25070,[FR] torch.(Generator|random).seed allows specifying the seed value triaged enhancement module: random,2019-08-23 01:17:28+00:00,,0,0,triaged enhancement module: random,True
25066,[FR][jit] torch.jit.script as a class decorator oncall: jit triaged jit-backlog,2019-08-23 00:39:29+00:00,,0,12,oncall: jit triaged jit-backlog,True
25057,ctc_loss computes different losses and gradients on batched utterances vs. individual utterances module: loss triaged,2019-08-22 23:33:23+00:00,,0,0,module: loss triaged,True
25047,ScriptModule and nn.Module parameter ordering difference oncall: jit triaged jit-backlog,2019-08-22 21:27:58+00:00,,0,0,oncall: jit triaged jit-backlog,True
25045,[Distance functions] F.pdist backward CUDA invalid configuration module: cuda triaged module: distance functions,2019-08-22 21:11:15+00:00,,0,11,module: cuda triaged module: distance functions,True
25039,Add new interpolation modes to `grid_sample` module: nn triaged function request module: interpolation,2019-08-22 18:56:24+00:00,,0,4,module: nn triaged function request module: interpolation,True
25034,"make add_module accept tuples as well or change containers(ModuleList, Sequential, etc) to allow this module: nn triaged enhancement",2019-08-22 17:30:29+00:00,,0,0,module: nn triaged enhancement,True
25032,[FYI] NestedTensor Project Progress feature triaged module: nestedtensor,2019-08-22 17:11:28+00:00,,0,16,feature triaged module: nestedtensor,False
25014,Benchmark cuDNN affine_grid_generator vs native module: performance module: cudnn module: nn module: cuda triaged,2019-08-22 06:58:11+00:00,,0,1,module: performance module: cudnn module: nn module: cuda triaged,True
25004,[C++] Call find_package(Torch REQUIRED) more than one time in downstream project causes CMake configuration error module: build triaged,2019-08-22 00:47:44+00:00,,0,10,module: build triaged,True
24985,Data worker should fetch a sample instead of a batch. module: dataloader triaged enhancement,2019-08-21 20:33:16+00:00,,0,6,module: dataloader triaged enhancement,True
24984,DataLoader slow down when `pin_memory=False` module: performance module: multiprocessing module: dataloader triaged,2019-08-21 20:10:00+00:00,,1,5,module: performance module: multiprocessing module: dataloader triaged,True
24963,torch.cuda.synchronize blocks CUDA execution on other threads using other devices. module: cuda triaged,2019-08-21 14:58:56+00:00,,0,8,module: cuda triaged,True
24956,[RPC] Fix logging initialization warning in ProcessGroupAgent module: internals triaged,2019-08-21 07:04:16+00:00,,0,0,module: internals triaged,True
24946,[RPC] Make ProcessGroupAgent send task non-blocking todo triaged module: rpc,2019-08-21 02:33:51+00:00,,1,7,todo triaged module: rpc,True
24931,"Consider not checking in autogenerated core/{Tensor.h,TensorMethods.h} module: build module: cpp triaged",2019-08-20 21:15:14+00:00,,0,7,module: build module: cpp triaged,True
24930,Successive Layer Normalization in nn.Transformer module: nn triaged oncall: transformer/mha,2019-08-20 21:12:42+00:00,,0,9,module: nn triaged oncall: transformer/mha,True
24915,Shared Dataset Functionality module: dataloader triaged better-engineering,2019-08-20 17:22:07+00:00,,0,8,module: dataloader triaged better-engineering,True
24904,tensorboard add_graph error triaged module: tensorboard,2019-08-20 13:01:43+00:00,,0,28,triaged module: tensorboard,True
24899,Export torch.cat to ONNX with Dynamic shape does not work on GPU caffe2 triaged,2019-08-20 09:58:16+00:00,,0,9,caffe2 triaged,True
24891,RuntimeError on PyTorch 1.2 under NVIDIA Nsight Systems module: cuda triaged module: third_party,2019-08-20 05:10:36+00:00,,0,14,module: cuda triaged module: third_party,True
24870,Recommendations for Grid Sample/Affine Grid/Displacement Fields/Optical Flow proposal accepted triaged module: interpolation,2019-08-19 22:22:03+00:00,,0,16,proposal accepted triaged module: interpolation,True
24859,Improve binary release for PyTorch domain library module: binaries triaged better-engineering,2019-08-19 20:14:18+00:00,,0,27,module: binaries triaged better-engineering,True
24836,Gloo scatter gives wrong result for stride != 1 oncall: distributed module: bootcamp triaged,2019-08-19 07:57:51+00:00,,0,3,oncall: distributed module: bootcamp triaged,True
24835,NetworkX's Version caffe2,2019-08-19 04:18:21+00:00,,0,2,caffe2,False
24834,PyTorch 1.2 'module' object has no attribute 'BFloat16StorageBase' triaged module: undefined reference module: vision,2019-08-19 04:03:19+00:00,,0,12,triaged module: undefined reference module: vision,True
24831,subprocess.CalledProcessError: Compile source in NVIDIA TX2 module: build module: cuda triaged,2019-08-19 03:09:43+00:00,,0,3,module: build module: cuda triaged,True
24826,Transformer Lack of Embedding Layer and Positional Encodings high priority module: docs feature module: nn triaged needs design oncall: transformer/mha,2019-08-18 22:46:01+00:00,,0,18,high priority module: docs feature module: nn triaged needs design oncall: transformer/mha,True
24823,"Problematic handling of NaN and inf in grid_sample, causing segfaults, corrupted CUDA memory, and incorrect results high priority module: crash module: cuda triaged module: interpolation",2019-08-18 21:55:41+00:00,,1,3,high priority module: crash module: cuda triaged module: interpolation,True
24810,"Doesn't install the python module ""torch"" module: build triaged",2019-08-18 01:31:26+00:00,,0,5,module: build triaged,True
24798,Installs empty directories under Python's sitelibdir module: build triaged,2019-08-17 00:14:45+00:00,,0,0,module: build triaged,True
24786,[jit] Dict iterator invalidation doesn't match Python oncall: jit triaged jit-backlog,2019-08-16 19:52:37+00:00,,0,0,oncall: jit triaged jit-backlog,True
24500,Failed to compile PyTorch on IBM Power 9 architecture with CUDA 10 module: build triaged,2019-08-16 17:31:43+00:00,,0,0,module: build triaged,True
24498,Migrate CPU_tensor_apply to TensorIterator in aten/src/ATen/native/TensorCompare.cpp:30 triaged better-engineering module: CPU_tensor_apply,2019-08-16 17:27:42+00:00,,0,0,triaged better-engineering module: CPU_tensor_apply,True
24478,Port CPU_tensor_apply functions to TensorIterator (umbrella issue) triaged better-engineering module: CPU_tensor_apply,2019-08-16 17:17:41+00:00,,0,0,triaged better-engineering module: CPU_tensor_apply,True
24470,Benchmark cudnn version of grid sampler module: performance module: cudnn triaged,2019-08-16 15:45:43+00:00,,0,0,module: performance module: cudnn triaged,True
24468,[Tensorboard] Write summaries to S3 or GCS bucket feature triaged module: tensorboard,2019-08-16 15:24:05+00:00,,1,16,feature triaged module: tensorboard,True
24463,Crashes on torch.cuda.memory_allocated(device) module: error checking triaged,2019-08-16 11:28:27+00:00,,0,3,module: error checking triaged,True
24456,Build PyTorch 1.2.0 occur `recipe for target bin/test_parallel' failed module: build triaged,2019-08-16 04:01:50+00:00,,0,0,module: build triaged,True
24432,Use a ScriptModule on GPU that was saved from CPU oncall: jit module: serialization triaged,2019-08-15 20:06:06+00:00,,0,0,oncall: jit module: serialization triaged,True
24422,Label tracking meta-issue (edit me to get automatically CC'ed on issues! cc bot) triaged,2019-08-15 18:28:22+00:00,,0,36,triaged,True
24419,Building Python bits separate from C++ bits and making one play well with the other module: build triaged enhancement,2019-08-15 18:20:11+00:00,,0,3,module: build triaged enhancement,True
24417,1.0rc0-6216 installs empty directories under include and duplicates under / module: build triaged,2019-08-15 17:59:52+00:00,,0,3,module: build triaged,True
24401,"When running model forward with large batch size, it reports the error: THCudaTensor sizes too large for THCDeviceTensor conversion module: cuda triaged",2019-08-15 10:52:58+00:00,,0,5,module: cuda triaged,True
24398,Significantly slower in latest version than in 0.4.0 needs reproduction module: performance module: cuda triaged,2019-08-15 08:41:33+00:00,,0,10,needs reproduction module: performance module: cuda triaged,True
24397,SyncBatchNorm error when using model.eval() with DistributedDataParallel needs reproduction oncall: distributed module: autograd triaged,2019-08-15 08:41:19+00:00,,0,6,needs reproduction oncall: distributed module: autograd triaged,True
24354,Default warning handler in C++ doesn't seem to unique warnings module: error checking triaged,2019-08-14 19:42:47+00:00,,0,1,module: error checking triaged,True
24353,Visual Studio Code not providing autosuggestions for submodules triaged,2019-08-14 19:34:03+00:00,,0,3,triaged,True
24344,CI Standardization for Domain APIs module: binaries module: build module: ci triaged better-engineering,2019-08-14 18:12:13+00:00,,1,4,module: binaries module: build module: ci triaged better-engineering,True
24343,Unified representation for enum types oncall: jit triaged,2019-08-14 18:12:09+00:00,,0,0,oncall: jit triaged,True
24338,Consider changing the behavior of Tensor.__contains__(Tensor) to make more sense triaged module: numpy module: ux,2019-08-14 16:59:03+00:00,,0,6,triaged module: numpy module: ux,True
24336,Auto tuner takes too much time in serialized model oncall: jit triaged,2019-08-14 16:55:11+00:00,,1,13,oncall: jit triaged,True
24313,Value_select to perform region-wise selection triaged function request,2019-08-14 07:34:07+00:00,,0,0,triaged function request,False
24303,Port `masked_fill` operator from the TH code to Aten module: cuda module: cpu triaged module: porting better-engineering,2019-08-14 02:15:22+00:00,,0,1,module: cuda module: cpu triaged module: porting better-engineering,True
24273,Check PyTorch version when initializing process groups oncall: distributed module: bootcamp triaged enhancement,2019-08-13 21:02:02+00:00,,0,1,oncall: distributed module: bootcamp triaged enhancement,True
24264,"TensorIterator ""builder"" options should be documented. module: internals triaged",2019-08-13 19:21:42+00:00,,1,0,module: internals triaged,True
24261,"""PyTorch core"" thread local flag module: internals triaged enhancement",2019-08-13 18:51:21+00:00,,0,3,module: internals triaged enhancement,True
24248,Loading custom Torchscript C++ operators in python segfaults due to ABI compatibility issue between pytorch and libtorch oncall: jit triaged,2019-08-13 17:28:28+00:00,,1,2,oncall: jit triaged,True
24243,hasSideEffects INTERNAL ASSERT FAILED when using .split method with JIT high priority oncall: jit triaged,2019-08-13 16:31:39+00:00,,0,7,high priority oncall: jit triaged,True
24236,Tensorboard: Add disable flag for debugging triaged enhancement module: tensorboard,2019-08-13 09:35:14+00:00,,0,2,triaged enhancement module: tensorboard,False
24234,torch.utils.tensorboard.SummaryWriter fails to flush at program exit triaged module: tensorboard,2019-08-13 07:56:36+00:00,,0,6,triaged module: tensorboard,True
24229,Confusing error message for Custom Class type mismatch oncall: jit triaged jit-backlog,2019-08-13 05:04:37+00:00,,1,0,oncall: jit triaged jit-backlog,True
24225,Allow forward method to be defined with .define() in new TorchScript API oncall: jit triaged jit-backlog,2019-08-13 03:13:22+00:00,,1,3,oncall: jit triaged jit-backlog,True
24205,deprecate cuda arch 3.5/3.7 in nightlies module: binaries triaged,2019-08-12 22:19:59+00:00,,0,11,module: binaries triaged,True
24193,TensorIterator stubs are designed for merge conflicts. module: internals triaged,2019-08-12 18:21:19+00:00,,1,0,module: internals triaged,True
24188,Pin flake8 version in CI module: ci triaged,2019-08-12 17:16:47+00:00,,0,1,module: ci triaged,True
24185,[feature request] Subset of eigenvalues/eigenvectors  module: performance feature module: cpu triaged,2019-08-12 15:31:15+00:00,,0,12,module: performance feature module: cpu triaged,True
24176,torch.fft crash when used with nn.DataParallel module: cuda triaged module: data parallel,2019-08-12 12:09:49+00:00,,0,6,module: cuda triaged module: data parallel,True
24168,torch.unique is inconsistent with NumPy's unique triaged module: numpy module: correctness (silent),2019-08-12 04:47:02+00:00,,0,1,triaged module: numpy module: correctness (silent),True
24167,Failed to build pytorch with NanoPi M4 module: build low priority triaged,2019-08-12 02:31:09+00:00,,0,3,module: build low priority triaged,True
24155,"""To compact weights again call flatten_parameters()"" is printed every step for every GPU module: nn module: rnn triaged module: data parallel",2019-08-11 05:52:46+00:00,,1,15,module: nn module: rnn triaged module: data parallel,True
24145,sccache crashes when building `Distribution.cu` on Windows module: build triaged module: build warnings,2019-08-10 09:29:26+00:00,,0,5,module: build triaged module: build warnings,True
24139,Allow incompatible shapes in load_state_dict(strict=False) module: serialization triaged enhancement,2019-08-10 00:45:35+00:00,,0,7,module: serialization triaged enhancement,True
24130,[RPC] Add type annotations for RPC-related Python files triaged better-engineering module: rpc,2019-08-09 21:40:15+00:00,,1,0,triaged better-engineering module: rpc,True
24102,[JIT] script doesn't convert dtypes back to torch.dtype from long oncall: jit triaged,2019-08-09 18:19:46+00:00,,0,0,oncall: jit triaged,True
24090,`suggest_memory_format` has ambiguity & cannot represent intended layout format for corner cases module: internals triaged,2019-08-09 17:04:14+00:00,,0,6,module: internals triaged,True
24089,torch.nn.functional.grid_sample with 'circular' border conditions module: nn triaged,2019-08-09 16:42:33+00:00,,0,3,module: nn triaged,True
24081,Multi-gpu example freeze and is not killable module: dependency bug module: multi-gpu module: multiprocessing module: cuda triaged module: deadlock has workaround module: data parallel quansight-nack,2019-08-09 13:27:51+00:00,,0,54,module: dependency bug module: multi-gpu module: multiprocessing module: cuda triaged module: deadlock has workaround module: data parallel quansight-nack,True
24079,[Caffe2] build android in v1.1.0 with headfile error caffe2 triaged,2019-08-09 07:02:03+00:00,,0,2,caffe2 triaged,True
24045,"torch.{save,load} data corruption when serializing a Module with __{get,set}state__ high priority module: serialization triaged quansight-nack",2019-08-08 22:09:49+00:00,,0,5,high priority module: serialization triaged quansight-nack,True
24041,tensor.var_mean variant for existing torch.var_mean (and same for std_mean) triaged function request module: reductions,2019-08-08 21:56:43+00:00,,0,1,triaged function request module: reductions,True
24037,Tests do not pass with the latest protobuf module: protobuf caffe2 triaged,2019-08-08 21:39:31+00:00,,0,2,module: protobuf caffe2 triaged,True
24031,fractional_max_pool2d_with_indices silently ignores output_ratio if output_size is provided module: error checking triaged module: pooling,2019-08-08 20:29:22+00:00,,0,1,module: error checking triaged module: pooling,True
24025,Refactor CircleCI config for version 2.1 module: ci triaged better-engineering,2019-08-08 18:19:51+00:00,,0,1,module: ci triaged better-engineering,True
24016,Multiplying a very large CUDA tensor with another tensor yields unexpected result module: dependency bug module: cuda triaged,2019-08-08 16:31:10+00:00,,0,9,module: dependency bug module: cuda triaged,True
24005,Using  `torch.utils.checkpoint.checkpoint_sequential` and `torch.autograd.grad` breaks when used in combination with `DistributedDataParallel` oncall: distributed module: checkpoint feature triaged,2019-08-08 06:27:44+00:00,,1,32,oncall: distributed module: checkpoint feature triaged,True
23966,Error out during compilation if USE_FBGEMM=1 is ignored module: build module: cpu module: ci triaged better-engineering,2019-08-07 19:55:22+00:00,,0,0,module: build module: cpu module: ci triaged better-engineering,True
23958,[jit] Python @property's not supported in TorchScript oncall: jit triaged quantization_release_1.3 jit-backlog,2019-08-07 18:25:32+00:00,,0,0,oncall: jit triaged quantization_release_1.3 jit-backlog,True
23946,We should run clang-tidy on all of master module: ci triaged better-engineering,2019-08-07 15:48:59+00:00,,0,4,module: ci triaged better-engineering,True
23940,Improve the performance of linear algebra operations in CUDA for small problem sizes module: performance module: cuda triaged module: linear algebra,2019-08-07 12:16:02+00:00,,0,0,module: performance module: cuda triaged module: linear algebra,True
23938,torch.nn.DataParallel causes incorrect gradients oncall: distributed module: autograd triaged,2019-08-07 11:22:51+00:00,,0,5,oncall: distributed module: autograd triaged,True
23896,Remove USE_C10D flag oncall: distributed module: build triaged,2019-08-06 20:48:04+00:00,,0,0,oncall: distributed module: build triaged,True
23890,[JIT] Can't use ndim in script oncall: jit triaged jit-backlog,2019-08-06 19:53:28+00:00,,0,4,oncall: jit triaged jit-backlog,True
23865,model->to(device) costs over a millisecond when doing nothing module: performance triaged,2019-08-06 11:09:38+00:00,,0,2,module: performance triaged,True
23859,CUDA: THTensor code complains about devices not matching when creating tensor from blob module: cpp triaged,2019-08-06 05:36:18+00:00,,0,2,module: cpp triaged,True
23854,Fan out calculation broken for group (depthwise) convolution module: convolution triaged module: initialization,2019-08-06 02:15:41+00:00,,1,5,module: convolution triaged module: initialization,True
23850,[quantization] jit::class_ for packed weights oncall: jit triaged quantization_release_1.3 jit-backlog,2019-08-06 01:45:46+00:00,,1,4,oncall: jit triaged quantization_release_1.3 jit-backlog,True
23849,Make MultiProcessTestCase pickable oncall: distributed module: tests triaged better-engineering,2019-08-06 01:21:48+00:00,,0,2,oncall: distributed module: tests triaged better-engineering,True
23787,Better version of chrome://tracing module: docs triaged small,2019-08-05 14:33:21+00:00,,0,0,module: docs triaged small,True
23780,Construction of MultivariateNormal much slower on GPU than CPU module: performance module: distributions module: cuda triaged,2019-08-05 07:32:03+00:00,,0,4,module: performance module: distributions module: cuda triaged,True
23771,SummaryWriter doesn't read comment if log_dir precised triaged module: tensorboard,2019-08-04 22:14:31+00:00,,0,1,triaged module: tensorboard,True
23768,Better documentation about PyTorch's dependencies module: build module: docs triaged module: third_party,2019-08-04 13:30:53+00:00,,0,1,module: build module: docs triaged module: third_party,True
23756,[feature request] Core API for invertible/inplace and flow-like ops + memory-saving (hookless?) reversible sequential container for RevNets to allow for much larger batch-sizes in academic setting high priority module: distributions feature module: nn triaged needs design,2019-08-03 14:02:58+00:00,,0,66,high priority module: distributions feature module: nn triaged needs design,True
23749,Wrong device in graph - Tensorboard SummaryWriter  triaged module: tensorboard,2019-08-02 21:31:06+00:00,,0,1,triaged module: tensorboard,True
23732,nn.Module.forward signature with **kwargs module: checkpoint module: nn triaged enhancement,2019-08-02 09:55:17+00:00,,0,5,module: checkpoint module: nn triaged enhancement,True
23730,Failed to build pytorch ... module: build triaged,2019-08-02 09:25:46+00:00,,0,5,module: build triaged,True
23721,Using PyTorch on AWS EFA network module: dependency bug oncall: distributed triaged,2019-08-02 04:48:55+00:00,,0,1,module: dependency bug oncall: distributed triaged,True
23720,segmentation faults when using multiprocessing_context='spawn' with large number of processes oncall: distributed module: multiprocessing module: dataloader triaged,2019-08-02 04:46:05+00:00,,0,2,oncall: distributed module: multiprocessing module: dataloader triaged,True
23676,"Bogus ""Your compiler (clang++) is not compatible"" message module: build triaged module: macos",2019-08-01 19:53:15+00:00,,0,0,module: build triaged module: macos,True
23657,Accelerate PyTorch just-in-time compilation using MKL-DNN oncall: jit feature triaged module: mkldnn,2019-08-01 16:41:56+00:00,,4,15,oncall: jit feature triaged module: mkldnn,True
23655,TorchScript GPU Fuser Doesn't Handle In-Place Operations module: performance oncall: jit triaged,2019-08-01 14:51:56+00:00,,0,8,module: performance oncall: jit triaged,True
23547,model use dilated conv backward in v1.1.0 is ~3x slower than in v0.4.1 on 1080Ti  module: performance module: cudnn module: cuda module: convolution triaged,2019-07-30 05:16:01+00:00,,0,5,module: performance module: cudnn module: cuda module: convolution triaged,True
23540,Error from PyTorch when finalizing Python embedded in C++ triaged module: pybind,2019-07-30 00:32:43+00:00,,0,1,triaged module: pybind,True
23525,MultiheadAttention output changes if input order is not exactly same needs reproduction module: nn triaged,2019-07-29 21:05:16+00:00,,0,2,needs reproduction module: nn triaged,True
23512,Build reconfiguration should consistently honor env variables module: build triaged,2019-07-29 18:17:01+00:00,,0,2,module: build triaged,True
23509,Enable PyTorch Bfloat16 for CPU and add MKL-DNN bfloat16 optimization for Cooper Lake module: performance module: cpu triaged,2019-07-29 17:03:59+00:00,,5,6,module: performance module: cpu triaged,True
23490,upcoming PEP 554: how much effort we need to support sub-interpreter feature triaged,2019-07-28 16:19:53+00:00,,0,4,feature triaged,False
23482,Build error due to unintended include path /usr/include module: build triaged,2019-07-27 20:50:21+00:00,,0,43,module: build triaged,True
23434,einsum equation with conditional mask works in numpy but not in PyTorch triaged module: type promotion module: linear algebra function request,2019-07-26 17:14:38+00:00,,0,5,triaged module: type promotion module: linear algebra function request,True
23430,[Feature request] Let DistributedSampler take a Sampler as input oncall: distributed feature module: dataloader triaged has workaround,2019-07-26 14:01:32+00:00,,0,21,oncall: distributed feature module: dataloader triaged has workaround,True
23429,Unreachable code in tanh caffe2 triaged,2019-07-26 12:35:34+00:00,,0,0,caffe2 triaged,True
23425,Hanging on when one gpu node return zero as loss in the context of distributed data parallel training oncall: distributed triaged,2019-07-26 10:09:21+00:00,,0,11,oncall: distributed triaged,True
23382,Tensor from mmaped storage loads the entire file into memory module: memory usage triaged,2019-07-25 17:24:04+00:00,,0,0,module: memory usage triaged,True
23364,dyndep function not working  caffe2,2019-07-25 03:31:47+00:00,,0,0,caffe2,False
23355,[jit] add named tuple as output type to the tracer oncall: jit triaged,2019-07-25 00:46:28+00:00,,0,0,oncall: jit triaged,True
23342,QNNpack tests should be skipped on ppc64le (not enabled there) module: tests triaged better-engineering,2019-07-24 21:36:41+00:00,,0,0,module: tests triaged better-engineering,True
23305,make torch.utils._download_url_from_file public and add a docstring module: docs triaged module: hub module: ux,2019-07-24 15:30:37+00:00,,0,0,module: docs triaged module: hub module: ux,True
23301,Error while using Libtorch + OpenCV + Qt Creator module: cpp triaged,2019-07-24 13:51:08+00:00,,0,0,module: cpp triaged,True
23299,Add gatherv/allgatherv primitives to support non-equal contribution oncall: distributed module: bootcamp feature triaged,2019-07-24 09:30:26+00:00,,0,0,oncall: distributed module: bootcamp feature triaged,True
23240,UBSAN failure in test_simple_model (__main__.TestTensorBoardNumpy): runtime error: call to function pybind11::class_<caffe2::GradientWrapper>::dealloc(pybind11::detail::value_and_holder&) through pointer to incorrect function type 'void (*)(pybind11::detail::value_and_holder &)' module: tests triaged,2019-07-23 17:49:34+00:00,,0,0,module: tests triaged,True
23233,"Unofficial ARMv6, ARMv7, ARMv8 builds module: build module: docs triaged",2019-07-23 16:35:47+00:00,,0,21,module: build module: docs triaged,True
23230,Consolidate multiprocessing helpers in distributed tests oncall: distributed triaged,2019-07-23 15:05:54+00:00,,0,0,oncall: distributed triaged,True
23217,Libtorch with deeplabv3_resnet101 will not forward. module: crash module: cpp triaged,2019-07-23 06:32:45+00:00,,0,14,module: crash module: cpp triaged,True
23159,"Can't `torch.sum(tensor, dim)` where `dim >= 64` module: error checking triaged module: TensorIterator module: reductions",2019-07-22 13:31:52+00:00,,1,1,module: error checking triaged module: TensorIterator module: reductions,True
23151,Deterministic mode for scatter_add operation triaged module: determinism function request module: scatter & gather ops,2019-07-22 04:51:45+00:00,,1,1,triaged module: determinism function request module: scatter & gather ops,True
23110,[RFC] RPC Based Distributed Model Parallel feature triaged module: rpc,2019-07-19 21:33:47+00:00,,0,21,feature triaged module: rpc,True
23103,Test utility for non-contiguous tensors module: tests triaged enhancement,2019-07-19 20:02:54+00:00,,0,0,module: tests triaged enhancement,True
23094,Versioning for libtorch nightlies module: binaries module: build triaged better-engineering,2019-07-19 18:46:46+00:00,,0,3,module: binaries module: build triaged better-engineering,True
23079,update docs that sorting is not needed in  module: docs module: rnn triaged,2019-07-19 15:25:25+00:00,,0,1,module: docs module: rnn triaged,False
23072,Inconsistent axis argument names in torch.diagonal and torch.transpose module: docs low priority triaged,2019-07-19 13:36:34+00:00,,0,4,module: docs low priority triaged,True
23068,[c++] torch::conv2d() expected output_padding to be a single integer value or a list of 3 values  module: docs module: cpp module: nn low priority module: convolution triaged,2019-07-19 10:15:05+00:00,,0,0,module: docs module: cpp module: nn low priority module: convolution triaged,True
23067,creation of a tensor from a numba.cuda array feature low priority triaged module: numba,2019-07-19 10:10:24+00:00,,0,4,feature low priority triaged module: numba,True
23061,The speed of `torch.einsum` and `torch.matmul` when using `fp16` is slow module: performance module: cuda triaged module: linear algebra,2019-07-19 06:25:28+00:00,,0,12,module: performance module: cuda triaged module: linear algebra,True
23054,ConcatDataset returns different error messages setting out of range plus index and minus index. module: docs low priority triaged,2019-07-19 02:57:09+00:00,,0,0,module: docs low priority triaged,True
23048,Add automatic tuning flags to utils.data.dataloader feature module: dataloader low priority triaged,2019-07-18 23:21:48+00:00,,0,3,feature module: dataloader low priority triaged,True
23032,Proposal: Optional AutogradMeta for Variable module: autograd triaged,2019-07-18 19:26:09+00:00,,0,1,module: autograd triaged,True
23026,BatchNorm1d fails on first run through GPU module: nn module: cuda triaged,2019-07-18 17:20:19+00:00,,0,0,module: nn module: cuda triaged,True
22961,performance much worse on 2080ti than 1080ti module: performance module: cuda triaged,2019-07-17 08:00:22+00:00,,0,37,module: performance module: cuda triaged,True
22954,Support serializing IValue to bytes (and deserialize from bytes) oncall: jit triaged,2019-07-17 02:05:41+00:00,,0,10,oncall: jit triaged,True
22924,[data loader] Graceful data loader threads exit on KeyboardInterrupt needs reproduction module: dataloader triaged,2019-07-16 20:14:52+00:00,,0,10,needs reproduction module: dataloader triaged,True
22919,Unify tensor shape formatting in shape checks module: error checking module: convolution triaged enhancement,2019-07-16 19:32:40+00:00,,0,4,module: error checking module: convolution triaged enhancement,True
22911,JIT trace parameter sharing error if Module attributes happen to be the same oncall: jit triaged,2019-07-16 16:21:37+00:00,,0,6,oncall: jit triaged,True
22879,Difference between dropout2d and dropout3d module: nn triaged,2019-07-15 20:28:54+00:00,,0,2,module: nn triaged,True
22868,Make it easier to bisect on PyTorch module: build module: ci triaged better-engineering,2019-07-15 17:30:51+00:00,,0,2,module: build module: ci triaged better-engineering,True
22859,CRITICAL:root:Cannot load caffe2.python. Error: DLL load failed: The specified module could not be found. caffe2,2019-07-15 09:46:17+00:00,,0,13,caffe2,True
22848,[feature request] Log-determinant for symmetric positive definite matrices triaged has workaround module: linear algebra function request,2019-07-14 15:10:11+00:00,,0,17,triaged has workaround module: linear algebra function request,False
22815,TensorImpl de-virtualization high priority module: dependency bug module: internals triaged quansight-nack,2019-07-12 19:12:40+00:00,,0,11,high priority module: dependency bug module: internals triaged quansight-nack,True
22803,Port `fmod` operator from the TH code to Aten module: cuda module: cpu triaged module: porting better-engineering,2019-07-12 15:55:55+00:00,,0,2,module: cuda module: cpu triaged module: porting better-engineering,True
22791,How to use mpi backend without CUDA_aware triaged module: mpi,2019-07-12 08:09:15+00:00,,0,2,triaged module: mpi,True
22788,Pytorch deadlock from distributed multiprocessing oncall: distributed triaged,2019-07-12 05:54:23+00:00,,0,3,oncall: distributed triaged,True
22785,Getting cuda runtime error (48) with Jetson TX2 when running simple program module: cuda triaged,2019-07-12 02:12:06+00:00,,0,2,module: cuda triaged,True
22778,In-place updating the original value tensor should also update version counter of sparse tensor's values_ tensor module: sparse module: autograd triaged,2019-07-11 22:57:03+00:00,,0,2,module: sparse module: autograd triaged,True
22766,assert_no_internal_overlap should pass const char* module: internals triaged,2019-07-11 21:10:19+00:00,,0,0,module: internals triaged,True
22755,[RFC] InstanceNorm default affine value module: docs triaged,2019-07-11 18:41:37+00:00,,0,2,module: docs triaged,True
22707,torch.fill_() exists and modifies the input tensor: Expected or bug? module: bc-breaking triaged module: deprecation module: ux,2019-07-10 21:59:18+00:00,,0,9,module: bc-breaking triaged module: deprecation module: ux,True
22687,DispatchStub should report what operator it failed to find kernel for module: internals module: cpu triaged,2019-07-10 15:56:18+00:00,,0,0,module: internals module: cpu triaged,True
22682,CMAKE_PARSE_IMPLICIT_LINK_INFO Function invoked with incorrect arguments module: build triaged module: flaky-tests has workaround,2019-07-10 13:50:43+00:00,,0,1,module: build triaged module: flaky-tests has workaround,True
22671,The training always freezes after some epochs. needs reproduction module: cuda triaged module: deadlock,2019-07-10 04:00:14+00:00,,0,18,needs reproduction module: cuda triaged module: deadlock,True
22669,BFloat16 numeric limits should contain more info triaged enhancement module: bfloat16,2019-07-10 00:56:41+00:00,,1,0,triaged enhancement module: bfloat16,True
22609,caffe_translator TranslateCrop fails when more than one dimensions is cropped caffe2,2019-07-09 04:42:11+00:00,,0,0,caffe2,False
22604,"""CrossEntropyLoss"" should mention in its name that it takes softmax for target module: nn module: loss triaged",2019-07-09 00:27:27+00:00,,0,1,module: nn module: loss triaged,True
22597,Autograd profiler memory leak when use_cuda=True needs reproduction module: autograd triaged,2019-07-08 21:04:34+00:00,,0,4,needs reproduction module: autograd triaged,True
22585,[dataloader] Mysterious error when using spawn start_method  module: dataloader triaged,2019-07-08 12:26:42+00:00,,0,4,module: dataloader triaged,True
22580,Pytorch compilation error on Mac OS module: build triaged,2019-07-08 02:02:07+00:00,,0,4,module: build triaged,True
22578,torch.gels runs 100 time slower on gpu than on cpu module: performance triaged,2019-07-07 21:31:39+00:00,,0,2,module: performance triaged,True
22577,Double backward 3 times slower for conv2d with padding = 1 module: dependency bug module: performance module: double backwards module: nn module: convolution triaged has workaround quansight-nack,2019-07-07 19:56:35+00:00,,0,14,module: dependency bug module: performance module: double backwards module: nn module: convolution triaged has workaround quansight-nack,True
22573,Batched symeig and qr are very slow on GPU module: performance module: cuda triaged module: linear algebra,2019-07-07 13:18:26+00:00,,0,15,module: performance module: cuda triaged module: linear algebra,True
22557,CPU torch.exponential_ function may generate 0 which can cause downstream NaN triaged module: random,2019-07-05 20:07:25+00:00,,0,0,triaged module: random,True
22550,Eigen Tensor library for convolutions on CPU module: performance module: cpu module: convolution triaged module: arm function request,2019-07-05 15:14:30+00:00,,0,2,module: performance module: cpu module: convolution triaged module: arm function request,True
22541,make[2]: *** No rule to make target 'libtorch/lib/libc10.so' module: build module: cpp triaged,2019-07-04 17:51:38+00:00,,0,5,module: build module: cpp triaged,True
22536,libtorch new op module: docs module: cpp triaged module: custom-operators,2019-07-04 10:01:10+00:00,,0,4,module: docs module: cpp triaged module: custom-operators,True
22532,Know which function is used by conv and force to use a function module: nn module: convolution triaged,2019-07-04 08:40:46+00:00,,0,8,module: nn module: convolution triaged,True
22513,No assertion when using scatter_ on a non-contiguous tensor module: error checking triaged module: partial aliasing module: scatter & gather ops,2019-07-03 22:11:41+00:00,,0,5,module: error checking triaged module: partial aliasing module: scatter & gather ops,True
22495,`binary_linux_libtorch_2.7m_cu100_devtoolset3_build` times out after running for 5 hours module: build triaged better-engineering,2019-07-03 18:03:32+00:00,,0,1,module: build triaged better-engineering,True
22494,Error in equation module: docs triaged,2019-07-03 17:58:52+00:00,,0,1,module: docs triaged,True
22487,"""Floating point exception"" after trying the method from the issue #22382 oncall: jit triaged",2019-07-03 09:56:06+00:00,,0,2,oncall: jit triaged,True
22455,"Move csrc/distributed/c10d/{comm,reducer} to libtorch.so oncall: distributed triaged",2019-07-02 17:39:28+00:00,,0,0,oncall: distributed triaged,True
22454,Tracing an RNN does not support torch.nn.utils.rnn.PackedSequence as input oncall: jit triaged,2019-07-02 16:31:00+00:00,,0,1,oncall: jit triaged,True
22447,Weak Symbols Resolution Causes Segmentation Fault in External Libraries triaged,2019-07-02 12:16:09+00:00,,0,3,triaged,True
22415,Add support for serializing Mkldnn Tensor module: serialization triaged module: mkldnn,2019-07-01 22:01:07+00:00,,0,2,module: serialization triaged module: mkldnn,True
22414,CPU random number generator is slow module: performance triaged module: random,2019-07-01 21:43:34+00:00,,0,7,module: performance triaged module: random,True
22406,Build failure with setup.py module: build triaged,2019-07-01 20:40:50+00:00,,0,4,module: build triaged,True
22402,PyTorch Tensor subclasses and protocols for NumPy interoperability high priority feature triaged module: numpy,2019-07-01 19:29:23+00:00,,1,26,high priority feature triaged module: numpy,True
22400,Sparse allreduce for ProcessGroupNCCL oncall: distributed feature triaged,2019-07-01 17:49:35+00:00,,1,5,oncall: distributed feature triaged,False
22383,Storage operation failing on second GPU module: cuda triaged,2019-06-30 22:31:26+00:00,,0,0,module: cuda triaged,True
22378,scatter_ supporting different reduction modes high priority module: sparse module: internals triaged enhancement module: scatter & gather ops,2019-06-30 06:14:30+00:00,,0,37,high priority module: sparse module: internals triaged enhancement module: scatter & gather ops,True
22375,Label.dim Enforcement Check in AccuracyOp caffe2 triaged,2019-06-29 22:15:24+00:00,,0,0,caffe2 triaged,True
22370,returned non-zero exit status 2. module: build triaged,2019-06-29 11:53:12+00:00,,0,0,module: build triaged,True
22358,"When I run python setup.py install to install Caffe2, I have an error: ""No such file or directory: 'nvcc': 'nvcc'"" caffe2 triaged",2019-06-28 22:59:49+00:00,,0,0,caffe2 triaged,True
22343,New Weight Scheduler Concept for Weight Decay feature module: optimizer triaged,2019-06-28 16:36:20+00:00,,0,6,feature module: optimizer triaged,False
22340,Reducer bucketing based on autograd profile  oncall: distributed feature triaged,2019-06-28 14:03:57+00:00,,0,0,oncall: distributed feature triaged,False
22338,Illegal instruction (core dumped) when running in qemu high priority module: crash module: cpu triaged module: vectorization,2019-06-28 13:53:30+00:00,,0,19,high priority module: crash module: cpu triaged module: vectorization,True
22331,Integer division by Zero giving large number results instead of NaN/inf on Windows module: cpu triaged,2019-06-28 04:10:49+00:00,,0,5,module: cpu triaged,True
22330,Training CNNs with deconvolution module: convolution triaged module: vision function request,2019-06-28 03:32:49+00:00,,0,5,module: convolution triaged module: vision function request,True
22281,Box constraints for optimizers feature triaged,2019-06-26 21:48:53+00:00,,0,4,feature triaged,False
22277,Handle all IntArrayRef expansions in ATen module: nn triaged enhancement,2019-06-26 20:26:08+00:00,,0,2,module: nn triaged enhancement,True
22274,second derivatives of unfold triaged enhancement module: derivatives,2019-06-26 19:57:52+00:00,,0,1,triaged enhancement module: derivatives,True
22259,using multi thread lead to gpu stuck with GPU-util 100% high priority needs reproduction module: cuda triaged quansight-nack,2019-06-26 11:18:51+00:00,,0,36,high priority needs reproduction module: cuda triaged quansight-nack,True
22252,[libtorch] header warning suppression triaged module: build warnings,2019-06-26 06:15:06+00:00,,0,0,triaged module: build warnings,True
22232,[FYI] Introducing Quantized Tensor module: docs triaged,2019-06-25 21:28:10+00:00,,0,0,module: docs triaged,True
22220,[doc] nn.Module.forward documentation unclear module: docs module: nn triaged enhancement,2019-06-25 19:18:34+00:00,,0,6,module: docs module: nn triaged enhancement,False
22169,[RFC] NestedTensor - 0.0.1 triaged module: batching,2019-06-24 23:48:43+00:00,,0,44,triaged module: batching,True
22155,[jit] Optional type refinement on non-named expressions oncall: jit triaged TSRootCause:TypeRefinement TSUsability,2019-06-24 19:13:35+00:00,,0,3,oncall: jit triaged TSRootCause:TypeRefinement TSUsability,True
22136,[dataloader] SIGCHLD handler should poll the queue for exception first module: dataloader triaged,2019-06-24 16:13:22+00:00,,0,1,module: dataloader triaged,True
22128,Automatic rank selection when using file:// initialization method oncall: distributed triaged enhancement,2019-06-24 09:26:01+00:00,,1,0,oncall: distributed triaged enhancement,True
22122,ASSERT FAILED at /pytorch/torch/csrc/autograd/generated/VariableType_0.cpp:12721 triaged module: assert failure,2019-06-23 19:37:35+00:00,,1,3,triaged module: assert failure,True
22083,Pytorch is slower on windows than on linux module: windows feature triaged,2019-06-21 19:28:51+00:00,,0,9,module: windows feature triaged,False
22070,[ONNX] BUG for Upsample operator export re-used by caffe2 caffe2,2019-06-21 11:31:09+00:00,,0,1,caffe2,True
22049,Cannot update part of the parameters in DistributedDataParallel. oncall: distributed triaged,2019-06-20 23:06:50+00:00,,0,14,oncall: distributed triaged,True
22040,SigAbort while running the Caffe2 unit test - thread_init_test - built on Clang7 +glibc 2.23 caffe2,2019-06-20 21:05:37+00:00,,0,1,caffe2,False
22035,Have a different way to check if gradient was computed in the optimizer (not checking for None) feature module: autograd module: optimizer triaged,2019-06-20 19:33:24+00:00,,0,1,feature module: autograd module: optimizer triaged,True
22024,Consolidate definition of operators/gradients where possible module: internals triaged better-engineering,2019-06-20 17:01:53+00:00,,0,1,module: internals triaged better-engineering,True
22022,nn.modules.functional.h does not support optional arguments module: cpp module: nn triaged,2019-06-20 14:52:37+00:00,,0,2,module: cpp module: nn triaged,True
22013,Mysterious Tensor Indexing Problem high priority module: error checking triaged module: numpy module: advanced indexing module: ux,2019-06-20 06:13:38+00:00,,0,8,high priority module: error checking triaged module: numpy module: advanced indexing module: ux,True
21956,nn.init.orthogonal_ doesn't work with multiprocessing module: dependency bug module: multiprocessing module: nn triaged module: initialization,2019-06-19 05:55:38+00:00,,0,18,module: dependency bug module: multiprocessing module: nn triaged module: initialization,True
21902,contradictory output values module: cpp triaged,2019-06-18 10:18:28+00:00,,0,2,module: cpp triaged,True
21899,Crash when using tensor.set_data() function in libtorch on windows module: cpp triaged,2019-06-18 07:24:52+00:00,,0,2,module: cpp triaged,True
21876,nn.TransformerLayer feature module: nn triaged oncall: transformer/mha,2019-06-17 22:02:34+00:00,,0,5,feature module: nn triaged oncall: transformer/mha,True
21856,Batch Normalization axis triaged module: batching function request module: norms and normalization,2019-06-17 12:44:14+00:00,,0,5,triaged module: batching function request module: norms and normalization,True
21847,Logging mode for saying when tensor broadcast occurs module: internals feature module: molly-guard triaged,2019-06-16 18:36:06+00:00,,0,2,module: internals feature module: molly-guard triaged,True
21843,[Caffe2/ONNX] ONNX LSTM Loading caffe2 triaged,2019-06-16 09:44:14+00:00,,0,0,caffe2 triaged,False
21828,torch::zeros is slow for small tensors (C++) module: performance module: cpp triaged,2019-06-15 18:44:56+00:00,,1,11,module: performance module: cpp triaged,True
21824,How about add torch::end for slicing in c++ frontend module: internals feature triaged,2019-06-15 16:12:06+00:00,,0,0,module: internals feature triaged,True
21818,C++ ABI - Coupling different libraries issue module: build triaged,2019-06-15 06:49:07+00:00,,0,5,module: build triaged,True
21794,Linker errors when building project with OpenCV module: build triaged,2019-06-14 18:05:56+00:00,,0,3,module: build triaged,True
21780,cumsum cuda numerical instability module: numerical-stability module: cuda triaged,2019-06-14 14:58:28+00:00,,0,2,module: numerical-stability module: cuda triaged,True
21779,"Use of word ""elements"" in `torch.utils.data` samplers module: docs triaged",2019-06-14 13:51:54+00:00,,0,3,module: docs triaged,True
21777,typo and missing return statements module: build triaged module: third_party,2019-06-14 09:53:48+00:00,,0,9,module: build triaged module: third_party,True
21766,Missing header files for prebuilt libtorch caffe2,2019-06-14 00:46:55+00:00,,0,2,caffe2,False
21760,[FR] faster reduce sum on expanded/unfolded tensors module: performance triaged module: linear algebra function request,2019-06-13 22:59:08+00:00,,0,7,module: performance triaged module: linear algebra function request,True
21745,torch.save also saves docstrings into pickle for some reason module: serialization triaged,2019-06-13 17:41:57+00:00,,0,14,module: serialization triaged,True
21738,symbol lookup error: libmkl_intel_lp64.so: undefined  symbol: mkl_blas_dsyrk (binaries built with static linking -DBUILD_SHARED_LIBS=OFF fail due to dynamic linker problem) module: build triaged module: static linking module: third_party has workaround,2019-06-13 16:00:15+00:00,,0,3,module: build triaged module: static linking module: third_party has workaround,True
21731,Improve multithreaded random number generation (RNG) module: cpu triaged module: random module: multithreading,2019-06-13 13:17:02+00:00,,0,2,module: cpu triaged module: random module: multithreading,True
21700,(LLD 8.0.0) ld: error: can't create dynamic relocation R_X86_64_DTPOFF32 against symbol: ideep::utils::computation_cache module: build triaged module: static linking module: mkldnn,2019-06-12 20:35:23+00:00,,0,4,module: build triaged module: static linking module: mkldnn,True
21688,Batched Conv2d for sequence data module: convolution triaged module: batching function request,2019-06-12 16:28:50+00:00,,0,3,module: convolution triaged module: batching function request,True
21684,The cuda problem in caffe2 caffe2,2019-06-12 14:41:10+00:00,,0,1,caffe2,True
21682,C++ module API footgun: assigning to parameter doesn't update `parameters()` list module: docs triaged,2019-06-12 13:42:10+00:00,,0,3,module: docs triaged,True
21673,[FR] Diagonal Transform for Distributions module: distributions feature triaged,2019-06-12 06:18:01+00:00,,0,3,module: distributions feature triaged,True
21645,Batch Dataloader and Dataset feature module: dataloader triaged,2019-06-11 18:35:25+00:00,,0,12,feature module: dataloader triaged,True
21602,Strange latency overhead of F.conv2d module: performance module: cpu triaged,2019-06-10 20:16:05+00:00,,0,2,module: performance module: cpu triaged,True
21587,RuntimeError: cublas runtime error  triaged module: cublas,2019-06-10 13:43:19+00:00,,0,2,triaged module: cublas,True
21567,torch.bernoulli's parameter generator not documented module: docs triaged module: random,2019-06-08 17:14:06+00:00,,0,4,module: docs triaged module: random,True
21554,Pytorch hangs when dataloader multiprocessing workers are killed module: dataloader triaged,2019-06-08 00:49:55+00:00,,0,2,module: dataloader triaged,True
21553,Feature Request: beta cdf module: distributions feature triaged,2019-06-08 00:37:05+00:00,,0,2,module: distributions feature triaged,True
21544,"[jit] In pickler, don't memoize if not necessary oncall: jit triaged jit-backlog",2019-06-07 21:43:51+00:00,,0,0,oncall: jit triaged jit-backlog,True
21518,`attn_mask` in nn.MultiheadAttention is additive module: docs module: nn triaged oncall: transformer/mha,2019-06-07 13:35:35+00:00,,1,8,module: docs module: nn triaged oncall: transformer/mha,True
21477,Not obvious how to install torchvision with PyTorch source build triaged module: vision,2019-06-06 18:04:12+00:00,,0,8,triaged module: vision,True
21467,No continuous integration coverage for Python 2 CUDA module: ci triaged,2019-06-06 14:36:31+00:00,,0,1,module: ci triaged,True
21462,"Slow convolution with large kernels, should be using FFT module: performance module: cudnn module: convolution triaged module: fft",2019-06-06 13:57:02+00:00,,0,12,module: performance module: cudnn module: convolution triaged module: fft,True
21459,RuntimeError: cublas runtime error  triaged module: cublas,2019-06-06 12:18:45+00:00,,0,3,triaged module: cublas,True
21457,downsampling with grid_sample doesn't match interpolate triaged module: vision module: interpolation,2019-06-06 11:46:04+00:00,,0,5,triaged module: vision module: interpolation,True
21454,[JIT] Memory Leak during tracing? oncall: jit triaged,2019-06-06 09:57:57+00:00,,1,3,oncall: jit triaged,True
21442,[JIT] kwarg with default doesn't work for class instantiation oncall: jit triaged jit-backlog,2019-06-06 00:24:50+00:00,,0,0,oncall: jit triaged jit-backlog,True
21411,Implementation of Group equivariant convolutions feature module: nn low priority triaged,2019-06-05 13:12:52+00:00,,0,3,feature module: nn low priority triaged,True
21405,how libtorch can work with  tensor data as same as  pytorch module: docs module: cpp low priority triaged,2019-06-05 09:49:17+00:00,,0,0,module: docs module: cpp low priority triaged,True
21399,collect_env ignores conda environment module: build module: docs low priority module: collect_env.py triaged,2019-06-05 04:53:56+00:00,,0,1,module: build module: docs low priority module: collect_env.py triaged,True
21318,"Undefined symbols for architecture x86_64: ""testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith(void const*)"" on Mac OS X module: build module: internals low priority triaged module: macos",2019-06-04 00:07:32+00:00,,0,4,module: build module: internals low priority triaged module: macos,True
21290,"Title of docs page includes ""PyTorch master documentation"" even for non-master branches. module: docs triaged",2019-06-03 16:59:19+00:00,,0,1,module: docs triaged,False
21277,[caffe2] check Range Operator inputs with bug caffe2,2019-06-03 10:15:27+00:00,,0,1,caffe2,True
21275,weight_norm is not supported in TorchScript oncall: jit triaged jit-backlog,2019-06-03 07:23:06+00:00,,0,2,oncall: jit triaged jit-backlog,True
21255,IsType<T>() ASSERT FAILED [Detectron e2e_mask_rcnn_R-50-C4_1x.yaml] caffe2 module: assert failure,2019-06-01 20:46:09+00:00,,0,1,caffe2 module: assert failure,True
21179,Failed to install pytorch from source on ubuntu. needs reproduction module: build triaged,2019-05-30 23:20:42+00:00,,0,4,needs reproduction module: build triaged,True
21176,Adding a method called `T` in native_functions causes undefined behavior on Windows module: windows triaged,2019-05-30 22:28:47+00:00,,0,7,module: windows triaged,True
21165,Getting Access to Blob/Tensor reference in jit::script::Module oncall: jit triaged,2019-05-30 20:16:21+00:00,,0,2,oncall: jit triaged,True
21144,Data Parallel Implementation Improvements oncall: distributed triaged,2019-05-30 15:09:02+00:00,,1,1,oncall: distributed triaged,True
21135,[cmake build] can't build pytorch with install mkl library  module: build triaged,2019-05-30 07:32:31+00:00,,0,0,module: build triaged,True
21120,[RFC] Adding MKL-DNN Int8 functions to PyTorch/Aten/JIT backend oncall: jit oncall: quantization triaged module: mkldnn,2019-05-30 05:08:47+00:00,,1,5,oncall: jit oncall: quantization triaged module: mkldnn,True
21064,[JIT] Expose subgraph execution for intermediate output extraction oncall: jit enhancement TSUsability TSRootCause:PoorIRVisibility,2019-05-29 15:04:15+00:00,,0,9,oncall: jit enhancement TSUsability TSRootCause:PoorIRVisibility,True
21061,Misleading Error when doing Large Batch Matrix Multiplication module: cuda triaged,2019-05-29 12:11:46+00:00,,0,5,module: cuda triaged,True
21050,output values not same and much slower than Python API module: cpp triaged,2019-05-29 01:49:11+00:00,,0,0,module: cpp triaged,True
21028,Zero-dim Tensors (scalars) should be printed at full precision module: printing triaged,2019-05-28 18:42:31+00:00,,0,4,module: printing triaged,True
21018,free(): invalid pointer Aborted (core dumped) needs reproduction triaged,2019-05-28 14:18:38+00:00,,0,7,needs reproduction triaged,True
21017,"Stop using ""AAA"" prefix for builds module: ci triaged",2019-05-28 13:25:30+00:00,,0,3,module: ci triaged,True
21016,[utils.bottleneck] throws initialization error for cuda profiling module: dataloader triaged,2019-05-28 13:00:58+00:00,,0,8,module: dataloader triaged,True
21015,How to use Infiniband for cpu-cluster with backend gloo? oncall: distributed triaged,2019-05-28 11:41:53+00:00,,0,7,oncall: distributed triaged,True
21012,"Error when creating new caffe2::Predictor(_initNet, _predictNet) caffe2",2019-05-28 08:20:30+00:00,,0,0,caffe2,True
20997,ReduceLROnPlateau will fail when add new parameter group to the optimizer module: optimizer triaged,2019-05-28 01:33:31+00:00,,0,2,module: optimizer triaged,False
20975,"build caffe2 operators failed, 'sorry, unimplemented: non-trivial designated initializers not supported' caffe2",2019-05-27 03:07:39+00:00,,0,0,caffe2,False
20967,Track running stats regardless of track_running_stats=False module: nn triaged,2019-05-26 14:59:54+00:00,,0,10,module: nn triaged,True
20963,official libtorch static build zip file error module: binaries module: build triaged,2019-05-26 06:36:35+00:00,,0,2,module: binaries module: build triaged,True
20935,torch.full_like missing documentation for out input variable module: docs triaged,2019-05-24 21:14:34+00:00,,0,1,module: docs triaged,False
20864,Import warning when using the wrong version of CUDA module: cuda triaged,2019-05-23 16:05:30+00:00,,0,3,module: cuda triaged,True
20858,convert Onnx Slice operator to caffe2 failed caffe2 triaged,2019-05-23 09:11:49+00:00,,0,1,caffe2 triaged,True
20822,"[Proposal] Data reading framework for PyTorch (Hive, MySQL, S3 etc.)  feature module: dataloader triaged needs research",2019-05-22 19:36:25+00:00,,1,45,feature module: dataloader triaged needs research,True
20805,LibTorch :About torch.jit.trace generate model.pt needs reproduction oncall: jit triaged,2019-05-22 11:11:10+00:00,,0,2,needs reproduction oncall: jit triaged,True
20777,"Caffe2: operator_test/instance_norm_test.py::TestInstanceNorm::test_instance_norm_gradients test case fails with ""RuntimeError: dim() called on undefined TensorError from operator"" caffe2",2019-05-21 21:44:07+00:00,,0,0,caffe2,False
20776,Caffe2: executor_test.py::ExecutorGPUResNetTest fails with an AssertionError caffe2,2019-05-21 21:32:10+00:00,,0,0,caffe2,False
20769,[JIT] Better Python String Support oncall: jit jit-backlog,2019-05-21 18:36:17+00:00,,0,2,oncall: jit jit-backlog,True
20753,cmake for Torch unusable in archlinux module: dependency bug module: build triaged,2019-05-21 12:49:29+00:00,,0,2,module: dependency bug module: build triaged,True
20748,Segmentation fault when use torch::from_blob module: crash module: cpp module: abi triaged,2019-05-21 08:09:28+00:00,,0,1,module: crash module: cpp module: abi triaged,True
20734,[jit] set up views for Autodiff and autograd hooks  oncall: jit triaged,2019-05-20 23:08:58+00:00,,0,0,oncall: jit triaged,True
20704,Remove unpack() in torch/csrc/nn/type_checks.h and its caller functions in the codebase module: internals module: nn good first issue triaged,2019-05-20 16:34:38+00:00,,0,7,module: internals module: nn good first issue triaged,True
20686,In-source build causes repeating filename annotations (Windows doesn't support out-of-source build) module: build module: windows triaged,2019-05-19 03:07:09+00:00,,1,23,module: build module: windows triaged,True
20682,[distribution] Support for various domain for AffineTransform module: distributions triaged,2019-05-18 14:49:12+00:00,,0,4,module: distributions triaged,True
20664,[JIT] List python builtin has wrong casting behavior oncall: jit triaged jit-backlog,2019-05-17 22:43:32+00:00,,0,2,oncall: jit triaged jit-backlog,True
20655,nn.Embedding backwards slow under high row contention module: performance module: nn module: cuda triaged,2019-05-17 19:13:30+00:00,,0,3,module: performance module: nn module: cuda triaged,True
20641,Main page broadcasting (?) example image bug module: docs triaged,2019-05-17 11:57:15+00:00,,0,3,module: docs triaged,True
20629,profiler seems not print all op calls triaged oncall: profiler,2019-05-17 06:45:13+00:00,,0,0,triaged oncall: profiler,True
20613,groupby function feature triaged,2019-05-16 22:34:16+00:00,,0,1,feature triaged,False
20591,@ignore annotation for user defined type oncall: jit triaged jit-backlog,2019-05-16 16:55:45+00:00,,0,2,oncall: jit triaged jit-backlog,True
20576,NVRTC_ERROR unknown when using self-built libtorch module: build triaged,2019-05-16 06:57:47+00:00,,0,1,module: build triaged,True
20562,Performance issue when accessing an extremely large (10GB) longtensor module: cudnn module: cuda triaged,2019-05-15 22:54:57+00:00,,1,3,module: cudnn module: cuda triaged,True
20551,torch.norm produces incorrect results triaged module: numerical-reproducibility module: norms and normalization,2019-05-15 21:28:51+00:00,,0,8,triaged module: numerical-reproducibility module: norms and normalization,True
20531,c++ torch::nn::Sequential increments count on name errors module: cpp triaged,2019-05-15 14:43:41+00:00,,0,1,module: cpp triaged,True
20497,Statically make `__setstate__` set all attributes/parameters  high priority module: serialization triaged,2019-05-14 19:07:32+00:00,,0,4,high priority module: serialization triaged,True
20488,RoiAlignTest.CheckCPUGPUEqual is still flaky caffe2 triaged module: flaky-tests better-engineering,2019-05-14 15:05:25+00:00,,0,1,caffe2 triaged module: flaky-tests better-engineering,True
20481,[docs] Automatically detect docs missing in rst module: docs module: tests triaged enhancement,2019-05-14 08:50:32+00:00,,0,4,module: docs module: tests triaged enhancement,False
20466,Lint rule to prevent direct use of #pragma omp module: lint triaged,2019-05-14 00:52:28+00:00,,0,2,module: lint triaged,True
20433,Dataloader's memory usage keeps increasing during one single epoch. module: dataloader triaged,2019-05-13 15:09:37+00:00,,1,5,module: dataloader triaged,True
20424,pos_weight argument in torch.nn.BCELoss module: nn triaged enhancement,2019-05-13 12:55:38+00:00,,0,2,module: nn triaged enhancement,True
20416,Class based Sampler for Class Incremental/Continual Learning research feature module: dataloader triaged,2019-05-13 00:43:02+00:00,,0,7,feature module: dataloader triaged,True
20412,String in tensor module: internals triaged,2019-05-12 16:21:20+00:00,,0,2,module: internals triaged,True
20405,CosineAnnealingLR has unexpected behavior with large step module: optimizer triaged,2019-05-12 02:55:07+00:00,,0,0,module: optimizer triaged,True
20375,Better documentation / molly-guards around use of multiprocessing with spawn in Jupyter/ipython notebooks module: docs triaged enhancement,2019-05-10 18:23:26+00:00,,0,1,module: docs triaged enhancement,True
20367,Overhead performance regression over time umbrella issue. high priority module: performance module: internals module: cuda module: cpu triaged quansight-nack,2019-05-10 16:12:20+00:00,,0,0,high priority module: performance module: internals module: cuda module: cpu triaged quansight-nack,True
20363,"Define portable M_PI replacement, use it instead of non-standard M_PI in math.h module: internals triaged",2019-05-10 13:57:33+00:00,,0,4,module: internals triaged,True
20359,[feature request] Run examples from docs as tests module: docs feature triaged,2019-05-10 12:16:33+00:00,,0,2,module: docs feature triaged,False
20343,torch.distributions.Binomial.sample() uses a massive amount of memory module: distributions module: memory usage triaged,2019-05-10 00:28:15+00:00,,0,13,module: distributions module: memory usage triaged,True
20342,TensorIterator resizes output to a scalar if there are no inputs module: internals triaged,2019-05-10 00:10:32+00:00,,0,0,module: internals triaged,True
20323,Support size to `torch.normal` triaged module: numpy function request,2019-05-09 14:56:03+00:00,,0,2,triaged module: numpy function request,False
20315,libtorch+opencv Mat result error: different from the python ones module: cpp triaged,2019-05-09 11:46:24+00:00,,0,2,module: cpp triaged,True
20273,Linker errors when linking statically (avx perfkernels) [Caffe2] caffe2,2019-05-08 14:26:39+00:00,,1,4,caffe2,False
20272,Redundantly saving sizes of SavedVariables in autograd Function module: bootcamp module: autograd low priority triaged,2019-05-08 14:06:56+00:00,,0,2,module: bootcamp module: autograd low priority triaged,True
20268,[Caffe2] Convert caffe to caffe2 with solverstate caffe2,2019-05-08 11:35:21+00:00,,0,0,caffe2,False
20248,Sparse tensors can't be used in DataLoader running many workers module: sparse module: multiprocessing feature triaged,2019-05-07 22:33:43+00:00,,0,11,module: sparse module: multiprocessing feature triaged,True
20245,ProcessGroupMPI tests in test_c10d.py oncall: distributed module: tests triaged,2019-05-07 20:24:08+00:00,,0,0,oncall: distributed module: tests triaged,True
20230,[jit] torch.tensor doesn't support list of tuples oncall: jit low priority triaged,2019-05-07 18:10:08+00:00,,1,1,oncall: jit low priority triaged,True
20206,Failed to build with system protobuf module: build module: protobuf triaged,2019-05-07 12:43:06+00:00,,0,8,module: build module: protobuf triaged,True
20204,"SyncBatchNorm should support 2D input (B, C) oncall: distributed module: nn triaged enhancement",2019-05-07 10:03:39+00:00,,0,8,oncall: distributed module: nn triaged enhancement,True
20189,Unify Caffe2 and PyTorch OpenMP initialization triaged module: multithreading,2019-05-06 23:12:28+00:00,,1,0,triaged module: multithreading,True
20183,Latex Errors when Compiling documentation to latexpdf module: docs triaged module: doc infra,2019-05-06 21:59:45+00:00,,0,3,module: docs triaged module: doc infra,False
20165,torch.nn.threshold cannot accept tensor as a threshold module: nn triaged enhancement,2019-05-06 16:18:13+00:00,,0,12,module: nn triaged enhancement,True
20154,"convert the model from pytorch to onnx to caffe2, but get a lower accuracy than before caffe2 triaged",2019-05-06 08:14:10+00:00,,0,2,caffe2 triaged,False
20151,RuntimeError: Given input size: (2048x1x1). Calculated output size: (2048x-5x-5). Output size is too small at /pytorch/aten/src/THNN/generic/SpatialAveragePooling.c:48 module: docs triaged module: tensorboard has workaround,2019-05-06 05:48:24+00:00,,0,7,module: docs triaged module: tensorboard has workaround,True
20149,Advanced indexing with uint8 tensor versus int64 tensor is inconsistent module: bc-breaking triaged module: advanced indexing module: boolean tensor,2019-05-06 04:33:39+00:00,,0,4,module: bc-breaking triaged module: advanced indexing module: boolean tensor,True
20138,Inconsistant values of lr_scheduler.get_lr and lr in optimizer.param_groups module: optimizer triaged,2019-05-05 09:13:24+00:00,,0,2,module: optimizer triaged,True
20134,"ERROR: Command ""python setup.py egg_info"" when dockerfile build needs reproduction module: build triaged",2019-05-05 04:00:55+00:00,,0,1,needs reproduction module: build triaged,True
20119,[JIT] Source highlighting doesn't line up when tabs are used for indentation oncall: jit triaged enhancement jit-backlog,2019-05-04 00:10:36+00:00,,0,1,oncall: jit triaged enhancement jit-backlog,True
20117,[FR] [RFC] add Sequential.append & .extend feature module: nn triaged,2019-05-03 22:17:40+00:00,,0,11,feature module: nn triaged,False
20111,Split libtorch binary build CI job into separate variants module: ci triaged,2019-05-03 20:46:41+00:00,,1,0,module: ci triaged,True
20102,LSTM forget bias must be initialized properly module: bc-breaking module: nn module: rnn triaged module: initialization,2019-05-03 14:31:09+00:00,,0,6,module: bc-breaking module: nn module: rnn triaged module: initialization,True
20086,CosineAnnealingLR giving unexpected learning rates on PyTorch 1.1. module: optimizer triaged,2019-05-02 22:50:33+00:00,,0,2,module: optimizer triaged,True
20066,Change devtoolset7 CUDA 9.0 nightlies to use a lower devtoolset module: ci triaged,2019-05-02 16:27:53+00:00,,1,0,module: ci triaged,True
20058,Implement noise_shape keyword for Dropout layers feature module: nn triaged,2019-05-02 12:11:20+00:00,,0,7,feature module: nn triaged,True
20056,Creation of too big multidimensional array returns empty tensor. module: error checking triaged module: tensor creation,2019-05-02 08:38:45+00:00,,0,19,module: error checking triaged module: tensor creation,True
20022,Precision of sparse float embeddings differs from dense embeddings on CPU module: nn triaged module: numerical-reproducibility,2019-05-01 18:01:51+00:00,,0,0,module: nn triaged module: numerical-reproducibility,True
20006,"RuntimeError: invalid argument 10: ldb should be at least max(1, 0), but have 0 at ../aten/src/TH/generic/THBlas.cpp:36 module: internals triaged",2019-05-01 11:52:32+00:00,,0,6,module: internals triaged,True
20005,install error from source proposal accepted module: internals triaged,2019-05-01 11:31:32+00:00,,0,10,proposal accepted module: internals triaged,True
19978,[JIT] traced model with optimization shows no performance improvement oncall: jit triaged,2019-04-30 22:17:45+00:00,,0,5,oncall: jit triaged,True
19960,"On first construction, CUDAContext changes default CPU allocator behavior caffe2",2019-04-30 11:21:02+00:00,,0,0,caffe2,False
19953,Performance difference between 0.4.1 and 1.1.0 module: performance triaged,2019-04-30 08:27:07+00:00,,0,8,module: performance triaged,True
19930,caffe2/resnet50 assert in fetch_blob() when base_learning_rate = 0 caffe2,2019-04-29 21:28:47+00:00,,0,0,caffe2,False
19911,"BatchNorm1d does not support batchsize>65535 in eval mode with 3 dimension (NxCxL), raise CUDNN_STATUS_NOT_SUPPORTED module: dependency bug module: cudnn triaged small",2019-04-29 10:04:08+00:00,,0,10,module: dependency bug module: cudnn triaged small,True
19826,vectorized convert_to_int_of_same_size <int64_t> can't handle nan module: cpu module: error checking triaged module: NaNs and Infs,2019-04-27 20:17:25+00:00,,0,5,module: cpu module: error checking triaged module: NaNs and Infs,True
19825,Performance issue master (a25b79531) module: performance module: cpu triaged module: multithreading,2019-04-27 20:01:03+00:00,,1,11,module: performance module: cpu triaged module: multithreading,True
19739,Importing open3d after PyTorch causes free(): invalid pointer module: crash triaged module: pybind,2019-04-25 12:33:53+00:00,,0,21,module: crash triaged module: pybind,True
19708,Test failure for depthwise3x3_conv caffe2,2019-04-25 01:27:17+00:00,,0,0,caffe2,True
19685,Bad overload order for zeros_like triaged module: pybind module: tensor creation,2019-04-24 18:33:22+00:00,,0,5,triaged module: pybind module: tensor creation,True
19682,View in Sequential - reasoned case for it module: nn triaged enhancement,2019-04-24 18:09:39+00:00,,0,5,module: nn triaged enhancement,True
19672,torch.set_flush_denormal not working on some (old) OSX machines triaged module: macos,2019-04-24 15:05:01+00:00,,0,0,triaged module: macos,True
19668,Importing matlab.engine after torch causes bad_alloc module: internals triaged module: assert failure,2019-04-24 10:46:16+00:00,,0,5,module: internals triaged module: assert failure,True
19652,[docs] torch.set_flush_denormal(...) to mention default mode module: docs triaged,2019-04-24 02:00:44+00:00,,0,1,module: docs triaged,False
19637,"[jit] Traced {zeros,empty}()/{zeros,empty}_like() calls do not respect device args. oncall: jit triaged",2019-04-23 22:11:16+00:00,,1,0,oncall: jit triaged,True
19628,[jit] nn.LSTM errors in nn.ScriptModule oncall: jit triaged,2019-04-23 19:56:28+00:00,,0,3,oncall: jit triaged,True
19599,cudnn conv doesn't check batch_size > 0 module: cudnn module: cuda module: error checking triaged,2019-04-23 02:32:40+00:00,,0,0,module: cudnn module: cuda module: error checking triaged,True
19581,[jit] Do aten::values dispatch at build time instead of runtime oncall: jit triaged,2019-04-22 21:31:12+00:00,,1,0,oncall: jit triaged,True
19571,[doc] Document general guidelines to work with CUDA async copying and streams high priority module: docs feature module: cuda triaged better-engineering,2019-04-22 19:13:08+00:00,,0,4,high priority module: docs feature module: cuda triaged better-engineering,True
19563,why my personal compiled libtorch is so slow?  2~3 times slower than caffe module: performance module: windows triaged,2019-04-22 13:33:09+00:00,,0,13,module: performance module: windows triaged,True
19529,failed to load model which is saved as text format(pickle_protocol=0) instead of binary format module: pickle module: serialization triaged,2019-04-20 01:06:00+00:00,,0,1,module: pickle module: serialization triaged,True
19504,[jit] Bad error when calling `ScriptModule`s with attributes/parameters oncall: jit triaged,2019-04-19 19:19:04+00:00,,0,1,oncall: jit triaged,True
19499,C++ API 'nn::Sequential' has inconsistent behavior with python conterpart module: cpp triaged,2019-04-19 17:46:30+00:00,,0,3,module: cpp triaged,True
19453,How to load PyTorch model with LSTM using C++ api module: cpp triaged,2019-04-19 02:11:06+00:00,,1,26,module: cpp triaged,True
19437,The `unary_kernel` call should be pushed into CopyKernel.cpp and it should completely replace the current `copy_kernel`. module: cpu triaged,2019-04-18 21:53:59+00:00,,1,0,module: cpu triaged,True
19416,[caffe2]Reloading model gives segmentation fault caffe2,2019-04-18 18:20:00+00:00,,0,0,caffe2,True
19408,Massive memory overhead over NumPy module: internals module: memory usage triaged,2019-04-18 14:28:30+00:00,,0,7,module: internals module: memory usage triaged,True
19393,[jit] requires_grad in JIT constructor/factories high priority triage review oncall: jit,2019-04-18 04:58:52+00:00,,0,9,high priority triage review oncall: jit,True
19349,Different behavior when trace model. oncall: jit triaged,2019-04-17 10:49:22+00:00,,0,10,oncall: jit triaged,True
19335,CUDA optimization: using `__restrict__` whenever possible module: cuda triaged,2019-04-17 02:32:43+00:00,,0,8,module: cuda triaged,True
19330,Multiprocessing on distributed Multi-nodes shutdown error: ‘spawn’ on slave node leads to semaphore_tracker leaked oncall: distributed triaged,2019-04-17 01:39:27+00:00,,0,3,oncall: distributed triaged,True
19267,torch.distributed.broadcast should default to current stream oncall: distributed triaged,2019-04-15 18:35:38+00:00,,0,4,oncall: distributed triaged,True
19263,GCP Base Image Wrong CUDA Version triaged module: doc infra,2019-04-15 16:19:54+00:00,,1,6,triaged module: doc infra,True
19248,add stable distribution in torch.distributions module: distributions feature low priority triaged,2019-04-14 10:48:32+00:00,,0,5,module: distributions feature low priority triaged,True
19244,[Caffe2] Retraining saved model caffe2,2019-04-13 16:52:03+00:00,,0,0,caffe2,False
19229,Different behavior of torch.nn.MultiMarginLoss on CPU/GPU Tensors module: cuda module: error checking triaged,2019-04-13 04:36:41+00:00,,0,1,module: cuda module: error checking triaged,True
19200,"Deprecate torch.add(tensor, value, other) module: bc-breaking triaged module: numpy module: deprecation module: ux",2019-04-12 17:12:28+00:00,,0,3,module: bc-breaking triaged module: numpy module: deprecation module: ux,True
19197,index_put_ take min when there are repeated indices low priority triaged enhancement module: advanced indexing,2019-04-12 14:59:17+00:00,,0,2,low priority triaged enhancement module: advanced indexing,True
19177,8 tests in test_c10d fail when running all tests in one command oncall: distributed module: tests triaged,2019-04-11 22:40:46+00:00,,1,4,oncall: distributed module: tests triaged,True
19172,[Feature Request] Common constants in the torch.* namespace triaged enhancement module: numpy,2019-04-11 18:19:47+00:00,,0,5,triaged enhancement module: numpy,True
19163,LayerNorm is very slow (almost frozen) in CPU of multiprocessing module: cpu triaged,2019-04-11 12:53:55+00:00,,0,5,module: cpu triaged,True
19160,Suggest model.eval() in torch.no_grad (and vice versa) module: docs triaged,2019-04-11 09:37:42+00:00,,0,11,module: docs triaged,True
19150,Improve unit test coverage of torch.unique module: tests triaged module: sorting and selection,2019-04-11 05:53:02+00:00,,1,0,module: tests triaged module: sorting and selection,True
19149,Make operators like logsumexp and cumsum operate over dimension 0 by default (or at least for 1D arrays) module: docs triaged enhancement module: numpy module: reductions,2019-04-11 05:43:44+00:00,,0,4,module: docs triaged enhancement module: numpy module: reductions,True
19143,Support memoryview() method on torch.Tensor feature triaged module: numpy,2019-04-11 02:47:17+00:00,,0,12,feature triaged module: numpy,True
19128,pytorch.version.cuda is None when compiling with CUDA support needs reproduction module: build module: cuda triaged,2019-04-10 21:08:40+00:00,,0,9,needs reproduction module: build module: cuda triaged,True
19126,weight_norm doesn't support eta and returns nan for zero weights module: nn triaged module: NaNs and Infs module: norms and normalization,2019-04-10 20:07:33+00:00,,0,8,module: nn triaged module: NaNs and Infs module: norms and normalization,True
19125,Multi-gpu via torch::nn::parallel::data_parallel oncall: distributed module: cpp feature triaged,2019-04-10 19:30:00+00:00,,0,5,oncall: distributed module: cpp feature triaged,True
19120,Embedding layer does not check input range module: cuda module: error checking triaged,2019-04-10 17:57:30+00:00,,0,1,module: cuda module: error checking triaged,True
19117,[jit] Can't `torch.jit.script` a lambda oncall: jit low priority triaged jit-backlog,2019-04-10 17:15:46+00:00,,0,1,oncall: jit low priority triaged jit-backlog,True
19107,pytorch/caffe2/onnx/backend.cc:1668:57: error: invalid conversion from ‘google::protobuf::int32 {aka int}’ to ‘onnx_torch::TensorProto::DataType {aka onnx_torch::TensorProto_DataType}’ [-fpermissive] caffe2,2019-04-10 11:23:56+00:00,,0,0,caffe2,False
19106,"Performance issue with torch.jit.trace(), slow prediction in C++ (CPU) triage review needs reproduction module: performance oncall: jit module: cpp",2019-04-10 10:49:01+00:00,,0,40,triage review needs reproduction module: performance oncall: jit module: cpp,True
19092,[RFC] Memory format (aka layout aka NHWC) support module: internals triaged module: mkldnn,2019-04-10 00:44:37+00:00,,1,73,module: internals triaged module: mkldnn,True
19053,RNN module weights not compacted oncall: jit,2019-04-09 02:45:16+00:00,,0,14,oncall: jit,False
19029,C++ custom module not thread safe needs reproduction module: cpp triaged,2019-04-08 17:56:44+00:00,,0,2,needs reproduction module: cpp triaged,True
19026,Python math module support oncall: jit triaged enhancement jit-backlog,2019-04-08 17:14:23+00:00,,0,13,oncall: jit triaged enhancement jit-backlog,True
19016,Clean up and consolidate DDP tests oncall: distributed module: tests triaged,2019-04-08 15:35:27+00:00,,1,0,oncall: distributed module: tests triaged,True
18998,torch.from_PIL() Request ? feature triaged module: vision,2019-04-07 14:59:56+00:00,,0,2,feature triaged module: vision,True
18993,ninja: build stopped: subcommand failed. module: build triaged,2019-04-07 02:21:58+00:00,,0,13,module: build triaged,True
18987,Performance issue of unique on CPU module: performance module: cpu triaged module: sorting and selection,2019-04-06 15:02:09+00:00,,1,0,module: performance module: cpu triaged module: sorting and selection,True
18975,Tracing and Scripting nn.Conv2d shows different op in the graph oncall: jit,2019-04-06 00:03:11+00:00,,0,4,oncall: jit,False
18944,Caffe2 on google colab:  caffe2,2019-04-05 09:40:31+00:00,,0,2,caffe2,False
18933,Completion of error handling module: internals low priority module: error checking triaged,2019-04-05 07:27:57+00:00,,0,5,module: internals low priority module: error checking triaged,True
18920,[c10d] CUDA tests for C++ reducer oncall: distributed triaged,2019-04-05 06:05:35+00:00,,1,0,oncall: distributed triaged,True
18906,Provide option to use alias method in Categorical.sample() module: distributions triaged,2019-04-04 23:46:13+00:00,,1,0,module: distributions triaged,True
18904,[FR] torch.dist along a dimension triaged function request module: reductions,2019-04-04 22:59:45+00:00,,0,0,triaged function request module: reductions,True
18856,CI with >8G CUDA memory module: cuda module: ci triaged module: 64-bit,2019-04-04 16:57:32+00:00,,0,1,module: cuda module: ci triaged module: 64-bit,True
18850,Allow tracing of models which output `None` oncall: jit feature triaged,2019-04-04 14:03:43+00:00,,1,4,oncall: jit feature triaged,True
18849,More efficient STFT on CUDA module: performance feature module: cuda triaged,2019-04-04 09:39:31+00:00,,0,1,module: performance feature module: cuda triaged,True
18805,Running custom operator tests manually is too difficult module: cpp-extensions module: tests triaged enhancement,2019-04-03 16:46:28+00:00,,1,1,module: cpp-extensions module: tests triaged enhancement,True
18796,[Feature Request] Flattened indices option for max pooling module: nn triaged enhancement module: pooling,2019-04-03 11:49:49+00:00,,1,2,module: nn triaged enhancement module: pooling,True
18776,Value of torch.backends.cudnn.benchmark Baked into JIT-Traced Modules ( 150x slowdown on ConvTranspose2d() ) [jit] [libtorch] [cudnn]  oncall: jit triaged,2019-04-02 22:47:54+00:00,,1,7,oncall: jit triaged,True
18730,Numerical instability KL divergence RelaxedOneHotCategorical module: numerical-stability module: distributions triaged,2019-04-02 15:05:13+00:00,,0,8,module: numerical-stability module: distributions triaged,True
18728,MaxPool with n-dimensional tensors module: nn triaged enhancement module: pooling,2019-04-02 14:39:45+00:00,,0,0,module: nn triaged enhancement module: pooling,True
18692,Add build tests for feature environment vars todo module: ci triaged,2019-04-01 16:15:53+00:00,,0,0,todo module: ci triaged,True
18677,How to compile/install caffe2 with cuda 9.0? caffe2,2019-04-01 06:54:50+00:00,,0,2,caffe2,False
18660,[FR] Warn in cuda init if cuda < 10 is used with RTX cards module: cuda module: molly-guard triaged,2019-03-30 22:38:58+00:00,,0,1,module: cuda module: molly-guard triaged,True
18658,[FR] add CPU information in collect_env.py module: collect_env.py triaged enhancement,2019-03-30 20:17:23+00:00,,0,0,module: collect_env.py triaged enhancement,False
18643,Memory not being deallocated in backward() module: autograd module: memory usage triaged quansight-nack,2019-03-30 00:44:22+00:00,,0,17,module: autograd module: memory usage triaged quansight-nack,True
18634,Speed-up torch.cat on CPU module: performance module: cpu triaged,2019-03-29 20:57:01+00:00,,0,18,module: performance module: cpu triaged,True
18631,FP32 depthwise convolution is slow in GPU high priority module: dependency bug module: performance module: cudnn module: cuda module: convolution triaged,2019-03-29 20:29:29+00:00,,0,66,high priority module: dependency bug module: performance module: cudnn module: cuda module: convolution triaged,True
18630,documentation for C++ / libtorch autograd profiler module: docs module: cpp triaged,2019-03-29 20:16:01+00:00,,0,4,module: docs module: cpp triaged,True
18536,Make it easier to figure out what CuDNN convolution algorithm we actually chose module: cudnn module: logging triaged,2019-03-27 19:24:46+00:00,,0,8,module: cudnn module: logging triaged,True
18524,[Caffe2] Missing CMAKE_CUDA_COMPILE_WHOLE_COMPILATION caffe2,2019-03-27 13:37:16+00:00,,0,11,caffe2,False
18520,Jit fail with TracingCheckError with tracing model with layers created after init. oncall: jit,2019-03-27 08:59:44+00:00,,0,2,oncall: jit,True
18476,[CPP] Allow binding config structs into the Python front end oncall: jit module: cpp,2019-03-26 14:21:38+00:00,,1,0,oncall: jit module: cpp,False
18475,The latest version of onnx-caffe2 does not support  “pow” ？ caffe2 triaged,2019-03-26 13:59:58+00:00,,0,10,caffe2 triaged,False
18451,download_mnist.py causes flaky tests oncall: releng module: ci module: tests triaged module: flaky-tests better-engineering,2019-03-25 20:51:38+00:00,,0,4,oncall: releng module: ci module: tests triaged module: flaky-tests better-engineering,True
18447,Tests on CI are not printing exceptions as they occur low priority module: ci module: tests triaged,2019-03-25 20:24:28+00:00,,0,2,low priority module: ci module: tests triaged,True
18436,UnpicklingError when trying to load multiple objects from a file module: pickle module: serialization triaged,2019-03-25 18:06:05+00:00,,0,1,module: pickle module: serialization triaged,True
18434,improve jit error message for legacy constructor module: docs triaged,2019-03-25 16:32:39+00:00,,0,4,module: docs triaged,True
18414,Add layer-wise adaptive rate scaling (LARS) optimizer feature module: optimizer triaged,2019-03-25 01:55:59+00:00,,0,16,feature module: optimizer triaged,False
18380,"C++ inference (CPU-only) stalls in Android, and crashes on Mac/Linux caffe2 module: android",2019-03-23 05:36:54+00:00,,0,2,caffe2 module: android,False
18376,Cleanup code in register_c10_ops.cpp oncall: jit,2019-03-23 00:11:01+00:00,,1,1,oncall: jit,False
18351,Port SpatialConvolutionMM and VolumetricConvolutionMM to ATen triaged module: porting,2019-03-22 20:04:46+00:00,,0,6,triaged module: porting,True
18317,Add a RandomBatchSampler ? module: dataloader triaged enhancement,2019-03-22 03:48:52+00:00,,0,4,module: dataloader triaged enhancement,True
18315,[FR] support default size (scalar) in torch.randint triaged module: numpy module: tensor creation function request,2019-03-22 02:41:41+00:00,,0,9,triaged module: numpy module: tensor creation function request,True
18312,[jit] Trace perform weirdly on BatchNorm oncall: jit,2019-03-22 01:59:23+00:00,,1,2,oncall: jit,False
18300,NCCL backend fails when calling broadcast from different threads high priority oncall: distributed triaged module: nccl,2019-03-22 00:09:44+00:00,,1,25,high priority oncall: distributed triaged module: nccl,True
18283,Doubly freed pointer in torch::cat error handling when called via pybind11. module: cpp module: error checking triaged,2019-03-21 18:06:31+00:00,,0,18,module: cpp module: error checking triaged,True
18273,[caffe2] Broken Operators Catalog caffe2,2019-03-21 10:53:49+00:00,,0,1,caffe2,False
18229,Add support for tuple type deduction in C++ custom operators module: internals module: bootcamp triaged enhancement,2019-03-20 17:42:28+00:00,,0,1,module: internals module: bootcamp triaged enhancement,True
18220,Context Manager that disables training mode with in a nn.Module. feature module: nn triaged,2019-03-20 14:12:23+00:00,,0,5,feature module: nn triaged,False
18212,[JIT] bitwise NOT does not handle tensor shapes correctly under JIT oncall: jit triaged,2019-03-20 06:55:45+00:00,,0,4,oncall: jit triaged,True
18206,Rename ignore_index to ignore_target in CrossEntropyLoss module: nn low priority triaged enhancement small,2019-03-20 02:43:08+00:00,,0,3,module: nn low priority triaged enhancement small,True
18182,Update weight initialisations to current best practices high priority module: bc-breaking feature module: nn triaged quansight-nack,2019-03-19 17:34:38+00:00,,0,46,high priority module: bc-breaking feature module: nn triaged quansight-nack,True
18173,[docs] How to achieve high-order derivation in my .cpp? module: docs module: cpp module: autograd triaged actionable,2019-03-19 08:34:46+00:00,,1,3,module: docs module: cpp module: autograd triaged actionable,True
18157,JIT does not batch linear layers in an ensemble oncall: jit triaged,2019-03-19 01:48:55+00:00,,0,11,oncall: jit triaged,True
18106,copy.copy not working for ScriptModule oncall: jit triaged jit-backlog,2019-03-16 19:24:49+00:00,,0,15,oncall: jit triaged jit-backlog,True
18101,Android NNAPI support of Caffe2 caffe2,2019-03-16 10:16:04+00:00,,0,1,caffe2,False
18095,torch.flip is inconsistent with np.flip and also uses `dims` arg instead of `dim` triaged module: numpy function request,2019-03-16 02:19:33+00:00,,0,12,triaged module: numpy function request,True
18094,[JIT Script] Need support on distributions.Categorical. oncall: jit triaged,2019-03-16 00:40:30+00:00,,1,5,oncall: jit triaged,True
18053,cuDNN error when using 3d convolutions module: dependency bug module: cudnn triaged,2019-03-15 08:56:39+00:00,,0,8,module: dependency bug module: cudnn triaged,True
18011,caffe2 Segmentation fault (core dumped) caffe2,2019-03-14 09:14:20+00:00,,0,1,caffe2,True
18005,Support 'bytes' type in torchscript oncall: jit triaged jit-backlog,2019-03-14 01:42:26+00:00,,2,5,oncall: jit triaged jit-backlog,True
18000,Unsupported type of tensor c10::half when running resnet50_trainer.py in Caffe2  caffe2,2019-03-13 22:36:23+00:00,,0,3,caffe2,False
17984,"""unknown builtin op"" error with static library module: cpp triaged",2019-03-13 17:34:43+00:00,,1,3,module: cpp triaged,True
17970,Binary not operator causes crash when Jit module is executed on different device oncall: jit triaged,2019-03-13 11:11:40+00:00,,1,7,oncall: jit triaged,True
17966,torch.cuda.is_available()  returns misleading value module: cuda triaged module: ux,2019-03-13 08:10:53+00:00,,0,1,module: cuda triaged module: ux,True
17936,[caffe2] resize_op_test.py::TestResize::test_nearest FAILED caffe2,2019-03-12 20:52:12+00:00,,0,1,caffe2,False
17932,JIT torch.ones_like with dtype starts failing on master oncall: jit,2019-03-12 19:15:01+00:00,,0,1,oncall: jit,True
17920,Build compact libtorch from source with cmake  module: build module: cpp triaged,2019-03-12 16:08:31+00:00,,0,18,module: build module: cpp triaged,True
17907,"[caffe2] resnet_trainer example fails with MNIST dataset, when parameter num_channels=1 is provided caffe2",2019-03-12 08:18:27+00:00,,0,0,caffe2,False
17902,Conjugate gradient method feature triaged module: derivatives function request,2019-03-12 03:42:35+00:00,,0,4,feature triaged module: derivatives function request,True
17901,should disable AVX on 32bit x86 / refine AVX availability tests module: build triaged,2019-03-12 03:36:46+00:00,,0,6,module: build triaged,True
17897,CUDA large matrix-vector product (torch.mv) causes illegal memory access module: dependency bug module: cuda triaged module: 64-bit module: cublas,2019-03-11 23:37:44+00:00,,0,3,module: dependency bug module: cuda triaged module: 64-bit module: cublas,True
17887,Caffe2 building failure caffe2,2019-03-11 21:31:14+00:00,,0,0,caffe2,True
17879,C++ nn::Sequential push_back() copies module if the module is concrete type module: cpp module: nn triaged,2019-03-11 20:15:25+00:00,,0,0,module: cpp module: nn triaged,True
17872,Linking Caffe2 with sequential MKL but still calling multi-threaded mkl caffe2,2019-03-11 16:48:22+00:00,,0,0,caffe2,False
17869,[C++ Frontend] ONNX export module: onnx triaged onnx-triaged,2019-03-11 15:23:25+00:00,,0,15,module: onnx triaged onnx-triaged,True
17853,Caffe2 building failure when turning off USE_OPENMP caffe2,2019-03-11 02:21:39+00:00,,0,2,caffe2,True
17850,Build fails for caffe2/CMakeFiles/caffe2.dir/__/aten/src/ATen/native/mkldnn/Conv.cpp.o; possibly MKL-DNN issue caffe2,2019-03-10 16:50:52+00:00,,0,0,caffe2,True
17849,Tensor::options() returns false for requires_grad when it is true module: cpp-extensions triaged,2019-03-10 11:21:02+00:00,,0,14,module: cpp-extensions triaged,True
17835,PyPy support module: binaries feature triaged,2019-03-09 08:30:14+00:00,,0,20,module: binaries feature triaged,True
17800,[Caffe2] cudnn mismatch  caffe2,2019-03-08 14:28:50+00:00,,0,7,caffe2,False
17798,Feature Request: deterministic CUDA torch.nn.CTCLoss feature module: nn module: loss triaged module: determinism,2019-03-08 13:52:12+00:00,,0,8,feature module: nn module: loss triaged module: determinism,True
17797,Non-coherent result for C++ with multithreading and GPU oncall: jit module: cpp triaged,2019-03-08 08:48:25+00:00,,1,4,oncall: jit module: cpp triaged,True
17796,[Caffe2] install error caffe2,2019-03-08 08:39:23+00:00,,0,5,caffe2,True
17774,RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED when batch size is too large module: cudnn triaged,2019-03-07 20:53:27+00:00,,0,7,module: cudnn triaged,True
17745,"distributed data parallel, gloo backend works, but nccl deadlock oncall: distributed triaged module: nccl module: deadlock",2019-03-07 13:37:26+00:00,,0,5,oncall: distributed triaged module: nccl module: deadlock,True
17716,"collate_fn returns subclass of torch.Tensor, but DataLoader transforms back to torch.Tensor module: dataloader triaged module: __torch_function__",2019-03-06 15:06:33+00:00,,0,3,module: dataloader triaged module: __torch_function__,True
17712,Can we to build Caffe2 custom ops independently from a Caffe2 build ? caffe2,2019-03-06 10:46:35+00:00,,0,1,caffe2,False
17703,Training hangs when using DistributedDataParallel in two pod on two nodes  oncall: distributed triaged,2019-03-06 03:25:16+00:00,,0,5,oncall: distributed triaged,True
17688,Improve save() method in torch.jit.ScriptModule oncall: jit low priority triaged jit-backlog,2019-03-05 18:17:01+00:00,,0,5,oncall: jit low priority triaged jit-backlog,True
17666,Building pytorch on ARM failed module: build triaged,2019-03-04 22:13:31+00:00,,0,2,module: build triaged,True
17658,C++ API: Crash in cudnnDestroy() when deconstructing module: cudnn module: cpp module: abi triaged,2019-03-04 15:08:14+00:00,,0,19,module: cudnn module: cpp module: abi triaged,True
17655,TracedModule 'to' attribute doesn't work for tensors created on forward. oncall: jit triaged,2019-03-04 13:23:09+00:00,,0,2,oncall: jit triaged,True
17644,"Not depend on third_party submodules, but self-built libraries? module: build triaged",2019-03-03 13:36:50+00:00,,0,1,module: build triaged,True
17634,[Caffe2] android app crashed while linked with libcaffe2_detectron_ops.so caffe2,2019-03-02 10:36:15+00:00,,0,0,caffe2,False
17632,Caffe2 C++ script for classification/object_detection with CMakeLists.txt  caffe2,2019-03-02 05:06:30+00:00,,0,0,caffe2,False
17614,Document whether it is possible to train TorchScript modules oncall: jit module: docs module: cpp triaged,2019-03-01 13:02:44+00:00,,1,19,oncall: jit module: docs module: cpp triaged,True
17612,The documentation to Module._version isn't visible. module: docs module: nn module: serialization triaged,2019-03-01 10:32:16+00:00,,0,5,module: docs module: nn module: serialization triaged,True
17554,Add ASSERT for calling accessor on a GPU tensor module: cpp module: molly-guard triaged,2019-02-27 20:44:41+00:00,,0,1,module: cpp module: molly-guard triaged,True
17553,"torch.Tensor.cpu talks about the object being ""on the correct device"" module: docs module: bootcamp triaged small",2019-02-27 20:08:30+00:00,,0,1,module: docs module: bootcamp triaged small,True
17538,ONNX->TensorRT parser library duplicated triaged,2019-02-27 06:57:38+00:00,,3,4,triaged,False
17525,cmake fails with -DBUILD_PYTHON=OFF -DUSE_NNPACK=OFF module: build oncall: releng triaged,2019-02-27 00:46:12+00:00,,0,4,module: build oncall: releng triaged,True
17495,RuntimeError: storage_.IsType<T>() ASSERT FAILED awaiting response (this tag is deprecated) caffe2 triaged,2019-02-26 07:34:56+00:00,,0,2,awaiting response (this tag is deprecated) caffe2 triaged,True
17458,Overflow in fbgemm caffe2,2019-02-25 02:56:12+00:00,,0,4,caffe2,False
17437,Automatic aggregation of a mix of sparse and dense gradients is not supported yet caffe2,2019-02-24 01:26:58+00:00,,0,0,caffe2,False
17425,"improved assert message in the case of ""CUDA error: device-side assert triggered"" module: bootcamp module: cuda module: molly-guard triaged",2019-02-23 00:03:24+00:00,,1,1,module: bootcamp module: cuda module: molly-guard triaged,True
17354,testConvnetBenchmarks intermittently segfaults triaged module: flaky-tests better-engineering,2019-02-21 15:07:15+00:00,,0,1,triaged module: flaky-tests better-engineering,True
17350,"torch.nn.CrossEntropyLoss with ""reduction"" sum/mean is not deterministic on segmentation outputs / labels module: cuda triaged",2019-02-21 12:45:43+00:00,,0,5,module: cuda triaged,True
17342,Code review on .circleci/ module: ci triaged,2019-02-21 04:31:33+00:00,,1,1,module: ci triaged,True
17313,Implement Adaptive Input Representations for Neural Language Modeling feature module: nn triaged,2019-02-20 20:44:25+00:00,,0,16,feature module: nn triaged,True
17286,Use standard docker image for XLA build oncall: releng triaged module: xla module: docker,2019-02-20 01:16:00+00:00,,0,0,oncall: releng triaged module: xla module: docker,True
17268,Generic object to tensor dispatching feature triaged,2019-02-19 18:59:03+00:00,,0,9,feature triaged,True
17257,Generated `__init__.pyi` contains invalid default values triaged module: codegen,2019-02-19 17:01:17+00:00,,0,0,triaged module: codegen,True
17249,Proposal: Add __tensor_wrap__ method similar to numpy __array_wrap__ module: internals triaged module: numpy,2019-02-19 09:15:41+00:00,,0,33,module: internals triaged module: numpy,True
17234,[feature request] build and move distributions w/ device and/or dtype module: build module: distributions triaged,2019-02-18 22:23:34+00:00,,0,2,module: build module: distributions triaged,True
17216,Group Norm Error When using FP16 needs reproduction triaged module: type promotion module: half module: norms and normalization,2019-02-17 04:55:40+00:00,,0,5,needs reproduction triaged module: type promotion module: half module: norms and normalization,True
17199,Deadlock with multiprocessing (using fork) and OpenMP / PyTorch should warn after OMP and fork that multithreading may be broken high priority module: docs module: multiprocessing triaged,2019-02-16 10:52:06+00:00,,0,23,high priority module: docs module: multiprocessing triaged,True
17169,Global Second Order Pooling todo feature triaged module: pooling,2019-02-15 19:11:16+00:00,,0,5,todo feature triaged module: pooling,True
17165,Better API / conversion for c++ objects to correct IValue objects oncall: jit low priority jit-backlog,2019-02-15 17:39:47+00:00,,0,6,oncall: jit low priority jit-backlog,True
17154,Error in converting pytorch model to caffe2 using onnx framework  caffe2,2019-02-15 07:23:48+00:00,,0,1,caffe2,True
17150,Conv2d layers should accept 4-tuple for padding argument module: nn module: convolution triaged enhancement,2019-02-15 03:38:18+00:00,,0,3,module: nn module: convolution triaged enhancement,True
17126,Support callables in scripted functions oncall: jit jit-backlog,2019-02-14 20:38:10+00:00,,1,3,oncall: jit jit-backlog,False
17113,Distributed training jobs do not terminate properly if there is a crash oncall: distributed triaged,2019-02-14 13:20:11+00:00,,1,13,oncall: distributed triaged,True
17057,"Be able to use ""@pytorchbot retest this please"" to re-run both CircleCI and Jenkins jobs feature module: ci triaged",2019-02-13 17:00:02+00:00,,0,3,feature module: ci triaged,True
17041,Errors running distributed example oncall: distributed triaged,2019-02-13 01:28:26+00:00,,1,9,oncall: distributed triaged,True
17023,Download speed issues with the pytorch conda channel module: dependency bug triaged,2019-02-12 20:29:14+00:00,,0,91,module: dependency bug triaged,True
16991,"[Caffe2] Dropout modules exported from PyTorch with ONNX: BlobIsTensorType(*blob, CPU). Blob is not a CPU Tensor: [BlobNumber] caffe2",2019-02-12 02:48:38+00:00,,0,0,caffe2,False
16956,Hardshrink for Sparse Tensors module: sparse feature triaged,2019-02-11 15:15:29+00:00,,1,2,module: sparse feature triaged,True
16954,torch.multiprocessing.pool.Pool broken module: multiprocessing triaged small,2019-02-11 14:10:33+00:00,,1,12,module: multiprocessing triaged small,True
16953,Seg fault with test_rnn_retain_variables on ppc64le module: crash triaged module: POWER,2019-02-11 10:25:40+00:00,,0,1,module: crash triaged module: POWER,True
16943,Multiple CPU processes using same GPU model for inference module: windows module: multiprocessing triaged,2019-02-10 21:27:08+00:00,,1,32,module: windows module: multiprocessing triaged,True
16940,Allow positional arguments to be passed as kwargs for autograd custom Function module: autograd triaged actionable,2019-02-10 20:23:52+00:00,,0,6,module: autograd triaged actionable,False
16901,Issue with dataloader using pin_memory = True module: dataloader triaged,2019-02-08 18:50:59+00:00,,0,9,module: dataloader triaged,True
16899,"Give clearer guidance about multithreading in PyTorch, and how to disable it module: docs triaged module: multithreading",2019-02-08 17:00:39+00:00,,0,5,module: docs triaged module: multithreading,False
16897,Implement `numpy.random.choice` equivalent high priority module: bootcamp feature triaged module: numpy,2019-02-08 15:33:11+00:00,,0,29,high priority module: bootcamp feature triaged module: numpy,True
16896,`nn.Linear` allows 1d input tensors module: docs triaged,2019-02-08 13:53:09+00:00,,0,2,module: docs triaged,True
16873,Allow building C++ custom ops that imports another custom ops module: cpp-extensions triaged,2019-02-07 23:31:32+00:00,,0,0,module: cpp-extensions triaged,True
16843,Allow to build pytorch for a *specific* architecture module: build triaged,2019-02-07 14:01:01+00:00,,0,19,module: build triaged,True
16805,cstddef not found when compiling C++ Extension - macOS module: cpp-extensions triaged,2019-02-06 15:48:57+00:00,,1,9,module: cpp-extensions triaged,True
16804,Wrong description of positive class weight in BCEWithLogitsLoss module: docs triaged,2019-02-06 15:31:05+00:00,,0,2,module: docs triaged,True
16797,"Should be a way to unpickle an object with a torch cuda tensor on a CPU-only machine when using plain ""pickle"" todo feature module: serialization triaged",2019-02-06 10:08:13+00:00,,0,9,todo feature module: serialization triaged,True
16737,torch.save overwrite module: docs triaged enhancement,2019-02-04 23:14:36+00:00,,1,5,module: docs triaged enhancement,True
16717,Performance regression on CPU from 0.4.1 to 1.0.0 on ResNet inference module: performance module: cpu triaged module: single threaded,2019-02-04 15:01:18+00:00,,1,14,module: performance module: cpu triaged module: single threaded,True
16710,culibos linker errors on binary_linux_conda_3.6_cu90_build module: build triaged,2019-02-04 03:37:48+00:00,,0,4,module: build triaged,True
16708,Better include path when compiling mkldnn module: build triaged,2019-02-04 02:42:41+00:00,,0,3,module: build triaged,True
16706,Very poor Uniform() sampling near floating 0.0 triaged module: random,2019-02-03 23:45:40+00:00,,0,29,triaged module: random,True
16703,split_with_sizes should accept a LongTensor as the split_sizes parameter feature triaged module: numpy,2019-02-03 10:23:22+00:00,,0,13,feature triaged module: numpy,True
16700,[feature request] Store accumulated gradients in separate GPU or on CPU memory triaged module: data parallel,2019-02-03 06:31:26+00:00,,1,9,triaged module: data parallel,True
16695,Check whether _cudnn_rnn_flatten_weight can avoid changing the TensorImpl or Storage pointer of tensors in `weight_arr` module: cudnn triaged module: assert failure,2019-02-02 20:00:26+00:00,,0,0,module: cudnn triaged module: assert failure,True
16668,Reduce fragmentation with CUDA caching allocator when using many streams module: cuda module: memory usage triaged,2019-02-01 20:27:30+00:00,,0,0,module: cuda module: memory usage triaged,True
16666,numa_test.py fails with AssertionError caffe2,2019-02-01 19:35:09+00:00,,0,1,caffe2,False
16663,new_zeros is not traced correctly oncall: jit good first issue triaged,2019-02-01 18:43:07+00:00,,0,12,oncall: jit good first issue triaged,True
16661,ROCm 2.1: test_gamma_gpu_sample test fails module: rocm triaged,2019-02-01 17:44:53+00:00,,0,0,module: rocm triaged,True
16655,"CUDA cache doubles on the second batch and causes OOM, `empty_cache` doesn't empty it module: dependency bug module: cudnn triaged",2019-02-01 11:11:51+00:00,,0,13,module: dependency bug module: cudnn triaged,True
16654,pytorch was blocked at loss.backward needs reproduction oncall: distributed triaged,2019-02-01 10:57:10+00:00,,0,7,needs reproduction oncall: distributed triaged,False
16602,Error while installing pytorch module: build caffe2,2019-01-31 06:24:02+00:00,,0,14,module: build caffe2,True
16599,Feature Request: Earth Mover's Distance Loss module: loss triaged function request,2019-01-31 04:58:16+00:00,,0,6,module: loss triaged function request,False
16592,[JIT] NVRTC unknown error needs reproduction oncall: jit has workaround,2019-01-31 03:07:29+00:00,,0,25,needs reproduction oncall: jit has workaround,True
16590,Benchmarking pre-trained model in Caffe2 caffe2,2019-01-31 02:52:52+00:00,,0,0,caffe2,False
16589,Make sure `data_ptr` for non-zero-size input tensors stays the same after the VariableType dispatch module: autograd module: molly-guard triaged module: assert failure,2019-01-31 02:37:34+00:00,,0,2,module: autograd module: molly-guard triaged module: assert failure,True
16556,"KeyError: 'No translator registered for layer: name: ""res3b_relu_norm""\\ntype: ""Normalize""\\nbottom: ""res3b""\\ntop: ""res3b_relu_norm""\\nnorm_param caffe2",2019-01-30 10:02:57+00:00,,0,0,caffe2,False
16542,Error with setting tensors to use cpu in packed_padded_sequence when CUDA tensor is set as default module: rnn module: cuda triaged,2019-01-30 03:47:54+00:00,,0,15,module: rnn module: cuda triaged,True
16523,"The word ""ints"" is not rendered correclty in docs. todo module: docs triaged",2019-01-29 20:54:01+00:00,,0,1,todo module: docs triaged,False
16500,Undefined GPU Reference when importing torch with caffe2.onnx.backend for cpu-only pytorch caffe2,2019-01-29 14:04:03+00:00,,0,1,caffe2,False
16499,TorchConfig.cmake always sets _GLIBCXX_USE_CXX11_ABI module: build triaged,2019-01-29 13:51:40+00:00,,0,1,module: build triaged,True
16494,caffe2 softmaxwithloss problem caffe2,2019-01-29 08:34:53+00:00,,0,1,caffe2,True
16458,TensorRTOpTest.test_vgg19 is flaky triaged module: flaky-tests,2019-01-28 20:04:39+00:00,,0,0,triaged module: flaky-tests,True
16436,How to retrain the modelzoo model in Caffe2?  caffe2,2019-01-28 09:02:59+00:00,,0,0,caffe2,False
16432,A bug in parallel.data_parallel when module_kwargs is not None module: cuda module: error checking triaged module: batching module: data parallel,2019-01-28 05:48:16+00:00,,0,4,module: cuda module: error checking triaged module: batching module: data parallel,True
16424,Flip is much slower than advanced indexing module: performance triaged module: viewing and reshaping,2019-01-27 22:52:34+00:00,,0,9,module: performance triaged module: viewing and reshaping,True
16405,error on cmake_version from tools/build_pytorch_libs.py module: build triaged,2019-01-26 14:27:44+00:00,,0,2,module: build triaged,True
16402,error with nccl when distributed training on caffe2 caffe2,2019-01-26 10:46:15+00:00,,0,0,caffe2,True
16385,Adding new module to caffe2 caffe2,2019-01-25 22:33:49+00:00,,0,1,caffe2,False
16375,Python-bound C++ frontend modules don't handle attributes well module: cpp triaged,2019-01-25 19:09:45+00:00,,1,3,module: cpp triaged,True
16363,No Schema registered for ConstantOfShape with domain_version of 9 caffe2,2019-01-25 10:18:03+00:00,,0,0,caffe2,False
16330,support `unique_indices` option for `unique` todo feature triaged module: numpy,2019-01-24 19:03:01+00:00,,1,11,todo feature triaged module: numpy,True
16328,Should torch.arange take a layout parameter? module: sparse triaged module: tensor creation module: ux,2019-01-24 18:49:14+00:00,,0,1,module: sparse triaged module: tensor creation module: ux,True
16322,We're binding a bunch of crap to 'torch' namespace which shouldn't be there triaged better-engineering,2019-01-24 16:52:26+00:00,,0,8,triaged better-engineering,True
16318,Pytorch with CUDA aware OpenMPI for Infiniband not working with HCOLL and MXM oncall: distributed triaged module: c10d distributed-backlog,2019-01-24 14:14:28+00:00,,0,0,oncall: distributed triaged module: c10d distributed-backlog,True
16299,caffe2::CudnnConvOp::RunOnDevice() fails on Squeezenet caffe2,2019-01-24 03:27:17+00:00,,0,1,caffe2,False
16298,caffe2::onnx::OnnxExporter::Caffe2OpToOnnxNodes failure caffe2,2019-01-24 03:19:18+00:00,,0,0,caffe2,True
16296,caffe2::onnx::OnnxExporter::CreateGemmNodes fails on bvlc_alexnet caffe2,2019-01-24 02:55:30+00:00,,0,0,caffe2,False
16295,ProcessGroupGlooTest.test_gather_stress is flaky oncall: distributed triaged module: flaky-tests module: c10d,2019-01-24 02:16:50+00:00,,0,0,oncall: distributed triaged module: flaky-tests module: c10d,True
16293,Unexpected behavior of jit.trace when PYTORCH_JIT=0 oncall: jit low priority,2019-01-24 01:43:08+00:00,,1,3,oncall: jit low priority,True
16291,Confusing documentation with distributions.Categorical about logits module: distributions module: docs triaged,2019-01-24 01:25:05+00:00,,0,1,module: distributions module: docs triaged,True
16280,caffe2: cudaHostRegister() and mbind()-related test failures on ppc64le caffe2,2019-01-23 22:10:12+00:00,,0,2,caffe2,False
16268,Complete dtype support for torch.norm module: docs triaged,2019-01-23 18:41:57+00:00,,0,2,module: docs triaged,False
16266,support for multiple torch.cuda.max_memory_allocated() counters todo feature module: cuda triaged,2019-01-23 18:18:48+00:00,,0,7,todo feature module: cuda triaged,True
16263,[JIT] Support C++ front end module and JIT interop oncall: jit,2019-01-23 17:42:57+00:00,,2,9,oncall: jit,True
16257,why check ArgumentInfo is_pod? suffer bugs module: docs low priority triaged,2019-01-23 14:19:36+00:00,,0,9,module: docs low priority triaged,True
16254,I hope Caffe2's python interface add DataLoader Module Similar to pytorch's DataSet and DataLoader caffe2,2019-01-23 09:13:44+00:00,,0,0,caffe2,False
16248,Assert In function importUnsqueeze caffe2,2019-01-23 03:26:15+00:00,,0,1,caffe2,False
16195,how to implement crf predict for segmentation task in Caffe2? caffe2,2019-01-20 03:02:20+00:00,,0,0,caffe2,False
16187,Sparse matrix multiplication is too slow module: sparse triaged,2019-01-19 15:54:07+00:00,,0,18,module: sparse triaged,True
16148,CategoricalCrossEntropy Loss runs with wrong tag module: nn module: loss triaged,2019-01-18 09:43:26+00:00,,0,1,module: nn module: loss triaged,True
16138,[Caffe2] resize5d stride/dim issue caffe2,2019-01-18 04:45:16+00:00,,0,0,caffe2,True
16090,build failure with NNAPI enabled (Caffe2 path but under pytorch umbrella) module: build caffe2 triaged,2019-01-16 22:29:09+00:00,,1,2,module: build caffe2 triaged,True
16014,[docs] Missing argument description (value) in scatter_   function documentation todo module: docs triaged module: scatter & gather ops,2019-01-14 15:49:44+00:00,,0,4,todo module: docs triaged module: scatter & gather ops,False
15998,[Caffe2] Failed to load ONNX model with python 3.6 caffe2,2019-01-13 11:05:05+00:00,,0,0,caffe2,False
15994,More data type support for gather_map module: nn triaged enhancement,2019-01-12 21:10:52+00:00,,0,6,module: nn triaged enhancement,True
15963,ProcessGroupGlooTest.test_scatter_stress_cuda is flaky oncall: distributed triaged module: flaky-tests module: c10d,2019-01-11 17:25:25+00:00,,0,6,oncall: distributed triaged module: flaky-tests module: c10d,True
15918,Broadcasting and Additional Dimensions for pairwise_distance module: nn triaged function request module: distance functions,2019-01-10 07:36:34+00:00,,0,7,module: nn triaged function request module: distance functions,True
15869,RuntimeError: [enforce fail at pybind_state.cc:1111] success. Error running net train caffe2,2019-01-09 09:15:59+00:00,,0,0,caffe2,True
15864,Deformable Convolution feature triaged module: vision,2019-01-09 07:05:36+00:00,,0,11,feature triaged module: vision,True
15849,DataLoader with option to re-use worker processes high priority feature module: dataloader triaged,2019-01-09 00:21:24+00:00,,1,59,high priority feature module: dataloader triaged,True
15829,computing  entropy of a tensor  high priority triaged function request,2019-01-08 16:27:09+00:00,,0,21,high priority triaged function request,True
15822,AvgPool2d doesn't test if kernel is smaller than input size module: bootcamp module: error checking triaged better-engineering module: pooling,2019-01-08 08:29:56+00:00,,0,5,module: bootcamp module: error checking triaged better-engineering module: pooling,True
15821,No test coverage for kwargs of AvgPool2d and AvgPool3d module: bootcamp module: nn triaged module: pooling,2019-01-08 08:14:54+00:00,,0,2,module: bootcamp module: nn triaged module: pooling,True
15812,Possible regression in incremental build module: build triaged,2019-01-08 00:15:15+00:00,,0,2,module: build triaged,True
15771,Implicit conversion error in caffe2 caffe2,2019-01-06 08:55:43+00:00,,1,2,caffe2,True
15738,Beta Distribution values wrong for a=b---> 0 module: distributions triaged,2019-01-04 17:33:11+00:00,,0,14,module: distributions triaged,True
15716,"Nn.dataparallel with multiple output, weird gradient result None oncall: distributed triaged",2019-01-03 23:18:11+00:00,,1,6,oncall: distributed triaged,True
15630,[Caffe2] Internal compiler error for CUDA. caffe2,2018-12-30 13:33:36+00:00,,0,14,caffe2,True
15617,Feature request: transposed locally connected layer todo module: nn triaged enhancement,2018-12-29 18:16:23+00:00,,0,0,todo module: nn triaged enhancement,True
15608,[Caffe2] How to switch to test phase? caffe2,2018-12-29 09:33:55+00:00,,0,2,caffe2,False
15607,documentation for adding a new type via C++ extensions module: docs triaged module: complex module: bfloat16,2018-12-29 07:32:25+00:00,,0,13,module: docs triaged module: complex module: bfloat16,True
15491,LibTorch: include cmake files for all distributed headers module: cpp triaged,2018-12-21 20:27:12+00:00,,0,3,module: cpp triaged,True
15465,Transplant caffe2 on my DNN-accelerator caffe2,2018-12-21 02:35:55+00:00,,0,1,caffe2,False
15457,Improve one_hot module: nn triaged function request,2018-12-21 00:16:40+00:00,,0,9,module: nn triaged function request,True
15432,caffe2: multi-thread(or multi-instance) predict is much slower than the single thread or single instance caffe2,2018-12-20 09:16:52+00:00,,0,13,caffe2,False
15431,IDEEP error:could not initialize a memory descriptor caffe2,2018-12-20 08:59:39+00:00,,0,2,caffe2,False
15421,JIT is not compatible with data parallel oncall: jit,2018-12-20 02:37:12+00:00,,1,13,oncall: jit,True
15386,F.grid_sample doesn't respect padding_mode when height of inputs is 1 module: docs triaged module: interpolation,2018-12-19 05:38:31+00:00,,0,1,module: docs triaged module: interpolation,True
15378,Projective Transformation grid generator todo triaged enhancement,2018-12-19 02:46:21+00:00,,0,5,todo triaged enhancement,True
15350,Proposal for build system under many system configuration testing module: build module: ci triaged,2018-12-18 17:05:17+00:00,,0,4,module: build module: ci triaged,True
15342,[JIT] Batch op. batch_sum return a mask which is on cpu needs reproduction oncall: jit,2018-12-18 13:43:52+00:00,,0,3,needs reproduction oncall: jit,False
15313,AttributeError: module 'caffe2.python._import_c_extension' has no attribute 'get_cudnn_version' when Caffe2 is not built with CuDNN caffe2,2018-12-17 22:10:24+00:00,,0,0,caffe2,True
15309,libtorch without cmake  module: build module: cpp triaged,2018-12-17 20:32:29+00:00,,0,1,module: build module: cpp triaged,True
15303,CMake Error at CMakeLists.txt:10 (find_package) in C++ module: build triaged,2018-12-17 18:42:22+00:00,,0,3,module: build triaged,True
15298,Momentum problem (1-momentum is correct?) in BatchNorm2d todo module: docs module: nn triaged,2018-12-17 11:44:47+00:00,,0,2,todo module: docs module: nn triaged,True
15294,manylinux2014 compatible wheels module: build triaged,2018-12-17 06:26:09+00:00,,0,6,module: build triaged,True
15291,linked error of Pytorch 1.0 release  module: build triaged module: macos,2018-12-17 02:22:38+00:00,,0,16,module: build triaged module: macos,True
15290,[caffe2] Installation problem on OSX caffe2,2018-12-17 02:10:32+00:00,,0,1,caffe2,True
15288,possible unsafety in torch.distributions.kl_divergence for Bernoullis module: distributions triaged module: NaNs and Infs,2018-12-16 22:05:10+00:00,,0,2,module: distributions triaged module: NaNs and Infs,True
15284,index_add_ with scalar values instead of tensors triaged enhancement module: advanced indexing,2018-12-16 15:54:08+00:00,,0,1,triaged enhancement module: advanced indexing,True
15260,MultiGPU for gru module: rnn triaged module: data parallel,2018-12-15 08:04:30+00:00,,0,4,module: rnn triaged module: data parallel,True
15259,err:torch.nn.CrossEntropyLoss module: docs triaged,2018-12-15 07:33:51+00:00,,0,0,module: docs triaged,True
15253,[Feature Request] cdist: pairwise distances between two sets of tensors with batch mode triaged module: batching function request module: distance functions,2018-12-15 04:56:44+00:00,,0,15,triaged module: batching function request module: distance functions,True
15249,[caffe2] controlled forward pass caffe2,2018-12-15 01:33:52+00:00,,0,2,caffe2,False
15245,Gather backward is faster than integer indexing on GPU module: performance triaged module: determinism,2018-12-15 00:08:44+00:00,,0,13,module: performance triaged module: determinism,True
15167,cudnn not found module: build module: cudnn triaged actionable,2018-12-13 11:47:09+00:00,,0,1,module: build module: cudnn triaged actionable,True
15161,Maybe a bug when using DataParallel triaged module: data parallel,2018-12-13 07:58:46+00:00,,0,3,triaged module: data parallel,True
15149,Use std::variant to represent C++ side enumerations (with binding support) module: cpp triaged,2018-12-13 00:25:17+00:00,,0,5,module: cpp triaged,True
15142,Update third_party/googletest - Ability to skip tests in GTEST module: tests triaged module: third_party,2018-12-12 22:54:49+00:00,,0,3,module: tests triaged module: third_party,True
15120,Different implementations of upsampleBilinear between pytorch and caffe2 caffe2,2018-12-12 13:42:15+00:00,,1,4,caffe2,False
15116,torch.save does not work if nn.Module has partial JIT. oncall: jit,2018-12-12 09:05:18+00:00,,1,3,oncall: jit,True
15070,In-place operations on `.data` or `.detach()` of sparse tensor doesn't update the original tensor module: sparse triaged,2018-12-11 20:03:50+00:00,,0,1,module: sparse triaged,True
15043,[JIT] jit.trace fails with custom GRUs and CUDA when the sequence is longer oncall: jit,2018-12-11 04:24:48+00:00,,1,9,oncall: jit,True
15009,[Caffe2] Caffe2Config.cmake caffe2,2018-12-10 19:58:47+00:00,,0,1,caffe2,False
15004,[Caffe2] How to link Caffe2 in cmake file for C++ compilation? caffe2,2018-12-10 18:37:18+00:00,,0,4,caffe2,False
14996,as_tensor does not use the device of the default tensor type module: docs module: cuda triaged,2018-12-10 16:47:15+00:00,,0,4,module: docs module: cuda triaged,True
14989,[Caffe2] CNN Training on CPU is faster than GPU caffe2,2018-12-10 13:01:32+00:00,,0,1,caffe2,False
14970,"[Caffe2] ""Same"" padding  caffe2",2018-12-09 23:01:31+00:00,,0,0,caffe2,False
14963,PyTorch 1.0 source build fails (error in caffe2/utils/fatal_signal_asan_no_sig_test.cc) caffe2,2018-12-09 15:32:33+00:00,,0,2,caffe2,False
14959,get/set device in c++ module: cpp triaged small,2018-12-09 13:43:16+00:00,,0,9,module: cpp triaged small,True
14945,[Feature request] create sparse coo matrix w/o index check module: performance module: sparse triaged enhancement,2018-12-08 23:12:21+00:00,,0,23,module: performance module: sparse triaged enhancement,True
14940,caffe2::Predictor: Cannot find operator schema for GivenTensorFill. Will skip schema checking. caffe2,2018-12-08 19:38:50+00:00,,0,3,caffe2,False
14939,caffe2/predictor missing from libtorch caffe2,2018-12-08 17:56:58+00:00,,0,1,caffe2,False
14925,Am I able to run caffe2 and gloo example: resnet50_trainer.py using pytorch v1.0rc1? caffe2,2018-12-08 03:54:29+00:00,,0,1,caffe2,False
14907,Missing dilation from several pooling modules (AvgPool) hackamonth triaged module: pooling function request,2018-12-07 20:31:05+00:00,,0,1,hackamonth triaged module: pooling function request,True
14867,[C++/Pytorch] Get input shapes oncall: jit,2018-12-07 01:02:50+00:00,,0,6,oncall: jit,False
14864,[discussion] Recommend a different file extension for models (.PTH is a special extension for Python) triaged,2018-12-07 00:47:56+00:00,,1,17,triaged,True
14844,Negative indexing for nn.Embedding inputs module: nn triaged needs design function request,2018-12-06 08:05:58+00:00,,0,5,module: nn triaged needs design function request,True
14839,Test OpenCV4 in CI module: ci triaged,2018-12-06 05:35:24+00:00,,0,0,module: ci triaged,True
14799,which operator of caffe2  have the same function as torch.nn.Parameter caffe2,2018-12-05 10:36:47+00:00,,0,0,caffe2,False
14797,Modern interface for Storage module: internals triaged,2018-12-05 06:14:19+00:00,,0,2,module: internals triaged,True
14796,Cannot build Caffe2 with TensorRT caffe2,2018-12-05 06:08:05+00:00,,1,1,caffe2,False
14790,Tensor.copy_() seems to work improperly with numpy/list indices module: docs triaged,2018-12-05 04:41:03+00:00,,0,3,module: docs triaged,True
14788,[caffe2] [NNPACK] how to set thread pool for NNPACK? caffe2,2018-12-05 03:35:28+00:00,,0,0,caffe2,False
14750,[caffe2] ConvTranspose with group attribute caffe2 module: op-unification,2018-12-04 11:33:49+00:00,,0,1,caffe2 module: op-unification,False
14731,Caffe2 C++ runs single threaded caffe2,2018-12-04 02:25:50+00:00,,0,3,caffe2,False
14726,[caffe2] Corresponding C++ API for prepare_prediction_net caffe2,2018-12-04 01:02:52+00:00,,0,0,caffe2,False
14702,pytorch_doc_push is racing with itself triaged,2018-12-03 15:22:11+00:00,,0,4,triaged,True
14701,CI: Flaky download from download.pytorch.org module: ci triaged module: flaky-tests better-engineering,2018-12-03 15:17:26+00:00,,0,11,module: ci triaged module: flaky-tests better-engineering,True
14699,[Feature Request] linux distribution friendly build system module: build triaged enhancement,2018-12-03 14:11:30+00:00,,0,10,module: build triaged enhancement,True
14687,Advanced indexing slower than numpy module: performance triaged module: numpy module: advanced indexing,2018-12-03 00:39:03+00:00,,0,6,module: performance triaged module: numpy module: advanced indexing,True
14685,[Caffe2] Exception when creating gradient for [Cast] SquaredL2Distance as output layer of CNN network caffe2,2018-12-02 21:40:44+00:00,,0,1,caffe2,True
14670,[ONNX CI] TestCaffe2End2End.test_squeezenet occasional error caffe2,2018-12-01 16:12:39+00:00,,0,1,caffe2,True
14652,fc_without_bias / FCWithoutBias caffe2,2018-11-30 23:46:24+00:00,,0,0,caffe2,False
14632,[c10d] Check that allgather/gather output tensors point to different storage oncall: distributed feature triaged,2018-11-30 19:09:34+00:00,,0,2,oncall: distributed feature triaged,True
14573,libtorch exports protobuf symbols high priority module: build module: protobuf module: cpp module: abi triaged,2018-11-29 22:05:44+00:00,,1,14,high priority module: build module: protobuf module: cpp module: abi triaged,True
14560,pin_memory/is_pinned API is too CUDA-centric high priority triaged module: memory format needs design,2018-11-29 19:30:01+00:00,,1,5,high priority triaged module: memory format needs design,True
14538,[Caffe2] How to fetch trainable parameters?  caffe2,2018-11-29 15:33:34+00:00,,0,0,caffe2,False
14508,[Caffe2] GPU test passed. Cannot see on nvidia-smi caffe2,2018-11-28 23:12:30+00:00,,0,1,caffe2,False
14489,"Batch matmul with sparse matrix, dense vector todo module: sparse triaged",2018-11-28 19:33:21+00:00,,1,16,todo module: sparse triaged,True
14487,TestAdagrad.test_row_wise_sparse_adagrad intermittently fails health check caffe2,2018-11-28 18:39:41+00:00,,0,1,caffe2,False
14486,"building bundled nccl fails in (caffe2, cuda 8, cudnn 7) CI environment caffe2",2018-11-28 18:35:28+00:00,,0,2,caffe2,True
14484,prim::ConstantChunk derivative formula doesn't handle undefined inputs oncall: jit,2018-11-28 18:20:39+00:00,,0,0,oncall: jit,False
14478,Flaky download from files.pythonhosted.org when installing botocore triaged module: flaky-tests better-engineering,2018-11-28 17:06:10+00:00,,0,4,triaged module: flaky-tests better-engineering,True
14477,"Add a debug mode which is -O0 for framework code, but -O for kernels module: build triaged",2018-11-28 16:25:15+00:00,,0,0,module: build triaged,True
14467,"Optimizer warning when parameters ""change"" module: optimizer triaged enhancement",2018-11-28 10:45:44+00:00,,0,0,module: optimizer triaged enhancement,True
14461,The speed of scatter is influenced by the data size while using nn.DataParallel module: performance module: dataloader triaged module: data parallel,2018-11-28 08:41:53+00:00,,0,4,module: performance module: dataloader triaged module: data parallel,True
14459,[Caffe2] Error protos.protos_size() == OutputSize() when loading dataset created by regular Caffe (datum) caffe2,2018-11-28 07:49:59+00:00,,0,1,caffe2,True
14455,[JIT] Tracing a script function/module where not all args are Tensors oncall: jit,2018-11-28 06:36:53+00:00,,0,14,oncall: jit,True
14436,Use of STL templates in cpu/ directory (compiling with different AVX settings) is silently hazardous module: build triaged,2018-11-27 23:17:28+00:00,,0,3,module: build triaged,True
14431,BUILD_CAFFE2_OPS=OFF is not tested in CI caffe2,2018-11-27 22:28:21+00:00,,0,0,caffe2,False
14408,DeviceOption::set_device_type() doesn't accept caffe2::CPU anymore caffe2,2018-11-27 11:04:47+00:00,,0,2,caffe2,False
14380,[caffe2] Caffe2 GlobalInit should be run before any other API calls caffe2,2018-11-26 19:15:08+00:00,,0,0,caffe2,False
14372,[c10d] Configurable timeout per operation for MPI backend oncall: distributed feature triaged distributed-backlog,2018-11-26 17:00:45+00:00,,1,0,oncall: distributed feature triaged distributed-backlog,True
14370,[Caffe2] Check failed: output->size() == values_.size() output size: 1 given size: 1563551 caffe2,2018-11-26 16:45:06+00:00,,0,3,caffe2,True
14367,Error: DeviceGuardImpl for cpu is not available (static linking PyTorch) module: build triaged module: static linking has workaround,2018-11-26 11:02:24+00:00,,0,32,module: build triaged module: static linking has workaround,True
14366,How to use model.net.Clip? caffe2,2018-11-26 10:58:50+00:00,,0,0,caffe2,False
14352,[libtorch] Catkin_make compilation error needs reproduction module: build triaged has workaround,2018-11-25 09:12:13+00:00,,0,8,needs reproduction module: build triaged has workaround,True
14348,[Caffe2] Error when loading a leveldb dataset using brew.db_input (Error protos.protos_size() == OutputSize().) caffe2,2018-11-24 13:10:09+00:00,,0,1,caffe2,True
14337,Caffe2 C++ tutorial is not working caffe2,2018-11-23 15:37:13+00:00,,0,1,caffe2,False
14332,[jit][script] support slicing with tensor literals oncall: jit,2018-11-23 06:26:38+00:00,,0,0,oncall: jit,False
14328,[Caffe2] How to use the euclidean loss (L2) as output of a CNN model? caffe2,2018-11-22 23:49:23+00:00,,0,0,caffe2,False
14320,[caffe2] Fails to build with fbgemm enabled caffe2,2018-11-22 13:12:06+00:00,,0,0,caffe2,False
14318,No code example for AdaptiveLogSoftmaxWithLoss module: docs triaged,2018-11-22 12:14:38+00:00,,0,1,module: docs triaged,False
14312,[Caffe2] Cannot get repeated argument in custom operator in CUDA context caffe2,2018-11-22 06:55:39+00:00,,0,0,caffe2,False
14277,Provide Protobuf library if libtorch was built with included version module: build module: protobuf module: cpp triaged,2018-11-21 15:09:50+00:00,,0,3,module: build module: protobuf module: cpp triaged,True
14270,Pytorch C++ API with cuda : Expected object of backend CPU but got backend CUDA for sequence element 1 in sequence argument at position #1 'tensors'  oncall: jit,2018-11-21 09:34:03+00:00,,0,6,oncall: jit,False
14262,[Caffe2] How can I use detectron with pytorch? caffe2,2018-11-21 02:59:06+00:00,,0,3,caffe2,False
14227,[bug] inconsistent behavior of indexing  triaged module: advanced indexing,2018-11-20 12:41:09+00:00,,0,3,triaged module: advanced indexing,True
14222,Caffe2->ONNX conversion issue due to spatial-bn caffe2,2018-11-20 09:04:27+00:00,,0,0,caffe2,True
14219,C++ model load error oncall: jit triaged,2018-11-20 06:53:28+00:00,,0,17,oncall: jit triaged,True
14200,Failed to run 'bash ../tools/build_pytorch_libs.sh --use-nnpack --use-mkldnn --use-qnnpack caffe2' module: build triaged,2018-11-19 21:11:15+00:00,,0,11,module: build triaged,True
14185,[Caffe2] Non-spatial batchnorm optimization caffe2,2018-11-19 10:27:12+00:00,,2,0,caffe2,False
14152,Failed to run 'bash ../tools/build_pytorch_libs.sh --use-cuda --use-nnpack --use-mkldnn --use-qnnpack caffe2' module: build triaged,2018-11-17 21:59:13+00:00,,0,23,module: build triaged,True
14131,running caffe2 float16 tensors results in aten runtime error caffe2,2018-11-16 23:53:34+00:00,,0,1,caffe2,True
14112,[sparse] add descriptions and examples for methods at torch.sparse doc page module: sparse triaged,2018-11-16 19:35:25+00:00,,0,2,module: sparse triaged,False
14095,[feature request] bincount along specified dimension(s) triaged module: numpy module: sorting and selection function request,2018-11-16 10:39:49+00:00,,0,6,triaged module: numpy module: sorting and selection function request,True
14047,Make torch.multiprocessing.SpawnContext usable module: multiprocessing feature triaged,2018-11-15 22:51:59+00:00,,1,0,module: multiprocessing feature triaged,True
14030,Cannot import caffe2_pybind11_state_gpu caffe2,2018-11-15 14:16:09+00:00,,0,1,caffe2,False
13993,[tracking task] FBGEMM guarding AVX2 properly module: build triaged module: third_party,2018-11-14 22:21:13+00:00,,0,17,module: build triaged module: third_party,True
13918,Pytorch very slow to convert list of numpy arrays into tensors high priority module: performance triaged enhancement module: numpy has workaround,2018-11-13 19:52:33+00:00,,1,14,high priority module: performance triaged enhancement module: numpy has workaround,True
13865,[c10d] Coordinated file truncation for FileStore oncall: distributed feature triaged distributed-backlog,2018-11-12 22:18:53+00:00,,2,5,oncall: distributed feature triaged distributed-backlog,True
13836,"Pytorch-Caffe2 export: ""Arrays are not almost equal to 3 decimals"" caffe2",2018-11-12 10:32:06+00:00,,0,7,caffe2,False
13818,test_spectral_norm: Backward is not reentrant module: autograd triaged module: data parallel module: norms and normalization,2018-11-11 03:11:32+00:00,,0,6,module: autograd triaged module: data parallel module: norms and normalization,True
13811,Feature request: von Mises-Fisher distribution module: distributions triaged,2018-11-10 19:31:29+00:00,,0,8,module: distributions triaged,True
13786,PyTorch streams are not cuda-memcheck clean module: cuda triaged,2018-11-09 20:48:28+00:00,,0,4,module: cuda triaged,True
13782,torch.linspace does not check for infinity and nan module: error checking triaged module: tensor creation,2018-11-09 18:58:46+00:00,,0,4,module: error checking triaged module: tensor creation,True
13772,Error in building Caffe2 on Windows (experimental operators) caffe2,2018-11-09 12:43:04+00:00,,0,7,caffe2,True
13746,[caffe2] test depthwise3x3_conv_op_test fails to run caffe2,2018-11-08 20:39:17+00:00,,0,1,caffe2,False
13726,"onnx_graph_to_caffe2_net takes a model, not a graph caffe2",2018-11-08 17:43:33+00:00,,0,3,caffe2,False
13716,depthwise convolution are slow on cpu  module: performance module: cpu module: convolution triaged,2018-11-08 09:50:30+00:00,,0,6,module: performance module: cpu module: convolution triaged,True
13715,caffe2 c++ load pb model fail caffe2,2018-11-08 09:32:48+00:00,,0,1,caffe2,False
13684,[caffe2] Adding CUDA operators for generate proposals and NMS layers caffe2,2018-11-07 22:08:29+00:00,,0,0,caffe2,False
13671,Caffe2 Build Static caffe2,2018-11-07 15:19:24+00:00,,0,0,caffe2,False
13667,[caffe2] Modify models in model zoo caffe2,2018-11-07 13:42:02+00:00,,0,0,caffe2,False
13664,Caffe2 install caffe2,2018-11-07 10:24:13+00:00,,0,2,caffe2,False
13655,[caffe2] How to export onnx model trained on Detectron in Caffe2? caffe2,2018-11-07 02:04:40+00:00,,0,0,caffe2,False
13598,caffe2: RuntimeError: [enforce fail at reshape_op.h:110]  with Alexnet onnx test with cuda caffe2,2018-11-05 23:55:36+00:00,,0,8,caffe2,False
13565,Unicode support for the MS Windows platform module: windows caffe2,2018-11-05 11:08:24+00:00,,0,1,module: windows caffe2,False
13508,OpenCL support for smartphones triaged,2018-11-02 14:08:55+00:00,,0,3,triaged,False
13505,[feature request] Singular values and spectral norm for convolutional layers  module: convolution triaged module: linear algebra module: fft function request,2018-11-02 12:48:03+00:00,,0,8,module: convolution triaged module: linear algebra module: fft function request,True
13498,Caffe2 missing headers in docker/conda caffe2,2018-11-02 04:28:33+00:00,,0,0,caffe2,False
13493,How to do inference with float16 (or half) in caffe2? caffe2,2018-11-02 00:52:25+00:00,,0,2,caffe2,False
13477,[jit] restrict promotion of single-element arguments to lists oncall: jit,2018-11-01 20:57:14+00:00,,0,0,oncall: jit,True
13447,Support gathering nested lists in DataParallel  triaged module: data parallel,2018-11-01 13:29:38+00:00,,0,3,triaged module: data parallel,True
13405,[caffe2] How to use Split operator? caffe2,2018-10-31 18:41:37+00:00,,0,0,caffe2,False
13402,batch_norm doesn't bump version counter of running stats high priority module: autograd module: nn triaged actionable fixathon,2018-10-31 18:12:32+00:00,,1,21,high priority module: autograd module: nn triaged actionable fixathon,True
13373,cannot run mobilenet_v2_quantized on pytorch/caffe2 caffe2,2018-10-31 05:59:13+00:00,,1,2,caffe2,False
13304,ASSERT FAILED at /opt/conda/conda-bld/pytorch-nightly_1539602533843/work/aten/src/ATen/core/blob.h:79 caffe2,2018-10-30 10:02:12+00:00,,0,8,caffe2,False
13300,Use a dill-based multiprocessing library and serialization module: multiprocessing triaged,2018-10-30 07:51:01+00:00,,0,5,module: multiprocessing triaged,True
13297,[Caffe2]How to accelerate group convolution in Caffe2? caffe2,2018-10-30 07:17:31+00:00,,0,0,caffe2,False
13268,Non-Zero Padding in Convolution Module module: nn module: convolution triaged,2018-10-29 21:23:48+00:00,,0,1,module: nn module: convolution triaged,True
13246,DataLoader num_workers > 0 causes CPU memory from parent process to be replicated in all worker processes high priority module: dependency bug module: multiprocessing module: dataloader module: molly-guard module: memory usage triaged,2018-10-29 13:23:59+00:00,,0,132,high priority module: dependency bug module: multiprocessing module: dataloader module: molly-guard module: memory usage triaged,True
13234,torch.utils.checkpoint is not compatible with nn.DataParallel module: checkpoint triaged module: data parallel,2018-10-29 03:03:08+00:00,,0,4,module: checkpoint triaged module: data parallel,True
13226,[Caffe2] cmake3 detection error? caffe2,2018-10-28 14:12:52+00:00,,0,1,caffe2,False
13224,Caffe2: Causes error when using flag remove_legacy_pad while converting from caffe to caffe2 caffe2,2018-10-28 10:18:58+00:00,,0,2,caffe2,True
13222,Memory inefficient in batched matmul when requiring gradients triaged module: linear algebra,2018-10-28 07:27:00+00:00,,0,2,triaged module: linear algebra,True
13218,Support for integer interpolation (torch.nn.functional.interpolate) module: nn triaged function request,2018-10-27 23:56:45+00:00,,0,3,module: nn triaged function request,False
13208,[caffe2] How to use the operators that are not included in brew through the python API? caffe2,2018-10-27 14:49:21+00:00,,0,1,caffe2,False
13207,[pytorch] [feature request] Error out if the needed GPU device capability is absent in runtime module: cuda module: error checking triaged,2018-10-27 14:12:23+00:00,,0,6,module: cuda module: error checking triaged,True
13188,cudnn explicit paths and GCC multilib suffixes prevents detection of good cudnn headers module: build triaged,2018-10-26 21:41:54+00:00,,0,2,module: build triaged,True
13170,Allow traced modules to return dictionaries oncall: jit,2018-10-26 18:18:35+00:00,,0,0,oncall: jit,False
13164,Tracing custom ops oncall: jit,2018-10-26 10:17:02+00:00,,0,1,oncall: jit,False
13162,circular module reference raises RecursionError module: nn triaged,2018-10-26 08:37:54+00:00,,0,1,module: nn triaged,True
13130,"arm64 port for PyTorch, libtorch module: ci triaged enhancement module: arm",2018-10-25 17:38:07+00:00,,0,14,module: ci triaged enhancement module: arm,True
13122,[onnx][caffe2] Is there schedule to support onnxwhile in onnx exporter? caffe2,2018-10-25 15:04:54+00:00,,0,0,caffe2,False
13120,[caffe2]  incompatible constructor arguments caffe2,2018-10-25 14:37:58+00:00,,0,0,caffe2,False
13118,"warning: attribute namespace ""clang"" is unrecognized; High Sierra / Fedora compilation with clang results in spurious clang errors in nvcc module: build module: cuda triaged module: build warnings",2018-10-25 13:33:27+00:00,,1,18,module: build module: cuda triaged module: build warnings,True
13079,Tests that download from internet should retry on failure  module: ci triaged,2018-10-24 22:11:16+00:00,,0,2,module: ci triaged,True
13058,Backward pass over torch.nn.functional.pad is extremely slow with half tensors module: performance module: cuda triaged module: half,2018-10-24 17:38:46+00:00,,0,2,module: performance module: cuda triaged module: half,True
13053,Generalized Data Class module: dataloader triaged module: data,2018-10-24 15:53:20+00:00,,0,3,module: dataloader triaged module: data,True
13041,[caffe2] How to handle multiple inputs and multiple outputs in the network architecture? caffe2,2018-10-24 09:44:02+00:00,,0,2,caffe2,False
13034,[caffe2]How can I export init_net.pb and predict_net.pb files on my own? caffe2 triaged,2018-10-24 03:03:13+00:00,,0,0,caffe2 triaged,True
13023,Python dataloader Improvements module: dataloader triaged,2018-10-23 23:36:40+00:00,,1,6,module: dataloader triaged,True
13018,Improved performance for torch.multinomial with small batches module: performance module: cpu triaged,2018-10-23 23:23:33+00:00,,0,4,module: performance module: cpu triaged,True
12983,Fail to Run Caffe2 with Successful Build: This caffe2 python run does not have GPU support caffe2,2018-10-23 12:36:09+00:00,,0,5,caffe2,True
12980,Installing pytorch from source on labs.cognitiveclass.ai triaged module: POWER,2018-10-23 07:26:42+00:00,,0,0,triaged module: POWER,True
12962,Come up with a better strategy for noticing BC-breaking attribute additions to serializable classes module: bc-breaking module: molly-guard triaged,2018-10-23 00:49:15+00:00,,0,0,module: bc-breaking module: molly-guard triaged,True
12931,USE_OPENMP=OFF is ignored [Caffe2] caffe2,2018-10-22 13:55:14+00:00,,0,1,caffe2,False
12913,ubuntu16.04 build from source caffe2 caffe2,2018-10-21 14:26:53+00:00,,0,10,caffe2,False
12895,RelaxedOneHotCategorical not implementing entropy (and other abstract methods) module: distributions triaged enhancement,2018-10-20 00:04:04+00:00,,0,1,module: distributions triaged enhancement,True
12879,Eigen in Caffe2 doesn't produce vectorized instructions caffe2,2018-10-19 17:13:13+00:00,,2,2,caffe2,False
12873,Massive initial memory overhead GPU module: cuda module: memory usage triaged,2018-10-19 14:34:09+00:00,,0,48,module: cuda module: memory usage triaged,True
12869,caffe2: Unsupported type of tensor: nullptr (uninitialized) caffe2,2018-10-19 11:23:25+00:00,,0,2,caffe2,False
12868,[CAPI] Increase of memory usage when exporting a Adam optimzer module: cpp module: optimizer module: memory usage triaged,2018-10-19 09:27:02+00:00,,0,0,module: cpp module: optimizer module: memory usage triaged,True
12855,Model with Caffe2 runs much slower than it with pytorch in GPU mode !!!! caffe2,2018-10-19 02:28:13+00:00,,0,5,caffe2,False
12851,TestAdadelta.test_adadelta flaky on CI caffe2 module: flaky-tests better-engineering,2018-10-19 00:51:01+00:00,,0,2,caffe2 module: flaky-tests better-engineering,False
12830,Test that (cd build && ninja) immediately after build is no-op in CI module: build module: ci triaged,2018-10-18 18:54:22+00:00,,0,0,module: build module: ci triaged,True
12828,BUILD_BINARY is a lie module: build triaged,2018-10-18 18:00:40+00:00,,0,3,module: build triaged,True
12819,Initialization error when moving data to the GPU module: cuda triaged,2018-10-18 12:08:33+00:00,,0,8,module: cuda triaged,True
12812,Pytorch BatchNorm2D Unstable triaged module: norms and normalization,2018-10-18 08:34:41+00:00,,0,1,triaged module: norms and normalization,False
12808,tutorial_blob ERROR caffe2,2018-10-18 07:03:23+00:00,,0,1,caffe2,True
12797,Port dragon4_scientific for pretty float tensor print. module: printing triaged enhancement,2018-10-17 23:52:08+00:00,,0,1,module: printing triaged enhancement,True
12795,C++ frontend: how to debug nan gradients module: cpp triaged enhancement,2018-10-17 23:22:57+00:00,,0,3,module: cpp triaged enhancement,True
12773,"Build the docker image from source, but torch.cuda.is_available()==false triaged module: docker",2018-10-17 16:50:51+00:00,,0,10,triaged module: docker,True
12764,[feature request] Operator Overloading todo feature triaged,2018-10-17 12:17:06+00:00,,0,2,todo feature triaged,False
12763,"Caffe2: Two entries of external_input ""data_0"" in mnist_predict_net.pbtxt file caffe2",2018-10-17 12:15:45+00:00,,0,0,caffe2,False
12760,[feature request] Spectral norm support in torch.norm or factor out Power Iteration from spectralnormalization in some other place and orthogonalization from PowerSGD hook triaged module: norms and normalization,2018-10-17 10:28:48+00:00,,0,6,triaged module: norms and normalization,False
12755,caffe2 python custom operator could not update parameters caffe2,2018-10-17 07:36:43+00:00,,0,0,caffe2,False
12716,cdf in torch.distributions.bernoulli throws NotImplementedError todo module: distributions triaged,2018-10-16 17:18:50+00:00,,0,3,todo module: distributions triaged,True
12715,pack_padded_sequence throws IndexError when only kwargs are specified module: rnn triaged,2018-10-16 16:55:41+00:00,,0,1,module: rnn triaged,True
12704,Error occuring while converting mnist or cifar model from caffe2 to onnx caffe2,2018-10-16 08:49:35+00:00,,0,0,caffe2,True
12702,How to run a pytorch-onnx-caffe2 model on GPU? caffe2 triaged,2018-10-16 07:58:19+00:00,,0,5,caffe2 triaged,True
12690,How can I build caffe2_gtest_main under pytorch/caffe2/test/ folder? caffe2 triaged,2018-10-15 23:10:17+00:00,,0,2,caffe2 triaged,True
12675,[feature request] ignore_index and size_average in nn.AdaptiveLogSoftMaxWithLoss module: nn triaged,2018-10-15 20:47:37+00:00,,0,1,module: nn triaged,True
12672,Move collate_fn functionality / responsibility into Dataset object module: dataloader triaged,2018-10-15 20:23:09+00:00,,0,5,module: dataloader triaged,True
12659,Differentiation through Module parameters updates feature module: autograd module: nn module: optimizer triaged,2018-10-15 16:23:37+00:00,,2,12,feature module: autograd module: nn module: optimizer triaged,True
12658,Move BigTensorSerialization tests out of default caffe2_cpu_tests caffe2,2018-10-15 16:17:51+00:00,,0,0,caffe2,False
12650,"pytorch/torch/utils/cpp_extension.py ignores compiler setting,  module: cpp-extensions triaged",2018-10-15 13:39:31+00:00,,0,2,module: cpp-extensions triaged,True
12646,Caffe2 Installation inside Pytorch caffe2,2018-10-15 10:52:28+00:00,,0,8,caffe2,False
12642,fail to visualize caffe2 model caffe2,2018-10-15 03:31:39+00:00,,0,4,caffe2,False
12641,Install Jetson TX2 Max Regcount Error needs reproduction module: build triaged module: jetson,2018-10-15 01:15:52+00:00,,0,5,needs reproduction module: build triaged module: jetson,True
12640,how to store a bounding box in Tensor? caffe2,2018-10-15 00:36:34+00:00,,0,0,caffe2,False
12609,Request for stripped down / inference only pytorch wheels module: build triaged,2018-10-12 16:33:17+00:00,,1,4,module: build triaged,True
12576,[feature request] `ignore_label` argument in Caffe2 `SoftmaxWithLoss` caffe2,2018-10-11 18:42:56+00:00,,0,1,caffe2,False
12535,[CMake] Linking against Intel OpenMP module: build triaged module: mkldnn module: openmp,2018-10-10 15:26:46+00:00,,1,8,module: build triaged module: mkldnn module: openmp,True
12532,Provide better documentation for torch.Size module: docs triaged,2018-10-10 14:12:04+00:00,,0,7,module: docs triaged,True
12530,[caffe2] Memory usage caffe2,2018-10-10 12:03:51+00:00,,0,1,caffe2,False
12515,how to use mask-rcnn in caffe2 c++ gpu caffe2,2018-10-10 03:14:40+00:00,,0,2,caffe2,False
12509,"The text design (color, type) makes it hard to read todo triaged",2018-10-10 00:31:43+00:00,,0,2,todo triaged,True
12501,C++: Calling Workspace::RunNet for a prediction on a different thread each time causes a GPU memory leak caffe2,2018-10-09 20:56:01+00:00,,0,5,caffe2,False
12498,Support calculating grad for dense in sparse @ dense  module: sparse triaged,2018-10-09 20:42:53+00:00,,1,6,module: sparse triaged,True
12484,CuDNN convolution on some CUDA devices will not preserve NaN weights (upstream bug) module: dependency bug module: cudnn low priority triaged,2018-10-09 15:50:30+00:00,,0,14,module: dependency bug module: cudnn low priority triaged,True
12482,[Feature request]:  add `LayerNormLSTMCell` module: rnn triaged,2018-10-09 12:27:01+00:00,,0,4,module: rnn triaged,False
12461,Certain operations cause implicity sync-points module: cuda triaged,2018-10-08 18:44:14+00:00,,0,5,module: cuda triaged,True
12460,"input_device, output_device, devices_used properties module: nn triaged",2018-10-08 18:32:05+00:00,,0,2,module: nn triaged,True
12449,"Could not find a package configuration file provided by ""Torch"" with any of   the following names: module: cpp-extensions triaged",2018-10-08 08:15:55+00:00,,0,36,module: cpp-extensions triaged,True
12435,`pstrf` on positive semi-definite matrices triaged module: linear algebra function request,2018-10-07 14:28:37+00:00,,0,9,triaged module: linear algebra function request,True
12401,AttributeError: Method VideoInput is not a registered operator caffe2,2018-10-05 21:37:35+00:00,,0,4,caffe2,False
12322,[Caffe2] Segmentation fault (core dumped) while import caffe2.python.core caffe2,2018-10-04 12:39:39+00:00,,0,2,caffe2,True
12308,nn.functional.linear() for sparse tensor module: sparse module: nn triaged,2018-10-04 00:00:54+00:00,,0,9,module: sparse module: nn triaged,True
12297,[caffe2] Inference using multiple GPU caffe2,2018-10-03 21:34:49+00:00,,0,0,caffe2,False
12280,`GLIBC_2.23' not found on Ubuntu14.04. caffe2,2018-10-03 10:44:20+00:00,,0,0,caffe2,False
12257,[Caffe2] Relink error after installing Caffe2 from conda caffe2,2018-10-02 09:01:03+00:00,,0,0,caffe2,True
12248,.cuda() changes a module's behavior when there are registered buffers with requires_grad=True module: autograd module: nn triaged,2018-10-02 03:21:12+00:00,,0,8,module: autograd module: nn triaged,True
12189,[Feature] Support Adaptive Max Gradient Norm / Clipping triaged enhancement module: norms and normalization,2018-09-29 00:11:13+00:00,,0,1,triaged enhancement module: norms and normalization,False
12181,Network surgery for transfer fails caffe2,2018-09-28 20:37:56+00:00,,0,0,caffe2,False
12159,clip_grad_norm_ does not work on grads of different types todo module: nn triaged,2018-09-28 05:41:02+00:00,,0,0,todo module: nn triaged,True
12146,Misleading step method in lr_scheduler.ReduceLROnPlateau todo module: optimizer triaged,2018-09-27 21:46:48+00:00,,0,1,todo module: optimizer triaged,True
12136,[Caffe2] GAN No Gradients in Generator caffe2,2018-09-27 13:08:25+00:00,,0,1,caffe2,False
12134,[caffe2] How to set different learning rates for different layers? caffe2,2018-09-27 12:18:43+00:00,,0,0,caffe2,False
12117,"Error: Internal Compiler Error (codegen): ""there was an error in verifying the lgenfe output!"" module: build module: cuda triaged internals",2018-09-26 21:00:05+00:00,,0,3,module: build module: cuda triaged internals,True
12086,Caffe2 issues with using Glog without GFlags caffe2,2018-09-26 14:31:27+00:00,,0,0,caffe2,False
12069,Stop using make_intrusive directly; provide some make_tensor module: internals triaged,2018-09-25 21:10:21+00:00,,0,0,module: internals triaged,True
12060,cuda test hangs if GPUs in Exclusive Process mode todo module: cuda triaged,2018-09-25 18:47:26+00:00,,0,0,todo module: cuda triaged,True
12045,Create a custom function using the functions from math_cpu.cc source code in Caffe2? caffe2,2018-09-25 11:33:44+00:00,,0,4,caffe2,False
12010,Caffe2 compiled with MKLDNN doesn't have device_type = MKLDNN caffe2,2018-09-24 11:58:10+00:00,,0,0,caffe2,False
12007,caffe2's installation is still problematic... caffe2,2018-09-24 09:34:16+00:00,,0,3,caffe2,False
11996,Test the Caffe2 Installation with GPU erro caffe2,2018-09-23 15:30:59+00:00,,0,0,caffe2,False
11982,[feature request] Publish wheels with debug symbols module: binaries triaged,2018-09-22 19:43:08+00:00,,0,12,module: binaries triaged,True
11980,[Enhancement] Increase user-friendliness of dataset.random_split triaged enhancement module: data,2018-09-22 18:36:55+00:00,,0,6,triaged enhancement module: data,True
11978,[Caffe2] Attempting to install Caffe2 in Google Colab caffe2,2018-09-22 17:43:12+00:00,,0,7,caffe2,False
11976,[caffe2] Documentation of Optimizers (Python API) caffe2,2018-09-22 15:57:13+00:00,,0,0,caffe2,False
11974,Specify out= argument to convolution module: convolution triaged enhancement,2018-09-22 13:56:42+00:00,,0,5,module: convolution triaged enhancement,True
11967,Jit cannot trace autograd for certain operator oncall: jit,2018-09-22 05:53:14+00:00,,0,1,oncall: jit,True
11951,dtype mismatch error messages can be misleading todo module: error checking module: molly-guard triaged,2018-09-21 21:33:38+00:00,,0,4,todo module: error checking module: molly-guard triaged,True
11937,[feature request] Kumaraswamy distribution module: distributions feature triaged,2018-09-21 15:57:18+00:00,,0,4,module: distributions feature triaged,True
11936,torch.bmm doesn't support CUDA uint8 (byte) tensor todo module: bootcamp module: cuda triaged enhancement module: linear algebra,2018-09-21 15:49:16+00:00,,0,9,todo module: bootcamp module: cuda triaged enhancement module: linear algebra,True
11890,caffe2 argmax and argmin documentation incorrect for output type caffe2,2018-09-20 15:23:31+00:00,,0,0,caffe2,True
11885,[caffe2] Bug for softmaxwithloss operator caffe2,2018-09-20 06:28:12+00:00,,0,0,caffe2,True
11869,[caffe2] Unaligned AVX instruction operands in LayerNorm implementation in OSS build caffe2,2018-09-19 18:09:49+00:00,,0,0,caffe2,False
11865,[Caffe2] Errors occured when running Cpp Predictor caffe2,2018-09-19 17:42:36+00:00,,0,5,caffe2,False
11861,[Caffe2 installation problem] caffe2,2018-09-19 12:30:42+00:00,,0,4,caffe2,False
11854,error: ‘array_size’ is not a class template module: build module: cpp triaged,2018-09-19 08:22:42+00:00,,0,1,module: build module: cpp triaged,True
11850,[Caffe2/Bug] Cannot enable MKL-DNN caffe2,2018-09-19 02:00:52+00:00,,0,5,caffe2,False
11793,DataParallel: Parallel_apply assert len(modules) == len(inputs) AssertionError oncall: distributed triaged,2018-09-18 08:16:58+00:00,,0,5,oncall: distributed triaged,True
11791,[caffe2]Can i build caffe2 library only for cpu inference purpose and reduce the binary size? caffe2,2018-09-18 07:28:40+00:00,,0,1,caffe2,False
11790,set num_workers on the dataloader make the jupyter kernel crash at the almost end of the epoch  module: dataloader triaged,2018-09-18 07:10:15+00:00,,0,1,module: dataloader triaged,True
11786,Mysterious error due to num_workers: 1 module: multiprocessing triaged,2018-09-18 04:39:45+00:00,,0,6,module: multiprocessing triaged,True
11735,Caffe2 installation:  libcaffe2_gpu.so: undefined reference to `caffe2::ClipTransformRGB caffe2,2018-09-16 01:34:53+00:00,,1,5,caffe2,False
11727,Semaphore leaks in dataloader module: dataloader triaged,2018-09-15 03:07:32+00:00,,0,8,module: dataloader triaged,True
11678,[caffe2] adam_op implementation is incorrect. caffe2,2018-09-14 00:32:47+00:00,,0,0,caffe2,False
11645,One GPU is more memory efficient than Multiple GPUs module: multi-gpu triaged module: data parallel,2018-09-13 16:38:37+00:00,,0,1,module: multi-gpu triaged module: data parallel,True
11636,undefined reference to caffe2 caffe2,2018-09-13 13:40:07+00:00,,0,1,caffe2,False
11635,[Feature request] Advanced indexing in functions like `expand` triaged module: advanced indexing,2018-09-13 12:59:37+00:00,,0,1,triaged module: advanced indexing,False
11633,DataLoader: Could not wrapper a exception in threads module: dataloader module: error checking triaged,2018-09-13 12:26:29+00:00,,0,2,module: dataloader module: error checking triaged,True
11624,Add min mode to embedding bags module: nn triaged enhancement,2018-09-13 03:50:48+00:00,,0,4,module: nn triaged enhancement,True
11612,[feature request] Triangular Matrix Representation feature triaged,2018-09-13 00:08:09+00:00,,0,4,feature triaged,False
11578,Request to import pytest in test/*.py module: tests triaged,2018-09-12 16:50:40+00:00,,1,22,module: tests triaged,True
11551,"CrossEntropyLoss, ignore_index does not prevent back-prop if the logits are -inf module: docs module: nn module: loss triaged",2018-09-11 21:29:27+00:00,,0,1,module: docs module: nn module: loss triaged,True
11537,"[CLEANUP] Context functions should return TypeExtendedInterface, not Type triaged better-engineering",2018-09-11 19:54:31+00:00,,0,0,triaged better-engineering,True
11532,[JIT][tracer] Slicing shape is specialized to tensor rank oncall: jit,2018-09-11 18:24:52+00:00,,0,0,oncall: jit,True
11516,at::Device makes it very easy to write buggy code triaged better-engineering,2018-09-11 15:16:58+00:00,,0,4,triaged better-engineering,True
11514,[feature request] - Allow sequences lengths to be 0 in PackSequence module: nn module: rnn triaged enhancement,2018-09-11 10:00:49+00:00,,0,1,module: nn module: rnn triaged enhancement,True
11496,"Tests with ""."" in the name cannot be run standalone module: tests triaged",2018-09-11 00:13:42+00:00,,0,0,module: tests triaged,True
11451,[caffe2] caffe2 openmp linking error with xcode 9.0(AppleClang 9.0) on mac caffe2,2018-09-10 09:27:55+00:00,,0,2,caffe2,True
11439,High leverage TH operations to port to ATen triaged module: porting better-engineering module: tensor creation,2018-09-09 17:33:25+00:00,,0,0,triaged module: porting better-engineering module: tensor creation,True
11433,Assorted issues in Caffe2's Metal ops caffe2,2018-09-09 07:19:18+00:00,,0,0,caffe2,False
11426,[Feature request] Intuitive error message when input to Linear is not cudarized module: error checking triaged,2018-09-08 15:54:26+00:00,,0,1,module: error checking triaged,True
11390,Views created in no_grad block still have requires_grad=True high priority module: docs module: autograd triaged,2018-09-07 18:23:56+00:00,,1,21,high priority module: docs module: autograd triaged,True
11389,[distributions] Torch distribution samplers slow on expanded parameters todo module: distributions triaged,2018-09-07 18:08:15+00:00,,0,0,todo module: distributions triaged,True
11372,Suggest: DataLoader add device parameter  todo module: dataloader triaged enhancement,2018-09-07 07:47:04+00:00,,0,8,todo module: dataloader triaged enhancement,True
11363,Hypothesis operator tests in Caffe2 generate too many warnings caffe2,2018-09-07 02:22:20+00:00,,0,1,caffe2,False
11354,caffe2 failed to build from source caffe2,2018-09-07 00:02:09+00:00,,0,8,caffe2,False
11340,Better user experience for using Generator object todo module: docs module: cuda triaged module: random,2018-09-06 18:27:52+00:00,,1,16,todo module: docs module: cuda triaged module: random,True
11324,Why does DistributedDataSampler not use default RNG? oncall: distributed triaged,2018-09-06 06:13:36+00:00,,0,3,oncall: distributed triaged,True
11268,How to re-shuffle lmdb per epoch in the caffe2 training process caffe2,2018-09-05 06:19:01+00:00,,0,0,caffe2,False
11267,How to simulate multi-node using single-node with 8 GPUs caffe2,2018-09-05 06:18:21+00:00,,0,0,caffe2,False
11213,Ubuntu 16.04 setup.py error - undefined reference to elfLink_Get_FatBinary_From_Object' /usr/lib/x86_64-linux-gnu/libcuda.so: undefined reference to elf32_section_header' module: build triaged module: undefined reference,2018-09-04 12:41:16+00:00,,0,23,module: build triaged module: undefined reference,True
11202,[pytorch][feature request] Cosine distance / simialrity between samples of own tensor or two tensors module: nn triaged module: numpy function request module: distance functions,2018-09-03 14:40:43+00:00,,0,21,module: nn triaged module: numpy function request module: distance functions,True
11197,[Caffe2] How to correctly add Caffe2 libraries to Visual Studio to write C++ programs? caffe2,2018-09-03 08:02:17+00:00,,0,0,caffe2,False
11180,A Caffe2 Implementation of Pose Estimation caffe2,2018-09-01 22:47:51+00:00,,0,0,caffe2,False
11119,[Caffe2] Error C2492: data with thread storage duration may not have dll interface caffe2,2018-08-30 23:44:06+00:00,,0,0,caffe2,True
11113,use_system_nccl flag does not work? module: build triaged,2018-08-30 22:09:37+00:00,,0,2,module: build triaged,True
11072,[...] operator for masked select does not broadcast anymore todo triaged module: sorting and selection,2018-08-30 09:28:13+00:00,,0,1,todo triaged module: sorting and selection,True
11071,undefined reference to 'caffe2::Caffe2FlagsRegistry[abi:cxx11]()' module: build triaged,2018-08-30 08:29:35+00:00,,0,2,module: build triaged,True
11062,"non-shuffling data loaders can affect random states, thus the results of shuffling data loaders. todo module: dataloader triaged",2018-08-30 02:56:33+00:00,,0,3,todo module: dataloader triaged,True
11058, A bug in roi_align.cc? [Caffe2] caffe2,2018-08-30 02:27:45+00:00,,0,1,caffe2,True
10996,Multiprocess Deadlock when using np.transpose and torch.stack  module: multiprocessing triaged,2018-08-29 12:46:09+00:00,,0,12,module: multiprocessing triaged,True
10991,Error when building  caffe2,2018-08-29 08:12:00+00:00,,0,4,caffe2,True
10990,[Caffe2] [Question] How to disable the bias parameter? caffe2,2018-08-29 07:33:20+00:00,,0,0,caffe2,False
10985,Where could I see all of the caffe2 operators and their arguments in python API? caffe2,2018-08-29 03:00:38+00:00,,0,2,caffe2,True
10984,[caffe2] Implement Gather for any value of `axis` caffe2,2018-08-29 02:57:33+00:00,,0,0,caffe2,False
10978,[feature request] padding for torch.cat  triaged enhancement module: viewing and reshaping,2018-08-28 22:01:36+00:00,,1,10,triaged enhancement module: viewing and reshaping,True
10956,Better dev docs for writing native CPU kernels with Vec256 triaged module: vectorization,2018-08-28 16:50:38+00:00,,0,0,triaged module: vectorization,True
10950,Cannot run torch in different sub-interpreters todo needs reproduction module: cpp triaged,2018-08-28 15:59:18+00:00,,0,0,todo needs reproduction module: cpp triaged,True
10940,[Caffe2] Model work Alright in Python but Failed in C++ caffe2,2018-08-28 10:09:21+00:00,,0,2,caffe2,True
10913,Cannot find operator schema for 'ATen' Caffe2 Ios caffe2 triaged,2018-08-27 20:35:24+00:00,,0,0,caffe2 triaged,True
10902,LNK2019 error when linking with MSVC [Caffe2] caffe2,2018-08-27 14:51:00+00:00,,0,4,caffe2,True
10875,how can i run two model in a simple project? caffe2,2018-08-25 09:06:04+00:00,,0,3,caffe2,False
10818,[Caffe2] Can't load pretrained model caffe2,2018-08-23 11:06:29+00:00,,0,0,caffe2,False
10808,[Feature Request] Crop op or ConvTranspose with output_shape caffe2,2018-08-23 02:47:57+00:00,,0,2,caffe2,False
10800,[distributed] Synchronization on CUDA side with MPI backend oncall: distributed module: docs triaged module: c10d distributed-backlog,2018-08-22 22:41:41+00:00,,0,0,oncall: distributed module: docs triaged module: c10d distributed-backlog,True
10756,Unexpected Behavior when Pointwise Operations Write to Expanded Tensors module: internals good first issue triaged module: partial aliasing,2018-08-21 23:10:22+00:00,,0,8,module: internals good first issue triaged module: partial aliasing,True
10746,[Caffe2] Failed to build dispatch_test. Error LNK2001: unresolved external symbol caffe2,2018-08-21 19:11:09+00:00,,0,0,caffe2,True
10738,[caffe2] the caffe2 operators document is too old caffe2,2018-08-21 17:24:18+00:00,,0,0,caffe2,False
10735,Have all C++ modules expose a __file__ attribute module: docs triaged enhancement,2018-08-21 17:11:23+00:00,,0,8,module: docs triaged enhancement,True
10719,size mismatch when trying to reconstruct predifined network triaged module: vision,2018-08-21 03:26:52+00:00,,0,0,triaged module: vision,True
10714,[feature request][caffe2] extend FC/FCTranspose op to handle 2d bias. caffe2,2018-08-21 01:33:09+00:00,,0,2,caffe2,False
10691,Build system doesn't prevent ATen/core from including non-core files module: build triaged,2018-08-20 18:05:58+00:00,,0,0,module: build triaged,True
10685,[feature request] convtbc with group convolution feature module: convolution triaged,2018-08-20 14:44:06+00:00,,0,0,feature module: convolution triaged,False
10684,make install error： [third_party/gloo/gloo/CMakeFiles/gloo.dir/all] Error 2 module: build triaged module: third_party,2018-08-20 14:22:27+00:00,,0,2,module: build triaged module: third_party,True
10683,Tensor.register_hook is not passing the tensor object to the hook function module: bc-breaking triaged enhancement,2018-08-20 14:03:44+00:00,,0,3,module: bc-breaking triaged enhancement,True
10675,GenerateProposals CUDA implementation caffe2,2018-08-20 02:17:58+00:00,,0,4,caffe2,False
10667,[Caffe2] Error importing ConvTranspose2d to Caffe2 with ONNX caffe2,2018-08-19 18:11:12+00:00,,0,2,caffe2,True
10615,Request for better memory management feature module: memory usage triaged,2018-08-17 10:38:47+00:00,,0,2,feature module: memory usage triaged,True
10604,"Bilinear interpolation behavior inconsistent with TF, CoreML and Caffe triaged module: interpolation",2018-08-17 00:45:34+00:00,,0,7,triaged module: interpolation,True
10582,[Caffe2] Unable to use MPI rendezvous in Caffe2 caffe2,2018-08-16 14:50:46+00:00,,0,3,caffe2,False
10577,[BUG]: unstable happend in saving model. module: serialization triaged,2018-08-16 04:48:09+00:00,,0,13,module: serialization triaged,True
10573,[Caffe2] Error C2375 when building DLL.  caffe2,2018-08-16 03:18:19+00:00,,0,0,caffe2,True
10536,[feature request] Adding Pre and Post padding functionalities to pad_sequence function feature module: nn triaged module: padding,2018-08-15 10:00:24+00:00,,0,6,feature module: nn triaged module: padding,False
10515,[Caffe2] Segmentation faults in multithreading Caffe2 caffe2,2018-08-14 20:40:06+00:00,,0,7,caffe2,False
10482,Reduce code duplication in interpolate and make it more generic feature triaged module: interpolation,2018-08-13 19:34:03+00:00,,1,15,feature triaged module: interpolation,True
10471,[Caffe2] Multithreading in Caffe2 caffe2,2018-08-13 17:27:05+00:00,,0,4,caffe2,False
10464,caffe2 CI does not test header install caffe2,2018-08-13 15:42:10+00:00,,0,0,caffe2,False
10461,Key already registered. Offending key: caffe2_print_stacktraces. caffe2,2018-08-13 15:01:55+00:00,,0,0,caffe2,False
10460,Type name float registered twice. This should not happen. Do you have duplicated CAFFE_KNOWN_TYPE? caffe2,2018-08-13 14:42:04+00:00,,0,0,caffe2,False
10454,[feature request] Rank-Revealing QR - Adding dgeqp3 support to torch.qr todo feature triaged module: linear algebra actionable,2018-08-13 08:48:59+00:00,,0,4,todo feature triaged module: linear algebra actionable,False
10440,"[caffe2] enforce fail at context_gpu.cu:285, cannot get GPU memory usage statistics caffe2",2018-08-11 20:44:09+00:00,,0,0,caffe2,False
10396,"[feature request] Provide a way to redirect shared memory prefix ""/torch_"" triaged enhancement",2018-08-10 07:13:32+00:00,,0,5,triaged enhancement,False
10386,[feature request] batch_first option in torch.utils.data module: dataloader triaged enhancement,2018-08-09 21:23:11+00:00,,0,1,module: dataloader triaged enhancement,True
10377,[Docs] Update broadcasting documentation for scalars / n-dimensional empty tensors todo module: docs triaged,2018-08-09 14:08:34+00:00,,0,3,todo module: docs triaged,False
10375,"Sending CUDA tensor to process, and then back, does not work module: bootcamp module: multiprocessing module: cuda triaged",2018-08-09 13:17:34+00:00,,0,10,module: bootcamp module: multiprocessing module: cuda triaged,True
10342,[Caffe2] which kind of database can I read from caffe2 caffe2,2018-08-08 03:29:32+00:00,,0,0,caffe2,False
10341,[Caffe2] Which document should we follow to install caffe2? caffe2,2018-08-08 03:01:19+00:00,,0,1,caffe2,False
10314,[Caffe2] ConvPoolOpBase operator caffe2,2018-08-07 17:04:25+00:00,,0,0,caffe2,False
10298,[feature request] Runtime warning for inappropriate labels (among others) module: loss module: error checking triaged,2018-08-07 12:38:14+00:00,,1,5,module: loss module: error checking triaged,True
10256,[Caffe 2] How to access/remove Blobs from Workspace? caffe2,2018-08-06 08:48:38+00:00,,0,0,caffe2,False
10254,"[Caffe2] I don't knw whether GPU support is right, is there something wrong with Hypothesis? caffe2",2018-08-06 07:57:13+00:00,,0,4,caffe2,False
10249,pytorch does not compatible with caffe2 caffe2,2018-08-06 03:36:14+00:00,,0,10,caffe2,False
10232,[Caffe2] AVX not enabled for pytorch/caffe2 caffe2,2018-08-04 05:39:36+00:00,,0,3,caffe2,False
10215,[Caffe2] Build failure on Ubuntu 16.04 caffe2,2018-08-03 19:15:45+00:00,,0,7,caffe2,True
10176,[Caffe2] Modelling using Ops instead of Helper Functions caffe2,2018-08-02 19:46:49+00:00,,0,0,caffe2,False
10172,"[Feature request] Batch eig/symeig functions (for small matrices, with CUDA) triaged module: batching module: linear algebra function request",2018-08-02 18:38:23+00:00,,0,4,triaged module: batching module: linear algebra function request,True
10170,build error caffe2,2018-08-02 18:14:45+00:00,,0,0,caffe2,True
10165,torch.utils.data.random_split() returns dataset index as tensor module: docs module: dataloader triaged,2018-08-02 15:24:20+00:00,,0,4,module: docs module: dataloader triaged,True
10161,error when building caffe2 from source with gcc 7 & cuda gcc 5.5 caffe2,2018-08-02 08:42:21+00:00,,0,3,caffe2,True
10155,"""No module named 'tools.setup_helpers'"" when use pip install caffe2 caffe2",2018-08-02 03:42:59+00:00,,0,4,caffe2,False
10119,"[Caffe2] MNIST Tutorial LMDB Error ""Cannot open db""  caffe2",2018-08-01 14:41:50+00:00,,0,21,caffe2,True
10088,[Caffe2]  Implementing CoordConv layer caffe2,2018-07-31 20:23:21+00:00,,0,3,caffe2,False
10070,UnicodeDecodeError while loading caffe2 model caffe2,2018-07-31 13:31:00+00:00,,0,3,caffe2,False
10062,Does this mean onnx do not support spatialBN operator? caffe2,2018-07-31 08:32:32+00:00,,0,8,caffe2,False
10043,Sparse tensor use cases module: sparse feature triaged,2018-07-31 01:06:25+00:00,,0,83,module: sparse feature triaged,True
10007,Upgrade Caffe2 Hypothesis caffe2,2018-07-30 15:35:10+00:00,,0,1,caffe2,False
10006,RNN gradients in eval mode in pytorch 0.4 module: nn module: rnn triaged,2018-07-30 14:42:09+00:00,,0,10,module: nn module: rnn triaged,True
9983,[feature request] Add matrix functions triaged module: numpy module: linear algebra function request,2018-07-29 03:25:01+00:00,,1,85,triaged module: numpy module: linear algebra function request,True
9975,python caffe2/python/operator_test/activation_ops_test.py  Segmentation fault (core dumped) caffe2,2018-07-28 14:38:48+00:00,,0,2,caffe2,True
9952,[feature request] Add COCOB Optimizer module: optimizer triaged function request,2018-07-27 20:47:25+00:00,,0,4,module: optimizer triaged function request,False
9950,Stop passing inplace/out arguments as (non-const) Tensor& to functions module: internals module: cpp triaged enhancement,2018-07-27 20:03:20+00:00,,0,0,module: internals module: cpp triaged enhancement,True
9945,WERROR=1 doesn't work with FULL_CAFFE2 caffe2,2018-07-27 19:01:16+00:00,,0,0,caffe2,False
9921,A serious problem when installing caffe2. Can anyone  help me? caffe2,2018-07-27 05:58:10+00:00,,0,7,caffe2,True
9912,Stop ifdef'ing out scatter/gather (comm) in libtorch module: cpp module: cuda triaged,2018-07-26 23:46:30+00:00,,0,1,module: cpp module: cuda triaged,True
9910,CRITICAL:root:Cannot load caffe2.python. Error: DLL load failed: A dynamic link library (DLL) initialization routine failed. caffe2,2018-07-26 23:04:26+00:00,,0,0,caffe2,True
9898,[Feature Request] Checkpoint manager feature triaged,2018-07-26 19:43:40+00:00,,1,3,feature triaged,False
9894,[docs] Script for releasing new versions of the docs module: docs triaged,2018-07-26 18:57:52+00:00,,1,0,module: docs triaged,False
9893,interaction with FindCUDA causes spurious re-cmakes module: build triaged,2018-07-26 18:31:21+00:00,,0,1,module: build triaged,True
9886,[doc] functionalities not documented module: docs good first issue triaged,2018-07-26 17:41:35+00:00,,0,20,module: docs good first issue triaged,True
9883,NetTest.OperatorWithExecutorHelper intermittently hangs caffe2,2018-07-26 17:08:48+00:00,,0,0,caffe2,False
9882,Unintuitive reduction of mini-batch loss for NLLLoss module: docs module: nn module: loss triaged,2018-07-26 16:47:03+00:00,,1,1,module: docs module: nn module: loss triaged,True
9881,cannot open caffe2.lib in VS2015 debug mode caffe2,2018-07-26 15:20:53+00:00,,0,0,caffe2,False
9875,onnx to caffe2 err caffe2,2018-07-26 12:08:54+00:00,,0,0,caffe2,False
9873,"Pytorch is slow when only using CPU, and cannot utilize multicore of CPU module: performance module: cpu triaged module: multithreading",2018-07-26 10:52:00+00:00,,0,24,module: performance module: cpu triaged module: multithreading,True
9867,How can I sure that I run caffe2 with GPU caffe2,2018-07-26 07:24:39+00:00,,0,1,caffe2,False
9854,TestConvolution.test_conv_separate_stride_pad_gradients failing caffe2,2018-07-26 00:40:42+00:00,,0,1,caffe2,False
9853,TestSequenceOps.test_gather_padding failing caffe2,2018-07-26 00:39:07+00:00,,0,1,caffe2,False
9833,RecurrentNetworkTest.test_sum_mul flaky on master caffe2 module: flaky-tests better-engineering,2018-07-25 18:37:47+00:00,,0,0,caffe2 module: flaky-tests better-engineering,False
9832,TestReduceFrontSum.test_col2im_gradients flaky caffe2 module: flaky-tests better-engineering,2018-07-25 18:31:35+00:00,,0,1,caffe2 module: flaky-tests better-engineering,False
9797,Build torch as a submodule with static linking doesn't work (CAFFE2_PERF_WITH_AVX2 is not defined) caffe2 module: static linking,2018-07-25 00:14:16+00:00,,1,4,caffe2 module: static linking,True
9761,[caffe2] Questions about conv_transpose && ConvTranspose caffe2,2018-07-24 14:31:44+00:00,,0,10,caffe2,False
9753,dataloader stuck at sched_yield =0 module: dataloader triaged,2018-07-24 09:11:48+00:00,,0,0,module: dataloader triaged,True
9708,"[caffe2]mpirun multi-node multi GPU in Distributed mode ,run resnet50_trainer.py get RuntimeError caffe2",2018-07-23 13:19:10+00:00,,0,6,caffe2,False
9707,"[caffe2]mpirun multi-node multi GPU in Distributed mode ,run resnet50_trainer.py get RuntimeError caffe2",2018-07-23 13:18:38+00:00,,0,0,caffe2,False
9704,"A newer protobuf should be used, so that cmake install will work when building for mobile caffe2",2018-07-23 04:52:19+00:00,,1,4,caffe2,False
9701,[request] speed-up multidim slicing backward todo module: performance module: autograd triaged,2018-07-23 04:20:40+00:00,,0,5,todo module: performance module: autograd triaged,True
9699,[Build error] libcudnn.so: error adding symbols: File in wrong format caffe2,2018-07-23 02:22:25+00:00,,0,6,caffe2,True
9681,[feature request] Support for 0-length sequences in packed_sequences module: nn module: rnn triaged,2018-07-21 18:26:23+00:00,,0,6,module: nn module: rnn triaged,True
9678,Remove BatchNorm layers once the training is completed. feature module: nn triaged module: norms and normalization,2018-07-21 12:29:41+00:00,,0,5,feature module: nn triaged module: norms and normalization,True
9676,"[caffe2]running problem,  No module named caffe2_pybind11_state_hip caffe2",2018-07-21 07:10:55+00:00,,1,2,caffe2,False
9674,The state of sparse Tensors module: sparse triaged,2018-07-21 04:03:19+00:00,,0,37,module: sparse triaged,True
9624,Request: Pycuda interoperability feature triaged,2018-07-20 13:40:42+00:00,,0,5,feature triaged,False
9604,WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode. CRITICAL:root:Cannot load caffe2.python. Error: libcaffe2.so: cannot open shared object file: No such file or directory caffe2,2018-07-19 21:20:38+00:00,,1,7,caffe2,True
9595,[JIT] Unify python -> SugaredValue construction + fix inlining of graphs with Tuple-typed inputs oncall: jit,2018-07-19 18:49:28+00:00,,0,0,oncall: jit,False
9570,[feature request] More options for Fractional Max Pooling triaged module: pooling,2018-07-18 22:21:27+00:00,,0,0,triaged module: pooling,True
9563,batch_sampler/test_worker_seed intermittently fails with address already in use on OS X todo module: serialization triaged module: flaky-tests,2018-07-18 20:57:32+00:00,,0,2,todo module: serialization triaged module: flaky-tests,True
9560,Incorrect term in _LRScheduler. todo module: optimizer triaged,2018-07-18 20:18:51+00:00,,0,5,todo module: optimizer triaged,True
9559,[feature request] Rename `Subset` -> `Resample` to reflect wider use todo module: dataloader triaged,2018-07-18 20:16:46+00:00,,0,2,todo module: dataloader triaged,True
9531,[caffe2]train on GPU and test on cpu failed caffe2,2018-07-18 11:52:22+00:00,,0,1,caffe2,False
9484,WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode. WARNING:root:Debug message: libcurand.so.9.0: cannot open shared object file: No such file or directory Segmentation fault (core dumped) caffe2,2018-07-17 05:58:23+00:00,,0,5,caffe2,True
9461,Windows CPU version much slower than Unix versions module: windows triaged,2018-07-16 01:13:57+00:00,,0,17,module: windows triaged,True
9449,PredictorTest.SimpleBatchSizedMapInput intermittently hangs caffe2,2018-07-14 18:50:22+00:00,,0,3,caffe2,False
9445,Build error: flat_hash_map.h(226) C2133 caffe2,2018-07-14 15:37:06+00:00,,0,4,caffe2,False
9381,[doc] many losses still mention size_average in formula module: docs module: nn module: loss triaged module: deprecation,2018-07-12 11:47:05+00:00,,1,0,module: docs module: nn module: loss triaged module: deprecation,True
9375,[Caffe2] Not handle attribute 'spatial' of ONNX operator 'BatchNormalization' caffe2,2018-07-12 04:12:05+00:00,,0,3,caffe2,False
9349,[Caffe2] Can't find Caffe2<->ONNX conversion tools caffe2,2018-07-11 16:57:39+00:00,,0,0,caffe2,False
9310,/usr/bin/ld: cannot find -lpthreads module: build triaged has workaround,2018-07-10 17:23:58+00:00,,0,8,module: build triaged has workaround,True
9286,Accumulate into accreal instead of real for CPU loss functions module: nn module: loss triaged,2018-07-09 21:08:59+00:00,,0,0,module: nn module: loss triaged,True
9282,[gradcheck] warn about the case that mulitple inputs share storage module: autograd triaged,2018-07-09 19:56:27+00:00,,1,1,module: autograd triaged,True
9250,[feature request] SSIM-based cost function as part of the standard set of loss functions module: loss triaged enhancement,2018-07-09 02:59:11+00:00,,0,6,module: loss triaged enhancement,False
9222,[feature request] Implementing Block Sparse Operations module: sparse feature triaged,2018-07-06 21:59:51+00:00,,0,20,module: sparse feature triaged,False
9218,[Caffe2] Cannot load caffe2.python. Error: libcaffe2.so: cannot open shared object file: No such file or directory caffe2,2018-07-06 21:11:29+00:00,,1,5,caffe2,True
9208,[Caffe2] compiling error with gcc-6 caffe2,2018-07-06 14:53:18+00:00,,0,4,caffe2,True
9207,Where is the include and lib path for caffe2? caffe2,2018-07-06 14:44:20+00:00,,1,2,caffe2,False
9204,/usr/bin/ld: cannot find -lpthreads caffe2,2018-07-06 09:17:34+00:00,,1,2,caffe2,False
9193,[Feature Request] Additional torch.nn.LSTM functionality todo module: rnn triaged enhancement,2018-07-05 21:18:55+00:00,,0,3,todo module: rnn triaged enhancement,False
9186,[pytorch] Make dtype second positional argument of tensor factory methods triaged module: numpy module: pybind module: ux,2018-07-05 15:09:42+00:00,,0,4,triaged module: numpy module: pybind module: ux,True
9174,[Caffe2] build_ios.sh => 'is only available on iOS 11 or newer' caffe2,2018-07-05 01:03:44+00:00,,0,1,caffe2,False
9171,Mismatch in behaviour of WeightedRandomSampler and other samplers todo module: dataloader triaged,2018-07-04 20:55:46+00:00,,0,3,todo module: dataloader triaged,True
9166,nvcc fatal : A single input file is required for a non-link phase when an outputfile is specified awaiting response (this tag is deprecated) caffe2,2018-07-04 11:04:33+00:00,,1,1,awaiting response (this tag is deprecated) caffe2,False
9150,[caffe2] Drop connections caffe2,2018-07-03 22:53:32+00:00,,0,0,caffe2,False
9137,ObserverTest.TestMultipleNetBase intermittently segfaults caffe2,2018-07-03 14:21:44+00:00,,1,1,caffe2,False
9133,[Caffe2] Error running net train when running resnet50 caffe2,2018-07-03 13:30:27+00:00,,0,0,caffe2,True
9098,[caffe2] how to substract the mean value ? caffe2,2018-07-02 08:16:53+00:00,,0,0,caffe2,False
9068,Deployment for ios for 1.0 caffe2,2018-06-30 17:57:28+00:00,,0,3,caffe2,False
9046,[feature request] freeze() for nn.Module module: nn low priority triaged enhancement,2018-06-29 20:51:14+00:00,,0,5,module: nn low priority triaged enhancement,True
9032,Lint check for non-Unicode characters in diffs / Unicode characters without coding module: lint triaged better-engineering,2018-06-29 14:47:12+00:00,,0,0,module: lint triaged better-engineering,True
9029,DataWorkersTest::testRNNInput timeout caffe2,2018-06-29 13:46:54+00:00,,0,1,caffe2,False
9028,"[caffe2]build problem, can not find caffe2_pybind11_state_hip awaiting response (this tag is deprecated) caffe2",2018-06-29 13:35:25+00:00,,1,9,awaiting response (this tag is deprecated) caffe2,False
9026,(CAFFE_ENFORCE_EQ_WITH_CALLER tensor.h) caffe2 C++ windows10  runtime error!  caffe2,2018-06-29 11:36:05+00:00,,0,5,caffe2,False
9024,cmake error:Could NOT find IDEEP (missing:  /usr/local/include)  caffe2,2018-06-29 10:08:03+00:00,,2,6,caffe2,False
9016,SimpleMetaNetDefInitializer intermittently hangs caffe2,2018-06-29 04:10:43+00:00,,0,4,caffe2,False
9003,cmake error caffe2,2018-06-28 21:54:45+00:00,,1,1,caffe2,True
8984,[Caffe2] segmentation fault caffe2,2018-06-28 14:49:16+00:00,,1,1,caffe2,True
8982,parallelize_bmuf_distributed_test intermittently hangs caffe2,2018-06-28 13:32:26+00:00,,0,3,caffe2,False
8979,Check failed: error == cudaSuccess unspecified launch failure [caffe2] caffe2,2018-06-28 07:45:15+00:00,,0,0,caffe2,True
8944,[caffe2] UnicodeDecodeError when running LeNet caffe2,2018-06-27 17:09:24+00:00,,1,3,caffe2,False
8937,[caffe2] Is there any method to implement learning rate scheduler? caffe2,2018-06-27 13:31:51+00:00,,0,0,caffe2,False
8921,[feature request] More methods for PackedSequence feature module: nn triaged,2018-06-26 22:32:21+00:00,,0,7,feature module: nn triaged,False
8913,Multiprocessing Self Test Error todo module: multiprocessing triaged,2018-06-26 20:59:55+00:00,,0,3,todo module: multiprocessing triaged,True
8902,[Caffe 2] Does caffe2 support Galaxy 5S with Android 6?  caffe2,2018-06-26 17:56:12+00:00,,0,0,caffe2,False
8886,[Caffe2] Dose caffe2 support the function similar to itersize of caffe  caffe2,2018-06-26 02:10:01+00:00,,0,3,caffe2,False
8874,Issues with dynamically created grad_fn for views module: autograd triaged module: viewing and reshaping,2018-06-25 22:22:41+00:00,,0,4,module: autograd triaged module: viewing and reshaping,True
8871,Use target_compile_options to set warning flags module: build triaged,2018-06-25 21:47:30+00:00,,0,1,module: build triaged,True
8853,Todo functions and autograd supports for Sparse Tensor module: sparse triaged,2018-06-25 17:23:07+00:00,,1,21,module: sparse triaged,True
8837,Inconsistency in implementation of _LRScheduler  module: nn module: optimizer triaged needs research,2018-06-25 02:11:05+00:00,,0,0,module: nn module: optimizer triaged needs research,True
8821,Storages still use legacy printing module: printing triaged,2018-06-23 14:15:08+00:00,,0,2,module: printing triaged,True
8818,MPI causing job to hang --- unresponsive to external (termination) signals oncall: distributed,2018-06-23 02:32:03+00:00,,0,1,oncall: distributed,False
8760,[JIT] Add peephole to delete unnecessary type_as. oncall: jit,2018-06-21 20:06:26+00:00,,0,0,oncall: jit,False
8741,[Feature Request] Add to() method for optimizers/schedulers todo module: optimizer triaged,2018-06-21 08:21:29+00:00,,1,6,todo module: optimizer triaged,False
8730,[caffe2] AffineChannelOp caffe2,2018-06-21 03:13:06+00:00,,0,1,caffe2,False
8650,Cannot use IterOp at runtime CUDA&CPU caffe2,2018-06-19 11:07:43+00:00,,0,1,caffe2,False
8648,Cannot allocate memory Error from operator caffe2,2018-06-19 08:00:28+00:00,,0,0,caffe2,True
8607,[Caffe2] Wrong prediction with simple FF caffe2,2018-06-18 16:31:04+00:00,,0,0,caffe2,False
8602,Come with a better strategy for TensorArg (error reporting) module: performance module: internals triaged,2018-06-18 14:38:37+00:00,,0,1,module: performance module: internals triaged,True
8595,[Caffe2] How to build caffe2/mobile/ulp2/ulp_test? caffe2,2018-06-18 04:37:07+00:00,,0,0,caffe2,False
8577,Update tests to no longer spew debug info module: tests triaged better-engineering,2018-06-16 00:14:18+00:00,,0,8,module: tests triaged better-engineering,True
8561,cleanup BLAS detection module: build triaged module: linear algebra,2018-06-15 18:33:21+00:00,,0,2,module: build triaged module: linear algebra,True
8556,[JIT] Interleaved C++-Python execution loses inner Python stacks oncall: jit,2018-06-15 15:35:11+00:00,,0,0,oncall: jit,True
8550,caffe2: Does caffe support the model running on different device (CPU and GPU) same time ? caffe2,2018-06-15 11:54:01+00:00,,0,1,caffe2,False
8548,[caffe2] [feature request]Doese caffe2 support conv_nd with group? caffe2,2018-06-15 10:00:50+00:00,,0,0,caffe2,False
8546,Q: how to generate my own pb files to C++ Predictor caffe2,2018-06-15 04:59:41+00:00,,0,0,caffe2,False
8533,[Caffe2] Runtime error while using a pre-trained style_transfer model caffe2,2018-06-14 22:04:26+00:00,,0,0,caffe2,True
8479,An error occurred while creating a new notebook.  caffe2,2018-06-14 09:09:36+00:00,,0,0,caffe2,True
8473,RuntimeError: /pytorch/torch/csrc/jit/tracer.h:117: getTracingState: Assertion `var_state == state` failed. oncall: jit,2018-06-14 04:11:10+00:00,,0,14,oncall: jit,False
8471,Does the caffe2 have the depthwise separable convolution oprater  ? caffe2,2018-06-14 03:19:07+00:00,,0,2,caffe2,False
8453,"[JIT] Generalize checkTrace() to also compare gradients of parameters, not just inputs oncall: jit",2018-06-13 21:43:47+00:00,,0,0,oncall: jit,False
8442,[caffe2] Why the GPU memory consumption has no change after enable optimize_gradient_memory? caffe2,2018-06-13 19:25:52+00:00,,0,0,caffe2,False
8433,Use CMAKE_<LANG>_COMPILER_LAUNCHER module: build triaged,2018-06-13 17:26:05+00:00,,0,2,module: build triaged,True
8430,batchnorm2d  track_running_stats module: serialization triaged,2018-06-13 15:57:45+00:00,,1,10,module: serialization triaged,True
8386,Properly release NCCL resources triaged module: nccl,2018-06-12 17:49:50+00:00,,0,2,triaged module: nccl,True
8374,[Feature request] Add torch.multiprocessing.Pipe module: bootcamp module: multiprocessing triaged enhancement small,2018-06-12 13:23:59+00:00,,0,3,module: bootcamp module: multiprocessing triaged enhancement small,True
8328,how can I fetch the result of output's Blobs_namet after  I sum two numpy array via brew.sum caffe2,2018-06-11 09:17:28+00:00,,0,0,caffe2,False
8290,[feature request] MPI init_method for torch.distributed oncall: distributed triaged,2018-06-08 20:44:15+00:00,,0,4,oncall: distributed triaged,False
8276,[Installation]: Support conda/pip install with ppc64le(power8) module: binaries triaged enhancement module: POWER,2018-06-08 07:18:52+00:00,,0,3,module: binaries triaged enhancement module: POWER,True
8242,Different behavior of LSTM and LSTMCell implementation  module: numerical-stability module: rnn triaged module: numerical-reproducibility,2018-06-07 15:55:01+00:00,,0,9,module: numerical-stability module: rnn triaged module: numerical-reproducibility,True
8191,error when import caffe2.python.onnx.backend caffe2,2018-06-06 09:15:10+00:00,,1,1,caffe2,True
8186,[Caffe2] Successive in-place operators cause RuntimeError of gradient operator versions caffe2,2018-06-06 05:02:17+00:00,,0,0,caffe2,True
8153,GRU is implementation of GRU v1 draft rather than final GRU paper algo module: docs module: rnn triaged,2018-06-05 06:38:02+00:00,,0,11,module: docs module: rnn triaged,True
8128,Cmake is getting permission denied when installed system wide caffe2,2018-06-04 19:38:32+00:00,,1,8,caffe2,False
8126,PyTorch multiprocessing using single CPU core todo module: multiprocessing triaged,2018-06-04 19:27:05+00:00,,0,7,todo module: multiprocessing triaged,True
8107,Export CC is ignored when I build pytorch module: build triaged,2018-06-04 07:49:10+00:00,,0,4,module: build triaged,True
8104,load_state_dict unexpectedly does not load Tensor to buffers that currently have None value module: bc-breaking module: docs module: nn triaged,2018-06-04 04:35:01+00:00,,0,9,module: bc-breaking module: docs module: nn triaged,True
8098, Please provide wheel package for windows on PyPI module: binaries triaged,2018-06-04 02:04:45+00:00,,0,3,module: binaries triaged,True
8097,manager.cpp:64: undefined reference to `shm_open (when building with GCC 5.x (sic)) module: build triaged,2018-06-04 00:36:03+00:00,,0,0,module: build triaged,True
8069,Document torch.acos() behavior near -1 and 1 module: docs triaged,2018-06-02 10:46:06+00:00,,0,12,module: docs triaged,True
8066,import problem caffe2,2018-06-02 03:25:11+00:00,,1,1,caffe2,True
8062,CrossEntropyLoss mishandles weights module: nn module: loss triaged,2018-06-02 00:28:04+00:00,,1,5,module: nn module: loss triaged,True
8044,[PyTorch] Windows CI CUDA mem leak check on BN tests are flaky module: memory usage triaged module: flaky-tests,2018-06-01 20:07:52+00:00,,0,4,module: memory usage triaged module: flaky-tests,True
8035,[PyTorch] EmbeddingBag comparison vs Embedding fails w/ small max_norm on CUDA triaged module: norms and normalization,2018-06-01 18:12:34+00:00,,0,0,triaged module: norms and normalization,True
8028,Build error : mpi/mpi_gpu_test.cc.o:  undefined reference to symbol '_ZN3MPI8Datatype4FreeEv' caffe2,2018-06-01 09:59:02+00:00,,1,9,caffe2,True
8012,[Caffe2] VideoInput/LMDB Reader to Standalone Predictor Question caffe2,2018-06-01 00:01:35+00:00,,0,2,caffe2,False
7986,[Caffe2] place some nodes of the same network on GPU and others on CPU? caffe2,2018-05-31 12:56:54+00:00,,0,2,caffe2,False
7946,how to use Softmax when do segmentation caffe2,2018-05-30 06:16:40+00:00,,0,2,caffe2,False
7945,[Caffe2]How to convert Caffe's mean file to Caffe2's ? caffe2,2018-05-30 05:16:39+00:00,,0,0,caffe2,False
7944,Better error message in DataChannelTCP::_receive triaged module: backend,2018-05-30 04:21:34+00:00,,0,1,triaged module: backend,True
7912,[Caffe2] Operators of Detectron module not registered/compiled when built on windows caffe2,2018-05-29 08:54:40+00:00,,0,3,caffe2,False
7904,TracedModules don't support parameter sharing between modules oncall: jit,2018-05-29 02:23:17+00:00,,0,13,oncall: jit,False
7900,Deprecate torch.Tensor triaged module: deprecation module: tensor creation,2018-05-28 20:09:07+00:00,,0,7,triaged module: deprecation module: tensor creation,True
7897,"[caffe2] build from source, cannot find my cudnn7.1 caffe2",2018-05-28 13:42:00+00:00,,1,3,caffe2,False
7890,[feature request] batch_first of RNN hidden weight for Multi GPU training module: nn triaged enhancement module: data parallel,2018-05-28 03:09:16+00:00,,0,8,module: nn triaged enhancement module: data parallel,True
7887,[Caffe2] convert ONNX to caffe2 caffe2,2018-05-28 00:20:35+00:00,,0,2,caffe2,False
7865,[PyTorch] weight tensor dimension assumption module: nn triaged,2018-05-25 22:38:53+00:00,,1,5,module: nn triaged,True
7857,Feature Request: Logistic Distribution module: distributions feature triaged,2018-05-25 19:51:39+00:00,,0,11,module: distributions feature triaged,True
7835,[caffe2] how to using the Mul operator to mul the input vetor caffe2,2018-05-25 03:19:09+00:00,,0,0,caffe2,False
7834,torch.Tensor.new() disappeared in 0.4 doc module: docs triaged module: deprecation module: tensor creation,2018-05-25 03:03:41+00:00,,0,4,module: docs triaged module: deprecation module: tensor creation,True
7807,LMDB read error for Mnist caffe2,2018-05-24 11:12:47+00:00,,0,4,caffe2,True
7806,OOM Exception when using torch.nn.grad.conv2d_weight (apparently because CuDNN backwards is not used) module: performance module: cudnn module: memory usage module: convolution triaged has workaround,2018-05-24 08:29:20+00:00,,0,7,module: performance module: cudnn module: memory usage module: convolution triaged has workaround,True
7801,Checkpointing is slow on nn.DataParallel models module: performance module: checkpoint triaged,2018-05-23 22:50:53+00:00,,0,10,module: performance module: checkpoint triaged,True
7795,[feature request] Add cudaification API for distributions module: distributions triaged enhancement,2018-05-23 20:25:49+00:00,,0,18,module: distributions triaged enhancement,True
7789,detectron net create error caffe2,2018-05-23 11:58:23+00:00,,0,1,caffe2,True
7786,[feature request] Simple and Efficient way to get gradients of each element of a sum feature module: autograd triaged,2018-05-23 07:56:00+00:00,,0,26,feature module: autograd triaged,False
7781,[Caffe2] Fail to build after upgrading to cuda 9.2 caffe2,2018-05-23 02:44:49+00:00,,0,3,caffe2,True
7773,[feature request] Add Local Contrast Normalization  module: nn triaged enhancement,2018-05-22 21:43:59+00:00,,0,4,module: nn triaged enhancement,False
7745,[Caffe2] Align Element-Wise Ops Broadcasting to Numpy caffe2,2018-05-21 23:07:15+00:00,,2,1,caffe2,False
7740,Conv3D can be optimized for cases when kernel is spatial (probably) module: performance module: convolution triaged,2018-05-21 18:58:52+00:00,,0,0,module: performance module: convolution triaged,True
7733,Inserting a tensor into a python dict causes strange behavior todo module: nn triaged,2018-05-21 16:26:35+00:00,,0,28,todo module: nn triaged,True
7729,[Caffe2][Caffe] Caffe to Caffe2 ParseError caffe2,2018-05-21 07:37:29+00:00,,0,0,caffe2,False
7686,Only one thread is used on macOS (super slow on CPU) triaged module: macos module: multithreading,2018-05-18 19:15:44+00:00,,0,20,triaged module: macos module: multithreading,True
7667,Caffe2 network exported from ONNX does not initialize the model inputs caffe2,2018-05-18 08:26:50+00:00,,0,1,caffe2,False
7617,"checkpoint(function, *args) should have the same requires_grad as function(*args) module: checkpoint module: autograd triaged",2018-05-16 18:02:54+00:00,,0,20,module: checkpoint module: autograd triaged,True
7614,[Caffe2] modify op in the net from init and predict files caffe2,2018-05-16 13:29:53+00:00,,0,0,caffe2,False
7610,Inconsistent interactions of PyTorch tensors and NumPy ops triaged module: numpy,2018-05-16 11:21:27+00:00,,0,6,triaged module: numpy,True
7610,Inconsistent interactions of PyTorch tensors and NumPy ops triaged module: numpy,2018-05-16 11:21:27+00:00,,0,6,triaged module: numpy,True
7571,"the latest version of caffe2,  Which file is  the function loadToNCHW()  caffe2",2018-05-15 10:09:20+00:00,,0,1,caffe2,False
7569,[Caffe2]How to set lr_mult and decay_mult in Conv layer? caffe2,2018-05-15 08:52:19+00:00,,0,0,caffe2,False
7546,[Caffe2] Build broken on macOS High Sierra: can't find sys headers in /usr/local/include. caffe2,2018-05-14 04:28:32+00:00,,1,2,caffe2,True
7541,[Bug] Dilated max-pooling fails due to padding check todo triaged module: pooling,2018-05-13 23:51:03+00:00,,0,4,todo triaged module: pooling,True
7535,[feature request] Global GPU Flag feature module: cuda triaged,2018-05-13 13:07:06+00:00,,0,12,feature module: cuda triaged,True
7493,Build from source with anaconda [caffe2] caffe2,2018-05-11 05:58:48+00:00,,1,1,caffe2,False
7491,[caffe2] How to know the input shape for certain pre-trained network caffe2,2018-05-11 03:57:20+00:00,,0,0,caffe2,False
7490,"[Caffe2]: caffe2.python.caffe_translator.py script doesn't convert ""Split"" and ""Slice"" Layer caffe2",2018-05-11 03:02:49+00:00,,0,0,caffe2,False
7480,[caffe2] How to use multiple CPUs? caffe2,2018-05-10 19:20:55+00:00,,0,0,caffe2,False
7471,Make c10d/FileStore cache file descriptor newcomer oncall: distributed feature triaged,2018-05-10 16:16:11+00:00,,1,0,newcomer oncall: distributed feature triaged,True
7457,[feature request] torch.nn.DataParallel should work nicely both for cpu and gpu devices triaged enhancement module: data parallel,2018-05-10 08:26:17+00:00,,2,3,triaged enhancement module: data parallel,True
7423,Multi queue for dataloader when workers > 1 module: dataloader triaged enhancement,2018-05-09 14:13:20+00:00,,0,1,module: dataloader triaged enhancement,True
7414,[Caffe2] Negative export to ONNX fails caffe2,2018-05-09 07:52:04+00:00,,0,1,caffe2,False
7374,[Caffe2] Can't use resnet50_trainer.py through redis. caffe2,2018-05-08 15:20:45+00:00,,0,0,caffe2,False
7365,nn.DataParallel fills None grads with 0 triaged module: data parallel,2018-05-08 03:42:16+00:00,,0,7,triaged module: data parallel,True
7362,[caffe2] Can I load the gpu data to cpu in .cu file? caffe2,2018-05-08 02:01:09+00:00,,0,0,caffe2,False
7359,[feature request] [PyTorch] Dynamic Samplers. feature module: dataloader triaged,2018-05-08 01:24:25+00:00,,0,10,feature module: dataloader triaged,False
7358,DataParallel on list inputs triaged module: data parallel,2018-05-08 00:18:52+00:00,,0,0,triaged module: data parallel,True
7353,"Windows MAGMA binary requires explicit linking against MKL LAPACK, or it will silently give  incorrect results module: build module: cuda triaged",2018-05-07 20:54:07+00:00,,0,1,module: build module: cuda triaged,True
7352,Check for F2C convention (for blas) at runtime module: build triaged,2018-05-07 20:47:23+00:00,,0,1,module: build triaged,True
7343,[memory leak] [PyTorch] .backward(create_graph=True) module: autograd module: memory usage triaged,2018-05-07 17:57:29+00:00,,0,10,module: autograd module: memory usage triaged,True
7342,[feature request] [PyTorch] More flexible optimizer API module: optimizer triaged enhancement,2018-05-07 17:52:11+00:00,,0,6,module: optimizer triaged enhancement,False
7330,Issue when importing both retro (from OpenAI) and torch triaged module: pybind,2018-05-06 16:03:18+00:00,,0,10,triaged module: pybind,True
7316,"Optional modifiers (e.g., Tensor?) are not checked for non-dispatched native functions triaged module: dispatch",2018-05-05 03:25:50+00:00,,0,0,triaged module: dispatch,True
7313,[proposal] [discussion] Refactor pruning/weight_norm using new Reparametrization functionality + actually deprecate old impl of SpectralNorm module: nn triaged,2018-05-04 23:03:28+00:00,,0,70,module: nn triaged,True
7250,feature request: support for new/future hardware accelerators feature triaged shadow review,2018-05-03 18:02:37+00:00,,0,3,feature triaged shadow review,False
7227,[Caffe2]Install problem caffe2,2018-05-03 04:52:22+00:00,,1,1,caffe2,True
7214,Do not put system paths in RPATH module: build triaged,2018-05-03 01:50:21+00:00,,1,3,module: build triaged,True
7180,[Caffe2 Bug] Windows timer is not accurate caffe2,2018-05-02 17:37:39+00:00,,0,0,caffe2,False
7179,[caffe2]when i do CMAKE caffe2,2018-05-02 14:57:07+00:00,,1,1,caffe2,False
7133,[Caffe2 warpctc] How to use the offered warpctc? caffe2,2018-05-01 14:08:44+00:00,,1,4,caffe2,False
7127,how can i use openMP for caffe2? why caffe2 not work in multi-threads mode? caffe2,2018-05-01 03:22:36+00:00,,0,2,caffe2,False
7078,Fail to import Caffe2Backend caffe2,2018-04-29 06:25:54+00:00,,0,5,caffe2,False
7066,LBFGS contribution  awaiting response (this tag is deprecated) module: optimizer triaged,2018-04-28 06:18:44+00:00,,0,7,awaiting response (this tag is deprecated) module: optimizer triaged,True
7060,[feature request] Provide Caffe2 CUDA dockerfile with USE_REDIS=ON caffe2,2018-04-28 01:07:00+00:00,,0,0,caffe2,False
7047,"""Undefined symbols for architecture arm64"" when linking libcaffe2.a for iOS caffe2",2018-04-27 19:51:19+00:00,,0,5,caffe2,False
7043,CuDNN version not supported todo module: build module: cudnn module: cuda triaged,2018-04-27 17:55:01+00:00,,0,0,todo module: build module: cudnn module: cuda triaged,True
7036,[Caffe2] compilation linker error `libtbb.so.2: undefined reference to std::__exception_ptr` module: crash module: build caffe2,2018-04-27 12:46:30+00:00,,1,0,module: crash module: build caffe2,True
7035,[Compilation] how to disable caffe2?  module: build triaged,2018-04-27 11:28:22+00:00,,0,9,module: build triaged,True
7032,[feature request] norm argument for RNNCells module: rnn triaged enhancement,2018-04-27 09:16:45+00:00,,0,2,module: rnn triaged enhancement,True
7020,[Caffe2] LT and GT cannot be exported to ONNX caffe2,2018-04-27 01:31:55+00:00,,0,4,caffe2,False
6998,[pytorch] Not handling python reload properly todo module: crash triaged,2018-04-26 16:25:46+00:00,,1,4,todo module: crash triaged,True
6979,[Caffe2] Is there any way to load custom operators in c++? caffe2,2018-04-26 03:35:15+00:00,,0,0,caffe2,False
6934,Feature request: SSIM/MS-SSIM triaged module: vision function request,2018-04-25 06:09:34+00:00,,0,43,triaged module: vision function request,False
6902,[Caffe2] Flatten Layer in caffe2 caffe2,2018-04-24 15:28:40+00:00,,0,1,caffe2,False
6901,[Caffe2] [feature request] Dilations in grouped convolutons caffe2,2018-04-24 14:52:13+00:00,,0,1,caffe2,False
6898,[Caffe2] cudnn versions compatibility issue. module: cudnn caffe2,2018-04-24 12:44:51+00:00,,1,1,module: cudnn caffe2,True
6875,[Bug] VS reports unresolved external symbol from caffe2 observer caffe2,2018-04-23 19:18:11+00:00,,0,1,caffe2,False
6868,[Caffe2] Android NNApi integration bugs. caffe2,2018-04-23 17:30:04+00:00,,0,1,caffe2,False
6857,[caffe2] Understanding WorkFlow for Training and Testing caffe2,2018-04-23 09:07:14+00:00,,0,11,caffe2,False
6850,[feature request] More `index_*_` functionality and/or lambda functionality feature triaged module: scatter & gather ops,2018-04-23 00:21:20+00:00,,0,9,feature triaged module: scatter & gather ops,False
6846,Jetson TX1 Caffe2 installation fails on latest JetPack release caffe2,2018-04-22 11:46:42+00:00,,0,2,caffe2,False
6845,[caffe2] global average pool in caffe2 caffe2,2018-04-22 07:33:42+00:00,,0,0,caffe2,False
6794,[Caffe2] TensorProtosDBInput AttributeError caffe2,2018-04-20 03:53:44+00:00,,0,10,caffe2,False
6785,[caffe2] Training and inference caffe2,2018-04-19 23:34:39+00:00,,0,0,caffe2,False
6783,[caffe2] IfOp caffe2,2018-04-19 23:03:59+00:00,,0,0,caffe2,False
6760,[Feature request] LayerNormLSTMCell and LayerNormLSTM triaged enhancement,2018-04-19 13:52:16+00:00,,0,4,triaged enhancement,False
6662,Autogenerate code example / tutorial outputs in documentation todo module: docs good first issue triaged module: doc infra,2018-04-17 15:23:03+00:00,,0,15,todo module: docs good first issue triaged module: doc infra,True
6660,[caffe2] GANs caffe2,2018-04-17 14:23:48+00:00,,1,5,caffe2,False
6657,[feature request] Stochastic Variance Reduced Gradient (SVRG) optimizer feature module: optimizer triaged needs research,2018-04-17 13:28:46+00:00,,0,2,feature module: optimizer triaged needs research,False
6618,[Caffe2] CUDNN_STATUS_BAD_PARAM Error with the LRN layer while trying to run the code using CUDA. The training works fine on CPU caffe2,2018-04-16 08:53:51+00:00,,0,1,caffe2,True
6613,[caffe2] benchmark performance for different operators  caffe2,2018-04-15 13:53:18+00:00,,0,1,caffe2,False
6581,[Caffe2] [feature request] How to freeze a layer ? (Selective Backward Propagation) caffe2,2018-04-13 04:51:29+00:00,,0,1,caffe2,False
6564,[Feature Request] Optimization with constraint (L-BFGS-B) proposal accepted todo triaged,2018-04-12 20:23:58+00:00,,0,4,proposal accepted todo triaged,False
6552,[Caffe2] - Run on different Cuda streams caffe2,2018-04-12 15:07:43+00:00,,0,2,caffe2,False
6549,caffe2/cuda_rtc can throw during the destructor caffe2,2018-04-12 13:00:10+00:00,,0,0,caffe2,False
6542,"【Train issue with caffe2 detectron】Aborted at 1523501813 (unix time) try ""date -d @1523501813"" if you are using GNU date *** PC: @     0x7fa35103733a (unknown) caffe2",2018-04-12 05:39:28+00:00,,0,3,caffe2,True
6505,[Caffe2] Onnx to Caffe2 Blob error caffe2,2018-04-11 14:21:35+00:00,,0,5,caffe2,True
6502,[Caffe2] Boolean Tensor not supported for mobile_exporter.Export caffe2,2018-04-11 08:24:20+00:00,,0,0,caffe2,False
6465,[caffe2] ChannelShuffle example caffe2 module: docs,2018-04-10 14:30:08+00:00,,0,0,caffe2 module: docs,False
6442,[feature request] dropout1d todo module: nn triaged enhancement,2018-04-09 22:07:23+00:00,,0,6,todo module: nn triaged enhancement,True
6437,[caffe2] Double precision for operators? caffe2,2018-04-09 20:53:15+00:00,,0,0,caffe2,False
6431,Examine contiguity requirements for gradInput triaged,2018-04-09 19:00:44+00:00,,0,9,triaged,True
6424,[utils.bottleneck] List of improvements todo triaged module: bottleneck,2018-04-09 16:18:08+00:00,,0,2,todo triaged module: bottleneck,True
6422,[caffe2] Run resnet50_trainer.py error between 2 machines using GLOO/Redis and ibverbs caffe2,2018-04-09 14:24:27+00:00,,0,5,caffe2,True
6408,[caffe2] Prefetching blobs for memory optimization caffe2,2018-04-08 17:41:23+00:00,,0,3,caffe2,False
6402,[caffe2] How to freeze a layer? caffe2,2018-04-08 10:13:46+00:00,,0,10,caffe2,False
6357,[feature request] Unpooling layer in Caffe2 caffe2 feature,2018-04-06 18:02:50+00:00,,0,0,caffe2 feature,False
6350,worker assignments in torch.utils.dataloader.py module: dataloader triaged,2018-04-06 15:07:39+00:00,,0,3,module: dataloader triaged,True
6328,[feature request] Include libomp support (macOS) module: build triaged module: macos,2018-04-05 23:16:56+00:00,,0,5,module: build triaged module: macos,True
6273,Multithreading Scaling Issue with MKL caffe2,2018-04-04 15:30:22+00:00,,0,10,caffe2,True
6265,[Caffe2] mobile_exporter init_net has code calling information caffe2,2018-04-04 07:54:53+00:00,,1,1,caffe2,False
6257,"[feature request] adding a nonzero element ""in-place"" in sparse tensor module: sparse triaged",2018-04-04 00:41:04+00:00,,1,3,module: sparse triaged,True
6165,Note about unusual stride situations in dev docs / make it easier to test for this in the library module: docs triaged module: memory format,2018-04-01 02:31:35+00:00,,0,0,module: docs triaged module: memory format,True
6138,[feature request] SIGNUM an optimizer that takes the sign of gradient or momentum. module: optimizer triaged,2018-03-30 16:18:00+00:00,,0,1,module: optimizer triaged,False
6135,Test suite should test implementations module: tests triaged,2018-03-30 15:43:29+00:00,,0,0,module: tests triaged,True
6107,"Well documented, safe method to deserialize model parameters from untrusted sources feature module: pickle module: serialization triaged onnx-triaged topic: security",2018-03-29 11:22:37+00:00,,0,9,feature module: pickle module: serialization triaged onnx-triaged topic: security,True
6083,[feature request] DC-ASGD (Delay Compensated Asynchronous Stochastic Gradient Descent) oncall: distributed module: optimizer triaged,2018-03-28 21:23:08+00:00,,0,3,oncall: distributed module: optimizer triaged,False
6071,Repeated 'python setup.py install' with clang leads to -lcpuinfo not found module: build triaged,2018-03-28 15:54:58+00:00,,0,4,module: build triaged,True
6010,Develop a strategy for writing leak tests for pytorch.  oncall: jit,2018-03-26 19:23:32+00:00,,0,1,oncall: jit,False
6007, Clean up the extra copies that occur in the execution engine and other places oncall: jit,2018-03-26 19:21:07+00:00,,0,0,oncall: jit,False
5953,"LBFGS always give nan results, why needs reproduction module: numerical-stability module: optimizer triaged",2018-03-23 01:26:27+00:00,,1,15,needs reproduction module: numerical-stability module: optimizer triaged,True
5924,Install doesn't work with spaces in directory todo module: build triaged has workaround,2018-03-21 17:56:20+00:00,,0,6,todo module: build triaged has workaround,True
5912,Conv-RNN combination slow in backward pass module: performance module: nn triaged,2018-03-20 20:22:47+00:00,,0,9,module: performance module: nn triaged,True
5900,"Build Fails on Gentoo with CUDA 9.1, GCC 6.4, Python 3.5 module: build triaged",2018-03-20 10:58:04+00:00,,1,7,module: build triaged,True
5857,Change THCudaCheck to suggest that device-side asserts likely mean that you have out of bound indices module: cuda module: error checking triaged module: assert failure small,2018-03-17 19:46:48+00:00,,0,6,module: cuda module: error checking triaged module: assert failure small,True
5790,Add hookable weights module: nn triaged enhancement,2018-03-14 20:50:43+00:00,,0,14,module: nn triaged enhancement,True
5740,[feature request] scalar input to scatter_add_ like scatter_ triaged function request module: scatter & gather ops,2018-03-13 14:27:07+00:00,,0,10,triaged function request module: scatter & gather ops,False
5737,[feature request] Callback on learning rate drop in torch.optim.lr_scheduler.ReduceLROnPlateau todo module: optimizer triaged,2018-03-13 11:35:29+00:00,,0,4,todo module: optimizer triaged,False
5580,"[feature request] F.interpolate to support integral data types: bool, int8, int32, int16, int64 ||| support uint8 on CUDA todo triaged",2018-03-05 23:30:28+00:00,,1,15,todo triaged,True
5565,[feature request] Different interpolation algos for 'grid_sample' function hackamonth module: docs triaged function request module: interpolation,2018-03-05 08:44:01+00:00,,0,19,hackamonth module: docs triaged function request module: interpolation,False
5528,TestNN.test_data_parallel takes 10G of memory module: memory usage module: tests triaged,2018-03-02 17:08:56+00:00,,0,0,module: memory usage module: tests triaged,True
5524,Redo torch.nn.functional docstring strategy module: docs triaged,2018-03-02 15:05:38+00:00,,0,1,module: docs triaged,False
5489,"torch.jit.trace(network, data) fails if data is an OrderedDict oncall: jit module: bootcamp days",2018-03-01 09:37:39+00:00,,1,0,oncall: jit module: bootcamp days,True
5435,Gaussian Sampling feature module: nn triaged,2018-02-27 16:00:19+00:00,,0,1,feature module: nn triaged,True
5434,RuntimeError: $ Torch: not enough memory: you tried to allocate 72GB. Buy new RAM! module: memory usage triaged,2018-02-27 15:16:02+00:00,,0,15,module: memory usage triaged,True
5405,scatter_add_ should support scalar source (including Python scalar) triaged module: scatter & gather ops,2018-02-25 23:09:42+00:00,,1,9,triaged module: scatter & gather ops,True
5388,Perf regression: indexing 1-d tensor module: performance in progress triaged,2018-02-23 21:28:23+00:00,,0,3,module: performance in progress triaged,True
5385,Handle python_arg_parser dtype constants better todo feature triaged,2018-02-23 20:18:59+00:00,,0,1,todo feature triaged,True
5353,Bugs: Score Function approach in REINFORCE for PONG todo module: crash module: loss module: cuda module: memory usage triaged,2018-02-22 18:35:59+00:00,,0,1,todo module: crash module: loss module: cuda module: memory usage triaged,True
5326,TestMultiprocessing.test_fd_sharing hangs with ASAN module: tests triaged,2018-02-21 05:29:07+00:00,,0,2,module: tests triaged,True
5280,BatchNorm1d raises RuntimeError (CUDNN_STATUS_BAD_PARAM) on 3D input. module: cudnn triaged,2018-02-17 00:50:35+00:00,,0,1,module: cudnn triaged,True
5272,Unsafe out= keyword argument with tensors sharing storage triaged module: numpy module: safe resize module: correctness (silent),2018-02-16 07:41:38+00:00,,0,6,triaged module: numpy module: safe resize module: correctness (silent),True
5266,[feature request] warnings for functions with unspecified dim arguments triaged enhancement,2018-02-15 20:50:22+00:00,,0,1,triaged enhancement,False
5231,[feature request] Stratified splits in random_split function module: dataloader triaged,2018-02-14 01:19:06+00:00,,0,0,module: dataloader triaged,True
5212,Weird error message in torch.split_size_or_sections triaged module: numpy,2018-02-13 13:16:40+00:00,,0,9,triaged module: numpy,True
5161,ASAN detected leaks on python -c 'import torch' module: memory usage triaged,2018-02-09 17:53:05+00:00,,0,3,module: memory usage triaged,True
5159,TakeBackward taking a significant portion of backward time module: performance module: autograd triaged,2018-02-09 17:03:48+00:00,,0,5,module: performance module: autograd triaged,True
5157,BCELoss - weight parameter shape incorrect module: nn module: loss triaged,2018-02-09 16:03:39+00:00,,0,6,module: nn module: loss triaged,True
5106,Saving model with runtime code changes module: serialization triaged,2018-02-07 11:40:37+00:00,,0,0,module: serialization triaged,True
5096,[feature request]Add an env variable to cover different pathes when testing code with openmp module: tests triaged better-engineering,2018-02-07 02:23:19+00:00,,0,4,module: tests triaged better-engineering,True
5070,NVIDIA_DRIVER_CAPABILITIES env variable is missing in pytorch docker images triaged module: docker,2018-02-06 03:48:20+00:00,,0,0,triaged module: docker,True
5063,Delete obsolete `THCDeviceTensor::downcastOuter` / `THCDeviceTensor::downcastInner` functions module: bootcamp module: cuda triaged small better-engineering,2018-02-05 22:28:57+00:00,,0,1,module: bootcamp module: cuda triaged small better-engineering,True
5013,[Feature Request] Calculating FLOPs for computational graph operations high priority module: performance feature triaged quansight-nack,2018-02-02 16:08:57+00:00,,0,25,high priority module: performance feature triaged quansight-nack,True
4987,MultiGPU hangs Titan Xp in multiprocessing/queue.py module: multiprocessing module: cuda triaged module: macos,2018-02-01 15:57:30+00:00,,0,15,module: multiprocessing module: cuda triaged module: macos,True
4959,Speed up data loading for `TensorDataset` if the underlying dataset supports index by a list of indices module: performance module: dataloader triaged,2018-01-31 15:40:23+00:00,,0,3,module: performance module: dataloader triaged,True
4958,[Feature Request] Extract glimpses from a batch of images (as in tf.image.extract_glimpse) triaged module: vision function request,2018-01-31 09:09:59+00:00,,0,1,triaged module: vision function request,False
4954,[docs] Docs website search finds duplicates and produces bad snippets module: docs triaged module: doc infra,2018-01-31 01:32:39+00:00,,0,7,module: docs triaged module: doc infra,False
4952,[Feature request] Optimize autograd/ATen when a gradient is clearly zero feature module: autograd triaged,2018-01-30 23:09:18+00:00,,0,3,feature module: autograd triaged,False
4930,[feature request] Type-1 Multi-layer bidirectional RNN module: cudnn module: rnn triaged function request,2018-01-30 09:22:10+00:00,,1,29,module: cudnn module: rnn triaged function request,True
4927,[Feature Request]would PackedSequence support unsorted sequences? feature module: nn triaged,2018-01-30 07:33:37+00:00,,0,0,feature module: nn triaged,False
4906,"Rebuild from no-CUDA to CUDA leads to: error: #error ""Expected GLOO_USE_CUDA to be defined"" module: build low priority triaged has workaround",2018-01-29 14:24:59+00:00,,0,9,module: build low priority triaged has workaround,True
4829, [Feature Request] clip_grad_norm for sparse gradients module: sparse triaged,2018-01-24 12:34:07+00:00,,0,4,module: sparse triaged,True
4825,[feature request]Support AVX512F intrinstics to vectorize operations feature module: cpu triaged,2018-01-24 00:59:23+00:00,,0,9,feature module: cpu triaged,False
4758,Clang color diagnostics don't work with ninja module: build triaged,2018-01-20 19:36:47+00:00,,0,0,module: build triaged,True
4731,Consider disallowing Variables that require grad in NCCL/comm functions module: autograd triaged module: nccl actionable,2018-01-18 21:32:53+00:00,,0,1,module: autograd triaged module: nccl actionable,True
4716,Compilation issue: problem with GPU capability check module: build module: cuda triaged,2018-01-18 00:47:08+00:00,,0,3,module: build module: cuda triaged,True
4703,Very slow on CPU module: performance module: rnn module: cpu triaged,2018-01-17 08:05:34+00:00,,0,23,module: performance module: rnn module: cpu triaged,True
4660,[Feature proposal] Add MC-derived optimizers feature module: optimizer triaged needs research,2018-01-14 02:26:56+00:00,,1,0,feature module: optimizer triaged needs research,False
4636,Better header hygiene in ATen module: internals triaged,2018-01-12 15:51:40+00:00,,0,2,module: internals triaged,True
4632,CUDNN_STATUS_INTERNAL_ERROR when training with conv3d module: cudnn module: convolution triaged,2018-01-12 08:47:38+00:00,,0,7,module: cudnn module: convolution triaged,True
4622,Protect user from No module named _C import error module: error checking triaged module: pybind,2018-01-12 01:27:30+00:00,,0,4,module: error checking triaged module: pybind,True
4574,`from` keyword in `random_` gives error module: distributions triaged,2018-01-10 05:14:47+00:00,,0,0,module: distributions triaged,True
4564,Mixed Tensor/TensorList arguments in ATen functions with explicit derivatives module: autograd triaged enhancement,2018-01-09 20:07:29+00:00,,0,4,module: autograd triaged enhancement,True
4551,Met 'cudnnDestroyDropoutDescriptor' while run multiply gpu-based models in multiply processes module: multi-gpu module: cudnn triaged,2018-01-09 03:18:00+00:00,,0,1,module: multi-gpu module: cudnn triaged,True
4542,Carefully audit contiguity requirements of code module: cudnn triaged,2018-01-08 21:55:10+00:00,,0,0,module: cudnn triaged,True
4530,descriptor 'add' of 'torch._C._VariableBase' object needs an argument module: docs triaged,2018-01-08 10:15:40+00:00,,0,3,module: docs triaged,False
4501,Bind in Python _backward ATen functions module: autograd triaged,2018-01-05 22:20:09+00:00,,0,4,module: autograd triaged,True
4474,[docs] Return value type hint to be specified in docs for torch.is_grad_enabled (i.e. -> bool) module: docs triaged,2018-01-04 14:47:52+00:00,,0,4,module: docs triaged,False
4440,Assert that some tests must not be skipped under certain CI configurations high priority module: ci module: tests triaged quansight-nack,2018-01-02 17:21:04+00:00,,0,8,high priority module: ci module: tests triaged quansight-nack,True
4406,DistributedDataParallel doesn't converge well when using MPI oncall: distributed triaged,2017-12-29 10:42:38+00:00,,0,7,oncall: distributed triaged,True
4400,Installation Optimise For Chinese Users Who Behind the Wall module: docs triaged,2017-12-29 01:43:37+00:00,,0,4,module: docs triaged,True
4392,"Cache CuDNN benchmark selection, turn it on by default, use it across PyTorch runs module: cudnn triaged",2017-12-28 21:39:31+00:00,,0,5,module: cudnn triaged,True
4358,Invoking MKL in multiprocessing with importing torch causes blocking module: multiprocessing triaged module: mkldnn module: mkl,2017-12-27 04:07:21+00:00,,0,3,module: multiprocessing triaged module: mkldnn module: mkl,True
4334,torch.cuda.device_count() returns 1 using 4 TitanX setup. needs reproduction module: cuda triaged,2017-12-23 14:43:27+00:00,,0,1,needs reproduction module: cuda triaged,True
4247,Feature request: sparse matrix max(axis) module: sparse triaged enhancement,2017-12-19 11:31:20+00:00,,1,6,module: sparse triaged enhancement,True
4241,single-gpu works but multi-gpu hangs module: cudnn triaged module: data parallel,2017-12-18 22:19:08+00:00,,0,2,module: cudnn triaged module: data parallel,True
4186,Feature Request: CPU performance optimization with MKL-DNN triaged module: mkldnn,2017-12-15 01:58:51+00:00,,0,27,triaged module: mkldnn,False
4181,Fused RNN refactor plan module: cudnn triaged,2017-12-14 22:50:03+00:00,,0,6,module: cudnn triaged,True
4159,Variable outputs of stochastic functions should never require grad module: distributions triaged,2017-12-13 20:50:43+00:00,,0,1,module: distributions triaged,True
4148,ATen explicitly differentiated native function resolution hazard (call is ambiguous) module: internals triaged,2017-12-13 14:41:33+00:00,,0,1,module: internals triaged,True
4145,[Proposal] Consistent `batch_first` effect for RNN modules module: docs module: nn module: rnn triaged,2017-12-13 08:02:47+00:00,,0,11,module: docs module: nn module: rnn triaged,True
4132,x.grad should be 0 but get NaN after x/0 module: docs module: autograd triaged module: NaNs and Infs has workaround needs design,2017-12-12 16:44:52+00:00,,0,16,module: docs module: autograd triaged module: NaNs and Infs has workaround needs design,True
4123,Use the int64 version of MKL calls module: internals triaged module: mkl,2017-12-11 20:35:48+00:00,,0,3,module: internals triaged module: mkl,True
4102,Make the generator tools data model more explicit triaged module: codegen,2017-12-10 05:41:18+00:00,,0,0,triaged module: codegen,True
4085,GridSampler behaviours module: cudnn triaged,2017-12-08 11:03:36+00:00,,0,5,module: cudnn triaged,True
4073,Feature request: Correlation module triaged,2017-12-07 16:10:31+00:00,,0,11,triaged,True
3990,Raise an error when using magma built against wrong version of cuda module: binaries module: build triaged,2017-12-03 23:54:52+00:00,,0,4,module: binaries module: build triaged,True
3931,Suppress hidden state output of RNNs? module: memory usage triaged enhancement,2017-11-28 23:52:16+00:00,,0,5,module: memory usage triaged enhancement,True
3920,[docs] Tensor.new is not documented module: docs triaged,2017-11-28 11:51:31+00:00,,1,7,module: docs triaged,False
3904,Implement DE in pytorch.optim module: optimizer triaged enhancement needs research,2017-11-27 18:06:14+00:00,,1,3,module: optimizer triaged enhancement needs research,False
3898,Sparse matrices in dataloader error module: sparse triaged,2017-11-27 11:32:13+00:00,,1,11,module: sparse triaged,True
3877,Wrap Cephes library for mathematical special functions feature triaged module: numpy,2017-11-25 18:40:37+00:00,,0,18,feature triaged module: numpy,True
3867,"[Feature Request] Implement ""same"" padding for convolution operations? high priority module: nn module: convolution triaged enhancement needs design",2017-11-25 07:52:39+00:00,,1,84,high priority module: nn module: convolution triaged enhancement needs design,True
3863,Considerable slowdown in Adam.step after a number of epochs with multiple losses awaiting response (this tag is deprecated) needs reproduction module: performance module: optimizer triaged,2017-11-24 20:53:31+00:00,,0,4,awaiting response (this tag is deprecated) needs reproduction module: performance module: optimizer triaged,True
3823,Fuse bias to CuDNN convolution triaged enhancement,2017-11-22 01:14:27+00:00,,0,4,triaged enhancement,True
3818,Memory leak when doing backward with grad as yourself module: autograd module: memory usage triaged quansight-nack,2017-11-21 20:50:43+00:00,,0,24,module: autograd module: memory usage triaged quansight-nack,True
3791,Make pytest stop printing docstrings in its default diagnostic output module: tests triaged enhancement,2017-11-20 12:54:51+00:00,,0,6,module: tests triaged enhancement,True
3790,"Add SGDR, SGDW, AdamW and AdamWR module: optimizer triaged",2017-11-20 09:20:23+00:00,,0,11,module: optimizer triaged,False
3697,Deprecate inplace argument in torch.nn.functional module: bc-breaking feature module: nn triaged module: deprecation,2017-11-14 17:50:55+00:00,,0,1,module: bc-breaking feature module: nn triaged module: deprecation,True
3667,Exposing CuDNN benchmark strategy selection  module: cudnn module: bootcamp feature triaged,2017-11-13 15:02:46+00:00,,0,7,module: cudnn module: bootcamp feature triaged,True
3625,Proposal: combine requires_grad and retain_grad() module: autograd triaged,2017-11-10 16:48:51+00:00,,0,12,module: autograd triaged,True
3619,Multiprocessing with torch.solve hangs module: multiprocessing triaged module: linear algebra,2017-11-10 04:20:43+00:00,,0,14,module: multiprocessing triaged module: linear algebra,True
3600,making .cuda() falls back to an identity function when gpu is not available module: cuda triaged,2017-11-09 15:24:09+00:00,,0,15,module: cuda triaged,True
3473,Feature Request: Distributed send arbitrary objects oncall: distributed feature module: pickle module: serialization triaged,2017-11-03 21:04:54+00:00,,0,4,oncall: distributed feature module: pickle module: serialization triaged,True
3468,improve performance of common CPU clone / contiguous calls with HPTT module: cpu triaged,2017-11-03 17:44:32+00:00,,1,4,module: cpu triaged,True
3428,[docs] Disable google indexing of old docs and of master docs and of function/module summary pages and of source code listings high priority triaged module: doc infra,2017-11-01 19:52:35+00:00,,1,59,high priority triaged module: doc infra,False
3396,CUDA topk is slow for some input sizes module: performance module: cuda triaged module: sorting and selection,2017-10-31 15:47:39+00:00,,0,12,module: performance module: cuda triaged module: sorting and selection,True
3390,High CPU use by clock_gettime syscall module: performance module: cuda triaged,2017-10-31 12:01:21+00:00,,0,2,module: performance module: cuda triaged,True
3364,type of torch.bernoulli and torch.multinomial inconsistent module: distributions triaged,2017-10-30 11:25:43+00:00,,0,3,module: distributions triaged,True
3356,Data sampling seems to be more complicated than necessary module: dataloader triaged,2017-10-29 17:58:53+00:00,,0,13,module: dataloader triaged,True
3281,"DataLoader ""casting"" non statndard objects to lists module: dataloader triaged",2017-10-25 10:53:01+00:00,,0,9,module: dataloader triaged,True
3176,Add safety checks in `index_add`/`scatter_add` triaged,2017-10-19 12:51:38+00:00,,0,0,triaged,True
3152,Sparse tensor .new(size) can be confusing module: sparse triaged,2017-10-17 18:19:13+00:00,,0,4,module: sparse triaged,True
3076,BN slows down double-backprop enormously module: performance triaged,2017-10-11 18:28:22+00:00,,1,14,module: performance triaged,True
3033,Autograd profiler should omit CUDA time columns on CPU profiler triaged oncall: profiler,2017-10-09 15:41:53+00:00,,0,0,triaged oncall: profiler,True
2854,Add the new lr_scheduler which called poly module: optimizer triaged,2017-09-25 11:33:47+00:00,,0,0,module: optimizer triaged,True
2849,Learning rate scheduler have different APIs module: optimizer triaged,2017-09-25 01:50:30+00:00,,0,2,module: optimizer triaged,True
2739,Proposal: simplify overloaded Tensor function signatures triaged,2017-09-14 19:39:14+00:00,,0,3,triaged,True
2732,support grid_sample with batch=1 but supprting batch affine parameters triaged module: vision module: interpolation,2017-09-14 06:37:56+00:00,,0,14,triaged module: vision module: interpolation,True
2724,DataLoader gets stuck after model initialization module: dataloader triaged,2017-09-13 17:43:27+00:00,,0,0,module: dataloader triaged,True
2703,Have ppc64le docker images?  triaged module: POWER,2017-09-12 07:06:45+00:00,,0,2,triaged module: POWER,True
2691,Feature Request: Support grad of grad in fused RNNs module: double backwards feature module: autograd module: nn triaged,2017-09-11 19:22:31+00:00,,0,1,module: double backwards feature module: autograd module: nn triaged,True
2629,[feature request] add pairwise ranking loss module: loss triaged,2017-09-05 16:05:07+00:00,,0,3,module: loss triaged,False
2628,BatchNorm{1-2-3}d are redundant module: nn triaged,2017-09-05 14:02:33+00:00,,0,3,module: nn triaged,True
2576,CUDA multinomial is limited to 2^24 categories high priority module: distributions module: cuda triaged module: 64-bit function request,2017-08-30 15:25:55+00:00,,0,11,high priority module: distributions module: cuda triaged module: 64-bit function request,True
2575,ImportError: dlopen: cannot load any more object with static TLS module: crash module: build triaged module: assert failure has workaround,2017-08-30 15:20:17+00:00,,2,58,module: crash module: build triaged module: assert failure has workaround,True
2569,Add a hash function for tensor data triaged function request,2017-08-30 03:31:04+00:00,,0,7,triaged function request,False
2545,Counter-intuitive Patience & Cooldown of ReduceLROnPlateau todo module: optimizer triaged,2017-08-27 00:48:32+00:00,,0,3,todo module: optimizer triaged,True
2539,"DataLoader converts cuda FloatTensor into cpu DoubleTensor when shape is (n,) needs reproduction module: dataloader triaged",2017-08-25 18:27:41+00:00,,0,9,needs reproduction module: dataloader triaged,True
2512,detach_() variant that affects all past uses too feature module: autograd triaged,2017-08-22 14:08:41+00:00,,0,1,feature module: autograd triaged,True
2482,"""Shared memory manager connection has timed out"" needs reproduction module: multiprocessing triaged",2017-08-18 15:20:55+00:00,,0,5,needs reproduction module: multiprocessing triaged,True
2478,ReduceLROnPlateau with a naive Backtracking feature module: optimizer triaged,2017-08-17 21:39:17+00:00,,1,8,feature module: optimizer triaged,False
2466,Autograd test failure on ppc64le triaged module: POWER,2017-08-17 00:16:13+00:00,,0,0,triaged module: POWER,True
2407,Hard-negative mining using __getitem__ directive in Dataset class module: dataloader triaged,2017-08-14 06:54:16+00:00,,0,5,module: dataloader triaged,True
2400,Add CRF Layer feature module: nn triaged,2017-08-13 04:27:43+00:00,,0,2,feature module: nn triaged,False
2312,DataParallel is not compatible with pack_padded_sequence awaiting response (this tag is deprecated) triaged module: data parallel,2017-08-07 07:12:10+00:00,,0,6,awaiting response (this tag is deprecated) triaged module: data parallel,True
2180,Discrepancy in BCEWithLogitsLoss and ClassNLLLoss module: loss module: cuda module: cpu triaged,2017-07-21 14:20:43+00:00,,0,4,module: loss module: cuda module: cpu triaged,True
2129,[Feature request] truncated normal initializer(sampler) triaged enhancement module: initialization,2017-07-17 08:15:29+00:00,,0,1,triaged enhancement module: initialization,True
2001,Implement similar PyTorch function as model.summary() in keras? feature module: nn triaged function request,2017-07-07 09:06:38+00:00,,0,22,feature module: nn triaged function request,False
1969,Naming inconsistencies module: nn triaged enhancement module: ux,2017-07-03 18:58:48+00:00,,0,5,module: nn triaged enhancement module: ux,True
1932,Feature Request: ReLU on LSTMs and GRUs feature module: nn module: rnn triaged,2017-06-28 19:51:03+00:00,,0,9,feature module: nn module: rnn triaged,True
1927,[feature request] time-distributed layers for application of normal layers to sequence data module: nn triaged,2017-06-28 12:59:25+00:00,,1,15,module: nn triaged,True
1861,Factorized Output Layer todo feature triaged,2017-06-21 08:27:10+00:00,,0,1,todo feature triaged,True
1794,Feature request: reverse_padded_sequence module: rnn triaged enhancement actionable,2017-06-13 19:44:25+00:00,,0,17,module: rnn triaged enhancement actionable,True
1750,Add support for colors (and maybe other attributes) to NVTX API newcomer module: cuda triaged,2017-06-07 18:41:12+00:00,,0,22,newcomer module: cuda triaged,True
1747,In-place bernoulli_ has more functionality than torch.bernoulli with output parameter module: distributions triaged,2017-06-07 14:04:57+00:00,,1,1,module: distributions triaged,True
1736,Expose optimizer options as attributes when there's a single param group module: bootcamp module: optimizer triaged enhancement,2017-06-06 10:45:53+00:00,,0,2,module: bootcamp module: optimizer triaged enhancement,True
1686,Feature Request: load_state_dict should take filenames feature module: nn triaged,2017-05-30 22:11:12+00:00,,0,3,feature module: nn triaged,False
1642,feature request: On-the-fly Operation Batching in Dynamic Computation Graphs feature triaged,2017-05-24 13:54:16+00:00,,0,1,feature triaged,False
1641,[Feature request] In-place 'max' method for Tensor todo feature triaged,2017-05-24 11:14:26+00:00,,0,7,todo feature triaged,False
1591,Pad PackedSequences to original batch length hackamonth triaged module: nestedtensor,2017-05-19 00:10:18+00:00,,1,7,hackamonth triaged module: nestedtensor,True
1529,[feature request] Caching allocator diagnostics and memory allocation tracing/visualization feature module: memory usage triaged,2017-05-10 20:08:34+00:00,,0,29,feature module: memory usage triaged,False
1512,[feature request] Support tensors of different sizes as batch elements in DataLoader feature module: dataloader triaged module: nestedtensor,2017-05-08 16:13:15+00:00,,0,18,feature module: dataloader triaged module: nestedtensor,True
1505,Cannot find Intel MKL module: dependency bug module: build triaged module: mkl,2017-05-07 18:36:31+00:00,,0,6,module: dependency bug module: build triaged module: mkl,True
1489,Optimizer should track parameter names and not id module: optimizer triaged,2017-05-05 16:26:45+00:00,,0,10,module: optimizer triaged,False
1487,[feature request/proposal] Relax scale_factor for nearest neighbor upsampling todo feature triaged,2017-05-05 15:35:21+00:00,,0,1,todo feature triaged,False
1468,avg_pool functions hold input for backward feature triaged module: pooling,2017-05-04 10:17:37+00:00,,0,2,feature triaged module: pooling,True
1462,"Change sparse_mask to take indexing mask, rather than entire sparse tensor module: sparse feature triaged",2017-05-03 20:01:36+00:00,,0,0,module: sparse feature triaged,True
1442,Optimizers can't be moved to a different GPU feature triaged,2017-05-02 19:23:02+00:00,,0,2,feature triaged,False
1410,BatchNorm should use Bessel's correction consistently module: nn triaged module: norms and normalization,2017-04-30 13:11:50+00:00,,0,7,module: nn triaged module: norms and normalization,True
1369,"""Sparsified"" mathematical operations module: sparse low priority triaged",2017-04-26 21:56:10+00:00,,0,15,module: sparse low priority triaged,True
1362,Feature Request: noise contrastive estimation/negative sampling feature module: nn triaged,2017-04-26 18:40:29+00:00,,0,31,feature module: nn triaged,False
1359,"Conjugate Gradient mentioned in docs, but not implemented todo feature module: optimizer triaged",2017-04-26 10:06:09+00:00,,0,9,todo feature module: optimizer triaged,False
1329,[proposed feature] Eve: Improving Stochastic Gradient Descent with Feedback feature module: optimizer triaged,2017-04-22 06:32:24+00:00,,0,1,feature module: optimizer triaged,False
1328,Unhelpful CrossEntropyLoss dimension error message module: loss module: cuda module: error checking triaged,2017-04-22 04:52:22+00:00,,0,8,module: loss module: cuda module: error checking triaged,True
1249,Dice Loss PR module: loss triaged enhancement Stale,2017-04-12 22:11:03+00:00,,0,48,module: loss triaged enhancement Stale,True
1178,Batched sparse QR factorizations and solves with cusolver module: sparse feature triaged module: linear algebra Stale,2017-04-03 15:03:12+00:00,,1,5,module: sparse feature triaged module: linear algebra Stale,True
934,dataloader parallels over elements vs over batches todo feature module: dataloader triaged,2017-03-06 06:20:39+00:00,,0,2,todo feature module: dataloader triaged,True
842,BCELoss doesn't accept LongTensor targets feature module: loss triaged Stale,2017-02-24 20:06:48+00:00,,0,4,feature module: loss triaged Stale,True
711,Feature Request: Easier to extend base RNN implementation feature triaged Stale,2017-02-09 22:45:17+00:00,,0,32,feature triaged Stale,False
634,Feature Request: NegativeSampling and HierarchicalSoftmax loss functions feature module: nn module: loss triaged Stale module: primTorch,2017-01-29 18:30:26+00:00,,0,11,feature module: nn module: loss triaged Stale module: primTorch,True
630,Add Peephole connections for LSTMs? feature triaged Stale,2017-01-29 06:14:27+00:00,,0,18,feature triaged Stale,False
499,Feature Request: Locally Connected Layer proposal accepted feature module: nn triaged Stale,2017-01-19 10:36:23+00:00,,0,23,proposal accepted feature module: nn triaged Stale,False
285,Keyword arguments passed to module's __call__ aren't forwarded to the hooks module: nn low priority triaged enhancement,2016-12-01 22:42:55+00:00,,0,1,module: nn low priority triaged enhancement,True
88,expose backend selection and cudnn settings to the end user module: cudnn feature triaged Stale,2016-10-01 21:30:04+00:00,,0,3,module: cudnn feature triaged Stale,True
